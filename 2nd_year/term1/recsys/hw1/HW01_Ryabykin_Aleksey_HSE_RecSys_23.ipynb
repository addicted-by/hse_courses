{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific scenario, the calculation of a user-item pair's score is based on the weighted sum of features, encompassing both user and item characteristics. When arranging items for a single user, the user-specific part of the prediction remains constant, and the list of sorted items is solely influenced by item features. Surprisingly, the sorted list for another user turns out to be the same due to the consistent nature of the user-specific component. This model, therefore, lacks the capacity for personalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Z_nyJUEY8gqcQArAGatB6LsScGirEb6u\n",
      "To: c:\\Users\\Aleksey Ryabykin\\Documents\\GitHub\\hse_courses\\2nd_year\\term1\\recsys\\hw1\\data\\animes.gz\n",
      "100%|██████████| 3.60M/3.60M [00:00<00:00, 10.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1CLWRy-mv-bZzlzlCFN07h2anl2E-CtXU\n",
      "To: c:\\Users\\Aleksey Ryabykin\\Documents\\GitHub\\hse_courses\\2nd_year\\term1\\recsys\\hw1\\data\\profiles.gz\n",
      "100%|██████████| 1.04M/1.04M [00:00<00:00, 4.49MB/s]\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1XQW-OQyo64RqsYEo5_gzfgygN-5O1pdn\n",
      "From (redirected): https://drive.google.com/uc?id=1XQW-OQyo64RqsYEo5_gzfgygN-5O1pdn&confirm=t&uuid=9560bdbc-5a4f-407c-ab77-33a0c0395efc\n",
      "To: c:\\Users\\Aleksey Ryabykin\\Documents\\GitHub\\hse_courses\\2nd_year\\term1\\recsys\\hw1\\data\\reviews.gz\n",
      "100%|██████████| 119M/119M [00:12<00:00, 9.64MB/s] \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import gdown\n",
    "DATA_PATH = Path(\"./data\")\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "ANIMES_DATA_PATH = DATA_PATH / \"animes.gz\"\n",
    "PROFILES_DATA_PATH = DATA_PATH / \"profiles.gz\"\n",
    "REVIEWS_DATA_PATH = DATA_PATH / \"reviews.gz\"\n",
    "\n",
    "urls = {\n",
    "    \"animes.gz\": \"https://drive.google.com/uc?id=1Z_nyJUEY8gqcQArAGatB6LsScGirEb6u\",\n",
    "    \"profiles.gz\": \"https://drive.google.com/uc?id=1CLWRy-mv-bZzlzlCFN07h2anl2E-CtXU\",\n",
    "    \"reviews.gz\": \"https://drive.google.com/uc?id=1XQW-OQyo64RqsYEo5_gzfgygN-5O1pdn\"\n",
    "}\n",
    "\n",
    "for file, url in urls.items():\n",
    "    gdown.download(url, str(DATA_PATH / file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "USER_COL = \"user_id\"\n",
    "ITEM_COL = \"anime_id\"\n",
    "RELEVANCE_COL = \"score\"\n",
    "\n",
    "animes_df = pd.read_csv(ANIMES_DATA_PATH, na_filter=False)\n",
    "reviews_df = pd.read_csv(REVIEWS_DATA_PATH)\n",
    "profiles_df = pd.read_csv(PROFILES_DATA_PATH, converters={'favorites_anime': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 37458\n",
      "Colunms: 5\n",
      "NaNs: 22846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthday</th>\n",
       "      <th>favorites_anime</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19202</th>\n",
       "      <td>W3lkin830</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[32, 849, 2001, 7311, 12189, 32281, 33489, 352...</td>\n",
       "      <td>https://myanimelist.net/profile/W3lkin830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>yniverse</td>\n",
       "      <td>Female</td>\n",
       "      <td>Apr 29</td>\n",
       "      <td>[7054, 20507, 21603, 22199, 28999, 32189, 33028]</td>\n",
       "      <td>https://myanimelist.net/profile/yniverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9404</th>\n",
       "      <td>ukiAY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2942, 10232, 13759, 21273, 31953, 34798, 3554...</td>\n",
       "      <td>https://myanimelist.net/profile/ukiAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  gender birthday  \\\n",
       "19202  W3lkin830    Male      NaN   \n",
       "1487    yniverse  Female   Apr 29   \n",
       "9404       ukiAY     NaN      NaN   \n",
       "\n",
       "                                         favorites_anime  \\\n",
       "19202  [32, 849, 2001, 7311, 12189, 32281, 33489, 352...   \n",
       "1487    [7054, 20507, 21603, 22199, 28999, 32189, 33028]   \n",
       "9404   [2942, 10232, 13759, 21273, 31953, 34798, 3554...   \n",
       "\n",
       "                                            link  \n",
       "19202  https://myanimelist.net/profile/W3lkin830  \n",
       "1487    https://myanimelist.net/profile/yniverse  \n",
       "9404       https://myanimelist.net/profile/ukiAY  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows: {}\\nColunms: {}\".format(*profiles_df.shape))\n",
    "print(\"NaNs: {}\".format(profiles_df.isna().sum().sum()))\n",
    "profiles_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 109297\n",
      "Colunms: 7\n",
      "NaNs: 0\n",
      "Duplicates: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>scores</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93035</th>\n",
       "      <td>272622</td>\n",
       "      <td>Mar33p</td>\n",
       "      <td>1793</td>\n",
       "      <td>It now being 2018 and seeing this anime for th...</td>\n",
       "      <td>6</td>\n",
       "      <td>{'Overall': '6', 'Story': '6', 'Animation': '6...</td>\n",
       "      <td>https://myanimelist.net/reviews.php?id=272622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40977</th>\n",
       "      <td>124599</td>\n",
       "      <td>Kundalini</td>\n",
       "      <td>18153</td>\n",
       "      <td>I'd rate this somewhere between 6 and 7 out of...</td>\n",
       "      <td>6</td>\n",
       "      <td>{'Overall': '6', 'Story': '7', 'Animation': '1...</td>\n",
       "      <td>https://myanimelist.net/reviews.php?id=124599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16010</th>\n",
       "      <td>199283</td>\n",
       "      <td>UrsulaCallistis</td>\n",
       "      <td>23623</td>\n",
       "      <td>Non Non Biyori Repeat returns once more to Ren...</td>\n",
       "      <td>9</td>\n",
       "      <td>{'Overall': '9', 'Story': '8', 'Animation': '1...</td>\n",
       "      <td>https://myanimelist.net/reviews.php?id=199283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid          user_id  anime_id  \\\n",
       "93035  272622           Mar33p      1793   \n",
       "40977  124599        Kundalini     18153   \n",
       "16010  199283  UrsulaCallistis     23623   \n",
       "\n",
       "                                                    text  score  \\\n",
       "93035  It now being 2018 and seeing this anime for th...      6   \n",
       "40977  I'd rate this somewhere between 6 and 7 out of...      6   \n",
       "16010  Non Non Biyori Repeat returns once more to Ren...      9   \n",
       "\n",
       "                                                  scores  \\\n",
       "93035  {'Overall': '6', 'Story': '6', 'Animation': '6...   \n",
       "40977  {'Overall': '6', 'Story': '7', 'Animation': '1...   \n",
       "16010  {'Overall': '9', 'Story': '8', 'Animation': '1...   \n",
       "\n",
       "                                                link  \n",
       "93035  https://myanimelist.net/reviews.php?id=272622  \n",
       "40977  https://myanimelist.net/reviews.php?id=124599  \n",
       "16010  https://myanimelist.net/reviews.php?id=199283  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows: {}\\nColunms: {}\".format(*reviews_df.shape))\n",
    "print(\"NaNs: {}\".format(reviews_df.isna().sum().sum()))\n",
    "print(\"Duplicates: {}\".format(reviews_df.duplicated().sum()))\n",
    "reviews_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 16216\n",
      "Colunms: 12\n",
      "NaNs: 0\n",
      "Duplicates: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "      <th>aired</th>\n",
       "      <th>episodes</th>\n",
       "      <th>members</th>\n",
       "      <th>popularity</th>\n",
       "      <th>ranked</th>\n",
       "      <th>score</th>\n",
       "      <th>img_url</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>6351</td>\n",
       "      <td>Clannad: After Story - Mou Hitotsu no Sekai, K...</td>\n",
       "      <td>Included in the 8th and final DVD of Clannad ~...</td>\n",
       "      <td>Drama, Romance, School</td>\n",
       "      <td>Jul 1, 2009</td>\n",
       "      <td>1</td>\n",
       "      <td>191025</td>\n",
       "      <td>567</td>\n",
       "      <td>663.0</td>\n",
       "      <td>7.93</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/10/19...</td>\n",
       "      <td>https://myanimelist.net/anime/6351/Clannad__Af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7392</th>\n",
       "      <td>16458</td>\n",
       "      <td>Perrine Monogatari Movie</td>\n",
       "      <td>Movie version of the TV series Perrine Monogat...</td>\n",
       "      <td>Drama, Historical, Shoujo, Slice of Life</td>\n",
       "      <td>Jun 30, 1990</td>\n",
       "      <td>1</td>\n",
       "      <td>283</td>\n",
       "      <td>12343</td>\n",
       "      <td>13535.0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/3/444...</td>\n",
       "      <td>https://myanimelist.net/anime/16458/Perrine_Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>1374</td>\n",
       "      <td>Kyoushoku Soukou Guyver (2005)</td>\n",
       "      <td>Sho Fukamachi, a normal teenager accidentally ...</td>\n",
       "      <td>Adventure, Sci-Fi, Shounen</td>\n",
       "      <td>Aug 6, 2005 to Feb 18, 2006</td>\n",
       "      <td>26</td>\n",
       "      <td>16662</td>\n",
       "      <td>3495</td>\n",
       "      <td>2462.0</td>\n",
       "      <td>7.32</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/6/369...</td>\n",
       "      <td>https://myanimelist.net/anime/1374/Kyoushoku_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anime_id                                              title  \\\n",
       "4537      6351  Clannad: After Story - Mou Hitotsu no Sekai, K...   \n",
       "7392     16458                           Perrine Monogatari Movie   \n",
       "1232      1374                     Kyoushoku Soukou Guyver (2005)   \n",
       "\n",
       "                                               synopsis  \\\n",
       "4537  Included in the 8th and final DVD of Clannad ~...   \n",
       "7392  Movie version of the TV series Perrine Monogat...   \n",
       "1232  Sho Fukamachi, a normal teenager accidentally ...   \n",
       "\n",
       "                                         genre                        aired  \\\n",
       "4537                    Drama, Romance, School                  Jul 1, 2009   \n",
       "7392  Drama, Historical, Shoujo, Slice of Life                 Jun 30, 1990   \n",
       "1232                Adventure, Sci-Fi, Shounen  Aug 6, 2005 to Feb 18, 2006   \n",
       "\n",
       "      episodes  members  popularity   ranked  score  \\\n",
       "4537         1   191025         567    663.0   7.93   \n",
       "7392         1      283       12343  13535.0   6.40   \n",
       "1232        26    16662        3495   2462.0   7.32   \n",
       "\n",
       "                                                img_url  \\\n",
       "4537  https://cdn.myanimelist.net/images/anime/10/19...   \n",
       "7392  https://cdn.myanimelist.net/images/anime/3/444...   \n",
       "1232  https://cdn.myanimelist.net/images/anime/6/369...   \n",
       "\n",
       "                                                   link  \n",
       "4537  https://myanimelist.net/anime/6351/Clannad__Af...  \n",
       "7392  https://myanimelist.net/anime/16458/Perrine_Mo...  \n",
       "1232  https://myanimelist.net/anime/1374/Kyoushoku_S...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows: {}\\nColunms: {}\".format(*animes_df.shape))\n",
    "print(\"NaNs: {}\".format(animes_df.isna().sum().sum()))\n",
    "print(\"Duplicates: {}\".format(animes_df.duplicated().sum()))\n",
    "animes_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take function from the seminar to obtain the test users who have at least 5 interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_pairs(reviews, favorites, n_pairs, score_cutoff, seed):\n",
    "    '''\n",
    "    Construct a dataset consisting of pairs of liked and disliked animes. The likes and dislikes\n",
    "    are defined by the ratings value: everything below threshold is a dislike, the rest are likes.\n",
    "\n",
    "    The function ensures that the amount of likes and dislikes is the same per each user in data.\n",
    "    The users that do not contain enough likes or dislikes are discarded from the result.\n",
    "    The result is to be used for evaluating the quality of recommendations by some algorithms.\n",
    "    Hence, user favorites are excluded to ensure that there is no trivial solution.\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def strict_sample_no_favs(series):\n",
    "        # sample `n_pairs` elements from `series`, if not enough data - return empty list,\n",
    "        # discard favorites, otherwise the evaluation on test pairs against favorites makes no sense\n",
    "        above_cutoff, user_id = series.name\n",
    "        allowed_items = np.setdiff1d(series.values, favorites.loc[user_id])\n",
    "        return rng.choice(allowed_items, n_pairs, replace=False) if len(allowed_items)>=n_pairs else []\n",
    "\n",
    "    test_pairs = (\n",
    "        reviews\n",
    "         # split by likes and dislikes, group by users\n",
    "        .groupby([(reviews[\"score\"] >= score_cutoff), 'user_id'])\n",
    "        # sample `n_pairs` items (both likes and dislikes), disregard user favorites\n",
    "        ['anime_id'].apply(strict_sample_no_favs)\n",
    "         # disregard users that have not enough items\n",
    "        .loc[lambda x: x.apply(len) > 0]\n",
    "         # make two columns of likes and dislikes\n",
    "        .unstack('score')\n",
    "        # ensure each user has both likes and dislikes\n",
    "        .dropna()\n",
    "         # rename by rule `score >= score_cutoff`\n",
    "        .rename(columns={False: 'dislikes', True: 'likes'})\n",
    "    )\n",
    "    return test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites = profiles_df.set_index('user_id')['favorites_anime']\n",
    "favorites_scores = pd.merge(\n",
    "    favorites.explode().rename('anime_id').reset_index(),\n",
    "    reviews_df[['user_id', 'anime_id', 'score']],\n",
    "    on = ['user_id', 'anime_id'],\n",
    "    how = 'left'\n",
    ")['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = get_test_pairs(reviews_df, favorites, 3, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write function for model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict\n",
    "def model_evaluate(\n",
    "        recommended_items: Iterable,\n",
    "        holdout_items: Iterable,\n",
    "        total_items: int=len(reviews_df['anime_id'].unique()),\n",
    "        top_n: int=10\n",
    "):\n",
    "    hits_mask = recommended_items[:, :top_n] == holdout_items[:, None]\n",
    "    hr = np.mean(hits_mask.any(axis=1))\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_users\n",
    "    coverage_score = len(np.unique(recommended_items)) / total_items\n",
    "    return hr, mrr, coverage_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(reviews, favorites, test_pairs, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    test_items = test_pairs['likes'].apply(lambda x: rng.choice(x))\n",
    "\n",
    "    train_items = (\n",
    "        reviews.query('user_id in @test_pairs.index')\n",
    "        .groupby('user_id')\n",
    "        ['anime_id'].apply(\n",
    "            lambda row: np.setdiff1d(\n",
    "                row.values, \n",
    "                favorites.loc[row.name] + [test_items.loc[row.name]]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return train_items, test_items\n",
    "\n",
    "train_items, test_items = split_train_test_data(reviews_df, favorites, test_pairs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the pipeline from the seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert sparse matrix to dense np array to apply standard scaler with mean.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def build_cb_model(config, trainset, trainset_description, logistic=False, binary_vectorizer=True):\n",
    "    \"\"\"\n",
    "    Config and fit cb model\n",
    "    \"\"\"\n",
    "    feature_matrix, word_vectorizer = generate_features(config, trainset, trainset_description, binary_vectorizer)\n",
    "    if logistic:\n",
    "        regressor = LogisticRegression\n",
    "    elif 'alpha' in config['model']:\n",
    "        regressor = Ridge\n",
    "    else:\n",
    "        regressor = LinearRegression\n",
    "    target_column = trainset_description['feedback']\n",
    "    model = regressor(**config['model']).fit(feature_matrix, trainset[target_column])\n",
    "    return model, word_vectorizer\n",
    "\n",
    "def generate_features(config, trainset, trainset_description, binary_vectorizer):\n",
    "    \"\"\"\n",
    "    Config and fit text vectorizer\n",
    "    \"\"\"\n",
    "    if binary_vectorizer:\n",
    "        word_vectorizer = CountVectorizer(**config['vectorizer']['binary'])\n",
    "    else:\n",
    "        word_vectorizer = Pipeline([(\"tfidf\", TfidfVectorizer(**config['vectorizer']['tfidf'])), \n",
    "                                    ('dense', DenseTransformer()), \n",
    "                                    (\"scaler\", StandardScaler())])\n",
    "    features_column = trainset_description['item_features']\n",
    "    feature_matrix = word_vectorizer.fit_transform(trainset[features_column])\n",
    "    return feature_matrix, word_vectorizer\n",
    "\n",
    "\n",
    "def transform_predict(params, tokens):\n",
    "    \"\"\"\n",
    "    Get recommendations from either classification or regression model\n",
    "    \"\"\"\n",
    "    model, word_vectorizer = params\n",
    "    tokens_encoded = word_vectorizer.transform(tokens)\n",
    "    try: # handle classification models\n",
    "        predictor = model.predict_proba\n",
    "    except AttributeError:\n",
    "        predictor = model.predict\n",
    "    scores = predictor(tokens_encoded)\n",
    "    if scores.ndim > 1: # handle classification\n",
    "        scores = scores[:, 1] # take class 1\n",
    "    return scores\n",
    "\n",
    "def cb_model_scoring(params, testset, testset_description):\n",
    "    \"\"\"\n",
    "    Select necessary features and get recommendations with the fitted pipeline\n",
    "    \"\"\"\n",
    "    tokens = testset[testset_description['item_features']]\n",
    "    scores = transform_predict(params, tokens)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_config = {\n",
    "    \"model\": dict(),\n",
    "    \"vectorizer\":{\n",
    "        \"binary\": dict( # simple binary token encoder\n",
    "            min_df = 1,\n",
    "            max_df = 0.9,\n",
    "            strip_accents='unicode',\n",
    "            stop_words = 'english',\n",
    "            analyzer = 'word',\n",
    "            binary = True,\n",
    "        ),\n",
    "        \"tfidf\": dict( # TfIDF Vectorizer\n",
    "            min_df = 1,\n",
    "            max_df = 0.9,\n",
    "            strip_accents='unicode',\n",
    "            stop_words = 'english',\n",
    "            analyzer = 'word',\n",
    "            use_idf = True,\n",
    "            smooth_idf = True,\n",
    "            sublinear_tf = True,\n",
    "            binary = False,\n",
    "            norm=\"l2\",\n",
    "        ),\n",
    "    }\n",
    "}\n",
    "# we also define a general representation of our dataset\n",
    "anime_description = {\n",
    "    'feedback' : \"score\",\n",
    "    \"item_features\": \"tokens\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: ' '.join(x.strip().split(', '))\n",
    "animes_df['tokens'] = animes_df['genre'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_user(reviews, anime_data, user_id, train_item_ids, all_item_ids, k=10, tokens='tokens', binary_vectorizer=True):\n",
    "    # Step 1: Filter user-specific reviews and fetch anime tokens\n",
    "    user_reviews = reviews[(reviews['user_id'] == user_id) & (reviews['anime_id'].isin(train_item_ids))]\n",
    "    user_reviews = user_reviews.merge(anime_data[['anime_id', tokens]], on='anime_id', how='left')\n",
    "\n",
    "    # Step 2: Build a content-based recommendation model\n",
    "    cb_model = build_cb_model(cb_config, user_reviews, anime_description, binary_vectorizer=binary_vectorizer)\n",
    "\n",
    "    # Step 3: Find items to score\n",
    "    items_to_score = np.setdiff1d(all_item_ids, train_item_ids)\n",
    "    features_for_scoring = anime_data[anime_data['anime_id'].isin(items_to_score)].copy()\n",
    "\n",
    "    # Step 4: Make predictions and sort by predicted score\n",
    "    features_for_scoring['predicted_score'] = cb_model_scoring(cb_model, features_for_scoring, anime_description)\n",
    "    top_k_anime_ids = features_for_scoring.sort_values(by='predicted_score', ascending=False)['anime_id'].head(k).tolist()\n",
    "\n",
    "    return top_k_anime_ids\n",
    "\n",
    "def make_predictions_for_all_users(reviews, anime_data, train_items, **kwargs):\n",
    "    all_animes = reviews['anime_id'].unique()\n",
    "    predictions = []\n",
    "\n",
    "    for user_id, train_animes in tqdm(\n",
    "        train_items.items(), \n",
    "        total=len(train_items)\n",
    "    ):\n",
    "        predictions.append(\n",
    "            get_predictions_for_user(\n",
    "                reviews, anime_data, user_id, \n",
    "                train_animes, all_animes, **kwargs\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "scores = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:37<00:00, 18.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.4 s\n",
      "Wall time: 37.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hr@10': 0.004310344827586207,\n",
       " 'mrr@10': 0.001955619412515964,\n",
       " 'coverage@10': 0.35620743844944996}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions = make_predictions_for_all_users(reviews_df, animes_df, train_items)\n",
    "hr, mrr, coverage = model_evaluate(predictions, test_items.values)\n",
    "scores[\"genre\"] = {\n",
    "    \"hr@10\": hr,\n",
    "    \"mrr@10\": mrr,\n",
    "    \"coverage@10\": coverage\n",
    "}\n",
    "scores['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:36<00:00, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23.4 s\n",
      "Wall time: 36.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hr@10': 0.004310344827586207,\n",
       " 'mrr@10': 0.001125478927203065,\n",
       " 'coverage@10': 0.4848088004190676}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions = make_predictions_for_all_users(reviews_df, animes_df, train_items, binary_vectorizer=False)\n",
    "hr, mrr, coverage = model_evaluate(predictions, test_items.values)\n",
    "scores[\"genre_tf_idf\"] = {\n",
    "    \"hr@10\": hr,\n",
    "    \"mrr@10\": mrr,\n",
    "    \"coverage@10\": coverage\n",
    "}\n",
    "scores['genre_tf_idf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"alpha\": 20000, \"random_state\": 0xDEAD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:38<00:00, 17.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.3 s\n",
      "Wall time: 39 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hr@10': 0.011494252873563218,\n",
       " 'mrr@10': 0.004395867542419266,\n",
       " 'coverage@10': 0.41618648507071765}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions = make_predictions_for_all_users(reviews_df, animes_df, train_items, binary_vectorizer=False)\n",
    "hr, mrr, coverage = model_evaluate(predictions, test_items.values)\n",
    "scores[\"genre_tf_idf_A2000\"] = {\n",
    "    \"hr@10\": hr,\n",
    "    \"mrr@10\": mrr,\n",
    "    \"coverage@10\": coverage\n",
    "}\n",
    "scores['genre_tf_idf_A2000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better with regularization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_df['advanced_tokens'] = animes_df['tokens'] + animes_df['synopsis'].apply(tokenize) + animes_df['title'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_description = {\n",
    "    'feedback' : \"score\",\n",
    "    \"item_features\": \"advanced_tokens\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/696 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [05:25<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 39s\n",
      "Wall time: 5min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hr@10': 0.035919540229885055,\n",
       " 'mrr@10': 0.014890644955300127,\n",
       " 'coverage@10': 0.33800419067574644}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions = make_predictions_for_all_users(reviews_df, animes_df, train_items, tokens=\"advanced_tokens\", binary_vectorizer=True)\n",
    "hr, mrr, coverage = model_evaluate(predictions, test_items.values)\n",
    "scores[\"advanced_A2000\"] = {\n",
    "    \"hr@10\": hr,\n",
    "    \"mrr@10\": mrr,\n",
    "    \"coverage@10\": coverage\n",
    "}\n",
    "scores['advanced_A2000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POP/RANDOM BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_animes(reviews, anime_data, n_users, k, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    animes_with_reviews = reviews['anime_id'].unique()\n",
    "    anime_with_ratings = anime_data.loc[anime_data['anime_id'].isin(animes_with_reviews)][['anime_id', 'score']]\n",
    "    sampled_animes = rng.choice(anime_with_ratings['anime_id'], size=(n_users, k), replace=True)\n",
    "    return sampled_animes\n",
    "\n",
    "def sample_popular_animes(reviews, anime_data, n_users, k, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    animes_with_reviews = reviews['anime_id'].unique()\n",
    "    anime_with_ratings = anime_data.loc[anime_data['anime_id'].isin(animes_with_reviews)][['anime_id', 'score']]\n",
    "    weights = anime_with_ratings['score'] / anime_with_ratings['score'].sum()\n",
    "    sampled_animes = rng.choice(anime_with_ratings['anime_id'], size=(n_users, k), replace=True, p=weights)\n",
    "    return sampled_animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hr@10': 0.0016666666666666679,\n",
       " 'mrr@10': 0.0004989509213647146,\n",
       " 'coverage@10': 0.5949122577265588}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds = np.random.SeedSequence(0xDEAD).spawn(100)\n",
    "hr_sum, mrr_sum, coverage_sum = 0, 0, 0\n",
    "for seed in seeds:\n",
    "    popular = sample_popular_animes(reviews_df, animes_df, len(test_items), 10, seed)\n",
    "    metrics = model_evaluate(popular, test_items.values)\n",
    "    hr_sum += metrics[0]\n",
    "    mrr_sum += metrics[1]\n",
    "    coverage_sum += metrics[2]\n",
    "\n",
    "\n",
    "scores[\"pop\"] = {\n",
    "    \"hr@10\": hr_sum / len(seeds),\n",
    "    \"mrr@10\": mrr_sum / len(seeds),\n",
    "    \"coverage@10\": coverage_sum / len(seeds),\n",
    "}\n",
    "\n",
    "scores[\"pop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hr@10': 0.0015517241379310342,\n",
       " 'mrr@10': 0.0004420954205436964,\n",
       " 'coverage@10': 0.5982700366684127}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_sum, mrr_sum, coverage_sum = 0, 0, 0\n",
    "for seed in seeds:\n",
    "    popular = sample_random_animes(reviews_df, animes_df, len(test_items), 10, seed)\n",
    "    metrics = model_evaluate(popular, test_items.values)\n",
    "    hr_sum += metrics[0]\n",
    "    mrr_sum += metrics[1]\n",
    "    coverage_sum += metrics[2]\n",
    "\n",
    "\n",
    "scores[\"random\"] = {\n",
    "    \"hr@10\": hr_sum / len(seeds),\n",
    "    \"mrr@10\": mrr_sum / len(seeds),\n",
    "    \"coverage@10\": coverage_sum / len(seeds),\n",
    "}\n",
    "\n",
    "scores[\"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'genre': scores['genre'],\n",
    "    'genre_tf_idf': scores['genre_tf_idf'],\n",
    "    'genre_tf_idf_A2000': scores['genre_tf_idf_A2000'],\n",
    "    'pop': scores['pop'],\n",
    "    'random': scores['random'],\n",
    "    'advanced_A2000': scores['advanced_A2000']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>genre_tf_idf</th>\n",
       "      <th>genre_tf_idf_A2000</th>\n",
       "      <th>pop</th>\n",
       "      <th>random</th>\n",
       "      <th>advanced_A2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hr@10</th>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.035920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrr@10</th>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.014891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage@10</th>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.484809</td>\n",
       "      <td>0.416186</td>\n",
       "      <td>0.594912</td>\n",
       "      <td>0.598270</td>\n",
       "      <td>0.338004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                genre  genre_tf_idf  genre_tf_idf_A2000       pop    random  \\\n",
       "hr@10        0.004310      0.004310            0.011494  0.001667  0.001552   \n",
       "mrr@10       0.001956      0.001125            0.004396  0.000499  0.000442   \n",
       "coverage@10  0.356207      0.484809            0.416186  0.594912  0.598270   \n",
       "\n",
       "             advanced_A2000  \n",
       "hr@10              0.035920  \n",
       "mrr@10             0.014891  \n",
       "coverage@10        0.338004  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
