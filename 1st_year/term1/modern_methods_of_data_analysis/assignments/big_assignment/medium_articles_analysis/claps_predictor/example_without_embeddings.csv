idx,title,publication,author,followers,reading_time,n_words,pure_text,date,n_code_chunks,bold_text_count,italic_text_count,mean_image_size,n_images,n_lists,n_vids,n_links,language,key_word_1,key_word_2,key_word_3,key_word_4,key_word_5,key_word_6,key_word_7,key_word_8,key_word_9,key_word_10,analytic,polarity,emotion
1,Understanding Variational Autoencoders (VAEs),Towards Data Science,Joseph Rocca,3700.0,23.0,4892,"This post was co-written with In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data, well-designed networks architectures and smart training techniques, deep generative models have shown an incredible ability to produce highly realistic pieces of content of various kind, such as images, texts and sounds. Among these deep generative models, two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).In a previous post, published in January of this year, we discussed in depth Generative Adversarial Networks (GANs) and showed, in particular, how adversarial training can oppose two networks, a generator and a discriminator, to push both of them to improve iteration after iteration. We introduce now, in this post, the other major kind of deep generative models: Variational Autoencoders (VAEs). In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term “variational” comes from the close relation there is between the regularisation and the variational inference method in statistics.If the last two sentences summarise pretty well the notion of VAEs, they can also raise a lot of questions. What is an autoencoder? What is the latent space and why regularising it? How to generate new data from VAEs? What is the link between VAEs and variational inference? In order to describe VAEs as well as possible, we will try to answer all this questions (and many others!) and to provide the reader with as much insights as we can (ranging from basic intuitions to more advanced mathematical details). Thus, the purpose of this post is not only to discuss the fundamental notions Variational Autoencoders rely on but also to build step by step and starting from the very beginning the reasoning that leads to these notions.Without further ado, let’s (re)discover VAEs together!In the first section, we will review some important notions about dimensionality reduction and autoencoder that will be useful for the understanding of VAEs. Then, in the second section, we will show why autoencoders cannot be used to generate new data and will introduce Variational Autoencoders that are regularised versions of autoencoders making the generative process possible. Finally in the last section we will give a more mathematical presentation of VAEs, based on variational inference.Note. In the last section we have tried to make the mathematical derivation as complete and clear as possible to bridge the gap between intuitions and equations. However, the readers that doesn’t want to dive into the mathematical details of VAEs can skip this section without hurting the understanding of the main concepts. Notice also that in this post we will make the following abuse of notation: for a random variable z, we will denote p(z) the distribution (or the density, depending on the context) of this random variable.In this first section we will start by discussing some notions related to dimensionality reduction. In particular, we will review briefly principal component analysis (PCA) and autoencoders, showing how both ideas are related to each others.In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data (data visualisation, data storage, heavy computation…). Although there exists many different methods of dimensionality reduction, we can set a global framework that is matched by most (if not any!) of these methods.First, let’s call encoder the process that produce the “new features” representation from the “old features” representation (by selection or by extraction) and decoder the reverse process. Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding.The main purpose of a dimensionality reduction method is to find the best encoder/decoder pair among a given family. In other words, for a given set of possible encoders and decoders, we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. If we denote respectively E and D the families of encoders and decoders we are considering, then the dimensionality reduction problem can be writtenwheredefines the reconstruction error measure between the input data x and the encoded-decoded data d(e(x)). Notice finally that in the following we will denote N the number of data, n_d the dimension of the initial (decoded) space and n_e the dimension of the reduced (encoded) space.One of the first methods that come in mind when speaking about dimensionality reduction is principal component analysis (PCA). In order to show how it fits the framework we just described and make the link towards autoencoders, let’s give a very high overview of how PCA works, letting most of the details aside (notice that we plan to write a full post on the subject).The idea of PCA is to build n_e new independent features that are linear combinations of the n_d old features and so that the projections of the data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance). In other words, PCA is looking for the best linear subspace of the initial space (described by an orthogonal basis of new features) such that the error of approximating the data by their projections on this subspace is as small as possible.Translated in our global framework, we are looking for an encoder in the family E of the n_e by n_d matrices (linear transformation) whose rows are orthonormal (features independence) and for the associated decoder among the family D of n_d by n_e matrices. It can be shown that the unitary eigenvectors corresponding to the n_e greatest eigenvalues (in norm) of the covariance features matrix are orthogonal (or can be chosen to be so) and define the best subspace of dimension n_e to project data on with minimal error of approximation. Thus, these n_e eigenvectors can be chosen as our new features and, so, the problem of dimension reduction can then be expressed as an eigenvalue/eigenvector problem. Moreover, it can also be shown that, in such case, the decoder matrix is the transposed of the encoder matrix.Let’s now discuss autoencoders and see how we can use neural networks for dimensionality reduction. The general idea of autoencoders is pretty simple and consists in setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks.Thus, intuitively, the overall autoencoder architecture (encoder+decoder) creates a bottleneck for data that ensures only the main structured part of the information can go through and be reconstructed. Looking at our general framework, the family E of considered encoders is defined by the encoder network architecture, the family D of considered decoders is defined by the decoder network architecture and the search of encoder and decoder that minimise the reconstruction error is done by gradient descent over the parameters of these networks.Let’s first suppose that both our encoder and decoder architectures have only one layer without non-linearity (linear autoencoder). Such encoder and decoder are then simple linear transformations that can be expressed as matrices. In such situation, we can see a clear link with PCA in the sense that, just like PCA does, we are looking for the best linear subspace to project data on with as few information loss as possible when doing so. Encoding and decoding matrices obtained with PCA define naturally one of the solutions we would be satisfied to reach by gradient descent, but we should outline that this is not the only one. Indeed, several basis can be chosen to describe the same optimal subspace and, so, several encoder/decoder pairs can give the optimal reconstruction error. Moreover, for linear autoencoders and contrarily to PCA, the new features we end up do not have to be independent (no orthogonality constraints in the neural networks).Now, let’s assume that both the encoder and the decoder are deep and non-linear. In such case, the more complex the architecture is, the more the autoencoder can proceed to a high dimensionality reduction while keeping reconstruction loss low. Intuitively, if our encoder and our decoder have enough degrees of freedom, we can reduce any initial dimensionality to 1. Indeed, an encoder with “infinite power” could theoretically takes our N initial data points and encodes them as 1, 2, 3, … up to N (or more generally, as N integer on the real axis) and the associated decoder could make the reverse transformation, with no loss during the process.Here, we should however keep two things in mind. First, an important dimensionality reduction with no reconstruction loss often comes with a price: the lack of interpretable and exploitable structures in the latent space (lack of regularity). Second, most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations. For these two reasons, the dimension of the latent space and the “depth” of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction.Up to now, we have discussed dimensionality reduction problem and introduce autoencoders that are encoder-decoder architectures that can be trained by gradient descent. Let’s now make the link with the content generation problem, see the limitations of autoencoders in their current form for this problem and introduce Variational Autoencoders.At this point, a natural question that comes in mind is “what is the link between autoencoders and content generation?”. Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well “organized” by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network.However, as we discussed in the previous section, the regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described.To illustrate this point, let’s consider the example we gave previously in which we described an encoder and a decoder powerful enough to put any N initial training data onto the real axis (each data point being encoded as a real value) and decode them without any reconstruction loss. In such case, the high degree of freedom of the autoencoder that makes possible to encode and decode with no information loss (despite the low dimensionality of the latent space) leads to a severe overfitting implying that some points of the latent space will give meaningless content once decoded. If this one dimensional example has been voluntarily chosen to be quite extreme, we can notice that the problem of the autoencoders latent space regularity is much more general than that and deserve a special attention.When thinking about it for a minute, this lack of structure among the encoded data into the latent space is pretty normal. Indeed, nothing in the task the autoencoder is trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised. Thus, if we are not careful about the definition of the architecture, it is natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it can… unless we explicitly regularise it!So, in order to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. Thus, as we briefly mentioned in the introduction of this post, a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space. The model is then trained as follows:In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. We will see in the next subsection that we ensure this way both a local and global regularisation of the latent space (local because of the variance control and global because of the mean control).Thus, the loss function that is minimised when training a VAE is composed of a “reconstruction term” (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a “regularisation term” (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. That regularisation term is expressed as the Kulback-Leibler divergence between the returned distribution and a standard Gaussian and will be further justified in the next section. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions.The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: continuity (two close points in the latent space should not give two completely different contents once decoded) and completeness (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded).The only fact that VAEs encode inputs as distributions instead of simple points is not sufficient to ensure continuity and completeness. Without a well defined regularisation term, the model can learn, in order to minimise its reconstruction error, to “ignore” the fact that distributions are returned and behave almost like classic autoencoders (leading to overfitting). To do so, the encoder can either return distributions with tiny variances (that would tend to be punctual distributions) or return distributions with very different means (that would then be really far apart from each other in the latent space). In both cases, distributions are used the wrong way (cancelling the expected benefit) and continuity and/or completeness are not satisfied.So, in order to avoid these effects we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder. In practice, this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced). This way, we require the covariance matrices to be close to the identity, preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far apart from each others.With this regularisation term, we prevent the model to encode data far apart in the latent space and encourage as much as possible returned distributions to “overlap”, satisfying this way the expected continuity and completeness conditions. Naturally, as for any regularisation term, this comes at the price of a higher reconstruction error on the training data. The tradeoff between the reconstruction error and the KL divergence can however be adjusted and we will see in the next section how the expression of the balance naturally emerge from our formal derivation.To conclude this subsection, we can observe that continuity and completeness obtained with regularisation tend to create a “gradient” over the information encoded in the latent space. For example, a point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases.Note. As a side note, we can mention that the second potential problem we have mentioned (the network put distributions far from each others) is in fact almost equivalent to the first one (the network tends to return punctual distribution) up to a change of scale: in both case variances of distributions become small relatively to distance between their means.In the previous section we gave the following intuitive overview: VAEs are autoencoders that encode inputs as distributions instead of points and whose latent space “organisation” is regularised by constraining distributions returned by the encoder to be close to a standard Gaussian. In this section we will give a more mathematical view of VAEs that will allow us to justify the regularisation term more rigorously. To do so, we will set a clear probabilistic framework and will use, in particular, variational inference technique.Let’s begin by defining a probabilistic graphical model to describe our data. We denote by x the variable that represents our data and assume that x is generated from a latent variable z (the encoded representation) that is not directly observed. Thus, for each data point, the following two steps generative process is assumed:With such a probabilistic model in mind, we can redefine our notions of encoder and decoder. Indeed, contrarily to a simple autoencoder that consider deterministic encoder and decoder, we are going to consider now probabilistic versions of these two objects. The “probabilistic decoder” is naturally defined by p(x|z), that describes the distribution of the decoded variable given the encoded one, whereas the “probabilistic encoder” is defined by p(z|x), that describes the distribution of the encoded variable given the decoded one.At this point, we can already notice that the regularisation of the latent space that we lacked in simple autoencoders naturally appears here in the definition of the data generation process: encoded representations z in the latent space are indeed assumed to follow the prior distribution p(z). Otherwise, we can also remind the the well-known Bayes theorem that makes the link between the prior p(z), the likelihood p(x|z), and the posterior p(z|x)Let’s now make the assumption that p(z) is a standard Gaussian distribution and that p(x|z) is a Gaussian distribution whose mean is defined by a deterministic function f of the variable of z and whose covariance matrix has the form of a positive constant c that multiplies the identity matrix I. The function f is assumed to belong to a family of functions denoted F that is left unspecified for the moment and that will be chosen later. Thus, we haveLet’s consider, for now, that f is well defined and fixed. In theory, as we know p(z) and p(x|z), we can use the Bayes theorem to compute p(z|x): this is a classical Bayesian inference problem. However, as we discussed in our previous article, this kind of computation is often intractable (because of the integral at the denominator) and require the use of approximation techniques such as variational inference.Note. Here we can mention that p(z) and p(x|z) are both Gaussian distribution. So, if we had E(x|z) = f(z) = z, it would imply that p(z|x) should also follow a Gaussian distribution and, in theory, we could “only” try to express the mean and the covariance matrix of p(z|x) with respect to the means and the covariance matrices of p(z) and p(x|z). However, in practice this condition is not met and we need to use of an approximation technique like variational inference that makes the approach pretty general and more robust to some changes in the hypothesis of the model.In statistics, variational inference (VI) is a technique to approximate complex distributions. The idea is to set a parametrised family of distribution (for example the family of Gaussians, whose parameters are the mean and the covariance) and to look for the best approximation of our target distribution among this family. The best element in the family is one that minimise a given approximation error measurement (most of the time the Kullback-Leibler divergence between approximation and target) and is found by gradient descent over the parameters that describe the family. For more details, we refer to our post on variational inference and references therein.Here we are going to approximate p(z|x) by a Gaussian distribution q_x(z) whose mean and covariance are defined by two functions, g and h, of the parameter x. These two functions are supposed to belong, respectively, to the families of functions G and H that will be specified later but that are supposed to be parametrised. Thus we can denoteSo, we have defined this way a family of candidates for variational inference and need now to find the best approximation among this family by optimising the functions g and h (in fact, their parameters) to minimise the Kullback-Leibler divergence between the approximation and the target p(z|x). In other words, we are looking for the optimal g* and h* such thatIn the second last equation, we can observe the tradeoff there exists — when approximating the posterior p(z|x) — between maximising the likelihood of the “observations” (maximisation of the expected log-likelihood, for the first term) and staying close to the prior distribution (minimisation of the KL divergence between q_x(z) and p(z), for the second term). This tradeoff is natural for Bayesian inference problem and express the balance that needs to be found between the confidence we have in the data and the confidence we have in the prior.Up to know, we have assumed the function f known and fixed and we have showed that, under such assumptions, we can approximate the posterior p(z|x) using variational inference technique. However, in practice this function f, that defines the decoder, is not known and also need to be chosen. To do so, let’s remind that our initial goal is to find a performant encoding-decoding scheme whose latent space is regular enough to be used for generative purpose. If the regularity is mostly ruled by the prior distribution assumed over the latent space, the performance of the overall encoding-decoding scheme highly depends on the choice of the function f. Indeed, as p(z|x) can be approximate (by variational inference) from p(z) and p(x|z) and as p(z) is a simple standard Gaussian, the only two levers we have at our disposal in our model to make optimisations are the parameter c (that defines the variance of the likelihood) and the function f (that defines the mean of the likelihood).So, let’s consider that, as we discussed earlier, we can get for any function f in F (each defining a different probabilistic decoder p(x|z)) the best approximation of p(z|x), denoted q*_x(z). Despite its probabilistic nature, we are looking for an encoding-decoding scheme as efficient as possible and, then, we want to choose the function f that maximises the expected log-likelihood of x given z when z is sampled from q*_x(z). In other words, for a given input x, we want to maximise the probability to have x̂ = x when we sample z from the distribution q*_x(z) and then sample x̂ from the distribution p(x|z). Thus, we are looking for the optimal f* such thatwhere q*_x(z) depends on the function f and is obtained as described before. Gathering all the pieces together, we are looking for optimal f*, g* and h* such thatWe can identify in this objective function the elements introduced in the intuitive description of VAEs given in the previous section: the reconstruction error between x and f(z) and the regularisation term given by the KL divergence between q_x(z) and p(z) (which is a standard Gaussian). We can also notice the constant c that rules the balance between the two previous terms. The higher c is the more we assume a high variance around f(z) for the probabilistic decoder in our model and, so, the more we favour the regularisation term over the reconstruction term (and the opposite stands if c is low).Up to know, we have set a probabilistic model that depends on three functions, f, g and h, and express, using variational inference, the optimisation problem to solve in order to get f*, g* and h* that give the optimal encoding-decoding scheme with this model. As we can’t easily optimise over the entire space of functions, we constrain the optimisation domain and decide to express f, g and h as neural networks. Thus, F, G and H correspond respectively to the families of functions defined by the networks architectures and the optimisation is done over the parameters of these networks.In practice, g and h are not defined by two completely independent networks but share a part of their architecture and their weights so that we haveAs it defines the covariance matrix of q_x(z), h(x) is supposed to be a square matrix. However, in order to simplify the computation and reduce the number of parameters, we make the additional assumption that our approximation of p(z|x), q_x(z), is a multidimensional Gaussian distribution with diagonal covariance matrix (variables independence assumption). With this assumption, h(x) is simply the vector of the diagonal elements of the covariance matrix and has then the same size as g(x). However, we reduce this way the family of distributions we consider for variational inference and, so, the approximation of p(z|x) obtained can be less accurate.Contrarily to the encoder part that models p(z|x) and for which we considered a Gaussian with both mean and covariance that are functions of x (g and h), our model assumes for p(x|z) a Gaussian with fixed covariance. The function f of the variable z defining the mean of that Gaussian is modelled by a neural network and can be represented as followsThe overall architecture is then obtained by concatenating the encoder and the decoder parts. However we still need to be very careful about the way we sample from the distribution returned by the encoder during the training. The sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called reparametrisation trick, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture and consists in using the fact that if z is a random variable following a Gaussian distribution with mean g(x) and with covariance H(x)=h(x).h^t(x) then it can be expressed asFinally, the objective function of the variational autoencoder architecture obtained this way is given by the last equation of the previous subsection in which the theoretical expectancy is replaced by a more or less accurate Monte-Carlo approximation that consists, most of the time, into a single draw. So, considering this approximation and denoting C = 1/(2c), we recover the loss function derived intuitively in the previous section, composed of a reconstruction term, a regularisation term and a constant to define the relative weights of these two terms.The main takeways of this article are:To conclude, we can outline that, during the last years, GANs have benefited from much more scientific contributions than VAEs. Among other reasons, the higher interest that has been shown by the community for GANs can be partly explained by the higher degree of complexity in VAEs theoretical basis (probabilistic model and variational inference) compared to the simplicity of the adversarial training concept that rules GANs. With this post we hope that we managed to share valuable intuitions as well as strong theoretical foundations to make VAEs more accessible to newcomers, as we did for GANs earlier this year. However, now that we have discussed in depth both of them, one question remains… are you more GANs or VAEs?Thanks for reading!Other articles written with towardsdatascience.comtowardsdatascience.com",24/09/2019,0,35,1,"(667, 233)",30,3,0.0,14,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
2,Using Mixed-Effects Models For Linear Regression,Towards Data Science,Guido Vivaldi,256.0,6.0,889,"Mixed-effects regression models are a powerful tool for linear regression models when your data contains global and group-level trends. This article walks through an example using fictitious data relating exercise to mood to introduce this concept. R has had an undeserved rough time in the news lately, so this post will use R as a small condolence to the language, though a robust framework exist in Python as well.Mixed-effect models are common in political polling analysis where national-level characteristics are assumed to occur at a state-level while state-level sample sizes may be too small to drive those characteristics on their own. They are also common in scientific experiments where a given effect is assumed to be present among all study individuals which needs to be teased out from a specific effect on a treatment group. In a similar vein, this framework can be helpful in pre/post studies of interventions.Data AnalysisThis data simulates a survey of residents of 4 states who were asked about their daily exercise habits and their overall mood on a scale from 1–10. We’ll assume for purposes of example that mood scores are linear, but in the real world we may want to treat this as an ordinal variable. What we will see (because I made up the data) is that exercise improves mood, however there are strong state-level effects.First Try: Fixed-Effect Linear RegressionThere are clear positive correlations between exercise and mood, though the model fit is not great: exercise is a significant predictor, though adjusted r-squared is fairly low. By the way, I love using R for quick regression questions: a clear, comprehensive output is often easy to find.We may conclude at this point that the data is noisy, but let’s dig a little deeper. Recall that one of the assumptions of linear regression is “homoscedasticity”, i.e. that variance is constant among for all independent variables. When heteroscedasticity is present (and homoscedasticity is violated), the regression may give too much weight to the subset of data where the error variance is the largest. You can validate this assumption by looking at a residuals plot.For reference, below is a linear regression that does not violate homoscedasticity.And then there is our data:This plot shows that a simple linear regression is not appropriate — the model consistently produces negative residuals for low mood scores, and positive residuals for high mood scores. We might suspect at this point that mood and state are correlated in a way or model is not incorporating, which is a good guess => variance in residuals differs by state.Second Try: A More Robust Linear RegressionBut wait — if state is a predictor, let’s include it in our regression and fix everything. We’ll see that this is mostly correct.R-squared improves significantly, but now the plotted line looks awfully goofy — we consistently undershoot, and the coefficient estimate for Exercise is near zero (and has a non-significant p-value). This is an example of the effect of heteroskedasticity — the groups (i.e. states) with larger variance override groups with smaller variance.We’re getting somewhere. What if we did a separate linear regression for each state? That way we’re not going to have problems with group interactions skewing our coefficients, right?Well, we have an opposite problem now — notice that in state C exercise is now decreasing mood. and the slope coefficient in other states is much lower than the 0.42951 that we saw in the Mood ~ Exercise regression. So now there is information in the high-level model that we’re neglecting because of our focus on the state-level models.Mixed-Effect ModelsThe final example above leads right into a mixed-effect model. In this model, we can allow the state-level regressions to incorporate some of the information from the overall regression, but also retain some state-level components. We can use the lme4 library to do this.The notation is similar to an lm regression with a key difference: the (1 + Exercise | State) notation allows the model to use a term with different slopes and intercepts for Mood ~ Exercise for each State value. See the coefficient values below.We have run 3 models now:We can calculate the RMSE of each model.RMSE improved significantly moving from models 2 to 3 — this suggests that the majority of the difference between states and mood is due to average mood in each state. Moving from model 2 to 3 captured this state-level intercept information, but also calculated a slope coefficient for Mood ~ Exercise for each state which incorporated information from the total dataset and from the state-level information (recall that using only state-level slope information produced a negative slope in State C).A few final notes on Mixed-Effect Models. There are multiple approaches and ongoing research into how to determine p-values for mixed-effect models. One can use an anova likelihood test to determine if an added variable is significant with respect to a model without that added variable.ConclusionMixed-Effect models provide a framework for smoothing global and group level characteristics in your data.I learned about these models primarily from Richard McElreath and his wonderful text Statistical Rethinking. I’d recommend it highly to any reader: it is a great help in rethinking many of the statistical assumptions that were made for me in entry-level classes that I never knew to reconsider.OJ Watson also has a well-done Kaggle post that presents a python-based framework for mixed-effect models.",18/05/2019,9,6,1,"(516, 304)",13,1,0.0,5,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
3,The New Moats,Greylock Perspectives,Jerry Chen,4700.0,11.0,2217,"To build a sustainable and profitable business, you need strong defensive moats around your company. This rings especially true today as we undergo one of the largest platform shifts in a generation as applications move to the cloud, are consumed on iPhones, Echoes, and Teslas, are built on open source, and are fueled by AI and data. These dramatic shifts are rendering some existing moats useless and leaving CEOs feeling like it’s almost impossible to build a defensible business.In this post, I’ll review some of the traditional economic moats that technology companies typically leverage and how they are being disrupted. I believe that startups today need to build systems of intelligence™ — AI powered applications — “the new moats.”Businesses can build several different moats and over time these moats can change. The following list is definitely not exhaustive and fair warning, it will read like a bad b-school blog!Some of the greatest and most enduring technology companies are defended by powerful moats. For example, Microsoft, Google, and Facebook all have moats built on economies of scale and network effects.One of the most successful cloud businesses, Amazon Web Services (AWS), has both the advantages of scale but also the power of network effects. More apps and services are built natively on AWS because “that’s where the customers and the data are.” In turn, the ecosystem of solutions attracts more customers and developers who build more apps that generate more data continuing the virtuous cycle while driving down Amazon’s cost through the advantages of scale.Strong moats help companies survive through major platform shifts, but surviving should not be confused with thriving. For example, high switching costs can partly account for why mainframes and “big iron” systems are still around after all these years. Legacy businesses with deep moats may not be the high growth vehicles of their prime, but they are still generating profits. Companies need to recognize and react when they are in the midst of an industry wide transformation, lest they become victims of their own success.Moreover, these massive platforms shifts — like cloud and mobile — are technology tidal waves that create openings for new players and enable founders to build paths over and around existing moats. Startup founders who succeed tend to execute a dual-pronged strategy: 1) Attack legacy player moats and 2) simultaneously build their own defensible moats that ride the new wave. For example, Facebook had the most entrenched social network, but Instagram built a mobile-first photo app that rode the smartphone wave to a $1B acquisition. In the enterprise world, SaaS companies like Salesforce are disrupting on-premise software companies like Oracle. Now with the advent of cloud, AWS, Azure, and Google Cloud are creating a direct channel to the customer. These platform shifts can also change the buyer and end user. Within the enterprise, the buyer has moved from a central IT team to an office knowledge worker, to someone with an iPhone, to any developer with a GitHub account.In this current wave of disruption, is it still possible to build sustainable moats? For founders, it may feel like every advantage you build can be replicated by another team down the street, or at the very least, it feels like moats can only be built at massive scale. Open source tools and cloud have pushed power to the “new incumbents,’ — the current generation of companies that are at massive scale, have strong distribution networks, high switching cost, and strong brands working for them. These are companies like Apple, Facebook, Google, Amazon, and Salesforce.Why does it feel like there are “no more moats” to build? In an era of cloud and open source, deep technology attacking hard problems is becoming a shallower moat. The use of open source is making it harder to monetize technology advances while the use of cloud to deliver technology is moving defensibility to different parts of the product. Companies that focus too much on technology without putting it in context of a customer problem will be caught between a rock and a hard place — or as I like to say, “between open source and a cloud place.” For example, incumbent technologies like Oracle’s proprietary database are being attacked from open source alternatives like Hadoop and MongoDB and in the cloud by Amazon Aurora and innovations like Google Spanner. On the other hand, companies that build great customer experiences may find defensibility through the workflow of their software.I believe that deep technology moats aren’t completely gone and defensible business models can still be built around IP. If you pick a place in the technology stack and become the absolute best of breed solution you can create a valuable company. However, this means picking a technical problem with few substitutes, that requires hard engineering, and needs operational knowledge to scale.Today the market is favoring “full stack” companies, SaaS offerings that offer application logic, middleware, and databases combined. Technology is becoming an invisible component of a complete solution (e.g. “No one cares what database backs your favorite mobile app as long as your food is delivered on time!”). In the consumer world, Apple made the integrated or full stack experience popular with the iPhone which seamlessly integrated hardware with software. This integrated experience is coming to dominate enterprise software as well. Cloud and SaaS has made it possible to reach customers directly and in a cost-effective manner. As a result, customers are increasingly buying full stack technology in the form of SaaS applications instead of buying individual pieces of the tech stack and building their own apps. The emphasis on the whole application experience or the “top of the technology stack” is why I also evaluate companies through an additional framework, the stack of enterprise systems.At the bottom of the stack of systems, is usually a database on top of which an application is built. If the data and app power a critical business function, it becomes a “system of record.” There are three major systems of record in an enterprise: your customers, your employees, and your assets. CRM owns your customers, HCM, owns your employees, and ERP/Financials owns your assets. Generations of companies have been built around owning a system of record and every wave produced a new winner. In CRM we saw Salesforce replace Siebel as the system of record for customer data, and Workday replace Oracle PeopleSoft for employee data. Workday has also expanded into financial data. Other applications can be built around a system of record but are usually not as valuable as the actual system of record. For example, marketing automation companies like Marketo and Responsys built big businesses around CRM, but never became as strategic or as valuable as Salesforce.Systems of engagement™ are the interfaces between users and the systems of record and can be powerful businesses because they control the end user interactions. In the mainframe era, the systems of record and engagement were tied together when the mainframe and terminal were essentially the same product. The client/server wave ushered in a class of companies that tried to own your desktop, only to be disrupted by a generation of browser based companies, only to be succeeded by mobile first companies. The current generation of companies vying to own the system of engagement include Slack, Amazon Alexa, and every other speech / text/ conversational UI startup. In China, WeChat has become a dominant system of engagement and is now a platform for everything from e-commerce to games. If it sounds like systems of engagement™ turn over more than systems of record, it’s probably because they do. The successive generations of systems of engagement™ don’t necessarily disappear but instead users keep adding new ways to interact with their applications. In a multi-channel world, owning the system of engagement is most valuable if you control most of the end user engagement or are a cross channel system that reaches users wherever they are. Perhaps the most strategic advantage of being a system of engagement is that you can coexist with several systems of record and collect all the data that passes through your product. Over time you can evolve your engagement position into an actual system of record using all the data you have accumulated.I believe that systems of intelligence™ are the new moats. What is a system of intelligence and why is it so defensible? What makes a system of intelligence valuable is that it typically crosses multiple data sets, multiple systems of record. One example is an application that combines web analytics with customer data and social data to predict end user behavior, churn, LTV, or just serve more timely content. You can build intelligence on a single data source or single system of record but that position becomes harder to defend against the vendor that owns the data. For a startup to thrive around incumbents like Oracle and SAP, you need to combine their data with other data sources (public or private) to create value for your customer. Incumbents will be advantaged on their own data. For example, Salesforce is building a system of intelligence, Einstein, starting with their own system of record, CRM. The next generation of enterprise products will use different artificial intelligence (AI) techniques to build systems of intelligence™. It’s not just applications that will be transformed by AI but also data center and infrastructure products. We can categorize three major areas where you can build systems of intelligence™: customer facing applications around the customer journey, employee facing applications like HCM, ITSM, Financials, or infrastructure systems like security, compute/ storage/ networking, and monitoring/ management. In addition to these broad horizontal use cases, startups can also focus on a single industry or market and build a system of intelligence around data that is unique to a vertical like Veeva in life sciences, or Rhumbix in construction.In all of these markets, the battle is moving from the old moats, the sources of the data, to the new moats, what you do with the data. Using a company’s data, you can upsell customers, automatically respond to support tickets, prevent employee attrition, and identify security anomalies. Products that use data specific to an industry (i.e. healthcare, financial services), or unique to a company (customer data, machine logs, etc.) to solve a strategic problem begin to look like a pretty deep moat, especially if you can replace or automate an entire enterprise workflow or create a new value-added workflow that was made possible by this intelligence.Enterprise applications that built systems of record have always been powerful businesses models. Some of the most enduring app companies like Salesforce and SAP are all built on deep IP, benefit from economies of scale, and over time they accumulate more data and operating knowledge as they get deeper within a company’s workflow and business processes. However, even these incumbents are not immune to platform shifts as a new generation of companies attack their domains. To be fair, we may be at risk of AI marketing fatigue, but all the hype reflects AI’s potential to change so many industries. One popular AI approach, machine learning (ML), can be combined with data, a business process, and an enterprise workflow to create the context to build a system of intelligence. Google was an early pioneer of applying ML to a process and workflow: they collected more data on every user and applied machine learning to serve up more timely ads within the workflow of a web search. There are other evolving AI techniques like neural networks that will continue to change what we can expect from these future applications. These AI-driven systems of intelligence™ present a huge opportunity for new startups. Successful companies here can build a virtuous cycle of data because the more data you generate and train on with your product, the better your models become and the better your product becomes. Ultimately the product becomes tailored for each customer which creates another moat, high switching costs. It is also possible to build a company that combines systems of engagement™ with intelligence or even all three layers of the enterprise stack but a system of intelligence or engagement can be the best insertion point for a startup against an incumbent. Building a system of engagement or intelligence is not a trivial task and will require deep technology, especially at speed and scale. In particular, technologies that can facilitate an intelligence layer across multiple data sources will be essential. Finally, there are some businesses that can build data network effects by using customer and market data to train and improve models that make the product better for all customers, which spins the flywheel of intelligence faster.  In summary, you can build a defensible business model as a system of engagement, intelligence, or record, but with the advent of AI, intelligent applications will be the fountain of the next generation of great software companies because they will be the new moats.Thanks to Saam Motamedi, Sarah Guo, Eli Collins, Peter Bailis, Elisa Schreiber, Michael Inouye, my Greylock partner Sarah Tavel, and the rest of my partners at Greylock for their input. This post was also helped through conversations with my friends at several Greylock-backed companies including Trifacta, Cloudera, and dozens of founders and CEOs that have influenced my thinking. All good ideas are shamelessly stolen and all bad ideas are mine alone.",24/04/2017,0,5,27,"(700, 405)",6,2,0.0,24,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
4,Region of Interest Pooling,Towards Data Science,Sambasivarao. K,239.0,4.0,523,"The major hurdle for going from image classification to object detection is fixed size input requirement to the network because of existing fully connected layers. In object detection, each proposal will be of a different shape. So there is a need for converting all the proposals to fixed shape as required by fully connected layers. ROI Pooling is exactly doing this.Region of Interest (ROI) pooling is used for utilising single feature map for all the proposals generated by RPN in a single pass. ROI pooling solves the problem of fixed image size requirement for object detection network.ROI pooling produces the fixed-size feature maps from non-uniform inputs by doing max-pooling on the inputs. The number of output channels is equal to the number of input channels for this layer. ROI pooling layer takes two inputs:ROI pooling takes every ROI from the input and takes a section of input feature map which corresponds to that ROI and converts that feature-map section into a fixed dimension map. The output fixed dimension of the ROI pooling for every ROI neither depends on the input feature map nor on the proposal sizes, It solely depends on the layer parameters.Pooled_width and pooled_height are hyperparameters which can be decided based on the problem at hand. These indicate the number of grids the feature map corresponding to the proposal should be divided into. This will be the output dimension of this layer. Let us assume that W, H are the width and height of the proposal and P_w,P_h are pooled width and height. Then the ROI will be divided into P_w*P_h blocks, each of dimensions (W/P_w, H/P_h).Spatial scale is a scaling parameter for resizing the proposal according to the feature map dimensions. Let's say in our network, the image size is 1056x640 and due to many convolution and pooling operations, the feature map size reduced to 66x40, which is being used by ROI pooling. Now the proposals are generated based on input image size, so we need to rescale the proposals to feature map size. In this case, we can divide all dimensions of proposal by 16 (1056/66=16 or 640/40=16). So the spatial scale will be 1/16 in our example.Now we got a clear understanding of each parameter, let us see how ROI pooling works. For every proposal in the input proposals, we take the corresponding feature map section and divide that section into W*H blocks defined by layer parameters. After that take the maximum element of each block and copy to the output. So the output size will be P_w*P_h for every ROI proposal and N*P_w*P_h for all N proposals which is a fixed dimension feature map irrespective of the various sizes of the input proposals.Below diagram illustrates the forward pass of ROI pooling layer.The main advantage of ROI pooling is that we can use the same feature map for all the proposals which enables us to pass the entire image to the CNN instead of passing all proposals individually.Hope this helps! Thanks, everyone!References:Subscribe to FOCUS — my weekly newsletter to get the latest updates and recent advances in AI along with curated stories from the medium on Machine Learning.",22/04/2019,4,5,0,"(700, 473)",2,2,0.0,3,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
5,Modern Gaussian Process Regression,Towards Data Science,Ryan Sander,173.0,10.0,1433,"Ever wonder how you can create non-parametric supervised learning models with unlimited expressive power? Look no further than Gaussian Process Regression (GPR), an algorithm that learns to make predictions almost entirely from the data itself (with a little help from hyperparameters). Combining this algorithm with recent advances in computing, such as automatic differentiation, allows for applying GPRs to solve a variety of supervised machine learning problems in near-real-time.In this article, we’ll discuss:This is the second article in my GPR series. For a rigorous, Ab initio introduction to Gaussian Process Regression, please check out my previous article here.Before we dive into how we can implement and use GPR, let’s quickly review the mechanics and theory behind this supervised machine learning algorithm. For more detailed derivations/discussion of the following concepts, please check out my previous article on GPR here. GPR:i. Predicts the conditional posterior distribution of test points conditioned on observed training points:ii. Computes the mean of predicted test point targets as linear combinations of observed target values, with the weights of these linear combinations determined by the kernel distance from the training inputs to the test points:iii. Uses covariance functions to measure the kernel distance between inputs:iv. Interpolates novel points from existing points by treating each novel point as part of a Gaussian Process, i.e. parameterizing the novel point as a Gaussian distribution:GPR can be applied to a variety of supervised machine learning problems (and in some cases, can be used as a subroutine in unsupervised machine learning). Here are just a few classes of problems that can be solved with this machine learning technique:Interpolation is a key task in a variety of fields, such as signal processing, spatial statistics, and control. This application is particularly common in fields that leverage spatial statistics, such as geostatistics. As a concrete example, consider the problem of generating a surface corresponding to the mountain below, given only a limited number of defined points on the mountain. If you’re interested in seeing a specific implementation of this, please check out my article here.This class of problems looks at projecting a time series into the future using historical data. Like kriging, time series forecasting allows for predicting unseen values. Rather than predicting unseen values at different locations, however, this problem applies GPR for predicting the mean and variance of unseen points in the future. This is highly applicable for tasks such as predicting electricity demand, stock prices, or the state-space evolution of a linear dynamical system.Furthermore, not only does GPR predict the mean of a future point, but it also outputs a predicted variance, enabling decision-making systems to factor uncertainty into their decisions.More generally, because GPR allows for predicting variance at test points, GPR can be used for a variety of uncertainty quantification tasks — i.e. any task for which it is relevant to estimate both an expected value, and the uncertainty, or variance, associated with this expected value.You may be wondering: Why is uncertainty important? To motivate this answer, consider predicting the trajectory of a pedestrian for an autonomous navigation safety system. If the predicted trajectory of a pedestrian has high predicted uncertainty, an autonomous vehicle should exercise increased caution to account for having low confidence in the pedestrian’s intention. If, on the other hand, the autonomous vehicle has low predicted variance of the pedestrian’s trajectory, then the autonomous car will be better able to predict the pedestrian’s intentions, and can more easily proceed along with its current driving plan.In a sense, by predicting uncertainty, decision-making systems can “weight” the expected values they estimate according to how uncertain they predict these expected values to be.You may be wondering — why should I consider using GPR instead of a different supervised learning model? Below, I enumerate a few comparative reasons.Below, we introduce several Python machine learning packages for scalable, efficient, and modular implementations of Gaussian Process Regression. Let’s walk through each of them!This is a great package for getting started with GPR. It allows for some model flexibility, and is able to carry out hyperparameter optimization and defining likelihoods under the hood. To use sklearn with your datasets, please make sure your datasets can be represented numerically with np.array objects. The main steps for using GPR with sklearn:To install dependencies for the example below using pip:Here is an example that fits and predicts a one-dimensional sinusoid using sklearn:This package is great for creating fully customizable, advanced, and accelerated GPR models that scale. This package supports everything from GPR model optimization via auto-differentiation to hardware acceleration via CUDA and PyKeOps.It’s recommended you have some familiarity with PyTorch and/or auto-differentiation packages in python before working with GPyTorch, but the tutorials make this framework easy to learn and use. Data for GPRs in GPyTorch are represented as torch.tensor objects. Here are the steps for fitting a GPR model in GPyTorch:To install dependencies for the example below using pip:Here is an example to fit a noisy one-dimensional sinusoid using gpytorch:Another GPR package that supports automatic differentiation (this time in tensorflow), GPFlow has extensive functionality built-in for creating fully-customizable models, likelihood functions, kernels, and optimization and inference routines. In addition to GPR, GPFlow has built-in functionality for a variety of other state-of-the-art problems in Bayesian Optimization, such as Variational Fourier Features and Convolutional Gaussian Processes.It’s recommended you have some familiarity with TensorFlow and/or auto-differentiation packages in Python before working with GPFlow. Data for GPRs in GPFlow are represented as tf.tensor objects. To get started with GPFlow, please check out this examples link.This package has Python implementations for a multitude of GPR models, likelihood functions, and inference procedures. Though this package doesn’t have the same auto-differentiation backends that power gpytorch and gpflow, this package’s versatility, modularity, and customizability make it a valuable resource for implementing GPR.Pyro is a probabilistic programming package that can be integrated with Python that also supports Gaussian Process Regression, as well as advanced applications such as Deep Kernel Learning.Gen is another probabilistic programming package built on top of Julia. Gen offers several advantages with Gaussian Process Regression: (i) It builds in proposal distributions, which can help to narrow down a search space by effectively imposing a prior on the set of possible solutions, (ii) It has an easy API for sampling traces from fit GPR models, (iii) As is the goal for many probabilistic programming languages, it enables for easily creating hierarchical models for tuning the priors of GPR hyperparameters.Stan is another probabilistic programming package that can be integrated with Python, but also supports other languages such as R, MATLAB, Julia, and Stata. In addition to having functionality built-in for Gaussian Process Regression, Stan also supports a variety of other Bayesian inference and sampling functionality.Built by the creators of GPyTorch, BoTorch is a Bayesian Optimization library that supports many of the same GPR techniques, as well as advanced Bayesian Optimization techniques and analytic test suites, as GPyTorch.In this article, we reviewed the theory behind Gaussian Process Regression (GPR), introduced and discussed the types of problems GPR can be used to solve, discussed how GPR compares to other supervised learning algorithms, and walked through how we can implement GPR using sklearn, gpytorch, or gpflow.To see more articles in reinforcement learning, machine learning, computer vision, robotics, and teaching, please follow me! Thank you for reading!Thank you to CODECOGS for their inline equation rendering tool, Carl Edward Rasmussen for open-sourcing the textbook Gaussian Processes for Machine Learning [5], and for Scikit-Learn, GPyTorch, GPFlow, and GPy for open-sourcing their Gaussian Process Regression Python libraries.[1] Pedregosa, Fabian, et al. “Scikit-learn: Machine learning in Python.” the Journal of machine Learning research 12 (2011): 2825–2830.[3] Gardner, Jacob R., et al. “Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration.” arXiv preprint arXiv:1809.11165 (2018).[3] Matthews, Alexander G. de G., et al. “GPflow: A Gaussian Process Library using TensorFlow.” J. Mach. Learn. Res. 18.40 (2017): 1–6.[4] GPy, “GPy.” http://github.com/SheffieldML/GPy.[5] Carl Edward Rasmussen and Christopher K. I. Williams. 2005. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press.[6] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. 2019. Pyro: deep universal probabilistic programming. J. Mach. Learn. Res. 20, 1 (January 2019), 973–978.[7] Gen: A General-Purpose Probabilistic Programming System with Programmable Inference. Cusumano-Towner, M. F.; Saad, F. A.; Lew, A.; and Mansinghka, V. K. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ‘19).[8] Stan Development Team. 2021. Stan Modeling Language Users Guide and Reference Manual, VERSION. https://mc-stan.org.[9] Balandat, Maximilian, et al. “BoTorch: A framework for efficient Monte-Carlo Bayesian optimization.” Advances in Neural Information Processing Systems 33 (2020).",24/03/2021,2,96,15,"(486, 175)",14,4,0.0,35,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
6,Understanding Latent Space in Machine Learning,Towards Data Science,Ekin Tiu,646.0,9.0,1634,"If I have to describe latent space in one sentence, it simply means a representation of compressed data.Imagine a large dataset of handwritten digits (0–9) like the one shown above. Handwritten images of the same number (i.e. images that are 3’s) are the most similar to each other compared to other images of different numbers (i.e. 3s vs. 7s). But can we train an algorithm to recognize these similarities? How?If you have trained a model to classify digits, then you have also trained the model to learn the ‘structural similarities’ between images. In fact, this is how the model is able to classify digits in the first place- by learning the features of each digit.If it seems that this process is ‘hidden’ from you, it’s because it is. Latent, by definition, means “hidden.”The concept of “latent space” is important because it’s utility is at the core of ‘deep learning’ — learning the features of data and simplifying data representations for the purpose of finding patterns.Intrigued? Let’s break latent space down bit by bit:Why do we compress data in ML?Data compression is defined as the process of encoding information using fewer bits than the original representation. This is like taking a 19D data point (need 19 values to define unique point) and squishing all that information into a 9D data point.More often than not, data is compressed in machine learning to learn important information about data points. Let me explain with an example.Say we would like to train a model to classify an image using a fully convolutional neural network (FCN). (i.e. output digit number given image of digit). As the model ‘learns’, it is simply learning features at each layer (edges, angles, etc.) and attributing a combination of features to a specific output.But each time the model learns through a data point, the dimensionality of the image is first reduced before it is ultimately increased. (see Encoder and Bottleneck below). When the dimensionality is reduced, we consider this a form of lossy compression.Because the model is required to then reconstruct the compressed data (see Decoder), it must learn to store all relevant information and disregard the noise. This is the value of compression- it allows us to get rid of any extraneous information, and only focus on the most important features.This ‘compressed state’ is the Latent Space Representation of our data.What do I mean by space?You may be wondering why we call it a latent space. After all, compressed data, at first glance, may not evoke any sort of “space.”But here’s the parallel.In this rather simplistic example, let’s say our original dataset are images with dimensions 5 x 5 x 1. We will set our latent space dimensions to be 3 x 1, meaning our compressed data point is a vector with 3-dimensions.Now, each compressed data point is uniquely defined by only 3 numbers. That means we can graph this data on a 3D Plane (One number is x, the other y, the other z).This is the “space” that we are referring to.Whenever we graph points or think of points in latent space, we can imagine them as coordinates in space in which points that are “similar” are closer together on the graph.A natural question that arises is how would we imagine space of 4D points or n-dimensional points, or even non-vectors (since the latent space representation is NOT required to be 2 or 3-dimensional vectors, and is oftentimes not since too much information would be lost).The unsatisfying answer is, we can’t. We are 3-dimensional creatures that cannot fathom n-dimensional space (such that n > 3). However, there are tools such as t-SNE which can transform our higher dimensional latent space representations into representations that we can visualize (2D or 3D). (See Visualizing Latent Space section below.)But you may be wondering, what are ‘similar’ images, and why does reducing the dimensionality of our data make similar images ‘closer’ together in space?What do I mean by similar?If we look at three images, two of a chair and one of a desk, we would easily say that the two chair images are the most similar whereas the desk is the most different from either of the chair images.But what makes these two chair images “more similar?” A chair has distinguishable features (i.e. back-rest, no drawer, connections between legs). These can all be ‘understood’ by our models by learning patterns in edges, angles, etc.As explained, such features are packaged in the latent space representation of data.Thus, as dimensionality is reduced, the ‘extraneous’ information which is distinct to each image (i.e. chair color) is ‘removed’ from our latent space representation, since only the most important features of each image are stored in the latent space representations.As a result, as we reduce dimensionality, the representations of both chairs become less distinct and more similar. If we were to imagine them in space, they would be ‘closer’ together.*Please note that the ‘closeness’ metric I have referred to throughout the article is an ambiguous term and not a definitive Euclidian distance, because there are multiple definitions of distance in space.The latent space concept is definitely intriguing. But how is it used? When do we use it? And most importantly, why?What we’ll find is that the latent space is ‘hidden’ in many of our favorite image processing networks, generative models, etc.Although the latent space is hidden from most, there are certain tasks in which understanding the latent space is not only helpful, but necessary.The latent space representation of our data contains all the important information needed to represent our original data point.This representation must then represent the features of the original data.In other words, the model learns the data features and simplifies its representation to make it easier to analyze.This is at the core of a concept called Representation Learning, defined as a set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data.In this use case, our latent space representations are used to transform more complex forms of raw data (i.e. images, video), into simpler representations which are ‘more convenient to process’ and analyze.Listed below are specific instances of representation learning.The latent space is an essential concept in manifold learning, a subfield of representation learning.Manifolds in data science can be understood as groups or subsets of data that are ‘similar’ in some way.These similarities, usually imperceptible or obscured in higher-dimensional space, can be discovered once our data has been represented in the latent space.Take the example of a ‘swiss roll’ below.In 3D, we know that there are groups of similar data points that exist, but it is much more difficult to delineate such groups with higher dimensional data.By reducing the dimensionality of our data to 2D, which in this case could be considered a ‘latent space’ representation, we are able to more easily distinguish the manifolds (groups of similar data) in our dataset.To learn more about manifolds and manifold learning, I recommend the following articles:towardsdatascience.comscikit-learn.orgA common type of deep learning model that manipulates the ‘closeness’ of data in the latent space is the autoencoder — a neural network that acts as an identity function. In other words, an autoencoder learns to output whatever is inputted.Now, if you’re new to the field, you may be wondering, why in the world would we need a model that does this? It seems rather useless if all it outputs is itself…Though this reasoning is valid, we don’t care so much about what the model outputs. We care more about what the model learns in the process.When we force a model to become an identity function, we are forcing it to store all of the data’s relevant features in a compressed representation so that there is enough information in that compressed form such that the model can ‘accurately’ reconstruct it. Sound familiar? It should, because this compressed representation is our latent space representation (red block in image above).We have seen how patterns can be more easily discovered in the latent space since similar data points will tend to cluster together, but we have not yet seen how we can sample points from this latent space to seemingly generate ‘new’ data.In the example above, we can generate different facial structures by interpolating on latent space, and using our model decoder to reconstruct the latent space representation into a 2D image with the same dimensions as our original input.What do I mean by interpolating on latent space?Let’s say that I have compressed the chair images from the previous section into the following 2D vectors, [0.4, 0.5] and [0.45, 0.45]. Let’s say the desk is compressed to [0.6, 0.75]. If I were to interpolate on latent space, I would sample points in latent space between the ‘chair’ cluster and the ‘desk’ cluster.We can feed these sampled 2D vectors into the model’s decoder, and voila! We get ‘new’ images that look like a morph between a chair and a desk. *new is in quotes because these generated images are not technically independent of the original data sample.Below is an example of linear interpolation between two types of chairs in latent space.Image generation is still an active area of research, and the latent space is an essential concept that must be understood. See the following articles for more use cases of generative models, and a hands-on example of latent space interpolation using a GAN (Generative Adversarial Network), another generative model that uses latent space representations.machinelearningmastery.commachinelearningmastery.comFor more on latent space visualization, I recommend Hackernoon’s article which provides a hands-on example of visualizing similarities between digit images in a 2D space with the t-SNE algorithm.hackernoon.comWhile learning about the latent space, I was fascinated by this ‘hidden,’ yet essential concept. I hope that this article demystified the latent space representation, and provided the ‘deeper understanding’ of deep learning that I longed for as a novice.",04/02/2020,0,15,43,"(674, 530)",14,1,0.0,7,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,expectation/interest
7,Nhận diện tên riêng (NER) với Bidirectional Long Short-Term Memory và Conditional Random Field,Zalo Technology Blog,Zalo Tech,501.0,16.0,2587,"Nhận diện tên riêng, còn được gọi là Nhận diện thực thể có tên (Named Entity Recognition — NER), là tác vụ cơ bản trong lĩnh vực Xử lý ngôn ngữ tự nhiên. Vai trò chính của tác vụ này là nhận dạng các cụm danh từ trong văn bản và phân loại chúng vào trong các nhóm đã được định trước như tên người, tổ chức, địa điểm, thời gian và giá trị tiền tệ. Với mục tiêu như vậy, NER có thể cung cấp thông tin hữu ích cho các ứng dụng xử lý ngôn ngữ tự nhiên khác như hệ thống hỏi đáp tự động, tóm tắt tin tức, máy tìm kiếm, …Hầu hết các hệ thống NER tập trung vào việc áp dụng các phương pháp máy học và không đòi hỏi nhiều kiến thức chuyên sâu về mặt ngôn ngữ. Chúng ta có thể gom nhóm các phương pháp máy học trong NER thành 2 nhóm lớn. Các phương pháp trong nhóm đầu tiên áp dụng các kỹ thuật trích xuất đặc trưng bằng tay và kết hợp với thuật toán gán nhãn theo chuỗi như Conditional Random Field (CRF), Hidden Markov Model (HMM), hoặc Maximum Entropy Markov Model (MEMM). Các phương pháp này đã nghiên cứu kỹ lượng trong thời gian dài và đặc biệt thích hợp với các ngôn ngữ có nguồn dữ liệu ít như Tiếng Việt. Tuy nhiên, điểm yếu của nhóm phương pháp này chính là độ chính xác phụ thuộc rất lớn vào việc lựa chọn chuẩn xác các đặc trưng bằng tay.Trong vài năm trở lại đây, cùng với quá trình phát triển của Deep Learning một vài kiến trúc Neural Network đã được đề xuất để giải quyết các bài toán gán nhãn chuỗi. Các phương pháp này hình thành nên nhóm phương pháp thứ 2 cho bài toán NER. Ưu điểm vượt trội của các kiến trúc Deep Neural Network là khả năng End-to-end Learning, tức là khả năng học được các quy luật gán nhãn chuỗi từ tập dữ liệu gán nhãn trước mà không cần có bất cứ sự can thiệp của con người. Điều này đã loại bỏ đi nhược điểm của nhóm phương pháp 1 phải dựa trên kiến thức ngôn ngữ để lựa chọn các đặc trưng. Tuy nhiên, nhóm phương pháp 2 cũng tồn tại nhược điểm liên quan đến kích cỡ tập dữ liệu huấn luyện. Thông thường, các kiến trúc Deep Neural Network yêu cầu tập dữ liệu khá lớn để có thể đạt được độ chính xác cao. Đối với ngôn ngữ có ít dữ liệu có nhãn như Tiếng Việt, giải pháp hữu hiệu nhất chính là sử dụng một ma trận Word Embedding tốt đã được huấn luyện từ một tập dữ liệu không nhãn lớn (vd, tin tức online, diễn đàn, …).Trong blog này, chúng tôi sẽ trình bày kiến trúc Neural Network phù hợp nhất cho bài toán NER. Kiến trúc này dựa trên sự kết hợp giữa Bidirectional Long Short-Term Memory (Bi-LSTM) và Conditional Random Field (CRF) trong một hệ thống End-to-end Learning. Ngoài ra, chúng tôi cũng tìm cách kết hợp các tri thức đạt được từ cấp độ ký tự để tạo nên một ma trận Word Embedding phù hợp cho bài toán NER. Hệ thống của chúng tôi đã đạt được F1 tổng quát 89.20% và 74 % lần lượt trên tập dữ liệu Development và Test, xếp thứ 2 chung cuộc của cuộc thi VLSP 2018 NER Shared Task [1].Recurrent Neural Network (RNN) là một loại Neural Network trong đó mỗi node trong các lớp ẩn có một kết nối với chính bản thân. Chính kết nối này tạo ra các trạng thái nội tại của kiến trúc mạng cho phép mô hình hoá các chuỗi với độ dài bất kỳ.Một RNN có thể nhận vào một chuỗi có chiều dài bất kỳ và tạo ra một chuỗi nhãn có chiều dài tương ứng nên kiến trúc này rất phù hợp với bài toán NER. Như mô tả trong Hình 1 bên trên, tại mỗi thời điểm t một RNN điển hình A sẽ tạo ra một vector h_t chứa toàn bộ thông tin của các dữ liệu đầu vào từ X_0 tới X_t. Tuy nhiên, kiến trúc RNN cổ điển rất khó để áp dụng trong thực tế vì vấn đề liên quan đến việc mất mát hoặc bùng nổ giá trị được dùng để cập nhật các trọng số của mạng thông qua quá trình học khi phải mô hình hoá các chuỗi rất dài (vanishing and exploding gradient).Long Short-Term Memory (LSTM), một biến thể nổi tiếng của RNN, được đề xuất như là một giải pháp cho vấn đề vừa được nêu trên. Điểm chính trong kiến trúc mạng của LSTM chính là các memory cell với các cổng cho phép lưu trữ hoặc truy xuất thông tin. Các cổng này cho phép ghi đè (input gate), loại bỏ dư thừa (forget gate) và truy xuất (output gate) các thông tin được lưu trữ bên trong các memory cell.Chi tiết về Long Short-Term Memory và cách thức hoạt động, bạn đọc có thể ghé thăm blog “Understanding LSTMs” của tác giả Colah tại link sau: http://colah.github.io/posts/2015-08-Understanding-LSTMs/Việc nhận dạng chính xác tên riêng trong một đoạn văn bản phụ thuộc không chỉ vào các thông tin phía trước của từ đang xét mà còn cả các thông tin phía sau. Tuy nhiên, một kiến trúc LSTM truyền thống với một lớp duy nhất chỉ có thể dự đoán nhãn của từ hiện tại dựa trên thông tin có được từ các từ nằm trước đó. Bidirectional LSTM (BiLSTM) đã được tạo ra để khắc phục điểm yếu trên. Một kiến trúc BiLSTM thường chứa 2 mạng LSTM đơn được sử dụng đồng thời và độc lập để mô hình hoá chuỗi đầu vào theo 2 hướng: từ trái sang phải (forward LSTM) và từ phải sang trái (backward LSTM) — Hình 3.Conditional Random Field (CRF) là một mô hình xác suất cho các bài toán dự đoán có cấu trúc và đã được áp dụng rất thành công trong rất nhiều lĩnh vực như thị giác máy tính, xử lý ngôn ngữ tự nhiên, sinh-tin học, … Trong mô hình CRF, các node chứa dữ liệu đầu vào và các node chứa dữ liệu đầu ra được kết nối trực tiếp với nhau, đối nghịch với kiến trúc của LSTM hoặc BiLSTM trong đó các đầu vào và đầu ra được kết nối gián tiếp qua các memory cell. CRF có thể được sử dụng để gán nhãn tên riêng với đầu vào là các đặc trưng của một từ được rút trích bằng tay như:Chi tiết về CRF, bạn đọc có thể tìm hiểu thêm từ link sau: Overview of CRF của ml2vec.Kiến trúc kết hợp BiLSTM và CRF được mô tả như Hình 4. Kiến trúc BiLSTM+CRF hoạt động theo các bước:Các tham số học của kiến trúc BiLSTM+CRF bao gồm: ma trận word embedding, ma trận trọng số của lớp BiLSTM, ma trận chuyển vị của lớp CRF. Toàn bộ các tham số này được cập nhật trong quá trình huấn luyện trên tập dữ liệu có nhãn thông qua thuật toán Back-propagation với Stochastic Gradient Descent (SGD).Trong quá trình huấn luyện, chúng tôi áp dụng kỹ thuật Dropout để tránh việc bị over-fitting, tức là kiến trúc mất đi tính tổng quát khi nhận diện tên riêng trên các đoạn văn bản mới ngoài tập huấn luyện. Dropout là kỹ thuật xét giá trị 0 cho ngẫu nhiên r phần trăm các node trong mỗi lớp của kiến trúc. Điều này tạo ra một lượng nhiễu (noise) ngẫu nhiên nhỏ trong quá trình học và làm cho kiến trúc trở nên linh động hơn khi xử lý các câu trong văn bản.Cách kết hợp 2 mô hình BiLSTM và CRF này tận dụng được điểm mạnh của cả 2: khả năng rút trích đặc trưng thông minh của LSTM và khả năng dự đoán chuỗi nhãn mạnh mẽ của CRF. Điều này đưa đến việc kiến trúc BiLSTM+CRF đạt độ chính xác cao trong bài toán NER trên Tiếng Anh.Dựa trên các minh chứng về mô hình toán học và kết quả đạt được trên các ngôn ngữ khác, chúng tôi mong đợi việc áp dụng kiến trúc BiLSTM+CRF vào bài toán NER trên Tiếng Việt sẽ đạt được độ chính xác cao.Các phương pháp máy học đều luôn yêu cầu đầu vào là một tập các giá trị số được thể hiện dưới dạng vector. Tuy nhiên, với các bài toán xử lý ngôn ngữ tự nhiên, dữ liệu đầu vào luôn là các chuỗi ký tự. Để áp dụng được các mô hình máy học, chúng ta cần phải chuyển đổi các chuỗi ký tự này thành dạng số mà vẫn giữ được các yếu tố về hình thái (hoa thường, nguyên âm, hợp âm, …), ngữ nghĩa và ngữ pháp.Đối với các thuật toán máy học dự đoán chuỗi như CRF, chúng ta làm bước chuyển đổi này bằng cách rút ra các đặc trưng và hiển hiện bằng các giá trị nhị phân — 0 thể hiện chuỗi đang xét không có tính chất f_i và 1 trong trường hợp ngược lại — hoặc các giá trị số thực bằng cách tính tần số xuất hiện (Tf-idf).Với các kiến trúc Neural Network, chúng ta sử dụng ma trận Word Embedding để ánh xạ các từ thành các vector số thực. Ma trận word embedding có số cột tương đương với số từ nằm trong bộ từ điển từ vựng, số dòng là số chiều của vector đại diện cho từ. Như vậy, khi chuyển đổi một từ thành vector số thực, chúng ta chỉ cần truy cập vào ma trận word embedding và lấy ra cột tương ứng. Các giá trị trong ma trận đạt được bằng cách áp dụng các thuật toán như word2vec (CBOW, Skip-gram), GloVe hoặc FastText trên tập dữ liệu văn bản không có nhãn như tin tức online. Ngoài ra, chúng ta cũng có thể khởi tạo các giá trị này ngẫu nhiên và được cập nhật chung trong quá trình huấn luyện mô hình gán nhãn. Bạn đọc xem thêm bài blog: Word Embeding của Data Science Group, IITR.Tuy ma trận word embedding có thể chứa đựng ngữ nghĩa của một từ ở một vài cấp độ, nhưng vẫn có thể gặp phải vấn đề về dữ liệu thưa (data sparity problem). Ví dụ, các từ có tần số xuất hiện rất thấp được đại diện bởi vector có đa số thành phần có giá trị gần 0, các từ không có trong danh sách từ điển, từ sai lỗi chính tả. Chúng tôi giải quyết vấn đề này bằng cách gia tăng các thông tin cho word embedding từ cấp độ ký tự. Như đã nói ở trên, BiLSTM phù hợp với việc mô hình hoá chuỗi các từ, nên chúng tôi sẽ áp dụng một BiLSTM thứ 2 (C-BiLSTM) để mô hình hoá chuỗi các ký tự của mỗi từ để tạo nên một vector đại diện cho từ đó. Sau đó kết hợp vector này với vector từ ma trận word embedding để tạo nên một vector duy nhất đại diện cho một từ. Hình 5 mô tả quá trình tạo ra vector word embedding cho mỗi từ trong từ điển.Dữ liệuBảng 1: Thống kê dữ liệu VLSP 2018 NER Shared TaskTiền xử lý dữ liệuVì dữ liệu VLSP NER 2018 được lưu dưới dạng XML nên chúng tôi tiến hành tiền xử lý và chuyển về dạng chuẩn CoNLL — mỗi dòng là một từ cùng với nhãn của từ đó, các câu phân tách nhau bởi dòng trống. Quá trình tiền xử lý như sau:Chúng tôi sử dụng bộ nhãn BIO cho việc gán nhãn cho từng từ. Ví dụ:Trong đó, O — Outside: nhãn bên ngoài cụm tên riêng, B-xxx — Begin: nhãn bắt đầu cụm tên riêng loại xxx và I-xxx — Inside: nhãn bên trong cụm tên riêng loại xxx.Độ đo đánh giá chất lượng hệ thốngBài toán Nhận dạng tên riêng thường sử dụng độ đo Precision (P), Recall (R) và F1. Precision là tỷ lệ tên riêng được xác định chính xác trên toàn bộ tên riêng mà hệ thống nhận dạng được. Recall là tỷ lệ tên riêng mà hệ thống nhận diện được trên tổng số tên riêng có trong tập Testing. F1 được tính bằng công thức 2*P*R /(P+R).Cài đặt thực nghiệm các biến thể kiến trúc BiLSTM+CRF trong thực nghiệmBảng 2 tóm tắt lại sự khác biệt quan trọng giữa các biến thể của kiến trúc BiLSTM+CRF được dùng trong thực nghiệmBảng 2: Danh sách các hệ thống thực nghiệmKết quả thực nghiệmBảng 3 thể hiện kết quả thực nghiệm của toàn bộ các hệ thống trên cả 3 độ đo Precision, Recall và F1. Cả 4 hệ thống biến thể (Sys.1–4) đạt được độ chính xác cao hơn hệ thống Baseline ở cả 3 độ đo. Điểm F1 và Recall của hệ thống 1 tăng vượt bậc so với baseline. Điều này chỉ ra các thông tin từ cấp độ ký tự của mỗi từ là cực kỳ hữu ích cho bài toán NER. Hơn thế nữa, khi áp dụng việc tách từ ở giai đoạn tiền xử lý cũng giúp cho hệ thống 2 tạo ra kết quả tốt hơn ở cả 3 độ đo so với hệ thống 1. Khi sử dụng ma trận word embedding (Sys.3–4) đạt được từ tập dữ liệu tin tức Báo Mới cũng giúp hệ thống đạt được độ chính xác vượt trội hơn trên cả 3 độ đo.Bảng 3: Kết quả đạt được của các hệ thống trên tập DevelopmentKết quả chính thức của BTC VLSP 2018Dựa trên các kết quả thực nghiệm ở trên, chúng tôi quyết định gửi 2 hệ thống 3 và 4 đến BTC VLSP 2018 để đánh giá độ chính xác trên bộ dữ liệu Test. Bảng 4 thể hiện kết quả đạt được trên 2 cấp độ nhãn: Top-level — nhãn ngoài cùng và Nested Level — nhãn ở tất cả cấp độ.Bảng 4: Kết quả chính thức của BTC VLSP 2018Trong blog này, chúng tôi đã trình bày phương pháp áp dụng kiến trúc kết hợp Bidirectional Long Short-Term Memory và Conditional Random Field để giải quyết bài toán Nhận dạng tên riêng trong văn bản Tiếng Việt. Ngoài ra, chúng tôi cũng đã tích hợp thêm kiến trúc BiLSTM ở cấp độ ký tự để giúp hệ thống BiLSTM+CRF có được ma trận word embedding tốt hơn.Với hệ thống nêu trên, chúng tôi đã đạt được độ chính xác F1 tổng quát 74% (Level 1) và 68% (Nested Level) trên tập dữ liệu Test của cuộc thi VLSP NER Shared Task, xếp vị trí thứ 2 chung cuộc.Điểm yếu trong mô hình của chúng tôi là không có khả năng xử lý các trường hợp nhãn lồng nhau. Để xử lý được vấn đề này, chúng tôi sẽ xem xét bài toán nhận dạng tên riêng như bài toán phân tích cây (tree parsing) như vậy sẽ thể hiện được các cấp độ khác nhau của nhãn tên riêng. Đây sẽ là hướng đi trong thời gian sắp tới của nhóm nghiên cứu.By Thang Luong[1] VLSP 2018 http://vlsp.org.vn/vlsp2018",23/04/2018,5,47,36,,5,5,0.0,9,vi,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
8,Introduction to Instance Segmentation on images and video,,Adam Simek,2.0,6.0,1114,"This article is short introduction to instance segmentation and discuss recent libraries. Reader will learn how to work with Detectron2 in way of commented python codes and will be provided with references to useful resources.Instance segmentation is popular image processing task, to understand this technique we must first understand it’s subtasks.Classification is categorization process, where each instance, in our case object in image is being labeled with information what it is and score describing weight of prediction in percentage.Object Detection extends classification for multiple objects in one image as well as their detection, it is usually described in results as bounding boxes.Semantic Segmentation in nutshell makes prediction for each pixel of image to decide which class, in result we can obtain layers of each present class in image, for example as if you wanted extract human from image in photoshop.Finally Instance Segmentation consist of all previously mentioned tasks with modification for semantic segmentation to apply locally for each object found by object detection and each individual instance of class.In this article we will focus only on archetype of Faster R-CNN version, which had many predecessors, for history fans or those interseted more in R-CNN achitecture this article is recommended. Now we will generally describe parts of this architecture.Backbone is CNN which transforms input image into features, which are further used by other components. It usually uses well known CNN architectures such as ResNet, VGG, ConvNet, etc.. Nowadays ResNet is one of the most popular especially with Feature Pyramid Network modification.RPN, Region Proposal Network is trained to detect objects on features obtained from backbone, objects are detected in rectangular areas, which can be modified to be found in various sizes and side size ratios (1:2, 1:1, 2:1). Outputs are anchors described by starting coordinates and sizes.ROI Pooling, because Regions of Interest from features and anchors have various sizes and shapes, it would be necessary to train following Neural Network parts for each configuration, which is time and space consuming. For this case there is workaround by ROI Pooling and Align, which transform shape and size of region by pooling and bilinear interpolation method to be uniform for all.Classifier takes ROI Pooling output features as input and labels instance of object with evaluating score of prediction.Mask R-CNN is extension of Faster R-CNN, which works as branch parallel with classifier and takes the same inputs and uses them to predict masks of objects.In summary outputs of this network are bounding boxes, classes, scores and masks of predicted objects.Detectron2 is successor of its older version by Facebook AI Research team, completely rewritten in Pytorch.The reason why it is my personal number one recommendation choice is design of library, which is easily usable with configuration files and many pre-trained models and at the same time allows extendable or exchangable backbone to use your preferred CNN with keeping other functionalities, other parts are also well structured to be comfortably customized.There were also improvements in training speed and library provides all standard functionalities, like GPU support and multi-GPU training.Other competitive libraries usually don’t provide as many perks for users, because they are raw products of research.Following codes describe inference with Detectron2, to install this library use documentation. These codes are slightly modified documentation examples with more descriptions.Input file is in format of openCV image, to set up Detectron2 Mask R-CNN you have to use pre-trained or trained by yourself model. You can choose from many models in ModelZoo.Output in form of dictionary doesn’t include image itself, therefore it has to be added to existing image with Visualiser.Following code is example of minimalistic video instance segmentation, it runs as fast as it can, because it is hard to reach even 30 FPS on fastest Instance segmentation (Detectron2 can achieve around 10 FPS real-time). Meaning it is slower than reality, but it gives you insight how fast it is. Still it is fun to run instance segmentation on random videos on youtube.I have used imutils buffered video stream, because it have transformation function which can be used for prediction, offers good base for building on this and has amazing detailed guide.Video is launched in openCV window you can stop it by pressing ‘Q’ key.This we will learn how to train Detectron2 with custom dataset. Datasets for Instance segmentation are very rare because it is lot of work to create precise masks, they are mostly roughly estimated by machine and then handcrafted by humans and it is not more fun than peeling potatoes.Luckily there are some portals, which provide these data for free. For example COCO datasets are very popular and also being used in competitions.COCO dataset always comes with images and annotation .json file dictionary with masks, classes, etc..Detectron2 have special MetaCatalog for representation of training data. First task is to transform data into MetaCatalog, this task is unique for your particular data structure, but targeted dataset format is very well described in documentation. Example of training balloon dataset to extend can be found on google colab, with good Python skills you can understand MetaCatalog structure from this example. Another option described in documentation is to load COCO dataset or your own dataset which has been designed with annotations file (similar to output from inference) in “.json” to match COCO format.This way automatically treats data as COCO dataset and creates MetaCatalog “my_dataset”, which can be used for training.However for larger training it is recommended to use python script detectron2/tools/train_net.py.Most of recent libraries use slightly modified FPN architecture with ResNet or RetinaNet, it is hard to properly benchmark since there is not a lot of data and every project has been most likely trained on most of data avaliable, this makes it slightly harder to define good validation dataset, therefore most accurate comparisons are image processing competitions, where networks are introduced to exclusive validation data.There are two popular types of R-CNN being used right now, first originates from Faster R-CNN, these aim to achieve best precision and examples are: Detectron2, Mask Scoring R-CNN, Path Aggregation Network for Instance Segmentation, RetinaMask.The other group are are sucessors of You Only Look Once (YOLO) and Single Shot Detector (SSD) networks, trying to achieve best speed, while maintaining reasonable precision, their Mask alternative for instance segmentation is called Yolact and it is capable of 30 FPS real-time video instance segmentation. Yolact might be most interesting alternative if you are interested in speed performance, also their library have lot of funcionalities videoplayer obviously included.Hopefully you found this article useful to get along with Detectron2 faster, and tried instance segmentation on some questionable videos. Or got hyped for Yolact, it is matter of preference. Anyway thank you for reading this article, and good luck in all your Machine Learning endeavors.Referencesgithub.comgithub.comgithub.comgithub.comgithub.com",02/01/2020,1,11,7,"(633, 468)",4,0,0.0,25,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
9,Intersection Over Union,BISA.AI,Alfi Salim,24.0,5.0,506,"Pada masalah deteksi objek, output yang dihasilkan berupa bounding box (kotak pembatas) hasil prediksi sistem terhadap objek yang telah ditentukan. Bounding box ini merepresentasikan posisi objek dalam sebuah gambar. Untuk mengevaluasi model deteksi objek yang telah kita latih terdapat beberapa cara, salah satu caranya adalah dengan menggunakan metode Intersection Over Union (IOU). IOU memanfaatkan bounding box yang terdapat pada gambar.Intersection Over Union (IOU) adalah nilai berdasarkan statistik kesamaan dan keragaman set sampel yang tujuannya untuk mengevaluasi area tumpang tindih (area yang beririsan) antara dua bounding box, yaitu bounding box hasil prediksi dan bounding box ground truth (kebenaran). Jadi, syarat untuk menerapkan IOU adalah mempunyai kedua bounding box tersebut. Berawal dari menerapkan IOU, kita dapat mengetahui nilai-nilai evaluasi yang lainnya, seperti precision, recall dan lain sebagainya. Persamaan intersection over union sebagai berikut:Persamaan Intersection Over Union dapat diilustrasikan dalam gambar berikut:Berdasarkan ilustrasi di atas, dapat kita lihat bahwa persamaan untuk mendapatkan nilai IOU hanyalah sebuah perbandingan dari area irisan dibagi dengan area gabungan. Dengan membagi kedua area tersebut, maka kita akan mendapatkan skor Intersection Over Union (IOU). Setelah mendapatkan skor IOU, aturan yang ada untuk menilai apakah skor IOU yang kita dapat baik atau buruk adalah semakin beririsan atau semakin dekat jarak antara bounding box prediksi dengan bounding box ground truth. Untuk lebih jelasnya, saya akan mengilustrasikan skor IOU yang kita kategorikan sebagai skor yang baik, lumayan dan buruk.Dari ilustrasi diatas dapat kita simpulkan bahwa skor akan semakin tinggi jika jarak antara bounding box prediksi dengan bounding box ground truth semakin dekat (area yang berisisan antara kedua bounding box semakin besar).Untuk teman-teman yang masih bertanya-tanya bagaimana kita mendapatkan bounding box ground truth akan saya jelaskan sedikit. Jadi, bounding box ground truth didapatkan dengan cara mengannotasi atau memberi label pada dataset secara manual. Telah banyak tools-tools yang mempermudah kita dalam mengannotasi sebuah data, khususnya data gambar. Pada artikel ini, kita tidak akan membahas mengenai data gambar dan cara mengannotasinya, mungkin materi tersebut akan dibahas secara lebih rinci pada artikel yang berbeda. Jadi, terus ikuti Medium kami.Setelah mengetahui beberapa hal tentang IOU, langsung saja kita implementasikan menggunakan bahasa pemrograman Python. Pada kasus ini, kita akan mencari nilai IOU pada gambar-gambar berikut:Sebelumnya, kita asumsikan bahwa kita telah memiliki model yang telah bisa memprediksi objek burung dalam gambar dengan cara mengeluarkan bounding box. Gambar-gambar di atas saya simpan di sebuah folder. Masing-masing gambar mempunyai satu file anotasi yang disimpan dalam folder yang sama dengan gambar.Langkah awalnya yaitu mengimport semua library yang dibutuhkan. selanjutnya read semua dataset :Selanjutnya, buat fungsi untuk mengitung skor IOU, disini saya beri nama fungsi_iou dengan 2 parameter yaitu bounding box ground truth (BBox_GroundT) dan bounding box prediksi (Bbox_prediksi)Koordinat bounding box ground truth dari semua gambar disimpan sebagai list pada variabel groundTruth. Output prediksi koordinat dari model kita, disimpan juga sebagai list pada variabel prediksi.Visualisasikan bounding box ground truth dan prediksi pada masing-masing gambar. Terakhir, hitung nilai IOU nya.Terakhir, kode di atas adalah untuk menampilkan hasil gambar dengan semua bounding box dan hasil IOU nya dari masing-masing gambar. Adapun output yang dihasilkan :Terima kasih, sekian dari saya. Semoga bermanfaat!",25/03/2020,5,14,39,"(590, 321)",7,4,0.0,1,id,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,no emotion
10,Adversarial Attacks in Machine Learning and How to Defend Against Them,Towards Data Science,Haohui,154.0,9.0,1860,"Big Data powered machine learning and deep learning has yielded impressive advances in many fields. One example is the release of ImageNet consisting of more than 15 million labelled high-resolution images of 22,000 categories which revolutionized the field of computer vision. State-of-the-art models have already achieved a 98% top-five accuracy on the ImageNet dataset, so it seems as though these models are foolproof and that nothing can go wrong.However, recent advances in adversarial training have found that this is an illusion. A good model misbehaves frequently when faced with adversarial examples. The image below illustrates the problem:The model initially classifies the panda picture correctly, but when some noise, imperceptible to human beings, is injected into the picture, the resulting prediction of the model is changed to another animal, gibbon, even with such a high confidence. To us, it appears as if the initial and altered images are the same, although it is radically different to the model. This illustrates the threat these adversarial attacks pose — we may not perceive the difference so we cannot tell an adversarial attack as happened. Hence, although the output of the model may be altered, we cannot tell if the output is correct or incorrect.This formed the motivation behind the talk for Professor Ling Liu’s keynote speech at the 2019 IEEE Big Data Conference, where she touched on types of adversarial attacks, how adversarial examples are generated, and how to combat against these attacks. Without further ado, I will get into the contents of her speech.Adversarial attacks are classified into two categories — targeted attacks and untargeted attacks.The targeted attack has a target class, Y, that it wants the target model, M, to classify the image I of class X as. Hence, the goal of the targeted attack is to make M misclassify by predicting the adversarial example, I, as the intended target class Y instead of the true class X. On the other hand, the untargeted attack does not have a target class which it wants the model to classify the image as. Instead, the goal is simply to make the target model misclassify by predicting the adversarial example, I, as a class, other than the original class, X.Researchers have found that in general, although untargeted attacks are not as good as targeted attacks, they take much less time. Targeted attacks, although more successful in altering the predictions of the model, come at a cost (time).Having understood the difference between targeted and untargeted attacks, we now come to the question of how these adversarial attacks are carried out. In a benign machine learning system, the training process seeks to minimize the loss between the target label and the predicted label, formulated mathematically as such:During the testing phase, the learned model is tested to determine how well it can predict the predicted label. Error is then calculated by the sum of the loss between the target label and the predicted label, formulated mathematically as such:In adversarial attacks, the following 2 steps are taken:One way the query input is changed from x to x’ is through the method called “adversarial perturbation”, where the perturbation is computed such that the prediction will not be the same as the original label. For images, this can come in the form of pixel noise as we saw above with the panda example. Untargeted attacks have the single goal of maximizing the loss between H(x) and H(x’) until the prediction outcome is not y (the real label). Targeted attacks have an additional goal of not only maximizing the loss between H(x) and H(x’) but also to minimize the loss between H(x’) and y’ until H(x’) = y’ instead of y.Adversarial perturbation can then be categorized into one-step and multi-step perturbation. As the names imply, the one-step perturbation only involves a single stage — add noise once and that is it. On the other hand, the multi-step perturbation is an iterative attack that makes small modifications to the input each time. Therefore, the one-step attack is fast but excessive noise may be added, hence making it easier for humans to detect the changes. Furthermore, it places more weight on the objective of maximizing loss between H(x) and H(x’) and less on minimizing the amount of perturbation. Conversely, the multi-step attack is more strategic as it introduces small amounts of perturbation at each time. However, this also means such an attack is computationally more expensive.Now that we have looked at how adversarial attacks are generated, some astute readers may realize one fundamental assumption these attacks take on — that the attack target prediction model, H, is known to the adversary. Only when the targeted model is known can it be compromised to generate adversarial examples by changing the input. However, attackers do not always know or have access to the targeted model. This may sound like a surefire way to ward off these adversarial attackers, but the truth is that black box attacks are also highly effective.Black box attacks are based on the notion of transferability of adversarial examples — the phenomenon whereby adversarial examples, although generated to attack a surrogate model G, can achieve impressive results when attacking another model H. The steps taken are as follows:This attack can be launched either with the training dataset being known or unknown. In the case where the dataset is known to the adversary, the model G can be trained on the same dataset as model H to mimic H.When the training dataset is unknown however, adversaries can leverage on Membership Inference Attacks, whereby an attack model whose purpose is to distinguish the target model’s behavior on the training inputs from its behavior on the inputs that it did not encounter during training is trained. In essence, this turns into a classification problem to recognize differences in the target model’s predictions on the inputs that it trained on versus the inputs that it did not train on. This enables the adversary to obtain a better sense of the training dataset D which model H was trained on, enabling the attacker to generate a shadow dataset S on the basis of the true training dataset so as to train the surrogate model G. Having trained G on S where G mimics H and S mimics D, black box attacks can then be launched on H.Now that we have seen how black box attacks vary from white box attacks in that the target model H is unknown to the adversary, we will cover the various tactics used in black box attacks.One simple way in which the query input is changed from x to x’ is by simply adding something physically (eg. bright colour) to disturb the model. One example is how researchers at CMU added eyeglasses to a person in an attack against facial recognition models. The image below illustrates the attack:The first row of images correspond to the original image modified by adding the eyeglasses, and the second row of images correspond to the impersonation targets, which are the intended misclassification targets. Just by adding the eyeglasses onto the original image, the facial recognition model was tricked into classifying the images on the top row as the images in the bottom row.Another example comes from researchers at Google who added stickers to the input image to change the classification of the image, as illustrated by the image below:These examples show how effective such physical attacks can be.Another way in which black box attacks are carried out is through out-of-distribution (OOD) attacks. The traditional assumption in machine learning is that all train and test examples are drawn independently from the same distribution. In an OOD attack, this assumption is exploited by providing images of a different distribution from the training dataset to the model, for example feeding TinyImageNet data into a CIFAR-10 classifier which would lead to an incorrect prediction with high confidence.Now that we have taken a look at the various types of adversarial attacks, a natural question then comes — how can we trust our machine learning models if they are so susceptible to adversarial attacks?One possible approach has been proposed by Chow et al. in 2019 in the paper titled “Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks”. The approach is centred around enabling machine learning systems to automatically detect adversarial attacks and then automatically repair them through the use of denoising and verification ensembles.First, input images have to pass through denoising ensembles that attempt different methods to remove any added noise to the image, for example adding Gaussian noise. Since the specific noise added to the image by the adversary is unknown to the defender, there is a need for an ensemble of denoisers to each attempt to remove each type of noise.The image below shows the training process for the denoising autoencoder — the original image is injected with some noise that the attacker might inject, and the autoencoder tries to reconstruct the original uncorrupted image. In the training process, the objective is to reduce the reconstruction error between the reconstructed image and the original image.By developing an ensemble of these autoencoders each trained to remove a specific type of noise, the hope is that the corrupted images would be sufficiently denoised such that it is close to the original uncorrupted image to allow for image classification.After the images have been denoised, they then go through a verification ensemble which reviews every denoised image produced by each denoiser and then classifies the denoised image. Each classifier in the verification ensemble classifies each denoised image, and the ensemble then votes to determine the final category the image belongs to. This means that although some images may not have been denoised the correct way in the denoising step, the verification ensemble votes on all the denoised images, thereby increasing the likelihood of making a more accurate prediction.Diversity of the denoisers and verifiers have found to be very important because firstly, adversarial attackers will get better at altering images so there is a need for a diverse group of denoisers that can handle a variety of corrupted images. Following this, there is also a need for verifiers to be diverse so they can generate a variety of classifications so that it would be difficult adversarial attackers to manipulate them just as how they have managed to manipulate normal classifiers that we trust and use so frequently in machine learning.This remains an open problem because, after all these decisions by the various verifiers, there is still a final decision maker that needs to decide whose opinion to listen to. The final decision maker would need to preserve the diversity present in the ensemble, which is not an easy task to tackle.We have taken a look at various types of adversarial attacks as well as a promising method to defend against these attacks. This is definitely something to keep in mind when we implement machine learning models. Instead of blindly trusting the models to produce the correct results, we need to guard against these adversarial attacks and always think twice before we accept the decisions made by these models.A huge thanks to Professor Liu for this enlightening keynote on this pressing problem in machine learning!",19/12/2019,0,1,2,"(700, 302)",6,2,0.0,11,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,trust/acceptance
11,An Introduction to Naïve Bayes Classifier,Towards Data Science,Yang S,163.0,4.0,492,"This blog will cover following questions and topic:1. What is Naïve Bayes Classifier?2. How to calculate parameters and make a prediction in Naïve Bayes Classifier?3. Laplace Smoothing4. Application in pythonThe Naïve Bayes Classifier belongs to the family of probability classifier, using Bayesian theorem. The reason why it is called ‘Naïve’ because it requires rigid independence assumption between input variables. Therefore, it is more proper to call Simple Bayes or Independence Bayes. This algorithm has been studied extensively since 1960s. Simple though it is, Naïve Bayes Classifier remains one of popular methods to solve text categorization problem, the problem of judging documents as belonging to one category or the other, such as email spam detection.The goal of Naïve Bayes Classifier is to calculate conditional probability:for each of K possible outcomes or classes Ck.Let x=(x1,x2,…,xn). Using Bayesian theorem, we can get:The joint probability can be written as:Assume that all features x are mutually independent, we can get:Therefore, formula can be written as:Therefore, this is the final formula for Naïve Bayes Classifier.2. How to calculate parameters and make a prediction in Naïve Bayes Classifier?Maximum Likelihood Estimation (MLE) is used to estimate parameters — prior probability and conditional probability.The prior probability equals the number of certain cases of y occur divided by the total number of records.The conditional probability of p(x1=a1|y=C1) equals the number of cases when x1 equals to a1 and y equals to C1 divided by the number of cases when y equals to C1.Naïve Bayes Classifier uses following formula to make a prediction:For example, 15 records in the table below are used to train a Naïve Bayes model, and then a prediction is made to a new record X(B, S).Use formula above to estimate prior and conditional probability, and we can get:Finally, as of X(B, S), we can get:P(Y=0)P(X1=B|Y=0)P(X2=S|Y=0)> P(Y=1)P(X1=B|Y=1)P(X2=S|Y=1), so y=0.3. Laplace SmoothingIn statistics, Laplace Smoothing is a technique to smooth categorical data. Laplace Smoothing is introduced to solve the problem of zero probability. By applying this method, prior probability and conditional probability can be written as:K denotes the number of different values in y and A denotes the number of different values in aj. Usually lambda in the formula equals to 1.By applying Laplace Smoothing, the prior probability and conditional probability in previous example can be written as:4. Application in pythonStep 1: Create a dataset.Step 2: Train Naïve Bayes Model by calculate prior and conditional probability.Step 3: Make a prediction.Summary:Naïve Bayes Classifier can be trained easily and fast and can be used as benchmark model. When variable selection is carried out properly, Naïve Bayes can perform as well as or even better than other statistical models such as logistic regression and SVM. Naive Bayes requires a strong assumption of independent predictors, so when the model has a bad performance, the reason leading to that may be the dependence between predictors.You can read more blogs by clicking on the following link:medium.comReference:[1] Christopher M. Bishop, (2009), Pattern Recognition and Machine Leaning[2] https://en.wikipedia.org/wiki/Naive_Bayes_classifier[3] https://en.wikipedia.org/wiki/Additive_smoothing",09/09/2019,0,8,14,"(420, 93)",15,1,0.0,4,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,neutral,joy/calmness
12,Dropout Technique and Ensemble Methods,unpackAI,Aleksandr,11.0,4.0,592,"About a decade ago deep learning networks were infamous due to overfitting issue. A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.One approach to reduce overfitting is to fit all possible different neural networks on the same dataset and to average the predictions from each model. This is not feasible in practice, and can be approximated using a small collection of different models, called an ensemble.Conceptually, ensemble learning combines multiple models in a single meta-model in order to minimize the generalization error. There are many ways to efficiently construct ensemble models.A problem even with the ensemble approximation is that it requires multiple models to be fit and stored, which can be a challenge if the models are large, requiring days or weeks to train and tune.Then, around 2012, the idea of Dropout emerged. The concept revolutionized Deep Learning. Much of the success that we have with Deep Learning is attributed to Dropout.Dropout changed the concept of learning all the weights together to learning a fraction of the weights in the network in each training iteration. This issue resolved the overfitting issue in large networks. And suddenly bigger and more accurate Deep Learning architectures became possible.Regularization reduces over-fitting by adding a penalty to the loss function. But L1 and L2 regularization methods did not completely solve the overfitting issue. The reason was Co-adaptation.In large networks co-adaptations are common — if all the weights are learned together some of the connections will have more predictive capability than the others. Over many iterations these connections become too powerful and others get ignored.L1 and L2 can’t prevent that. Result is — expanding the neural network size would not help. And that limits our neural net!Dropout is an approach to regularization in neural networks which helps reducing interdependent learning amongst the neurons by learning a fraction of the weights in the network in each training iteration.For intermediate layers, choosing (1-p) = 0.5 for large networks is ideal. For the input layer, (1-p) should be kept about 0.2 or lower. This is because dropping the input data can adversely affect the training. To understand why 0.5 is good you need to look into math.In Dropout we are dropping a connection with probability (1-p). Put mathematically we have the connection weights multiplied with a random variable, 𝛿, where 𝛿 ~ Bernoulli(p). We can replace the Bernoulli gate with another gate. For example, a Gaussian Gate. And this gives us a Gaussian-Dropout.Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.The weights of the network will be larger than normal because of dropout. Therefore, before finalizing the network, the weights are first scaled by the chosen dropout rate. The network can then be used as per normal to make predictions.Dropout is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.Because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training. As such, a wider network, e.g. more nodes, may be required when using dropout.This also takes care of the Ensemble problem, we have Ensemble of different neural networks and all of those are created from one single network/architecture dropping some nodes from it.Sources:",01/02/2021,0,6,0,"(571, 267)",6,1,0.0,4,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
13,MIT Presents New Approach for Sequence-to-Sequence Learning with Latent Neural Grammars,SyncedReview,Synced,24000.0,4.0,481,"Sequence to sequence modelling (seq2seq) with neural networks has become the de facto standard for sequence prediction tasks such as those found in language modelling and machine translation. The basic idea is to use an encoder to transform the input sequence into a context vector; then use a decoder to extract an output sequence that predicts the next value from that vector.Despite their power and impressive achievements, seq2seq models are often sample-inefficient. Also, due to their relatively weak inductive biases, these models can fail spectacularly on benchmarks designed to test for compositional generalization.The new MIT CSAIL paper Sequence-to-Sequence Learning with Latent Neural Grammars proposes an alternative, hierarchical approach to seq2seq learning with quasi-synchronous grammars, developing a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without requiring manual feature engineering.The paper identifies three ways in which the proposed approach differs from previous work in this area:Typically, quasi-synchronous grammars define a monolingual grammar over target strings conditioned on a source tree, where the grammar’s ruleset depends dynamically on the source tree. This work instead uses probabilistic quasi-synchronous context-free grammars (QCFG), which transduce the output tree by aligning each target tree node to a subset of source tree nodes, making it suitable for tasks where syntactic divergences are common.Also, this grammar does not need to capture hierarchical structures implicitly within a neural network’s hidden layers; rather it can explicitly model the hierarchical structure on both the source and target side, resulting in a more interpretable generation process.As each source tree node often occurs a few times in the training corpus, parameter sharing is required. While previous work on QCFGs involved intensive manual feature engineering to share parameters across rules, this approach instead employs a neural parameterization to enable efficient sharing of parameters over the combinatorial space of derivation rules.For evaluation purposes, the proposed approach was applied to various seq2seq learning tasks, including a SCAN language navigation task designed to test for compositional generalization, style transfer on the English Penn Treebank, and small-scale English-French machine translation.In the experiments, the proposed approach achieved decent performance on niche datasets such as SCAN and StylePTB, but woefully underperformed compared to a well-tuned transformer on machine translation tasks.Overall, the study shows that the formalism of quasi-synchronous grammars can provide a flexible tool for imbuing inductive biases, operationalize constraints, and interface with models. The paper proposes future work in this area could involve revisiting richer grammatical formalisms with contemporary parameterizations, conditioning on images/audio for grounded grammar induction, adaption to programs and graphs, and investigating the integration of grammars and symbolic models with pretrained language models to solve practical tasks.The paper Sequence-to-Sequence Learning with Latent Neural Grammars is on arXiv.Author: Hecate He | Editor: Michael Sarazen, Chain ZhangWe know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",17/09/2021,0,5,3,"(700, 379)",5,1,0.0,2,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,trust/acceptance
14,Hairstyle Transfer — Semantic Editing GAN Latent Code,The Startup,Azmarie Wang,77.0,9.0,1753,"Recent advances in Generative Adversarial Network (GAN) have shown impressive results in the quality and resolution of the images synthesized, especially in the field of style transfer. Motivated by the recent success of StyleGAN [1], where stochastic variation is incorporated in the realistic-looking synthesized images, we propose to focus on one of the most practical variations — hairstyle.A hairstyle, as an important part of personal appearance, can be expressive of one’s personality and overall style. More often than not, the right hairstyle can only be discovered through trial and error. Thus, being able to virtually “try on” a novel hairstyle through a computer vision system seems to hold practical value in reality.Input: A human face imageOutput: Images with the same face but different hairstylesPrincipal Method: Exploring GAN latent spaceThe rationale of GAN is to learn a non-linear mapping from a latent distribution to real data through adversarial training.Usually, the relation between latent space and semantic attributes is unknown. For example, how does the latent code determine the hairstyle generated, such as bangs, colors, and etc? Also, it is hard to judge if these attributes are entangled with each other.Our approach in this project is to explore how a single or multiple hairstyle semantics are encoded in the latent space of trained GAN models, such as PG GAN [1] and StyleGAN [2].We take advantage of the notion brought up by the InterFaceGAN [3] paper: for any binary semantic, there exists a hyperplane in the latent space serving as the separation boundary. Based on this idea, we managed to disentangle attribute representations by linear transformations.The first model this work is based on is the Progressive Growing (PG) GAN [1], which comprises a hierarchy of generators and discriminators with increasing resolutions. They capture large-scale structures on coarse levels and details on fine levels.All generators and discriminators are trainable during the training phase and we usually add layers incrementally as the training advances. Skip connections between a layer’s input and its output are utilized to retain the results of the previous layer.PG GAN manages to solve some traditional GAN problems.It is hard for traditional GANs to generate high-resolution images because it is easier to tell the generated images apart from the real images at a higher resolution. This may lead to big gradients that force the network to converge slower. Besides, larger images require smaller batch sizes that lead to unstable training.The hierarchical structure solves some traditional GAN problems by training from coarse to fine:Another underlying model that we use is StyleGAN [2].First of all, StyleGAN converts the input latent code z into an intermediate latent code w in a non-linear manner.Z refers to the original latent code space and reflects the probability density of the training data, which often leads to unavoidable entanglements. W here is the intermediate latent space and is induced by a learned piecewise continuous mapping from Z.In the synthesis network:The synthesis network also follows a hierarchical structure where each convolution layer adjusts the style of the image at a different resolution. This controls the strength of image features at different scales.Now, let’s dive into how you could semantically edit your favourite image.The following workflow allows you to take a human image, generate its latent code estimation, and semantically edit it with the hair attributes that you care about. Let the fun begin!To do semantic editing, we first need to find the query image inside the StyleGAN latent space. Now the question is, with a given input image, how to find a latent vector z such that if we send z through the generator, we can still get the same input image?One way to do this is to optimize the feature vector that has a high-level semantic meaning of what’s in the image.First, we send the input image into a pre-trained Residual Network for an initial latent code estimation in StyleGAN. We then take this estimation and send it to the generator. This will give us an initial guess of the original input image. To this image, we can apply a pre-trained image classifier for feature extraction purposes. Meanwhile, we will do the same feature extraction for the input image.In the feature space, we then perform gradient descent — minimizing the L2 loss of the features vectors and updating the latent code estimation (red arrow). This method of doing gradient descent on semantic feature vectors has an edge over gradient descent on pixel loss because using L2 optimization directly in the pixel space can lead us to get stuck easily in bad local optima.We can now use this approach to find ANY image inside the StyleGAN latent space. Below are some examples of input images and their latent code representations. Pretty close, right?Same as Interface GAN [3], we will define “semantic editing” as editing an image with a target attribute only while maintaining all the other information as well as possible.Before we dive into editing, we need to look for specific boundaries that can separate binary attributes in the latent space. Each of the boundaries will be corresponding to one hair attribute in particular.With respect to this project, here are the hair attributes we are interested in studying:So how to find the boundaries? We first need to do latent space separation. This work [3] has introduced a robust approach for such a purpose.With the assumption that for any binary attribute, there exists a hyperplane in latent space such that all samples from the same side are with the same attribute, we can train independent linear SVM responsible for each attribute. Our job is to find such a hyperplane from the 512-dimensional latent space from StyleGAN.To look for the hyperplane, we need paired data of latent code and score for this attribute. One obvious solution is to find face images where such attribute is potent and manually label them with 0/1 scores. We toyed with this idea and manually labelled 50 images to confirm the feasibility of finding the boundary. Later we decided to use pre-trained classifiers trained on a large dataset (CelebA) for the hair attributes, which are provided with StyleGAN.We used 10 classifiers that match the 10 attributes to generate ~20k latent code and score pairs. With the paired latent code/scores, we trained independent linear SVMs on the hair attributes mentioned earlier and then evaluated them on the validation set, reaching accuracies ~80%.Putting things together, for each input image, we will first find its specific location in the StyleGAN latent space, and then move it along a specific direction for semantic editing.With the linear hyperplane for each attribute, we will take its normal vector as the direction along which the output faces will have continuous changes with regard to the target attribute. For example, in the figure above, we found the latent code of the image of young Leonardo DiCaprio inside StyleGAN space, drew a direction orthogonal to the bangs hyperplane, moved the location of the latent code alongside the direction. This would create a morphing sequence of DiCaprio having no bangs, fewer bangs, bangs, and more bangs!Lastly, we want to talk about Conditional Boundary, which is also introduced in InterfaceGAN [3]. More often than not, many attributes can be coupled with each other. For example, receding hairline is associated with age, long wavy hair is more frequent in female faces, and facial hair such as mustache and sideburns are often spotted in male faces only. Thus, it’s crucial to disentangle the target attribute from another attribute that correlates with it.As shown in the figure above, given two hyperplanes with normal vectors n1 and n2, moving along the projected direction n1 − (transpose(n1) *n2) * n2 can change attribute 1 without affecting attribute 2. This operation is called conditional manipulation as pointed out in InterfaceGAN [3].From our experiments, we discovered that receding hairline is an attribute that’s correlated with the attribute smile: the output of receding hairline tends to slightly open their mouth and smile. We imagine this is because, in the dataset, people with receding hairline seem to be more friendly and smiley. Thus, to produce an output with receding hairline without them smiling, we can subtract the projection from the primal direction (receding hairline) onto the plane that is constructed by the conditioned direction (smile). Below is a result of the face of George Clooney with and without a conditional boundary.Based on our experiment, we realize that many attributes are correlated in the latent space. For example, when editing the volume of hairs, the person will become older or younger based on how much hairs are on the person’s head.Not every result we get is perfect. When the workflow fails to generate reasonable output in our experiments, we can often optimize our boundary with conditions accordingly. If an input image has a large distortion, we can find which attribute resembles more to the distortion then apply this attribute as a conditional attribute to our primary attribute. For example, when editing facial hairs, we can conditionally bound it with the smile attribute to the person so out the output face won’t have their mouth open.However, there are also some failure cases that are beyond saving, such as heavy distortion, vampirizing the face, or no results at all.In addition, we found the generative model is possibly biased. Since the dataset used to the training generator is based on real humans, gender-specific attributes seem to only appear with a specific gender. For example, adding a mustache to female faces will make the person look more masculine yet it produces little to no mustache on the face.In conclusion, we can edit one attribute of the human face by finding the hyperplane boundary in the latent space, which can generate amazing yet not perfect results. So far, we can set one attribute as a conditional attribute along with the primary attribute, as discussed in InterfaceGAN [3]. Additionally, when using one attribute to edit our face, some other attributes may also be changed because of their correlations. We believe using a better classifier can control more than two conditions at the same time and make the boundary more explicit. Last but not the least, this model can’t generate a female face with male attributes, and vice versa, we think this can be solved by using a special dataset as the training data for the generator.[1] Progressive GAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation[2] StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks[3] InterFace GAN: Interpreting the Latent Space of GANs for Semantic Face Editing[4] StyleGAN Encoder: Converts real images to latent space",24/05/2020,0,9,11,"(663, 300)",17,3,0.0,6,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
15,CNN Transfer Learning & Fine Tuning,Towards Data Science,Victor Roman,2100.0,9.0,1066,"Learn how to apply these powerful techniques to take your Deep Learning models to a whole new level!As we have seen in previous articles, we can use the architectures developed by research teams and leverage their power to make predictions and obtain better results in our Deep Learning models.Training a neural network takes time, luckily nowadays there are ways to avoid having to:We already have seen ways to avoid having to define the architecture here, and it consists of using predefined architectures that are known to work well: ResNet, AlexNet, VGG, Inception, DenseNet, etc.And what about avoiding training it from scratch? What do I mean by that?Neural networks are initialized with random weights (usually) that after a series of epochs reach some values that allow us to properly classify our input images.What would happen if we could initialize those weights to certain values that we know beforehand that are already good to classify a certain dataset?In this way, we would not need a dataset as big as if we were to train a network from zero (from hundreds of thousands or even millions of images we could go to a few thousands) nor would we need to wait a good number of epochs for the weights to take good values for the classification, they would have it much easier due to their initialization.Let’s explore how to do this with the techniques of transfer learning and fine-tuning:Let’s take the VGG16 network trained on the ImageNet dataset as an example. Let’s see its architecture:We know that ImageNet consists of a dataset of about 1.2 million images for training, 50,000 for validation and 100,000 for testing, belonging to 1000 categories.Now imagine that what we want is to apply the VGG16 trained on ImageNet to another dataset, let’s say that we choose the CIFAR-10. How could we do it?Remembering the general scheme of a CNN, we have a feature extractor in the first stage and then a classifier:What if we removed the last layer of the VGG16, which simply takes a probability for each of the 1000 classes in the ImageNet and replaces it with a layer that takes 10 probabilities? This way, we could take all the knowledge that VGG16 has trained on the ImageNet and apply it to our problem!As we have seen, what we will do is change the classification stage, so that the last layer is one of 10 neurons (our CIFAR 10 has 10 classes) and then we will retrain the network allowing the weights of the fully connected layers to be changed, that is, the classification stage.For this, we would initialize our network with the weights from the ImageNet, and then freeze all the convolutional and max-pooling layers so that they do not modify their weights, leaving only the fully connected ones free.Once this is done, we would start retraining. In this way, we manage to take advantage of the feature extraction stage of our network and only tune the final classifier to work better with our dataset. This is what is known as Transfer Learning because we take advantage of the knowledge of another problem to solve the one we are dealing with.This approach can also be done by saving the characteristics given by the last layer of max pooling and then putting that data into any classifier (SVM, logreg, etc).Let’s see how we could do it:We haven’t had to train at all, and we’ve had not bad results! Keep in mind that if we did it randomly the probability of getting it right would be 1/10=0.1 or 10%, since we have 10 classes.The more similar the dataset on which the network was originally trained and the dataset of our problem are, the better results we get.And if our dataset has nothing to do with the one of ImageNet or we want to improve the results even more?Well, for that, we use fine-tuning.With fine-tuning, we first change the last layer to match the classes in our dataset, as we have done before with transfer learning. But besides, we also retrain the layers of the network that we want.Remember again the architecture of the VGG16:What we did in the previous example was to change only the layers of the classification stage, keeping the knowledge that the network obtained when extracting characteristics (patterns) in the previous task, from which we are loading the weights (ImageNet).With fine-tuning we are not limited to retraining only the classifier stage (i.e. the fully connected layers), but what we will do is retrain also the feature extraction stage, i.e. the convolutional and pooling layers.It’s important to keep in mind that in a neural network, the first layers detect simpler and more general patterns, and the more we advance in the architecture, the more specific to the dataset and the more complicated the patterns they detect.Therefore, we could allow the last block of convolution and pooling layers to be retrained.When do I do fine-tuning and transfer learning? How do I choose which layer to retrain from?Well, as a general rule, the first thing we will do is transfer learning, i.e. we will not retrain our network. That will give us a baseline that we will have to overcome. Then, we will retrain only the classification stage, and then we can try to retrain some convolutional block as well.Following these steps most of the time you will reach a suitable result for your problemIt also depends on the type of problem you have. If:When using any of these techniques, you must take into account the possible restrictions of the pre-trained models. For example, they may require a minimum image size.Besides, when retraining networks, we usually choose lower learning rates than if we do it from scratch since we start from the initialization of weights that is assumed to be good.So,the way to go when we face a Deep Learning problem would be to star always using fine-tunning and building a baseline model which we will try to improve later.As always, I hope you enjoyed the post, and that you gained an intuition about how to implement and develop a convolutional neural network with transfer learning and fine-tuning!If you liked this post then you can take a look at my other posts on Data Science and Machine Learning here.If you want to learn more about Machine Learning, Data Science and Artificial Intelligence follow me on Medium, and stay tuned for my next posts!",27/03/2020,10,28,6,"(537, 401)",16,3,0.0,5,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
16,Deep Learning and Visual Question Answering,Towards Data Science,franky,404.0,3.0,343,"Visual Question Answering is a research area about building a computer system to answer questions presented in an image and a natural language. First of all, let’s examine three datasets in Visual Question Answering.In VQA Dataset from www.visualqa.org, the computer system needs to address issues, such as, a binary classification problem (Is the umbrella upside down?), a counting problem (How many children are in the bed?), or an open-ended question (Who is wearing glasses? Where is the child setting?)In CLEVR Dataset from Stanford, the computer system needs to answer questions about the shape/color/size/material of the objects, and its spatial/logical relationship.In FigureQA Dataset from Maluuba, the computer system needs to answer questions presented by bar charts, pie charts, or line plots.Because Visual Question Answering requires techniques involving image recognition and natural language processing, one major direction in research is on Deep Learning: using Convolutional Neural Network (CNN) for image recognition, using Recurrent Neural Network (RNN) for natural language processing, then combining the results to deliver the final answer as shown in Figure 4.Keras presents a generic model for Visual Question Answering as shown in Figure 5.One interesting and important idea in the area of Visual Question Answering is Relation Network presented by DeepMind [1,2]。The major goal of Relation Network is to explore the spatial relation or the logical relation among objects presented in the image and the question, such as, “… the same size as …” in the question of Figure 6 and “… is left of …” in the question of Figure 7.Figure 7 illustrates the architecture of relation network inside a Visual Question Answering system. Note that the relation network might explores the relationship in object-to-object-based or in feature-to-feature-based. Figure 8 shows a simple implementation about feature extraction and relation extraction in Keras/Theano.Visual Question Answering is an interesting challenge combing differenet disciplines, including computer vision, natural language understanding, and deep learning. Hopefully we could see more articles in this area under Medium.[1] VQA Dataset[2] CLEVR Dataset[3] FigureQA Dataset[4] Keras VQA Model[5] Relation Network from DeepMind[6] AI Progress Measurement on Visual Question Answering",13/02/2018,0,46,3,"(700, 374)",6,1,0.0,17,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,surprise/amazement
17,TensorFlow 2 Object Detection API With Google Colab,The Startup,Nisarg Kapkar,40.0,9.0,974,"UPDATE:Thank you all for your amazing support. Recently hit almost 35k views on this blog!In this tutorial, we will use Google Colab (for model training) and Google Drive (for storage).Colab is a free Jupyter NoteBook environment hosted by Google that runs on the cloud. Google Colab provides free access to GPUs (Graphical Processing Units) and TPUs (Tensor Processing Units).You can read more about Google Colab on their Intro and FAQ page.NOTE:Sessions on Google Colab are 12 hours long. After 12 hours everything on Colab storage is wiped out (Notebooks will also disconnect from Virtual Machines if they are left idle for too long). So, it is advisable to use Google Drive for storage rather than using Colab’s storage.Considering that you know the basics of Colab, let’s start with our Object Recognition Model!We need to provide properly labeled images to the Object Detection API. These images will be used to train our model.The first step is to gather images for all the objects you want your model to classify. You can collect images from the internet, or use some public datasets. You can search for public datasets using Google’s Dataset Search.Next, we need to label all the desired objects in the collected images. LabelImg is a superb tool for annotating images. You can find the installation and usage instructions on its GitHub page. (skip this step if you are using a public dataset and you already have labeled images)After labeling, divide the dataset into two parts- train (80% of images with their corresponding XML files) and test (remaining 20% of images with their corresponding XML files).For this tutorial, I am using Fruit Image for Object Detection Dataset from Kaggle. The database already contains labeled images divided into two sets (train and test).A label_map maps each class(label) to an int value. label_map file should have the extension as .pbtxt.Below is the label_map file for the Fruit Detection dataset:Similarly, you must make a label_map.pbtxt file for your dataset.We will use pre-trained models provided by TensorFlow for training.Download any per-trained model of your choice from the TensorFlow 2 Detection Model Zoo. (just click on the name of the model you want to use to start the download)For this tutorial, I am using the SSD Resnet50 V1 FPN 640X640 model.This script(generate_tfrecords.py) will be used to covert the annotations into the TFRecord format. Download the script from here.Huge thanks to Lyudmil Vladimirov for allowing me to use some of the content from their amazing TensorFlow 2 Object Detection API Tutorial for Local Machines!Go to your Google Drive and make a new folder named “TensorFlow”.Make a directory structure in your TensorFlow folder as shown below.(You can give names of your choice to folders. If you are using different names, change all the paths in Jupyter NoteBook according to your folder names)We will now add all the collected files (from Step 1) to their respective directories.After uploading all the files, this is how your directory structure should look like: (new files and folders highlighted in bold)We will now do most of the steps on Google Colab.I have made a Notebook containing all the steps and relevant codes. (Run the cell with a particular step number to execute that step)You can download the NoteBook from my GitHub Repository.Open Colab and load the downloaded Notebook.On Colab, go to Runtime→Change Runtime Type and select Hardware accelerator as GPU.NOTE:If you have given different names to your folders and files, don’t forget to change the paths in cells according to your files and folder in Colab Notebook!You will be given a URL and you will be asked to enter an authentication code to mount your google drive.You should now have a new folder named ‘models’ in your TensorFlow directory!NOTE:Some steps in the tutorial are not compatible with the latest version of TensorFlow 2. So, we will be using an older version of the repository for this tutorial (Date of older version: 21st Sept 2020)If all the installations were successful, you should see output similar to the one shown below.You should now have two new files “test.record” and “train.record” in ‘workspace/training_demo/annotations’ folder.Go to ‘training_demo/models/my_ssd_resnet50_v1_fpn’. (or the folder you have created for the downloaded model in your ‘training_demo/models’ directory)Open the pipeline.config file. (you can open a file in Colab by simply double-clicking it)Change the lines shown below according to your dataset. (set paths according to your folders name and downloaded pre-trained-model)TensorBoard allows you to track and visualize various training metrics while training is ongoing.You can read more about TensorBoard here.Initially, you will get a message saying “No dashboards are active for the current data set”.But once the training start, you will see various training metrics.Once your model training starts, you should see output similar to one shown below:You can see various training parameters/metrics (like classification_loss, total_loss,learning_rate…) in your TensorBoard. The training log displays loss once after every 100 steps.Training time depends on several factors, such as batch_size, the complexity of objects, hyper-parameters, etc; so be patient and don’t cancel the process.A new checkpoint file is saved every 1000 steps. (These checkpoints can be used to restore training progress and continue model training)It is advisable to train the model until the loss is constantly below 0.3! If you do not achieve good results, you can continue training the model (the checkpoints will allow you to restore training progress) until you get satisfactory results!We have finished training our model, it’s time to extract our saved_model. This saved_model will be used to perform object recognition.You should now have a new folder named ‘my_model’ inside your ‘training_demo/exported-models’ directory. This folder contains our saved_model.Now it’s time to test our trained model!If everything is successful, you should see your loaded images with bounding boxes, labels, and accuracy!Huge Thanks to Lyudmil Vladimirov for allowing me to use some of the content from their amazing TensorFlow 2 Object Detection API for Local Machines!Link to their GitHub Repository.",21/09/2020,21,87,20,"(395, 329)",4,7,0.0,20,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
18,Train GPT-2 in your own language,Towards Data Science,Arshabhi Kayal,35.0,6.0,678,"We all know modern day Natural Language Processing (NLP) has progressed by leaps and bounds in the past couple of years following the development of attention networks and transformers. It paved the way for a plethora of new algorithms achieving State-Of-The-Art (SOTA) for the different tasks of NLP.OpenAI has been one of the leaders in providing their own language model (now released GPT-3) which is trained on a huge corpus of internet data. Since, GPT-3 is a recent phenomenon and in English at the moment, and is only accessible through API given by OpenAI, we shift our focus on the earlier version of it i.e. GPT-2. To know about the internal nuts and bolts of GPT-2, I’d suggest you to go through this link. For more depths into Attention and Transformers, here are some excellent links:GPT-2 was also released for English, which makes it difficult for someone trying to generate text in a different language.So why not train your own GPT-2 model on your favourite language for text generation? That is exactly what we are going to do. So, without further ado, let us jump in.For the demo, I have considered a non-Latin alphabet script (Bengali here), because why not!! I have used Huggingface’s implementation for the model.Gathering good quality data is one of the most important stages as all Data Scientists would agree. So, we are going to assume that you already have a folder containing .txt files having all the data cleaned and stored. For ease, you can use the Wikipedia article data, which is available and can be downloaded with the following code.This will create a folder containing all Wikipedia files looking like:Note: due to resource constraint, and since it is for demo purpose, I have trained the model in a small subset of books by Satyajit Ray, especially his detective Feluda series.Now, the second step will be to tokenize the data. For that, we use the following class.So what we do here is tokenize our data and save it in a folder. Two files will be created (merges.txt and vocab.json) in a specified directory. To run the file, use the following code:Before the real magic begins, we need to make sure the artilleries are ready. Let us start with some initializations.We also create a single string from all our documents and tokenize it.After we have encoded the whole string, we now move on to make a TensorFlow dataset, slicing the data into equal intervals, so that our model can learn. Here we use a block size of 100 (length of token in each example) and a batch size of 16. This is kept low else we can run it with ease on a RTX 2060 GPU.Now comes the part we’ve been waiting for, making the model and training. So we define our optimizer, loss functions and the metrics, and start training.Now, let’s train the modelTo predict, we just need to simply encode the input text and pass it to the modelNow, if you are a Bengali, then you can point it out that the output although the sentence is syntactically correct, it doesn’t look cohesive. True, but for this demo, I have kept this demo a minimal as possible.Well, after long training time, what good will it do if we close our session and all our trained model is just lost and we again need to train it from scratch. So, let’s save the model and the tokenizer so that we can retrain from where we left offWe have already done all the hard work, so to load the saved model and tokenizer, we only need to execute two lines of code and we’re all set.Voila! Now you can train your own model in your own language. And create content which can race with some of the best literary works in any language.This blog gives a framework of how can one train GPT-2 model in any language. This is not at par with some of the pre-trained model available, but to reach that state, we need a lot of training data and computational power.huggingface.cohuggingface.co",25/08/2020,10,3,2,"(630, 385)",3,2,0.0,13,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
19,How To Build a GPT-3 Chatbot with Python,,John Mannelly,,20.0,3716,"We are going to create a messaging Chatbot that is driven by OpenAIs GPT3. This is the closest thing I have seen to a bot having consciousness and we are going to do it in less than 50 lines of code!This is the written tutorial but if you want to watch the video tutorial on Youtube click here.We are going to be doing this in the Python and use the Flask frameworkWe will be writing our code in the VS Code editorWe will use the GitHub desktop appWe will be using an API key from OpenAI to access to GPT3We will be using Twilio to handle the messaging serviceAnd finally we will use Render to run our chatbotI am not a GitHub maestro but I know enough to get around. We are going to install specific python packages and the best way to do this is with a virtual environmentA virtual environment is a copy of the Python interpreter into which you can install packages privately, without affecting the global Python interpreter installed in your system. Your future self will thank you for using a virtual environment whenever possible. It helps to keep code contained and make it more replicable since all the dependencies and site packages are in one place. People set up virtual environments numerous ways, but here are the commands I follow:Create a new project folder. We are going to call ours:Change into the new gpt3-jabebot directory we just created.We are going to call our virtual environment venv. The -m venv option runs the venv package from the standard library as a standalone script, passing the desired name as an argument.Again, you’ll see around the internet that most people use ‘venv’ as the virtual environment folder but feel free to name it whatever. Make sure your current directory is set to gpt3-chatbot and run this command.After the command completes, you’ll have a subdirectory with the name venv inside gpt3-jabebot.Now we need to activate the virtual environment. To do so, run the following command:Great. Now we are all set up. Make sure you always activate your virtual environment before making any changes to our code, or else you will run into some errors.Again, the beauty of the virtual environment is that we can install all packages and dependencies in one place, making it easy to share and update. We will use the pip command to get the packages we want.Downloading a package is very easy with pip. Simply go to your terminal and make sure you are in your virtual environment, then tell pip to download the package you want by typing the following:Let’s run through all the packages we need to install:Open the GitHub desktop app and in the menu bar at the top you should see the option to create a ‘New Repository’ under fileFrom there we will give it a name and then use the option to open it in VSCode. Let’s call it gpt3-chatbot. Before you hit create repository, make sure to add a python gitignore file. This will prevent us from uploading our virtual environment to Github as well as our .env file that contains our super secret API access tokens.After you launch the Desktop app you will see the option to open the repository in an external editor. If you have Visual Studio Code installed, it should be the default option. Go ahead and click that button.Great. We are nearly ready to start writing our code!In order to access GPT3 and all of it’s awesomeness, you need an API key from OpenAI. I magically obtained one via a Tweet at Greg Brockman (@gdb) who was kind enough to hand out beta invites after the launch. You can now apply on their site although I am unsure how long it takes to get accepted.Fast forward to when you’ve been accepted and you will want to copy your secret API key within your Account Settings.Our chatbot application will need to reference this API key so we need to add it to a new .env file.The reason we are hiding it behind an .env file is because it’s a secret key and we don’t want anyone stealing it and using it for evil which we will have to pay for. By putting it in an .env file (short form environment) we can import it as a variable. Note that the .env file is included in our default gitignore file we created.Create an .env file in your project directory (note the leading dot) and add this line of code, but replace your-super-secret-api-key with your actual API key.I will say it one more time, MAKE SURE YOU DO NOT EXPOSE YOUR SECRET API KEY TO THE PUBLIC.Now that we have the skeleton of our project setup, we are going to give it a brain. We will be using GPT3 for this.I am not going to spend a lot of time going over exactly what GPT3 is or how it works. Partially because I still don’t understand it and mostly because there is a ton of literature out there if you want to learn more. I will pull some excerpts from the OG Twilio blog post to help paint the picture.“GPT-3 (Generative Pre-trained Transformer 3) is a highly advanced language model trained on a very large corpus of text. In spite of its internal complexity, it is surprisingly simple to operate: you feed it some text, and the model generates some more, following a similar style and structure.”“GPT-3 is non-deterministic, in the sense that given the same input, multiple runs of the engine will return different responses.”**For more information I recommend reading the following: The Ultimate Guide to OpenAI’s GPT-3 Language Model**The OpenAI playground allows us to explore GPT3 and all its intricacies. The general idea behind everything is that you “train” (aka prime) the GPT3 engine by giving examples for it to learn from. With just an example or two, GPT3 will fill in the blanks and basically mimic what you have taught it.The main text area is where we provide the text example inputs. The right sidebar is where we modify variables to change the desired text output.Pretty neat right!? So let’s take a moment to look at what’s happening on the right sidebar which will be driving the responses we get in the Playground. I’ll again reference highlights from Twilios Ultimate Guide (linked again conveniently right here) with a touch of my own wording to help you digest it.Engine: OpenAI has four engines to choose from. This is definitely the black box part of GPT3. I have read that Davinci is the most “advanced and capable” so we will stick with it per recommendations across the interwebs.Response Length: Controls how much text is generated. Think character count here for all you Microsoft Word or Google Doc users. If we set it at 150 that means that GPT-3 will add 150 tokens to the text. A token is defined as a word or a punctuation mark.Temperature: This setting controls the randomness of the generated text. The higher the temperature the crazier what gets spit out. A value of 0 makes the engine deterministic, which means that it will always generate the same output for a given input. A value of 1 makes the engine take the most risks aka makes it the most creative.Top P: This parameter also has some control over the randomness and creativity of the text generated by GPT3. For some reason it is used less than the Temperature. The OpenAI documentation recommends that only one of Temperature and Top P are used, so when using one of them, make sure that the other is set to 1.Frequency penalty: Frequency penalty works by lowering the chances of a word being selected again the more times that word has already been used.Presence Penalty: Presence penalty does not consider how frequently a word has been used, but just if the word exists in the text. This helps to make it less repetitive and seem more natural.Per the Twilio Ultimate Guide, “the difference between these two options is subtle, but you can think of Frequency Penalty as a way to prevent word repetitions, and Presence Penalty as a way to prevent topic repetitions.”Best Of: can be used to have GPT-3 generate multiple responses to a query. The Playground then selects the best one and displays it. Recommend going with defaults here.Stop Sequence: helps to prevent GPT3 from cutting off mid-sentence if it runs up against the max length permitted by the response length parameter. The stop sequence basically forces GPT3 to stop at a certain point. The returned text will not contain the stop sequence.Start Text: Text to automatically append after the user’s input. This will happen before sending a request to GPT3 which will come in handy when we are building our bot.Restart Text: Text to append after the models generation to continue the patterned structure. In other words, the restart text helps so you don’t need to type the prefix.As always, the best way to truly learn what each of these things do is to experiment with them. Change the values and see what happens to your text output in the Playground.For our chatbot, we are going to give it an identity to start before adding in any Q&A examples. To do this we will add a short text description of our bot at the top. The cool thing is we can reference known people who exist on the internet because GPT3 is trained on a “corpus” of information which basically means all of the recorded internet.After the bot’s identity description, we will write a handful of questions and answers to train the GPT3 engine to follow the Q&A format. We are following this format specifically since we are creating a chatbot which will respond to questions (and comments).This was modified directly from the Q&A preset from OpenAI. The amazing thing about GPT3 is that we only need a few examples to give our bot life.What we’re really doing here is helping to prompt GPT3 with certain prefixes. The keyword being “prompt.”Notice how for each question we start with Person: and for each answer we start with Jabe:. The GPT3 engine can understand that this is a conversation between a person and the bot, in our case named Jabe.Our start text is a return plus Jabe: meaning we want the engine to stop generating output once the bot is done answering the question. Our restart text is [enter], [enter], Person: which means the bot is awaiting an input from us before generating an output. This will be key when we are reading in text messages via Twilio later on.The bold text in the playground is what GPT3 will use as in input. Again, we created multiple Q&A examples to teach GPT3 what type of text we want it to generate when prompted. The reason we ended a question from Person: is because this is what provides GPT3 the cue that it needs to generate an answer to complete the conversation that matches the examples above.If we hit Submit, we will see our chatbot’s response!Now that we have a vague understanding of how everything works, we can export the Playground code easily so we can convert it to python. To do this, click the < > Export Code button.Copy this and paste it into a new file called jabebot.py. You can obviously name this anything else you want if your bot is not named Jabe, but just pay attention to anywhere I reference jabebot in the code and be sure to update it accordingly.We’re ready to start using the OpenAI’s API to generate GPT3 responses in python.To kick things off we have to do some initialization in the recently created jabebot.py file. This is where we will reference our API key to tell OpenAI that it is us making the request and to not worry.You’ll notice that we are referencing the previously downloaded dotenv package which we installed with pip at the start of the tutorial. This will allow us to reference environment variables stored in our .env file so no one can copy them when we publish this to a private (or public) github repo.We will also import openai package which we installed and store this in a completion object. This object is what we will use to send queries back and forth to the GPT3 API. Please also import Flask which we will get to later when we need to connect to Twilio.We can copy the start_sequence and restart_sequence directly from our code we exported from the Playground. Let’s also add a session_prompt variable and set it equal to the text we wrote earlier in our playground. These are going to be global variables which is why they are referenced outside of any functions. We use them to teach our bot how to speak and answer questions.The next part will focus on going back and forth with our bot. It is the follow up after the prompting we did above. We will create a new ask() function which takes in two arguments. The first being the actual question. This one is mandatory and will be the text input from a person on the other end of the chat. The second argument being the chat_log which will be optional. The chat_log is meant to keep a running list of everything that was said in the conversation so the bot has some context. To start we can set it equal to None.We also need to add the response variable which makes a request to the GPT3 engine via the completion.create() function that OpenAI provides us. This is where we will package up various arguments to tell GPT3 how creative or crazy to be and how many tokens to use. The API will return a response which we conveniently set equal to a response variable that we can parse and make use of.Between the open parenthesis for the object, we can add in the arguments which we previously exported from the Playground. A quick recap of what each one does, brought to you by this similar blog post from Twilio:Another change we want to make to the ask function is to create a new prompt_text variable and then add the following line:The f’ makes this a string variable which basically combines all of the history of the chatlot and then the restart sequence, question and start sequence that is needed to prompt GPT3. We will need to add in an argument to our code where we set the prompt = prompt_text. That means our code should now look like thisThe finishing touches we need to add to the ask function is to actually take in the GPT3 response, which will come as a nested JSON, get the text we care about and return the text so our bot can text it.OpenAIs API documentation tells us that the Create Completion object that we referenced “Returns the predicted completion for the given prompt, and can also return the probabilities of alternative tokens at each position if requested.”The part of the response that we want from the Request Body is nested within choices and keyed on “text.” We can access this with the following line which we will story in a story variable:Every good python function needs a good return statement. So let’s add a return which converts our story to a string for good measure. Our final ask function should look like this:The missing piece here is building out the functionality for our bot to add to and reference previous messages via the chat_log. We will do this with another function which we can call append_interaction_to_chat_log() and it will take three arguments: question, answer and chat_log. Each time we will check if there is already a chat_log and if the answer is no (aka = None) then we will use the session_prompt to get us started. If there is a previous chat_log from our bot, then we will reference it before the restart_sequence and before the person texts their question input. We will also need to add the start_sequence to connect the answer that we get from the API response. All in all our function will look like this:At this point we have everything we need to speak with the GPT3 bot. The next section will focus on routing the messages via Flask so we can send and receive them via text messages on Twilio.To empower our bot to respond in real time via Twilio SMS API, we need to use a webhook. This webhook is what will tell Twilio there is an incoming message. We will format it using Flask, a python framework. Flask makes it quick and easy to define an endpoint with a URL that can be used as a webhook. We will configure Twilio to let it know about our Flask endpoint and then it can start receiving incoming text messages which can be routed to GPT3 and eventually sent back via a Twilio response.Create a new app.py file in our root directory and copy and paste this code:As I mentioned before, if you call your bot a different name, pay close attention to the jabe() function and the /jabebot endpoint. Also, if your bot ever starts to act up, be sure to generate a new random string and replace the ‘SECRET_KEY’ config.What we are doing within the jabe() function is just calling both the ask and append_interaction_to_chat_log functions that we created previously in close succession. You’ll see we are importing each of these functions at the top, along with Flask and the Twilio messaging functions.Flask provides us help with the request.values special object that takes incoming data and exposes it in a dictionary format. We will take the Body parameter and save it as an incoming_msg variable which serves as the question argument in our ask() function. We are also going to use Flasks session to get any chat_logs that previously existed for this user so GPT3 can remember it and blow their mind!Once we have the incoming_msg (the question that was texted via SMS) and the chat_log, we can pass it through our ask() function and save it to an answer variable. We can then pass that answer variable into the append_interaction_to_chat_log() function along with the other arguments it required. We will save this as a new output for the session chat_log which we store with the session[‘chat_log’] dictionary format.From here, we sprinkle on some Twilio Messaging magic and return a string with our message. Voila! You just built a Flask app. Again, this app will route through the/jabebot webhook and then use the functions we defined to return an answer which we will format into a proper string for Twilio to send as a text message via SMS.The final few parts of this tutorial are going to jump around a bit but it will make sense at the end. Great job making it this far :)Time to fire back up our GitHub desktop app. We’re ready to commit all the code we’ve written so far.Before doing this, let’s also add a requirements.txt file you haven’t already. We can run this command and it will take all of the packages being used in our virtual environment and put it in a nix txt file so Render and Github can make use of it. Make sure you are in your gpt3-jabebot directory and run:First thing we need to do is publish our repository to Github. Give it a name and a description and publish away.After that we are finally ready to commit our code. Give your future self a nice reminder on what you did for this commit just in case anything goes wrong or you need to reference at a later date.While we’ve been working within VS Code, we’ve actually been working on a GitHub branch. Since this was a solo project and a rather small one, we’ve been on the main branch. What we want to do now is “push” all of our files and code to our GitHub repo.Go ahead and open up your profile on Github and you should see your repo there!This is where a lot of folks would instruct you to use AWS or a new and improved Heroku but we are going to go with Render because I found it to be super easy. Somewhat costly, but easy.First thing you need to do is sign up for a free Render account. Take 5 to do that.Welcome back. At the top of the page, clicked the New + button and select a “Web Service.” You’ll be taken to a screen where you can connect your GitHub account via authentication. Go ahead and give Render access to the recent repository we just created.Render has solid documentation for how to deploy a Flask app. We need to give it a name, tell it we are using a Python 3 environment, give a region nearby, instruct it to use the master branch, use the following for a Build Command pip install -r requirements.txt and use the following for a Start Command gunicorn app:app.Render uses Gunicorn to serve our app in a production setting. Since we used the generic app.py naming we can just reference the app:app.From there we can header to our dashboard and Manually Deploy our first build. On this screen we also need to grab the URL for our hosted webhook which we will use to configure Twilio properly.Make sure you see a successful build but just know our bot won’t come alive until we handle the final Twilio puzzle piece.Step 1: is to sign up for Twilio.Step 2: is to buy a phone number.Step 3: is to add a small balance to your numberOnce you’ve completed those steps, meet me at the Active Numbers Dashboard. Here is where you will configure Twilio to use the webhook and paste the webhook URL we just got from Render. Make sure to select HTTP POST since our Flask app has POST functionality.After you’ve done this, you are ready to whip out your smartphone and start texting your bot. Feel free to share it with friends and be sure to have them grab screengrabs of anything funny!I hope you enjoyed this written tutorial. You can find more videos like this on my Learn With Jabe Youtube channel.",28/02/2021,17,28,8,"(700, 399)",20,3,0.0,16,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
20,The Evolution of Deeplab for Semantic Segmentation,Towards Data Science,Beeren Sahu,158.0,7.0,1318,"In computer vision, a simple image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels). Image segmentation is a long standing computer Vision problem. However, semantic segmentation is the technique of segmenting image with “understanding” of image in pixel level. In other words, semantic segmentation is analysis and classification of each pixel into multiple classes (labels).There are many applications of semantic segmentation that include autonomous driving, human-machine interaction, computational photography, image search engines, and augmented reality to name a few.Quite a few algorithms have been designed to solve this task, such as the Watershed algorithm, Image thresholding, K-means clustering, Graph partitioning methods, etc. The simplest method would be the thresholding method. In this method a gray-scale image is converted into a binary image based on a threshold value. In spite of many traditional image processing techniques however the deep learning methods has been the game changer.In order to properly understand how semantic segmentation is tackled by modern deep learning architectures, it is important to know that it is not an isolated field. Rather it is a natural step in the progression from coarse to fine inference.The origin could be located at classification, which consists of making a prediction for a whole input, i.e., predicting the object in an image. The next step towards fine-grained inference is localization or detection, providing not only the classes but also additional information regarding the spatial location of those classes. A semantic segmentation can be seen as a dense-prediction task. In dense prediction, the objective is to generate an output map of the same size as that of the input image. Now, it is obvious that semantic segmentation is the natural step to achieve fine-grained inference. Its goal is to make dense predictions inferring labels for every pixel. This way, each pixel is labeled with the class of its enclosing object or region. Further improvements can be made, such as instance segmentation (separate labels for different instances of the same class).A general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network:One of the very early Deep Convolutional Neural Networks (DCNNs) used for semantic segmentation is Fully Convolutional network (FCN). The FCN network pipeline is an extension of the classical CNN. The main idea is to make the classical CNN take as input arbitrary-sized images. The restriction of CNNs to accept and produce labels only for specific sized inputs comes from the fully-connected layers which are fixed. Contrary to them, FCNs only have convolutional and pooling layers which give them the ability to make predictions on arbitrary-sized inputs.One issue in this specific FCN is that by propagating through several alternated convolutional and pooling layers, the resolution of the output feature maps is down sampled. Therefore, the direct predictions of FCN are typically in low resolution, resulting in relatively fuzzy object boundaries. A variety of more advanced FCN-based approaches have been proposed to address this issue, including SegNet, UNet, DeepLab, and Dilated Convolutions.In the following section we will discuss the Deeplab for semantic segmentation and its evolution.DeepLab is a state-of-the-art semantic segmentation model designed and open-sourced by Google. The dense prediction is achieved by simply up-sampling the output of the last convolution layer and computing pixel-wise loss. The Deeplab applies atrous convolution for up-sample.Atrous ConvolutionThe repeated combination of max-pooling and striding at consecutive layers in DCCN reduces significantly the spatial resolution of the resulting feature maps. One solution is to use deconvolution layers to up-sample the resulting map. However, it requires additional memory and time. Atrous convolution offers a simple yet powerful alternative to using deconvolutional. Atrous convolution allows to effectively enlarge the field of view of filters without increasing the number of parameters or the amount of computation.Atrous convolution as a shorthand for convolution with up-sampled filters. Filter up-sampling amounts to inserting holes (‘trous’ in French) between nonzero filter taps.Mathematically, atrous convolution y[i] for a one-dimensional signals x[i] with a filter w[k] of length K and stride rate r is defined as:Success of Deeplabv1 in the task of semantic segmentation is due to some advancements added to the previous state of the art models, specifically to the FCN model. These advancements addresses the following two challenges.Challenge 1: reduced feature resolutionDue to multiple pooling and down-sampling (‘stride’) in DCNN, there is a significant reduction in spatial resolution. They remove the down-sampling operator from the last few max-pooling layers of DCNNs and instead up-sample the filters (atrous) in subsequent convolutional layers, resulting in feature maps computed at a higher sampling rate.Challenge 2: reduced localization accuracy due to DCNN invarianceIn order to capture fine details by employing a fully connected Conditional Random Field (CRF). The CRF potentials incorporate smoothness terms that maximize label agreement between similar pixels, and can integrate more elaborate terms that model contextual relationships between object classes. Following figure illustrates the improvement of segmentation map after few iterations of CRF.The Deeplabv1 model takes the images as input and passes through usual DCCN layers followed by one or two atrous layer and results in a coarse score map. This map is then up-sampled to the original size of the image, using bi-linear interpolation. Finally, to improve the segmentation result fully connected CRF is applied.To further improve the performance of the Deeplabv1 architecture, the next challenge is existence of object at multiple scales.Challenge: existence of objects at multiple scalesTo represent the object in multiple scales, a standard way to deal with this is to present to the DCNN re-scaled versions of the same image and then aggregate the feature or score maps.Solution: Use of Atrous Spatial Pyramid Pooling (ASPP). The idea is to apply multiple atrous convolution with different sampling rates to the input feature map, and fuse together. As objects of the same class can have different scales in the image, ASPP helps to account for different object scales which can improve the accuracy.The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations (atrous convolution) at multiple rates and multiple effective fields-of-view (ASPP). The next challenge was to capture sharper object boundaries by gradually recovering the spatial information.Challenge: capture sharper object boundariesDeeplabv3 architecture adopts a novel encoder-decoder with atrous separable convolution to address the above issue. The encoder-decoder model is able to obtain sharp object boundaries. The general encoder-decoder networks have been successfully applied to many computer vision tasks, including object detection, human pose estimation, and also semantic segmentation.Typically, the encoder-decoder networks contain:In addition to the above encoder-decoder network, it also applies depth-wise separable convolution to increases computational efficiency. This is achieved by factorizing a standard convolution into a depth-wise convolution followed by a point-wise convolution (i.e., 1 × 1 convolution). Specifically, the depth-wise convolution performs a spatial convolution independently for each input channel, while the point-wise convolution is employed to combine the output from the depth-wise convolution.This, extends DeepLabv3 by adding a simple yet effective decoder module to further refine the segmentation results especially along object boundaries.Encoder: Compared to Deeplabv3, it uses Aligned Xception instead of ResNet-101 as its main feature extractor (encoder), but with a significant modification. All max pooling operations are replaced by depth-wise separable convolution.Decoder: The encoder is based on an output stride of 16, i.e. the input image is down-sampled by a factor of 16. So, instead of using bilinear up-sampling with a factor of 16, the encoded features are first up-sampled by a factor of 4 and concatenated with corresponding low level features from the encoder module having the same spatial dimensions. Before concatenating, 1 x 1 convolutions are applied on the low level features to reduce the number of channels. After concatenation, a few 3 x 3 convolutions are applied and the features are up-sampled by a factor of 4. This gives the output of the same size as that of the input image.Hope you enjoyed reading!!!Cover Photo by Suzanne D. Williams on Unsplash",12/07/2019,0,15,15,"(620, 301)",11,2,0.0,14,it,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,contempt/reluctance
21,A Quick Introduction to Text Summarization in Machine Learning,Towards Data Science,Dr. Michael J. Garbade,2900.0,3.0,619,"Text summarization refers to the technique of shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document.Automatic text summarization is a common problem in machine learning and natural language processing (NLP).Skyhoshi, who is a U.S.-based machine learning expert with 13 years of experience and currently teaches people his skills, says that “the technique has proved to be critical in quickly and accurately summarizing voluminous texts, something which could be expensive and time consuming if done without machines.”Machine learning models are usually trained to understand documents and distill the useful information before outputting the required summarized texts.Propelled by the modern technological innovations, data is to this century what oil was to the previous one. Today, our world is parachuted by the gathering and dissemination of huge amounts of data.In fact, the International Data Corporation (IDC) projects that the total amount of digital data circulating annually around the world would sprout from 4.4 zettabytes in 2013 to hit 180 zettabytes in 2025. That’s a lot of data!With such a big amount of data circulating in the digital space, there is need to develop machine learning algorithms that can automatically shorten longer texts and deliver accurate summaries that can fluently pass the intended messages.Furthermore, applying text summarization reduces reading time, accelerates the process of researching for information, and increases the amount of information that can fit in an area.There are two main types of how to summarize text in NLP:The extractive text summarization technique involves pulling keyphrases from the source document and combining them to make a summary. The extraction is made according to the defined metric without making any changes to the texts.Here is an example:Source text: Joseph and Mary rode on a donkey to attend the annual event in Jerusalem. In the city, Mary gave birth to a child named Jesus.Extractive summary: Joseph and Mary attend event Jerusalem. Mary birth Jesus.As you can see above, the words in bold have been extracted and joined to create a summary — although sometimes the summary can be grammatically strange.The abstraction technique entails paraphrasing and shortening parts of the source document. When abstraction is applied for text summarization in deep learning problems, it can overcome the grammar inconsistencies of the extractive method.The abstractive text summarization algorithms create new phrases and sentences that relay the most useful information from the original text — just like humans do.Therefore, abstraction performs better than extraction. However, the text summarization algorithms required to do abstraction are more difficult to develop; that’s why the use of extraction is still popular.Here is an example:Abstractive summary: Joseph and Mary came to Jerusalem where Jesus was born.Usually, text summarization in NLP is treated as a supervised machine learning problem (where future outcomes are predicted based on provided data).Typically, here is how using the extraction-based approach to summarize texts can work:1. Introduce a method to extract the merited keyphrases from the source document. For example, you can use part-of-speech tagging, words sequences, or other linguistic patterns to identify the keyphrases.2. Gather text documents with positively-labeled keyphrases. The keyphrases should be compatible to the stipulated extraction technique. To increase accuracy, you can also create negatively-labeled keyphrases.3. Train a binary machine learning classifier to make the text summarization. Some of the features you can use include:4. Finally, in the test phrase, create all the keyphrase words and sentences and carry out classification for them.Text summarization is an interesting machine learning field that is increasingly gaining traction. As research in this area continues, we can expect to see breakthroughs that will assist in fluently and accurately shortening long text documents.What are your thoughts about this this exciting field?Please share your comments below.",19/09/2018,0,12,16,"(615, 416)",1,3,0.0,3,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
22,Getting into data visualization — where should I start?,Data Visualization,Nick Brown,417.0,7.0,1286,"I love data — and I broadcast that fact pretty widely.If you’ve attended a party with me recently, I apologize for talking your ear off about data visualization tools for the web or the cool R package I was playing with recently.If you play fantasy sports with me, you’re welcome for the charts. So many charts.That has perhaps unsurprisingly led to me hearing this question more and more frequently:“Nick, I want to get into data analysis and visualization, where should I start?”Unfortunately there is no perfect one-size-fits-all solution — everyone’s needs are a little different, and what folks already know can vary widely. One of the things I love about the current technical/educational/business climate is that smart people from very different career paths and backgrounds are saying to themselves “I need to get more into data.”But amid those differences and after many conversations I have seen enough commonalities to put together what I think is a useful starter list. Obviously this isn’t for everyone. Are you an experienced engineer laughing at the idea of learning Javascript or Python for the first time? Already know D3.js and wondering whether to learn one of these or roll your own chart library on top of D3? This isn’t for you. This is for the academic scientist, school teacher, research consultant, project manager, funemployed guy or MBA grad (all recent examples) who wants to begin from closer to scratch.If that describes you, organized from “no coding” up through “I ♥ code”, this is where I think you should start.First, if you haven’t pushed Excel’s boundaries, it’s worth doing. Seriously. Learn pivot tables at least. It may sound lame, but Excel can do a lot more than people expect. It can even make pretty charts if you try hard enough.If you have some data already and just want a good tool to explore it visually or to export more compelling charts, Tableau is incredibly popular and powerful. There is a free public version and a very expensive paid version which you can get for free as a student. It can publish to the web, or to static graphics to include in research papers, post to Instagram or print out as giant wall-sized charts. The Tableau Public website has a lot of quality examples posted for you to get inspiration from.Sadly, the next “No coding” tool I like to recommend, Infoactive, is shutting down…but on the bright side it is because they were acquired by Tableau. This hopefully means good things for Tableau Public in the future. I will plug a free book spearheaded by the Infoactive team that is useful background on data visualization design using any of the tools I cover here:If I were picking one single programming language to use solely with data I would pick R. It’s free, supported by tons of ongoing development adding useful packages on top of the base language, and there are great free resources to learn it. First among those resources — I cannot recommend these Coursera classes highly enough:www.coursera.orgTaking all of them might be overkill for a true beginner, but the track of classes walks a nice line from the introduction of key data science terms and ideas, through exploratory data analysis (which covers useful packages for R like ggplot, a very popular visualization tool) all the way to adding interactivity, publishing to the web via Shiny and storytelling with data.R is what I use most frequently for small, quick analyses and ad hoc visualization — if you’ve got a dataset that Excel is struggling with (too big, not flexible enough, poor visualizations), R is perfect for exploring quickly.This is also the time for a quick “yes, you should probably learn some SQL.” SQL is very targeted in scope compared to R (really, it’s far from an apples to apples comparison)—but if there are databases that you need to dive into to gather data for use with any of these other tools or languages, there is a good chance you’ll want to know SQL, and it will pay dividends in the long run.More often than not, the question of “where should I start?” comes in response to a fantastic interactive visualization presented on the web. I’m a huge fan of all the recent innovation in this area (see my in-depth survey of innovative work here).Unfortunately, if you really love this piece:www.r2d3.us…it can be disheartening to find out how much you have to learn to be able to build your own. It’s worth reiterating up front that “being as good as the New York Times” is a tough goal. A worthy one, but tough.Fortunately, there are many great resources to help.The library behind the interactive piece above, and many of the data visualizations running in the browser today is D3.js, created by Mike Bostock. If you want to publish online or make interactives, D3.js is a great tool to learn. This does mean you’ll need to learn some Javascript in general and then D3.js specifically.Bostock’s website is a gold mine of examples and tutorials (you can’t beat learning from the creator of the library…). I’d also recommend Interactive Data Visualization for the Web by Scott Murray, which you can either buy from O’Reilly or work through for free online:This is a book about programming data visualizations for nonprogrammers. If you’re an artist or graphic designer with visual skills but no prior experience working with data or code, this book is for you. If you’re a journalist or researcher with lots of data but no prior experience working with visuals or code, this book is for you, too.The online version is excellent — you actually write code snippets within the book itself, run them, and compare your output to interactive examples that run within the book itself too. Murray also does a nice job of targeting the book at beginners, walking you through the basics of how web browsers work, HTML/CSS and Javascript, before diving headlong into the details of D3.One area to call out as a particular strength of D3 is geospatial visualizations. D3 is great at creating maps of many flavors, and there are nice dedicated tutorials available if that’s your area of focus:bost.ocks.orgD3 can be difficult to use directly, but there are many tools you can use on top of it to make your life a little easier. I’d recommend learning at least the basics of D3 rather than only using a more abstract plotting library, but if that proves intractable, a tool like Plot.ly can help make things feel more approachable.Finally, if you really want to learn a do-it-all programming language that just happens to be great at data visualization, go with Python. Python is the most general purpose and powerful tool of anything I’ve listed, and it’s quite popular in the data science community.I find Python very approachable as a multi-purpose programming language, but in truth it is probably overkill if all you want to do is explore and visualize data. Youtube is built with Python, for example…1 million lines of it. If you do go the Python route, the Code Academy course is a short (10–20 hours) and fun introduction to the language.Finally, much like D3.js for Javascript or ggplot for R, there are many Python libraries dedicated to data visualization. Seaborn (which builds on an older popular library, matplotlib) and Bokeh are probably the best-in-class right now, but this is a quickly evolving and improving landscape. Both the Seaborn and Bokeh websites include galleries showing off the kinds of visualizations you can create with those tools.Phew. That’s a lot. Have fun — and if you build something cool, send it my way! Find me anytime @uptownnickbrown on Twitter.Find more of my writing here on Medium:medium.commedium.comOr at http://uptownnickbrown.com/:uptownnickbrown.commind-sized-bites.firebaseapp.com",02/09/2015,0,0,1,"(562, 404)",4,0,0.0,28,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,expectation/interest
23,Stopping stepwise: Why stepwise selection is bad and what you should use instead,Towards Data Science,Peter Flom,1500.0,13.0,2584,"This is crossposted from my statistics site: www.StatisticalAnalysisConsulting.comIn this paper, I discuss variable selection methods for multiple linear regression with a single dependent variable y and a set of independent variablesaccording toIn particular, I discuss various stepwise methods (defined below). I show how they can be implemented in SAS (PROC GLMSELECT) and offer pointers to how they can be done in R and Python.Stepwise methods are also problematic for other types of regression, but we do not discuss these. The essential problems with stepwise methods have been admirably summarized by Frank Harrell (2001) in Regression ModelingStrategies, and can be paraphrased as follows:1. R^2 values are biased high2. The F statistics do not have the claimed distribution.3. The standard errors of the parameter estimates are too small.4. Consequently, the confidence intervals around the parameter estimates are too narrow.5. p-values are too low, due to multiple comparisons, and are difficult to correct.6. Parameter estimates are biased away from 0.7. Collinearity problems are exacerbated.This means that your parameter estimates are likely to be too far away from zero; your variance estimates for those parameter estimates are not correct either; so confidence intervals and hypothesis tests will be wrong; and there are no reasonable ways of correcting these problems.Most devastatingly, it allows the analyst not to think. Put in another way, for a data analyst to use stepwise methods is equivalent to telling his or her boss that his or her salary should be cut. One additional problem is that the methods may not identify sets of variables that fit well, even when such sets exist Miller (2002).I detail why these methods are poor, and suggest some better alternatives. In the remainder of this section, I discuss the SAS implementation of the stepwise methods. Next, I show these methods violate statistical theory; then I show that the theoretical violations have important practical consequences in commonly encountered situations. In the penultimate section I briefly discuss some better alternatives, including implementations SAS PROC GLMSELECT (with pointers to code in R and Python). I close by summarizing our results, making recommendations, and suggesting further readings.A variable selection method is a way of selecting a particular set of independent variables (IVs) for use in a regression model. This selection might be an attempt to find a ‘best’ model, or it might be an attempt to limit the number of IVs when there are too many potential IVs. There are a number of commonly used methods which I call stepwise techniques. These include• Forward selection begins with no variables selected (the null model). In the first step, it adds the most significant variable. At each subsequent step, it adds the most significant variable of those not in the model, until there are no variables that meet the criterion set by the user.• Backward selection begins with all the variables selected, and removes the least significant one at each step, until none meet the criterion.• Stepwise selection alternates between forward and backward, bringing in and removing variables that meet the criteria for entry or removal, until a stable set of variables is attained.• Bivariate screening starts by looking at all bivariate relationships with the DV, and includes any that are significant in a main model.SAS implements forward, backward, and stepwise selection in PROC REG with the SELECTION option on the MODEL statement. Default criteria are p = 0.5 for forward selection, p = 0.1 for backward selection, and both of these for stepwise selection. The criteria can be adjusted with the SLENTRY and SLSTAY options.The essential problem is that we are applying methods intended for one test to many tests. The F-test and all the other statistics generated by PROC GLM or PROC REG (or their equivalent in other programs) are based on a single hypothesis being tested. In stepwise regression, this assumption is grossly violated in ways that are difficult to determine. For example, if you toss a coin ten times and get ten heads, then you are pretty sure that something weird is going on. You can quantify exactly how unlikely such an event is, given that the probability of heads on any one toss is 0.5. If you have 10 people each toss a coin ten times, and one of them gets 10 heads, you are less suspicious, but you can still quantify the likelihood. But if you have a bunch of friends (you don’t count them) toss coins some number of times (they don’t tell you how many) and someone gets 10 heads in a row, you don’t even know howsuspicious to be. That’s stepwise.As a result of the violation of the assumption, the following can be shown to be true Harrell (2001):• Standard errors are biased toward 0• p-values also biased toward 0• Parameter estimates biased away from 0• Models too complexOne test of a technique is whether it works when all the assumptions are precisely met. We generate multivariate data for a that meets all the assumptions of linear regression1. e is normally distributed with mean 0 and constant variance.2. Linearity of relationship between IVs and DV. For our first example, we ran a regression with 100 subjects and 50 independent variables — all white noise. We used the defaults in SAS stepwise, which are a entry level and stay level of 0.15; in forward, an entry level of 0.50, and in backward a stay level of 0.10. The final stepwise model included 15 IVs, 5 of which were significant at p < .05. Forward selection yielded a final model with 29 IVs, 5 sig at p < .05. Backward selection yielded 10 IVs, 8 sig at p < .05.Of course, this violates the rule of thumb about how many subjects you should have for each IV. Therefore, for our second example we ran a similar test with 1000 subjects. Results were not encouraging: Stepwise led to 10 IVs with 5 significant at 0.05; forward to 28 IVs, with 5 significant at 0.05, and backward to 10 IVs, with 8 significant at 0.05. This is more or less what we would expect with those p values, but it does not give one much confidence in these methods’ abilities to detect signal and noise.Usually, when one does a regression, at least one of the independent variables is really related to the dependent variable, but there are others that are not related. For our third example we added one real relationship to the above models. However, since measurements contain noise, we also added noise to the model, so that the correlation of the ‘real’ IV with the DV was 0.32. Inthis case, with 100 subjects, 50 ‘false’ IVs, and one ‘real’ one, stepwise selection did not select the real one, but did select 14 false ones. Forward and backward both included the real variable, but forward also included 23 others. Backward did better, including only one false IV. When the number of subjects was increased to 1000, all methods included the real variable, but allalso included large numbers of false ones.That’s what happens when the assumptions aren’t violated. But sometimes there are problems. For our fourth example we added one outlier, to the example with 100 subjects, 50 false IVs and 1 real IV, the real IV was included, but the parameter estimate for that variable, which ought to have been 1, was 0.72. With two outliers (example 5), the parameter estimate wasreduced to 0.44.In this section I review some of the many alternatives to stepwise selection. First, I discuss methods that are not automatic,but that rely on judgement. Then I discuss some automatic methods. However, it is myview that no method can be sensibly applied in a truly automatic manner. The methods we discuss below perform better than stepwise, but their use is not a substitutefor substantive and statistical knowledge. A difficulty with evaluating different statistical methods of solving a problem (such as variable selection) is that, to be general, the evaluation should not rely on the particular issues related to a particular problem. However, in actually solving data analytic problems, these particularities are essential.One option that seems to often be neglected in research is leaving non-significant variables in the model. This is problematic in some cases, for example, if there are too many potential IVs, or if the IVs are collinear. However, there is nothing intrinsic in multiple regression that requires only significant IVs to be included. In fact, there are several reasons these IVs may be interesting despite their non-significance.The problem with this method is that adding variables to the regression equation increases the variance of the predicted values (see e.g. Miller (2002)) — this is the price paid for the decreased bias in the predicted values. This bias-variance tradeoff is central to the selection of a good method and a good model.Another excellent alternative that is often overlooked is using substantive knowledge to guide variable selection. Many researchers seem to believe that the statistical analysis should guide the research; this is rarely the case: Expert knowledge should guide the research. Indeed, this method ought not really be considered an alternative, but almost a prerequisite to good modeling.Although the amount of substantive theory varies by field, even the fields with the least theory must have some, or there would be no way to select variables, however tentatively.Space does not permit a full discussion of model averaging, but the central idea is to first develop a set of plausible models, specified independently of the sample data, and then obtain a plausibility index for each model. This index can, for example, be based on Akaike Information Criterion weights given byWhere the delta_i are differences in ordered AIC and K is the number of models. Then, these models are combined using:When one has too many variables, a standard data reduction technique is principal components analysis (PCA), and some have recommended PCA regression. This involves reducing the number of IVs by using the largest eigenvalues of X’X. There are two problems with this approach.• The principal components may have no sensible interpretation• The dependent variable may not be well predicted by the principal components, even though it would be well predicted by some other linear combination of the independent variables (Miller (2002)).Partial least squares finds linear combinations of the IVs that are related to the DV. One way of looking at this is to note that principal component regression is based on the spectral decomposition of X’X, partial least squares is based on the decomposition of X’Y’.The lasso is one of a class of shrinkage methods (perhaps the best-known shrinkage method is ridge regression). The lasso parameter estimates are given by Trevor Hastie & Friedman (2001) as:subject towhere- N is sample size- y_i are values of the dependent variable- b_0 is a constant, often parameterized to 0 by standardizing the predictors- x_(i j) are the values of the predictor variables- s is a shrinkage factorLeast Angle Regression was developed by Efron, Hastie, Johnstone & Tibshirani (2004). It begins by centering all the variables and scaling the covariates. Initially, all parameters are set to 0, and then parameters are added based on correlations with current residuals.Cross-validation is a resampling method, like the bootstrap or the jackknife, which takes yet another approach to model evaluation. When people talk about using hold-out samples, this is not really cross-validation. Cross-validation typically takes K replicate samples of the data, each one using (K-1)/K of the data to build the model and the remaining 1/K of the data to test the model in some way. This is called a K-fold cross-validation. For a sample of size N, leave-one-out-cross-validation, or LOOCV, acts a little like a jackknife structure, taking N-1 of the data points to build the model and testing the results against the remaining single data point, in N systematic replicates, with the kth point being dropped in the kth replicate. So this is theK-fold cross-validation taken to its extreme, with K=N. In addition, the random K-fold cross-validation does not split the data into a partition of K subsets, but takes K independent samples of size N*(K-1)/K instead.PROC GLMSELECT was introduced early in version 9, and is now standard in SAS. GLMSELECT has many features, and I will not discuss all of them; rather, I concentrate on the three that correspond to the methods just discussed.The GLMSELECT statement is as follows:The MODEL statement allows you to choose selection options including:• Forward• Backward• Stepwise• Lasso• LARand also allows you to select choose options:• The CHOOSE = criterion option chooses from a list of models based on a criterion• Available criteria are: adjrsq, aic, aicc, bic, cp ,cv, press, sbc, validate• CV is residual sum squares based on k-fold CV• VALIDATE is avg. sq. error for validation dataThe STOP criterion option stops the selection process. Available criteria are: adjrsq, aic aicc, bic, cp cv, press, sbc, sl, validate.When applied to the above problems, with the default options LASSO and LAR performed quite well. Below are results for LASSO, results for LAR were almost identical.• N = 100, 50 IVs, all noise . . . none selected• N = 1000, 50 IVs, all noise . . . none selected• N = 100, 50 noise variables, 1 real . . . none selected• N = 1000, 50 noise variables, 1 real . . . only real selected• N = 100, 50 noise variables, 1 real, 1 outlier . . . …. param est now .99• N = 100, 50 noise variables, 1 real, 2 outliers . . . ….no variables includedLIMITATIONSAlthough Lasso and LAR methods are superior alternatives to other methods, they are not panaceas. The methods still make assumptions, and these assumptions need to be checked. In addition to the standard statistical assumptions, they assume that the models being considered make substantive sense. As Weisberg notes in his discussion of Bradley Efron & Tibshirani (2004), neither LAR nor any other method of automatic method ‘has any hope of solving [the problem of model building] because automatic methods by their very nature do not consider the context of the problem at hand’. Or, as I often say in various contexts:Solving statistical problems without context is like boxing while blindfolded.You might hit your opponent in the nose, or you might break your hand on the ring post’.I am a SAS user by choice. However, R offers LASSO in several packages, including glmnet:Python offers LASSO through sklearn:Although no method can substitute for substantive and statistical expertise, LASSO and LAR offer much better alternatives than stepwise as a starting point for further analysis. The wide range of options available in both these methods allows for considerable exploration, and for eliminating models that do not make substantive sense.For additional information on the problems posed by stepwise, Harrell (2001) offers a relatively nontechnical introduction, together with good advice on regression modeling in general. Burnham & Anderson (2002) offers a more detailed approach, the first chapter outlines the problem, and the remaining chapters offer two general frameworks for solving it (one based oninformation criteria and the other on multimodel averaging). For more on LASSO and LAR, see Hastie & Friedman (2001).Bradley Efron, Trevor Hastie, I. J. & Tibshirani, R. (2004), ‘Least angle regression’, Annals of Statistics 32, 407–499.Burnham, K. P. & Anderson, D. R. (2002), Model selection and multimodel inference, Springer, New York.Harrell, F. E. (2001), Regression modeling strategies: With applications to linear models, logistic regression, and survivalanalysis, Springer-Verlag, New York.Miller, A. J. (2002), Subset selection in regression, Chapman & Hall, London.Trevor Hastie, R. T. & Friedman, J. (2001), The elements of statistical learning, Springer-Verlag, New York.",23/09/2018,3,0,1,"(479, 154)",7,1,0.0,1,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,expectation/interest
24,What is Artificial Intelligence(AI)? — Simple Definition,,Sagar Pokhrel,166.0,2.0,237,"Artificial intelligence (AI) is an area of computer science that emphasizes the creation of intelligent machines that work and react like humans. Some of the activities computers with artificial intelligence are designed for include:Artificial intelligence is a branch of computer science that aims to create intelligent machines. It has become an essential part of the technology industry.Research associated with artificial intelligence is highly technical and specialized. The core problems of artificial intelligence include programming computers for certain traits such as:Knowledge engineering is a core part of AI research. Machines can often act and react like humans only if they have abundant information relating to the world. Artificial intelligence must have access to objects, categories, properties and relations between all of them to implement knowledge engineering. Initiating common sense, reasoning and problem-solving power in machines is a difficult and tedious approach.Machine learning is another core part of AI. Learning without any kind of supervision requires an ability to identify patterns in streams of inputs, whereas learning with adequate supervision involves classification and numerical regressions. Classification determines the category an object belongs to and regression deals with obtaining a set of numerical input or output examples, thereby discovering functions enabling the generation of suitable outputs from respective inputs. Mathematical analysis of machine learning algorithms and their performance is a well-defined branch of theoretical computer science often referred to as computational learning theory.Originally published at datasagar.com on March 24, 2013.",24/03/2013,0,3,3,"(700, 377)",1,2,0.0,1,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,trust/acceptance
25,Architecting a Machine Learning System for Risk,The Airbnb Tech Blog,AirbnbEng,59000.0,11.0,2070,"By Naseem Hakim & Aaron KeysAt Airbnb, we want to build the world’s most trusted community. Guests trust Airbnb to connect them with world-class hosts for unique and memorable travel experiences. Airbnb hosts trust that guests will treat their home with the same care and respect that they would their own. The Airbnb review system helps users find community members who earn this trust through positive interactions with others, and the ecosystem as a whole prospers.The overwhelming majority of web users act in good faith, but unfortunately, there exists a small number of bad actors who attempt to profit by defrauding websites and their communities. The trust and safety team at Airbnb works across many disciplines to help protect our users from these bad actors, ideally before they have the opportunity to impart negativity on the community.There are many different kinds of risk that online businesses may have to protect against, with varying exposure depending on the particular business. For example, email providers devote significant resources to protecting users from spam, whereas payments companies deal more with credit card chargebacks.We can mitigate the potential for bad actors to carry out different types of attacks in different ways.Many risks can be mitigated through user-facing changes to the product that require additional verification from the user. For example, requiring email confirmation, or implementing 2FA to combat account takeovers, as many banks have done.Scripted attacks are often associated with a noticeable increase in some measurable metric over a short period of time. For example, a sudden 1000% increase in reservations in a particular city could be a result of excellent marketing, or fraud.Fraudulent actors often exhibit repetitive patterns. As we recognize these patterns, we can apply heuristics to predict when they are about to occur again, and help stop them. For complex, evolving fraud vectors, heuristics eventually become too complicated and therefore unwieldy. In such cases, we turn to machine learning, which will be the focus of this blog post.For a more detailed look at other aspects of online risk management, check out Ohad Samet’s great ebook.Different risk vectors can require different architectures. For example, some risk vectors are not time critical, but require computationally intensive techniques to detect. An offline architecture is best suited for this kind of detection. For the purposes of this post, we are focusing on risks requiring realtime or near-realtime action. From a broad perspective, a machine-learning pipeline for these kinds of risk must balance two important goals:These may seem like competing goals, since optimizing for realtime calculations during a web transaction creates a focus on speed and reliability, whereas optimizing for model building and iteration creates more of a focus on flexibility. At Airbnb, engineering and data teams have worked closely together to develop a framework that accommodates both goals: a fast, robust scoring framework with an agile model-building pipeline.In keeping with our service-oriented architecture, we built a separate fraud prediction service to handle deriving all the features for a particular model. When a critical event occurs in our system, e.g., a reservation is created, we query the fraud prediction service for this event. This service can then calculate all the features for the “reservation creation” model, and send these features to our Openscoring service, which is described in more detail below. The Openscoring service returns a score and a decision based on a threshold we’ve set, and the fraud prediction service can then use this information to take action (i.e., put the reservation on hold).The fraud prediction service has to be fast, to ensure that we are taking action on suspicious events in near realtime. Like many of our backend services for which performance is critical, it is built in java, and we parallelize the database queries necessary for feature generation. However, we also want the freedom to occasionally do some heavy computation in deriving features, so we run it asynchronously so that we are never blocking for reservations, etc. This asynchronous model works for many situations where a few seconds of delay in fraud detection has no negative effect. It’s worth noting, however, that there are cases where you may want to react in realtime to block transactions, in which case a synchronous query and precomputed features may be necessary. This service is built in a very modular way, and exposes an internal restful API, making adding new events and models easy.Openscoring is a Java service that provides a JSON REST interface to the Java Predictive Model Markup Language (PMML) evaluator JPMML. Both JPMML and Openscoring are open source projects released under the Apache 2.0 license and authored by Villu Ruusmann (edit — the most recent version is licensed the under AGPL 3.0) . The JPMML backend of Openscoring consumes PMML, an xml markup language that encodes several common types of machine learning models, including tree models, logit models, SVMs and neural networks. We have streamlined Openscoring for a production environment by adding several features, including kafka logging and statsd monitoring. Andy Kramolisch has modified Openscoring to permit using several models simultaneously.As described below, there are several considerations that we weighed carefully before moving forward with Openscoring:After considering all of these factors, we decided that Openscoring best satisfied our two-pronged goal of having a fast and robust, yet flexible machine learning framework.A schematic of our model-building pipeline using PMML is illustrated above. The first step involves deriving features from the data stored on the site. Since the combination of features that gives the optimal signal is constantly changing, we store the features in a json format, which allows us to generalize the process of loading and transforming features, based on their names and types. We then transform the raw features through bucketing or binning values, and replacing missing values with reasonable estimates to improve signal. We also remove features that are shown to be statistically unimportant from our dataset. While we omit most of the details regarding how we perform these transformations for brevity here, it is important to recognize that these steps take a significant amount of time and care. We then use our transformed features to train and cross-validate the model using our favorite PMML-compatible machine learning library, and upload the PMML model to Openscoring. The final model is tested and then used for decision-making if it becomes the best performer.The model-training step can be performed in any language with a library that outputs PMML. One commonly used and well-supported library is the R PMML package. As illustrated below, generating a PMML with R requires very little code.This R script has the advantage of simplicity, and a script similar to this is a great way to start building PMMLs and to get a first model into production. In the long run, however, a setup like this has some disadvantages. First, our script requires that we perform feature transformation as a pre-processing step, and therefore we have add these transformation instructions to the PMML by editing it afterwards. The R PMML package supports many PMML transformations and data manipulations, but it is far from universal. We deploy the model as a separate step — post model-training — and so we have to manually test it for validity, which can be a time-consuming process. Yet another disadvantage of R is that the implementation of the PMML exporter is somewhat slow for a random forest model with many features and many trees. However, we’ve found that simply re-writing the export function in C++ decreases run time by a factor of 10,000, from a few days to a few seconds. We can get around the drawbacks of R while maintaining its advantages by building a pipeline based on Python and scikit-learn. Scikit-learn is a Python package that supports many standard machine learning models, and includes helpful utilities for validating models and performing feature transformations. We find that Python is a more natural language than R for ad-hoc data manipulation and feature extraction. We automate the process of feature extraction based on a set of rules encoded in the names and types of variables in the features json; thus, new features can be incorporated into the model pipeline with no changes to the existing code. Deployment and testing can also be performed automatically in Python by using its standard network libraries to interface with Openscoring. Standard model performance tests (precision recall, ROC curves, etc.) are carried out using sklearn’s built-in capabilities. Sklearn does not support PMML export out of the box, so have written an in-house exporter for particular sklearn classifiers. When the PMML file is uploaded to Openscoring, it is automatically tested for correspondence with the scikit-learn model it represents. Because feature-transformation, model building, model validation, deployment and testing are all carried out in a single script, a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data, and then rapidly deploy the new model into production.Although this blog post has focused mostly on our architecture and model building pipeline, the truth is that much of our time has been spent elsewhere. Our process was very successful for some models, but for others we encountered poor precision-recall. Initially we considered whether we were experiencing a bias or a variance problem, and tried using more data and more features. However, after finding no improvement, we started digging deeper into the data, and found that the problem was that our ground truth was not accurate.Consider chargebacks as an example. A chargeback can be “Not As Described (NAD)” or “Fraud” (this is a simplification), and grouping both types of chargebacks together for a single model would be a bad idea because legitimate users can file NAD chargebacks. This is an easy problem to resolve, and not one we actually had (agents categorize chargebacks as part of our workflow); however, there are other types of attacks where distinguishing legitimate activity from illegitimate is more subtle, and necessitated the creation of new data stores and logging pipelines.Most people who’ve worked in machine learning will find this obvious, but it’s worth re-stressing:If your ground truth is inaccurate, you’ve already set an upper limit to how good your precision and recall can be. If your ground truth is grossly inaccurate, that upper limit is pretty low.Towards this end, sometimes you don’t know what data you’re going to need until you’ve seen a new attack, especially if you haven’t worked in the risk space before, or have worked in the risk space but only in a different sector. So the best advice we can offer in this case is to log everything. Throw it all in HDFS, whether you need it now or not. In the future, you can always use this data to backfill new data stores if you find it useful. This can be invaluable in responding to a new attack vector.Although our current ML pipeline uses scikit-learn and Openscoring, our system is constantly evolving. Our current setup is a function of the stage of the company and the amount of resources, both in terms of personnel and data, that are currently available. Smaller companies may only have a few ML models in production and a small number of analysts, and can take time to manually curate data and train the model in many non-standardized steps. Larger companies might have many, many models and require a high degree of automation, and get a sizable boost from online training. A unique challenge of working at a hyper-growth company is that landscape fundamentally changes year-over-year, and pipelines need to adjust to account for this.As our data and logging pipelines improve, investing in improved learning algorithms will become more worthwhile, and we will likely shift to testing new algorithms, incorporating online learning, and expanding on our model building framework to support larger data sets. Additionally, some of the most important opportunities to improve our models are based on insights into our unique data, feature selection, and other aspects our risk systems that we are not able to share publicly. We would like to acknowledge the other engineers and analysts who have contributed to these critical aspects of this project. We work in a dynamic, highly-collaborative environment, and this project is an example of how engineers and data scientists at Airbnb work together to arrive at a solution that meets a diverse set of needs. If you’re interested in learning more, contact us about our data science and engineering teams!Originally published at nerds.airbnb.com on June 16, 2014.",16/06/2014,0,10,5,"(538, 249)",4,3,0.0,25,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,trust/acceptance
26,The fall of RNN / LSTM,Towards Data Science,Eugenio Culurciello,7900.0,8.0,1797,"We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. Now it is time to drop them!It is the year 2014 and LSTM and RNN make a great come-back from the dead. We all read Colah’s blog and Karpathy’s ode to RNN. But we were all young and unexperienced. For a few years this was the way to solve sequence learning, sequence translation (seq2seq), which also resulted in amazing results in speech to text comprehension and the raise of Siri, Cortana, Google voice assistant, Alexa. Also let us not forget machine translation, which resulted in the ability to translate documents into different languages or neural machine translation, but also translate images into text, text into images, and captioning video, and … well you got the idea.Then in the following years (2015–16) came ResNet and Attention. One could then better understand that LSTM were a clever bypass technique. Also attention showed that MLP network could be replaced by averaging networks influenced by a context vector. More on this later.It only took 2 more years, but today we can definitely say:“Drop your RNN and LSTM, they are no good!”But do not take our words for it, also see evidence that Attention based networks are used more and more by Google, Facebook, Salesforce, to name a few. All these companies have replaced RNN and variants for attention based models, and it is just the beginning. RNN have the days counted in all applications, because they require more resources to train and run than attention-based models. See this post for more info.Remember RNN and LSTM and derivatives use mainly sequential processing over time. See the horizontal arrow in the diagram below:This arrow means that long-term information has to sequentially travel through all cells before getting to the present processing cell. This means it can be easily corrupted by being multiplied many time by small numbers < 0. This is the cause of vanishing gradients.To the rescue, came the LSTM module, which today can be seen as multiple switch gates, and a bit like ResNet it can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.But not all of it, as you can see from the figure above. Still we have a sequential path from older past cells to the current one. In fact the path is now even more complicated, because it has additive and forget branches attached to it. No question LSTM and GRU and derivatives are able to learn a lot of longer term information! See results here; but they can remember sequences of 100s, not 1000s or 10,000s or more.And one issue of RNN is that they are not hardware friendly. Let me explain: it takes a lot of resources we do not have to train these network fast. Also it takes much resources to run these model in the cloud, and given that the demand for speech-to-text is growing rapidly, the cloud is not scalable. We will need to process at the edge, right into the Amazon Echo! See note below for more details.At this time (September 2018) I would seriously consider this approach here. This is a 2D convolutional based neural network with causal convolution that can outperform both RNN/LSTM and Attention based models like the Transformer.The Transformer has definitely been a great suggestion from 2017 until the paper above. It has great advantages in training and in number of parameters, as we discussed here.Alternatively: If sequential processing is to be avoided, then we can find units that “look-ahead” or better “look-back”, since most of the time we deal with real-time causal data where we know the past and want to affect future decisions. Not so in translating sentences, or analyzing recorded videos, for example, where we have all data and can reason on it more time. Such look-back/ahead units are neural attention modules, which we previously explained here.To the rescue, and combining multiple neural attention modules, comes the “hierarchical neural attention encoder”, shown in the figure below:A better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.Notice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. This is also similar to Temporal convolutional network (TCN), reported in Note 3 below.In the hierarchical neural attention encoder multiple layers of attention can look at a small portion of recent past, say 100 vectors, while layers above can look at 100 of these attention modules, effectively integrating the information of 100 x 100 vectors. This extends the ability of the hierarchical neural attention encoder to 10,000 past vectors.This is the way to look back more into the past and be able to influence the future.But more importantly look at the length of the path needed to propagate a representation vector to the output of the network: in hierarchical networks it is proportional to log(N) where N are the number of hierarchy layers. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T >> N.It is easier to remember sequences if you hop 3–4 times, as opposed to hopping 100 times!This architecture is similar to a neural Turing machine, but lets the neural network decide what is read out from memory via attention. This means an actual neural network will decide which vectors from the past are important for future decisions.But what about storing to memory? The architecture above stores all previous representation in memory, unlike neural Turning machines. This can be rather inefficient: think about storing the representation of every frame in a video — most times the representation vector does not change frame-to-frame, so we really are storing too much of the same! What can we do is add another unit to prevent correlated data to be stored. For example by not storing vectors too similar to previously stored ones. But this is really a hack, the best would be to be let the application guide what vectors should be saved or not. This is the focus of current research studies. Stay tuned for more information.So in summary forget RNN and variants. Use attention. Attention really is all you need!Tell your friends! It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. Please tell them about this post.About training RNN/LSTM: RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. In short, LSTM require 4 linear layer (MLP layer) per cell to run at and for each sequence time-step. Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units. And it is easy to add more computational units, but hard to add more memory bandwidth (note enough lines on a chip, long wires from processors to memory, etc). As a result, RNN/LSTM and variants are not a good match for hardware acceleration, and we talked about this issue before here and here. A solution will be compute in memory-devices like the ones we work on at FWDNXT.Note 1: Hierarchical neural attention is similar to the ideas in WaveNet. But instead of a convolutional neural network we use hierarchical attention modules. Also: Hierarchical neural attention can be also bi-directional.Note 2: RNN and LSTM are memory-bandwidth limited problems (see this for details). The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize them! The external bandwidth is never going to be enough, and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth. The best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory, or that can be re-used for multiple computation per byte transferred (high arithmetic intensity).Note 3: Here is a paper comparing CNN to RNN. Temporal convolutional network (TCN) “outperform canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory”.Note 4: Related to this topic, is the fact that we know little of how our human brain learns and remembers sequences. “We often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks” — these chunks remind me of small convolutional or attention like networks on smaller sequences, that then are hierarchically strung together like in the hierarchical neural attention encoder and Temporal convolutional network (TCN). More studies make me think that working memory is similar to RNN networks that uses recurrent real neuron networks, and their capacity is very low. On the other hand both the cortex and hippocampus give us the ability to remember really long sequences of steps (like: where did I park my car at airport 5 days ago), suggesting that more parallel pathways may be involved to recall long sequences, where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task.Note 5: The above evidence shows we do not read sequentially, in fact we interpret characters, words and sentences as a group. An attention-based or convolutional module perceives the sequence and projects a representation in our mind. We would not be misreading this if we processed this information sequentially! We would stop and notice the inconsistencies!Note 6: A recent paper trained unsupervised with attention/transformer and showed amazing performance in transfer learning. The VGG of NLP? This works is also an extension of pioneering work by Jeremy and Sebastian, where an LSTM with ad-hoc training procedures was able to learn unsupervised to predict the next word in a sequence of text, and then also able to transfer that knowledge to new tasks. A comparison of the effectiveness of LSTM and Transformer (attention based) is given here and shows that attention is usually attention wins, and that “The LSTM onlyoutperforms the Transformer on one dataset — MRPC.”Note7: Here you can find a great explanation of the Transformer architecture and data flow!I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more…",13/04/2018,0,12,10,"(700, 394)",5,0,0.0,46,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,positive,joy/calmness
27,An End-to-End Project on Time Series Analysis and Forecasting with Python,Towards Data Science,Susan Li,25000.0,9.0,1077,"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.Time series are widely used for non-stationary data, like economic, weather, stock price, and retail sales in this post. We will demonstrate different approaches for forecasting retail sales time series. Let’s get started!We are using Superstore sales data that can be downloaded from here.There are several categories in the Superstore sales data, we start from time series analysis and forecasting for furniture sales.We have a good 4-year furniture sales data.Timestamp(‘2014–01–06 00:00:00’), Timestamp(‘2017–12–30 00:00:00’)This step includes removing columns we do not need, check missing values, aggregate sales by date and so on.Our current datetime data can be tricky to work with, therefore, we will use the averages daily sales value for that month instead, and we are using the start of each month as the timestamp.Have a quick peek 2017 furniture sales data.Some distinguishable patterns appear when we plot the data. The time-series has seasonality pattern, such as sales are always low at the beginning of the year and high at the end of the year. There is always an upward trend within any single year with a couple of low months in the mid of the year.We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise.The plot above clearly shows that the sales of furniture is unstable, along with its obvious seasonality.We are going to apply one of the most commonly used method for time-series forecasting, known as ARIMA, which stands for Autoregressive Integrated Moving Average.ARIMA models are denoted with the notation ARIMA(p, d, q). These three parameters account for seasonality, trend, and noise in data:This step is parameter Selection for our furniture’s sales ARIMA Time Series Model. Our goal here is to use a “grid search” to find the optimal set of parameters that yields the best performance for our model.The above output suggests that SARIMAX(1, 1, 1)x(1, 1, 0, 12) yields the lowest AIC value of 297.78. Therefore we should consider this to be optimal option.We should always run model diagnostics to investigate any unusual behavior.It is not perfect, however, our model diagnostics suggests that the model residuals are near normally distributed.To help us understand the accuracy of our forecasts, we compare predicted sales to real sales of the time series, and we set forecasts to start at 2017–01–01 to the end of the data.The line plot is showing the observed values compared to the rolling forecast predictions. Overall, our forecasts align with the true values very well, showing an upward trend starts from the beginning of the year and captured the seasonality toward the end of the year.The Mean Squared Error of our forecasts is 22993.58The Root Mean Squared Error of our forecasts is 151.64In statistics, the mean squared error (MSE) of an estimator measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated. The MSE is a measure of the quality of an estimator — it is always non-negative, and the smaller the MSE, the closer we are to finding the line of best fit.Root Mean Square Error (RMSE) tells us that our model was able to forecast the average daily furniture sales in the test set within 151.64 of the real sales. Our furniture daily sales range from around 400 to over 1200. In my opinion, this is a pretty good model so far.Our model clearly captured furniture sales seasonality. As we forecast further out into the future, it is natural for us to become less confident in our values. This is reflected by the confidence intervals generated by our model, which grow larger as we move further out into the future.The above time series analysis for furniture makes me curious about other categories, and how do they compare with each other over time. Therefore, we are going to compare time series of furniture and office supplier.According to our data, there were way more number of sales from Office Supplies than from Furniture over the years.((2121, 21), (6026, 21))We are going to compare two categories’ sales in the same time period. This means combine two data frames into one and plot these two categories’ time series into one plot.We observe that sales of furniture and office supplies shared a similar seasonal pattern. Early of the year is the off season for both of the two categories. It seems summer time is quiet for office supplies too. in addition, average daily sales for furniture are higher than those of office supplies in most of the months. It is understandable, as the value of furniture should be much higher than those of office supplies. Occasionally, office supplies passed furniture on average daily sales. Let’s find out when was the first time office supplies’ sales surpassed those of furniture’s.Office supplies first time produced higher sales than furniture is 2014–07–01.It was July 2014!Released by Facebook in 2017, forecasting tool Prophet is designed for analyzing time-series that display patterns on different time scales such as yearly, weekly and daily. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints. Therefore, we are using Prophet to get a model up and running.We already have the forecasts for three years for these two categories into the future. We will now join them together to compare their future forecasts.Now, we can use the Prophet Models to inspect different trends of these two categories in the data.Good to see that the sales for both furniture and office supplies have been linearly increasing over time and will be keep growing, although office supplies’ growth seems slightly stronger.The worst month for furniture is April, the worst month for office supplies is February. The best month for furniture is December, and the best month for office supplies is October.There are many time-series analysis we can explore from now on, such as forecast with uncertainty bounds, change point and anomaly detection, forecast time-series with external data source. We have only just started.Source code can be found on Github. I look forward to hearing feedback or questions.References:A Guide to Time Series Forecasting with ARIMA in Python 3A Guide to Time Series Forecasting with Prophet in Python 3",09/07/2018,29,13,5,"(672, 326)",21,0,0.0,9,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,positive,anger/irritation
28,Machine Learning Types and Algorithms,Towards Data Science,Chethan Kumar GN,599.0,4.0,263,"Different types of machine learning types and algorithms, also when and where these are used.Prerequisite: Need for Machine Learning? (for better understanding)Common terms used:There are four types of machine learning (some might say three but here we will go with four the “more the merrier right!!!”).Key points:2. Unsupervised Learning: “The outcome or output for the given inputs is unknown”, here input data is given and the model is run on it. The image or the input given are grouped together here and insights on the inputs can be found here(which is the most of the real world data available). The main algorithms include Clustering algorithms( ) and learning algorithms.Key points:3. Semi-supervised Learning: It is in-between that of Supervised and Unsupervised Learning. Where the combination is used to produce the desired results and it is the most important in real-world scenarios where all the data available are a combination of labelled and unlabeled data.4. Reinforced Learning: The machine is exposed to an environment where it gets trained by trial and error method, here it is trained to make a much specific decision. The machine learns from past experience and tries to capture the best possible knowledge to make accurate decisions based on the feedback received.Key points:To put the above in a nutshell view the image below from en.proft.meMore references:Next I have Machine Learning implementing example in 5 minutes coming up make sure to follow me on medium, linkedin, twitter, Instagram to get more updates. And also if you liked this article make sure to give a clap and share it.Join our WhatsApp community here.",06/09/2018,0,50,43,"(603, 312)",5,6,0.0,10,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
29,Transformers - L’ultimo cavaliere è il testamento di Michael Bay,The Shelter,Alessandro Di Romolo,105.0,5.0,1104,"Pare sia impossibile avere un’idea su Michael Bay senza capire nulla di cinema. Non ne capisce nulla chi lo ritiene un genio capace di gestire una quantità abnorme di informazioni visive in un singolo frame, ma a sentire l’altra campana non ne capisce nulla nemmeno chi lo ritiene un mediocre che non conosce la grammatica del linguaggio cinematografico. La polarizzazione delle opinioni crea mostri; le provocazioni, unico mezzo espressivo conosciuto da certi fenomeni social, li alimentano, riducendo se non addirittura annullando ogni margine di discussione. Chi non ha alcun interesse a schierarsi può decidere di ignorare la questione e far finta che un tizio che ha generato entrate pari a circa 6 miliardi di dollari al botteghino semplicemente non esista, oppure può approfondire e cercare di capire perché Bay divida il pubblico in maniera così netta.Non nego (né rinnego) di essermi lasciato sedurre in passato da uno dei fenomeni social di cui sopra (particolarmente avvezzo alla mistificazione della realtà) e di aver messo nero su bianco, anche su queste pagine, il mio disprezzo per l’autore di Armageddon. Non credo potrò mai entrare in completa sintonia con il suo stile, non credo scatterà mai quella scintilla che non è scattata nemmeno riguardando i più recenti prodotti della sua filmografia, dai quattro episodi di Transformers al bel 13 Hours, concedendo una seconda possibilità anche a Pain & Gain. Tuttavia, riconosco al suo pensiero una dignità e una personalità che è difficile individuare in mezzo all'immondizia informe nella quale gran parte della critica è solita relegare i suoi film. È un cinema maleducato e magniloquente, che non rispetta le regole e sacrifica forme narrative convenzionali in nome di un ideale estetico, che è quello del dinamismo a tutti i costi. È un cinema di movimento in cui i personaggi non sono mai statici e, quando lo sono, è lo sguardo del regista a muoversi attorno a loro e a infarcire l’inquadratura di elementi audiovisivi in moto. Elementi difficili da decodificare dall'occhio dell’essere umano medio, spesso troppo piccolo e troppo lento per gli standard di Bay. Un ideale perfettamente sintetizzato da una frase del nostro apparsa su un vecchio articolo del 2001 pubblicato sul New York Times.“Quello che ricordo di questo film [West Side Story] è che non devi necessariamente innamorarti dei personaggi o appassionarti alla storia d’amore. È più una questione di stile e di coreografie e di energia e di fantastica musica.”Sospetto che la sintassi dell’ultima frase non sia casuale: Bay non si ferma nemmeno per mettere una virgola. Stile e coreografia ed energia e fantastica musica, in buona sostanza è tutto qui. È il suo credo ma è anche il suo più grosso limite, perché la negazione della staticità toglie respiro al racconto, specie quando fisiologicamente il film ha bisogno di fermarsi un attimo e far riflettere lo spettatore su quello che ha visto.Ma siccome descrivere a parole qualcosa che andrebbe visto per essere compreso appieno diventa alla lunga noioso, vi consiglio di guardare il video che segue prima di parlarvi della sua ultima opera.Sono uscito dalla sala frastornato, svuotato, esausto. La visione di Transformers - L’ultimo cavaliere è l’anello di congiunzione tra l’esperienza sedentaria del guardare un film e la prestazione sportiva. Centocinquanta minuti che volano via, ma ti lasciano spompato, ti violentano gli occhi e le orecchie e ti mandano fuori giri il cervello. La prima ora di film è interlocutoria: si va avanti un po’ a singhiozzo, tra brusche accelerate alternate a momenti più compassati e complicati (quanto necessari) spiegoni che pongono ordine all'enorme quantità di informazioni sparate a raffica. Come da tradizione, la natura ipertrofica del franchise impone l’introduzione di una tonnellata di nuovi personaggi e di un numero spropositato (anche per un film così lungo) di trame e sottotrame, che si intrecciano e si sovrappongono abbracciando un arco temporale di 1600 anni. Il quinto episodio della saga dei giocattoli Hasbro inizia dalla leggenda di Re Artù e finisce in un futuro distopico in cui la distruzione di Chicago e l’arrivo a intermittenza di nuovi Autobot e Decepticon ha inasprito la diffidenza nei confronti dei Transformers. La vicenda di Optimus Prime, tornato su Cybertron per incontrare i suoi creatori, s’intreccia con quella di Cade Yeager (Mark Wahlberg), divenuto un fuorilegge che offre asilo ai robot dopo gli eventi de L’età dell’estinzione, e di un misterioso personaggio interpretato da Anthony Hopkins, ultimo membro di un ordine che negli anni ha insabbiato la collaborazione di lunga data tra umani e organismi robotici alieni.Sistemati i (tanti) pezzi sulla scacchiera, il film va su di giri e non scende più. In barba a ogni regola narrativa e a ogni convenzione sulla gestione del ritmo, Bay mette in scena una quantità fuori controllo di location, di spostamenti, di ribaltamenti di prospettiva e di colpi di scena, regalando un finale lungo quasi un’ora e mezza che fa sembrare l’ultima ora del terzo episodio una roba dimessa. È il suo testamento: un estenuante ma estremamente divertito tripudio di apocalisse e di effetti speciali, di combattimenti tra giganti di metallo, di inseguimenti, di pianeti che inglobano altri pianeti e di battaglie campali che vedono coinvolti draghi e demoni meccanici, eredi di Mago Merlino, militari, entità cosmiche e robot. Nulla a che vedere, insomma, con l’annoiato quarto episodio.È confusionario, spesso dà l’impressione di essere un insieme di set piece tenuti insieme alla carlona da una sceneggiatura incongruente e inconcludente e la vena comica, elemento ricorrente della saga, è alimentata da un umorismo che funziona una volta si e due (ma anche tre) no. Però mi sono divertito. Ho riso più di quanto avessi fatto nei quattro precedenti episodi messi insieme. Sono rimasto affascinato (a tratti) dalle manie di grandezza dell’autore e dal suo sguardo più largo delle possibilità fisiche del comune mortale, al punto che se guardi una porzione di schermo rischi di perderti quello che accade nel resto del fotogramma: guardarlo in IMAX deve essere un’esperienza soverchiante. Mi sono gasato per i discorsi pomposi di Optimus Prime e per l’eroismo un tanto al chilo dei Transformers; ho ammirato la capacità della produzione di far sembrare il film tre volte più costoso di quello che è costato in realtà e certe immagini mi rimarranno impresse sulla retina per diverso tempo. Continuo a preferire altre forme di narrazione — non necessariamente più raffinate e meno casiniste (per dire, preferisco la strada intrapresa da Fast & Furious) — e a trovare il cinema di Bay pieno di storture e di cose che non mi piacciono, ma trovo anche che sia ingeneroso gettarlo nella spazzatura: L’ultimo cavaliere è quanto di meglio possa offrire il franchise di Transformers, il pattume è decisamente altra cosa.",29/06/2017,0,11,11,"(700, 355)",2,0,0.0,2,it,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
30,What do You expect Your Reminder App to do?,,Ratul Aich,,2.0,50,A reminder app is a utility application. People need to access it all the time. Very swiftly and quickly. So a reminder app should be simple but must include the things a user want.Think about your personal problems while using a reminder application.Please provide with your insight comment and suggestions.,12/10/2014,0,0,0,"(700, 311)",1,1,0.0,0,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,positive,joy/calmness
31,The road to Sparse R-CNN — key ideas and intuition,ResponsibleML,Jakub Wiśniewski,25.0,6.0,990,"Briefly explaining the former SOTA models for object detection and the innovations that were brought by Sparse R-CNN.Sparse R-CNN is a new SOTA algorithm that has been presented at Computer Vision and Pattern Recognition (CVPR) conference 2021. It is faster and in some cases better than its predecessors. But what new solutions does it propose and how it differs from what has been before? In this blog, I will try to explain how object detection developed, and in the end, I will show you the intuition behind how the new state-of-the-art machine learning model works.Two-stage object detection groups algorithms first propose regions where some objects will most likely be. The first one called R-CNN was based on selective search. The idea here is simple, we don’t want our boxes to be placedrandomly, nor we want a big number of them. Remember that boxes might be small or huge, ranging in position and shape. This is why it is better to look at color, texture, and size. That is why Regions with CNN features (R-CNN) is a great combination. Instead of hundreds of thousands of boxes, we get only around 2 thousand densely populated boxes. However, this algorithm was very slow, taking 40–50 seconds per image. Such slow processing was then sped up by the successors Fast R-CNN and Faster R-CNN. In this blog I will only focus on the latter.Faster R-CNN is composed out of two modules, first Region Proposal Network (RPN) and the second one is using RoI pooling and then classifying and regressing the bounding boxes. RPN based on feature maps outputs therectangular proposals and their objectness score (not looking at particular object class but probability that there is an object at all). It is a neural network which uses sliding window through the output of the last convolutional layer. Every window takes a few bounding boxes (originally 9) which scale and aspect ratio varies. So for example for a typical convolutional feature map there could be over 10 thousand bounding box proposals which then are classified and regressed.Two-stage detection is usually more accurate and precise, but it lacks the speed to be used in true real-time scenarios. The most famous one-stage object detection algorithm is YOLO which in its simplicity achieves high scores on benchmarking datasets. It is a single neural network that uses features from the whole image to predict each bounding box. That is why it is suitable to run in real-time. However, it has some limitations, for example, detecting smaller objects or objects that are close to each other. YOLO algorithm had been developed and improved (at the time of writing this article there are 5 versions of YOLO). But I will focus on a model referenced by Sun et al., 2021 which is RetinaNet.RetinaNet’s backbone is the Feature Pyramid Network (FPN) which helps with detection of objects of different scales. It is built on top of the ResNet. At each image in an image pyramid a few anchors (originally 9) with different aspect ratios. These sub-images are feed to the classification subnet and the regression subnet. The classification convolutional neural net subnet and box subnet have the same parameters for all pyramid levels. The main attribution of RetinaNet is a new kind of loss called Focal Loss. It focuses on mitigating the effect of class imbalance between the foreground and background objects.Sparse R-CNN is the two-stage object detection algorithm. It rejects the idea of dense candidate regions. Authors point out that such proposals are mostly redundant even when considering more dense scenes (for example crowds). So many potential objects in an image will make the predictor slower than it needs to be. Sparse R-CNN only uses a small, fixed number of potential boxes (in the COCO dataset they needed only 100 initial boxes). This method stands out using both sparse boxes and sparse features, both of which are learnable through models training.The boxes are at first generated randomly. Then through backpropagation, the box placement is being optimized so if certain classes are usually found on the top right corner, a few boxes will be placed there. Authors suggest that it may be interpreted as looking at the statistics of potential object locations. For example, if one class is a rug then some of the boxes will be wider and placed on the bottom of an image.Another innovation is learnable proposal features that is the same as a number of boxes. The features are expected to encode not only the localization but also pose, texture, shape, etc. It is done with the help of RoI features and proposal features that interact with each other (through such interaction less meaningful and biased ones will be filtered out). For each bounding box proposal, we now have a specific encoding that hopefully will make the future predictions process easier. It is done in the so-called Dynamic Instance Interactive Head.Each feature and each box is then fed into the Dynamic Instance Interactive Head that is conditioned on the feature and then the classification and bounding box regression is performed as you can see in the figure below.On MS COCO benchmarks it is faster (indicated by higher Frames Per Second (FPS)) and significantly better for smaller objects (which is indicated by Average Precision for small objects (APs)).It also proves to be easier to train than its predecessors. It means that with less computing time and therefore time we may be able to obtain better results.Object detection field has gone a long way. Through innovations, it achieves better performance with less computing power. Such systems are also somewhat easy to implement through libraries like pytorch or tensorflow. The code for the new state-of-the-art object detector can be found here. If you want to read more about the developments of the object detection field I encourage you to read a survey on object detection. And if you want to know more details about Sparse R-CNN there is an excellent article here.",02/08/2021,0,2,0,"(583, 293)",7,0,0.0,19,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
32,Cart-Pole Balancing with Q-Learning,,Matthew Chan,215.0,5.0,1000,"The OpenAI Gym provides many standard environments for people to test their reinforcement algorithms. These environments include classic games like Atari Breakout and Doom, and simulated physical environments like this one where you have to keep a humanoid standing:I decided to start from the Cart-Pole balancing problem. This is essentially the classic inverted pendulum problem which you could find in a typical undergraduate control course. However, instead of applying control theories, the goal here is to solve it using controlled trial-and-error, also known as reinforcement learning.In here, we represent the world as a graph of states connected by transitions (or actions). It means that to predict your future state, you will only need to consider your current state and the action that you choose to perform. The key here is that you don’t need to consider your previous states. This is what people call a Markov Model. Although your past does have influences on your future, this model works because you can always encode information about the past in your current state.In human, our state includes our memory which encodes everything that we have done (or we think that we have done) in the past. Not all states are fully observable, however. We often can only guess what the true states are from the observable states. For instance, I don’t know what you are thinking and can only infer it through your action. This is what people called a Hidden Markov Model.The Cart-Pole world consists of a cart that moves along the horizontal axis and a pole that is anchored on the cart. At every time step, you can observe its position (x), velocity (x_dot), angle (theta), and angular velocity (theta_dot). These are the observable states of this world. At any state, the cart only has two possible actions: move to the left or move to the right.In other words, the state-space of the Cart-Pole has four dimensions of continuous values and the action-space has one dimension of two discrete values.In this environment, you get a reward as long as the pole is still somewhat upright and the cart is still within the bound. An episode is over as soon as the pole falls beyond a certain angle or the cart strays too far off to the left or right. The problem is considered “solved’ when it stays upright for over 195 time steps, 100 times consecutively.A policy is simply the action that you take at a given state. Here, you want to find the policy that can maximize your reward. In other words, you want to find the optimal policy for each possible state.Q-Learning is a method of finding these optimal policies. You can read more about it on this page. Essentially, through trials-and-errors, you find a Q-value for each state-action pair. This Q-value represents the desirability of an action given the current state. Over time, if the world is static (i.e. the physics or the cause-and-effects don’t change), the Q-values would converge and the optimal policy of a given state would be the action with the largest Q-value.To use Q-Learning, you would have to discretize the continuous dimensions to a number of buckets. In general, you want to have fewer buckets and keep the state-space as small as possible. Having fewer optimal polices to find means faster training. However, discretizing the state-space too coarsely might prevent convergence as important information might be discretized away.My first thought was that I would only need theta, which is the angle between the pole and the vertical axis. After all, the goal is to keep the pole up right. Yes, the cart does move around, but the bound is sufficiently wide that it probably won’t go out before the pole falls over. I didn’t see how the linear position nor either of the velocities could be useful. Moreover, I thought that having a one-dimensional state-space would make populating the Q-table much quicker.However, I was wrong. I wasn’t able to keep the pole upright for more than 60 time steps. I tried tweaking the learning rate, the number of buckets, exploration rate, etc. Nothing worked.Then I thought; I do know the optimal policy. When the pole is tilted to the left, move to the left. If it is tilted to the right, move to the right. So I manually inputted this optimal policy, it didn’t work any better either! At that point, I realized that I needed the other state components as well.I quickly looked up an MatLab implementation of the solution to gauge what were the number of buckets that I would need for each component. It turned out that it only used 3 buckets for x, x_dot, and theta_dot, and 6 buckets for theta!With that I was able to solve that Cart-Pole problem within 681 episodes! It was a still a far cry from the best solution. Without changing the structure of the algorithm, I started wondering what I could do to solve the problem quicker.I tried setting the decay of exploration rate faster. I thought it might be exploring for too long without exploiting. It didn’t really work. It was using sub-optimal policy too quickly.When I was staring at the animation, I noticed that the cart didn’t really move out of bound that often. Also, I only needed to balance the pole for 200 time steps. It typically didn’t drift that far while balancing the pole. Therefore, I removed the x and x_dot components and leaving only the theta, and theta_dot components. With this significantly reduced state-space, I was able to solve the problem much quicker in only 136 episodes !I could see that the cart was inching to the left or right every now and then. However, the movement was so slow that an episode was usually completed (reached 200 time steps) before it could get out of the bound. Dimensionality reduction was once again the key to faster learning!Below is the code for this implementation. Also check out my next post on replacing the Q-table with a Q-network!",13/11/2016,0,0,17,"(627, 306)",5,0,0.0,17,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
33,難兄難弟：IA與WAP,石墨工房8.0,Fred Jame,6500.0,11.0,12,"如果您還不知道的話，那您就得看看這篇文章了。因為這裡要聊到的，就是這兩個廠商搞得熱火朝天，但是大多數消費者都還在冷眼旁觀、甚至一頭霧水的難兄難弟。您會買上網冰箱嗎？想想。如果這部上網冰箱（電視、烤箱、微波爐……）還得內建數據機、要拉一條電話分機線接在後面、要下載食譜還要撥接上網的話，我不會買。如果每個月還要花好幾千塊接雙向電視纜線數據機（cable modem）或ADSL，才能讓您的這些IA（Information Appliances，也就是「資訊家電」，或者叫做「Internet Appliances」，也就是「網路家電」）看起來有發揮一點作用，您會買嗎？我大概也不會買。結論：資訊家電很好，但是不知道買來要做什麼用。以現在電腦的普及，實在不需要用電視看網頁（用電視看網頁的畫面通常不太清楚，而且電視用的瀏覽軟體跟很多Java之類的複雜程式都不太相容）。上網冰箱？裡面要放多大的硬碟和記憶體？至於上網微波爐，那就實在真的不知道要做什麼了。「你是石器時代的人嗎？」「不要抗拒新技術，你這種人就是電腦化最大的障礙」，您也許會這麼說。其實，電視可以透過網路隨選世界各地的節目內容、冰箱會自動通知超市來外送補貨、微波爐會記錄您的烹調習慣送給醫師做參考，都是很酷的事情，值得家裡的小朋友明早快步跑到學校去，跟還在用電腦上網的遜同學炫耀一番。問題是，值得天天看的網路電視台、送貨到家的網路超市、透過網路收集資料的家庭醫師已經在我們的環境裡出現了嗎？我們的網路資源已經便宜到像水費一樣，水龍頭一個月忘了關也不會要人命嗎？您一定要相信，以後這些一定都會實現──只要您常燒好香的話。那麼，今天的IA市場在哪裡？IA的希望在網路，網路的希望在台灣。別忘了，台灣是「準亞太營運中心」，也是「準亞太通訊中心」，這些基本建設理當不會太含糊。只要有健全、穩定、價格低廉的網路通訊架構（也就是洋文兒裡講的「communication infrastructure」），IA應該就會有市場。最近國內有許多廠商投入了Linux作業系統的市場，希望能在微軟的「包山包海」之外另闢一條賺錢的蹊徑。殊不知在Linux的「自由傳播」原則下，這些廠商光靠賣系統光碟片和手冊是賺不到錢的；而在個人電腦市場又暫時打不過微軟和蘋果，所以就開始動資訊家電這塊處女地的腦筋。以往在這個市場出現的只有Windows，但是因為Windows實在難以簡化到能在家電上安裝和執行的程度，所以並不成功，即使是Windows CE，都還是太複雜了一點。而Linux以它可大可小、架構精簡速度快、跑起來又穩定的特性，頓時成為資訊家電市場的救世主，從手錶到冰箱到汽車，同樣也是「包山包海」。所以，在一方面理想的作業系統出現，二來廠商急著另闢財源的條件撮合之下，資訊家電這一行就成了電腦業界的新寵，幾乎每一家大一點的廠商都在偷偷的搞，似乎如果沒有玩玩這類產品，就會把下一波賺錢機會擋在門外似的。講到這裡，您有沒有聞到了一點奇怪的味道？說來奇怪，這些搞IA的人怎麼都沒有問問消費者有沒有興趣呢？答案是「沒必要問」。因為就現在的通訊環境來看，IA並不是一種為了因應消費者需求而誕生的產品，而是為今天的資金找出路、為明天的賺錢賭性命的不得不然。究竟經濟學裡還有一句「供給創造需求」的詞兒可以讓廠商印在護身符上用用，也許消費者們只是「未經人事」，只要扮演警幻仙子（18歲以上的讀者請參閱紅樓夢第五回）的IA廠商一出現，那就一拍即合、一切OK了。往好處想，身為消費者，我們應該感謝這些廠商肯花錢「明天的氣力，今天就幫我們傳便便」。等到家裡拉專線一個月只要600元，跟看第四台一樣的時候，大家別忘了買部附送攝影機的網路馬桶捧捧場。您會用手機上網嗎？想想。跟IA一樣，手機上網所碰到的，不僅是大家常聽到的「內容」和「服務」問題而已，而且還牽涉到使用的成本和通訊環境。如果使用起來真的像廠商說的那麼方便：隨時可以看今天的星座運勢、上演電影、股票漲跌、餐廳訂位、電子郵遞、即時新聞……，您覺得以現在的通話費用加上上網費用，一個月大概要花多少錢？手機上網最大的優點，在於它不像一般家用電話需要撥接，而是只要一開機，手機馬上會向通訊網路報到，之後就一直保持在線上的狀態，就像是一般公司用的專線、或是家裡用的雙向cable modem或ADSL一樣。換言之，只要您的手機開著，就等於已經在網路上，隨時準備好接收資訊。所以無論跟專線或ADSL相比，手機的「專線性質」實在是太便宜了；因為除了基本月租費之外，一毛錢也不用花。很棒吧？一點都不棒。手機上網的優點，目前只停留在網路相關的技術層面而已。雖然它隨時都在線上，但是並不表示您就可以像是用專線一樣，沒事就查查e-mail、下載個MP3音樂來聽、或是上聊天室打屁到天亮。在這裡，它跟專線正好相反；用專線不管事查信或聊天，都不需要再另外交錢，但是如果用手機做一樣的事情，花的錢不僅是「以秒計費」，甚至等於是「以byte計費」。自己算算嘛，手機上網速度不過9.6Kbps（一般電話撥接實際上是大約53K），就算它可以「全速前進」好了，您下載任何1KByte的資料，大約就要花上一塊錢，論單位成本，就已經超過電腦撥接上網不只百倍。當然，日理萬機的大老闆、或是在股市殺進殺出的闊太太們，是不會在乎這種小錢的，問題是在於如果沒有大眾的支持和使用，這種在技術上和便利性上都十分優秀的應用，也不免會墜入所謂的「死亡漩渦」──太貴大家不用，大家不用就沒有人要做內容、沒有人做內容就沒人看、看的人不多價格就還是貴，沒完沒了。所以手機上網沒希望囉？那也不盡然。光是「手機＝專線終端機」這一點，裡頭就不知道蘊含了多少的商機。在筆者最近翻譯過的一些文章裡，就提到了許多很有創意的做法，充分利用了這種特性。例如當您的車子開近某一家餐廳的時候，手機螢幕上就會自動出現這家餐廳的廣告；而這種形態的廣告被接受、並且立即轉換成消費行為的機率，自然要比已經沒什麼人在看的電腦橫幅廣告要高得多，所以廣告主也應該更樂意投資在這種有效的新應用上。換言之，手機上網絕對是一個趨勢，而且普及率達到百分之百只是時間的問題而已。然而，如果沒有適當的誘因，讓消費者樂於一試（對消費者來說，最好的誘因就是「免費」；如果有某家系統商宣佈WAP上網免費，裡頭的伺服器不擠到爆才怪）、讓廣告主願意在這個媒體上花錢，結果就會好像慈禧太后買汽車一樣，空有最新的好機器，但是沒油可加、沒地方可去。雖然日本什麼都貴，一碗叉燒肉比餛飩皮還薄的拉麵要台幣300元，但是用手機上網倒是相當便宜。以最近相當轟動，讓許多台灣公司絡繹不絕前去取經的DoCoMo公司（這個名字也很有趣，除了是「Do Communicate over the Mobile Network」的縮寫之外，在日文裡是「哪兒都可以」的意思）所提供的「i-mode」上網服務為例，基本費只要300日圓，之後的收費方式是以傳輸的資料量來計算：以包含128位元資料的封包（packet）為單位，每個封包0.3日圓。依照DoCoMo公司提供的一般平均用量範例，每個人每月的傳輸量大約是50,000位元組。如果這麼多的內容都是純文字，那也很夠瞧的，不過對於許多像我這樣的「重度上網症」患者來說，一天份的e-mail內容就遠超過此數了。不過對大多數只是偶爾看看信、或是算算命的使用者來說，應該差不多就是這個量。50,000位元組大約等於391個封包，391乘以0.3日圓，等於一個月只要花117日圓，差不多30塊台幣，再加上基本費也不過一百塊出頭。您有在用WAP嗎？趕快試試差不多的玩法一個月要多少錢，然後寫信（要用手機寫喔！）告訴我。雖然i-mode的收費離「免費」的完美境界還有一段距離（不過看起來也不遠了），但是跟國內的WAP環境比起來，實在還是相當便宜，而且因為是以封包數量計費，所以使用起來也比較沒有時間的壓力；萬一網路剛好有點塞，也不用一手拿手機、一手看秒針、中間還得抽空擦個汗。i-mode由於速度、價格、線上服務等等條件的絕佳組合，所以從去年推出以來，目前使用者已經超過1,000萬人，而且還以每天四萬人註冊的速度在增加之中。以這樣的使用人數和頻率，誰曰手機上網沒搞頭？這篇文章裡講的，其實都並不是技術問題；而且無論是WAP或IA，最大的問題也都不在於技術，而是筆者最關心的重點：要怎麼讓大眾享用好的技術，而不要讓這些產品被不成熟的環境、不良的經營模式、以及彆腳的行銷手法殺死。目前美國人在手機上網這方面最重視的問題，在於如何針對手機強烈的「個人化」特性，讓資訊服務廠商可以針對每個人的需要（例如餐飲、交通、閱讀等習慣），設計出量身訂做的內容，讓消費者願意使用。而日本人則是從經營形態入手，先吸引許多人上網嘗試，再利用這些人潮帶動線上服務和周邊產品的發展。有些時候，經常會覺得台灣是一個「工程師王國」；任何在資訊科技界做過產品或行銷經理的人，看到這句話應該多少會有同感。這句話的意思是，台灣的科技廠商以研發見長，工程師都有很強的技術能力，可以開發或生產出世界頂尖的電子產品；但也許是因為幫國外廠商代工做久了的關係，相較於歐美日本而言，產品和市場行銷部門多半處於比較弱勢的地位；也就是說，研發單位的想法跟行銷、產品、市場部門的橫向溝通，往往有著很大的問題。把這個景象放大來看，就可以看到很多國內廠商在孜孜不倦的開發各種資訊家電，但是講到市場就是一言以蔽之：「現在先做了卡位，將來有機會可以外銷」；或者宣佈諸如「WAP可以跟家庭保全系統結合」之類可以暫時炒一炒、但是卻看不到市場在哪裡的新聞。最近出版的一冊「將太的壽司」漫畫裡有一句台詞說得好：「與其辛苦模仿，不如努力超越」。我們在許多技術方面已經不太需要去超越誰了，但是當產品出來之後，卻似乎一下子歐洲、一下子美國日本，不曉得應該怎麼讓這些產品為廠商帶來利潤、為消費者帶來方便。對於像手機上網或是資訊家電這類的新產品來說，也需要有新的環境和新的經營創意才能成功；對於傳統的行動通訊市場來說，i-mode也是一種突破，而它就是個成功的例子。而在這幾個條件之中，最重要的還是經營形態必須根據市場的特性，做出超越模仿的創新。這一點跟筆者過去在許多關於網路或趨勢產業的文章中，不斷強調的重點其實還是一樣的：無論技術如何進步、網路點子如何創新，紮實有效的經營方式和利潤來源，才是企業長治久安的基礎。說真的，如果廠商做IA的目的就是外銷、手機上網就是算算命，那也就無話可說了。因為這些現象所反映出來的，不僅是大家對於這些產品和其中蘊含的商機看似熱炒、實則絕望的心態；還有國內通訊環境的落後與昂貴，讓台灣製的資訊家電只有在外國才找得到舞台。許多上網人士批評國內的通訊環境跟先進國家比起來算是「二等公民」，實在是其來有自。讀到這裡，您應該比較容易了解為什麼會說IA跟WAP是「難兄難弟」了。這兩個東西在今天都有點生不逢時；理論上都是已經上市，而且是「即插即用」的產品和服務，但是因為成本和整體通訊架構的關係，大家卻好像都興趣缺缺。總而言之，這兩個東西今天用起來：所以連罹患「重度上網症」的我，也還是提不起興趣。也許有一天，會有人忽然想通：系統廠商的錢要從廠商那邊賺，也許考慮一下暫時不要急著「使用者付費」（或是改成「使用者付一點點費」）。消費者的錢即使先付給了網路上的餐廳、旅行社、內容供應商這些可以看到「立即效應」的商家；不過只要有點耐心，終究還是會回到系統商的手上；不過以其間促進貨幣流通、振興國家經濟的貢獻來說，功德可就十分無量了。fredjame.com",10/10/2000,1,0,0,"(700, 466)",1,0,0.0,1,zh-tw,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
34,Data Visualization vs Dashboards,Blog DP6,DP6 Team,467.0,4.0,636,"Atenção, este post foi produzido há algum tempo e pode estar desatualizado.(this text is also available in English)Em 2006 Hans Rosling fez uma das apresentações mais populares do TED, mostrando dados estatísticos de uma maneira visual e de fácil aprendizado. No mesmo ano, Al Gore ganhava o Oscar com seu alerta de aquecimento global no documentário “A Verdade Inconveniente”, em que usou e abusou de apresentações e gráficos para explicar sua teoria. Nas Olimpíadas de Londres, a equipe do NY Times deu um show visual mostrando como os recordes dos 100m rasos tem diminuído. O que esses 3 exemplos tem em comum? Visualização de dados.Para realizar decisões baseadas em dados é mais fácil utilizar indicadores visuais e gráficos com uma linguagem visual coerente. No entanto, é difícil encontrar visualizações que sejam coerentes de acordo com o contexto, com dados coletados com qualidade e soluções práticas de atualização, customização e exploração de dados. E em marketing digital, estamos falando de algumas premissas como:Foi com essa abordagem que investimos em uma área de Data Visualization (ou Dataviz, para abreviar), começando pela indicação de uma plataforma ideal para trabalhar com os dados de cada cliente, ajudando a padronizar e integrar as fontes de dados, além de configurar cada visualização de dados. Acima de tudo isso, em um ritmo de trabalho que estamos acostumados a realizar com as demandas de análise de dados, compatível com áreas de marketing.Algumas diferenças entre dashboards convencionais e nossa proposta de Data Visualization:Dashboard ConvencionalData VisualizationDados estáticosInteratividadeLayout fixoHTML5, mobilidadeBaixo nível de customizaçãoRelatórios e gráficos altamente customizáveisDados atualizados manualmenteFontes de dados com conectores automatizados (APIs, feeds, banco de dados) ou dados manuaisVisualização de base de dadosVisualização de desafios de negócioNa pesquisa da Gartner “Interactive Visualization Solutions for Data Analysis and Creation of Dashboards” de 2011, diversos fornecedores de ferramentas são divididos em 3 categorias:É possível observar o crescente número de empresas que tem surgido com soluções avançadas e, ao mesmo tempo, cada vez mais voltadas para equipes de negócio, do que áreas técnicas como TI. Desta forma, a dp6 realiza constantemente testes e pesquisas com fornecedores de tecnologia para estar sempre no limite e anuncia oficialmente duas parcerias:Bime Analytics (ex-Wearecloud) é uma das empresas francesas inovadoras de business intelligence baseado em software as a service — na nuvem — fornecendo uma solução de visualização e exploração de dados com muitos conectores automatizados para fontes de dados, como por exemplo Google Analytics, Google Docs, Sales Force, bancos de dados (Bigquery, MySQL, Oracle, SQL, Redshift), Dropbox, SAP, arquivos Excel, CSV, Facebook e muitos outros, para que seja possível focar na análise e visualização de dados de uma forma prática e com um banco de dados histórico de informações. A DP6 representa a plataforma no Brasil e América do Sul, fornecendo todos serviços necessários para visualizar seus dados da forma ideal. Clique aqui para conferir um exemplo de dashboard do Bime com dados do torneio de Wimbledon 2013.Sweetspot é uma plataforma que trabalha com o conceito de “Digital Insight Management”, indo além das fontes de dados (que podem ser totalmente customizadas) e dashboards, para prover uma plataforma de comunicação que melhore processos de decisões internos. A metodologia de trabalho da Sweetspot foca em:Com um portfolio de clientes como P&G, Philips, BBVA e Danone, a Sweetspot esteve conosco no evento eShow, onde palestramos sobre o tema de multicanalidade, atribuição e decisão baseada em dados:A nossa proposta de valor em Data Visualization é oferecer muito mais do que um simples dashboard estático, sendo mais rápidos e com melhor custo-benefício do que soluções convencionais de business intelligence. É visualização de dados com a cabeça de quem trabalha com analytics e está acostumado com demandas de marketing digital. Para conhecer mais, não deixe de entrar em contato conosco.Perfil do autor: Herman Fuchs | O(A) autor(a) desse conteúdo não faz mais parte da equipe DP6.",29/07/2013,0,6,8,"(375, 201)",6,3,0.0,11,pt,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,no emotion
35,How Variational Autoencoders make classical recommender systems obsolete.,Snipfeed,Quentin Bacuet,165.0,14.0,1627,"With the increase of information overload, it’s getting nearly impossible to acquire exciting items through a sea of content. That’s why the recommendation system is here to the rescue. Recommender systems are the models that help a user base explore new content such as music and news by showing them what they will find potentially interesting.Here at Snipfeed, we deal with thousands of content every day with a demanding user base: Gen Z. By leveraging state of the art deep-learning recommender systems, we help the users navigate through their favorite videos, news, quizzes, and podcasts.(The beta version of our app is coming soon, but you can try the messenger version for a sneak peek).As McKinsey estimates,“Already, 35 percent of what consumers purchase on Amazon and 75 percent of what they watch on Netflix come from product recommendations based on such algorithms.”With the growing popularity of the recommendation system, the questions emerge: what new models and algorithms can bring the recommendation to a new level? How well can they perform compared with more classic methods like matrix factorization?To answer the questions, I decided to compare nine methods and focus on two metrics: Normalized Discounted Cumulative Gain (NDCG) and the personalization index, using the infamous MovieLens dataset to conduct my experiments. I used TensorFlow and Keras to implement the models and trained them with the Google Colab Free GPU.For the analysis we will use the well-known dataset MovieLens 20M.This dataset contains more than 20 million ratings from MovieLens, a movie recommendation service. Below, a sample of the dataframe:The dataset lists 138K users and over 27K movies. After cleaning and filtering (we only take the positive reviews) we have:We can also see from the histograms below that the majority of movies have under 5,000 ratings…… and the majority of users have rated under 500 movies.This is consistent with most of RS problems: very few users rated a lot of movies, and very few movies have a lot of ratings.We can build a click matrix from this data. A click matrix follows the format shown below. The cell on row u and column i contains a 1 if user u has interacted with item i, and 0 otherwise.We also define the click vector xᵤ as the vector obtained by taking the uth row of the click matrix.To assess the quality of our models, we will split the dataset into 3 subsets, one for training, one for validation, one for testing. We'll use the first subset to train the model, the second to select the best model during the training, and the last one to get the final metrics.As previously stated, we’ll be using two metrics to evaluate our models. The first will be the NDCG, which measures the quality and utility of the order of the items in our recommendations. We first need to define the Discounted Cumulative Gain (DCG). The higher the DCG is, the better. The DCG@p is defined as:I being the indicator function, elemᵢ the ith item in the ordered recommendation, and test the set of items that we wish to obtain in our recommendations (our target, if you will). To illustrate this abstract formula, here’s a short example:Recommendation expected: {A,B,C}Note that the recommendations have an order. Hence we have: DCG₁ > DCG₂, as the first two items in our prediction 1 are items we were targeting, whereas these items are at the end of the list for our prediction 2.The NDCG is the normalized cousin of the DCG, which means we’re projecting the scores between 0 and 1 so they translate between models.The personalization index measures how unique recommendations are to a user. It computes the distance between each pair of recommendations, and then the average of that. To compare different personalization indices, we normalize them (like we did for the NDCG, we project the scores between 0 and 1). To illustrate this metric, let’s look at the example below:Recommendation 1:Recommendation 2:Recommendation systems can be divided into 2 categories: collaborative, and content-based filtering.Collaborative filtering is a sub-family of RS based on user-similarity. It makes predictions on the interests of user u by analyzing the tastes of users which are close to u. It’s based on the hypothesis that tightly related users are more likely to enjoy the same type of content than dissimilar users.Content-based filtering is another category of RS based on user preferences and content similarity, which means it’s based on the idea that if you like item i, then you’ll be more likely to like items that resemble i than items that are different from it.The content-based method, as described above, uses item descriptions to find the closest items to what a user has already seen. I implemented this method to be as exhaustive as I could, but a dataset with few features is always a limit for that method. The MovieLens dataset only provides the movies’ genre.However, we’ve developed a simple method, described in the pseudo-code below:With dist(i,j), the distance between two items i and j. I used the cosine distance between vectors of genres.The NDCG is very low, which was expected as the number of features for each item is very limited.Memory-based recommendation is a simple method to compute the similarity between users and items. As opposed to model-based methods, memory-based recommendation has no parameters to optimize. It’s a very simple algorithm that can be summarized into those few lines of code:In our case, we implemented the algorithm with:Non-negative matrix factorization (NMF) is a well-known algorithm for recommendation systems that arose during the Netflix contest. The idea is to decompose the click matrix into two lower-dimension rectangular matrices, one for the users and one for the items, “embedded” into vectors of computable dimensions (we call that a latent space). Multiplying these two matrices back together results in a new matrix, with values close to the original click matrix where they existed, and all the gaps filled in with (hopefully) good predictions.Neural Matrix Factorization (NeuMF) is a new approach that attempts to generalize the classic NMF seen above. It was developed in this paper. The model takes two integers (two indices) as inputs representing the item i and user u, and output a number between 0 and 1. The output represents the probability that the user u will be interested in item i. The architecture of the Neural Net(NN) can be split into two parts: the Matrix Factorization part and the fully-connected part. These parts are concatenated, and then passed on to a sigmoid layer.Below, we added the learning curve of the validation set of the NDCG@100 against the epochs. Even though I tried to regularize with lots of different parameters, over-fitting was unavoidable.Restricted Boltzmann machines (RBM) are a generative stochastic artificial neural network with a very simple architecture (one input layer and one hidden layer) that can be used to learn a probability distribution over the inputs, in our case the click vectors.Below, we added the learning curve of the validation set of the NDCG@100 along the epochs.Deep collaborative is a straight-forward collaborative model that aims to predict the most useful items for an user. The input is a user’s click vector, and the raw output is our recommendation. To train this model, I used 70% of the user’s click vector as input (the 30% left is replaced with 0s), and the rest as output.The architecture is simple. There’s an input and an output of the same size (#items), and multiple hidden layers of the same size (1000 neurons).Below, same as usual (NDCG@100 on the validation set during learning):Autoencoders (AE) were initially used to learn a representation of the data (encoding). They decompose into two parts: the encoder, which reduces the shape of the data with a bottleneck, and the decoder, that transforms the encoding back into its original form. As there is a dimension reduction, the NN will need to learn a representation in lower dimension of the input (the latent space) to be able the reconstruct the input. In the context of RS, they can be used to predict new recommendation. To do so, the input and the output are both the click vector (it is usual for AE that the input and the output are the same) and we will use dropout after the input layer. This means that the model will have to reconstruct the click vector as some element from the input will be missing, hence learning to predict the recommendation for a given click vector.Below, we added the learning curve of the validation set of the NDCG@100 along the epochs. Even though we tried to regularize with a lot of different parameters, it quickly overfit.Variational Autoencoders (VAE) are an extension of AE. Instead of having a simple dense layer for the bottleneck, it will have a sampling layer. This layer will use the mean and variance from the last layer of the encoder to get a Gaussian sample and use it as input for the decoder. Same for the AE we use dropout for the first layer.Below, that same old NDCG@100 on the validation set during learning.Hybrid models offer the best of both worlds (memory-based and model-based approaches), and are therefore very popular in RS.To implement the hybrid method, I chose to use a VAE and then average its results with memory-based results.We can now compare all our models. The best model when looking at the NDCG@100 is the VAE. For the personalization index, it’s the RBM.New methods like VAE, AE, or Deep Collaborative outperform classical methods like NMF on the NDCG metric. Non-linear probabilistic models such as variational autoencoders enable us to go beyond the limited modeling capacity of linear factor models.In my next post, I will do a deep dive into the VAE implementation for recommender systems with code and illustrations, so stay tuned !",02/04/2019,2,54,13,"(563, 352)",23,34,0.0,8,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
36,Image classification using transfer learning (VGG-16),Nerd For Tech,Muhammad Rizwan Munawar,60.0,7.0,915,"Before starting, you must need to have some knowledge regarding convolutional neural networks implementation with tensorflow/Keras. I have already written an article on convolutional neural networks, which you can look at that from link.We know these days image classification is becoming popular and its applications are increasing rapidly. In this blog, we will use convolutional neural networks for image classification on skin cancer data.“we will start with google colab because there no issue with python libraries their dependencies and also its cloud base environment so we will not need a lot of configuration.”Note: Let’s start implementation, if you will follow step by step tutorial then there will be no error at the end.Step-1: we need to create a folder in google drive with the name “image classification”. This is not a necessary name you can create a folder with another name as well.Step-2: Now, we need to make a folder of the “dataset” inside the image classification folder in which we will store our training and testing data. This is not a necessary name you can create a folder with another name as well.You can use any dataset but in this article, I will focus on binary classification, which means the dataset I will use have two classes. for multi-class classification, the procedure will be the same, but at some steps little changing needed, which I will tell in every step mentioned below.Step-3: Now, we need to add data inside the “dataset” folder, you can use any dataset, while the dataset I have used is from Kaggle and the data is regarding “skin cancer binary classification”. you can download the dataset from the link.Step-4: Now, we need to make a notebook inside the “image classification” folder, because we will write code inside that file and also will able to access the dataset from google drive.you can open the “image classification” folder and then clickNew->More->Google Colaboratory (process for making google colab file in folders)Now, we have set the dataset path and notebook file created. let start with a code for classifying cancer in the skin.Step-5: Open google colab file, Here we first need to mount google drive for accessing the dataset stored in the “image classification” folder. You can use the below-written code to mount google drive.once you run the above code. It will ask you for an authorization code, once you add that, your google drive will be mounted.Note: google drive and google colab account must be the same for authorization. If the google account changed then google drive will not mount.Step-6: Now, we need to import libraries for dataset reading and CNN (convolutional neural network) model creation.Step-7: Now, we need to set the path of training, testing, and validation directories. You can use only (test and train folders), validation folder usage is not necessary.Note: you can select a path by clicking on a folder in the left vertical tab->drive->My Drive->Folder PathStep-8: Now, we need data from these folders with the help of the os library.Until now, our colab notebook has four cells containing code as shown in the image below.Step-9: Now, let’s take a look at, how many training and testing images we have in our dataset?Step-10: Now, we need to set the size (height, width) of images. This step mostly needs when dataset images have different sizes, it will speed up the training process. I used an image shape of (224,224).Note: we need to resize images to (224,224) because VGG-16 only accepts that image size.Step-11: Now, we need to preprocess data (train, test, validation), which includes, rescaling and shuffling.Step-12: Before proceeding down, let check class names, image data generator will use folder names as class names.Now, Our Data preprocessing steps are completed, it’s time to download VGG-16 pre-trained weights.Step-13: Let’s download VGG-16 weights, by including the top layer parameter as false.We know VGG-16 is trained with many classes, so if we use (top_layer = True), then we need to retrain it on all classes at which VGG-16 trained, but if we use (top_layer = False), then in retraining, we only need to add our training classes.Step-14: Now, we need to freeze the training layers of VGG-16. (because VGG-16, already trained on huge data).Step-15: As all layers of VGG-16 are frozen, we need to modify the last layer for our classes. I added one max polling, one dense layer, one dropout, and one output with the last layer of VGG-16.for “Multiclass classification”, change the last dense layer value with 3, and activation with softmax.Step-16: Now, we need to merge the original VGG-16 layers, with our custom layers.Step-17: let’s compile the model, before starting training.for “Multiclass classification”, change the loss with categorical_crossentropy.Step-18: Let’s check the model summary.Step-19: Finally, we need to start our training process.Note: I trained the model on five epochs. For better results, 50–60 epochs can be tested, in order to achieve 85% accuracy on testing data.If, you followed all the above steps, then now, you can able to see epochs running after step-19 code also shown in the below picture.Step-20: Now, we can test our model on testing data.Step-21: Now, we need to save our model.Note: Congratulations, you have retrained VGG-16 on your own data. This article only focuses on binary classification, while you can test on your own data (binary or multiclass classification).If you have videos and want to develop a dataset from these videos, read out my articles regarding these,If you have data and want to label that for object detection, object tracking, etc, read out my article regarding that,",29/05/2021,17,47,11,"(632, 213)",13,2,0.0,5,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,subjective,neutral,joy/calmness
37,Visualizing discussions on Reddit with a D3 network and Embedly,Embedly Notes,Embedly,1500.0,4.0,656,"Recently, we had an Embedly hack week where internally we played with ideas to make something cool. I made a Reddit discussion network visualization, powered by D3 with previews generated by Embedly jQuery, and UI by Foundation.I’ve been using Reddit a lot lately. My introduction to user-generated-link-site-addiction was Hacker News. When I moved onto Reddit, I first noticed how deeply nested conversations were. I found that interesting, especially because sometimes these deeply nested threads turned out hilarious. I like playing with social data, and figured I could visualize this nested structure as a network by linking comments to what they are referring to- as a new way to browse Reddit. At the same time I could see how different conversations are structured.Getting Reddit data is fairly simple. You can see a previous post where we analyze subreddits. You just add a “.json” to the URL. Given a discussion, I recursively crawl the json response and pull all of the comments, noting the username, ID, parent, and body of the comment.The D3 force directed layout needs an array of nodes and links, so I add those as the comments are parsed. The size of the node is based on the score. The original poster (OP) is orange, and all other nodes are black unless they comment more than once, in which case they are given a color.To improve the user experience, I added link previews withEmbedly jQuery and buttons for the Reddit Front Page. Each link from the front page is run through Embedly to get an embed of the link. A hover event for each button displays the respective preview.There are also link previews for the comments. On hovering over a comment, the body is parsed for link text. The link text is replaced with a URL (just wrap it with an `a` tag), and used to get the embed preview. You can see an example of this in the Arnold Schwarzenegger AMA below.One common pattern for deeply nested threads is that a user shows up in alternating responses to their comment, reflecting a dialogue. You can see this in the thread below the original post (large orange node) below. Click the image to open up the network.You can also see how conversations can get ‘derailed’ and focus on the top comment thread, instead of the original post, as seen in the thread to the right of the first post.AskReddit’s have much more comment upvoting than other discussions. You can see this by how large the nodes are.AMA’s, as expected, have lots of comments from OP, as well as upvoting. Here is the top AMA from the last year:and here is the infamous Morgan Freeman one. It was far less engaging, and you can see the difference:And while we’re on AMAs, here is the Arnold Schwarzenegger one. I point it out because he used a unique answering method of handwriting the responses and posting them onto imgur.This is a recent TIL I liked. You can see quite a few double (or more) comments by the same user along the threads. This was a particularly popular one, so there is more upvoting on this one than I’ve normally seen.Here is one of the higher scoring /r/javascript posts. As well as being much smaller ,it is also focused much more towards dialogue. More users post more than once, with comments responding back and forth, highlighted by the node colors.Here is the top post over the past year in /r/programming. It is showcasing a project. It looks like an AMA given the amount of times OP responds.If you haven’t already, you can play around with the network here. Paste a link to the comments section of a reddit submission to see the network. The observations above are from looking at it over the past few days. I’d love to hear other things you notice from browsing reddit networks, and of course, here is the code, everything is client side.",05/08/2013,0,0,0,"(486, 450)",9,0,0.0,10,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
38,"I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers",,Xiaohan Zeng,5300.0,13.0,2529,"In the five days from July 24th to 28th 2017, I interviewed at LinkedIn, Salesforce Einstein, Google, Airbnb, and Facebook, and got all five job offers.It was a great experience, and I feel fortunate that my efforts paid off, so I decided to write something about it. I will discuss how I prepared, review the interview process, and share my impressions about the five companies.I had been at Groupon for almost three years. It’s my first job, and I have been working with an amazing team and on awesome projects. We’ve been building cool stuff, making impact within the company, publishing papers and all that. But I felt my learning rate was being annealed (read: slowing down) yet my mind was craving more. Also as a software engineer in Chicago, there are so many great companies that all attract me in the Bay Area.Life is short, and professional life shorter still. After talking with my wife and gaining her full support, I decided to take actions and make my first ever career change.Although I’m interested in machine learning positions, the positions at the five companies are slightly different in the title and the interviewing process. Three are machine learning engineer (LinkedIn, Google, Facebook), one is data engineer (Salesforce), and one is software engineer in general (Airbnb). Therefore I needed to prepare for three different areas: coding, machine learning, and system design.Since I also have a full time job, it took me 2–3 months in total to prepare. Here is how I prepared for the three areas.While I agree that coding interviews might not be the best way to assess all your skills as a developer, there is arguably no better way to tell if you are a good engineer in a short period of time. IMO it is the necessary evil to get you that job.I mainly used Leetcode and Geeksforgeeks for practicing, but Hackerrank and Lintcode are also good places. I spent several weeks going over common data structures and algorithms, then focused on areas I wasn’t too familiar with, and finally did some frequently seen problems. Due to my time constraints I usually did two problems per day.Here are some thoughts:This area is more closely related to the actual working experience. Many questions can be asked during system design interviews, including but not limited to system architecture, object oriented design，database schema design，distributed system design，scalability, etc.There are many resources online that can help you with the preparation. For the most part I read articles on system design interviews, architectures of large-scale systems, and case studies.Here are some resources that I found really helpful:Although system design interviews can cover a lot of topics, there are some general guidelines for how to approach the problem:With all that said, the best way to practice for system design interviews is to actually sit down and design a system, i.e. your day-to-day work. Instead of doing the minimal work, go deeper into the tools, frameworks, and libraries you use. For example, if you use HBase, rather than simply using the client to run some DDL and do some fetches, try to understand its overall architecture, such as the read/write flow, how HBase ensures strong consistency, what minor/major compactions do, and where LRU cache and Bloom Filter are used in the system. You can even compare HBase with Cassandra and see the similarities and differences in their design. Then when you are asked to design a distributed key-value store, you won’t feel ambushed.Many blogs are also a great source of knowledge, such as Hacker Noon and engineering blogs of some companies, as well as the official documentation of open source projects.The most important thing is to keep your curiosity and modesty. Be a sponge that absorbs everything it is submerged into.Machine learning interviews can be divided into two aspects, theory and product design.Unless you are have experience in machine learning research or did really well in your ML course, it helps to read some textbooks. Classical ones such as the Elements of Statistical Learning and Pattern Recognition and Machine Learning are great choices, and if you are interested in specific areas you can read more on those.Make sure you understand basic concepts such as bias-variance trade-off, overfitting, gradient descent, L1/L2 regularization，Bayes Theorem，bagging/boosting，collaborative filtering，dimension reduction, etc. Familiarize yourself with common formulas such as Bayes Theorem and the derivation of popular models such as logistic regression and SVM. Try to implement simple models such as decision trees and K-means clustering. If you put some models on your resume, make sure you understand it thoroughly and can comment on its pros and cons.For ML product design, understand the general process of building a ML product. Here’s what I tried to do:Here I want to emphasize again on the importance of remaining curious and learning continuously. Try not to merely using the API for Spark MLlib or XGBoost and calling it done, but try to understand why stochastic gradient descent is appropriate for distributed training, or understand how XGBoost differs from traditional GBDT, e.g. what is special about its loss function, why it needs to compute the second order derivative, etc.I started by replying to HR’s messages on LinkedIn, and asking for referrals. After a failed attempt at a rock star startup (which I will touch upon later), I prepared hard for several months, and with help from my recruiters, I scheduled a full week of onsites in the Bay Area. I flew in on Sunday, had five full days of interviews with around 30 interviewers at some best tech companies in the world, and very luckily, got job offers from all five of them.All phone screenings are standard. The only difference is in the duration: For some companies like LinkedIn it’s one hour, while for Facebook and Airbnb it’s 45 minutes.Proficiency is the key here, since you are under the time gun and usually you only get one chance. You would have to very quickly recognize the type of problem and give a high-level solution. Be sure to talk to the interviewer about your thinking and intentions. It might slow you down a little at the beginning, but communication is more important than anything and it only helps with the interview. Do not recite the solution as the interviewer would almost certainly see through it.For machine learning positions some companies would ask ML questions. If you are interviewing for those make sure you brush up your ML skills as well.To make better use of my time, I scheduled three phone screenings in the same afternoon, one hour apart from each. The upside is that you might benefit from the hot hand and the downside is that the later ones might be affected if the first one does not go well, so I don’t recommend it for everyone.One good thing about interviewing with multiple companies at the same time is that it gives you certain advantages. I was able to skip the second round phone screening with Airbnb and Salesforce because I got the onsite at LinkedIn and Facebook after only one phone screening.More surprisingly, Google even let me skip their phone screening entirely and schedule my onsite to fill the vacancy after learning I had four onsites coming in the next week. I knew it was going to make it extremely tiring, but hey, nobody can refuse a Google onsite invitation!LinkedInThis is my first onsite and I interviewed at the Sunnyvale location. The office is very neat and people look very professional, as always.The sessions are one hour each. Coding questions are standard, but the ML questions can get a bit tough. That said, I got an email from my HR containing the preparation material which was very helpful, and in the end I did not see anything that was too surprising. I heard the rumor that LinkedIn has the best meals in the Silicon Valley, and from what I saw if it’s not true, it’s not too far from the truth.Acquisition by Microsoft seems to have lifted the financial burden from LinkedIn, and freed them up to do really cool things. New features such as videos and professional advertisements are exciting. As a company focusing on professional development, LinkedIn prioritizes the growth of its own employees. A lot of teams such as ads relevance and feed ranking are expanding, so act quickly if you want to join.Salesforce EinsteinRock star project by rock star team. The team is pretty new and feels very much like a startup. The product is built on the Scala stack, so type safety is a real thing there! Great talks on the Optimus Prime library by Matthew Tovbin at Scala Days Chicago 2017 and Leah McGuire at Spark Summit West 2017.I interviewed at their Palo Alto office. The team has a cohesive culture, and work life balance is great there. Everybody is passionate about what they are doing and really enjoys it. With four sessions it is shorter compared to the other onsite interviews, but I wish I could have stayed longer. After the interview Matthew even took me for a walk to the HP garage :)GoogleAbsolutely the industry leader, and nothing to say about it that people don’t already know. But it’s huge. Like, really, really HUGE. It took me 20 minutes to ride a bicycle to meet my friends there. Also lines for food can be too long. Forever a great place for developers.I interviewed at one of the many buildings on the Mountain View campus, and I don’t know which one it is because it’s HUGE.My interviewers all look very smart, and once they start talking they are even smarter. It would be very enjoyable to work with these people.One thing that I felt special about Google’s interviews is that the analysis of algorithm complexity is really important. Make sure you really understand what Big O notation means!AirbnbFast expanding unicorn with a unique culture and arguably the most beautiful office in the Silicon Valley. New products such as Experiences and restaurant reservation, high end niche market, and expansion into China all contribute to a positive prospect. Perfect choice if you are risk tolerant and want a fast growing, pre-IPO experience.Airbnb’s coding interview is a bit unique because you’ll be coding in an IDE instead of whiteboarding, so your code needs to compile and give the right answer. Some problems can get really hard.And they’ve got the one-of-a-kind cross functional interviews. This is how Airbnb takes culture seriously, and being technically excellent doesn’t guarantee a job offer. For me the two cross functionals were really enjoyable. I had casual conversations with the interviewers and we all felt happy at the end of the session.Overall I think Airbnb’s onsite is the hardest due to the difficulty of the problems, longer duration, and unique cross-functional interviews. If you are interested, be sure to understand their culture and core values.FacebookAnother giant that is still growing fast, and smaller and faster-paced compared to Google. With its product lines dominating the social network market and big investments in AI and VR, I can only see more growth potential for Facebook in the future. With stars like Yann LeCun and Yangqing Jia, it’s the perfect place if you are interested in machine learning.I interviewed at Building 20, the one with the rooftop garden and ocean view and also where Zuckerberg’s office is located.I’m not sure if the interviewers got instructions, but I didn’t get clear signs whether my solutions were correct, although I believed they were.By noon the prior four days started to take its toll, and I was having a headache. I persisted through the afternoon sessions but felt I didn’t do well at all. I was a bit surprised to learn that I was getting an offer from them as well.Generally I felt people there believe the company’s vision and are proud of what they are building. Being a company with half a trillion market cap and growing, Facebook is a perfect place to grow your career at.This is a big topic that I won’t cover in this post, but I found this article to be very helpful.Some things that I do think are important:All successes start with failures, including interviews. Before I started interviewing for these companies, I failed my interview at Databricks in May.Back in April, Xiangrui contacted me via LinkedIn asking me if I was interested in a position on the Spark MLlib team. I was extremely thrilled because 1) I use Spark and love Scala, 2) Databricks engineers are top-notch, and 3) Spark is revolutionizing the whole big data world. It is an opportunity I couldn’t miss, so I started interviewing after a few days.The bar is very high and the process is quite long, including one pre-screening questionnaire, one phone screening, one coding assignment, and one full onsite.I managed to get the onsite invitation, and visited their office in downtown San Francisco, where Treasure Island can be seen.My interviewer were incredibly intelligent yet equally modest. During the interviews I often felt being pushed to the limits. It was fine until one disastrous session, where I totally messed up due to insufficient skills and preparation, and it ended up a fiasco. Xiangrui was very kind and walked me to where I wanted to go after the interview was over, and I really enjoyed talking to him.I got the rejection several days later. It was expected but I felt frustrated for a few days nonetheless. Although I missed the opportunity to work there, I wholeheartedly wish they will continue to make greater impact and achievements.From the first interview in May to finally accepting the job offer in late September, my first career change was long and not easy.It was difficult for me to prepare because I needed to keep doing well at my current job. For several weeks I was on a regular schedule of preparing for the interview till 1am, getting up at 8:30am the next day and fully devoting myself to another day at work.Interviewing at five companies in five days was also highly stressful and risky, and I don’t recommend doing it unless you have a very tight schedule. But it does give you a good advantage during negotiation should you secure multiple offers.I’d like to thank all my recruiters who patiently walked me through the process, the people who spend their precious time talking to me, and all the companies that gave me the opportunities to interview and extended me offers.Lastly but most importantly, I want to thank my family for their love and support — my parents for watching me taking the first and every step, my dear wife for everything she has done for me, and my daughter for her warming smile.Thanks for reading through this long post.You can find me on LinkedIn or Twitter.Xiaohan Zeng10/22/17PS: Since the publication of this post, it has (unexpectedly) received some attention. I would like to thank everybody for the congratulations and shares, and apologize for not being able to respond to each of them.This post has been translated into some other languages:It has been reposted in Tech In Asia.Breaking Into Startups invited me to a live video streaming, together with Sophia Ciocca.CoverShr did a short QnA with me.",23/10/2017,0,9,0,"(185, 105)",7,7,0.0,19,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,positive,joy/calmness
39,K-Means Clustering: From A to Z,Towards Data Science,Azika Amelia,244.0,7.0,1175,"Data is essential for data science (as if the name isn’t suggestive enough). With tons of data being generated every millisecond, it’s no surprise that most of this data is unlabeled. But that’s okay, because there are different techniques available to make do with unlabeled datasets. In fact, there’s an entire domain of Machine Learning called “Unsupervised Learning” that deals with unlabeled data.Sometimes we just want to see how the data is organized, and that’s where clustering comes into play. Even though it’s mostly used of unlabeled data, but it works just fine for labeled data as well. The word ‘clustering’ means grouping similar things together. The most commonly used clustering method is K-Means (because of it’s simplicity).This post explains how K-Means Clustering work (in depth), how to measure the quality of clusters, choose the optimal number of K, and mentions other clustering algorithms.Imagine you’re opening a small book store. You have a stack of different books, and 3 bookshelves. Your goal is place similar books in one shelf. What you would do, is pick up 3 books, one for each shelf in order to set a theme for every shelf. These books will now dictate which of the remaining books will go in which shelf.Every time you pick a new book up from the stack, you would compare it with those first 3 books, and place this new book on the shelf that has similar books. You would repeat this process until all the books have been placed.Once you’re done, you might notice that changing the number of bookshelves, and picking up different initial books for those shelves (changing the theme for each shelf) would increase how well you’ve grouped the books. So, you repeat the process in hopes of a better outcome.K-Means algorithm works something just like this.K-means clustering is a good place to start exploring an unlabeled dataset. The K in K-Means denotes the number of clusters. This algorithm is bound to converge to a solution after some iterations. It has 4 basic steps:You don’t have to start with 3 clusters initially, but 2–3 is generally a good place to start, and update later on.As a starting point, you tell your model how many clusters it should make. First the model picks up K, (let K = 3) datapoints from the dataset. These datapoints are called cluster centroids.Now there are different ways you to initialize the centroids, you can either choose them at random — or sort the dataset, split it into K portions and pick one datapoint from each portion as a centriod.From here on wards, the model performs calculations on it’s own and assigns a cluster to each datapoint. Your model would calculate the distance between the datapoint & all the centroids, and will be assigned to the cluster with the nearest centroid. Again, there are different ways you can calculate this distance; all having their pros and cons. Usually we use the L2 distance.The picture below shows how to calculate the L2 distance between the centeroid and a datapoint. Every time a datapoint is assigned to a cluster the following steps are followed.Because the initial centroids were chosen arbitrarily, your model the updates them with new cluster values. The new value might or might not occur in the dataset, in fact, it would be a coincidence if it does. This is because the updated cluster centorid is the average or the mean value of all the datapoints within that cluster.Now if some other algo, like K-Mode, or K-Median was used, instead of taking the average value, mode and median would be taken respectively.Since step 2 and 3 would be performed iteratively, it would go on forever if we don’t set a stopping criterion. The stopping criterion tells our algo when to stop updating the clusters. It is important to note that setting a stopping criterion would not necessarily return THE BEST clusters, but to make sure it returns reasonably good clusters, and more importantly at least return some clusters, we need to have a stopping criterion.Like everything else, there are different ways to set the stopping criterion. You can even set multiple conditions that, if met, would stop the iteration and return the results. Some of the stopping conditions are:The goal here isn’t just to make clusters, but to make good, meaningful clusters. Quality clustering is when the datapoints within a cluster are close together, and afar from other clusters.The two methods to measure the cluster quality are described below:You have to specify the number of clusters you want to make. There are a few methods available to choose the optimal number of K. The direct method is to just plot the datapoints and see if it gives you a hint. As you can see in the figure below, making 3 clusters seems like a good choice.Other method is to use the value of inertia. The idea behind good clustering is having a small value of inertia, and small number of clusters.The value of inertia decreases as the number of clusters increase. So, its a trade-off here. Rule of thumb: The elbow point in the inertia graph is a good choice because after that the change in the value of inertia isn’t significant.When you’ve formed a cluster, you give a name it, and all the datapoints in that cluster are assigned this name as their label. Now your dataset has labels! You can perform testing using these labels. To find insights about your data, you can see what similarity do the datapoints within a cluster have, and how does it differ from other clusters.Once you’ve finalized your model, it can now assign a cluster to a new data point. The method of assigning cluster remains same, i.e., assigning it to the cluster with the closet centroid.It’s important to preprocess your data before performing K-Means. You would have to convert your dataset into numerical values if it is not already, so that calculations can be performed. Also, applying feature reduction techniques would speed up the process, and also improve the results. These steps are important to follow because K-Means is sensitive to outliers, just like every other algo that uses average/mean values. Following these steps alleviate these issues.I used to be very intimidated by clustering and any unsupervised algorithm in general, because I knew very little about it. I remember the first time I had to use K-Means for a music recommendation engine that I was working on, I kept thinking how would I test the final model without the labels. In this post, I tired to pack every important thing about K-Means, and mentioned the alternatives. I hope this article helps you, if you ever find yourself in the situation that I was in.If you found this article helpful please give it a clap 👏 Clapping makes a post accessible to more people. 🐦New to data science? Give this article a read! 📖If you have any questions or suggestions, feel free to post it down below. You may also connect with me on Linkedin. 💼Until then Peace out. ✌",27/09/2018,0,5,0,"(558, 411)",8,3,0.0,6,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,subjective,positive,joy/calmness
40,Let’s cluster data points using DBSCAN,,Vibhor Agarwal,30.0,5.0,579,"Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a very popular density based data clustering algorithm commonly used in data mining and machine learning. DBSCAN clusters the data points to separate the areas of high density with the areas of low density. It also marks data points as outliers that are in the low density regions. The clusters formed can be of varying shapes based on the density of data points.Let’s take an example of e-commerce. Suppose an e-commerce company wants to improve its sales by recommending relevant products to its customers. The relevant products are suggested based on the products already purchased or searched by the customers. All the customers are clustered based on the items they have purchased or searched for. Then the products purchased by one customer is suggested to another customer only if they both lie in the same cluster that is, they are somewhat similar to each other. That’s one application where clustering can be used.This article will cover the following topics:The DBSCAN algorithm captures the dense regions as clusters and mainly requires two parameters for finding clusters:The MinPts = 4 means minimum 4 points are required to form a dense cluster. Also, a pair of points must be separated by a distance of less than or equal to Eps to be considered as neighbors. Based on the above two parameters, data points are classified into 3 categories as follows:Initially, the algorithm begins by selecting a point randomly uniformly from the set of data points. Checks if the selected point is a core point. Again, a point is a core point if it contains at least MinPoints number of minimum points in its epsilon-neighborhood.Then, finds the connected components of all the core points, ignoring non-core points.Assign each non-core point to the nearest cluster if the cluster is its epsilon-neighbor. Otherwise, assign it to noise.The algorithm stops when it explores all the points one by one and classifies them as either core, border or noise point.The DBSCAN algorithm is very commonly used clustering algorithm these days. We shall see DBSCAN implementation using sklearn — python below.2. Load the dataset. Note that dataset will not contain any labels since clustering is unsupervised learning algorithm. Here we’re creating a dummy dataset for the demonstration purpose.3. Instantiate the DBSCAN model. Here, epsilon is 3 and minimum number of points required to form a cluster is 2.4. Fit the model with data in array X.5. Print the predicted labels for each data point. Here, the same predicted labels represent the data points belonging to the same clusters.Note that [1, 2], [2, 2] and [2, 3] have same label = 0. It means they belong to same cluster. While other points with label = 1 belong to other cluster. One point: [25, 8] has label = -1. It’s a noise point (an outlier). That’s why it doesn’t belong to any cluster.In this article, we’ve looked at detailed explanation of DBSCAN density-based clustering algorithm, its two important parameters and advantages and disadvantages of using it. Then, we looked at its implementation in python using scikit-learn.I hope this article will help you in understanding DBSCAN clustering algorithm and its basic implementation.Hope it was easy to follow this blog. For any further discussion on this article or any other project, please leave your comments below or drop me a message on LinkedIn or Twitter. I’ll be 😄 to discuss.Thanks for reading!If you like this article, please clap, clap, clap… 👏 👏 👏",27/06/2019,5,46,6,"(468, 291)",4,7,0.0,5,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,contempt/reluctance
41,Word to Vectors — Natural Language Processing,Towards Data Science,Shubham Agarwal,376.0,5.0,897,"Why is Natural Language Processing difficult?Computers interact with humans in programming languages which are unambiguous, precise and often structured. However, natural (human) language has a lot of ambiguity. There are multiple words with same meaning (synonyms), words with multiple meanings (polysemy) some of which are entirely opposite in nature (auto-antonyms), and words which behave differently when used as noun and verb. These words make sense contextually in natural language which humans can comprehend and distinguish easily, but machines can’t. That’s what makes NLP one of the most difficult and interesting tasks in AI.What can be accomplished using NLP?There are several tasks which can be accomplished by enabling computers to “understand” human language. One live example is this software that I am using to check spellings and grammar in this article. Here are some tasks which are being done presently using NLP:How to represent words?To start off, we need to be able to represent words as input to our Machine Learning models. One mathematical way of representing words is as vectors. There are an estimated 13 million words in English language. But many of these are related. Spouse to partner, hotel to motel. So do we want separate vectors for all 13 million words?No. We must search for a N-dimensional vector space (where N << 13 million) that is sufficient to encode all semantics in our language. We need to have a sense of similarity and difference between words. We can exploit concept of vectors and distances between them (Cosine, Euclidean etc. ) to find similarities and differences between words.How do we represent meaning of words?If we use separate vectors for all 13 million words (or maybe more) in English vocabulary, we’ll be facing several problems. Firstly, we’ll have large vectors with a lot of ‘zeroes’ and one ‘one’ (in different position representing a different word). This is also known as one-hot encoding. Secondly, when we search for phrases such as “hotels in New Jersey” in Google, we want results pertaining to “motel”, “lodging”, “accommodation” in New Jersey returned as well. And if we are using one-hot encoding, these words have no natural notion of similarity. Ideally, we would want dot products (since we are dealing with vectors) of synonym / similar words to be close to one.Distributional similarity based representations:You shall know a word by the company it keeps — J. R. FirthIn very simple layman terms, let’s take a word ‘bank’. One of many meanings of this word is a financial institution and another one is land alongside a body of water. If in a sentence, bank occurs with neighboring words as money, government, treasury, interest rates etc. we can understand it’s the former meaning. Contrarily, if neighboring words are water, shore, river, land etc. the case is latter. We can exploit this concept to deal with polysemy and synonyms and make our model learn.Word2VecHow can we build simple, scalable, fast to train models which can run over billions of words that will produce exceedingly good word representations? Let’s look into Word2Vec model to find answer to this.Word2Vec is a group of models which helps derive relations between a word and its contextual words. Let’s look at two important models inside Word2Vec: Skip-grams and CBOWSkip-gramsIn Skip-gram model, we take a centre word and a window of context (neighbor) words and we try to predict context words out to some window size for each centre word. So, our model is going to define a probability distribution i.e. probability of a word appearing in context given a centre word and we are going to choose our vector representations to maximize the probability.Continuous Bag of Words model (CBOW)In abstract terms, this is opposite of skip-gram. In CBOW, we try to predict centre word by summing vectors of surrounding words.This was about converting words into vectors. But where does the “learning” happen? Essentially, we begin with small random initialization of word vectors. Our predictive model learns the vectors by minimizing the loss function. In Word2vec, this happens with a feed-forward neural network and optimization techniques such as Stochastic gradient descent. There are also count-based models which make a co-occurrence count matrix of words in our corpus; we have a large matrix with each row for the “words” and columns for the “context”. The number of “contexts” is of course large, since it is essentially combinatorial in size. To overcome this size issue, we apply SVD to the matrix. This reduces the dimensions of the matrix retaining maximum information.In summary, converting words into vectors, which deep learning algorithms can ingest and process, helps to formulate a much better understanding of natural language. Novelist EL Doctorow has expressed this idea quite poetically in his book ‘Billy Bathgate’.It’s like numbers are language, like all the letters in the language are turned into numbers, and so it’s something that everyone understands the same way. You lose the sounds of the letters and whether they click or pop or touch the palate, or go ooh or aah, and anything that can be misread or con you with its music or the pictures it puts in your mind, all of that is gone, along with the accent, and you have a new understanding entirely, a language of numbers, and everything becomes as clear to everyone as the writing on the wall. So as I say there comes a certain time for the reading of the numbers.",18/05/2017,0,11,1,"(700, 459)",2,1,0.0,1,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,surprise/amazement
42,Understand how works Resnet… without talking about residual,,Pierre Guillou,1000.0,2.0,109,"I was 99% satisfied with all blog posts I found about Resnet (for example the great blog post “Decoding the ResNet architecture”), but not at 100%.Then, I searched in the Web and found, I thought, this 1% in the blog post “Understanding Advanced Convolutional Neural Networks”.In the Resnet paragraph, the author (Mohit Deshpande) explains the beauty of Resnet in 3 points :You can even forget the vanishing gradient problem and just look at an image of a Resnet network : the identity matrix transmits forward the input data that avoids the loose of information (the data vanishing problem).That’s it :-)Link to my post in the Fastai forum : http://forums.fast.ai/t/thread-for-blogs-just-created-one-for-resnet/7359/205?u=pierreguillou",20/04/2018,0,5,1,"(700, 305)",1,1,0.0,6,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
43,How to get 4x speedup and better generalization using the right batch size,Towards Data Science,Daniel Huynh,158.0,13.0,2301,"Once upon a Tweet, I came across this conversation from Jeremy Howard quoting Yann LeCun about batch sizes :As this subject was something that has always been in some part of my mind since I came across the very nice learning rate finder from Fastai, I always wondered if there could be a useful batch size finder, that people could use to quickly start training their model with a good batch size.As a reminder, the learning rate finder used in Fastai helps to find the right learning rate by testing different learning rates to find which one gives the sharpest decrease in loss. A more detailed explanation can be found here: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.htmlThis idea of having a batch size finder was in my mind for a long time, and after getting the motivation from Jeremy, I have decided to go on this journey of implementing a batch size finder for training Neural Networks.I want to share today both the journey and the destination, of implementing a paper, as they are both interesting in my opinion, and maybe will motivate you to try more stuff as well!I. A story of sizeOne common perception is that you should not use large batch sizes, because this will only cause the model to overfit, and you might run out of memory. While the latter is obviously true, the former is more complicated than that, and to answer it, we will have a little dive into an OpenAI paper “ An Empirical Model of Large-Batch Training”.This paper, which I recommend reading, explains many simple ideas that are good to remember.First, our goal is to minimize a loss through a Stochastic Gradient Descent approach, and there is a true underlying landscape upon which we will minimize it. Nonetheless, we do not have access to the true gradient over the entire dataset (or more precisely over the distribution it is drawn from), therefore we must approximate the gradient with a finite batch size.Because we average over a batch, if our batch size is small, there will be a lot of noise present, and we might train our model only on noise. Nonetheless, applying several successive updates will push in the right direction, but we might as well have directly used a larger batch size, which is more computationally efficient and directly averages out the noise. Nonetheless, after a certain size, if your gradient is already accurate, there is no point in making the batch size even bigger, because it will just be a computational waste as there will be little gain in accuracy.Moreover, by using bigger batch sizes (up to a reasonable amount that is allowed by the GPU), we speed up training, as it is equivalent to taking a few big steps, instead of taking many little steps. Therefore with bigger batch sizes, for the same amount of epochs, we can sometimes have a 2x gain in computational time!Second, there is a statistic called “Simple Noise Scale”, which helps us determine what would be a good batch size, and it is defined as :with G being the real gradient of our loss L, over the n parameters.Without going too much into the details of the paper as it is thoroughly explained, the idea is if we use a batch size smaller than the Simple Noise Scale, we could speed up training, by increasing the batch size, and on the opposite, if we use a too large batch size bigger than the Simple Noise Scale, we will just waste computational power.To understand more what this statistic means, let us study each term :Therefore, the noisier our gradient is, the bigger batch size we want, which is natural, as we want to take gradient steps in the right direction. On the opposite, if the gradient is not noisy, we will benefit more from taking smaller steps, as we do not need to average out a lot of observations and use them separately.On the other hand, the closer we are to the minimum, the bigger the batch size, as we are expected to take more careful steps the closer we are to a local minimum as we not want to overshoot it and miss the right direction.Finally, the Simple Noise Scale gives us a tool to answer the question “Bigger batch sizes will make us overfit, while smaller batch sizes help regularize ” :Not necessarily! If your task is already complex, and the approximate gradient will be noisy, it might be in your interest to have a bigger batch size to make sure your model is not training on too much noise. It’s not as if a bigger batch size will make you overfit, it’s more that a smaller batch size will add more regularization through the noise injecting, but do you want to add regularization if you can not even fit properly?II. Implementing the paperSo now that we have seen a little bit why choosing the right batch size matters, and how we could find a good batch size through the Simple Noise Scale statistic, it’s now time to implement it! Yummy!Just to remember, the Simple Noise Scale equation is :The problem with this is that not only do we need to know the real gradient but also we need to know the variance of this gradient, which makes it more difficult. To tackle this issue, the authors have proposed two different statistics to approximate both the numerator and the denominator of the Simple Noise Scale.Here, we use two different batch sizes, B big and B small, to compute two different estimators of the real gradient using the formula :Once we have those two approximates, we can finally compute the Simple Noise Scale with the formula :To make sure this estimator has low variance, the authors computed several Simple Noise Scale estimators throughout the training, and have averaged it.As explained in the paper, one natural way to do this is make use of the several GPUs to compute the local gradient of each GPU, which will be the small gradient, and then compare it to the average gradient across the different GPUs, which will be the big gradient. Nonetheless, this method assumes we have a multi GPU, which is not the case for most of us.Therefore, an efficient way to implement this for single GPU has to be found, and it was not described in the original paper. So that’s where I have started and I will now share you my reasoning on how to solve this issue!The code used in the rest of the article can be found here: https://colab.research.google.com/drive/15lTG_r03yqSwShZ0JO4XaoWixLMXMEmvIn the first lines of codes, I set up a Fastai environment to run a model on MNIST, as this dataset was already tested in the paper, and they got an average Simple Noise Scale of 900.I will not explain the code too much into detail as it will take me a whole article to explain how Fastai puts everything together with their API, but the code should be a good start. If you want further help to understand the code, tell me in the comments and I can explain it, or even write an article on the coding part.A. First approach using an exponential moving averageGiven that I found the proposed statistics in the paper not really helpful because I did not have a multi-GPU setup, I thought that I could skip it, and simply compute directly the sum of variances, by doing approximations :First I have approximated the real gradient with the estimated gradient for a given batch.Then, as the computation of the Covariance matrix can be seen as two averages, I have tried to approximate it with an exponential moving average, as I did not want to store many gradients across training.As you can see here, the results are weird, the Simple Noise Scale is way too bumpy, the noise is much greater than the noise, which gives a very negative Simple Noise Scale which does not make sense.B. Storing gradientsWe saw that using the exponential moving average is not a good idea to approximate the covariance matrix.Another way to tackle this issue is simply to set in advance a number N of gradients to keep, then we will simply compute N different gradients, and use those N gradients to approximate the covariance matrix.It starts to show results, but the way it is computed is tricky: the x-axis is the number of batches I stored to compute the Simple Noise Scale in this fashion. Though it seems to provide some kind of result, it is not usable in practice as I have stored hundreds of gradients!C. Run two trainingsAfter failing another time, I have decided that I will follow the idea of the paper, and compute their two statistics. Nonetheless I need to have a way to get two batches of different sizes during training while I had only one GPU.Then I thought, why do a single training epoch, when I could run do two training epochs with two different batch sizes and compute it afterwards?So I went with this idea, and used B big=2 * B small, which will allow me to compute their respective gradient, and use them to compute G and S in an exponential moving average manner as in described in the paper.Ouch! As in the same approach as the first one, it yielded weird results! Moreover, when I think about it, the batches I get might not be the same between the two runs, as nothing forces the small batch to be included in the big batch. In addition, I need to run two training epochs to compute this so it was not good.D. Sequential batchesFinally, I realized that the best approach seemed to be the second one, but something had to be modified because I did not want to keep tons of gradients to compute the statistic.Then a very simple, but effective thought came to mind: what if instead of averaging several batches in a parallel fashion as they did in the paper, I instead average consecutive batches in a sequential way?This simply means that I will just need to set a parameter, that I call n_batch, which is the number of batch I have to store before computing the big and small gradient, and then I will be able to compute the statistics of the paper in a sequential way!After implementing it this way, I have the following result :Now ain’t she a beauty! In the paper, they described that the growing trend is to be expected, as the noise will more likely remain the same, while the scale of the gradient will decrease as we get closer to a minimum, which will lead to a growing Simple Noise Scale.Because we most likely did not have the same setup, nor did I have access to their code, our results slightly diverge, but in the paper, the authors mentioned a Simple Noise Scale starting at around 50, and reaching 900, which is what matters. Given the many approximations that come to play both theoretically, and in practice, results can vary, but as explained in the paper, there should not be variations of more than an order of magnitude.So after this long journey, there seems to be an implementation that is working, even though the paper gave little help to do so, and the best part is, to use it in practice, you only need one line of code!Here the parameters correspond to :III. Testing the batch size finder on different tasksNow that we have an implementation working, it could be interesting to have a look at how it helps in practice to find a good batch size.First, we will study the Rossmann dataset. This data set has been explored in Fastai courses v3, which you can find here: https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson6-rossmann.ipynbHere I will simply run my batch size finder, and do the exact same training as the original, but with a batch size taking account the Simple Noise Scale.Now how to interpret this? This means, that for this given learning rate, the training seems to converge to a Simple Noise Scale of around 500, i.e. the noise and the scale stabilize later in the training. Therefore, the best tradeoff between computing time and efficiency seems to be having a batch size of 512.After running the same training with batch sizes 512 and 64, there are a few things we can observe.With a batch size 512, the training is nearly 4x faster compared to the batch size 64! Moreover, even though the batch size 512 took fewer steps, in the end it has better training loss and slightly worse validation loss.Then if we look at the second training cycle losses for each batch size :We can see here that training is much bumpier with a batch size 64, compared to batch size 512, which is not overfitting, as the validation loss continues to decrease.Finally, we can observe the following results for the last training cycle :So in the end, if we sum up the results on Rossmann, using a batch size of 512 instead of 64I have looked into text and image data, but given that they are much heavier, especially with pretrained models with huge bodies, when I tried running training with batch sizes I ran into CUDA out of memory, so I will not show the results here but you can have a look on the Colab Notebook.ConclusionWe have seen a lot of things throughout this article! I hope you enjoyed the ride, and if there are things you have to remember it would be :So I hope you enjoyed reading this article, would be great to have your feedback and I will try to post more articles in the future.If you have any question, do not hesitate to contact me on Linkedin , you can also find me on Twitter !",04/11/2019,0,30,11,"(453, 225)",23,4,0.0,8,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
44,GAN 2.0: NVIDIA’s Hyperrealistic Face Generator,SyncedReview,Synced,24000.0,4.0,713,"Look at the two pictures below. Can you tell which is a photograph and which was generated by AI?The truth is… wait for for it… both images are AI-generated fakes, products of American GPU producer NVIDIA’s new work with generative adversarial networks (GANs). The research was published today in the paper A Style-Based Generator Architecture for Generative Adversarial Networks, which proposes a new generator architecture that has achieved state-of-the-art performance in face generation.Since GANs were introduced in 2014 by Google Researcher Ian Goodfellow, the tech has been widely adopted in image generation and transfer. After some early wiry failures, GANs have made huge breakthroughs and can now produce highly convincing fake images of animals, landscapes, human faces, etc. Researchers know what GANs can do, however a lack of transparency in their inner workings means GAN improvement is still achieved mainly through trial-and-error. This allows only limited control over the synthesized images.The NVIDIA paper proposes an alternative generator architecture for GAN that draws insights from style transfer techniques. The system can learn and separate different aspects of an image unsupervised; and enables intuitive, scale-specific control of the synthesis.Here’s how it works: Given an input facial image, the style-based generator can learn its distribution and apply its characteristics on a novel synthesized image. While previous GANs could not control what specific features they wanted to regenerate, the new generator can control the effect of a particular style — for example high-level facial attributes such as pose, identity, shape — without changing any other features. This enables better control of specific features such as eyes and hair styles. Below is a video demo of how GAN-generated images vary from one to another given different inputs and styles.Behind the new feature is a technique NVIDIA calls “style-mixing.” From the paper: “To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training. When generating such an image, we simply switch from one latent code to another — an operation we refer to as style mixing — at a randomly selected point in the synthesis network.”Stochastic variation is another key property allowing GANs to realize the randomization of detailed facial features, such as the placement of facial hair, stubble density, freckles, pores, etc. The paper proposes adding per-pixel noise after each convolution layer. The added noise does not affect the overall composition or the high-level attributes of images, and changing noise in different layers produces matching stochastic variation results.To quantify interpolation quality and disentanglement, the paper proposes two new, automated methods — perceptual path length and linear separability — that are applicable to any generator architecture.Researchers saw impressive results using the new generator to forge images of bedrooms, cars, and cats with the Large-scale Scene Understanding (LSUN) dataset.Alongside today’s paper, NVIDIA has also released a huge new dataset of human faces. Flickr- Faces-HQ (FFHQ) contains 70,000 high-quality images at 1024 resolution. The dataset will soon be available to the public.The paper’s first author is Tero Karras, a principal research scientist at NVIDIA Research with a primary research interest in deep learning, generative models, and digital content creation. His paper Progressive Growing of GANs for Improved Quality, Stability, and Variation, or known as ProgressiveGAN has won accolades and was accepted by ICLR 2018.Synced, as a natural fan of deep learning and GAN, has noticed that more than a few papers on GANs have picked up momentum and prompted discussions this year. DeepMind researchers proposed BigGAN two months ago, and the model achieved an Inception Score (IS) of 166.3 — a more than 100 percent improvement over the previous state of the art (SotA) result of 52.52. Meanwhile a team of Tsinghua University and Cardiff University researchers introduced CartoonGAN to simulate the styles of Japanese anime maestri from snapshots of real world scenery.Click this link to view the new NVIDIA paper.Journalist: Tony Peng | Editor: Michael Sarazen2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon. Apply for Insight Partner Program to get a complimentary full PDF report.Follow us on Twitter @Synced_Global for daily AI news!We know you don’t want to miss any stories. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates.",15/12/2018,0,17,6,"(700, 426)",4,0,0.0,10,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
45,Clustering electricity usage profiles with K-means,Towards Data Science,Luciano Guivant Viola,113.0,4.0,625,"Machine Learning has a wide range of applications for the energy sector. A very exciting one is extracting insights into electricity consumption behavior. The way in which an individual or family uses energy across the day is also known as “energy fingerprint”.In this article, we will go through how to find patterns in the daily load profiles of a single household with the K-means clustering algorithm.The dataset contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). You can find it here.The plot above shows all the daily-load profiles of 1456 days plotted together. We can see two clear patterns of consumption behavior by looking at the darker regions (where more curves are concentrated).K-means is an unsupervised machine learning algorithm in which the number of clusters has to be defined a priori. This leaves the question of how many clusters to pick.A common method to address this is to use the silhouette value. It is a measure of how similar a point is to its own cluster compared to other clusters. It ranges from -1 to 1, where a high value indicates that a point has a good match with the cluster it belongs.We take the average of the silhouette across all load-profiles in order to have a global view of how the algorithm is performing.I experiment with a range of cluster numbers (from 2 to 30). It is important to scale each period within the same range so that the magnitude of the energy load does not interfere in the selection of the cluster.The maximum average silhouette occurs when there are only 2 clusters, but to better illustrate this example, I choose 3. Let’s see how they look:As we can see, K-means found three unique groups of load-profiles.The green cluster contains loads that maintain a steady use of energy throughout the afternoon. Maybe these are days where the occupants stayed at home, like weekends and special dates.The blue cluster has a high peak in the morning, a decline in usage during the afternoon and high again at night. This pattern seems to fit business days when occupants leave for work and/or school.Finally, the red cluster shows days when consumption is low throughout the whole day. Maybe a case of holidays when only a few appliances are left on?One way we can validate the results of the clustering algorithm is to use a form of dimensionality reduction and plot the points in a 2D plane. Then, we can color them according to the cluster they belong.A popular algorithm for this purpose is called t-SNE. The inner workings of the algorithm are beyond the scope of this article, but a very good explanation can be found here.The thing to keep in mind is that t-SNE doesn’t know anything about the clusters found by K-means.In the above plot, each point represents a daily load-profile. They were reduced from 24 to 2 dimensions. Theoretically, the distance between points in the higher dimensional space was preserved, so points that are close together refer to similar load-profiles. The fact that most blue, red and green points are close together is an indication that the clustering worked well.This article presented a way to find clusters of electricity usage with the K-means algorithm. We used the silhouette score to find the optimal number of clusters and t-SNE to validate the results.As for next steps, we could try different clustering algorithms. Scikit-learn has a bunch of them to explore. Some don’t require the choice of the number of clusters to be made a priori.Another interesting application would be to extend this model to different households and find clusters of similar energy consumption behavior across families.I hope you enjoyed it! If you have any comments and/or suggestions feel free to contact me.",17/09/2018,0,0,0,"(700, 450)",5,0,0.0,10,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
46,Neural networks and back-propagation explained in a simple way,DataThings,Assaad MOAWAD,975.0,15.0,2802,"Any complex system can be abstracted in a simple way, or at least dissected to its basic abstract components. Complexity arises by the accumulation of several simple layers. The goal of this post, is to explain how neural networks work with the most simple abstraction. We will try to reduce the machine learning mechanism in NN to its basic abstract components. Unlike other posts that explain neural networks, we will try to use the least possible amount of mathematical equations and programming code, and focus only on the abstract high level concepts.A supervised neural network, at the highest and simplest representation, can be presented as a black box with 2 methods learn and predict as following:The learning process takes the inputs and the desired outputs and updates its internal state accordingly, so the calculated output get as close as possible to the desired output. The predict process takes an input and generate, using the internal state, the most likely output according to its past “training experience”. That’s why machine learning is called sometimes model fitting.In order to achieve this, we will decompose the learning process into its several building blocks:The easiest example to start with neural network and supervised learning, is to start simply with an input and an output and a linear relation between them. The goal of the supervised neural network is to try to search over all possible linear functions which one fits the best the data. Take for instance the following dataset:For this example, it might seems very obvious to you that the output = 2 x input, however it is not the case for most of the real datasets (where the relationship between the input and output is highly non-linear and not that obvious).The first step of the learning, is to start from somewhere: the initial hypothesis. Like in genetic algorithms and evolution theory, neural networks can start from anywhere. Thus a random initialization of the model is a common practice. The rational behind is that from wherever we start, if we are perseverant enough and through an iterative learning process, we can reach the pseudo-ideal model.In order to give an analogy, take for instance a person who has never played football in his life. The very first time he tries to shoot the ball, he can just shoot it randomly. Similarly, for our numerical case study, let’s consider the following random initialization: (Model 1): y=3.x. The number 3 here is generated at random. Another random initialization can be: (Model 2): y=5.x, or (Model 3): y=0,5.x.We will explore later, how, through the learning process, all these models can converge to the ideal solution (y=2.x) (which we are trying to find).In this example, we are exploring which model of the generic form y=W.x can fit the best the current dataset. Where W is called the weights of the network and can be initialized randomly. These types of models are simply called linear layers.The next natural step to do after initializing the model at random, is to check its performance. We start from the input we have, we pass them through the network layer and calculate the actual output of the model straightforwardly.This step is called forward-propagation, because the calculation flow is going in the natural forward direction from the input -> through the neural network -> to the output.At this stage, in one hand, we have the actual output of our randomly initialized neural network. On the other hand, we have the desired output we would like the network to learn. Let’s put them both in the same table.If we compare this to our football player shooting for the first time, the actual output will be the final position of the ball, the desired output would be that the ball goes inside the goal. In the beginning, our player is just shooting randomly. Let’s say the ball went most of the time, to the right side of the goal. What he can learn from this, is that he needs to shoot a bit more to the left next time he trains.In order to be able to generalize to any problem, we define what we call: loss function. Basically it is a performance metric on how well the NN manages to reach its goal of generating outputs as close as possible to the desired values.The most intuitive loss function is simply loss = (Desired output — actual output). However this loss function returns positive values when the network undershoot (prediction < desired output), and negative values when the network overshoot (prediction > desired output). If we want the loss function to reflect an absolute error on the performance regardless if it’s overshooting or undershooting we can define it as:loss = Absolute value of (desired — actual ).If we go back to our football player example, if our newbie guy shoots the ball 10m to the right or 10m to the left of the goal, we consider, in both cases, that he missed its target by 10m regardless the direction (right or left). In this case we will add a new column to the table -> the absolute error.However, several situations can lead to the same total sum of errors: for instance, lot of small errors or few big errors can sum up exactly to the same total amount of error. Since we would like the prediction to work under any situation, it is more preferable to have a distribution of lot of small errors, rather than a few big ones.In order to encourage the NN to converge to such situation, we can define the loss function to be the sum of squares of the absolute errors (which is the most famous loss function in NN). This way, small errors are counted much less than large errors! (the square of 2 is 4, but the square of 10 is 100! So an error of 10, is penalised 25 times more than an error of 2 — not only 5 times!). Our table becomes the following:Notice how, if we consider only the first input 0, we can say that the network predicted correctly the result!However, this is just the beginner’s luck in our football player analogy who can manage to score from the first shoot as well. What we care about, is to minimize the overall error over the whole dataset (total of the sum of the squares of the errors!).As a summary, the loss function is an error metric, that gives an indicator on how much precision we lose, if we replace the real output by the actual output generated by our trained neural network model. That’s why it’s called loss!Simply speaking, the machine learning goal becomes then to minimize the loss function (to reach as close as possible to 0).We can just transform our machine learning problem now to an optimization process that aims to minimize this loss function.Obviously we can use any optimization technique that modifies the internal weights of neural networks in order to minimize the total loss function that we previously defined. These techniques can include genetic algorithms or greedy search or even a simple brute-force search:In our simple numerical example, with only one parameter of weight to optimize W, we can search from -1000.0 to +1000.0 step 0.001, which W has the smallest sum of squares of errors over the dataset.This might works if the model has only very few parameters and we don’t care much about precision. However, if we are training the NN over an array of 600x400 inputs (like in image processing), we can reach very easily models with millions of weights to optimize and brute force can’t be even be imaginable, since it’s a pure waste of computational resources!Luckily for us, there is a powerful concept in mathematics that can guide us how to optimize the weights called differentiation. Basically it deals with the derivative of the loss function. In mathematics, the derivative of a function at a certain point, gives the rate or the speed of which this function is changing its values at this point.In order to see the effect of the derivative, we can ask ourselves the following question: how much the total error will change if we change the internal weight of the neural network with a certain small value δW. For the sake of simplicity will consider δW=0.0001. in reality it should be even smaller!.Let’s recalculate the sum of the squares of errors when the weight W changes very slightly:Now as we can see from this table, if we increase W from 3 to 3.0001, the sum of squares of error will increase from 30 to 30.006. Since we know that the best function that fits this model is y=2.x, increasing the weights from 3 to 3.0001 should obviously create a little bit more error (because we are going further from the intuitive correct weight of 2. We have: 3.0001 > 3 > 2 thus the error is higher). But what we really care about is the rate of which the error changes relatively to the changes on the weight. Basically here this rate is the increase of 0.006 in the total error for each 0.0001 increasing weight -> that’s a rate of 0.006/0.0001 = 60x! It works in both direction, so basically if we decrease the weights by 0.0001, we should be able to decrease the total error by 0.006 as well! Here is the proof, if we run again the calculation at W=2.9999, we get an error of 29.994. We managed to decrease the total error by 0.006, we learned something! we improved the model!We could have guessed this rate by calculating directly the derivative of the loss function. The advantage of using the mathematical derivative is that it is much faster and more precise to calculate (less floating point precision problems).Here is what our loss function looks like:If we initialize randomly the network, we are putting any random point on this curve (let’s say w=3) . The learning process is actually saying this:- Let’s check the derivative.- If it is positive, meaning the error increases if we increase the weights, then we should decrease the weight.- If it’s negative, meaning the error decreases if we increase the weights, then we should increase the weight.- If it’s 0, we do nothing, we reach our stable point.In a simple matter, we are designing a process that acts like gravity. No matter where we randomly initialize the ball on this error function curve, there is a kind of force field that drives the ball back to the lowest energy level of ground 0.In this example, we used only one layer inside the neural network between the inputs and the outputs. In many cases, more layers are needed, in order to reach more variations in the functionality of the neural network.For sure, we can always create one complicated function that represent the composition over the whole layers of the network. For instance, if layer 1 is doing: 3.x to generate a hidden output z, and layer 2 is doing: z² to generate the final output, the composed network will be doing (3.x)² = 9.x². However in most cases composing the functions is very hard. Plus for every composition one has to calculate the dedicated derivative of the composition (which is not at all scalable and very error prone).In order to solve the problem, luckily for us, derivative is decomposable, thus can be back-propagated.We have the starting point of errors, which is the loss function, and we know how to calculate its derivative, and if we know how to calculate the derivative of each function from the composition, we can propagate back the error from the end to the start.Let’s consider the simple linear example: where we multiply the input 3 times to get a hidden layer, then we multiply the hidden (middle layer) 2 times to get the output.input -> 3.x -> 2.x -> output.A 0.001 delta change on the input, will be translated to a 0.003 delta change after the first layer, then to 0.006 delta change on the output.which is the case if we compose both functions into one:input -> 6.x -> output.Similarly an error on the output of 0.006, can be back-propagated to an error of 0.003 in the middle hidden stage, then to 0.001 on the input.If we create a library of differentiable functions or layers where for each function we know how to forward-propagate (by directly applying the function) and how to back-propagate (by calculating the derivative of the function), we can compose any complex neural network. We only need to keep a stack of the function calls during the forward pass and their parameters, in order to know the way back to back-propagate the errors using the derivatives of these functions. This can be done by de-stacking through the function calls. This technique is called auto-differentiation, and requires only that each function is provided with the implementation of its derivative. In a future blog post, we will explain how to accelerate auto-differentiation by implementing basic mathematical operations over vectors/matrices/and tensors.In neural network, any layer can forward its results to many other layers, in this case, in order to do back-propagation, we sum the deltas coming from all the target layers. Thus our linear calculation stack can become a complex calculation graph.This figure shows the process of back-propagating errors following this schemas:Input -> Forward calls -> Loss function -> derivative -> back-propagation of errors. At each stage we get the deltas on the weights of this stage.As we presented earlier, the derivative is just the rate of which the error changes relatively to the weight changes. In the numerical example presented earlier, this rate is 60x. Meaning that 1 unit of change in weights leads to 60 total units of change in the loss function.And since we know that the loss error is currently at 30 units, by extrapolating the rate, in order to reduce the error to 0, we need to reduce the weights by 0.5 units.However, for real-life problems we shouldn’t update the weights with such big steps. Since there are lot of non-linearity, any big change in weights will lead to a chaotic behavior. We should not forget that the derivative is only local at the point where we are calculating the derivative.Thus as a general rule of weight updates is the delta rule:New weight = old weight — Derivative * learning rateThe learning rate is introduced as a constant (usually very small), in order to force the weight to get updated very smoothly and slowly (to avoid big steps and chaotic behaviour). (To remember: Learn slow and steady!)In order to validate this equation:Now several weight update methods exist. These methods are often called optimizers. The delta rule is the most simple and intuitive one, however it has several draw-backs. This excellent blog post presents the different methods available to update the weights.In the numerical example we presented here, we only have 5 input/output training set. In reality, we might have millions of entries. Previously, we were talking about minimizing the error cost function (the loss function) over the whole dataset. This is called batch learning, and might be very slow for big data. What we can do instead, is to to update the weights every BatchSize=N of training, providing that the dataset is shuffled randomly. This is called mini-batch gradient descend. And if N=1, we call this case full online-learning or stochastic gradient descent, since we are updating the weights after each single input output observed!Any optimizer can work with these 3 modes (full online/mini-batch/full-batch).Since we update the weights with a small delta step at a time, it will take several iterations in order to learn.This is very similar to genetic algorithms where after each generation we apply a small mutation rate and the fittest survives.In neural network, after each iteration, the gradient descent force updates the weights towards less and less global loss function.The similarity is that the delta rule acts as a mutation operator, and the loss function acts a fitness function to minimize.The difference is that in genetic algorithms, the mutation is blind. Some mutations are bad, some are good, but the good ones have higher chance to survive. The weight update in NN are however smarter since they are guided by the decreasing gradient force over the error.How many iterations are needed to converge?In order to summarize, Here is what the learning process on neural networks looks like (A full picture):An example and a super simple implementation of a neural network is provided in this blog post.In case you still have any questions, please do not hesitate to comment or contact me at: assaad.moawad@datathings.comI would always be glad to reply to you, improve this article, or collaborate if you have any idea in mind. If you enjoyed reading, follow us on: Facebook, Twitter, LinkedIn",01/02/2018,5,38,15,"(676, 401)",9,4,0.0,9,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
47,You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks (Part I),The DownLinQ,Adam Van Etten,1200.0,9.0,1646,"Detection of small objects over large swaths is one of the primary drivers of interest in satellite imagery analytics. Previous posts (4, 5) detailed efforts to localize boats in DigitalGlobe images using sliding windows and HOG feature descriptors. These efforts proved successful in both open water and harbor regions, though such techniques struggle in regions of highly non-uniform background. To address the shortcomings of classical object detection techniques we implement an object detection pipeline based upon the You Only Look Once framework. This pipeline (which we dub You Only Look Twice) greatly improves background discrimination over the HOG-based approach, and proves able to rapidly detect objects of vastly different scales and over multiple sensors.The ImageNet competition has helped spur rapid advancements in the field of computer vision object detection, yet there are a few key differences between the ImageNet data corpus and satellite imagery. Four issues create difficulties: in satellite imagery objects are often very small (~20 pixels in size), they are rotated about the unit circle, input images are enormous (often hundreds of megapixels), and there’s a relative dearth of training data (though efforts such as SpaceNet are attempting to ameliorate this issue). On the positive side, the physical and pixel scale of objects are usually known in advance, and there’s a low variation in observation angle. One final issue of note is deception; observations taken from hundreds of kilometers away can sometimes be easily fooled. In fact, the front page of The New York Times on October 13, 2016 featured a story about Russian weapon mock-ups (Figure 1).The HOG + Sliding Window object detection approach discussed in previous posts (4, 5) demonstrated impressive results in both open water and harbor (F1 ~ 0.9). Recall from Section 2 of 5 that we evaluate true and false positives and negatives by defining a true positive as having a Jaccard index (also known as intersection over union) of greater than 0.25. Also recall that the F1 score is the harmonic mean of precision and recall and varies from 0 (all predictions are wrong) to 1 (perfect prediction).To explore the limits of the HOG + Sliding Window pipeline, we apply it to a scene with a less uniform background and from a different sensor. Recall that our classifier was trained on DigitalGlobe data with 0.5 meter ground sample distance (GSD), though our test image below is a Planet image at 3m GSD.We adapt the You Only Look Once (YOLO) framework to perform object detection on satellite imagery. This framework uses a single convolutional neural network (CNN) to predict classes and bounding boxes. The network sees the entire image at train and test time, which greatly improves background differentiation since the network encodes contextual information for each object. It utilizes a GoogLeNet inspired architecture, and runs at real-time speed for small input test images. The high speed of this approach combined with its ability to capture background information makes for a compelling case for use with satellite imagery.The attentive reader may wonder why we don’t simply adapt the HOG + Sliding Window approach detailed in previous posts to instead use a deep learning classifier rather than HOG features. A CNN classifier combined with a sliding window can yield impressive results, yet quickly becomes computationally intractable. Evaluating a GoogLeNet-based classifier is roughly 50 times slower on our hardware than a HOG-based classifier; evaluation of Figure 2 changes from ~2 minutes for the HOG-based classifier to ~100 minutes. Evaluation of a single DigitalGlobe image of ~60 square kilometers could therefore take multiple days on a single GPU without any preprocessing (and pre-filtering may not be effective in complex scenes). Another drawback to sliding window cutouts is that they only see a tiny fraction of the image, thereby discarding useful background information. The YOLO framework addresses the background differentiation issues, and scales far better to large datasets than a CNN + Sliding Window approach.The framework does have a few limitations, however, encapsulated by three quotes from the paper:To address these issues we implement the following modifications, which we name YOLT: You Only Look Twice (the reason for the name shall become apparent later):“Our model struggles with small objects that appear in groups, such as flocks of birds”“It struggles to generalize objects in new or unusual aspect ratios or configurations”“Our model uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the original image”The output of the YOLT framework is post-processed to combine the ensemble of results for the various image chips on our very large test images. These modifications reduce speed from 44 frames per second to 18 frames per second. Our maximum image input size is ~500 pixels for NVIDIA GTX Titan X GPU; the high number of parameters for the dense grid we implement saturates the 12GB of memory available on our hardware for images greater than this size. It should be noted that the maximum image size could be increased by a factor of 2–4 if searching for closely packed objects is not required.Training data is collected from small chips of large images from both DigitalGlobe and Planet. Labels are comprised of a bounding box and category identifier for each object.We initially focus on four categories:We label 157 images with boats, each with an average of 3–6 boats in the image. 64 image chips with airplanes are labeled, averaging 2–4 airplanes per chip. 37 airport chips are collected, each with a single airport per chip. We also rotate and randomly scale the images in HSV (hue-saturation-value) to increase the robustness of the classifier to varying sensors, atmospheric conditions, and lighting conditions.With this input corpus training takes 2–3 days on a single NVIDIA Titan X GPU. Our initial YOLT classifier is trained only for boats and airplanes; we will treat airports in Part II of this post. For YOLT implementation we run a sliding window across our large test images at two different scales: a 120 meter window optimized to find small boats and aircraft, and a 225 meter window which is a more appropriate size for larger vessels and commercial airliners.This implementation is designed to maximize accuracy, rather than speed. We could greatly increase speed by running only at a single sliding window size, or by increasing the size of our sliding windows by downsampling the image. Since we are looking for very small objects, however, this would adversely affect our ability to differentiate small objects of interest (such as 15m boats) from background objects (such as a 15m building). Also recall that raw DigitalGlobe images are roughly 250 megapixels, and inputting a raw image of this size into any deep learning framework far exceeds current hardware capabilities. Therefore either drastic downsampling or image chipping is necessary, and we adopt the latter.We evaluate test images using the same criteria as Section 2 of 5, also detailed in Section 2 above. For maritime region evaluation we use the same areas of interest as in (4, 5). Running on a single NVIDIA Titan X GPU, the YOLT detection pipeline takes between 4–15 seconds for the images below, compared to the 15–60 seconds for the HOG + Sliding Window approach running on a single laptop CPU. Figures 6–10 below are as close to an apples-to-apples comparison between HOG + Sliding Window and YOLT pipeline as possible, though recall that the HOG + Sliding window is trained to classify the existence and heading of boats, whereas YOLT is trained to produce boat and airplane localizations (not heading angles). All plots use a Jaccard index detection threshold of 0.25 to mimic the results of 5.The YOLT pipeline performs well in open water, though without further post-processing the YOLT pipeline is suboptimal for extremely dense regions, as Figure 9 demonstrates. The four areas of interest discussed above all possessed relatively uniform background, an arena where the HOG + Sliding Window approach performs well. As we showed in Figure 2, however, in areas of highly non-uniform background the HOG + Sliding Window approach struggles to differentiate boats from linear background features; convolutional neural networks offer promise in such scenes.To test the robustness of the YOLT pipeline we analyze another Planet image with a multitude of boats (see Figure 11 below).A final test is to see how well the classifier performs on airplanes, as we show below.In this post we demonstrated one of the limitations of classical machine learning techniques as applied to satellite imagery object detection: namely, poor performance in regions of highly non-uniform background. To address these limitations we implemented a fully convolutional neural network classifier (YOLT) to rapidly localize boats and airplanes in satellite imagery. The non-rotated bounding box output of this classifier is suboptimal in very crowded regions, but in sparse scenes the classifier proves far better than the HOG + Sliding Window approach at suppressing background detections and yields an F1 score of 0.7–0.85 on a variety of validation images. We also demonstrated the ability to train on one sensor (DigitalGlobe), and apply our model to a different sensor (Planet). While the F1 scores may not be at the level many readers are accustomed to from ImageNet competitions, we remind the reader that object detection in satellite imagery is a relatively nascent field and has unique challenges, as outlined in Section 1. We have also striven to show both the success and failure modes of our approach. The F1 scores could possibly be improved with a far larger training dataset and further post-processing of detections. Our detection pipeline accuracy might also improve with a greater number of image chips, though this would also reduce the current processing speed of 20–50 square kilometers per minute for objects of size 10m — 100m.In Part II of this post, we will explore the challenges of simultaneously detecting objects at vastly different scales, such as boats, airplanes, and airstrips.May 29, 2018 Addendum: See this post for paper and code details.",07/11/2016,0,11,9,"(700, 563)",13,5,0.0,29,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
48,Black-Box models are actually more explainable than a Logistic Regression,Towards Data Science,Samuele Mazzanti,2300.0,8.0,1168,"Who works as a data scientist knows it better: one of the main clichés around Machine Learning is that you have to choose between:In the following, we will show that not only is there no need to choose between power and explainability, but that more powerful models are even more explainable than the shallower ones.By way of illustration, we will be using one of the most well-known datasets: the iconic Titanic dataset. We have a bunch of variables about Titanic passengers, and we want to predict how likely each passenger is to survive.The variables at our disposal are:(Note: for the sake of simplicity, some other variables have been dropped and almost no data preprocessing has been carried out).The data look like this:For what concerns classification problems, Logistic Regression is often taken as the baseline.After having one-hot encoded the qualitative features (Ticket class, Passenger sex and Port of embarkation), we fit a plain Logistic Regression on the training data. The accuracy, computed on the validation set, amounts to 81.56%.What insights can we get from this model? Since Logistic Regression is built asthe analysis revolves around the β’s. It is easy to see that for an increment of 1 unit in xⱼ the odds will increase by a factor of exp(βⱼ).The interpretation is straightforward: an increase of one year in Age will end up in a ×0.961 increase in odds.That is:The limitations of this model in terms of explainability are self-evident:Let’s now try a “black-box” model. In this example, we will be using Catboost, an algorithm for gradient boosting on decision trees.Performing a quick Catboost (without any hyper-parameter tuning) on the same training data (no need for one-hot encoding this time) results in an 87.15% accuracy on validation data.As we expected, Catboost has dramatically outperformed LogReg (87.15% vs. 81.56%). So far, no wonder.Now, the $64,000 question in Machine Learning is: if Catboost is so better than LogReg in predicting survival on unseen data, shouldn’t we just trust it and not worrying too much about the gears?Well, it depends. If it is a Kaggle competition, the answer may be “yes”, but in general it is “definitely not!”. If you are wondering why, just check out some high profile incidents such as Amazon “sexist AI recruitment tool” or Microsoft “racist chatbot”.So, in order to figure out what decisions Catboost is making, another tool, called SHAP (SHapley Additive exPlanations), comes to our aid. SHAP (probably the state of the art in Machine Learning explainability) was born in 2017 and it is a brilliant way to reverse-engineer the output of any predictive algorithm.Applying SHAP analysis to our data and the Catboost output produces a matrix, which has the same dimension of the original data matrix, containing the SHAP values. The SHAP values look like this:The higher the SHAP value the higher the probability of survival and vice versa. Moreover, a SHAP value greater than zero leads to an increase in probability, a value less than zero leads to a decrease in probability.Each SHAP value expresses, this is the important part here, the marginal effect that the observed level of a variable for an individual has on the final predicted probability for that individual.This means: suppose that for the first individual we know every variable except Age. Its SHAP sum is then -0.36 -0.76 +0.05 -0.03 -0.3 -0.08 = -1,48. Once we get to know the individual’s Age (also taking into account the interaction between Age and the other variables) we can update the sum: -1.48 +0.11 =-1.37.You can use the Python library dedicated to SHAP to obtain fancy plots (basically containing descriptive statistics about SHAP values). For instance, you can easily obtain a plot summarizing the SHAP values for each observation, broken down by feature. In our example:Can you see the elephant in the room?If you were to show this plot to the layman (or even your boss), he would probably say: “Pretty colors. But what is the ruler down there? Are those dollars? Kilograms? Years?”In a nutshell, SHAP values are not understandable for humans (not even for data scientists).The concept of probability is far easier to grasp. Actually, it is even more than that: it is innate (after all, this is why people keep taking planes).Wishing to pass from SHAP to probabilities, the most obvious thing to do is to plot the Predicted Probability of survival (for each individual) with respect to the SHAP sum (for each individual).It is immediately apparent that this is a deterministic function. That is, we can switch from one quantity to another without error. After all, the only difference between the two is that probability is bound to be in [0,1], while SHAP can assume any real number. Therefore:where f(.) is a monotonically increasing s-shaped function that maps any real number to the [0,1] interval (for simplicity f() could be a plain interpolation function bounded in [0,1]).Let use take an individual. Suppose that, knowing all the variables except Age, its SHAP sum equals to 0. Now imagine that the SHAP value relative to Age is 2.It is sufficient for us to know the function f() to quantify the impact of Age on Predicted Probability of survival: it is simply f(2)-f(0). In our case, it is f(2)-f(0) = 80%-36% = 44%No doubt that probability of survival is more digestible than SHAP of survival. What is more understandable: to say that this individual age leads to a rise of 2 in SHAP or to a rise of 44% in probability?Based on what we have just seen, starting from the SHAP matrix seen above, it is enough to apply the formula:to obtain the following:For instance, having a 3rd Class ticket reduces the survival probability of the first passenger by -4.48% (which corresponds to -0.36 SHAP). Notice that also passenger n. 3 and passenger n. 5 were in 3rd Class. Their impacts on probability (respectively -16.65% and -5.17%) are different due to the interaction with other features.Several analyses can be carried out on this matrix. We just report some plots as an example.The red line identifies the average effect (mean of Age effect for all individuals in a group), while the blue band (mean ± standard deviation) expresses the variability of Age effect between the individuals in the same group. The variability is due to the interaction between Age and the other variables.The value added of this method is that:Simple models such as Logistic Regression make a great deal of simplifications. Black-box models are more flexible and thus more adaptable to complex (but very intuitive) real world behaviours, such as non-linear relationships and interactions between variables.Explainability means expressing a model’s choices in a way that is intelligible for humans, based on their perception of reality (including complex behaviours).We have shown a way to translate SHAP values into probabilities. This makes it possible to visualize a black-box functioning and to ensure that it is consistent (both qualitatively and quantitatively) with our knowledge of the world: a world that is more faceted than the one depicted by simpler models.The code can be found here:github.comEnjoy!",21/09/2019,0,13,4,"(638, 354)",14,5,0.0,7,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
49,Steel Defect Detection — Image Segmentation using TensorFlow,Geek Culture,Akhil Penta,8.0,9.0,1029,"Steel Defect Detection is a competition hosted on Kaggle by one of the largest steel manufacturing company Severstal. Since the production process of flat sheet steel involves a line up of many sub-processes in which this flat sheet has to touch several machines by the time it’s ready to ship. So Severstal is using images from high-frequency cameras to power a defect detection algorithm. Through this competition, Severstal is expecting participants to improve the algorithm by localizing and classifying surface defects on a steel sheet.The objective of this competition is to predict the location and type of defects found in steel manufacturing using the images provided.18074(12568 Train images and 5506 Test images) images are provided with unique image ID’s. Each image may have no defects, a defect of a single class or defects of multiple class. There are four classes of defects.Along with the train and test images folder, train.csv is provided which contains the information about the type of defects and their respective locations of train images. These data points (7095 Nos.) of train.csv gives details about the defective images (6666 unique IDs).The 3 columns of the train.csv are ImageId, ClassId and EncodedPixels:i. ImageId: Unique Id for each image.ii. ClassId: Type/class Id of defect that image has at the pixel locations mentioned in the EncodedPixels column.iii. EncodedPixels: This column gives info about the pixel location of the defect in the image. The defective pixel locations lists are encoded to list of pairs of values (1st value of each pair represents start position and the 2nd value represents it’s run length) to reduce the file size. You can read more about it here.Snap view of first few lines of train.csv:It is an image segmentation problem, for a given image we need to detect and localize the defect in the steel sheet.This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty.[1]. Building a Model to Classify and Predict defects in Steel:[2]. Steel Defect Detection: Image Segmentation using Keras:[3]. Semantic Image Segmentation using Fully Convolutional Networks:[4]. A Detailed Case Study on Severstal: Steel Defect Detection, can we detect and classify defects in steel? — Beginner to Advanced!:[5]. Detection of Steel Defects: Image Segmentation using Keras and Tensorflow:From the EDA it is clear that these defective data points are highly imbalanced.Displaying a few Defective images:Displaying a few Non-Defective images:It looks like even some of the images from the Non-defective set(ImgaeIds that are not in train.csv) are defective. Maybe these defect classes are not in the ClassIds [1, 2, 3, 4].We formatted the provided train.csv so that each row contains ImageId and their corresponding mask encodings for all 4 defective classes as below.This whole data is split randomly with 85:15 ratio for training & Validation respectively.This U-Net architecture was originally developed for Biomedical Image Segmentation. This architecture mainly contains two paths: Encoder Path & Decoder Path. And the connections between Encoder & Decoder at each level is the key feature of this architecture.The Encoder path(or the contracting path) is a stack of convolution and max-pooling layers which helps in capturing the context in the image. And the Decoder Path(or the expanding path) is transposed-convolution and convolution layers which helps in localizing the captured contexts(in encoder path).Through those connections between encoder and decoder at each level are passing outputs of those encoder blocks and concatenating them to input tensors of the decoder blocks. This helps to reconstruct the image and bring it up to match the original size of the image.Implemented U-Net Architecture(referring to the work U-Net: Convolutional Networks for Biomedical Image Segmentation) using TensorFlow as below:This LinkNet architecture is almost similar to U-Net with a little modification. The Encoder and Decoder path construct remains same as in U-Net. But the outputs of the encoder blocks after passing through the connection gets added(concatenation in case of U-Net ) to the input tensors of the decoder blocks.Here the number of trainable parameters gets reduced, so LinkNet is a lighter weight network when compared to U-Net.Implemented LinkNet Architecture(referring to the work LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation) using TensorFlow as below:In this architecture, we are replacing the simple convolutions in each of the Encoder & Decoder blocks of U-Net architecture with Residual blocks i.e., added skip connection at each of the blocks.Refer to this article to understand and to get better insight about this architecture: UNet with ResBlock for Semantic Segmentation .Implemented(using TensorFlow) this architecture by replacing each of conv2D_block in UNet architecture with ResBlock as below:In this architecture, we are replacing(same as U-Net with ResNet Backbone) the simple convolutions in each of the Encoder & Decoder blocks of LinkNet architecture with Residual blocks i.e., added skip connection at each of the blocks.Implemented(using TensorFlow) this architecture by replacing each of conv2D_block in LinkNet architecture with ResBlock as below:Referring to the work UNet++: A Nested U-Net Architecture for Medical Image Segmentation, implemented U-Net++ Architecture. It’s a powerful architecture for image segmentation.In this architecture, we are generating decoder paths at every level of the encoder path. And we are making skip connection between all the blocks at each level. These help in reducing the semantic gap between the Encoder path and Decoder subpaths.Implemented U-Net++ Architecture using TensorFlow as below:Each of the above models is trained for 60 epochs with these compile configurations:Each of the trained models was uploaded to Kaggle and evaluated with both public and private test data. The results are tabulated below:Created a simple Flask application to deploy this trained U-Net ++ model:The trained Unet++ TensorFlow model is converted to TensorFlow Lite model using tf.lite.TFLiteConverter . By this, we reduced the size of the model by 3 times with a slight degradation of performance(almost negligible in our case). This technique is called as Post-Training Quantization. Read more about it here.You can find this quantization code-execution for our model here.Refer to my GitHub repo for the detailed code.github.comConnect with me on LinkedIn 🙂.",16/04/2021,4,24,1,"(639, 327)",10,9,0.0,36,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
50,What are RMSE and MAE?,Towards Data Science,Shwetha Acharya,35.0,5.0,545,"Root Mean Squared Error (RMSE)and Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. These metrics tell us how accurate our predictions are and, what is the amount of deviation from the actual values.Technically, RMSE is the Root of the Mean of the Square of Errors and MAE is the Mean of Absolute value of Errors. Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable. They are calculated as follows :On close inspection, you will see that both are average of errors.Let’s understand this with an example. Say, I want to predict the salary of a data scientist based on the number of years of experience. So, salary is my target variable (Y) and experience is the independent variable(X). I have some random data on X and Y and we will use Linear Regression to predict salary. Let’s use pandas and scikit-learn for data loading and creating linear model.Now, we have ‘yp’ — our array of salary prediction and we will evaluate our model by plotting predicted(yp) and actual salary(y). I am using bohek for my visualizations.From the graph above, we see that there is a gap between predicted and actual data points. Statistically, this gap/difference is called residuals and commonly called error, and is used in RMSE and MAE. Scikit-learn provides metrics library to calculate these values. However, we will compute RMSE and MAE by using the above mathematical expressions. Both methods will give you the same result.This is our baseline model. MAE is around 5.7 — which seems to be higher. Now our goal is to improve this model by reducing this error.Let’s run a polynomial transformation on “experience” (X) with the same model and see if our errors reduce.I have used Scikit-learn PolynomialFeatures to create a matrix of 1, X, and X2 and passed that as input to my model.Calculating our error metrics and ….Voilaaa… they are much lower this time. It fits better than our baseline model! Let’s plot y and yp (like how we did before) to check the overlap.The gap between the 2 lines has reduced. Let’s observe the distribution of residuals or errors(y-yp) using seaborn’s residual plot functionWe see that residuals tend to concentrate around the x-axis, which makes sense because they are negligible.There is a third metric — R-Squared score, usually used for regression models. This measures the amount of variation that can be explained by our model i.e. percentage of correct predictions returned by our model. It is also called the coefficient of determination and calculated by the formula:Let’s compute R2 mathematically using the formula and using sklearn library and compare the values. Both methods should give you the same result.Thus, overall we can interpret that 98% of the model predictions are correct and the variation in the errors is around 2 units. For an ideal model, RMSE/MAE=0 and R2 score = 1, and all the residual points lie on the X-axis. Achieving such a value for any business solution is almost impossible!Some of the techniques we can use to improve our model accuracy include:In my next article, I will be explaining some of the above concepts in detail. You can refer to my notebook here for the above code.",14/05/2021,8,11,0,"(490, 346)",7,1,0.0,11,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
51,Named Entity Recognition with NLTK and SpaCy,Towards Data Science,Susan Li,25000.0,6.0,632,"Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions, such as:This article describes how to build named entity recognizer with NLTK and SpaCy, to identify the names of things, such as persons, organizations, or locations in the raw text. Let’s get started!I took a sentence from The New York Times, “European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.”Then we apply word tokenization and part-of-speech tagging to the sentence.Let’s see what we get:We get a list of tuples containing the individual words in the sentence and their associated part-of-speech.Now we’ll implement noun phrase chunking to identify named entities using a regular expression consisting of rules that indicate how sentences should be chunked.Our chunk pattern consists of one rule, that a noun phrase, NP, should be formed whenever the chunker finds an optional determiner, DT, followed by any number of adjectives, JJ, and then a noun, NN.Using this pattern, we create a chunk parser and test it on our sentence.The output can be read as a tree or a hierarchy with S as the first level, denoting sentence. we can also display it graphically.IOB tags have become the standard way to represent chunk structures in files, and we will also be using this format.In this representation, there is one token per line, each with its part-of-speech tag and its named entity tag. Based on this training corpus, we can construct a tagger that can be used to label new sentences; and use the nltk.chunk.conlltags2tree() function to convert the tag sequences into a chunk tree.With the function nltk.ne_chunk(), we can recognize named entities using a classifier, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.Google is recognized as a person. It’s quite disappointing, don’t you think so?SpaCy’s named entity recognition has been trained on the OntoNotes 5 corpus and it supports the following entity types:We are using the same sentence, “European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.”One of the nice things about Spacy is that we only need to apply nlp once, the entire background pipeline will return the objects.European is NORD (nationalities or religious or political groups), Google is an organization, $5.1 billion is monetary value and Wednesday is a date object. They are all correct.During the above example, we were working on entity level, in the following example, we are demonstrating token-level entity annotation using the BILUO tagging scheme to describe the entity boundaries.""B"" means the token begins an entity, ""I"" means it is inside an entity, ""O"" means it is outside an entity, and """" means no entity tag is set.Now let’s get serious with SpaCy and extracting named entities from a New York Times article, — “F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is Fired.”188There are 188 entities in the article and they are represented as 10 unique labels:The following are three most frequent tokens.Let’s randomly select one sentence to learn more.Let’s run displacy.render to generate the raw markup.One miss-classification here is F.B.I. It is hard, isn’t it?Using spaCy’s built-in displaCy visualizer, here’s what the above sentence and its dependencies look like:Next, we verbatim, extract part-of-speech and lemmatize this sentence.Named entity extraction are correct except “F.B.I”.Finally, we visualize the entity of the entire article.Try it yourself. It was fun! Source code can be found on Github. Happy Friday!",17/08/2018,20,1,1,"(608, 361)",19,1,0.0,13,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,anger/irritation
52,Gaussian Mixture Models Explained,Towards Data Science,Oscar Contreras Carrasco,286.0,12.0,2306,"In the world of Machine Learning, we can distinguish two main areas: Supervised and unsupervised learning. The main difference between both lies in the nature of the data as well as the approaches used to deal with it. Clustering is an unsupervised learning problem where we intend to find clusters of points in our dataset that share some common characteristics. Let’s suppose we have a dataset that looks like this:Our job is to find sets of points that appear close together. In this case, we can clearly identify two clusters of points which we will colour blue and red, respectively:Please note that we are now introducing some additional notation. Here, μ1 and μ2 are the centroids of each cluster and are parameters that identify each of these. A popular clustering algorithm is known as K-means, which will follow an iterative approach to update the parameters of each clusters. More specifically, what it will do is to compute the means (or centroids) of each cluster, and then calculate their distance to each of the data points. The latter are then labeled as part of the cluster that is identified by their closest centroid. This process is repeated until some convergence criterion is met, for example when we see no further changes in the cluster assignments.One important characteristic of K-means is that it is a hard clustering method, which means that it will associate each point to one and only one cluster. A limitation to this approach is that there is no uncertainty measure or probability that tells us how much a data point is associated with a specific cluster. So what about using a soft clustering instead of a hard one? This is exactly what Gaussian Mixture Models, or simply GMMs, attempt to do. Let’s now discuss this method further.A Gaussian Mixture is a function that is comprised of several Gaussians, each identified by k ∈ {1,…, K}, where K is the number of clusters of our dataset. Each Gaussian k in the mixture is comprised of the following parameters:Let us now illustrate these parameters graphically:Here, we can see that there are three Gaussian functions, hence K = 3. Each Gaussian explains the data contained in each of the three clusters available. The mixing coefficients are themselves probabilities and must meet this condition:Now how do we determine the optimal values for these parameters? To achieve this we must ensure that each Gaussian fits the data points belonging to each cluster. This is exactly what maximum likelihood does.In general, the Gaussian density function is given by:Where x represents our data points, D is the number of dimensions of each data point. μ and Σ are the mean and covariance, respectively. If we have a dataset comprised of N = 1000 three-dimensional points (D = 3), then x will be a 1000 × 3 matrix. μ will be a 1 × 3 vector, and Σ will be a 3 × 3 matrix. For later purposes, we will also find it useful to take the log of this equation, which is given by:If we differentiate this equation with respect to the mean and covariance and then equate it to zero, then we will be able to find the optimal values for these parameters, and the solutions will correspond to the Maximum Likelihood Estimates (MLE) for this setting. However, because we are dealing with not just one, but many Gaussians, things will get a bit complicated when time comes for us to find the parameters for the whole mixture. In this regard, we will need to introduce some additional aspects that we discuss in the next section.We are now going to introduce some additional notation. Just a word of warning. Math is coming on! Don’t worry. I’ll try to keep the notation as clean as possible for better understanding of the derivations. First, let’s suppose we want to know what is the probability that a data point xn comes from Gaussian k. We can express this as:Which reads “given a data point x, what is the probability it came from Gaussian k?” In this case, z is a latent variable that takes only two possible values. It is one when x came from Gaussian k, and zero otherwise. Actually, we don’t get to see this z variable in reality, but knowing its probability of occurrence will be useful in helping us determine the Gaussian mixture parameters, as we discuss later.Likewise, we can state the following:Which means that the overall probability of observing a point that comes from Gaussian k is actually equivalent to the mixing coefficient for that Gaussian. This makes sense, because the bigger the Gaussian is, the higher we would expect this probability to be. Now let z be the set of all possible latent variables z, hence:We know beforehand that each z occurs independently of others and that they can only take the value of one when k is equal to the cluster the point comes from. Therefore:Now, what about finding the probability of observing our data given that it came from Gaussian k? Turns out to be that it is actually the Gaussian function itself! Following the same logic we used to define p(z), we can state:Ok, now you may be asking, why are we doing all this? Remember our initial aim was to determine what the probability of z given our observation x? Well, it turns out to be that the equations we have just derived, along with the Bayes rule, will help us determine this probability. From the product rule of probabilities, we know thatHmm, it seems to be that now we are getting somewhere. The operands on the right are what we have just found. Perhaps some of you may be anticipating that we are going to use the Bayes rule to get the probability we eventually need. However, first we will need p(xn), not p(xn, z). So how do we get rid of z here? Yes, you guessed it right. Marginalization! We just need to sum up the terms on z, henceThis is the equation that defines a Gaussian Mixture, and you can clearly see that it depends on all parameters that we mentioned previously! To determine the optimal values for these we need to determine the maximum likelihood of the model. We can find the likelihood as the joint probability of all observations xn, defined by:Like we did for the original Gaussian density function, let’s apply the log to each side of the equation:Great! Now in order to find the optimal parameters for the Gaussian mixture, all we have to do is to differentiate this equation with respect to the parameters and we are done, right? Wait! Not so fast. We have an issue here. We can see that there is a logarithm that is affecting the second summation. Calculating the derivative of this expression and then solving for the parameters is going to be very hard!What can we do? Well, we need to use an iterative method to estimate the parameters. But first, remember we were supposed to find the probability of z given x? Well, let’s do that since at this point we already have everything in place to define what this probability will look like.From Bayes rule, we know thatFrom our earlier derivations we learned that:So let’s now replace these in the previous equation:And this is what we’ve been looking for! Moving forward we are going to see this expression a lot. Next we will continue our discussion with a method that will help us easily determine the parameters for the Gaussian mixture.Well, at this point we have derived some expressions for the probabilities that we will find useful in determining the parameters of our model. However, in the past section we could see that simply evaluating (3) to find such parameters would prove to be very hard. Fortunately, there is an iterative method we can use to achieve this purpose. It is called the Expectation — Maximization, or simply EM algorithm. It is widely used for optimization problems where the objective function has complexities such as the one we’ve just encountered for the GMM case.Let the parameters of our model beLet us now define the steps that the general EM algorithm will follow¹.Step 1: Initialise θ accordingly. For instance, we can use the results obtained by a previous K-Means run as a good starting point for our algorithm.Step 2 (Expectation step): EvaluateWell, actually we have already found p(Z|X, θ). Remember the γ expression we ended up with in the previous section? For better visibility, let’s bring our earlier equation (4) here:For Gaussian Mixture Models, the expectation step boils down to calculating the value of γ in (4) by using the old parameter values. Now if we replace (4) in (5), we will have:Sounds good, but we are still missing p(X, Z|θ*). How can we find it? Well, actually it’s not that difficult. It is just the complete likelihood of the model, including both X and Z, and we can find it by using the following expression:Which is the result of calculating the joint probability of all observations and latent variables and is an extension of our initial derivations for p(x). The log of this expression is given byNice! And we have finally gotten rid of this troublesome logarithm that affected the summation in (3). With all of this in place, it will be much easier for us to estimate the parameters by just maximizing Q with respect to the parameters, but we will deal with this in the maximization step. Besides, remember that the latent variable z will only be 1 once everytime the summation is evaluated. With that knowledge, we can easily get rid of it as needed for our derivations.Finally, we can replace (7) in (6) to get:In the maximization step, we will find the revised parameters of the mixture. For this purpose, we will need to make Q a restricted maximization problem and thus we will add a Lagrange multiplier to (8). Let’s now review the maximization step.Step 3 (Maximization step): Find the revised parameters θ* using:WhereWhich is what we ended up with in the previous step. However, Q should also take into account the restriction that all π values should sum up to one. To do so, we will need to add a suitable Lagrange multiplier. Therefore, we should rewrite (8) in this way:And now we can easily determine the parameters by using maximum likelihood. Let’s now take the derivative of Q with respect to π and set it equal to zero:Then, by rearranging the terms and applying a summation over k to both sides of the equation, we obtain:From (1), we know that the summation of all mixing coefficients π equals one. In addition, we know that summing up the probabilities γ over k will also give us 1. Thus we get λ = N. Using this result, we can solve for π:Similarly, if we differentiate Q with respect to μ and Σ, equate the derivative to zero and then solve for the parameters by making use of the log-likelihood equation (2) we defined, we obtain:And that’s it! Then we will use these revised values to determine γ in the next EM iteration and so on and so forth until we see some convergence in the likelihood value. We can use equation (3) to monitor the log-likelihood in each step and we are always guaranteed to reach a local maximum.It would be nice to see how we can implement this algorithm using a programming language, wouldn’t it? Next, we will see parts of the Jupyter notebook I have provided so you can see a working implementation of GMMs in Python.Just as a side note, the full implementation is available as a Jupyter notebook at https://bit.ly/2MpiZp4I have used the Iris dataset for this exercise, mainly for simplicity and fast training. From our previous derivations, we stated that the EM algorithm follows an iterative approach to find the parameters of a Gaussian Mixture Model. Our first step was to initialise our parameters. In this case, we can use the values of K-means to suit this purpose. The Python code for this would look like:Next, we execute the expectation step. Here we calculateAnd the corresponding Python code would look like:Note that in order to calculate the summation we just make use of the terms in the numerator and divide accordingly.We then have the maximization step, where we calculateThe corresponding Python code for this would be the following:Note that in order to simplify the calculations a bit, we have made use of:Finally, we also have the log-likelihood calculation, which is given byThe Python code for this would beWe have pre-computed the value of the second summation in the expectation step, so we just make use of that here. In addition, it is always useful to create graphs to see how the likelihood is making progress.We can clearly see that the algorithm converges after about 20 epochs. EM guarantees that a local maximum will be reached after a given number of iterations of the procedure.Finally, as part of the implementation we also generate an animation that shows us how the cluster settings improve after each iteration.Note how the GMM improves the centroids estimated by K-means. As we converge, the values for the parameters for each cluster do not change any further.Gaussian Mixture Models are a very powerful tool and are widely used in diverse tasks that involve data clustering. I hope you found this post useful! Feel free to approach with questions or comments. I would also highly encourage you to try the derivations yourself as well as look further into the code. I look forward to creating more material like this soon.Enjoy![1] Bishop, Christopher M. Pattern Recognition and Machine Learning (2006) Springer-Verlag Berlin, Heidelberg.[2] Murphy, Kevin P. Machine Learning: A Probabilistic Perspective (2012) MIT Press, Cambridge, Mass,",03/06/2019,0,27,56,"(619, 135)",39,1,0.0,1,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,expectation/interest
53,Background Matting: The World is Your Green Screen,Towards Data Science,Vivek Jayaram,74.0,7.0,1185,"Do you wish that you could make professional quality videos without a full studio? Or that Zoom’s virtual background function worked better during your video conferences?Our recently published paper [1] in CVPR 2020 provides a new and easy method to replace your background for a wide variety of applications. You can do this at home in everyday settings, using a fixed or handheld camera. Our method is also state-of-the-art and gives outputs comparable to professional results. In this article we walk through the motivation, technical details, and usage tips for our method.You can also checkout out our project page and codebase.Matting is the process of separating an image into foreground and background so you can composite the foreground onto a new background. This is the key technique behind the green screen effect, and it is widely used in video production, graphics, and consumer apps. To model this problem, we represent every pixel in the captured image as a combination of foreground and background:Our problem is to solve for the foreground (F), background (B), and transparency (alpha) for every pixel given a captured image (C). Clearly this is highly undetermined, and since images have RGB channels, this requires solving 7 unknowns from 3 observed values.One possible approach is to use segmentation to separate foreground for compositing. Although segmentation has made huge strides in recent years, it does not solve the full matting equation. Segmentation assigns a binary (0,1) label to each pixel in order to represent foreground and background instead of solving for a continuous alpha value. The effects of this simplification are visible in the following example:The areas around the edge, particularly in the hair, have a true alpha value between 0 and 1. Therefore, the binary nature of segmentation creates a harsh boundary around the foreground, leaving visible artifacts. Solving for the partial transparency and foreground color allows much better compositing in the second frame.Because matting is a harder problem than segmentation, additional information is often used to solve this unconstrained problem, even when using deep learning.Many existing methods [3][4][5] use a trimap, or a hand-annotated map of known foreground, background, and unknown regions. Although this is possible to do for an image, annotating video is extremely time consuming and is not a feasible research direction for this problem.We choose instead to use a captured background as an estimate of the true background. This makes it easier to solve for the foreground and alpha value. We call it a “casually captured” background because it can contain slight movements, color differences, slight shadows, or similar colors as the foreground.The figure above shows how we can easily provide a rough estimate of the true background. As the person leaves the scene, we capture the background behind them. The figure below shows what this looks like:Notice how this image is challenging because it has a very similar background and foreground color (particularly around the hair). It was also recorded with a handheld phone and contains slight background movements.“We call it a casually captured background because it can contain slight movements, color differences, slight shadows, or similar colors as the foreground.”Although our method works with some background perturbations, it is still better when the background is constant and best in indoor settings. For example, it does not work in the presence of highly noticeable shadows cast by the subject, moving backgrounds (e.g. water, cars, trees), or large exposure changes.We also recommend capturing the background by having the person leave the scene at the end of the video, and pulling that frame from the continuous video. Many phones have different zoom and exposure settings when you switch from video mode to photo mode. You should also enable auto-exposure lock when filming with a phone.A summary of the capture tips:Another natural question is whether this is like background subtraction. Firstly, if it were easy to use any background for compositing, the movie industry would not be spending thousands of dollars on green screens all these years.In addition, background subtraction does not solve for partial alpha values, giving the same hard edge as segmentation. It also does not work well when there is a similar foreground and background color or any motions in the background.The network consists of a supervised step followed by an unsupervised refinement. We’ll briefly summarize them here, but for full details you can always check out the paper.In order to first train the network, we use the Adobe Composition-1k dataset, which contains 450 carefully annotated ground truth alpha mattes. We train the network in a fully supervised way, with a per pixel loss on the output.Notice that we take several inputs, including the image, background, soft segmentation, and temporal motion information. Our novel Context Switching Block also ensures robustness to poor inputs.The problem with supervised learning is that the adobe dataset only contains 450 ground truth outputs, which is not nearly enough to train a good network. Obtaining more data is extremely difficult because it involves hand-annotating the alpha matte of an image.To solve this problem, we use a GAN refinement step. We take the output alpha matte from the supervised network and composite it on a new background. The discriminator then tries to tell if it is a real or fake image. In response, the generator learns to update the alpha matte so the resulting composite is as real as possible in order to fool the discriminator.The important part here is that we don’t need any labelled training data. The discriminator was trained with thousands of real images, which are very easy to obtain.What’s also useful about the GAN is that you can train the generator on your own images to improve results at test time. Suppose you run the network and the output is not very good. You can update the weights of the generator on that exact data in order to better fool the discriminator. This will overfit to your data, but will improve the results on the images you provided.Although the results we see are quite good, we are continuing to make this method more accurate and easy to use.In particular, we would like to make this method more robust to circumstances like background motions, camera movements, shadows, etc. We are also looking at ways to make this method work in real-time and with less computational resource power. This could enable a wide variety of use cases in areas like video streaming or mobile apps.If you have any questions feel free to reach out to me, Vivek Jayaram, or Soumyadip Sengupta[1] S. Sengupta, V. Jayaram, B. Curless, S. Seitz, and I. Kemelmacher-Shlizerman, Background Matting: The World is Your Green Screen (2020), CVPR 2020[2] L.C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (2018), ECCV 2018[3] Y.Y. Chuang, B. Curless, D. H. Salesin, and R. Szeliski, A Bayesian Approach to Digital Matting (2001), CVPR 2001[4] Q. Hou and F. Liu. Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation (2019), ICCV 2019[5] H. Lu, Y. Dai, C. Shen, and S. Xu, Indices Matter: Learning to Index for Deep Image Matting (2019), ICCV 2019",09/04/2020,0,0,4,"(665, 271)",10,1,0.0,10,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
54,The Journalist-Engineer,,matthew_daniels,4400.0,6.0,1011,"Lately, some of the best articles in the NY Times and Bloomberg are 99% code. The end-product is predominantly software, not prose.Here’s an example: the NY Times’ mapping of migration in the US.Several years ago, this article might have been a few thousand words. There’d be tables and charts. They’d reference academic studies and correlate the data with something like unemployment.This example is different. It’s a well-designed data dump. It’s raw numbers without any abstractions. There’s no attachment to the news cycle. There’s no traditional thesis. It cannot be made in Photoshop or Illustrator. You must write software.It represents the present-day revolution within news organizations. Some call it data journalism. Or explorable explanations. Or interactive storytelling. Whatever the label, it’s a huge shift from ledes and infographics.Here’s another example: a graphic from the NY Times on yield curve data:The story is the code. It depicts the yield curve, an incredibly complex system, in all of its glory. It’s an amazing piece of software (I bet financial companies would even buy it).Here’s another example: The Parable of the Polygons, an explanation of an academic paper about segregation.It’s is a very elegant presentation of a system using code. For reference, here’s how the original author conveyed the idea, back in 1991.No need to hate on the design — it worked just fine, pre-Internet. But today, code makes the possibilities so much richer.Here’s two more excellent depictions of complex ideas using code, one on OPEC prices and another on machine learning:Note that all of these examples brilliantly include some prose. There’s an expert presenting her beliefs about the data, which acts as a guide to the data and launches the reader into their own discovery process.Creative coders turned their sights from media art to journalism. They’re writing software about ideas that have eluded traditional news organizations, either because they were too complex to explain in prose or they were trapped in a spreadsheet/academic paper.And that’s what I’m doing with at The Pudding…liberating those ideas.A couple months ago, I published an article comparing historic and present-day popularity of older music. I used two huge datasets: 50,000 Billboard songs and 1,4M tracks on Spotify.If I were writing an academic paper, I’d do a ton of analysis, regression, and modeling to figure out why certain songs have become more popular over time.Or I could just make some sick visualizations…Instead of reporting on my “theory”, I wagered that readers would get more out of an elegant presentation of the data, not an analysis of it. It’s a completely different approach to storytelling.Here’s that same approach on another project: rappers and the size of their vocabulary. The process: depict the system (the vocabulary among rappers, Shakespeare, and Melville) rather than a thesis/point.Instead of proving that one rapper was better than another, readers are really good at absorbing the data, and they’d much rather form their own judgements.A few years ago, Bret Victor wrote about the notion of passive and active readers:“An active reader asks questions, considers alternatives, questions assumptions, and even questions the trustworthiness of the author…An active reader doesn’t passively sponge up information, but uses the author’s argument as a springboard for critical thought and deep understanding.”In theory, this sounds great…but kinda crazy. Imagine it: the Internet engaging in intense discourse over data.It would represent a big shift in journalistic voice and place an enormous burden on the reader: “you find the story. You’re the data analyst.” It’s the opposite role of traditional media, which assumes the role of informer: “we have the knowledge; you don’t. We’re an authoritative source. Read, listen, watch this thing we researched.”But it’s happening — there are active readers. I’ve been shocked at readers’ response for the handful of projects that I’ve worked on. Readers feel powerful. They don’t know what to call it — it feels foreign.I believe it’s a response to “too long, didn’t read.” I open a 10,000 word article, and I anxiously wonder whether the time investment will pay-off. Maybe the author’s point will suck.An experience for active readers doesn’t create anxiety. They don’t feel the burden of time — it’s at their own pace. Give readers the right depiction, avoid abstractions, add a narrative to guide them through the experience, and they’ll data science the shit out of a story.There’s a few reasons why things are so different in 2015.News organizations had to accept that code-driven content wouldn’t have a viable print-version. The NY Times launched a data-led blog, The Upshot, to address this tension. Michael Bloomberg is subsidizing all of Bloomberg Business’s engineers in the Editorial Department. No one else is even close to making a head-count commitment.We needed a pretty unique skill-set: people who could design, write, and code. The talent pool has arrived: all of the coders who were creating apps, dashboards, and analytics tools could shift their design sense from users to readers. Like traditional journalists, engineers now had plenty of empathy for how people consume information.D3 came along. Visualizing millions of data points on the Internet used to be impossible. And browsers are now robust enough to render our creations.D3 also made it easier to be creative with design. Pre-D3, we were taking screenshots of charts from Excel. Now, you can create something that best expresses the data, instead of limiting yourself to traditional, pre-Internet design patterns (e.g., bar chart, scatter charts, pie charts, etc.).Here’s an example of that process: an evolution of how Mike Bostock explored various bespoke designs for a visualization of corporate tax rates.Note that very few of these designs would be possible in statistics packages or design programs. Long live D3.There’s 4.3 million people subscribed to Reddit’s /r/dataisbeautiful sub. It’s a top 50 subreddit. The Internet has grossly undervalued our intrinsic interest in visualization. I expect the market for this sort of content to explode.I’m psyched for the next wave of software that makes journalism easier to code. Someone will write a framework for Oculus Rift. Someone will figure out D3 for mobile. Even on desktop, scroll-based events are still in their infancy.In the mean-time, I’ll be busy coding.",25/10/2015,0,5,13,"(528, 297)",11,0,0.0,32,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,neutral,joy/calmness
55,Towards Reliable MLOps with Drift Detectors,data from the trenches,Simona Maggio,19.0,10.0,1994,"Data is constantly changing, as the world from which it is collected. When a model is deployed in production, detecting changes and anomalies in new incoming data is key to make sure the predictions obtained are valid and can be safely consumed. So, how do we detect data drift?In this article you will learn about two techniques used to assess whether dataset drift is occurring: the domain classifier, looking directly at the input features, and the black-box shift detector, indirectly studying the output predictions.While both techniques work without ground truth labels, the black-box shift detector is cheaper (as it does not require any training) but in some cases less effective as it needs more data to trigger an alert than the domain classifier.Although they perform similarly in many cases, they are designed to capture different changes and are complementary for specific types of shift, overall covering a wide spectrum of drift types. Let’s see how they work!The Electricity DatasetHere we focus on classification tasks and take the electricity dataset as an example, on which we train a classifier, say a random forest, to predict whether the price will go UP or DOWN. We call this classifier the primary model. Now let’s imagine we get a second new electricity dataset. We want to know if there is a substantial difference between the original dataset and the new dataset before scoring it with our primary model.What is generally called data drift is a change in the distribution of data used in a predictive task. The distribution of the data used to train a ML model, called the source distribution, is different from the distribution of the new available data, the target distribution.When this change only involves the distribution of the features, it is called covariate shift. One simple example of covariate shift is the addition of random noise to some of the features in the dataset. Another type of data drift is prior shift, occurring where only the distribution of the target variable gets modified, for instance changing the proportion of samples from one class. A combination of the previous shifts is also a possibility, if we are unlucky.Probably the most challenging type of shift is the concept shift, in which case the dependence of the target on the features evolves over time. If you want to know more about the data drift, have a look at our earlier post, A Primer on Data Drift.An illustration of the possible types of drifts for the electricity dataset is shown in Figure 1. In this post we’ll focus on covariate and prior shifts, as their detection can be addressed without ground truth labels.We can train a binary classifier on the same features used in the primary model to predict which domain each observation belongs to: source or target. This is exactly how we build a domain classifier. If the two datasets are different enough, the domain classifier can easily discriminate between the two domains and exhibits high accuracy, which serves as a symptom of drift. We perform a Binomial test checking how likely is to get such accuracy when there is no drift. If the p-value — i.e., the probability of getting at least such accuracy under the no drift hypothesis — is low enough, this means that a drift might be occurring. In practice, we trigger a drift alert when the p-value of the Binomial test is less than a desired significance level.Using the Binomial test on the domain classifier accuracy is more reliable than directly looking at the accuracy itself. Figure 2 shows how the distributions of the p-values change on the source and target dataset, illustrating how effective this tool is to discriminate the two situations.By design, the domain classifier is meant to detect alterations of the input features (covariate shift)and is not adequate in case of shifts affecting only the label distribution.This technique requires performing a new training every time that a batch of new incoming data is available. In this sense, the domain classifier is expensive, but it has many advantages. One of its most appealing aspects is that it can be used locally, at the sample level, to predict which observations are the most different from the original dataset. Thus, this simple technique allows a deeper analysis of the drift condition and, in particular, is able to deal with partial drift, a situation when only a part of the observations or a part of the features are drifted.What if we can’t afford training a new model every time? Or if we have no access to the features used by the deployed model?In this case, we can build a shift detector on the top of our primary model. Here, we use the primary model as a black-box predictor and look for changes in the distribution of the predictions as a symptom of drift between the source and target datasets [1].Unlike the domain classifier, the black-box shift detector is designed to capture alterations in the label distribution (prior shift) and is not adequate in case of covariate shifts that have little impact on the prediction, such as small random noise.After collecting the predictions of the primary model on both source and target datasets, we need to perform a statistical test to check if there is significant difference between the two distributions of predictions. One possibility is to use the Kolmogorov-Smirnov test and compute again the p-value, i.e., the probability of having at least such distance between the two distributions of predictions in case of absent drift.For this technique we are looking at the predictions (which are vectors of dimension K, the number of classes), and perform K independent univariate Kolmogorov-Smirnov tests. Then we apply the Bonferroni correction, taking the minimum p-value from all the K tests and requiring this p-value to be less than a desired significance level divided by K. Again we can assume there is a drift when the minimum p-value is less than the scaled desired significance level. Figure 3 shows how the distributions of the p-values change on the source and target datasets.How can we be sure that only looking at new predictions, one can tell whether the features or the labels distribution has changed? What is the connection between predictions, features and true (unknown) labels?Under some assumptions, [1] the distribution of the primary model predictions is theoretically shown to be a good surrogate of both features and true unknown labels distributions.In order to meet the required assumptions to use a black-box shift detector, we first need to have a primary model with decent performance. This one should be easy — hopefully. Second, the source dataset should contain examples from every class, and third we need to be in a situation of prior shift. This one is impossible to know beforehand, as we have no ground-truth labels. Although in real life scenarios it’s impossible to know whether those theoretical hypotheses are met, this technique is still useful in practice.In order to benchmark the drift detectors in a controlled environment, we synthetically apply different types of shift to the electricity dataset:Prior and Covariate Resampling shifts are the most common in a real-world scenario because of the possible selection bias during data collection.In the experiments, we use 28 different types of shifts from these four broad categories, as detailed in the following table for the curious readers:For the experiments and to generate a part of the shifts, we use the library provided in the Failing Loudly github repo [2]. For the adversarial shifts, we use the Adversarial Robustness Toolbox [5], while for some of the resampling techniques, we use imbalanced-learn [6].An ideal drift detector is not only capable of detecting when drift is occurring but is able to do so with only a small amount of new observations. This is essential for the drift alert to be triggered as soon as possible. We want to evaluate both accuracy and efficiency.We apply 28 different types of shifts to the electricity target dataset from the four categories highlighted in the previous section. For each shift situation, we perform 5 runs of drift detection by both domain classifier and black-box dhift detector. For each run, we perform the detection comparing subsampled versions of the source and drifted datasets at six different sizes: 10, 100, 500, 1000, 5000, and 10000 observations.Both the primary model underlying the black-box shift detector and the domain classifier are random forests with scikit-learn default hyper-parameters.An example of the detection results obtained for a set of covariate resampling shifts is shown in Figure 4. The lines show the average p-values across the runs, while the error areas show the standard deviation of the p-values. The more samples available, the higher the chances to detect the drift.Some of the drifts will cause the primary model to vastly underperform with a large accuracy drop. Although in a real scenario it would be impossible to compute this accuracy drop without ground truth labels, we highlight this information to better illustrate the different types of shift. The drifts yielding to high accuracy drop are the most harmful for our model. Still, whatever alteration may affect the accuracy, it is important to be aware that some change is occurring in the new data and we should probably deal with it.In order to measure the efficiency of the drift detectors, we score the two techniques with respect to the minimum dataset size required to detect a particular drift. We assign a score from 6 to 1 if the detector first detects a drift at dataset size from 10 to 10000; the detector gets 0 score if it can’t detect the drift at all.The results obtained for the black-box shift detector and the domain classifier are averaged by type of drift and shown in Figure 5 alongside with the average accuracy drop.We can see that the two methods perform similarly in case of Gaussian noise shift. The adversarial noise is so subtle that the Domain Classifier has some trouble catching changes in the features. On the other hand, as the adversarial noise is designed to make the predictions change, it does not go unnoticed to the black-box shift detector, which is more efficient in this case. The prior and covariate resampling shifts are less harmful to the primary model for this dataset, which also makes the black-box shift detector miss some of them, revealing the domain classifier as more efficient for the most frequent real-world types of shift.Domain classifier and black-box shift detector are two techniques looking for changes in the data, either directly looking at the features or indirectly at the model predictions.They perform similarly on many different situations of drift, but there is a divergence in two situations: the domain classifier is more efficient in detecting covariate resampling drift (the most common case of dataset shift) while the black-box shift detector is more efficient in catching adversarial noise.The black-box shift detector is the way to go if the features are not available and the primary model in production is indeed a black-box, but high model performance might be a hard requirement for some specific types of drift.The domain classifier has also a great advantage of being able to score individual samples, thus providing a tool to perform a deeper local analysis and identify possible anomalies.We’re excited to be working on this topic for Dataiku DSS — check out this Dataiku plugin to monitor Model Drift in your DSS projects.[1] Z. C. Lipton et al. “Detecting and Correcting for Label Shift with Black Box Predictors”, ICML 2018.[2] S. Rabanser et al. “Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift”, NeurIPS 2019, and its github repo.[3] PY. Chen et al. “ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models”, ACM Workshop on Artificial Intelligence and Security, 2017.[4] W. Brendel et al. “Decision-based Adversarial attacks: reliable attacks against black-box machine learning models”, ICLR 2018.[5] Adversarial Robustness Toolbox: IBM toolbox to generate adversarial attacks.[6] imbalanced-learn: library with resampling techniques for imbalanced datasets.",16/07/2020,0,15,11,"(700, 462)",6,1,0.0,19,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,surprise/amazement
56,Тестовая версия Баунти-платформы TokenGo,,TokenGo Platform_RU,294.0,2.0,291,"Уважаемые участники!Мы уже писали в последних новостях, что TokenGo, продолжая свое развитие по этапам, готовится к тестированию одного из модулей — платформы для проведения Баунти-кампании стартапов, которые уже запустили или готовят запуск своих процедур ICO.Почему мы хотим запустить данное тестирование?1️. Во-первых, потому что сейчас самое время его провести. 2️. Во-вторых, потому что данный модуль входит в программу наших разработок.Мы уже получили достаточно опыта на функционировании собственной баунти-кампании и, при этом, имеем существенный запас времени до запуска готового продукта. Именно сейчас подходящее время представить заинтересованным лицам возможности баунти-кабинета, заняться оттачиванием деталей, адаптированию найденных решений под общие нужды, стандартизацией единых процессов, доработкой мелочей и рассмотрением критики и предложений пользователей с целью исправления имеющихся и внедрения дополнительных инструментов.Мы понимаем, что для запуска полностью рабочей технологии нам потребуются коллективные усилия и достаточное время, поэтому начинаем этим заниматься заранее.Мы будем давать возможность фаундерам подключать собственные проекты, оформлять и запускать в работу задачи для участников и прочие активности.Мы дадим возможность участникам выступать в роли Баунти-менеджеров по управлению Баунти-кампаниями и реализуем схему процентных отчислений за поиск и ведение проектов.Мы организуем специальный чат для Баунти-менеджеров, посредством которого мы сможем выстроить конструктивное общение с целью обмена информацией и улучшения свойств и качеств разрабатываемой платформы.После обновления вход в Баунти-кампанию TokenGo изменится. Для каждого проекта теперь предусмотрен отдельный вход. Вам понадобится немного времени, чтобы привыкнуть.Подводя итоги, хочется всех поздравить с запуском первого масштабного прототипа модуля, который в ходе нашего общения, работы с фаундерами и доработок со временем превратится в хорошо востребованную и высокоавтоматизированную площадку, и это здорово!Движение вперед продолжается, и мы желаем нашим участникам-баунтистам много интересных заданий и хороших вознаграждений в этом непростом, но интересном деле! А нашим участникам-профи и всем вновь присоединившимся — новых знаний, грамотных кураторов и, конечно, удачи!Адрес чата для участников, кто хочет вести проекты в роли Баунти-менеджеров: http://t.me/bounty_for_all",02/04/2018,0,0,0,"(700, 368)",1,0,0.0,1,ru,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
57,Patterns in Self-Supervised Learning,Towards Data Science,Nishant Sinha,168.0,4.0,740,"Self-Supervision is in the air (and the talks). Explaining the difference between self-, un-, weakly-, semi-, distantly-, and fully-supervised learning (and of course, RL) just got exponentially tougher. :) Nevertheless, we are gonna try.The problem, in context, is to encode an object (a word, sentence, image, video, audio, …) into a general-enough representation (blobs of numbers) which is useful (preserves enough object features) for solving multiple tasks, e.g., find sentiment of a sentence, translate it into another language, locate things in an image, make it higher-resolution, detect text being spoken, identify speaker switches, and so on.Given how diverse images or videos or speech can be, we must often make do with representations tied to a few tasks (or even a single one), which break down if we encounter new examples or new tasks. Learning more, and repeatedly, and continuously, from new examples (inputs labeled with expected outputs) is our go-to strategy (supervised learning). We’ve secretly (and ambitiously) wished that this tiresome, repeated learning process will eventually go away and we’d learn good universal representations for these objects. Learn once, reuse forever. But, the so-called unsupervised learning paradigm (only-input-no-labels) hasn’t delivered much (mild exceptions like GANs and learn-to-cluster models).Enter Self-Supervision: Thankfully, strewn through the web of AI research, a new pattern of learning has quietly emerged, which promises to get closer to the elusive goal. The principle is pretty simple: to encode an object, you try to setup learning tasks between parts of it or different views of it (the self).Given one part (input) of the object, can you predict / generate the other part (output)?There are a couple of flavors of this principle.Because you are simply playing around with the object, these are free lunch tasks — no external labels needed.By happy chance, we now have (plenty of) auto-generated input-output examples, and we’re back in the game. Go ahead and use every hammer from your supervised learning toolkit to learn a great (universal?) representation for the object from these examples.By trying to predict the self-output from the self-input, you end up learning about the intrinsic properties / semantics of the object, which otherwise would have taken a ton of examples to learn from.Self-supervision losses have been the silent heroes for a while now, across representation learning for multiple domains (as auto-encoders, word embedders, auxiliary losses, many data augmentations, …). A very nice slide deck here. Now, with the ImageNet moment for NLP (ELMo, BERT and others), I guess they’ve made it on their own. The missing gap in the supervision spectrum that everyone (including AGI ;) has been waiting for.Understandably, there is flurry of research activity around newer self-supervision tricks, getting SoTA with fewer examples, and mixing various kinds of supervisions (hello NeurIPS!). Till now, the self-supervised methods mostly try to relate the components of an object, taking one part as input, predict the other part. Or, change the object’s view by data augmentation, and predict the same label.Going ahead, let’s see how creative the community gets when playing around with the new hammer. There are many questions that remain: for example, how do you compare multiple different self-supervised tricks — which one learns better representations than others? How do you pick the output? For example, instead of having explicit labels as outputs, UDA uses the intrinsic output distribution D as the label — ensure D changes minimally when the view of input x changes.Also, I’m very curious who claims they were the first to do it :)Update: an interesting twitter thread discussing whether self-supervision is re-branding unsupervised (or something else) or not.tldr: Self supervised learning is an elegant subset of unsupervised learning where you can generate output labels ‘intrinsically’ from data objects by exposing a relation between parts of the object, or different views of the object.About me: I’m an independent computer science researcher, engineer and speaker who loves to distill and transform complex technology into consumable products. Have worked across academia, industry and startups. I help companies understand and maneuver through the complex, evolving AI space and build deep learning based solutions that maximize ROI. If you enjoyed this article, please do clap and post your comments. You can follow me and read my other articles here, find me on linkedin or email me directly.PS: if you are looking for someone to ‘supervise’ you (weakly, fully, remotely or even co-supervise) to solve some very interesting text, vision and speech problems, get in touch with me at nishant@offnote.co !",25/05/2019,0,18,28,"(523, 692)",2,1,0.0,12,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
58,Machine Learning is Fun! Part 2,,Adam Geitgey,55000.0,15.0,2722,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in Italiano, Español, Français, Türkçe, Русский, 한국어 Português, فارسی, Tiếng Việt or 普通话.Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!In Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven’t already read part 1, read it now!).This time, we are going to see one of these generic algorithms do something really cool — create video game levels that look like they were made by humans. We’ll build a neural network, feed it existing Super Mario levels and watch new ones pop out!Just like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished.Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this:We ended up with this simple estimation function:In other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house’s value.Instead of using code, let’s represent that same function as a simple diagram:However this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn’t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn’t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model?To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases:Now we have four different price estimates. Let’s combine those four price estimates into one final estimate. We’ll run them through the same algorithm again (but using another set of weights)!Our new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model.Let’s combine our four attempts to guess into one big diagram:This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions.There’s a lot that I’m skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click:It’s just like LEGO! We can’t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together:The neural network we’ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it’s a stateless algorithm.In many cases (like estimating the price of house), that’s exactly what you want. But the one thing this kind of model can’t do is respond to patterns in data over time.Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess?I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter.Our model might look like this:But let’s make the problem harder. Let’s say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem.Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:Robert Cohn was once middleweight boxiWhat letter is going to come next?You probably guessed ’n’ — the word is probably going to be boxing. We know this based on the letters we’ve already seen in the sentence and our knowledge of common words in English. Also, the word ‘middleweight’ gives us an extra clue that we are talking about boxing.In other words, it’s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English.To solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently.Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters.This is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory.Predicting the next letter in a story might seem pretty useless. What’s the point?One cool use might be auto-predict for a mobile phone keyboard:But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over — forever? We’d be asking it to write a complete story for us!We saw how we could guess the next letter in Hemingway’s sentence. Let’s try generating a whole story in the style of Hemingway.To do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github.We’ll create our model from the complete text of The Sun Also Rises — 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway’s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example.As we just start to train the RNN, it’s not very good at predicting letters. Here’s what it generates after a 100 loops of training:hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h’e btentmuhgehi bcgdltt. gey heho grpiahe.Ddelnss.eelaishaner” cot AAfhB ht ltnyehbih a”on bhnte ectrsnae abeahngyamo k ns aeo?cdse nh a taei.rairrhelardr er deffijhaYou can see that it has figured out that sometimes words have spaces between them, but that’s about it.After about 1000 iterations, things are looking more promising:hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been somsinick stalling that aid. “Hon’t me and acrained on .Hw’s don’t you for the roed,” In’s pair.” “Alough marith him.”The model has started to identify the patterns in basic sentence structure. It’s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there’s also still a lot of nonsense.But after several thousand more training iterations, it looks pretty good:He went over to the gate of the café. It was like a country bed. “Do you know it’s been me.” “Damned us,” Bill said. “I was dangerous,” I said. “You were she did it and think I would a fine cape you,” I said. “I can’t look strange in the cab.” “You know I was this is though,” Brett said. “It’s a fights no matter?” “It makes to do it.” “You make it?” “Sit down,” I said. “I wish I wasn’t do a little with the man.” “You found it.” “I don’t know.” “You see, I’m sorry of chatches,” Bill said. “You think it’s a friend off back and make you really drunk.”At this point, the algorithm has captured the basic pattern of Hemingway’s short, direct dialog. A few sentences even sort of make sense.Compare that with some real text from the book:There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave. “Sit down,” said Harvey, “I’ve been looking for you.” “What’s the matter?” “Nothing. Just looking for you.” “Been out to the races?” “No. Not since Sunday.” “What do you hear from the States?” “Nothing. Absolutely nothing.” “What’s the matter?”Even by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing!We don’t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters.For fun, let’s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of “Er”, “He”, and “The S”:Not bad!But the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern.In 2015, Nintendo released Super Mario Maker™ for the Wii U gaming system.This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It’s like a virtual LEGO set for people who grew up playing Super Mario Brothers.Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels?First, we need a data set for training our model. Let’s take all the outdoor levels from the original Super Mario Brothers game released in 1985:This game has 32 levels and about 70% of them have the same outdoor style. So we’ll stick to those.To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game’s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game’s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime.Here’s the first level from the game (which you probably remember if you ever played it):If we look closely, we can see the level is made of a simple grid of objects:We could just as easily represent this grid as a sequence of characters with one character representing each object:We’ve replaced each object in the level with a letter:…and so on, using a different letter for each different kind of object in the level.I ended up with text files that looked like this:Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:The patterns in a level really emerge when you think of the level as a series of columns:So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well.To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up:Just like we saw when creating the model of Hemingway’s prose, a model improves as we train it.After a little training, our model is generating junk:It sort of has an idea that ‘-’s and ‘=’s should show up a lot, but that’s about it. It hasn’t figured out the pattern yet.After several thousand iterations, it’s starting to look like something:The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the “P”s in the data should appear in 2x2 clusters. That’s pretty cool!With a lot more training, the model gets to the point where it generates perfectly valid data:Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:This data looks great! There are several awesome things to notice:Finally, let’s take this level and recreate it in Super Mario Maker:Play it yourself!If you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9–0000–0157-F3C3.The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‘toy’ instead of cutting-edge is that our model is generated from very little data. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model.If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can’t — because Nintendo won’t let us have them. Big companies don’t give away their data for free.As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That’s why companies like Google and Facebook need your data so badly!For example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate.But without Google’s massive trove of data in every language, you can’t create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you’ve ever been.In machine learning, there’s never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach.Readers have sent me links to other interesting approaches to generating Super Mario levels:If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.Now continue on to Machine Learning is Fun Part 3!",03/01/2016,6,10,61,"(677, 357)",20,4,0.0,56,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
59,Explaining your business model,SlideMagic,Jan Schultink,1100.0,3.0,651,"Most investor pitches I see claim year 5 revenues of $50m to $100m, so putting in just that piece of information is not going to convince investors, you just sound like everyone else. What you need to make believable is why you are going to hit that target. Showing an incredibly complicated Excel model (”look, we did our homework”) is not going to get you there either. So the top line number is not convincing, nor is the detailed model, what works? The napkin.When a modeling economics, I usually go brought a cycle. Start with a very simple calculation that gets to a ballpark answer, and is easy to follow and verify. Then, go I to incredible detail in an Excel model, understanding why I do, or do not get close to my initial ballpark. After the rock solid model is finished and bug free, it is time to simplify down to the level, of that very first ballpark number.Simplification is not simple. You need to pick which drivers of your business are the most important, you need to decide which factors to show, which ones to hide. Your challenge is to stay close to values that are linked to everyday reality, not accounting. Messages per user per month, price per message instead of $m depreciation.With all this preparation, you are now able to let your potential investor write her own ballpark or napkin calculation of the company’s potential. You provide her with the basic framework, what are the 6 numbers you need to multiply in order to get to your $75m in year 5. She might not agree with all the numbers, but you gave her a framework to which to apply her own estimates. Getting the point estimate right is not important, agreeing on the order of magnitude, and the way how to get there, is.There is also psychology involved here. Your numbers are likely to be outrageous, but can you defend them using more or less realistic inputs? Are you open to input and change your calculation? Again, discussing the financials is another way to figure you out as a CEO. Are you a sane person to work with on a day–to–day basis?Everyone understands that you basically made up the numbers for your business forecast and their credibility might not be as high as an investment analyst forecasting next quarter’s earnings of IBM. Still there is something that will completely kill the credibility of your analysis: lack of consistency.If you use 2 different sales numbers on 2 different pages it is a red flag that you might not be on top of your material. Even if the footnote says that the number on page 34 includes VAT and the number on page 22 does not.Calculation errors are the same, if an investor starts checking your math on every page in the presentation, she will not be paying attention to what you have to say. When the final version of the presentation is ready, take out the calculator and check every page carefully, even if you copied data straight from Excel. Look for typos, or funny rounding errors and correct them manually.You probably noticed that we have not discussed data from market research agencies to support your financial forecast. I consider them a data point, but not the most important input for your business plan. Almost all markets discussed by IDC or Gartner are a $billion, and most of the time have a far too broad scope for a new startup. There is one useful thing about these forecasts though, they provide a good categorization for technology markets. So now you can say “where a in the so and so business”. On the other hand, I have seen startups that had great difficulty explaining what they do because a VC could not put them in an IDC box.So in short, explain your business’ economics in a simple human language.",05/06/2012,0,0,0,,0,0,0.0,0,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
60,"HealthTap launches Dr. A.I.— Meet your new, personal AI-powered physician",,HealthTap,3500.0,4.0,743,"Trained by the knowledge of thousands of doctors and millions of user queries, HealthTap’s Dr. A.I. seamlessly translates your symptoms into a personalized course of care.PALO ALTO, Calif. — (BUSINESS WIRE) HealthTap, the leader in providing access to high quality, virtual primary healthcare, launched Dr. A.I. today. Dr. A.I. is an Artificial Intelligence-driven symptom checker that routes users to personalized care, such as doctor-written insights and 24/7 access to speak with a physician through text, phone, or video call.Each year, over one billion people search the Web for health information, with approximately 10 billion symptom-related health searches per year on Google alone. Unlike a doctor, the Web only provides access to generic content related to these symptoms. The Internet does not know how to ask follow-up questions, assess the severity of symptoms, or offer personalized recommendations of what to do next — an art and science known as “medical triage.” Doctors know that accurate and effective triage requires detailed knowledge of a patient’s personal health, making context critical to providing comprehensive, quality care.HealthTap’s new Dr. A.I. takes into account both patient context and the clinical expertise of doctors who have helped triage hundreds of millions of patients worldwide. Just like an in-person primary care doctor, Dr. A.I. converses with the user to understand their current complaints or concerns and uses the user’s health profile to compute the probable causes of their symptoms.First, Dr. A.I. analyzes the user’s current symptoms in the context of relevant data from the personal health record they created on HealthTap, including age, gender, prior medical conditions, and medications — to name a few. Next, based on the user’s symptoms, Dr. A.I. uses advanced deep learning algorithms and HealthTap’s vast database of doctor knowledge and expertise to guide patients to care tailored just for them. Dr. A.I. immediately routes patients towards a variety of solutions that doctors previously suggested to other patients with similar symptoms and health histories.These personalized pathways of care can range from suggesting the patient reads relevant doctor-written content to connecting the patient with a doctor for a live virtual consult. In a virtual consult, doctors can help patients schedule in-person office visits with the appropriate specialist, based on the patient’s symptoms and characteristics.Dr. A.I. is built from the collective clinical knowledge developed over the course of six-years of applying doctor-expertise to real-world patient questions, from a network of more than 140,000 licensed doctors across 163 specialties.“Over the past six years, we’ve collected data from tens of thousands of the leading U.S. doctors who’ve collectively triaged millions of patients throughout their careers,” said Geoff Rutledge MD, PhD, HealthTap’s Chief Medical Officer and a Fellow of the American College of Medical Informatics. “We’ve used this data as a training set to prepare Dr. A.I. for helping triage patients at scale. With an average of twenty years of experience per doctor in the HealthTap Medical Expert Network, the knowledge in the network equates to hundreds of thousands of years of medical school training and more than a million years of collective medical practice experience. Applying Bayesian thinking and advanced techniques of Machine Learning and Artificial Intelligence to the rich data we’ve collected from billions of transactions between tens of thousands of doctors and hundreds of millions of patients on HealthTap, we’ve built an expert system that can help triage people to the care they need when they need it most.”Available via HealthTap’s Android and Apple apps and website, Dr. A.I. is also trained in the art of digital empathy to converse with people using a patient, polite, and compassionate bedside manner. Using an intuitive, user-friendly interface, Dr. A.I. asks dynamic, easily understandable questions, helping people of all ages and demographics seamlessly and conveniently understand the best course of care for them.Dr. A.I. is accessible for consumers starting today from any mobile device or personal computer through the HealthTap iOS or Android apps or the HealthTap website.To read more about HealthTap visit https://www.healthtap.com/resourcesAbout HealthTapHealthTap is the leader in providing consumers with immediate, affordable access to high quality primary care. Through our user-friendly, AI-driven app and website, we provide an unparalleled level of personalized care to each and every HealthTap member. Members have 24/7, unlimited access to virtual doctor consults, an AI-powered symptom checker, and a large library of doctor-authored content. Our powerful AI technology is built from the collective knowledge of thousands of doctors and millions of user questions, providing consumers with the most comprehensive and seamless primary care experience.",16/12/2016,0,6,3,"(454, 205)",6,0,0.0,8,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
61,Gradient Boosted Decision Trees Explained with a Real-Life Example and Some Python Code,Towards Data Science,Carolina Bento,3100.0,9.0,1633,"This is the third and last article in a series dedicated to Tree Based Algorithms, a group of widely used Supervised Machine Learning Algorithms.The first article was about Decision Trees, while the second explored Random Forests. Everything explained with real-life examples and some Python code.Gradient Boosting algorithms tackle one of the biggest problems in Machine Learning: bias.Decision Trees is a simple and flexible algorithm. So simple to the point it can underfit the data.An underfit Decision Tree has low depth, meaning it splits the dataset only a few of times in an attempt to separate the data. Because it doesn’t separate the dataset into more and more distinct observations, it can’t capture the true patterns in it.When it comes to tree-based algorithms Random Forests was revolutionary, because it used Bagging to reduce the overall variance of the model with an ensemble of random trees.In Gradient Boosted algorithms the technique used to control bias is called Boosting.In the late 1980’s and 1990’s, Robert Schapire and Yoav Freund developed one of the most popular Boosting algorithms, AdaBoost, to address underfitting and reduce bias.But they were not alone. The field of Computer Science and Theoretical Machine Learning was riding this wave of enthusiasm and groundbreaking algorithms, and more scientists started developing Boosting approaches.Friedman explored how Boosting can be the optimization method for an adequate loss function. While Schapire and Freund took inspiration on a question posed by Michael Kearns, Can a set of weak learners create a single strong learner?In this context, a weak learner is any model that is slightly better than a random model, and will never efficiently achieve a training error of zero. A strong learner is the opposite, a model that can be improved and efficiently bring the training error down to zero.Around the same time Jerome Freidman, was also experimenting with Boosting. He ended up developing several new algorithms, the most popular being Gradient Boosted Decision Trees.The ingenious efforts of these and other scientists contributed to a vibrant new chapter in Machine Learning. It generated robust and powerful algorithms. Many of which are still relevant today.The motto with Boosting is that the result is greater than the sum of its parts.Boosting is based on the assumption that it’s much easier to find several simple rules to make a prediction than it is to find the one rule that is applicable to all data and generates the best possible prediction[1].The intuition behind Boosting is that you train the same weak learner, a model with simple rules, several times. Then combine its weak predictions into a single, more accurate result. The model is always of the same type same but, each time, it is trained with different weights for the observations it misclassified.Predictions that were previously misclassified will have more weight on the next weak learner, so each weak learner focuses on the hardest observations[1].When you combine different weak learners sequentially, each one will make some incorrect predictions. But they will cover each others blindspots, making better predictions where other learners were not as good. Each model learns slowly from the errors the previous one made[2].All Boosting algorithms are ensemble Classifiers or Regressors, depending on the task at hand, because they combine the results of several models.In the aggregation step of a Regression task you might compute the weighted sum of all predictions for each observation. While in a classification task, a majority vote decides which class to assign.Most algorithms focus on parameter estimation, to find the parameters that minimize their loss function. Boosting algorithms focus on function estimation.The ensemble of weak learners is actually a set of functions that map features to targets and minimize the loss function[3].And because the optimization algorithm used to find the next weak learner is Gradient Descent, the loss function must differentiable and convex.However, this time Gradient Descent is used in a function space.Typically, Gradient Descent starts off at a random point in a differentiable convex function, and only stops when it finds its minimum value.The same concept is translated Boosting algorithms with a Gradient Descent, but in the function space.The algorithm starts off with a very simple weak learner, for instance, always predicting the class 0.With each weak learner added to the ensemble the algorithm is taking a step towards the right targets, because it only adds weak learners that minimizes the loss function.The exponential loss is typically used in Classification tasks while in Regression tasks it’s the Squared Loss.All Boosting algorithms share a common principle, they use Boosting to create an ensemble of learners that are later combined. But each of them has distinct characteristics that make them more suitable for particular tasks.For instance, in Gradient Boosted Decision Trees, the weak learner is always a decision tree.In Stochastic Gradient Boosting, Friedman introduces randomness in the algorithm similarly to what happens in Bagging. At each iteration, instead of using the entire training dataset with different weights, the algorithm picks a sample of the training dataset at random without replacement and uses that to train the next learner[4].Adaboost is in many ways similar to Gradient Boosted Decision Trees, in a sense that you have the distribution of weights for each observation in the training dataset and, in each iteration the algorithm adjusts those weights based on how the previous weak learner performed. But its adaptive nature comes into play when, during each iteration, it chooses a new the learning rate alpha based on the weak learner’s error rate[1].Now that you know a bit more about the different Boosting algorithms, let’s see them in practice!Planning a vacation is challenging. The hardest part, picking a destination.But you’re a Data Scientist. You’re confident that Machine Learning can give an algorithmic opinion on what should be your next vacation destination.Whenever planning a vacation, you always take into account:When it comes to picking a classification algorithm, you know Decision Trees are said to mimic how humans make decisions. But it has limitations, just a single Decision Tree is not a very powerful predictor.Random Forests would get you better results, if you’re focused on controlling bias, not model variance. So Random Forests is not an option.Since using a low bias model is your goal, Boosting algorithms are exactly what you’re looking for.The firs thing you do is to go through your old notes and collect some data on how you picked previous vacation destinations. You gather all the information like your budget at the time and the duration of the vacation, and your previous destinations, which were typically a choice between Beach and Countryside.In the code below I’m creating a synthetic dataset to simulate that data.The whole reason you want to use a Boosting algorithm is to control model bias.In a Regression task, where you predict numerical values, Bias is described by this equation:In this case, you just need plug the numbers in the formula.But you’re trying to get an algorithmic opinion on your next vacation destination, a classification task.For this purpose, you can use the misclassification rate, i.e, average of the 0–1 loss.If your model has a high proportion of misclassified observations in the training set, it means it has high bias.Boosting algorithms ✅ Method to calculate Bias ✅So you start with the the simplest algorithm Decision Trees. With ScikitLearns’ Decision Tree Classifier you create a single decision tree that only splits the dataset twice. That’s why max_depth=2.The average misclassification rate in the training set, the 0–1 loss, is 44%. That’s almost as good as a random model! High Bias ✅Now you turn to a Gradient Boosted Classifier.You set the depth of each weak learner in the ensemble to 1, using the parameter max_depth once again. In this case you’re training the smallest tree possible, what in the machine learning lingo it’s called a decision stump.And in terms of the size of the ensemble, you picked a rather arbitrary number, 210 trees. That’s the parameter n_estimators in the GradientBoostingClassifier method.Gradient Boosted Decision Trees did indeed reduce Bias. A 0–1 loss of 32% is much better!Ok, you built a model with relatively low bias. But you know there are other Boosting algorithms out there, besides Gradient Boosted Decision Trees.How much better would Adaboost be at predicting your next vacation destination?In the case of Adaboost, training the same number of trees didn’t reduce bias as much. But it still brought the misclassification rate down to 38%, from the 44% observed with Decision Trees.When it goes to picking your next vacation destination, with the dataset at hand, Gradient Boosted Decision Trees is the model with lowest bias.Now all you need to do is give the algorithm all information about your next vacation, i.e., the duration, if you’re feeling adventurous and all that.The algorithm will tell you if your next vacation should be a Beach vacation or on the Countryside.Now you can be confident about using Gradient Boosting Decision Trees to predict your next vacation destination. Instead of training just a single Decision Tree.What sets Gradient Boosted Decision trees apart from other approaches is:Event thought it is more complex than Decision Trees, it continues to share the advantages of tree-based algorithms.But there’s always a catch. Gradient Boosted Decision Trees make two important trade-offs:Even though one of the advantages mentioned above is the memory efficiency, that is when it comes to make predictions.The algorithm is more complex and builds each tree sequentially. It’s predictions are fast, but the training phase is computationally more demanding.Similarly to what happens with Random Forests, you trade-off performance for interpretability.You’ll no longer be able to see exactly how the algorithm split each tree node to reach the final prediction.Hope you enjoyed learning about Boosting algorithms, the problems the address and what ultimately makes them unique.Make sure to check the other two articles in this Tree-based Machine Learning algorithms series. The first one explores Decision Trees and the second Random Forests.Thanks for reading!",19/08/2021,0,18,56,"(596, 258)",15,4,0.0,26,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
62,Buy Me That Look,,Satishkumar Moparthi,,11.0,1928,"Fashion Recommendation System, Computer Vision.This blog is all about the Fashion Recommendation system. It is a unique recommendation system compared with others because here based on provided photo/picture system recommends similar clothes or articles worn by the model in the picture. The architecture and design components are inspired by a Paper: Buy Me That Look: An Approach for Recommending Similar Fashion Products by Myntra.The research paper states, in short, is to detects all products in an image and retrieves similar fashion clothes from the database with a product buy link.Online business has become important in day-to-day life for everyone. Virtual stores allow people to shop from the comfort of their homes without the pressure of a salesperson. In this paper, the authors focus on the retrieval of multiple fashion items at once and propose an architecture that detects all products from an image and recommends similar kinds of products.In stores, we can carry a piece of cloth and request the salesperson to show us similar kinds of products matching Color, design, thickness, etc; But online it is not possible and it is time-consuming while searching for similar kinds of products. So, here we can upload an image and search for similar kinds by using computer vision.Let’s discuss the architecture of the model in this session. Will divide this problem into various stages:Stage 1: (Pose Estimation)In this stage, We will detect whether the image is a Full-Front-pose image or not. So this will be a binary classifier (Yes/No)Stage 2: (Localization and Article Detection)In this stage, we detect all the articles (clothes) and particular places the article is placed or located. This will be Both a classification and Regression Problem. Classification because of Article Detection and Regression because of Localization (Bounding Box Co-ordinates)Stage 3: (Image_embeddings)In this stage, we will generate the embedding ( dense vectors ) for the images as discussed below.Stage 4: (Getting similar Images)In this stage, we will use Faiss library to fetch similar clothes based on search query.I have used the data available in Kaggle which scrapped by Shreyas. I tried scrapping but it is time-consuming. I will provide the code snippets in GitHub to scrape the data from Myntra.https://www.kaggle.com/shreyas90999/mycasestudy02eeThe data has different types of clothes(upper ware, lower wear, and footwear) for ladies. As this data scrapped from Myntra we have masks or bounding boxes for article/clothes localization/detection. So, for the article detection and localization part, I have taken the data from Kaggle Competiton: iMaterialist (Fashion) 2019 at FGVC6. This Fashion data has 45.2k files approximately with output In Encoded_pixel Format with class_labels.I took the below picture from a research paper.From the paper, the architecture is as explained below.2. Pose detection: https://nanonets.com/blog/human-pose-estimation-2d-guide/In this blog, various approaches are used for posing detection problems. By using these pre-trained models, we can save a lot of time. Choosing the architecture that works best on the dataset then later fine-tune or modify the architecture to get the best results.3. Localization / article detection: https://valohai.com/blog/clothes-detection-for-fashion-recommendation/In this blog, users have explained about different labeled datasets for fashion object detection. This pre-trained dataset can be used on top of our data to increase the accuracy of the model. The blog has a detailed explanation of how to use object detection API for Tensorflow with good snippets of code which will help try the above models first as a black box and then later choose the architecture that gives the best results on our datasets.object detection API for Tensorflow came several pre-implemented architectures with pre-trained weights on the COCO (Common Objects in Context) dataset, such asSSD (Single-Shot Multi-box Detector) with Mobile NetsSSD with Inception V2.R-FCN (Region-based Fully Convolutional Networks) with Resnet 101.Faster RCNN (Region-based Convolutional Neural Networks) with Resnet 101.Faster RCNN with Inception Resnet v24. Triplet Loss: https://towardsdatascience.com/image-similarity-using-trip%20let-loss-3744c0f67973This blog has a good explanation of how to use Triplet loss for image similarity problems. So, my understanding of Triplet Loss architecture helps us to learn distributed embedding by the notion of similarity and dissimilarity. It’s a kind of neural network architecture where multiple parallel networks are trained that share weights among each other. During prediction time, input data is passed through one network to compute distributed embeddings representation of input data.Loss function: The cost function for Triplet Loss is as follows:L(a, p, n) = max(0, D(a, p) — D(a, n) + margin)where D(x, y): the distance between the learned vector representation of x and y. As a distance metric L2 distance or (1 — cosine similarity) can be used. The objective of this function is to keep the distance between the anchor and positive smaller than the distance between the anchor and negative.Here I will be explaining my implementation of the business problem.Module_1:In Module1, for pose detection, I tried using HRNet and TensorFlow lite models. Both model outputs are almost similar. So, I picked up TensorFlow lite. From the below snippet it is clear that both models have similar results.So, here I have used the TensorFlow lite “Posenet” pre-trained model from my research section to find all full poses and front posing images from my corpus.How do Posenet works?Pose estimation is the task of using an ML model to estimate the pose of a person from an image or a video by estimating the spatial locations of key body joints (keypoints).Pose estimation refers to computer vision techniques that detect human figures in images and videos, so that one could determine, for example, where someone’s elbow shows up in an image. It is important to be aware of the fact that pose estimation merely estimates where key body joints are and does not recognize who is in an image or video.The PoseNet model takes a processed camera image as the input and outputs information about key points. The key points detected are indexed by a part ID, with a confidence score between 0.0 and 1.0. The confidence score indicates the probability that a key point exists in that position.Results:Module_2:In module 2, we have to detect all the articles and localize them. For that, I have used the MaskRcnn model. I took data from Kaggle for the competition “iMaterialist (Fashion) 2019 at FGVC6”. After the localization, we have to crop the images and pass them to Module 3 for generating embedding.How do MaskRCNN works?Mask R-CNN (regional convolutional neural network) is a two-stage framework: the first stage scans the image and generates proposals (areas likely to contain an object). And the second stage classifies the proposals and generates bounding boxes and masks. Mask R-CNN paper is an extension of its predecessor, Faster R-CNN, by the same authors. Faster R-CNN is a popular framework for object detection, and Mask R-CNN extends it with instance segmentation, among other things.This tutorial requires TensorFlow version 1.15.3 and Keras 2.2.4. It does not work with TensorFlow 2.0+ or Keras 2.2.5+ because a third-party library has not been updated at the time of writing.!pip install — no-deps tensorflow==1.15.3!pip install — no-deps keras==2.2.4Mask R-CNN is basically an extension of Faster R-CNN. Faster R-CNN is widely used for object detection tasks. The Mask R-CNN framework is built on top of Faster R-CNN. So, for a given image, Mask R-CNN, in addition to the class label and bounding box coordinates for each object, will also return the object mask.Similar to the ConvNet that we use in Faster R-CNN to extract feature maps from the image, we use the ResNet 101 architecture to extract features from the images in Mask R-CNN. So, the first step is to take an image and extract features using the ResNet 101 architecture. These features act as an input for the next layer.Now, we take the feature maps obtained in the previous step and apply a region proposal network (RPM). This basically predicts if an object is present in that region (or not). In this step, we get those regions or feature maps that the model predicts contain some objects.The regions obtained from the RPN might be of different shapes, right? Hence, we apply a pooling layer and convert all the regions to the same shape. Next, these regions are passed through a fully connected network so that the class label and bounding boxes are predicted.Till this point, the steps are almost like how Faster R-CNN works. Now comes the difference between the two frameworks. In addition to this, Mask R-CNN also generates the segmentation mask.For that, we first compute the region of interest so that the computation time can be reduced. For all the predicted regions, we compute the Intersection over Union (IoU) with the ground truth boxes. We can computer IoU like this:IoU = Area of the intersection / Area of the unionNow, only if the IoU is greater than or equal to 0.7+, we consider that as a region of interest. Otherwise, we neglect that region. We do this for all the regions and then select only a set of regions for which the IoU is greater than 0.7+.Building a Model for our DataFirst, let’s build a model with coco_weights by calling mask rcnn pre-trained model. Then modify the config python file as per our requirements ( please refer to GitHub link below ).Create our own dataset by modifying by inheriting the “utils. Dataset” python file. Now split the data into train and test.After few epochs, I stopped my model which neither overfits not underfits.After localization and article detection these particular articles are cropped and sent to Module-3 for generating the embedding.Results:Module_3:In Module 3, I tried with DenseNet121, ResNet50, ResNet101, MobileNet, and InceptionV3. Out of all these. DenseNet121 gave good results.DenseNet121 is low sparsity compared with others. I choose Densenet121.DenseNet generated 1024 dimensional embedding with low sparsity.As we saw that we have 8 categories of data, I have divided them into 3 super categories for indexing as below.Upper_wear: women_shirts_tops_tessLower_wear: women jeans juggings, women skirts, women trousers Foot_wear: women casual shoes, flats, heelsModule_4:In Module 4, I have used FAISS (Facebook AI Similarity Search) library to retrieve similar articles.Faiss works only with float32 type ndarray. So, first we have convert our embedding into ndarray type float32.Now let’s create 3type indexies with faiss for upper ware , lower ware , and footware.github.comFrom the Facebook Github page provided above, IndexFlatL2 is a brute-force one compared with others that use Euclidean distance to calculate the nearest distance. So, I used it. We can use cosine as well, but we have to normalize the vector before using cosine. Normally cosine distance is used in text similarities.The index takes only one parameter, which is nothing but a vector with any shape but if we are passing multiple vectors make sure all vectors are in the same shape.we have a search method with Faiss which depends upon index values to retrieve similar articles. In the search method also we have to pass the vector with the shape that matches with the indexing vector.As the generate embedding in module 3 returns a list with lenght 2014, not in the same shape ( row vector ) and type ( float 32 ndarray). we have to convert it first before searching.So we have an out final solution. Model is able to detect and retrieve fashion objects from the given image. So there are a few wrong object detection and wrong retrieval but this is because the model is trained for fewer epochs. Some wrong retrievals are because whole images are embedded and not the objects. Also, the database size is also very small. But overall, we have a first-cut solution that can be further expanded and optimized. please check out in Github link.Please go through my GitHub profile to have a glance at the code.Click here to connect with me on LinkedIn.",29/04/2021,0,35,4,"(577, 488)",13,7,0.0,20,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
63,An overview of model explainability in modern machine learning,Towards Data Science,Rui Aguiar,67.0,10.0,1832,"Model explainability is one of the most important problems in machine learning today. It’s often the case that certain “black box” models such as deep neural networks are deployed to production and are running critical systems from everything in your workplace security cameras to your smartphone. It’s a scary thought that not even the developers of these algorithms understand why exactly the algorithms make the decisions they do — or even worse, how to prevent an adversary from exploiting them.While there are many challenges facing the designer of a “black box” algorithm, it’s not completely hopeless. There are actually many different ways to illuminate the decisions a model makes. It’s even possible to understand which features are the most salient in a model’s predictions.In this article, I give a comprehensive overview of model explainability for deeper models in machine learning. I hope to explain how deeper models more traditionally considered “black boxes” can actually be surprisingly explainable. We use model-agnostic methods to apply interpretability to all different kinds of black box models.A partial dependence plot shows the effect of a feature on the outcome of a ML model.Partial dependence works by marginalizing the machine learning model output over the distribution of the features we are not interested in (denoted by features in set C). This makes it such that the partial dependence function shows the relationship between the features we do care about (which we denote by buying in set S) and the predicted outcome. By marginalizing over the other features, we get a function that depends only on features in S. This makes it easy to understand how varying a specific feature influences model predictions. For example, here are 3 PDP plots for Temperature, Humidity and Wind Speed as relating to predicted bike sales by a linear model.PDP’s can even be used for categorical features. Here’s one for the effect of season on bike rental.For classification, the partial dependence plot displays the probability for a certain class given different values for features. A good way to deal with multi-class problems is to have one PDP per class.The partial dependence plot method is useful because it is global. It makes a point about the global relationship between a certain feature and a target outcome across all values of that feature.AdvantagesPartial Dependence Plots are highly intuitive. The partial dependence function for a feature at a value represents the average prediction if we have all data points assume that feature value.DisadvantagesYou can really only model a maximum of two features using the partial dependence function.Assumption of independence: You are assuming the features that you are plotting are not correlated with any other features. For example, if you are predicting blood pressure off of height and weight, you have to assume that height is not correlated with weight. The reason this is the case is that you have to average over the marginal distribution of weight if you are plotting height (or vice-versa). This means, for example, that you can have very small weights for someone who is quite tall, which you probably not see in your actual dataset.Great! I want to implement a PDP for my model. Where do I start?Here’s an implementation with with scikit-learn.Permutation feature importance is a way to measure the importance of a feature by calculating change in a model’s prediction error after permuting the feature. A feature is “important” if permuting its values increases the model error, and “unimportant” if permuting the values leaves the model error unchanged.The algorithm works as follows:After you have sorted the features by descending FI, you can plot the results. Here is the permutation feature importance plot for the bike rentals problem.AdvantagesInterpretability: Feature importance is just how much the error increases when a feature is distorted. This is easy to explain and visualize.Permutation feature importance provides global insight into the model’s behavior.Permutation feature importance does not require training a new model or retraining an existing model, simply shuffling features around.DisadvantagesIt’s not clear whether you should use training or test data for your plot.If features are correlated, you can get unrealistic samples after permuting features, biasing the outcome.Adding a correlated feature to your model can decrease the importance of another feature.Great! I want to implement Permutation Feature Importance for my model. Where do I start?Here’s an implementation with the eli5 model in Python.ALE plots are a faster and unbiased alternative to partial dependence plots. They measure how features influence the prediction of a model. Because they are unbiased, they handle correlated features much better than PDP’s do.If features of a machine learning model are correlated, the partial dependence plot cannot be trusted, because you can generate samples that are very unlikely in reality by varying a single feature. ALE plots solve this problem by calculating – also based on the conditional distribution of the features – differences in predictions instead of averages. One way to interpret this is by thinking of the ALE as saying“Let me show you how the model predictions change in a small “window” of the feature.”Here’s a visual interpretation of what is going on in an ALE plot.This can also be done with two features.Once you have computed the differences in predictions over each window, you can generate an ALE plot.ALE plots can also be done for categorical features.AdvantagesALE plots are unbiased, meaning they work with correlated features.ALE plots are computationally fast to compute.The interpretation of the ALE plot is clear.DisadvantagesThe implementation of ALE plots is complicated and difficult to understand.Interpretation still remains difficult if features are strongly correlated.Second-order or 2D ALE plots can be hard to interpret.Generally, it is better to use ALE’s over PDP’s, especially if you expect correlated features.Great! I want to implement ALE’s for my model. Where do I start?Here’s a library that provides an ALE implementation.Individual Conditional Expectation (ICE) plots display one line per data point. It produces a plot that shows how the model’s prediction for a data point changes as a feature varies across all data points in a set. For the plot below, you can see the ICE plots for varying temperature, humidity and wind speed across all instances in the training set bike rental data.Looking at this plot, you may ask yourself: what is the point of looking at an ICE plot instead of a PDP? It seems much less interpretable.PDPs can only show you what the average relationship between what a feature and a prediction looks like. This only works well if the interactions between the features for which the PDP is calculated and the other features are uncorrelated, but in the case of strong, correlated interactions, the ICE plot will be more insightful.AdvantagesLike PDP plots, ICE plots are very intuitive to understand.ICE plots can uncover heterogeneous relationships better than PDP plots can.DisadvantagesICE curves can only display one feature at a time.The plots generated by this method can be hard to read and overcrowded.Great! I want to implement ICE for my model. Where do I start?Here’s an overview of interpretability with an ICE implementation.A surrogate model is an interpretable model (such as a decision tree or linear model) that is trained to approximate the predictions of a black box. We can understand the black box better by interpreting the surrogate model’s decisions.The algorithm for generating a surrogate model is straightforward.One way to measure how well the surrogate replicates the black box through the R-squared metric:The R-squared metric is a way to measure the variance captured by the surrogate model. An R-squared value close to 1 implies the surrogate model captures the variance well, and close to 0 implies that it is capturing very little variance, and not explaining the black box model well.AdvantagesThis approach is intuitive: you are learning what the black box model thinks is important by approximating it.Easy to measure: It’s clear how well the interpretable model performs in approximating the black box through the R-squared metric.DisadvantagesThe linear model may not approximate the black box model well.You are drawing conclusions about the black box model and not the actual data, as you are using the black box’s model predictions as labels without seeing the ground truth.Even if you do approximate the black box model well, the explainability of the “interpretable” model may not actually represent what the black box model has learned.It may be difficult to explain the interpretable model.Great! I want to implement a surrogate model.Where do I start?Here’s an overview of interpretability with a surrogate model implementation.As machine learning becomes more prominent in daily life, the future of interpretability is more important than ever before. I believe a few trends will categorize the future of interpretability, and this will shape how we interact with AI models in the future.Model agnostic interpretability focusAll the trends in deep learning research point to the fact that deep networks are not saturating with our current computing and data limits. It’s important to realize that as our models get deeper and deeper in everything from image recognition to text generation, there is a need for methods that can provide interpretability across all types of models. The generalizability aspect will become more and more useful the more machine learning takes a hold in different fields. The methods I discussed in this blog post are a start, but we need to take interpretability more seriously as a whole to better understand why the machine learning systems powering our day-to-day are making the decisions they do.Models that explain themselvesOne trend that I have not seen take hold in most ML systems that I believe will exist in the future is the idea of a self-explainable model. Most systems today simply make a decision with reasons that are opaque to the user. In the future, I believe that will change. If a self-driving car makes a decision to stop, we will know why. If Alexa cannot understand our sentence, it will tell us in specific detail what went wrong and how we can phrase our query more clearly. With models that explain themselves, we can better understand how the ML systems in our lives work.Increased model scrutinyFinally, I believe that we as a society have pushed black-box model scrutiny under the rug. We do not understand the decisions that our models are making, and that doesn’t seem to be bothering anyone in particular. This will have to change in the future. Engineers and Data Scientists will be held accountable as models start to make mistakes, and this will lead to a trend where we examine the decisions our model makes with the same rigor we would a dataset that the model is trained on.I hope you enjoyed this post. I certainly found it illuminating to write, and I hope it helps you with your studies or research in the field of machine learning.Works Cited:Most of the examples here inspired from the excellent Interpretable Machine Learning book.(Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/)I highly encourage you to buy it if you wish to further your knowledge in the topic.",05/12/2019,2,19,0,"(700, 416)",11,0,0.0,8,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,neutral,surprise/amazement
64,Personality for Your Chatbot with Recurrent Neural Networks,Towards Data Science,5agado,665.0,9.0,1648,"In a previous short entry, I gave an introduction to chatbots: their current high popularity, some platform options and basic design suggestions.In this post, I am going instead to illustrate what I believe is a more intriguing scenario: a deep-learning-based solution for the construction of a chatbot off-topic behavior and “personality”. In other words, when confronted with off-topic questions, the bot will try to automatically generate a possibly relevant answer from scratch, based only on a pre-trained RNN model.What follow are four self-contained sections, so you should be able to jump around and focus on just the one(s) you are interested in without problems.Chatbots (or conversational agents) can be decomposed into two separate but dependent tasks: understand and answer.Understanding is about interpretation and assignment of a semantic and pragmatic meaning to user input. Answering is about providing the most suited response, based on the information obtained during the understanding phase and based on the chatbot tasks/goals.This post provides a very good overview of two different models for the answering task, as well as going into great details for the application of deep learning in chatbots.For retrieval-based models, the answering process consists mostly of some kind of lookup (with various degrees of sophistication) from a predefined set of answers. Chatbots currently used in production environments, presented or handled to clients and customers, will most likely belong to such category.On the other hand, generative-based models are expected to, well… generate! They are most often based on basic probabilistic models or on machine learning ones. They don’t rely on a fixed set of answers, but they still need to be trained, in order to generate new content. Markov chains have originally been used for the task of text generation, but lately, Recurrent neural networks (RNN) have gained more popularity, after many promising practical examples and showcases (Karpathy’s article) Generative models for chatbots still belongs to the research sector, or to the playfield of the ones that simply enjoy building and demoing test application of their own models. I believe that, for most business use cases out there, they are still not suited for a production environment. I cannot picture a client who wouldn’t bring up Tay if proposed with a generative model option.A Recurrent Neural Network is a deep learning model dedicated to the handling of sequences. Here an internal state is responsible for taking into consideration and properly handle the dependency that exists between successive inputs (crash course on RNN).Apart from the relative elegance of the model, it's impossible not to get captured and fascinated by it, simply from the many online demos and examples showcasing its generative capabilities. From handwriting to movie script generation.Given its properties, this model is really well suited for various NLP tasks, and exactly in the text generation context I started exploring it, playing with basic concepts using Theano and Tensorflow for then moving to Keras for the final models training. Keras is a high-level neural networks library, that can run on top of either Theano or Tensorflow, but if you are willing to learn and play with the more basic mechanisms of RNN and machine learning models in general, I suggest to give a try to one of the other libraries mentioned, especially if following again the great tutorials by Denny Britz.For my task I trained a sequence-to-sequence model at word-level: I feed to the network a list of words and expect the same as output. Instead of using a vanilla RNN, I used a long/short term memory (LSTM) layer, which guarantees better control over the memory mechanism of the network (understanding LSTM). The final architecture includes just two LSTM layers, each followed by dropout.As for now I still rely on one-hot encoding of each word, often limiting the size of the vocabulary (<10000). A highly advised next step would be to explore the option of using words embedding instead.I trained the model on different corpora: personal conversations, books, songs, random datasets and movie subtitles. Initially the main goal has been pure text generation: starting from nothing and generating arbitrarily long sequences of words, exactly like in Karpathy’s article. With my modest setup, I still obtained fairly good results, but you can see how this approach doesn’t work on the same assumptions of text generation for chatbots, which is at the end a question-answering scenario.Question-answering is another big NLP research problem, with its own ecosystem of complex and component-heterogeneous pipelines. Even when focusing only on deep learning, different solutions with different levels of complexity exists. What I wanted to do here is to first experiment with my baseline approach and see the results for off-topic questions handling.I used the Cornell Movie — Dialogs Corpus, and built a training dataset based on the concatenation of two consecutive interactions that resembled a question-answer situation. Each of such q-a pair ends up constituting a sentence of the final training set. During training the model gets as input a sentence truncated from the last element, while the expected output is the same truncated from the first word.Given such premises, the model is not really learning what is an answer and what is a question, but should build an internal representation that can coherently generate text. This either by generating a sentence from scratch starting with a random element, or simply by proceeding in the completion of a seed sentence (the potential question), one word at a time, until predefined criteria are met (e.g. a punctuation symbol is produced). All the newly generated text is then retained and provided as candidate answer.You can find additional details and WIP implementation in my Github repository. All critiques and comments are more than welcome.Interfacing with the chatbot is as simple as sending a message on Facebook Messanger, but the complete solution involves different heterogeneous components. Here a minimal view of the current architectureData processing and RNN model training have been operated on a Spark instance hosted on IBM Data Science platform. I interfaced with it directly via a Jupyter Notebooks, which simply rocks!Using Keras callbacks system I automatically kept track of the model performances during training, and backed-up the weights when appropriate. At the end of each training the best snapshot (model weights) was then persistently moved to the Object Storage connected with the Spark instance, together with Keras model architecture and additional data related to the training corpus (e.g. vocabulary indexing).The second piece is the model-as-a-service component; a basic Flask RESTful API that exposes the trained models for text generation via REST calls. Each model is a different endpoint and accepts different parameters to use for the generation task. Examples of parameters areInternally this application is responsible for retrieving and loading in memory the models from the remote Object Storage, such that they are ready to generate text when corresponding endpoints are called.The final component is a Java Liberty web application which acts as a broker for the Facebook Messanger Platform. This is responsible for handling Facebook webhooks and subscriptions, storing users chat history and implement the answering-task logic. From one side it relies on the system described in my previous article, using IBM Watson services like Language Recognition and Conversation, on the other, when specific requirements are met or when no valid answer has been provided, can rely on the text generation bit, and call the Flask API at the most convenient endpoint.Both the Java and the Python app are hosted on Bluemix, and regarding the former one, I’m currently working on the coverage of additional messaging platforms like Slack, Whatsapp and Telegram.You can interact with the chatbot simply via Facebook Messanger, but making your bot public (usable by whoever reaches its page) requires some work, demonstration videos and successive official approval from Facebook. As for now, I have to manually add people as testers of my app to allow them to use it, so in case you are interested, just drop me a note.Nevertheless, I have to admit that already seeing the interactions of the few current testers was a pretty entertaining experience, a good emotional roller-coaster of amusement, shame, creepiness, pride…Let’s start with some mixed results (on the right the bot replies, on the left some friends’ inputs, nicely anonymized).Notice that here there is mixed behavior: the “marry me” response is based on Natural Language Classification, while the rest are all generated. There are already grammatically wrong sentences, but at the same time I was for example nicely impressed by the second answer, being fooled into reading a sense of conscious omnipotence in it.Sometimes replies seem totally random, but still build a nice interaction, a simulation of a shy, confused and ashamed personality, a bit romantic too maybe.Given the training data, it also learned proper punctuation, so it’s likely to reply with a sentence starting with a punctuation symbol if the previous input was not ending with one itself.It can also come up with some seemingly deep stuff, for then failing miserably especially given the momentary new high expectations.Notice that in no way there is context retention between answers, it’s simply not built in the models nor system, is just the interaction flow that gives this illusion, and sometimes can coincidentally give a really good impression:I am aware that there is no breakthrough in all this, and results might me “mhe”, but after all, so many people get crazily excited about their “baby’s first words”, which from my knowledge are way below the bar I set here…My decent understanding of the mechanisms behind it, while observing it talking, makes everything even more fascinating. Feels irrationally surprising all it can formulate, with no actual semantic knowledge of what it’s being told and what it’s replying, just statistical signals and patterns… and I wonder, after all, how many people might be actually working in pretty much the same way, just with a more powerful borrowed computation instance in their skull, and a longer and richer running training on their shoulder.",29/03/2017,0,7,0,"(406, 318)",7,2,0.0,22,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
65,Graduating in GANs: Going from understanding generative adversarial networks to running your own,Towards Data Science,Cecelia Shao,1300.0,11.0,1917,"Generative Adversarial Networks (GANs) have taken over the public imagination —permeating pop culture with AI- generated celebrities and creating art that is selling for thousands of dollars at high-brow art auctions.In this post, we’ll explore:There is a wealth of resources for catching up on GANs, so our focus for this article is to understand how GANs can be evaluated. We’ll also walk you through running your own GAN to generate handwritten digits like MNIST.Since its inception in 2014 with Ian Goodfellow’s ‘Generative Adversarial Networks’ paper, progress with GANs has exploded and led to increasingly realistic outputs.Just three years ago, you could find Ian Goodfellow’s reply on this Reddit thread to a user asking about whether you can use GANs for text:“GANs have not been applied to NLP because GANs are only defined for real-valued data.GANs work by training a generator network that outputs synthetic data, then running a discriminator network on the synthetic data. The gradient of the output of the discriminator network with respect to the synthetic data tells you how to slightly change the synthetic data to make it more realistic.You can make slight changes to the synthetic data only if it is based on continuous numbers. If it is based on discrete numbers, there is no way to make a slight change.For example, if you output an image with a pixel value of 1.0, you can change that pixel value to 1.0001 on the next step.If you output the word “penguin”, you can’t change that to “penguin + .001” on the next step, because there is no such word as “penguin + .001”. You have to go all the way from “penguin” to “ostrich”.Since all NLP is based on discrete values like words, characters, or bytes, no one really knows how to apply GANs to NLP yet.”Now GANs are being used to create all kinds of content including images, video, audio, and (yup) text. These outputs can be used as synthetic data for training other models or just for spawning interesting side projects like thispersondoesnotexist.com, thisairbnbdoesnotexist.com/, and This Machine Learning Medium post does not exist. 😎A GAN is comprised of two neural networks — a generator that synthesizes new samples from scratch, and a discriminator that compares training samples with these generated samples from the generator. The discriminator’s goal is to distinguish between ‘real’ and ‘fake’ inputs (ie. classify if the samples came from the model distribution or the real distribution). As we described, these samples can be images, videos, audio snippets, and text.To synthesize these new samples, the generator is given random noise and attempts to generate realistic images from the learnt distribution of the training data.The gradient of the output of the discriminator network (a convolutional neural network) with respect to the synthetic data informs how to slightly change the synthetic data to make it more realistic. Eventually the generator converges on parameters that reproduce the real data distribution, and the discriminator is unable to detect the difference.You see and play with these converging data distributions with GAN Lab:poloclub.github.ioHere’s a selection of the best guides on GANs :Quantifying the progress of a GAN can feel very subjective — “Does this generated face look realistic enough?” , “Are these generated images diverse enough?” — and GANs can feel like black boxes where it’s not clear which components of the model impact learning or result quality.To this end, a group from the MIT Computer Science and Artificial Intelligence (CSAIL) Lab, recently released a paper, ‘GAN Dissection: Visualizing and Understanding Generative Adversarial Networks’, that introduced a method for visualizing GANs and how GAN units relate to objects in an image as well as the relationship between objects.Using a segmentation-based network dissection method, the paper’s framework allow us to dissect and visualize the inner workings of a generator neural network. This occurs by looking for agreements between a set of GAN units, referred to as neurons, and concepts in the output image such as tree, sky, clouds, and more. As a result, we’re able to identify neurons that are responsible for certain objects such as buildings or clouds.Having this level of granularity into the neurons allows for edits to existing images (e.g. to add or remove trees as shown in the image) by forcefully activating and deactivating (ablating) the corresponding units for those objects.However, it’s not clear if the network is able to reason about objects in a scene or if it’s simply memorizing these objects. One way to get closer to an answer for this question was to try to distort the image in unrealistic ways. Perhaps the most impressive part of MIT CSAIL’s interactive web demo of GAN Paint was how the model is seemingly able to limit these edits to ‘photorealistic’ changes. If you try to impose grass onto the sky, here’s what happens:Even though we’re activating the corresponding neurons, it appears as though the GAN has suppressed the signal in later layers.Another interesting way of visualizing GANs is to conduct latent space interpolation (remember, the GAN generate new instances by sampling from the learned latent space). This can be a useful way of seeing how smooth the transitions across generated samples are.These visualizations can help us understand the internal representations of a GAN, but finding quantifiable ways to understand GAN progress and output quality is still an active area of research.Two commonly used evaluation metrics for image quality and diversity are: the Inception Score and the Fréchet Inception Distance (FID). Most practitioners have shifted from the Inception Score to FID after Shane Barratt and Rishi Sharma released their paper ‘A Note on the Inception Score’ on key shortcomings of the former.Invented in Salimans et al. 2016 in ‘Improved Techniques for Training GANs’, the Inception Score is based on a heuristic that realistic samples should be able to be classified when passed through a pre-trained network, such as Inception on ImageNet. Technically, this means that the sample should have a low entropy softmax prediction vector.Besides high predictability (low entropy), the Inception Score also evaluates a GAN based on how diverse the generated samples are (e.g. high variance or entropy over the distribution of generated samples). This means that there should not be any dominating classes.If both these traits are satisfied, there should be a large Inception Score. The way that you combine the two criteria is by evaluating the Kullback-Leibler (KL) divergence between the conditional label distribution of samples and the marginal distribution from all the samples.Introduced by Heusel et al. 2017, FID estimates realism by measuring the distance between the generated distribution of images and the true distribution. FID embeds a set of generated samples into a feature space given by a specific layer of Inception Net. This embedding layer is viewed as as a continuous multivariate Gaussian, then the mean and covariance are estimated for both the generated data and the real data. The Fréchet distance between these two Gaussians (a.k.a Wasserstein-2 distance) is then used to quantify the quality of generated samples. A lower FID corresponds to more similar real and generated samples.An important note is that FID needs a decent sample size to give good results (suggested size = 50k samples ). If you use too few samples, you will end up over-estimating your actual FID and the estimates will have a large variance.For a comparison of how Inception Scores and FID scores have differed across papers, see Neal Jean’s post here.Aji Borji’s paper, ‘Pros and Cons of GAN Evaluation Measures’ includes an excellent table with more exhaustive coverage of GAN evaluation metrics:Interestingly, other researchers are taking different approaches by using domain-specific evaluation metrics. For text GANs, Guy Tevet and his team proposed using traditional probability-based language model metrics to evaluate the distribution of text generated by a GAN in their paper ‘Evaluating Text GANs as Language Models’.In ‘How good is my GAN?’, Konstantin Shmelkov and his team use two measures based on image classification, GAN-train and GAN-test, which approximate the recall (diversity) and precision (quality of the image) of GANs respectively. You can see these evaluation metrics in action in the Google Brain research paper, ‘Are GANS created equal’, where they used a dataset of triangles to measure the precision and the recall of different GAN models.To illustrate GANs, we’ll be adapting this excellent tutorial from Wouter Bulten that uses Keras and the MNIST dataset to generate written digits.See the full tutorial notebook here.This GAN model takes in the MNIST training data and random noise as an input (specifically, random vectors of noise) to generate:The Generator and Discriminator models together form the adversarial model — for this example, the generator will perform well if the adversarial model serves an output classifying the generated images as real for all inputs.See the full code here and the full Comet Experiment with results hereWe’re able to track the training progress for both our Generator and Discriminator models using Comet.ml.We’re plotting both the accuracy and loss for our discriminator and adversarial models — the most important metrics to track here are:See the training progression for this experiment here.You also want to confirm that your training process is actually using GPUs, which you can check in the Comet System Metrics tab.You notice that our training for-loop includes code to report images from the test vector:Part of the reason why we want to report generated output every few steps is so that we can visually analyze how our generator and discriminator models are performing in terms of generating realistic handwritten digits and correctly classifying the generated digits as ‘real’ or ‘fake, respectively.Let’s take a look at these generated outputs!See the generated outputs on your own in this Comet ExperimentYou can see how the Generator models starts off with this fuzzy, grayish output (see 0.png below)that doesn’t really look like the handwritten digits we expect.As training progresses and our models’ losses decline, the generated digits become clearer and clear. Check out the generated outputs at:Step 500:Step 1000:Step 1500:And finally at Step 10,000 — you can see some samples of the GAN-generated digits in the red outlined boxes belowOnce our GAN model is done training, we can even review our reported outputs as a movie in Comet’s Graphics tab (just press the play button!).To complete the experiment, make you sure run experiment.end() to see some summary statistics around the model and GPU usage.We could train the model longer to see how that impacts performance, but let’s try iterating with a few different parameters.Some of the parameters we play around with are:From Wouter’s original blog post, he mentions his own efforts with testing parameters:I have tested both SGD, RMSprop and Adam for the optimizer of the discriminator but RMSprop performed best. RMSprop is used a low learning rate and I clip the values between -1 and 1. A small decay in the learning rate can help with stabilizingWe’ll try increasing the discriminator’s dropout probability from 0.4 to 0.5 and increasing both the discriminator’s learning rate (from 0.008 to 0.0009) and the generator’s learning rate (from 0.0004 to 0.0006). Easy to see how these changes can get out of hand and difficult to track…🤯To create a different experiment, simply run the experiment definition cell again and Comet will issue you a new url for your new experiment! It’s nice to keep track of your experiments, so you can compare the differences:Unfortunately, our adjustments did not improve the model’s performance! In fact, it generated some funky outputs:That’s it for this tutorial! If you enjoyed this post, feel free to share with a friend who might find it useful 😎",04/04/2019,1,28,4,"(670, 504)",22,5,0.0,43,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
66,K-Nearest Neighbor,The Startup,Antony Christopher,90.0,8.0,837,"IntroductionK-nearest neighbors (KNN) is a type of supervised learning algorithm used for both regression and classification. KNN tries to predict the correct class for the test data by calculating the distance between the test data and all the training points. Then select the K number of points which is closet to the test data. The KNN algorithm calculates the probability of the test data belonging to the classes of ‘K’ training data and class holds the highest probability will be selected. In the case of regression, the value is the mean of the ‘K’ selected training points.Let see the below example to make it a better understandingSuppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category.Why do we need a K-NN Algorithm?Suppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset. Consider the below diagram:How does K-NN work?The K-NN working can be explained on the basis of the below algorithm:Suppose we have a new data point and we need to put it in the required category. Consider the below image:How to choose a K value?Kvalue indicates the count of the nearest neighbors. We have to compute distances between test points and trained labels points. Updating distance metrics with every iteration is computationally expensive, and that’s why KNN is a lazy learning algorithm.Then how to select the optimal K value?Now you will get the idea of choosing the optimal K value by implementing the model.Calculating distance:The first step is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are — Euclidian, Manhattan (for continuous) and Hamming distance (for categorical).Euclidean Distance: Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (y).Manhattan Distance: This is the distance between real vectors using the sum of their absolute difference.Hamming Distance: It is used for categorical variables. If the value (x) and the value (y) are the same, the distance D will be equal to 0 . Otherwise D=1.Ways to perform K-NNKNeighborsClassifier(n_neighbors=5, *, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’Brute ForceLets consider for simple case with two dimension plot. If we look mathematically, the simple intuition is to calculate the euclidean distance from point of interest ( of whose class we need to determine) to all the points in training set. Then we take class with majority points. This is called brute force method.k-d tree is a hierarchical binary tree. When this algorithm is used for k-NN classficaition, it rearranges the whole dataset in a binary tree structure, so that when test data is provided, it would give out the result by traversing through the tree, which takes less time than brute search.For a better understanding of how this can look like in a computer science topic, you can find below an HTML-code. A tree helps to structure a website and websites can normally be depicted using a tree.Similar to k-d trees, Ball trees are also hierarchical data structure. These are very efficient specially in case of higher dimensions.The final resulting Ball Tree as follows,Comparison and SummaryBrute Force may be the most accurate method due to the consideration of all data points. Hence, no data point is assigned to a false cluster. For small data sets, Brute Force is justifiable, however, for increasing data the KD or Ball Tree is better alternatives due to their speed and efficiency.KNN model implementationLet’s start the application by importing all the required packages. Then read the telecommunication data file using read_csv() function.DatasetAs you can see, there are 12 columns, namely as region, tenure, age, marital, address, income, ed, employ, retire, gender, reside, and custcat. We have a target column, ‘custcat’ categorizes the customers into four groups:Now it’s time to improve the model and find out the optimal k value.From the plot, you can see that the smallest error we got is 0.59 at K=37. Further on, we visualize the plot between accuracy and K value.Now you see the improved results. We got the accuracy of 0.41 at K=37. As we already derived the error plot and got the minimum error at k=37, so we will get better efficiency at that K value.Thanks for reading, I hope you enjoyed it!",02/02/2021,5,67,0,"(573, 366)",19,9,0.0,0,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
67,Gradient-based Adversarial Attacks : An Introduction,The Startup,Siddhant Haldar,42.0,10.0,1978,"Neural networks have lately been providing the state-of-the-art performance on most machine learning problems, even to the extent of producing superhuman performance on several tasks. However, though such superior performance might seem very alluring, it has been observed that these neural networks are not very robust in terms of dealing with slightly different distributions. Consider a neural network that has been trained on an input distribution X with the corresponding label set L. Given a input example x with a corresponding label l, it can be shown that an adversarial example x’ can be obtained from x by adding a very small perturbation to the original input such that x’ is classified differently as compared to x. The fact that such a small perturbation can cause the existing neural networks to falter prevents these models from being used in security-critical areas. In this blog, we will be summarising the basic gradient-based attack mechanisms that have been developed for performing white-box (model parameters and weights are available to the attacker) adversarial attacks on neural networks.Gradient based adversarial attacks exploit a very simple idea originating from the concepts involved in back-propagation(an algorithm used to train deep neural networks). We will go through back-propagation in a very informal manner in this blog since the main focus here is on summarising gradient based techniques for developing adversarial examples.For training deep neural networks, the back-propagation algorithm is widely used. The first step here involves calculating the error(using a specific error function) between the desired output and the network output corresponding to a particular input. Now keeping the input constant, the calculated error is used to compute the gradients corresponding to each parameter(also known as weight) of the network. These gradients are used update the weights at each step taking into consideration a specific learning rate.Gradient based attacks use this concept to develop a perturbation vector for the input image by making a slight modification to the back-propagation algorithm. Contrary to common practice, while back-propagating through the network, it considers the model parameters(or weights) to be constant and the input to be a variable. Hence, gradients corresponding to each element of the input (for example, pixels in case of images) can be obtained. These gradients have been utilised in various manners to obtain the perturbation vector, such that the new adversarial example has a greater tendency towards being misclassified, provided that it satisfies the constraint of being very similar to the input. Since the input gradients are used to obtain these perturbation vectors, these are known as gradient based attacks.Some of these gradient based adversarial attack techniques have been explained below.A prerequisite for understanding the mathematics behind these methods is a basic knowledge of calculus and the concept of norms of vectors and matrices.In their paper, the authors argue that :“ .. neural networks are too linear to resist linear adversarial perturbation. LSTMs (Hochreiter & Schmidhuber, 1997), ReLUs (Jarrett et al., 2009; Glorot et al., 2011), and maxout networks (Goodfellow et al., 2013c) are all intentionally designed to behave in very linear ways, so that they are easier to optimize. More nonlinear models such as sigmoid networks are carefully tuned to spend most of their time in the non-saturating, more linear regime for the same reason. This linear behaviour suggests that cheap, analytical perturbations of a linear model should also damage neural networks.”Therefore, the authors focus on producing an adversarial example x’ = x + η such that x’ is classified incorrectly by the neural network. In order to make x’ and x produce different outputs, η should be bigger than the precision of the features. For example, if each pixel of an image is represented by 8 bits, any information below 1/255 of the dynamic range is discarded. In order to achieve this, the authors use the following scheme for computing η :Here, J is the cost function used to train the neural network, θ represents the parameter of a model, x is the input to the model and y is the target associated with x (for machine learning tasks that have targets). Here, ε decides the size of the perturbation and sign of every element of the perturbation vector(or matrix/tensor) is decided by the sign of the input gradient at the element. This solution is motivated by linearizing the cost function and solving for the perturbation that maximizes the cost subject to an L∞ constraint. This linear perturbation technique method reliably causes a wide variety of models to misclassify their input. This method does not require an iterative procedure to compute adversarial examples, and thus is much faster than other considered methods.In this paper, the authors suggest a very simple improvement to FGSM. They suggest applying the same step as FGSM multiple times with a small step size and clip the pixel values of intermediate results after each step to ensure that they are in an ε-neighbourhood of the original image. This method is also referred to as Iterative-FGSM or IFGSM. Mathematically, the attacking scheme can be demonstrated as :For the experiments, the value of α used is 1, i.e., the pixel values are changed only by 1 at each step. . The number of iterations were chosen heuristically; it is sufficient for the adversarial example to reach the edge of the ε max-norm ball but restricted enough to keep the computational cost of experiments manageable.Both the methods described till now have focused on simply trying to increase the cost of the correct class, without specifying which of the incorrect classes the model should select. A technique for this has been shown in the paper. Since this blog focuses on a basic overview of gradient based adversarial attacks, we are skipping explicit details from the paper. Kindly refer to the paper for knowledge about the same.The momentum method is a technique for accelerating gradient descent algorithms by accumulating a velocity vector in the gradient direction of the loss function across iterations. The memorisation of previous gradients helps to barrel through narrow valleys, small humps and poor local minima or maxima. The momentum method also shows its effectiveness in stochastic gradient descent to stabilize the updates. We apply the idea of momentum to generate adversarial examples and obtain tremendous benefits.FGSM generates an adversarial example by applying the sign of the gradient to a real example only once by the assumption of linearity of the decision boundary around the data point. However in practice, the linear assumption may not hold when the distortion is large, which makes the adversarial example generated by FGSM underfit the model, limiting its attack ability. As an improvement, iterative FGSM greedily moves the adversarial example in the direction of the sign of the gradient in each iteration. However, in this case, the adversarial example can easily drop into poor local maxima and overfit the model, which is not likely to transfer across models. The property of producing attacks that can be transferred to other models whose parameters are not accessible to the attacker is known as the transferability of an attack.Thus, in this paper, the authors integrate momentum into the iterative FGSM for the purpose of stabilizing update directions and escaping from the poor local maxima. By using momentum, the gradients are updated at each step by accumulating the velocity vector in the gradient direction asAfter gradient calculation, the adversarial sample at the (t+1)-th step is computed asThe momentum-based method helps retain the transferability of adversarial examples when increasing iterations (by preventing the adversarial example from dropping into a local maxima), and at the same time acts as a strong adversary for the white-box models like iterative FGSM. It alleviates the trade-off between the attack ability and the transferability, demonstrating strong black-box attacks.The Projected Gradient Descent (PGD) attack is essentially the same as BIM (or IFGSM) attack. The only difference is that PGD initializes the example to a random point in the ball of interest (decided by the L∞ norm) and does random restarts, while BIM initializes to the original point.In general, Projected Gradient Descent is a well-known method in the optimization literature [paper]. In that sense, it is better to use “PGD” to refer to this (quite general) method instead of the narrow “IFGSM” terminology. Strictly speaking, the version of PGD that we are talking about is the non-euclidean, L∞-PGD that uses the L∞ norm as a distance function.In this approach, the authors propose to generate adversarial samples by considering the following optimization problemwhere x is fixed and the goal is to find δ that minimizes D(x, x+δ). Thus, we want to find a small change δ such that when added to an image x, the image is misclassified (to a targeted class t) by the model but the image is still a valid image. Here, D is some distance metric (L0, L2 or L∞ according to the paper) and C is the model being used.One problem here is that it very difficult for the existing algorithms to solve the constraint C(x+δ) = t due to its highly non-linear nature. Thus, the authors express the problem in a different form suitable for optimization. They define an objective function f such that C(x+δ) = t if and only if f(x + δ) ≤ 0. The paper proposes a list of possible choices of fwhere s is the correct classification, (e)+ is short-hand for max(e, 0), softplus(x) = log(1 + exp(x)), and lossF,s(x) is the cross entropy loss for x. Some of the above formula have been adjusted by adding a constant. However, this has been done only so that the function respects the original problem definition and it does not affect the final result. It simply scales the minimization function.Now, instead of formulating the problemthis alternate formulation is used :where c > 0 is a suitably chosen constant. Considering the distance metric D to be the p-th norm of a set of vectors, the optimization problem is reduced to finding δ that solvesNow, in order to satisfy the constraint in the optimization problem, a change-of-variables is applied and thus, we optimize over w, settingUsing this change-of-variable, since −1 ≤ tanh(w) ≤ 1, the constraint 0 ≤ x+δ ≤ 1 is satisfied. Hence, this becomes a simple minimization problem now and according to the experiments conducted by the authors, the most effective optimization algorithm for solving this problem is the Adam optimizer.For more details about searching for the optimal value of c and convergence proofs, kindly refer to the paper.Standard supervised training does not mandate that the chosen model will be resistant to adversarial examples. This property must be encoded into the training procedure somehow and that is where adversarial training comes in.It can be shown that by training on a mixture of adversarial and clean examples, a neural network could be regularized somewhat. In general, data augmentation uses techniques such as translation, rotation, etc in order to produce input samples similar to what might be expected in the test set. However, training on adversarial samples is a little different in the sense that it uses inputs that are unlikely to occur naturally but that expose flaws in the ways that the model conceptualises its decision function. Hence, a wide range of literature in the domain of adversarial machine learning uses adversarial training to make models robust against such adversarial examples. And since these adversarial attacks have been observed to be transferable, adversarial training using samples generated from a single model provides robustness to other models performing the same task as well.This blog has briefly touched upon some of the gradient based techniques for generating adversarial samples. Other techniques that one must definitely familiarise themselves with is DeepFool and the research paper “Learn to Pay Attention”. I hope this blog serves as a worthy introduction to the field of adversarial attacks.Thank you for your time in reading this blog. Kindly contact me if you have any queries about the topic.",09/04/2020,0,18,65,"(332, 105)",12,0,0.0,8,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,trust/acceptance
68,Earth at a Cute Angle,Nightingale,Robert Simmon,2000.0,7.0,1106,"Satellite imagery surrounds us — from Google Maps and daily weather forecasts to the graphics illustrating news stories — but almost all of it is from a map-like, top-down perspective. This view allows satellite data to be analyzed over time and compared with other sources of data. Unfortunately, it’s also a distorted perspective. Lacking many of the cues we use to interpret the world around us, top-down satellite imagery (often called nadir imagery in remote sensing jargon) appears unnaturally flat. It’s a view that is disconnected from our everyday experience.We’re used to seeing things from the side. Walking around at ground level, standing on a mountaintop, or even gazing out an airplane window, we’re never looking straight down. Our everyday perspective is more like this view of San Francisco from Maxar’s Worldview-3 satellite. Although not the most common type of imagery from orbit, these oblique views (also known as off-nadir views) connect our own lifetime of experiences with the unfamiliar view from space.In many ways, the prevalence of nadir views—pictures taken directly beneath a satellite as it flies over the Earth’s surface—is an anomaly. The first decades of aerial photography — such as this view of San Francisco in the aftermath of the 1906 earthquake, photographed from a camera suspended by a kite — consisted entirely of pictures taken at an angle.Even the first photos from space were oblique. This panorama was acquired from a V2 rocket launched from White Sands Proving Ground, New Mexico on July 26, 1948. It shows much of the U.S. southwest and the Earth’s limb — the thin veil of atmosphere that separates us from outer space.Early spy satellites, such as the Corona missions, used photos of the Earth’s horizon to help analysts determine the precise location of the imagery.Astronauts & cosmonauts continued this tradition, snapping photographs through the windows of their spacecraft from the earliest orbital flights, until today.This sequence of photos from the International Space Station illustrates the structure of Sarychev Volcano’s ash plume during a 2009 eruption. Look closely and you can see the ash flowing down the side of the volcano.In some ways, even geostationary satellites (positioned exactly over the equator but far enough above the Earth to see almost an entire hemisphere at once) collect primarily oblique data, since their footprint extends far from directly beneath the satellite.I personally became interested in working with oblique imagery from Planet’s SkySat constellation while trying to visualize a series of deadly debris flows that struck Montecito, California after devastating wildfires swept through the mountains above the town.Unfortunately, the top-down view lacks the visual cues that convey a sense of depth and obscures the relationship between terrain and landslide. Oblique imagery, however, is more like our everyday perspective and gives an immediate sense of how topography contributed to the disaster.Let’s take a look at how oblique satellite imagery is acquired with the help of this image of another California natural hazard—wildfire. The Kincade Fire ravaged Sonoma county in October and November 2019. This view, acquired while the fire was at its height, highlights the relationship between fire, smoke, and terrain. To get it, Planet’s collection planning team had to aim the satellite far off its ground track.The image was collected at 60˚off nadir — more than twice as far from straight down as a high-res satellite typically operates (plus or minus 30˚). Smaller angles are considered “near-nadir” meaning the imagery can be made to behave more or less like a map.At 60˚ off-nadir at an altitude of 500 kilometers (310 miles), the satellite was extraordinarily far away from the target — in the case of the Kincade Fire more than 1,000 kilometers (620 miles) to the east of Healdsburg, over central Utah.How does oblique imagery differ from nadir imagery? This view of downtown Houston was collected at only 12˚ off-nadir. It’s sharp, high-resolution, and relatively easy to line up features in the image with their true position on the ground.This oblique image taken at 60˚ shows the profile of the downtown skyline at the expense of the map-like precision we take for granted in satellite imagery. It’s also lower resolution, has more interference from the atmosphere, and was harder to collect and process. And yet…there’s something compelling about this viewpoint.So why bother? There are a few concrete applications — like the ability to see under things. This off-nadir view of the North Korean port of Sinpho, reveals a submarine berthed beneath a newly constructed awning, meant to hide its presence from overhead observers.Combine oblique images with nadir images and it’s possible to derive the 3D shape of a surface and create a digital elevation model. The more images collected at a wider variety of angles, the more accurate and detailed the map.If you are interested in collecting video from orbiting satellites, gathering oblique imagery is almost inevitable due to their high speeds. Watch how this view of the ski trails of the Breckenridge resort evolves as the satellite swings further and further from nadir.But to me, the most interesting aspect of oblique imagery is the way it reveals the form of a landscape and acts as a bridge between our lived experience and abstract data. From the dramatic cascade of ancient, hardened lava down the sides of the Grand Canyon to the rapids of the Colorado River……to the u-shaped valleys, hanging glaciers, and towering granite cliffs of Baffin Island, Canada. See if you can spot a silhouette of the Empire State Building, an element of human scale on the landscape.Zoom in and the fractal patterns of the glaciated landscape give way to snow-covered slopes, and long shadows cast by immense vertical walls. This combination of perspective and scale is impossible to achieve in any other way.The off-nadir perspective can even help illustrate the extremes of sport, such as these images that show the length and steepness of two of the most difficult and storied climbs of the Tour de France.Cartographers have taken advantage of the oblique view for centuries, using a bird’s-eye view to illustrate everything from the whole of Italy to Maui and Haleakalā National Park.The connection between what we see from the familiar ground-level viewpoint and the novel, top-down perspective of a satellite view is what makes oblique imagery so powerful. Likewise, presenting new information in the context of pre-existing knowledge is an essential element in successfully communicating unfamiliar ideas. In both science communication and data visualization, it is essential to use the familiar to build a bridge to the novel.In your own discipline, try to find the examples that connect the tangible to the intangible, the every day to the exotic, and the known to the unknown.",25/01/2021,0,0,3,"(776, 481)",35,0,0.0,66,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
69,A basic Named entity recognition (NER) with SpaCy in 10 lines of code in Python,Python in Plain English,Aaron Yu,39.0,5.0,642,"Have you ever come to a situation that you find a bunch of old articles in your hard drive and you believe they were precious, however, you couldn’t remember what the articles are about. Or a situation that you remembered you read something in your archive and was trying to find out which document that was, however, you just could not find it. Well, it happened to me quite often. I hoped I could have an index to all the articles I stored so that I can locate them quickly. And now, I am trying to create a small piece of Python code to do that for me.I want to properly tag my documents with the keywords which best define the content. The good thing is that I have a list of keywords that I use to organize my documents, such as Cloud, Security, Architecture, Digital, etc.I need a Named entity recognition (NER) library to extract entities from my document. Lucky for me, there are a few good libraries to choose from, e.g. NLTK and spaCy. I have no intention to get a degree in NER, so I made a quick decision to try spaCy. With a few lines of code, I am all set.spaCy can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning. It’s good for tokenization, NER, Rule-based Matching, Text Classification. More details refer to the spaCy online doc. To start simple, rule-based matching is good enough for my problem. There are 2 types of rule-based matchers provided by spaCy: Token Matcher and Phrase Matcher.I have another article discussing more interesting rule-based matching with EntityRuler A Closer Look at EntityRuler in SpaCy Rule-based MatchingLet’s start with the simpler one: Phrase Matcher.10 lines of code exactly, haha. Well, you need more code to complete the script. However, you get the idea of how simple it is to use spaCy. What it does is to create a match pattern from phrases defined in the terms list. Then add the pattern to phraseMatcher. “matches” are the matched output from spaCy, which give you the match id, start and end position of the match in the input document. So a simple for loop can help to print out the result more friendly.However, there is a problem. I was trying to find out instances of IT, in the upper case as Information Technology. But phrase matcher gave me IT and it as well. This is where Token matcher comes in. The attribute LOWER lets you create case-insensitive match patterns, but it applies to the matcher level. Token Matcher allows you to control the token at each token level.You can see token matcher takes in patterns, which allows you to define aTry the code, and it works! It only matches the upper case “IT”, instead of “it” or “It”.I’d like to find the most mentioned terms in the documents so that I can eventually classify/tag them. Counter library is handy to get it done. It simply counts the occurrence of match terms and picks the top 3 as in my case.Now, I know the above input document is more about cloud computing!Give it a try with other token attributes (LEMMA, IS_NUM, LIKE_EMAIL, etc.) and extended patterns (IN, NOT_IN, etc.), you can achieve much more with spaCy.The full code is here:Did you know that we have four publications? Show some love by giving them a follow: JavaScript in Plain English, AI in Plain English, UX in Plain English, Python in Plain English — thank you and keep learning!Also, we’re always interested in helping to promote good content. If you have an article that you would like to submit to any of our publications, send an email to submissions@plainenglish.io with your Medium username and what you are interested in writing about and we will get back to you!",10/05/2020,5,11,6,"(446, 218)",4,2,0.0,8,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
70,For You is for YOU,,Refind,2600.0,1.0,153,"Our feed is front and center and the goal is clear: We want Refind to be the feed with the highest signal-to-noise ratio for your (professional) interests. We have a long way to go but today we’re announcing another step.Over the last couple of days, we made some changes to the underlying ranking algorithm of the For you feed: links by friends with similar interests appear closer to the top.When we started out, the ranking was a simple function with two arguments: the age of a link, and the number of saves. Over time, we added a couple of features to the ranking and today we’re also employing a simple neural net to learn how strongly the interest of you and all your friends align.The neural net detects changes in your interests and adapts the ranking of links in the For you feed accordingly. It’s not Skynet, but close — watch your back.",03/08/2016,0,0,5,"(700, 1300)",1,0,0.0,0,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
71,A simple design pattern for recurrent deep learning in TensorFlow,,Dev Nag,2400.0,5.0,792,"tl; dr: You can hide/encapsulate the state of arbitrary recurrent networks with a single page of codeIn an ideal world, every deep learning paper proposing a new architecture would link to a readily-accessible Github repository with implemented code.In reality, you often have to hand-code the translated equations yourself, make a bunch of assumptions, and do a lot of debugging before you get something that may or may not be related to the authors’ intent.This process is especially fraught when dealing with recurrent architectures (aka “recurrent neural networks”): computational graphs which are DGs (directed graphs) but not DAGs (directed acyclic graphs). Recurrent architectures are especially good at modeling/generating sequential data — language, music, video, even video games — anything where you care about the order of data rather than just pure input/output mapping.However, because we can’t directly train directed graphs with directed cycles (whew!), we have to implement and train graphs that are transformations of the original graph (going from “cyclic” to “unrolled” versions) and then use Backpropagation through time (BPTT) on these shadow models. In essence, we’re mapping connections across time to connections across space:Now, if you’re just using vanilla LSTM/GRU, there are off-the-shelf components that you can duct-tape together easily. That’s not the problem. The hard part is taking a new recurrent architecture and trying to code the novel graph while also handling all the unrolled state tricks without introducing new bugs.For example, suppose you found yourself perusing Alex Graves’ bodice-ripping classic from 2013, “Generating Sequences with Recurrent Neural Networks”, and wanted to implement his LSTM.Sigh.Everywhere you see t-1 as a subscript is yet another place (and yes, Virginia, there are 7 in that little brick of symbols) that you need to worry about recurrent state: initializing it, retrieving it from the past, and saving it for the future.If you look at TensorFlow tutorials, you’ll see a lot of code dedicated to worrying about packing and unpacking the various recurrent states. There’s much room for error here, and a cruel and unusual intermingling of architecture and logistics. It becomes half-declarative, half-procedural…and all-fugly.But with just a tiny amount of bookkeeping code, you can make it so much easier, and almost live the dream (!) of typing in recurrent equations and getting working TensorFlow code out the other side. We can even steal TensorFlow’s idiom for get-or-create scoped variables. Let’s briefly look at the relevant stanza:Here, the bp variable is a BPTT object that’s responsible for all recurrent state. There are two interesting method calls here. bp.get_past_variable() handles both initialization from a random constant and retrieval of past state (t-1), and bp.name_variable() saves the current state for future suitors.Look how close this code is to the raw mathematical equations — I’ve left out the shape definitions for the weight matrices and bias vectors for clarity, but for the most part it’s a 1-to-1 mapping: easy to write and easy to read.The only mental overhead is retrieving the recurrent variable(immediately before usage) and saving it (inline with usage), all in local context. There’s no reference to this state anywhere else in the graph-building code. In fact, the rest of the code bears a striking resemblance to non-recurrent TensorFlow.Then, to generate the shadow (unrolled) model, we just call on the bp object to generate the sequence of connected graphs with a single line:This sequence of graphs has placeholders at the right places (where those inline constants will come back to make a dramatic cameo) and are stitched together at every bp.get_past_variable() call.During training (or inference), there are three places where all this recurrent state must be brought back into play. First, we have to send the working state into the feed dictionary (either the initialized constants defined above, or the working state from a previous training loop), and insert the training data into the unrolled placeholders. Second, we have to define the state variables we want returned from the session.run() method. Third, we have to take the post-session.run() results and extract out the state for the future.The BPTT object takes care of that bookkeeping as well.Note that we’re also passing a flag (bp.DEEP) in many of the calls here, during the training phase. This is because another common design pattern of recurrent networks is that you first train the unrolled/deep network but then infer using the cyclic/shallow network (with the same, shared post-training parameters).When we infer, we use the bp.SHALLOW flag which has a different set of placeholder variables and thus needs to manage a different state pipeline. There’s also a convenience method (copy_state_forward()) to copy the final unrolled/deep state (recurrent variables) into the cyclic/shallow network before starting inference.Recurrent deep learning in TensorFlow can be — if not easy — a little bit easier.Want to check out the code + sample usage? Say no more.",08/10/2016,0,18,19,"(700, 241)",6,0,0.0,4,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,neutral,joy/calmness
72,37 Reasons why your Neural Network is not working,Slav,Slav Ivanov,4700.0,10.0,1575,"The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. “What did I do wrong?” — I asked my computer, who didn’t answer.Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. I’ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.0. How to use this guide?I. Dataset issuesII. Data Normalization/Augmentation issuesIII. Implementation issuesIV. Training issuesA lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:If the steps above don’t do it, start going down the following big list and verify things one by one.Check if the input data you are feeding the network makes sense. For example, I’ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.Try passing random numbers instead of actual data and see if the error behaves the same way. If it does, it’s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.Your data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.Check if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn’t an universal way to detect this as it depends on the nature of the data.This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn’t learn. Check a bunch of input samples manually and see if labels seem off.The cutoff point is up for debate, as this paper got above 50% accuracy on MNIST using 50% corrupted labels.If your dataset hasn’t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.Are there a 1000 class A images for every class B image? Then you might need to balance your loss function or try other class imbalance approaches.If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, people say you need a 1000 images per class or more.This can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.This paper points out that having a very large batch can reduce the generalization ability of the model.Thanks to @When testing new network architecture or writing a new piece of code, use the standard datasets first, instead of your own data. This is because there are many reference results for these datasets and they are proved to be ‘solvable’. There will be no issues of label noise, train/test distribution difference , too much difficulty in dataset, etc.Did you standardize your input to have zero mean and unit variance?Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.If you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?CS231n points out a common pitfall:“… any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation/test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. “Also, check for different preprocessing in each sample or batch.This will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.Again from the excellent CS231n: Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.After this, try increasing the regularization strength which should increase the loss.If you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.If you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.If your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.Sometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.Did you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.Check if you unintentionally disabled gradient updates for some layers/variables that should be learnable.Maybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.If your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.If you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: 1 2 3.Overfit a small subset of the data and make sure it works. For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.If unsure, use Xavier or He initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.Maybe you using a particularly bad set of hyperparameters. If feasible, try a grid search.Too much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent “Practical Deep Learning for coders” course, Jeremy Howard advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.Maybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.Some frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.“For weights, these histograms should have an approximately Gaussian (normal) distribution, after some time. For biases, these histograms will generally start at 0, and will usually end up being approximately Gaussian (One exception to this is for LSTM). Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification if the distribution of classes is very imbalanced.”Your choice of optimizer shouldn’t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.Check this excellent post by Sebastian Ruder to learn more about gradient descent optimizers.A low learning rate will cause your model to converge very slowly.A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.Play around with your current learning rate by multiplying it by 0.1 or 10.Getting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:Did I miss anything? Is anything wrong? Let me know by leaving a reply below.Hey friend, I’m Slav, entrepreneur and developer. Also, I’m the co-founder of Encharge — marketing automation software for SaaS companies.If you liked this article, please help others find it: hold the clap icon for as long as you think this article is worth it. Thanks a lot!http://cs231n.github.io/neural-networks-3/http://russellsstewart.com/notes/0.htmlhttps://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-classhttps://deeplearning4j.org/visualizationhttps://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phasehttp://book.caltech.edu/bookforum/showthread.php?t=4113https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm",25/07/2017,0,6,2,"(399, 287)",5,5,0.0,41,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
73,Amazon Adds Photographic Product Search To iOS App,,Richard Yao,,1.0,100,"Amazon is raising the stakes of showrooming for retailers once again, folding its “Flow” technology, previously found in a standalone app released by its subsidiary, A9, into its main shopping app for iOS. “Flow” is visual product search, allowing users to photograph an object and see details about it on Amazon, which is even simpler than the previous norm of barcode recognition. Amazon’s competitive pricing is its main advantage in comparison to retailers, and by more effectively using other retailers as showrooms for the products it sells, it has the potential to further extend its dominance in more consumer categories.",07/02/2014,0,0,0,,0,0,0.0,0,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
74,Why TRANSFORMERS 5 SUCKS so bad,,Vimal ‘Malinc,2.0,4.0,769,"I’m sure you been hooked onto the trailers bombing all over social platformsof the release of T5 since early this year and closer to releasing date. I was one of them and was just dying to grab the tickets on premiere. YET deep down something was telling me its just gonna be bad, knowing its directed by Michael, again.My pal and I have seen all Transformers premieres and time after time have proven me why it sucks every time. but the latest has brought it to whole new level of SUCKINESS and i just have to pen it why:Did you ever notice Optimus transforming any shots in the film? Even for a super mili-second? Either his just swimming through the galaxy or peeping from behind or the best, just appears out from fading smoke while rolling the wheels. Not even once you will see the main character transforming as the armoured “Last Knight” as they call it in the movie. I know it sounds petty, but why am i paying the damn tickets for after waiting for 2 bloody years?! You just ruined a man’s childhood that very moment.Being an artist, i simply couldn’t accept the fact that they would spend millions for CGI-ing unfamiliar or boring robots’ transformations instead of the main ones. Now BumbleBee can be detached and reattached. Sounds cool and so called “new concept” but are you simply or unknowingly renaming the title itself: Transformers to Detachformers? Also, Megatron has gotten his 3rd facelift Which pretty much looks alike with the cartoon Character. But here’s the million dollar question: WHY THE HELL DOES HE NEED THAT? He MIA most times, just like the previous and current movies and escapes with a simple stunt just before the “Grand ending”. Yeah i get the idea of keeping him alive for the next upcoming movie/s but sadly he has no bloody relevance to the current. Michael should have either skipped him or simply just throw him to other planets or galaxy, probably making him stumble onto Unicorn thus his awakening. At least the Dinobots could have been utilised better. The amount of time and effort could have been utilised into really constructing a concrete storyboard with really relevant characters and their massive impact with audience.So much of time, money and effort have been wasted for the wrong things in so many ways. I know hollywood love big-bucks blockbusters, but do they have any idea for what and where and whom their really splurging on?Truly an utter disaster.It’s so weird when the cybertron queen reprogrammes Optimus to her interest but it simply gets overprogrammed in a jiffy by Bumblebee’s voice. I know Micheal is trying to retrace back their friendship which Optimus have been longing to hear BB’s voice, but it could have stretched further in the film probably closer to the end, where Prime gets confused and slowly realises and regains himself. That would have also given the true meaning for “ The Last Knight” title and also some tension teasers for the audience.To me it felt like Michael realised “oh shit i’ve been too obsessed with my fireworks. let’s turnaround the big-guy before its too late or forgotten to picture the composition in my kickass camera”. The comeback was lame as lama.If you been following all of Transformer’s movies, You will notice the familiar scenes, dialogues and other nonsensical stuffs. Let me break it down for ya:Well there are still many more things that i wish to share but i guess i’ll just give a pass.When i heard Michael isn’t gonna direct the next Transformers, it truly was a sense of relief. I know you feel that i been bombarding the talented man, but that never was my intention.Sir Bay has feasted us some really remarkable movie with amazing screenplay and effects even before the times of CGI was born. He a visualiser from that time and has always amazed the 80s kids.But for now, Transformers requires more than just CGIs or effects. The story from older cartoons have to be referred in conjunction for better understanding of the characters and learn how it can be moulded or redefined to suit today’s story-telling or picturization.One thing i truly liked was the concept of how Michael fused human history with Transformers. It brought in alot of interest for even common viewers to see the film and also learn alittle of the altered past live,though not real.The past can never re-written but hoping the next Transformers will be redefining alot of concepts of the world of robots and their collide with the human race with better and stronger storyboarding.",03/07/2017,0,4,0,"(700, 394)",1,1,0.0,0,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
75,Training and Deploying A Deep Learning Model in Keras MobileNet V2 and Heroku: A Step-by-Step Tutorial Part 1,HackerNoon.com,Mohammed Alnakli,39.0,9.0,1454,"This tutorial will guide you step-by-step on how to train and deploy a deep learning model. Having scoured the internet far and wide, I found it difficult to find tutorials that take you from the beginning to the end of building and deploying deep learning models. While there are some excellent articles on various stages of this process (that I used in my own journey to deep learning), I wrote this tutorial to close what I view is an important gap.This tutorial walks you through the entire process of training a model in TensorFlow and deploying it to Heroku — code available in the GitHub repo here.The full list of the technology we are going to use:TensorFlow, Keras (and of course python) have been increasingly adopted across industries and research communities:TensorFlow has gradually increased its power score due to ease of use — it offers APIs for beginners and experts alike to quickly get into developing for desktop, mobile, web or cloud. For an easy introduction to TensorFlow see: (easy-tensorflow)The Keras website explains why it’s user adoption rate has been soaring in 2018:Since training and deployment are complicated and we want to keep it simple, I have divided this tutorial into 2 parts:In order to benefits from this blog:One of the hardest parts of solving a deep leaning problem is having a well-prepared dataset. There are three general steps to preparing data:In statistics, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population are less likely to be included than others. It results in a biased sample, a non-random sample of a population (or non-human factors) in which all individuals, or instances, were not equally likely to have been selected. If this is not accounted for, results can be erroneously attributed to the phenomenon under study rather than to the method of sampling.(Khan Academy provides great examples on how to identify bias in samples and surveys.)2. Remove outliers from data.Outliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.More on outlier detection.3. Transform our dataset into a language a machine can understand — numbers. I will only focus in this tutorial, on transforming datasets as the other two points require a blog to cover them in full details.To prepare a dataset you must of course first have a dataset. We are going to use the Fashion-MNIST dataset because it is already optimized and labeled for a classification problem.(Read more about the Fashion-MNIST dataset here.)The Fashion-MNIST dataset has 70,000 grayscale, (28x28px) images separated into the following categories:Fortunately, the majority of deep learning (DL) frameworks support Fashion-MNIST dataset out of the box, including Keras. To download the dataset yourself and see other examples you can link to the github repo — here.By default load_data()function returns training and testing dataset.It is essential to split your data into training and testing sets.Training data: is used to train the Neural Network (NN)Testing data : is used to validate and optimize the results of the Neural Network during the training phase, by tuning and re-adjust the hyperparameters.Hyperparameter are parameters whose value is set before the learning process begins.After training a Neural Network, we run the trained model against our validation dataset to make sure that the model is generalized and is not overfitting.What is overfitting? :)Overfitting means a model predicts the right result when it tests against training data, but otherwise fails to predict accurately. However, if a model predicts the incorrect result for the training data, this is called underfitting. For further explanation of overfitting and underfitting.Thus, we use the validation dataset to detect overfitting or underfitting. But, most of the time we train the model multiple times in order to have a higher score in the training and validation datasets. Since we retrain the model based on the validation dataset result, we can end up overfitting not only in training dataset but also in the validation set. In order to avoid this, we use a third dataset that is never used during training, which is the testing dataset.Here are some of the samples of the dataNormalize the data dimensions so that they are of approximately the same scale. In general, normalization makes very deep NN easier to train, especially in Convolutional and Recurrent neural network. Here are a nice explanation video and an articleOne hot encoding is a representation of categorical variables as binary vectors. Here is the full explanation if you would like to have a deep understanding, and do not hesitate to ask if you have a questionMobileNet V2 model accepts one of the following formats: (96, 96), (128, 128), (160, 160),(192, 192), or (224, 224). In addition, the image has to be 3 channel (RGB) format. Therefore, We need to resize & convert our images. from (28 X 28) to (96 X 96 X 3).Running the previous code in all our data, may eat up a lot of memory resources; therefore, we are going to use a generator. Python Generator is a function that returns an object (iterator) which we can iterate over (one value at a time).After we have split, normalized and converted the dataset, now we are going to train a model.There are many techniques for training the model, we will only cover one of them, though I believe it is one of the most important methods or strategies, — Transfer Learning.Transfer learning in deep learning means to transfer knowledge from one domain to a similar one. In our example, I have chosen the MobileNet V2 model because it’s faster to train and small in size. And most important, MobileNet is pre-trained with ImageNet dataset.ImageNet is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated.[2]Since our dataset is kind of subset of the ImageNet dataset, then we are going to transfer the knowledge of this model onto our datasets. A nice article that explains this in more detail: A Gentle Introduction to Transfer Learning for Deep LearningThe below code is important to understand when working using Transfer Learning as a technique:As using a pre-trained model (e.g. MobileNetV2 in our case), you need to pay close attention to a concept call Fine Tuning.‘Fine Tuning’, generally, is when we freeze the weights of all the layers of the pre-trained neural networks (on dataset A [e.g. ImageNet]) except the penultimate layer and train the neural network on dataset B [e.g. Fashion-MNIST], just to learn the representations on the penultimate layer. We usually replace the last (softmax) layer with another one of our choice (depending on the number of outputs we need for the new problem.[3]In our case, we have 10 classes, so we have the followingWhen do we use need Fine Tuning?Now we compile the model. Some compilation definitions:The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.[4]A loss function (categorical_crossentropy) is a measure of how good a prediction model does in terms of being able to predict the expected outcome. [5]categorical_accuracy is a metric function that is used to judge the performance of your model.[6]It is essential to the understand the following when training any deep learning model.Epoch is when an entire dataset is passed forward and backward through the neural network only once. [7]Batch Size is the total number of training examples present in a single batch [7]. And it goes a long with python generate mention previouslyIterations (steps_per_epoch) is the number of batches needed to complete one epoch [7].For a more detailed understanding read — Epoch vs Batch Size vs Iterations.94% accuracy after 5 epochs of training, but how do would we do in the test dataset:86% seems reasonable for the amount spent training — , 1 hour of CPU time.Things you could try to improve the accuracyMake sure you save the model because we are going to use in the next partWe have prepared the fashion dataset for the MobileNetV2 model. Further, we used the MobileNet model as our base model for Transfer Learning.In part 2 we are going to prepare the model to be server by TensorFlow Serving. Then we will deploy the model to Heroku.Questions/suggestions? — Leave them below in the comments.",03/11/2018,10,32,2,"(486, 217)",4,9,0.0,38,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
76,Reuters issues a 1-page online annual report,SlideMagic,Jan Schultink,1100.0,2.0,419,"Slowly, large publicly traded companies are starting to experiment with Internet and social media as alternatives to the dry and boring annual report. Reuters just issued a 1-page version (see it here), with a promise that over the next few days, this will be followed up with more detailed blog posts about the company and its financial results.Early online annual reports were a pain. They basically were print documents put online and you had to keep on clicking on “next” links to get to the following page. Rendering of tables and data in basic HTML also did not provide good results. Combine that with slow page loads and you get a pretty useless experience.New display protocols (and certainly HTML5) will give PowerPoint-like fast navigation controls to online documents. I think a hybrid of a traditional slide presentation, advanced (and fast) web navigation, video and other multi-media, plus social aspects will give a powerful distribution platform for financial data. You get the best of both worlds: PowerPoint-style visualization of data and key strategic messes, and nicely-formated text for those who want more details. Certainly for public investor presentations such as this one, but also for confidential documents. You can distribute login details to (potential) investors you want to share your data with and exclude them from the online data room as soon as the deal process has stopped.There are also great opportunities in sales. I rarely use a PowerPoint presentation anymore to introduce my own services for example. Rather, I just take people through a few sections of my web site, either in person or via a remote presentation tool. SalesCrunch, the company that is organizing my upcoming New York presentations, is trying to create a platform for these remote sales presentations.Back to the Reuters annual report. I learned something from the way the graphics of the financials are done. I like the dividend history chart, a bit unusual, but it shows both the trend and the detail of the numbers. The big column charts might not be the right format though. Large stable companies do not grow that fast and the columns show hardly any change. Also, the big difference in absolute size between the revenue and profit column does not look optimal. It would have been better to put the $ values as text, but rather make a graph of the % increase.I am looking forward to the additional blog posts that will come out over the next week.Thank you Dominic Jones for leading me to this report.",25/03/2011,0,0,0,"(300, 200)",1,0,0.0,7,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,anger/irritation
77,Messing around with OpenAI Gym,Craftworkz,Deevid De Meyer,55.0,2.0,351,"So while I was looking around for interesting Python-based AI projects I came across OpenAI Gym, backed by mister Elon Musk himself. This application aims to provide the ultimate sandbox environment for AI-research, but at the same succeeds in providing the ultimate geek toy for AI-researchers. Let me show you why.First of all it might be useful to explain what OpenAI Gym actually does: OpenAI Gym aims to provide an easy environment to develop and test reinforcement learning algorithms. To be clear, OpenAI Gym doesn’t power any algorithms itself, leaving it up to more specialised packages like TensorFlow or Theano.So what makes this the ultimate geek toy for AI-researchers? Well, this is because of the many environments OpenAI Gym provides, one of them being the ‘atari’ environment. That’s right, you can test the performance of your reinforcement learning algorithms on a variety of different atari games and what’s more, you can automatically upload the performance of your algorithms and compare them to other people’s approaches.To save myself a few weeks of research and computing time, I decided to download one of the top performing algorithms and see how it performs. The only input the algorithm gets is the raw RGB values of the screen pixels and the current score. Additionally, the algorithm is also made aware of all the possible actions within the game thanks to the OpenAI Gym framework.On the right you can see that the final algorithm is the result of about eighty thousand iterations. The result, seen on the left, is quite impressive: notice for example how miss pacman waits in the top-right corner until a ghost arrives, which she promptly consumes using the power pellet.To conclude, OpenAI Gym is a cool platform for anybody involved with reinforcement learning algorithms. The platform will allow you to test your algorithms in a variety of different environments without having to go through the hassle of making the right inputs available to your algorithm. It has been proven many times before that open-sourcing encourages innovation, and we can’t wait to see what kind of influence OpenAI will have in the future!",23/11/2016,0,0,2,"(782, 297)",3,0,0.0,1,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,neutral,joy/calmness
78,TensorFlow: How to freeze a model and serve it with a python API,metaflow-ai,Morgan,2600.0,4.0,553,"We are going to explore two parts of using an ML model in production:Note: if you want to see the kind of graph I save/load/freeze, you can hereIf you wonder how to save a model with TensorFlow, please have a look at my previous article before going on.let’s start from a folder containing a model, it probably looks something like this:The important files here are the “.chkp” ones. If you remember well, for each pair at different timesteps, one is holding the weights (“.data”) and the other one (“.meta”) is holding the graph and all its metadata (so you can retrain it etc…)But when we want to serve a model in production, we don’t need any special metadata to clutter our files, we just want our model and its weights nicely packaged in one file. This facilitate storage, versioning and updates of your different models.Luckily in TF, we can easily build our own function to do it. Let’s explore the different steps we have to perform:Note that the two first steps are the same as when we load any graph in TF, the only tricky part is actually the graph “freezing” and TF has a built-in function to do it!I provide a slightly different version which is simpler and that I found handy. The original freeze_graph function provided by TF is installed in your bin dir and can be called directly if you used PIP to install TF. If not you can call it directly from its folder (see the commented import in the gist).So let’s see:Now we can see a new file in our folder: “frozen_model.pb”.As expected, its size is bigger than the weights file size and lower than the sum of the two checkpoints files sizes.Note: In this very simple case, the weights file size is very small, but it is usually multiple Mbs.Naturally, after knowing how to freeze a model, one might wonder how to use it.The little trick to have in mind is to understand that what we dumped to the disk was a graph_def ProtoBuf. So to import it back in a python script we need to:We can build a convenient function to do so:Now that we built our function to load our frozen model, let’s create a simple script to finally make use of it:Note: when loading the frozen model, all operations got prefixed by “prefix”. This is due to the parameter “name” in the “import_graph_def” function, by default it prefixes everything by “import”.This can be useful to avoid name collisions if you want to import your graph_def in an existing Graph.For this part, I will let the code speaks for itself. After all this is a TF series about TF and not so much about how to build a server in python. Yet it felt kind of unfinished without it, so here you go, the final workflow:Note: We are using flask in this exampleThis article is part of a more complete series of articles about TensorFlow. I’ve not yet defined all the different subjects of this series, so if you want to see any area of TensorFlow explored, add a comment! So far I wanted to explore those subjects (this list is subject to change and is in no particular order):Note: TF is evolving fast right now, those articles are currently written for the 1.0.0 version.",25/11/2016,0,9,11,"(636, 174)",3,5,0.0,11,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,subjective,positive,joy/calmness
79,Word Level English to Marathi Neural Machine Translation using Encoder-Decoder Model,Towards Data Science,Harshall Lamba,1000.0,13.0,2567,"Recurrent Neural Networks (or more precisely LSTM/GRU) have been found to be very effective in solving complex sequence related problems given a large amount of data. They have real time applications in speech recognition, Natural Language Processing (NLP) problems, time series forecasting, etc. This blog nicely explains some of these applications.Sequence to Sequence (often abbreviated to seq2seq) models are a special class of Recurrent Neural Network architectures typically used (but not restricted) to solve complex Language related problems like Machine Translation, Question Answering, creating Chat-bots, Text Summarization, etc.The purpose of this blog post is to give a detailed explanation on how sequence to sequence models are built and to give an intuitive understanding of how they solve these complex tasks.We will take the problem of Machine Translation (translating a text from one language to another, in our case from English to Marathi) as the running example in this blog. However the technical details apply to any sequence to sequence problem in general.Since we are using Neural Networks to perform Machine Translation, more commonly it is called as Neural Machine translation (NMT).This post assumes that you:a. Know Fundamental concepts in Machine Learning and Neural Networksb. Know High School Linear Algebra and Probabilityc. Have working knowledge of LSTM networks in Python and KerasThe Unreasonable Effectiveness of Recurrent Neural Networks (explains how RNNs can be used to build language models) and Understanding LSTM Networks (explains the working of LSTMs with solid intuition) are two brilliant blogs that I strongly suggest to go through if you haven’t. The concepts explained in these blogs are extensively used in my post.The most common architecture used to build Seq2Seq models is the Encoder Decoder architecture. This is the one we will use for this post. Below is a very high level view of this architecture.Points to note:a. Both encoder and the decoder are typically LSTM models (or sometimes GRU models)b. Encoder reads the input sequence and summarizes the information in something called as the internal state vectors (in case of LSTM these are called as the hidden state and cell state vectors). We discard the outputs of the encoder and only preserve the internal states.c. Decoder is an LSTM whose initial states are initialized to the final states of the Encoder LSTM. Using these initial states, decoder starts generating the output sequence.d. The decoder behaves a bit differently during the training and inference procedure. During the training, we use a technique call teacher forcing which helps to train the decoder faster. During inference, the input to the decoder at each time step is the output from the previous time step.e. Intuitively, the encoder summarizes the input sequence into state vectors (sometimes also called as Thought vectors), which are then fed to the decoder which starts generating the output sequence given the Thought vectors. The decoder is just a language model conditioned on the initial states.Now we will understand all the above steps in detail by considering the example of translating an English sentence (input sequence) into its equivalent Marathi sentence (output sequence).This section provides a brief overview of the main components of the Encoder LSTM. I will keep this intuitive without going into the Mathematical arena. So here’s what they are:The LSTM reads the data one sequence after the other. Thus if the input is a sequence of length ‘k’, we say that LSTM reads it in ‘k’ time steps (think of this as a for loop with ‘k’ iterations).Referring to the above diagram, below are the 3 main components of an LSTM:a. Xi => Input sequence at time step ib. hi and ci => LSTM maintains two states (‘h’ for hidden state and ‘c’ for cell state) at each time step. Combined together these are internal state of the LSTM at time step i.c. Yi => Output sequence at time step iImportant: Technically all of these components (Xi, hi, ci and Yi) are actually vectors of floating point numbers (explained below)Let’s try to map all of these in the context of our problem. Recall that our problem is to translate an English sentence to its Marathi equivalent. For the purpose of this blog we will consider the below example. Say, we have the following sentenceInput sentence (English)=> “Rahul is a good boy”Output sentence (Marathi) => “राहुल चांगला मुलगा आहे”For now just focus on the input i.e. the English sentenceNow a sentence can be seen as a sequence of either words or characters. For example in case of words, the above English sentence can be thought of as a sequence of 5 words (‘Rahul’, ‘is’, ‘a’, ‘good’, ‘boy’). And in case of characters, it can be thought of as a sequence of 19 characters (‘R’, ‘a’, ‘h’, ‘u’, ‘l’, ‘ ‘, ……, ‘y’).We will break the sentence by words as this scheme is more common in real world applications. Hence the name ‘Word Level NMT’. So, referring to the diagram above, we have the following input:X1 = ‘Rahul’, X2 = ‘is’, X3 = ‘a’, X4 = ‘good, X5 = ‘boy’.The LSTM will read this sentence word by word in 5 time steps as followsBut one question that we must answer is how to represent each Xi (each word) as a vector?There are various word embedding techniques which map (embed) a word into a fixed length vector. I will assume the reader to be familiar with the concept of word embeddings and won’t cover this topic in detail. However, we will use the built-in Embedding Layer of the Keras API to map each word into a fixed length vector.The next question is what is the role of the internal states (hi and ci) at each time step?In very simple terms, they remember what the LSTM has read (learned) till now. For example:h3, c3 =>These two vectors will remember that the network has read “Rahul is a” till now. Basically its the summary of information till time step 3 which is stored in the vectors h3 and c3 (thus called the states at time step 3).Similarly, we can thus say that h5, c5 will contain the summary of the entire input sentence, since this is where the sentence ends (at time step 5). These states coming out of the last time step are also called as the “Thought vectors” as they summarize the entire sequence in a vector form.Then what about h0,c0? These vectors are typically initialized to zero as the model has not yet started to read the input.Note: The size of both of these vectors is equal to number of units (neurons) used in the LSTM cell.Finally, what about Yi at each time step? These are the output (predictions) of the LSTM model at each time step.But what type of a vector is Yi? More specifically in case of word level language models each Yi is actually a probability distribution over the entire vocabulary which is generated by using a softmax activation. Thus each Yi is a vector of size “vocab_size” representing a probability distribution.Depending on the context of the problem they might sometimes be used or sometimes be discarded.In our case we have nothing to output unless we have read the entire English sentence. Because we will start generating the output sequence (equivalent Marathi sentence) once we have read the entire English sentence. Thus we will discard the Yi of the Encoder for our problem.We will read the input sequence (English sentence) word by word and preserve the internal states of the LSTM network generated after the last time step hk, ck (assuming the sentence has ‘k’ words). These vectors (states hk and ck) are called as the encoding of the input sequence, as they encode (summarize) the entire input in a vector form. Since we will start generating the output once we have read the entire sequence, outputs (Yi) of the Encoder at each time step are discarded.Moreover you must also understand what type of vectors are Xi, hi, ci and Yi. What are their sizes (shapes) and what do they represent. If you have any confusion understanding this part, then you need to first strengthen your understanding of LSTM and language models.Unlike the Encoder LSTM which has the same role to play in both the training phase as well as in the inference phase, the Decoder LSTM has a slightly different role to play in both of these phases.In this section we’ll try to understand how to configure the Decoder during the training phase, while in the next section we’ll understand how to use it during inference.Recall that given the input sentence “Rahul is a good boy”, the goal of the training process is to train (teach) the decoder to output “राहुल चांगला मुलगा आहे”. Just as the Encoder scanned the input sequence word by word, similarly the Decoder will generate the output sequence word by word.For some technical reasons (explained later) we will add two tokens in the output sequence as follows:Output sequence => “START_ राहुल चांगला मुलगा आहे _END”Now consider the diagram below:The most important point is that the initial states (h0, c0) of the decoder are set to the final states of the encoder. This intuitively means that the decoder is trained to start generating the output sequence depending on the information encoded by the encoder. Obviously the translated Marathi sentence must depend on the given English sentence.In the first time step we provide the START_ token so that the decoder starts generating the next token (the actual first word of Marathi sentence). And after the last word in the Marathi sentence, we make the decoder learn to predict the _END token. This will be used as the stopping condition during the inference procedure, basically it will denote the end of the translated sentence and we will stop the inference loop (more on this later).We use a technique called “Teacher Forcing” wherein the input at each time step is given as the actual output (and not the predicted output) from the previous time step. This helps in more faster and efficient training of the network. To understand more about teacher forcing, refer this blog.Finally the loss is calculated on the predicted outputs from each time step and the errors are back propagated through time in order to update the parameters of the network. Training the network over longer period with sufficiently large amount of data results in pretty good predictions (translations) as we’ll see later.The entire training process (Encoder + Decoder) can be summarized in the below diagram:Let’s now try to understand the setup required for inference. As already stated the Encoder LSTM plays the same role of reading the input sequence (English sentence) and generating the thought vectors (hk, ck).However, the decoder now has to predict the entire output sequence (Marathi sentence) given these thought vectors.Let’s try to visually understand by taking the same example.Input sequence => “Rahul is a good boy”(Expected) Output Sequence => “राहुल चांगला मुलगा आहे”Step 1: Encode the input sequence into the Thought Vectors:Step 2: Start generating the output sequence in a loop, word by word:At t = 1At t = 2At t = 3At t = 4At t = 5a. During inference, we generate one word at a time. Thus the Decoder LSTM is called in a loop, every time processing only one time step.b. The initial states of the decoder are set to the final states of the encoder.c. The initial input to the decoder is always the START_ token.d. At each time step, we preserve the states of the decoder and set them as initial states for the next time step.e. At each time step, the predicted output is fed as input in the next time step.f. We break the loop when the decoder predicts the END_ token.The entire inference procedure can be summarized in the below diagram:Nothing beats the understanding developed when we actually implement the code, no matter how much efforts are put in to understand the theory (that does not however mean that we do not discuss any theory, but what I mean to say is theory must always be followed by implementation).Download and unzip mar-eng.zip file from here.Before we start building the models, we need to perform some data cleaning and preparation. Without going into too much details, I will assume the reader to understand the below (self explanatory) steps that are usually a part of any language processing project.Below we compute the vocabulary for both English and Marathi. We also compute the vocabulary sizes and the length of maximum sequence for both the languages. Finally we create 4 Python dictionaries (two for each language) to convert a given token into an integer index and vice-versa.Then we make a 90–10 train and test split and write a Python generator function to load the data in batches as follows:Then we define the model required for training as follows:You should be able to conceptually connect each and every line with the explanation I have provided in sections 4 and 5 above.Let’s look at the model architecture generated from the plot_model utility of the Keras.We train the network for 50 epochs with a batch size of 128. On a P4000 GPU, it takes slightly more than 2 hours to train.Finally, we generate the output sequence by invoking the above setup in a loop as follows:At this point you must be able to conceptually connect each and every line of the code in the above two blocks with the explanation provided in section 6.The purpose of this blog post was to give an intuitive explanation on how to build basic level sequence to sequence models using LSTM and not to develop a top quality language translator. So keep in mind that the results are not world class (and you don’t start comparing with google translate) for many reasons. The most important reason being is that the dataset size is very small, only 33000 pairs of sentences (yes these are too few). If you want to improve the quality of translations, I will list some suggestions towards the end of this blog. However for now, let’s see some results generated from the above model (they are not too bad either).Even though the results are not the best, they are not that bad as well. Certainly much better than what a randomly generated sequence would result in. In some sentences we can even note that the words predicted are not correct but they are semantically quite close to the correct words.Also, another point to be noticed is that the results on training set are a bit better than the results on test set, which indicates that the model might be over-fitting a bit.If you are interested to improve the quality, you can try out below measures:a. Get much more data. Top quality translators are trained on millions of sentence pairs.b. Build more complex models like Attention.c. Use dropout and other forms of regularization techniques to mitigate over-fitting.d. Perform Hyper-parameter tuning. Play with learning rate, batch size, dropout rate, etc. Try using bidirectional Encoder LSTM. Try using multi-layered LSTMs.e. Try using beam search instead of a greedy approach.f. Try BLEU score to evaluate your model.g. The list is never ending and goes on.If the article appeals you, do provide some comments, feedback, constructive criticism, etc.Full code on my GitHub repo here.If you like my explanations, you can follow me as I plan to release some more interesting blogs related to Deep Learning and AI.You can connect on LinkedIn herea. https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7b. https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.htmlc. https://arxiv.org/abs/1409.3215d. https://arxiv.org/abs/1406.1078",09/02/2019,0,9,7,"(696, 337)",14,1,0.0,13,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
80,"Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs",SyncedReview,Synced,24000.0,4.0,499,"Transformer architectures have come to dominate the natural language processing (NLP) field since their 2017 introduction. One of the only limitations to transformer application is the huge computational overhead of its key component — a self-attention mechanism that scales with quadratic complexity with regard to sequence length.New research from a Google team proposes replacing the self-attention sublayers with simple linear transformations that “mix” input tokens to significantly speed up the transformer encoder with limited accuracy cost. Even more surprisingly, the team discovers that replacing the self-attention sublayer with a standard, unparameterized Fourier Transform achieves 92 percent of the accuracy of BERT on the GLUE benchmark, with training times that are seven times faster on GPUs and twice as fast on TPUs.Transformers’ self-attention mechanism enables inputs to be represented with higher-order units to flexibly capture diverse syntactic and semantic relationships in natural language. Researchers have long regarded the associated high complexity and memory footprint as an unavoidable trade-off on transformers’ impressive performance. But in the paper FNet: Mixing Tokens with Fourier Transforms, the Google team challenges this thinking with FNet, a novel model that strikes an excellent balance between speed, memory footprint and accuracy.FNet is a layer normalized ResNet architecture with multiple layers, each of which consists of a Fourier mixing sublayer followed by a feedforward sublayer. The team replaces the self-attention sublayer of each transformer encoder layer with a Fourier Transform sublayer. They apply 1D Fourier Transforms along both the sequence dimension and the hidden dimension. The result is a complex number that can be written as a real number multiplied by the imaginary unit (the number “i” in mathematics, which enables solving equations that do not have real number solutions). Only the result’s real number is kept, eliminating the need to modify the (nonlinear) feedforward sublayers or output layers to handle complex numbers.The team decided to replace self-attention with Fourier Transform — based on 19th century French mathematician Joseph Fourier’s technique for transforming a function of time to a function of frequency — because they found it a particularly effective mechanism for mixing tokens, enabling it to provide the feedforward sublayers sufficient access to all tokens.In their evaluations, the team compared multiple models, including BERT-Base, an FNet encoder (replace every self-attention sublayer with a Fourier sublayer), a Linear encoder (replace each self-attention sublayer with linear sublayers), a Random encoder (replace each self-attention sublayer with constant random matrices) and a Feed Forward-only encoder (remove the self-attention sublayer from the Transformer layers).The team summarized their results and FNet performance as:The study shows that replacing a transformer’s self-attention sublayers with FNet’s Fourier sublayers achieves remarkable accuracy while significantly speeding up training time, indicating the promising potential of using linear transformations as a replacement for attention mechanisms in text classification tasks.The paper FNet: Mixing Tokens with Fourier Transforms is on arXiv.Author: Hecate He | Editor: Michael SarazenWe know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.",14/05/2021,0,5,3,"(687, 367)",9,1,0.0,2,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,trust/acceptance
81,Unsupervised Learning and Data Clustering,Towards Data Science,Sanatan Mishra,263.0,15.0,1929,"A task involving machine learning may not be linear, but it has a number of well known steps:One good way to come to terms with a new problem is to work through identifying and defining the problem in the best possible way and learn a model that captures meaningful information from the data. While problems in Pattern Recognition and Machine Learning can be of various types, they can be broadly classified into three categories:Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. We will focus on unsupervised learning and data clustering in this blog post.Unsupervised LearningIn some pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine how the data is distributed in the space, known as density estimation. To put forward in simpler terms, for a n-sampled space x1 to xn, true class labels are not provided for each sample, hence known as learning without teacher.Issues with Unsupervised Learning:Why Unsupervised Learning is needed despite of these issues?Unsupervised Learning can be further classified into two categories:What is Clustering?Clustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data. A loose definition of clustering could be “the process of organizing objects into groups whose members are similar in some way”. A cluster is therefore a collection of objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters.Distance-based clustering.Given a set of points, with a notion of distance between points, grouping the points into some number of clusters, such thatThe Goals of ClusteringThe goal of clustering is to determine the internal grouping in a set of unlabeled data. But how to decide what constitutes a good clustering? It can be shown that there is no absolute “best” criterion which would be independent of the final aim of the clustering. Consequently, it is the user who should supply this criterion, in such a way that the result of the clustering will suit their needs.In the above image, how do we know what is the best clustering solution?To find a particular clustering solution , we need to define the similarity measures for the clusters.Proximity MeasuresFor clustering, we need to define a proximity measure for two data points. Proximity here means how similar/dissimilar the samples are with respect to each other.There are various similarity measures which can be used.A “good” proximity measure is VERY application dependent. The clusters should be invariant under the transformations “natural” to the problem. Also, while clustering it is not advised to normalize data that are drawn from multiple distributions.Clustering AlgorithmsClustering algorithms may be classified as listed below:In the first case data are grouped in an exclusive way, so that if a certain data point belongs to a definite cluster then it could not be included in another cluster. A simple example of that is shown in the figure below, where the separation of points is achieved by a straight line on a bi-dimensional plane.On the contrary, the second type, the overlapping clustering, uses fuzzy sets to cluster data, so that each point may belong to two or more clusters with different degrees of membership. In this case, data will be associated to an appropriate membership value.A hierarchical clustering algorithm is based on the union between the two nearest clusters. The beginning condition is realized by setting every data point as a cluster. After a few iterations it reaches the final clusters wanted.Finally, the last kind of clustering uses a completely probabilistic approach.In this blog we will talk about four of the most used clustering algorithms:Each of these algorithms belongs to one of the clustering types listed above. While, K-means is an exclusive clustering algorithm, Fuzzy K-means is an overlapping clustering algorithm, Hierarchical clustering is obvious and lastly Mixture of Gaussians is a probabilistic clustering algorithm. We will discuss about each clustering method in the following paragraphs.K-Means ClusteringK-means is one of the simplest unsupervised learning algorithms that solves the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centres, one for each cluster. These centroids should be placed in a smart way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. The next step is to take each point belonging to a given data set and associate it to the nearest centroid. When no point is pending, the first step is completed and an early groupage is done. At this point we need to re-calculate k new centroids as barycenters of the clusters resulting from the previous step. After we have these k new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop we may notice that the k centroids change their location step by step until no more changes are done. In other words centroids do not move any more.Finally, this algorithm aims at minimizing an objective function, in this case a squared error function. The objective functionwhereis a chosen distance measure between a data point xi and the cluster centre cj, is an indicator of the distance of the n data points from their respective cluster centres.The algorithm is composed of the following steps:where, ‘ci’ represents the number of data points in ith cluster.Although it can be proved that the procedure will always terminate, the k-means algorithm does not necessarily find the most optimal configuration, corresponding to the global objective function minimum. The algorithm is also significantly sensitive to the initial randomly selected cluster centres. The k-means algorithm can be run multiple times to reduce this effect.K-means is a simple algorithm that has been adapted to many problem domains. As we are going to see, it is a good candidate for extension to work with fuzzy feature vectors.The k-means procedure can be viewed as a greedy algorithm for partitioning the n samples into k clusters so as to minimize the sum of the squared distances to the cluster centers. It does have some weaknesses:This last problem is particularly troublesome, since we often have no way of knowing how many clusters exist. In the example shown above, the same algorithm applied to the same data produces the following 3-means clustering. Is it better or worse than the 2-means clustering?Unfortunately there is no general theoretical solution to find the optimal number of clusters for any given data set. A simple approach is to compare the results of multiple runs with different k classes and choose the best one according to a given criterion, but we need to be careful because increasing k results in smaller error function values by definition, but also increases the risk of overfitting.Fuzzy K-Means ClusteringIn fuzzy clustering, each point has a probability of belonging to each cluster, rather than completely belonging to just one cluster as it is the case in the traditional k-means. Fuzzy k-means specifically tries to deal with the problem where points are somewhat in between centers or otherwise ambiguous by replacing distance with probability, which of course could be some function of distance, such as having probability relative to the inverse of the distance. Fuzzy k-means uses a weighted centroid based on those probabilities. Processes of initialization, iteration, and termination are the same as the ones used in k-means. The resulting clusters are best analyzed as probabilistic distributions rather than a hard assignment of labels. One should realize that k-means is a special case of fuzzy k-means when the probability function used is simply 1 if the data point is closest to a centroid and 0 otherwise.The fuzzy k-means algorithm is the following:For a better understanding, we may consider this simple mono-dimensional example. Given a certain data set, suppose to represent it as distributed on an axis. The figure below shows this:Looking at the picture, we may identify two clusters in proximity of the two data concentrations. We will refer to them using ‘A’ and ‘B’. In the first approach shown in this tutorial — the k-means algorithm — we associated each data point to a specific centroid; therefore, this membership function looked like this:In the Fuzzy k-means approach, instead, the same given data point does not belong exclusively to a well defined cluster, but it can be placed in a middle way. In this case, the membership function follows a smoother line to indicate that every data point may belong to several clusters with different extent of membership.In the figure above, the data point shown as a red marked spot belongs more to the B cluster rather than the A cluster. The value 0.2 of ‘m’ indicates the degree of membership to A for such data point.Hierarchical Clustering AlgorithmsGiven a set of N items to be clustered, and an N*N distance (or similarity) matrix, the basic process of hierarchical clustering is this:Clustering as a Mixture of GaussiansThere’s another way to deal with clustering problems: a model-based approach, which consists in using certain models for clusters and attempting to optimize the fit between the data and the model.In practice, each cluster can be mathematically represented by a parametric distribution, like a Gaussian. The entire data set is therefore modelled by a mixture of these distributions. A mixture model with high likelihood tends to have the following traits:Main advantages of model-based clustering:Mixture of GaussiansThe most widely used clustering method of this kind is based on learning a mixture of Gaussians:A mixture model is a mixture of k component distributions that collectively make a mixture distribution f(x):The αk represents the contribution of the kth component in constructing f(x). In practice, parametric distribution (e.g. gaussians), are often used since a lot work has been done to understand their behaviour. If you substitute each fk(x) for a gaussian you get what is known as a gaussian mixture models (GMM).The EM AlgorithmExpectation-Maximization assumes that your data is composed of multiple multivariate normal distributions (note that this is a very strong assumption, in particular when you fix the number of clusters!). Alternatively put, EM is an algorithm for maximizing a likelihood function when some of the variables in your model are unobserved (i.e. when you have latent variables).You might fairly ask, if we’re just trying to maximize a function, why don’t we just use the existing machinery for maximizing a function. Well, if you try to maximize this by taking derivatives and setting them to zero, you find that in many cases the first-order conditions don’t have a solution. There’s a chicken-and-egg problem in that to solve for your model parameters you need to know the distribution of your unobserved data; but the distribution of your unobserved data is a function of your model parameters.Expectation-Maximization tries to get around this by iteratively guessing a distribution for the unobserved data, then estimating the model parameters by maximizing something that is a lower bound on the actual likelihood function, and repeating until convergence:The Expectation-Maximization algorithmProblems associated with clusteringThere are a number of problems with clustering. Among them:Possible ApplicationsClustering algorithms can be applied in many fields, for instance:",20/05/2017,0,22,57,"(377, 166)",20,23,0.0,0,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
82,Unconventional balance sheet visualization,SlideMagic,Jan Schultink,1100.0,1.0,71,"Financial statements are completely unsuitable to put on a PowerPoint slide: too dense, too much information. I like to use column charts to represent this information and dramatically cut the number of categories in the process. After a while, even accountants get used to it. The chart below gives an example of a balance sheet, in a real presentation I would add data labels rounded to 1 digit behind the dot.",30/08/2011,0,0,0,"(455, 334)",1,0,0.0,0,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,anger/irritation
83,"Multi-Label, Multi-Class Text Classification with BERT, Transformers and Keras",Towards Data Science,Emil Lykke Jensen,40.0,8.0,787,"The internet is full of text classification articles, most of which are BoW-models combined with some kind of ML-model typically solving a binary text classification problem. With the rise of NLP, and in particular BERT (take a look here, if you are not familiar with BERT) and other multilingual transformer based models, more and more text classification problems can now be solved.However, when it comes to solving a multi-label, multi-class text classification problem using Huggingface Transformers, BERT, and Tensorflow Keras, the number of articles are indeed very limited and I for one, haven’t found any… Yet!Therefore, with the help and inspiration of a great deal of blog posts, tutorials and GitHub code snippets all relating to either BERT, multi-label classification in Keras or other useful information I will show you how to build a working model, solving exactly that problem.And why use Huggingface Transformers instead of Googles own BERT solution? Because with Transformers it is extremely easy to switch between different models, that being BERT, ALBERT, XLnet, GPT-2 etc. Which means, that you more or less ‘just’ replace one model for another in your code.With data. Looking for text data I could use for a multi-label multi-class text classification task, I stumbled upon the ‘Consumer Complaint Database’ from data.gov. Seems to do the trick, so that’s what we’ll use.Next up is the exploratory data analysis. This is obviously crucial to get a proper understanding of what your data looks like, what pitfalls there might be, the quality of your data, and so on. But I’m skipping this step for now, simply because the aim of this article is purely how to build a model.If you don’t like googling around take a look at these two articles on the subject: NLP Part 3 | Exploratory Data Analysis of Text Data and A Complete Exploratory Data Analysis and Visualization for Text Data.We have our data and now comes the coding part.First, we’ll load the required libraries.Then we will import our data and wrangle it around so it fits our needs. Nothing fancy there. Note that we will only use the columns ‘Consumer complaint narrative’, ‘Product’ and ‘Issue’ from our dataset. ‘Consumer complaint narrative’ will serve as our input for the model and ‘Product’ and ‘Issue’ as our two outputs.Next we will load a number of different Transformers classes.Here we first load a BERT config object that controls the model, tokenizer and so on.Then, a tokenizer that we will use later in our script to transform our text input into BERT tokens and then pad and truncate them to our max length. The tokenizer is pretty well documented so I won’t get into that here.Lastly, we will load the BERT model itself as a BERT Transformers TF 2.0 Keras model (here we use the 12-layer bert-base-uncased).We are ready to build our model. In the Transformers library, there are a number of different BERT classification models to use. The mother of all models is the one simply called ‘BertModel’ (PyTorch) or ‘TFBertModel’ (TensorFlow) and thus the one we want.The Transformers library also comes with a prebuilt BERT model for sequence classification called ‘TFBertForSequenceClassification’. If you take a look at the code found here you’ll see, that they start by loading a clean BERT model and then they simply add a dropout and a dense layer to it. Therefore, what we’ll do is simply to add two dense layers instead of just one.Here what our model looks like:And a more detailed view of the model:If you want to know more about BERTs architecture itself, take a look here.Now that we have our model architecture, all we need to do is write it in code.Then all there is left to do is to compile our new model and fit it on our data.Once the model is fitted, we can evaluate it on our test data to see how it performs.As it turns out, our model performs fairly okay and has a relatively good accuracy. Especially considering the fact that our output ‘Product’ consists of 18 labels and ‘Issue’ consists of 159 different labels.There are, however, plenty of things you could do to increase performance of this model. Here I have tried to do it as simple as possible, but if you are looking for better performance consider the following:(remember to add attention_mask when fitting your model and set return_attention_mask to True in your tokenizer. For more info on attention masks, look here. Also I have added attention_mask to the gist below and commented it out for your inspiration.)That’s it — hope you like this little walk-through of how to do a ‘Multi-Label, Multi-Class Text Classification with BERT, Transformer and Keras’. If you have any feedback or questions, fire away in the comments below.",25/08/2020,9,0,4,"(568, 409)",2,2,0.0,18,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
84,A Rock Album For AI,,Carlos Beltran,535.0,9.0,1916,"AI, immortality, and the simulation theory are all explored in Avenged Sevenfold’s “The Stage”https://open.spotify.com/album/0jwnYwJz6XHNrVAYEclQPdIt’s awesome that Avenged Sevenfold became interested in AI and wrote an entire album that revolves around the idea. In an interview with Rolling Stone, lead singer M. Shadows says the initial interest came after reading Tim Urban’s article over at waitbutwhy. It’s one of the things (along with movies like Her and The Matrix of course) that spiked my interest in AI as well, so I’d highly recommend reading it. Tim does a phenomenal job of explaining the topic, current challenges engineers are facing, and the very possible implications of this technology.The term “artificial intelligence” was first coined half a century ago. Fast forward to today, where we have have giant companies like Intel and Apple acquiring AI startups like there’s no tomorrow. It’s not a matter of whether or not we’ll be able to create machines that surpass our own capabilities, but when.Theoretical physicist and futurist Dr. Michio Kaku thinks it is possible for machines as smart as us to exist by the end of the century. Google’s chief futurist, Ray Kurzweil, believes such technology will exist as soon as 2029. The band is right in wanting its fans, and the general public, to be more aware of these ideas — they could be right around the corner.I’m no expert, but I’d like to discuss the ideas behind some of the songs and include references in case you’d like to delve deeper. And if you want to read more on the possible future of AI, I’d recommend reading Kurzweil’s book The Singularity Is Near. Although some of his predictions have been met with skepticism, the ideas presented are thought-provoking.Simply put, nanomachines are microscopic machines that will enhance us in almost every way imaginable. They’ll be able to help our immune system fight off diseases. They would create super soldiers. This technology is actually at the center of a great game series, Metal Gear Solid. This “hack” in our biological makeup will also increase our lifespans. Kurzweil imagines a future where biotechnology is so advanced that we will live forever. This is the same idea behind the song “Paradigm”. Lyrics include:Engineer the wires to your brainArchitect a code so you won’t feel the painWith your family by your side and vigor in your eyes foreverLive ForeverThe song also raises the question of what it really means to be human. What do we become when we merge with machines? Will we lose what fundamentally makes us human? It can be argued that this “merge” is the next logical step in evolution, as there is no there is no evolutionary pressure for us to do so anymore. We’ll become, as Kurzweil puts it, “Godlike”.I’m way up, a god in sizeBeyond the reach of mortals I shed my human sideFather, O’ FatherI stare at my reflection, have I lost that boy inside?Final paradigmExpanding the brain’s neocortex will allow us, for example, to pose questions in our thoughts and know the answer almost immediately (most likely thanks to our direct “brain-to-Google” connection). We’ll always have witty jokes on hand, and learning Calculus will be as simple as purchasing downloadable content. Plug and play.Besides swapping out failing body parts with prosthetics and enhancing our brains, there’s another way we’ll be able to gain immortality. Both Dr. Kaku and Kurzweil firmly believe that the advances in brain-computer interfaces will eventually allow us to upload our consciousness to machines.This sounds pretty far out there, and it really is.Scientists still have no clue how the brain works, how the billions of neurons form connections that result in learned behavior, or what dreaming is. But once these secrets are known (which might never actually happen) and we know how our brain functions, as well as what the “consciousness switch” is, the possibilities are endless. To get an idea of what’s possible, check out Black Mirror’s episode Playtest. The brain-computer interface for the game is so advanced that the player can’t distinguish between what’s real and what isn’t. I don’t want to spoil anything, but get ready for a mind fuck. Black Mirror does a great job of weaving technology with a dystopia that we might inhabit, showing a darker side of our society. It’s on Netflix, so check it out.Elon Musk sure does. He claims that the chances of us living in “base reality” is one in billions. I’d recommend watching the 3-minute video. His logic is as follows: we had Pong some 40 years ago. Two rectangles and a dot were rendered on-screen for what we called a videogame. Today, we have games with realistic graphics and they keep getting better every year. Better yet, virtual and augmented reality are right around the corner, pushing the boundaries of gaming. Eventually, we’ll have the technology to create simulated worlds that are indistinguishable from reality. Therefore, Musk claims, it is likely that we are living in an ancestor simulation created by an advanced future civilization some 10,000 years from now.Yeeaaah.The album’s 7th song, “Simulation” explores the idea that our reality might not be what it seems. Think of it this way — the brain and nervous system which we use to automatically react to the environment around us is the same brain and nervous system which tells us what the environment is.I question all the voices in my headAre they mine or have I been misled?Total understanding doesn’t seem to mean a thingWhen you can’t see behind the silver screen, a figurineThroughout the song, the “patient” is having thoughts that challenge the simulation they are living in. They are — in a sense — waking up. A darker voice, which I believe is meant to represent the ones running the show, has to reprimand the patient, reminding them that they “…only exist because we allow it”. To control the situation, the patient is to be sedated with blue comfort, a reference from The Matrix, which will make them forget they’re living in a simulation. Blissful ignorance.I won’t try to explain this one. Just watch the video. And here’s a quote from that man that might get your attention:“I have, in my life, come to a very strange place because I never expected that the movie The Matrix might be an accurate representation of the place in which we live.” — Dr. James Gates, Jr.Imagine an entity so intelligent……but that’s just it. You can’t imagine it. In the second part to his article on AI, Tim Urban compares this to a chimp being unable to understand a skyscraper is not just a part of its environment, but that humans built it. It’s not the chimp’s fault or anything, its brain is just not made to have that level of information processing.The same thing will happen when we build a machine with the collective knowledge of some 200,000 years of Homo Sapien existence. Therefore, there is no way to know what it will do or what the consequences will be. Tim depicts our situation with this entity, what he refers to as Artificial Superintelligence (ASI), beautifully:Evolution has advanced the biological brain slowly and gradually over hundreds of millions of years, and in that sense, if humans birth an ASI machine, we’ll be dramatically stomping on evolution. Or maybe this is part of evolution — maybe the way evolution works is that intelligence creeps up more and more until it hits the level where it’s capable of creating machine superintelligence, and that level is like a tripwire that triggers a worldwide game-changing explosion that determines a new future for all living things — Tim Urban (waitbutwhy.com)Mark Zuckerberg is right in saying we should be hopeful of the amount of good AI could do, but some of the smartest minds in existence are genuinely concerned. Stephen Hawking acknowledges that the successful creation of an AI will be the biggest event in history, but warns it could also end mankind. Elon Musk founded a research company OpenAI as a way to “neutralize the threat of a malicious artificial super-intelligence”.“Creating God” describes AI as a modern messiah, “the very last invention man would ever need”. It paints the picture of a utopia where this intelligence exists. At the same time, the song suggests that we could be “summoning the demon”, unable to control the outcomes. We could just be its stepping stone, as our existence after its creation becomes irrelevant.We’re creating god, master of our designsWe’re creating god, unsure of what we’ll findWe’re creating god, in search of the divineWe’re creating god. Committing suicideThe album wraps up with a 15-minute eargasm. I can’t produce words that will do “Exist” any justice. As the band described it, it’s like listening to what the Big Bang might’ve sounded like. Neil deGrasse Tyson makes a cameo at the end of the song that serves as a reminder that our problems and conflicts are minuscule in the grand scheme of things. We’re all a part of the same universe and once we as a society realize this, we can truly make progress. Here’s the full thing:“We have one collective hope: the Earth. And yet, uncounted people remain hopeless, famine and calamity abound. Sufferers hurl themselves into the arms of war; people kill and get killed in the name of someone else’s concept of God. Do we admit that our thoughts and behaviors spring from a belief that the world revolves around us? Each fabricated conflict, self-murdering bomb, vanished airplane, every fictionalized dictator, biased or partisan, and wayward son, are part of the curtains of society’s racial, ethnic, religious, national, and cultural conflicts, and you find the human ego turning the knobs and pulling the levers. When I track the orbits of asteroids, comets, and planets, each one a pirouetting dancer in a cosmic ballet, choreographed by the forces of gravity, I see beyond the plight of humans. I see a universe ever-expanding, with its galaxies embedded within the ever-stretching four-dimensional fabric of space and time. However big our world is, our hearts, our minds, our outsize atlases, the universe is even bigger. There are more stars in the universe than grains of sand on the world’s beaches, more stars in the universe than seconds of time that have passed since Earth formed, more stars than words and sounds ever uttered by all humans who have ever lived. The day we cease the exploration of the cosmos is the day we threaten the continuing of our species. In that bleak world, arms-bearing, resource-hungry people and nations would be prone to act on their low-contracted prejudices, and would have seen the last gasp of human enlightenment. Until the rise of a visionary new culture that once again embraces the cosmic perspective; a perspective in which we are one, fitting neither above nor below, but within.” — Neil deGrasse TysonThe Stage is an exceptional album, in my opinion. The band’s intentions were for fans to educate themselves, or be a bit more aware of what’s going on in this area. We can enjoy it as a rock album as well as explore the ideas behind the lyrics. I had an awesome time writing this, digging up things I’ve read and seen and unifying them in a way so others can hopefully become more interested as well. And come on, don’t tell me that the idea that we’re living in a simulation isn’t thought-provoking.Tap the ❤ button below :)My name’s Carlos and I generally write about personal development, tech, and entrepreneurship. Hit me up on Twitter!",07/11/2016,0,6,19,"(700, 524)",5,0,0.0,26,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
85,JARKAM: Lindungi Mahasiswa Dari DO dan Skorsing,Kolumnar,Serikat Mahasiswa Progresif UI,199.0,7.0,1341,"Ada hantu yang menggentayangi mahasiswaHantu DO dan Skorsing…Cobalah anda mengetik kata skorsing atau drop out di mesin pencari anda, maka akan muncul banyak berita tentang mahasiswa yang terkena drop out (DO) maupun skorsing dari kampusnya. Berdasarkan data yang dihimpun oleh Jaringan Kaum Muda (Jarkam), selama rentang periode 2016 hingga 2018 ada sedikitnya 13 kasus terkait sanksi DO yang menimpa mahasiswa dari berbagai kampus negeri maupun swasta. Data yang dihimpun Jarkam hanyalah data yang terangkat atau diberitakan oleh media-media arus utama, fakta sebenarnya mungkin lebih besar lagi. Jika melihat data yang dikeluarkan oleh Kemenristekdikti dalam buku Statistik Pendidikan Tinggi Tahun 2017 memperlihatkan ada 195. 176 mahasiswa yang terkenaData yang dirilis Jarkam memperlihatkan medio 2016–2018 ada sekitar kurang lebih 5.000 mahasiswa terkena sanksi DO dari kampusnya. Beberapa alasan yang dijadikan dasar sanksi oleh kampus yang menjadi sorotan yaitu: Pertama, mahasiswa tidak mampu membayar uang kuliah, sehingga terkena DO. Kedua, DO yang diberikan kepada mahasiswa yang ‘vokal’ mengkritik kebijakan kampusnya. Ketiga, Mahasiswa yang terkena evaluasi, tidak aktif maupun melewati masa studi.Jarkam menyoroti terkait DO yang diberikan karena tidak mampu untuk membayar uang kuliah dan mahasiswa yang kritis atas kebijakan kampusnya. DO dengan alasan tidak melunasi uang kuliah menimpa sekitar 500 mahasiswa. Pepatah ada uang ada barang sangat terlihat disini, alih-alih berusaha melakukan advokasi atau solusi bagi mahasiswanya yang tidak mampu membayar kuliah, kampus lebih memilih jalan pintas ‘membuang’ orang-orang tidak mampu tersebut.Jenis kasus yang kedua, yaitu DO yang menimpa mahasiswa yang melakukan kritik atas kebijakan kampusnya. Kampus yang sedianya menjadi tempat demokratis menjadi wilayah dimana kritik yang membangun justru dibungkam dengan senjata DO dan skorsing. Kampus dengan pejabat-pejabatnya menjadi bangunan angkuh yang anti kritik dan menjadi pelaku penindas disamping pelaku komersialisasi. Sedikitnya ada 74 mahasiswa yang terkena DO karena melakukan kritik kepada kampus. Kritik-kritik yang dilakukan mahasiswa bukannya disambut dengan perbaikan justru dijawab dengan represi oleh kampus. Kasus yang paling anyar adalah kasus Julio Belnanda Harianja yang diskorsing karena kerap melakukan advokasi mahasiswa-mahasiswa yang tidak mampu membayar uang kuliah, Julio juga kerap melancarkan kritik terhadap kebijakan kampusnya. Namun, yang didapatkan Julio justru hadiah skorsing yang penuh kejanggalan.Tiadanya PerlindunganKenapa kampus gampang sekali melakukan DO atau skorsing? Jawabannya tidak lain adalah tidak adanya perlindungan hukum bagi mahasiswa. D.O atau pemutusan status mahasiswa menjadi otonomi sepenuhnya dikuasai kampus. Undang-Undang no 12 Tahun 2012 tentang Pendidikan Tinggi tidak mengatur masalah DO atau skorsing mahasiswa, lebih parah lagi UU Dikti memberikan otonomi yang luas pada para pejabat kampus untuk mengatur rumah tangganya. Sedangkan pada Permenristekdikti no. 44 tahun 2015 tentang Standar Nasional Pendidikan Tinggi hanya mengatur batas masa studi.Otonomi yang dimiliki kampus menjadikan kampus memiliki kuasa untuk menentukan nasib mahasiswanya. Kampus mendapat otonomi tidak hanya otonomi keilmuan namun juga otonomi untuk mengatur mahasiswa dan dosen sesuai yang diinginkan kampus. Kampus menjadi hakim, jaksa, sekaligus algojo dalam melaksanakan kebijakannya. Kalau kita ambil contoh, Kasus DO yang menimpa mahasiswa UNJ pada Januari 2016, keputusan DO diberikan secara sewenang-wenang tanpa ada kesempatan untuk membela diri. Kampus menjadi penentu mana yang benar atau salah, kritik dianggap dosa dan pelakunya adalah lalat pengganggu yang harus cepat-cepat dipukul.Ketika sanksi DO ataupun skorsing menimpa mahasiswa, mereka kesulitan untuk melapor dan mencari advokasi bagi dirinya. Hal ini dikarenakan tidak adanya lembaga yang khusus menangani advokasi terkait DO dan skorsing. Mereka hanya bisa mengandalkan misalnya dalam kasus Julio, Komnas HAM. Advokasi yang dilakukan misalnya lewat komnas HAM atau LBH juga seringkali terbentur karena adanya otonomi kampus yang membuat advokasi hanya sebatas himbauan dan kurang bertaji. Dibatalkannya DO atau skorsing seringkali tergantung pada seberapa besar perhatian media atau solidaritas dari sesama mahasiswa.Jika pencegahan DO dan skorsing masih bersandar pada pengerahan massa dan pembentukan opini publik maka kemampuan atau sumber daya yang dikerahkan tidak akan mencukupi. selama peraturan yang ada belum bisa melindungi dari DO ataupun skorsing yang dilakukan semena-mena maka kasus DO dan skorsing akan terus bertambah. Usaha untuk mengadakan Keberadaan LBH Pendidikan yang didirikan pada 2004 juga tidak terlalu berpengaruh pada usaha-usaha advokasi terkait DO dan skorsing tanpa adanya dukungan peraturan yang memadai.Melindungi Hak Atas PendidikanPerlindungan bagi mahasiswa dari skorsing ataupun DO harusnya dilandasi semangat bahwa pendidikan adalah hak bagi segenap rakyat Indonesia. Tujuan yang tercantum dalam pembukaan (preambule) konstitusi kita ini menjadikan bahwa setiap individu warga negara Indonesia dapat menempuh pendidikan hingga tuntas. Keputusan DO atau skorsing yang hanya membutuhkan persetujuan dari pihak pejabat kampus dengan alasan “melanggar peraturan kampus” adalah efek dari otonomi yang diberikan kepada kampus. Ketika korban DO melapor kepada kementrian untuk memohon dibatalkannya DO atau skorsing sering kali kementrian menyerahkan kembali kepada kampus masing-masing. Di sini terlihat lemahnya kontrol pemerintah atas kampus-kampus di Indonesia mencegah DO atau skorsing yang tidak berhubungan dengan kondisi akademik mahasiswa.Keputusan DO atau skorsing dapat diberlakukan atas alasan non-akademik seperti kritisnya mahasiswa atau tidak mampu melunasi uang kuliah adalah akibat dari otonomi yang dimiliki kampus. Peraturan mengenai otonomi ini membuat kampus dapat menetapkan mekanisme memutus status mahasiswa dengan mudah. Mekanisme DO yang tidak mempunyai peraturan khusus menjadi disandarkan pada peraturan kewenangan otonomi. Kewenangan otonomi kampus yang diatur dalam pasal 63 © UU no. 12 tahun 2012 tentang pendidikan tinggi menyebutkan bahwa otonomi kampus meliputi Otonomi pengelolaan di bidang non-akademik yaitu penetapan norma dan kebijakan operasional serta pelaksanaan: a. Organisasi; b. Keuangan; c. kemahasiswaan; d. ketenagaan; dan f. sarana prasarana. Peraturan tersebut memberikan legitimasi pada kampus untuk menentukan mana yang benar dan salah tanpa ada kompromi. Mahasiswa hanya menjadi boleh mematuhi tanpa membantah atau mempertanyakan apalagi mengkritik. Jika berani protes siap-siap saja, skorsing dan DO menunggu.Mahasiswa sebagai stakeholder kampus dan pemeran utama dalam pendidikan digeser kedudukannya hanya sebagai objek yang bisa digeser sana-sini. Ketiadaan sistem yang demokratis dan melibatkan mahasiswa dalam pengambilan keputusan di kampus menjadikan mahasiswa hanya sebagai robot dengan papan kendali di tangan pejabat kampus. Patuh atau punah adalah kepastian, Kampus yang menjunjung pemikiran kritis dan terbuka tidak berlaku ketika berhadapan dengan pejabat kampus.Keputusan DO dan skorsing menjadikan mahasiswa tercerabut haknya atas pendidikan. Padahal memastikan setiap warga negara mendapat pendidikan adalah amanat konstitusi. Jika kita andaikan DO sebagai PHK dalam sistem tenaga kerja di Indonesia maka sistem pendidikan kita ketinggalan dalam melindungi warga negaranya. Dalam UU tenaga kerja, buruh dilindungi dari PHK sepihak oleh perusahaan. Pasal 151 menyebutkan Pengusaha, pekerja/buruh, serikat pekerja/serikat buruh, dan pemerintah, dengan segala upaya harus mengusahakan agar jangan terjadi pemutusan hubungan kerja. Bandingkan dengan peraturan pendidikan yang tidak mempunyai peraturan serupa bahkan menyerahkan segalanya kepada kampus (lihat pasa 62 UU dikti). Dalam ayat dua (2) pasal 151 UU tenaga kerja dapat kita baca Dalam hal segala upaya telah dilakukan, tetapi pemutusan hubungan kerja tidak dapat dihindari, maka maksud pemutusan hubungan kerja wajib dirundingkan oleh pengusaha dan serikat pekerja/serikat buruh atau dengan pekerja/buruh apabila pekerja/buruh yang bersangkutan tidak menjadi anggota serikat pekerja/serikat buruh. Sedangkan dalam UU dikti sama sekali tidak mengatur adanya hak mahasiswa untuk membela diri atau melakukan usaha-usaha advokasinya.Dalam UU tenga kerja, masih di pasal yang sama ayat tiga (3) disebutkan Dalam hal perundingan sebagaimana dimaksud dalam ayat (2) benar-benar tidak menghasilkan persetujuan, pengusaha hanya dapat memutuskan hubungan kerja dengan pekerja/buruh setelah memperoleh penetapan dari lembaga penyelesaian perselisihan hubungan industrial. Sedangkan bila kita bandingkan dengan UU pendidikan tinggi, tidak ada ketentuan untuk mengadakan lembaga yang khusus menangani advokasi terkait DO dan skorsing. Keterlibatan mahasiswa juga sangat dibatasi dilihat dari sedikitnya penjabaran atas hak mahasiswa dalam undang-undang dikti. Bila hal ini dibiarkan, maka hak-hak warga negara atas pendidikan yang demokratis, terjangkau dan menyeluruh tidak akan terwujud.Melihat hal tersebut maka Jaringan Kaum Muda (JARKAM) menyatakan sikap sebagai berikut:Demikian rilis ini kami sampaikan sebagai bagian dari perjuangan kami melakukan revolusi pendidikan di Indonesia.Hidup Rakyat Indonesia!Panjang Umur Perjuangan!29 Juli 2018Atas namaJaringan Kaum Muda (JARKAM)Referensi“Advokasi Kebijakan: Judicial Review UU DIKTI, upaya menolak liberalisasi pendidikan tinggi!” https://www.bantuanhukum.or.id/web/advokasi-kebijakan-judicial-review-uu-dikti-upaya-menolak-liberalisasi-pendidikan-tinggi/ diakses pada 25 Juli 2018“Pendidikan Tinggi Di Tengah Privatisasi dan Cengkeraman Pemerintah” diunduh dari http://www.elsam.or.id/?id=2016&lang=in&act=view&cat=c/12 diakses pada 25 Juli 2018.Putusan Mahkamah Konstitusi №33/PUU-XI/2013. Diunduh dari http://www.mahkamahkonstitusi.go.id/index.php?page=download.Putusan&id=1682 pada 25 Juli 2018Umar, Ahmad Rizky Mardhatillah “UU Pendidikan Tinggi dalam Jerat Kapitalisme” diunduh dari http://indoprogress.com/uu-pendidikan-tinggi-dalam-jerat-kapitalisme/ diakses pada 25 Juli 2018Undang-Undang Tenaga Kerja. Diunduh dari http://www.dpr.go.id/dokjdih/document/uu/196.pdf diakses pada 29 Juli 2018Undang-Undang №12 Tentang Pendidikan Tinggi diunduh dari http://kelembagaan.ristekdikti.go.id/index.php/download/undang-undang-nomor-12-tahun-2012-tentang-pendidikan-tinggi/ diakses pada 25 Juli 2018Kemenristekdikti. Buku Statistik Pendidikan Tinggi 2017. Diunduh dari https://ristekdikti.go.id/epustaka/buku-statistik-pendidikan-tinggi-2017/ diakses pada 28 Juli 2018.",02/08/2018,0,5,19,"(835, 865)",5,1,0.0,8,id,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,no emotion
86,Understanding HDBSCAN and Density-Based Clustering,Towards Data Science,Pepe Berba,267.0,21.0,3915,"HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander [8]. It stands for “Hierarchical Density-Based Spatial Clustering of Applications with Noise.”In this blog post, I will try to present in a top-down approach the key concepts to help understand how and why HDBSCAN works. This is meant to complement existing documentation such as sklearn’s “How HDBSCAN works” [1], and other works and presentations by McInnes and Healy [2], [3].Let’s start at the very top. Before we even describe our clustering algorithm, we should ask, “what type of data are we trying to cluster?”We want to have as few assumptions about our data as possible. Perhaps the only assumptions that we can safely make are:To motivate our discussion, we start with the data set used in [1] and [3].With only 2 dimensions, we can plot the data and identify 6 “natural” clusters in our dataset. We hope to automatically identify these through some clustering algorithm.Knowing the expected number of clusters, we run the classical K-means algorithm and compare the resulting labels with those obtained using HDBSCAN.Even when provided with the correct number of clusters, K-means clearly fails to group the data into useful clusters. HDBSCAN, on the other hand, gives us the expected clustering.Briefly, K-means performs poorly because the underlying assumptions on the shape of the clusters are not met; it is a parametric algorithm parameterized by the K cluster centroids, the centers of gaussian spheres. K-means performs best when clusters are:Let us borrow a simpler example from ESLR [4] to illustrate how K-means can be sensitive to the shape of the clusters. Below are two clusterings from the same data. On the left, data was standardized before clustering. Without standardization, we get a “wrong” clustering.We go back to our original data set and by simply describing it, it becomes obvious why K-means has a hard time. The data set has:While each bullet point can be reasonably expected from a real-world dataset, each one can be problematic for parametric algorithms such as K-means. We might want to check if the assumptions of our algorithms are met before trusting their output. But, checking for these assumptions can be difficult when little is known about the data. This is unfortunate because one of the primary uses of clustering algorithms is data exploration where we are still in the process of understanding the dataTherefore, a clustering algorithm that will be used for data exploration needs to have as few assumptions as possible so that the initial insights we get are “useful”; having fewer assumptions make it more robust and applicable to a wider range of real-world data.Now, we have an idea what type of data we are dealing with, let’s explore the core ideas of HDBSCAN and how it excels even when the data has:HDBSCAN uses a density-based approach which makes few implicit assumptions about the clusters. It is a non-parametric method that looks for a cluster hierarchy shaped by the multivariate modes of the underlying distribution. Rather than looking for clusters with a particular shape, it looks for regions of the data that are denser than the surrounding space. The mental image you can use is trying to separate the islands from the sea or mountains from its valleys.How do we define a “cluster”? The characteristics of what we intuitively think as a cluster can be poorly defined and are often context-specific. (See Christian Hennig’s talk [5] for an overview)If we go back to the original data set, the reason we identify clusters is that we see 6 dense regions surrounded by sparse and noisy space.One way of defining a cluster which is usually consistent with our intuitive notion of clusters is: highly dense regions separated by sparse regions.Look at the plot of 1-d simulated data. We can see 3 clusters.X is simulated data from a mixture of normal distributions, and we can plot the exact probability distribution of X.The peaks correspond to the densest regions and the troughs correspond to the sparse regions. This gives us another way of framing the problem assuming we know the underlying distribution, clusters are highly probable regions separated by improbable regions. Imagine the higher-dimensional probability distributions forming a landscape of mountains and valleys, where the mountains are your clusters.For those not as familiar, the two statements are practically the same:One describes the data through its probability distribution and the other through a random sample from that distribution.The PDF plot and the strip plot above are equivalent. PDF, probability density function, is interpreted as the probability of being within a small region around a point, and when looking at a sample from X, it can also be interpreted as the expected density around that point.Given the underlying distribution, we expect that regions that are more probable would tend to have more points (denser) in a random sample. Similarly, given a random sample, you can make inferences on the probability of a region based on the empirical density.Denser regions in the random sample correspond to more probable regions in the underlying distributions.In fact, if we look at the histogram of a random sample of X, we see that it looks exactly like the true distribution of X. The histogram is sometimes called the empirical probability distribution, and with enough data, we expect the histogram to converge to the true underlying distribution.Again, density = probability. Denser = more probable.Sadly, even with our “mountains and valleys” definition of clusters, it can be difficult to know whether or not something is a single cluster. Look at the example below where we shifted one of the modes of X to the right. Although we still have 3 peaks, do we have 3 clusters? In some contexts, we might consider 3 clusters. “Intuitively” we say there are just 2 clusters. How do we decide?By looking at the strip plot of X’, we can be a bit more certain that there are just 2 clusters.X has 3 clusters, and X’ has 2 clusters. At what point does the number of clusters change?One way to define this is to set some global threshold for the PDF of the underlying distribution. The connected components from the resulting level-sets are your clusters [3]. This is what the algorithm DBSCAN does, and doing at multiple levels would result to DeBaCl [7].This might be appealing because of its simplicity but don’t be fooled! We end up with an extra hyperparameter, the threshold 𝜆, which we might have to fine-tune. Moreover, this doesn’t work well for clusters with different densities.To help us choose, we color our cluster choices as shown in the illustration below. Should we consider blue and yellow, or green only?To choose, we look at which one “persists” more. Do we see them more together or apart? We can quantify this using the area of the colored regions.On the left, we see that the sum of the areas of the blue and yellow regions is greater than the area of the green region. This means that the 2 peaks are more prominent, so we decide that they are two separate clusters.On the right, we see that the area of green is much larger. This means that they are just “bumps” rather than peaks. So we say that they are just one cluster.In the literature [2], the area of the regions is the measure of persistence, and the method is called eom or excess of mass. A bit more formally, we maximize the total sum of persistence of the clusters under the constraint that the chosen clusters are non-overlapping.By getting multiple level-sets at different values of 𝜆, we get a hierarchy. For a multidimensional setting, imagine the clusters are islands in the middle of the ocean. As you lower the sea level, the islands will start to “grow” and eventually islands will start to connect with one another.To be able to capture and represent these relationships between clusters (islands), we represent it as a hierarchy tree. This representation generalizes to higher dimensions and is a natural abstraction that is easier to represent as a data structure that we can traverse and manipulate.By convention, trees are drawn top-down, where the root (the node where everything is just one cluster) is at the top and the tree grows downward.If you are using the HDBSCAN library, you might use the clusterer.condensed_tree_.plot() API. The result of this, shown below, is equivalent to the one shown above. The encircled nodes correspond to the chosen clusters, which are the yellow, blue and red regions respectively.When using HDBSCAN, this particular plot may be useful for assessing the quality of your clusters and can help with fine-tuning the hyper-parameters, as we will discuss in the “Parameter Selection” section.In the previous section, we had access to the true PDF of the underlying distribution. However, the underlying distribution is almost always unknown for real-world data.Therefore, we have to estimate the PDF using the empirical density. We already discussed one way of doing this, using a histogram. However, this is only useful for one-dimensional data and becomes computationally intractable as we increase the number of dimensions.We need other ways to get the empirical PDF. Here are two ways:For each point, we draw a 𝜀-radius hypersphere around the point and count the number of points within it. This is our local approximation of the density at that point in space.We do this for every point and we compare the estimated PDF with the true value of the PDF (which we only do now because we simulated the data and its distribution is something we defined).For our 1-dimensional simulated data, the neighbor count is highly correlated with the true value of the PDF. The higher the number of neighbors results in a higher estimated PDF.We see that this method results in good estimates of the PDF for our simulated data X. Note that this can be sensitive to the scale of the data and the sample size. You might need to iterate over several values of 𝜀 to get good results.In this one, we get the complement of the previous approach. Instead of setting 𝜀 then counting the neighbors, we determine the number of neighbors we want and find the smallest value of 𝜀 that would contain these K neighbors.The results are what we call core distances in HDBSCAN. Points with smaller core distances are in denser regions and would have a high estimate for the PDF. Points with larger core distances are in sparser regions because we have to travel larger distances to include enough neighbors.We try to estimate the PDF on our simulated data X. In the plots above, we use 1/core_distance as the estimate of the PDF. As expected, the estimates are highly correlated with the true PDF.While the previous method was sensitive to both the scale of the data and the size of the data set, this method is mainly sensitive to the size of the data set. If you scale each dimension equally, then all core distances will proportionally increase.The key takeaway here is:So when we refer to a point’s core distance, you can think of implicitly referring to the PDF. Filtering points based on the core distance is similar to obtaining a level-set from the underlying distribution.Whenever we have core_distance ≤ 𝜀, there is an implicit pdf(x) ≥ 𝜆 happening. There is always a mapping between 𝜀 and 𝜆, and we will just use symbol 𝜆 for both core distances and the PDF for simplicity.Recall that in the previous examples, we get a level-set from the PDF and the resulting regions are our clusters. This was easy because a region was represented as some shape. But when we are dealing with points, how do we know what the different regions are?We have a small data set on the left and its corresponding PDF on the right.The first step is to find the level-set at some𝜆. We filter for regions pdf(x) ≥ 𝜆 or filter for points with core_distance ≤ 𝜆 .Now we need to find the different regions. This is done by connecting “nearby” points to each other. “Nearby” is determined by the current density level defined by 𝜆 and we say that two points are near enough if their Euclidean distance is less than 𝜆.We draw a sphere with radius 𝜆 around each point.We connect the point to all points within its 𝜆-sphere. If two points are connected they belong to the same region and should have the same color.Do this for every point and what we are left with are several connected components. These are our clusters.This is the clustering you get at some level-set. We continue to “lower the sea” and keep track as new clusters appear, some clusters grow and eventually some merge together.Here are four visualizations where we show 4 clusters at 4 different level-sets. We keep track of the different clusters so that we can build the hierarchy tree which we have previously discussed.I’d like to highlight that points can be inside the 𝜆-sphere but they still won’t be connected. They have to be included in the level-set first so 𝜆 should be greater than its core distance for the point to be considered.The value of 𝜆 at which two points finally connected can be interpreted as some new distance. For two points to be connected they must be:For a and b, we get the following inequalities in terms of 𝜆 :(1) and (2) are for the “In a dense enough region”. (3) is for the “Close enough to each other”Combining these inequalities, the smallest value of 𝜆 needed to be able to directly connect a and b isThis is called the mutual reachability distance in HDBSCAN literature.Note: This “lambda space” is a term not found in the literature. This is just for this blog.Instead of using Euclidean distance as our metric, we can now use the mutual reachability distance as our new metric. Using it as a metric is equivalent to embedding the points in some new metric space, which we would simply call 𝜆-space*.This has an effect of spreading apart close points in sparse regions.Due to the randomness of a random sample, two points can be close to each other in a very sparse region. However, we expect points in sparse regions to be far apart from each other. By using the mutual reachability distance, points in sparse regions “repel other points” if they are too close to it, while points in very dense regions are unaffected.Below is a plot of the points in 𝜆-space projected using Multidimensional Scaling to show its effect more concretely.We can see this repelling effect on the left and on top. The four points on the left are spread out the most because they are in a very sparse space.Recall that to build the hierarchy tree, we have the following steps:Notice that when doing step (3), connecting two points that already belong the same connected component is useless. What really matters are the connections across clusters. The connection that would connect two clusters correspond to the pair of points from two different clusters with the smallest mutual reachability distance. If we ignore these “useless” connections and only note the relevant ones, what we are left with is an ordered list of edges that always merge two clusters (connected components).This might sound complicated but this can be simplified if we consider the mutual reachability distance as our new metric:If this sounds familiar, it’s the classical agglomerative clustering. This is just the single linkage clustering in 𝜆-space!Doing single linkage clustering in Euclidean space can be sensitive to noise since noisy points can form spurious bridges across islands. By embedding the points in 𝜆-space, the “repelling effect” makes the clustering much more robust to noise.Single linkage clustering is conveniently equivalent to building a minimum spanning tree! So we can use all the efficient ways of constructing the MST from graph theory.Now we go through notes regarding the main parameters of HDBSCAN, min_samples and min_cluster_size , and HDBSCAN in general.Recall our simulated data X, where we are trying to estimate the true PDF.We try to estimate this using the core distances, which is the distance to the K-th nearest neighbor. The hyperparameter K is referred to as min_samples in the HDBSCAN API.These are just empirical observations from the simulated data. We compare the plot we have above with the estimated PDF based on different values of min_samples .As you can see, setting min_samples too low will result in very noisy estimates for the PDF since the core distances become sensitive to local variations in density. This can lead to spurious clusters or some big cluster can end up fragmenting into many small clusters.Setting min_samples too high can smoothen the PDF too much. The finer details of the PDF are lost, but at least you are able to capture the bigger more global structures of the underlying distribution. In the example above, the two small clusters were “blurred” into just one cluster.Determining the optimal value for min_samples might be difficult, and is ultimately data-dependent. Don’t be mislead by the high value of min_samples that we are using here. We used 1-d simulated data that has smooth variations in density across the domain and only 3 clusters. Typical real-world data are wholly different characteristics and smaller values for min_samples are enough.The insight on the smoothing effect definitely applicable in other datasets. Increasing the value of min_samples smoothens the estimated distribution so that small peaks flattened and we get to focus only on the denser regions.The simplest intuition for what min_samples does is provide a measure of how conservative you want you clustering to be. The larger the value of min_samples you provide, the more conservative the clustering – more points will be declared as noise, and clusters will be restricted to progressively more dense areas. [7]Be cautious, one possible side-effect of this is that it might require longer running times because you have to find more “nearest neighbors” per point, and might require more memory.Notice that the underlying PDF that we are trying to estimate is very smooth, but because we are trying to estimate with a sample, we expect some variance in our estimates.This results in a “bumpy” estimated PDF. Let’s focus on a small area of the PDF to illustrate this.What is the effect of this bumpiness in the hierarchy tree? Well, this affects the persistence measures of the clusters.Because the little bumps are interpreted as mini-clusters, the persistence measures of the true clusters are divided into small segments. Without removing the bumps, the main cluster may not be seen by the excess of mass method. Instead of seeing a large smooth mountain, it sees it as a collection of numerous mini-peaks.To solve this, we flatten these small bumps. This is implemented by “trimming” the clusters that are not big enough in the hierarchy tree. The effect of this is that the excess of mass method is no longer distracted by the small bumps and can now see the main cluster.min_cluster_size dictates the maximum size of a “bump” before it is considered a peak. By increasing the value of min_cluster_size you are, in a way, smoothening the estimated PDF so that the true peaks of the distributions become prominent.Since we have access to the true PDF of X, we know a good value of min_samples which will result in a smooth estimated PDF. If the estimates are good, then the min_cluster_size is not as important.Let’s say we used a smaller value for min_samples and set it to 100. If you look at the PDF plot it has the general shape of the PDF but there is noticeable variance.Even though we know there should only be 3 peaks, we see a lot of small peaks.If you see a more extreme version of this, perhaps you can’t even see the colors of the bars anymore, then that would mean that the hierarchy tree is complex. Maybe it’s because of the variance of the estimates or maybe that’s really how the data is structured. One way can address this is by increasing min_cluster_size, which helps HDBSCAN simplify the tree and concentrate on bigger more global structures.Although we’ve established that HDBSCAN can find clusters even with some arbitrary shape, it doesn’t mean there is no need for any data transformations. It really depends on your use cases.Scaling certain features can increase or decrease the influence of that feature. Also, some transformations such as log and square root transform can change the shape of the underlying distribution altogether.Another insight that should be noted is that classical ways of assessing and summarizing clusters may not be as meaningful when using HDSCAN. Some metrics such as the silhouette score work best when the clusters are round.For the “moons” dataset in sklearn, K-means has a better silhouette score than the result of HDBSCAN even though we see that the clusters in HDBSCAN are better.This also applies in summarizing the clusters by getting the mean of all the points of the cluster. This is very useful for K-means and is a good prototype of the cluster. But for HDBSCAN, it can be problematic because the clusters aren’t round.The mean point can be far from the actual cluster! This can be very misleading and can lead to wrong insight. You might want to use something like a medoid which is a point that is part of the cluster that is closest to all other points. But be careful, you can lose too much information to try to summarize a complex shape with just one point in space.This all really depends on what kind of clusters you prefer and the underlying data you are processing. See Henning’s talk [5] for an overview on cluster assessment.We’re done! We have discussed the core ideas of HDBSCAN! We will breeze through some specific implementation details as a recap.A rough sketch of the HDBSCAN’s implementation goes as follows:This basically is the way we “estimate the underlying pdf”The mutual reachability distance is a summary at what level of 𝜆 two points together will connect. This is what we use as a new metric.Building the minimum spanning tree is equivalent to single linkage clustering in 𝜆-space, which is equivalent to iterating through every possible level-set and keeping track of the clusters.Briefly, since what we have is just an estimate PDF, we expect to have some variance. So even if the underlying distribution is very smooth, the estimated PDF can be very bumpy, and therefore result to a very complicated hierarchy tree.We use the parameter min_cluster_size to smoothen the curves of the estimated distribution and as a result, simplifying the tree into the condensed_tree_Using the condensed tree, we can estimate the persistence of each cluster and then calculate for the optimal clustering as discussed in the previous section.[1] https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html[2] McInnes, Leland, and John Healy. “Accelerated hierarchical density clustering.” arXiv preprint arXiv:1705.07321 (2017).[3] John Healy. HDBSCAN, Fast Density Based Clustering, the How and the Why. PyData NYC. 2018[4] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media, 2009.[5] Christian Hennig. Assessing the quality of a clustering. PyData NYC. 2018.[6] Alessandro Rinaldo. DeBaCl: a Density-based Clustering Algorithm and its Properties.[7] https://hdbscan.readthedocs.io/en/latest/parameter_selection.html[8] Campello, Ricardo JGB, Davoud Moulavi, and Jörg Sander. “Density-based clustering based on hierarchical density estimates.” Pacific-Asia conference on knowledge discovery and data mining. Springer, Berlin, Heidelberg, 2013.Photos by Dan Otis on Unsplash, Creative Vix from Pexels, Egor Kamelev from Pexels, Jesse Gardner on Unsplash, Casey Horner on Unsplash, Keisuke Higashio on Unsplash, Kim Daniel on Unsplash",17/01/2020,1,6,107,"(700, 345)",52,12,0.0,24,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,positive,trust/acceptance
87,Review: SegNet (Semantic Segmentation),Towards Data Science,Sik-Ho Tsang,6100.0,4.0,222,"In this story, SegNet, by University of Cambridge, is briefly reviewed. Originally, it was submitted to 2015 CVPR, but at last it is not being published in CVPR (But it’s 2015 arXiv tech report version and still got over 100 citations). Instead, it is published in 2017 TPAMI with more than 1800 citations. And right now the first author has become the Director of Deep Learning and AI in Magic Leap Inc. (Below is the demo from authors:There is also an interesting demo that we can choose a random image or even upload our own image to try the SegNet. I have tried as below:DeconvNet and U-Net have similar structures as SegNet.[2015 arXiv] [SegNet]SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling[2017 TPAMI] [SegNet]SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image SegmentationImage Classification[LeNet] [AlexNet] [ZFNet] [VGGNet] [SPPNet] [PReLU-Net] [STN] [DeepImage] [GoogLeNet / Inception-v1] [BN-Inception / Inception-v2] [Inception-v3] [Inception-v4] [Xception] [MobileNetV1] [ResNet] [Pre-Activation ResNet] [RiR] [RoR] [Stochastic Depth] [WRN] [FractalNet] [Trimps-Soushen] [PolyNet] [ResNeXt] [DenseNet] [PyramidNet]Object Detection[OverFeat] [R-CNN] [Fast R-CNN] [Faster R-CNN] [DeepID-Net] [R-FCN] [ION] [MultiPathNet] [NoC] [G-RMI] [TDM] [SSD] [DSSD] [YOLOv1] [YOLOv2 / YOLO9000] [YOLOv3] [FPN] [RetinaNet] [DCN]Semantic Segmentation[FCN] [DeconvNet] [DeepLabv1 & DeepLabv2] [ParseNet] [DilatedNet] [PSPNet] [DeepLabv3]Biomedical Image Segmentation[CUMedVision1] [CUMedVision2 / DCAN] [U-Net] [CFS-FCN] [U-Net+ResNet]Instance Segmentation[DeepMask] [SharpMask] [MultiPathNet] [MNC] [InstanceFCN] [FCIS]Super Resolution[SRCNN] [FSRCNN] [VDSR] [ESPCN] [RED-Net] [DRCN] [DRRN] [LapSRN & MS-LapSRN]",10/02/2019,0,31,0,"(648, 279)",11,14,0.0,93,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,trust/acceptance
88,Automatic Topic Clustering Using Doc2Vec,Towards Data Science,Rik Nijessen,59.0,4.0,737,"“Imagine you are a manager of a big company and want to keep your customer data save. This means you have to be up to date with the current trends and threats in cybersecurity. However, the cybersecurity scene is going very fast, so staying up to date is hard.”.This was the start of the pitch we did at the HackDelft hackathon last weekend. The goal of our project was to create a new way to present cybersecurity trends to managers. In this blog I will explain how we attacked this problem and which technology we used.Currently, KPMG and Owlin (the sponsors of our case) have a portal which presents articles about cybersecurity topics ranked by importance. This is all nice, but to be able to understand the trends, you still have to go through a lot of articles. We therefore wanted to build a portal which could cluster recent articles into topics with a nice summary of that topic next to it. This way, you can see what is currently important at a glance.LDA is a much used algorithm for topic discovery. However, in my experience LDA can spit out some hard to understand topic clusters. Another approach could be clustering based on tf-idf vectors, but because Word2Vec and Doc2Vec have shown to generate awesome results in the Natural Language Processing scene, we decided to try those, just for fun.Word2Vec is an unsupervised algorithm developed by Google that tries to learn meaningful vector representations of words from a dataset of text. It does so based on the distributional hypothesis, which states that words that appear in the same context, probably have similar meaning.You can do cool things with these word vectors, for example doing the following vector calculation: King - Man + Woman will give you a vector that is very close to the word vector of Queen. If you want to try these yourself, you can do so here. Also, Google has made available a pretrained Word2Vec model here. Doc2Vec (also called Paragraph Vectors) is an extension of Word2Vec, which learns the meaning of documents instead of words.This paper shows that by training Word2Vec and Doc2Vec together, the vector of documents are placed near words describing the topic of those documents. This gave us the following idea: what if we cluster documents, take the mean of those clusters and look at the words similar to the vector representation of those means? Maybe this will give us some words describing the clusters.We started by training Doc2Vec and Word2Vec together on the dataset, delivered by KPMG and Owlin, using the Gensim Python library. This dataset contained around 100k news articles from the last 200 days. Because it contained a lot of duplicates, we had to remove those first. Furthermore, we also removed special characters and URLs and lowercased everything before training to remove noise.After training we took news articles from the last 3 days and retrieved the vector representation of those using the trained Doc2Vec model. We then used the K-means algorithm (from the nltk python library) to cluster the vectors. Because similarity is usually measured using the cosine distance, we used cosine distance instead of euclidian distance.This gave us some great results. Some examples of the clusters it generated are:What was interesting to see is that it placed articles about a keylogger in HP laptops in a cluster with articles about the release of Samsung’s smartphone. Maybe it learned that both companies are developing consumer hardware? Furthermore it produced multiple clusters about WannaCry: one about it spreading, one about it hitting a lot of hospitals and one about Microsoft releasing a patch for it for Windows XP.Unfortunately, our idea about using Word2Vec for summarising the clusters didn’t work out. It produced words which did not summarise the clusters at all. Instead, we tried a much simpler approach: we took all titles per cluster, removed stopwords and digits and counted the number of occurrences of each word. We then selected the 5 most occurring words per cluster as keywords for that cluster. This approach worked surprisingly well. Some examples are:In the end we have created a model that was able to cluster similar articles using Doc2Vec and generate keywords describing the content of those clusters in under 24 hours. We have released our code on Github here, so you can play with it yourself. The clusters it generated during the weekend can be found in this JSON file.",15/05/2017,0,0,8,,0,2,0.0,12,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,expectation/interest
89,Why Gradient descent isn’t enough: A comprehensive introduction to optimization algorithms in neural networks,Towards Data Science,vikashraj luhaniwal,230.0,14.0,2149,"The goal of neural networks is to minimize the loss, for producing better and accurate results. In order to minimize the loss, we need to update the internal learning parameters(especially weights and biases). These parameters are updated based on some update rule/function. Generally, we think about Gradient descent as an update rule. Now two types of questions arise w.r.t parameters update.This post revolves around these two questions and answers in the simplest way in the context of better optimization. In this post, I will present an intuitive vision of optimization algorithms, their different types, and variants.Additional NOTEThis article assumes that the reader has basic knowledge about the concept of the neural network, forward and backward propagation, weight initalization, activation functions, etc. In case you are not familiar then I would recommend you to follow my other articles on these topics.Forward propagation in neural networks — Simplified math and code versionWhy better weight initialization is important in neural networks?Analyzing different types of activation functions in neural networks — which one to prefer?Optimization algorithm tries to minimize the loss(cost) by following some update rule. The loss is a mathematical function denoting the difference between the predicted value and actual value. Loss is dependent on the actual value which is derived with the help of learning parameters(weights and biases) and inputs. Therefore learning parameters are very important for better training and producing accurate results. To find out the optimal value of these parameters we need to continuously update them. There should be some update rule for this purpose. So we use various optimization algorithms to follow some update rule and each optimization algorithm has a different approach to calculate, update and find out the optimal value of model parameters.Based on our first question “How much data should be used for an update” optimization algorithms can be classified as Gradient Descent, Mini batch Gradient Descent, and Stochastic Gradient Descent.In fact, the basic algorithm is Gradient Descent. Mini-batch Gradient descent and Stochastic Gradient Descent are two different strategies based on the amount of the data taken. These two are also known as the variants of Gradient Descent.Gradient descent is most commonly used and popular iterative machine learning algorithm. It is also the foundation for other optimization algorithms. Gradient descent has the following update rule for weight parameterSince during backpropagation for updating the parameters, the derivative of loss w.r.t. a parameter is calculated. This derivative can be dependent on more than one variable so for its calculation multiplication chain rule is used. For this purpose, a Gradient is required. A gradient is a vector indicating the direction of increase.For gradient calculation, we need to calculate derivatives of loss w.r.t the parameters and update the parameters in the opposite direction of the gradient.The above ideal convex curve image displays the weight update in the opposite direction of the gradient. As we can notice for too large and small values of weights the loss is maximum and our goal is to minimize the loss so the weights are updated. If the gradient is negative then descent(dive) towards the positive side and if the gradient is positive then descent towards the negative side until the minimal value of gradient is found.Algorithm for Gradient descent using a single neuron with sigmoid activation function in PythonThe above animation represents how the algorithm converges after 1000 epochs. The error surface used in this animation is as per the input. This error surface is animated in 2D space. For 2D, a contour map is used where the contours represent the third dimension i.e. error. The red regions represent the high values of error, more the intensity of the red region more the error. Similarly, the blue regions represent the low values of error, less the intensity of the blue region less the error.Standard Gradient descent updates the parameters only after each epoch i.e. after calculating the derivatives for all the observations it updates the parameters. This phenomenon may lead to the following caveats.Now let see how different variations of gradient descent can address these challenges.Stochastic gradient descent updates the parameters for each observation which leads to more number of updates. So it is a faster approach which helps in quicker decision making.Algorithm for Stochastic Gradient descent using a single neuron with sigmoid activation function in PythonQuicker updates in different directions can be noticed in this animation. Here, lots of oscillations take place which causes the updates with higher variance i.e. noisy updates. These noisy updates help in finding new and better local minima.Disadvantages of SGDNow let see how another variant of gradient descent can address these challenges.Another variant of GD to address the problems of SGD, it lies in between GD and SGD. Mini-batch Gradient descent updates the parameters for a finite number of observations. These observations together are referred to a batch with some fixed size. Batch size is chosen as a multiple of 64 e.g. 64, 128, 256, etc. Many more updates take place in one epoch through Mini-batch GD.Algorithm for Mini-batch Gradient descent using a single neuron with sigmoid activation function in PythonAs we can see there are fewer oscillations in Mini-batch in contrast to SGD.Basic notations1 epoch = one pass over the entire data1 step = one update for parametersN = number of data pointsB = Mini-batch sizeAdvantages of Mini-batch GDThe error surface contains more sloppy as well less sloppy areas. During back propagation, there will be more update in parameters for the regions with more slope whereas less update in parameters for the regions with a gentle slope. More change in parameters leads to more change in loss, similarly less change in parameters leads to less change in the loss.If the parameter initialization lands in a gentle slope area then it requires a large number of epochs to navigate through these areas. It happens so because the gradient will be very small in gentle slope regions. So it moves with small baby steps in gentle regions.Consider a case with initialization in a flat surface as shown below where GD is used and the error is not reducing when the gradient is in the flat surface.Even after a large number of epochs for e.g. 10000 the algorithm is not converging.Due to this issue, the convergence is not achieved so easily and the learning takes too much time.To overcome this problem Momentum based gradient descent is used.Consider a case where in order to reach to your desired destination you are continuously being asked to follow the same direction and once you become confident that you are following the right direction then you start taking bigger steps and you keep getting momentum in that same direction.Similar to this if the gradient is in a flat surface for long term then rather than taking constant steps it should take bigger steps and keep the momentum continue. This approach is known as momentum based gradient descent.Momentum-based gradient descent update rule for weight parameterGamma parameter(γ) is the momentum term which indicates how much acceleration you want. Here along with the current gradient (η∇w(t)), the movement is also done according to history (γV(t−1)) so the update becomes larger which leads to faster movement and faster convergence.v(t) is exponentially decaying weighted sum, as t increases γV(t−1) becomes smaller and smaller i.e. this equation holds the farther updates by a small magnitude and recent updates by a large magnitude.Momentum-based gradient descent in Python for sigmoid neuronThis algorithm adds momentum in the direction of consistent gradients and cancels the momentum if the gradients are in different directions.Issues with momentum based Gradient descentIn the valley that leads to exact desired minima, there are a large number of oscillations using momentum-based GD. Because it overshoots the minima with larger steps and takes a U-turn but again overshoots so this process repeats. Which means moving with larger steps is not always good.To overcome this issue Nesterov accelerated Gradient Descent is used.In momentum based GD as the gradient heads to the valley(minima region), it makes a lot of U-turns(oscillations) before it converges. This problem was initially identified and responded by a researcher named Yurii Nesterov.He suggested, make the movement first by history amount(previous momentum) then calculate the temporary gradient at this point and then update the parameters. In other words, before making an update directly first it looks ahead by moving with the previous momentum then it finds what the gradient should be.This looking ahead helps NAG in finishing its job(finding the minima) quicker than momentum-based GD. Hence the oscillations are less compared to momentum based GD and also there are fewer chances of missing the minima.NAG update rule for weight parameterNAG algorithm in Python for sigmoid neuronHere v_w and v_b refer to v(t) and v(b) respectively.As per the update ruleThe update is directly proportional to the gradient(∇w). Smaller the gradient smaller the update and the gradient is directly proportional to the input. Therefore the update is dependent on the input also.Need for an adaptive learning rateFor the real-time datasets, most of the features are sparse i.e. having zero values. Due to this for most of the cases, the corresponding gradient is zero and therefore the parameters update is also zero. To resonate this problem, these update should be boosted i.e. a high learning rate for sparse features. Therefore the learning rate should be adaptive for fairly sparse data.In other words, if we are dealing with sparse features then learning rate should be high whereas for dense features learning rate should be low.Adagrad, RMSProp, Adam algorithms are based on the concept of adaptive learning rate.It adopts the learning rate(η) based on the sparsity of features. So the parameters with small updates(sparse features) have high learning rate whereas the parameters with large updates(dense features) have low learning rate. Therefore adagrad uses a different learning rate for each parameter.Adagrad update rule for weight parameterv(t) accumulates the running sum of square of the gradients. Square of ∇w(t) neglects the sign of gradients. v(t) indicates accumulated gradient up to time t. Epsilon in the denominator avoids the chances of divide by zero error.So if v(t) is low (due to less update up to time t) for a parameter then the effective learning rate will be high and if v(t) is high for a parameter then effective learning rate will be less.Adagrad algorithm in Python for sigmoid neuronDisadvantage with AdagradFor parameters(especially bias) corresponding to dense features, after a few updates, the learning rate decays rapidly as the denominator grows rapidly due to the accumulation of squared gradients. So after a finite number of updates, the algorithm refuses to learn and converges slowly even if we run it for a large number of epochs. The gradient reaches to a bad minimum (close to desired minima) but not at exact minima. So adagrad results in decaying and decreasing learning rate for bias parameters.RMSProp overcomes the decaying learning rate problem of adagrad and prevents the rapid growth in v(t).Instead of accumulating squared gradients from the beginning, it accumulates the previous gradients in some portion(weight) which prevents rapid growth of v(t) and due to this the algorithm keeps learning and tries to converge.RMSProp update rule for weight parameterHere v(t) is exponentially decaying average of all the previous squared gradients. The beta parameter value is set to a similar value as the momentum term. The running average v(t) up to time t is dependent on weighted previous average gradients and current gradient. v(t) maintains (∇w(t))² for a fixed window time.Adagrad algorithm in Python for sigmoid neuronIssues with RMSPropSo far in Adagrad, RMSProp we were calculating different learning rates for different parameters, can we have different momentums for different parameters. Adam algorithm introduces the concept of adaptive momentum along with adaptive learning rate.Adaptive Moment Estimation (Adam) computes the exponentially decaying average of previous gradients m(t) along with an adaptive learning rate. Adam is a combined form of Momentum-based GD and RMSProp.In Momentum-based GD, previous gradients(history) are used to compute the current gradient whereas, in RMSProp previous gradients(history) are used to adjust the learning rate based on the features. Therefore Adam deals with adaptive learning rate and adaptive momentum where RMSProp ensures v(t) does not grow rapidly to avoid the chances of decaying learning rate and m(t) from Momentum-based GD ensures it calculates the exponentially decaying average of previous gradients, not the current gradient.Adam update rule for weight parameterHere m(t) and v(t) are values of the mean obtained from the first moment.Adam uses bias corrected values (uncentered variance) of gradients for update rule and these values are obtained through the second moment.The final update rule is given asAdam algorithm in Python for sigmoid neuronSo in Adam unlike RMSProp fewer oscillations and it moves more deterministically in the right direction which leads to faster convergence and better optimization.In this article, I have discussed the different types of optimization algorithms and the common issues one might encounter while using each of them. Generally, Adam with mini-batch is preferred for the training of deep neural networks.",07/05/2019,8,208,282,"(421, 220)",24,7,0.0,3,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
90,GPT-3: Creative Potential of NLP,Towards Data Science,Merzmensch,2800.0,7.0,1062,"Updated: 18th of November 2021: “access without waitlist”It was last year in February, as OpenAI published results on their training of unsupervised language model GPT-2. Trained in 40Gb texts (8 Mio websites) and was able to predict words in proximity. GPT-2, a transformer-based language applied to self-attention, allowed us to generated very convincing and coherent texts. The quality was that good, so the main model with 1.5 billion parameters wasn’t initially publicly accessible, to prevent uncontrolled fake news. Luckily, the complete model was later published and could be even used with Colab Notebooks.This year OpenAI strikes back with new language model GPT-3. With 175 billion parameters (read also: GPT-3 Paper).Unnecessary spoiler: it’s incredibly good.There are already some profound articles on TDS examining features and paper of GPT-3:towardsdatascience.comtowardsdatascience.comtowardsdatascience.comOpenAI is building an API, currently accessible via waiting list:beta.openai.comFortunately, I could get access and experiment with GPT-3 directly. Here are some of my initial outcomes.Update (18.11.2021): OpenAI’s API is now available with no waitlist.The AI Playground interface looks simple, but it bears the power within. For the first, here is a setting dialog, which lets you configure text length, temperature (from low/boring to standard to chaotic/creative), and other features.You also can define where the generated text has to start and to stop, these are some of the control functions that have a direct impact on textual results.The simple interface provides also some GPT-3 presets. The amazing thing about transformer-driven GPT-models is among others the ability to recognize a specific style, text character, or structure. In case you begin with lists, GPT-3 continues generating lists. In case your prompt has a Q&A structure, it will be kept coherently. If you ask for a poem, it writes a poem.You can do your own presets, or use the existing, which are:Chat.A typical setting for a chatbot. You ask - AI answers. It’s possible to change the “characters” or setting also. As you can see, the chat situation was accomplished perfectly (even if my, Human’s, third question was kind of unfair).To demonstrate the contextual impact, let’s change the AI character from “helpful” and “very friendly” to “brutal, stupid and very unfriendly”. You will see how the whole dialogue will be influenced:I think, we re-invented Marvin the Paranoid Android.Q&AThis preset consists of a clear dual structure: Question and Answer. You need some training before it starts to answer the question (and get the rules), but then it works perfectly. I asked some random questions from various areas and here you go:I’d say, perfect!Parsing unstructured dataThis one is fascinating and shows a good comprehension of the unstructured text — extracting structured data from the full text.Summarizing for a 2nd graderThis preset shows another level of comprehension — including rephrasing of difficult concepts and sentences in clear words.I tried Wittgenstein:The simple proverb can be paraphrased convincingly:Or look at this pretty well and clear transition of Sigmund Freud’s time distancing concept:As you see, compression of text and its coherent “translation” is one of the strengths of GPT-3.GPT-2 was already a great language model when it was about English. You could generate amazing texts, especially with 1.5 billion parameters. I used GPT-2 for a screenplay of this short movie — and its absurdity could be rather understood as a good tradition of David Lynch and Beckett:The dialogues were logical, even if spontaneous. But it was regarding English. If you’ve tried with inputs in other languages, you would face the barrier of understanding. GPT-2 tried to imitate languages, but you needed to fine-tune it on text corpus in a specific language to get good results.GPT-3 is different.Its processing in other languages is phenomenal.I tried German, Russian, and Japanese.German.It was rather my daughter, who tried to let GPT-3 write a fairy tale. She began with “Eine Katze mit Flügeln ging im Park spazieren” (“A cat with wings took a walk in a park”).The emerged story was astonishingly well written. With irony, vivid characters, and some leitmotifs. This is not just a collection of topoi or connected sentences. This is… a story!Russian.I trained once GPT-2 on Pushkin’s poetry and have got some interesting neologisms, but it was a grammar mess. Here I input some lines of Pushkin’s poem — and the result I’ve got was… interesting. It hadn’t rhymes, but stylistically intense power. It was not Pushkin style, though. But almost without any mistakes or weird grammar. And… it works as poetry (especially if you are ready to interpret it).Japanese.This was something special. I entered just a random sentence:今日は楽しい一日になりますように！と言いました。// Today was funny and entertaining day, I said.And the result was a small story about prayer, happiness, wisdom, and financial investment. In well written Japanese (neutral politeness form, like the input).It does mean: GPT-3 is ready for multilingual text processing.My first try was, of course, to write a Shakespearean sonnet. So the prompt was just:The result was this:Perfect iambic verse, great style, nice rhymes… If not one thing:The first two lines are actually from Alexander Pope, The Rape of the Lock. And here we have a reason to be cautious: GPT-3 produces unique and unrepeatable texts, but it can reuse the whole quotes of existing texts it was trained on.Re-examination of results is inevitable if you want to guarantee a singularity of a text.I wonder, if there are some possibilities for “Projection” like StyleGAN2 feature, just in opposite to StyleGAN2 (where it compares the image with latent space), in GPT-3 it would compare with the dataset it was trained on? To prevent accidental plagiarism.But the thing is: GPT-3 can write poems on demand, in particular styles.Here is another example:As I still hadn’t accessed, I asked a friend to let GPT-3 write an essay on Kurt Schwitters, a German artist, and Dadaist:The outcome is: GPT-3 has already a rich knowledge, which can be recollected. It is not always reliable (you have to fine-tune it to have a perfect meaning match), but it’s still very close to the discourse.Another mindblowing possibility is using GPT-3 is quite different cases than just text generation:You can get support by CSS:And calling it General Intelligence is already a thing:We are still at the beginning, but the experiments with GPT-3 made by the AI community show its power, potential, and impact. We just have to use it with reason and good intention. But that’s the human factor. Which is not always the best one.For more wonderful text experiments I highly recommend you to read Gwern:www.gwern.net",14/07/2020,1,17,8,"(664, 414)",16,0,0.0,15,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
91,The Human Mind Is A Powerful Thing but It Is Wonderful When Applied to A Single Task at A Single Time.,The Inspiring Cafe,Mahil Ahmad,10.0,4.0,788,"That evening I was at a friend’s house for a wedding ceremony. The ceremony was held on a wide lawn of the house. All the guests were engaged in gossip when suddenly we heard the voice of broken glass. We found out that a tray full of water glasses was dropped from a waiter’s hand and all the glasses in it fell down to the marble floor. The waiter was shocked and was helplessly looking at the broken glasses. When I tried to find out the cause of this incident, I found out that someone put his bag in the armpit of the waiter who was busy with his work and asked the waiter to leave it in his car. The waiter shook his head saying “OK sir”, He had a tray in both his hands, suddenly his mobile phone in the front pocket also started ringing. Considering it an emergency call, he just looked at the mobile phone when, unfortunately, he collided with the chair and all the glasses fell to the ground.In life, we try to do so many things at the same time in an effort to save time or work harder. By doing so we lose productivity and ultimately end up with more stress. The human mind is a powerful thing but it is wonderful when applied to a single task at a single time.God(Allah) has given man the ability to live an organized life and in fact, a successful person is the one who saves every minute of his time. He sets goals and tasks for himself and tries to achieve them. There was a time when a man was considered very capable who could handle many tasks at a time, but today modern research has proved that multitasking is harmful instead of beneficial.Ø Performing two or more tasks at a time.Ø Doing several tasks simultaneously and shifting from one task to another, from another to the third, and then to the first.Ø Doing a lot of work one after the other.Multi-tasking may seem like a big achievement but in reality, it is harmful to the mental health and performance of a person.According to the website, www.verywellmind.com, “a person thinks that he has the ability to do many things at one time, but in fact, the human mind is not ready for it.” The fact is that repeated transitions from one job to another impair creative performance. People who focus on just one task at a time are more successful and give better results than multi-taskers.Just like our eyes that can focus on one point at one time, the same is the case with the human mind. You can say the human mind also has an eye, and this eye can only fully focus on one thing at a time. supposeØ You are sitting on a comfortable sofa in your drawing room writing an article.Ø There is also a hot football match on TV.Ø Your favorite pizza is on the table in front of you, making you hungrier.Ø A message from your old friend also comes on the mobile phone lying next to you.In this case, your mind is busy with four tasks. You are looking at the TV screen when there is a craving for pizza as well and you have also picked up your mobile phone to reply to a friend. What do you think, will you be able to write a good article? Absolutely not. Writing will not be as good as it would be in a quiet atmosphere.Multi-tasking is also called the “silent killer” because it slowly kills the human ability.Research shows that “multitasking decreases our productivity by up to 40%.” Because shifting from one job to another at a time, then moving from there to the first, turns down the brain capacity to focus. The concentration level of the mind does not stay the same and as a result, the productivity of man is diminished.Multi-tasking is a habit that makes your mind dull instead of sharp. In a study by the University of London, multi-tasking students participated in a mental exercise competition and the management was surprised to see that the results of multi-tasking students turned out very weak. They were looking tired and it was as if they hadn’t slept all night.It is also a proven fact that the ratio of mistakes of a person working on different tasks at one time is much more than a person handling a single task at a time.Be aware of the times when you’re multi-tasking. There’s a good chance you would possibly don’t even notice when you’re doing it. Doing one task at a time may help you become more productive and it’s going to make each task more enjoyable.",31/03/2021,0,5,0,"(700, 467)",3,0,0.0,7,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
92,What The Heck Are VAE-GANs?,Towards Data Science,Enoch Kan,533.0,5.0,815,"Yep, you read the title correctly. While a few friends of mine are vegans, none of them knew anything about VAE-GANs. VAE-GAN stands for Variational Autoencoder- Generative Adversarial Network (that is one heck of a name.) Before we get started, I must confess that I am no expert in this subject matter (I don’t have PhD in electrical engineering, just sayin’). But after reading several research papers and watching Ian Goodfellow’s 30-minute long intro to GANs, here is a short (yet concise) summary of my major takeaways:Variational Autoencoders (VAEs)The simplest way of explaining variational autoencoders is through a diagram. Alternatively, you can read Each input image has features that can normally be described as single, discrete values. Variational autoencoders describe these values as probability distributions. Decoders can then sample randomly from the probability distributions for input vectors. Let me guess, you’re probably wondering what a decoder is, right? Let’s take a step back and look at the general architecture of VAE.Typical setup of a variational autoencoder is nothing other than a cleverly designed deep neural network, which consists of a pair of networks: the encoder and the decoder. The encoder can be better described as a variational inference network, which is responsible for the mapping of input x​​​ to posteriors distributions q​θ​​(z∣x). Likelihood p(x∣z) is then parametrized by the decoder, a generative network which takes latent variables z and parameters as inputs and projects them to data distributions p​ϕ​​(x∣z).A major drawback of VAEs is the blurry outputs that they generate. As suggested by Dosovitskiy & Brox, VAE models tend to produce unrealistic, blurry samples. This has to do with how data distributions are recovered and loss functions are calculated in VAEs in which we will discuss further below. A 2017 paper by Zhao et. al. has suggested modifications to VAEs to not use variational Bayes method to improve output quality.Generative Adversarial Networks (GANs)The dictionary definition of adversarial is involving or characterized by conflict or opposition. This is in my opinion a very accurate description of what GANs are. Just like VAEs, GANs belong to a class of generative algorithms that are used in unsupervised machine learning. Typical GANs consist of two neural networks, a generative neural network and a discriminative neural network. A generative neural network is responsible for taking noise as input and generating samples. The discriminative neural network is then asked to evaluate and distinguish the generated samples from training data. Much like VAEs, generative networks map latent variables and parameters to data distributions.The major goal of generators is to generate data that increasingly “fools” the discriminative neural network, i.e. increasing its error rate. This can be done by repeatedly generating samples that appear to be from the training data distribution. A simple way to visualize this is the “competition” between a cop and a cyber criminal. The cyber criminal (generator) attempts to create online identities that resemble ordinary citizens, while the cop (discriminator) tries to distinguish fake profiles from the real ones.Variational Autoencoder Generative Adversarial Networks (VAE-GANs)Okay. Now that we have introduced VAEs and GANs, it’s time to discuss what VAE-GANs really are. The term VAE-GAN is first introduced in the paper “Autoencoding beyond pixels using a learned similarity metric” by A. Larsen et. al. The authors suggested the combination of variational autoencoders and generative adversarial networks outperforms traditional VAEs.Remember GANs are subdivided into generators and discriminator networks? The authors suggested a GAN discriminator can be used in place of a VAE’s decoder to learn the loss function. The motivation behind this modification is as mentioned above, VAEs tend to produce blurry outputs during the reconstruction phase. This “blurriness” is somehow related to the way VAE’s loss function is calculated. I am not going into the nitty gritty of how this new loss function is calculated, but all you need to know is this set of equationNow that’s a lot of L’s. But jokes aside, the above equations assume the lth layer of the discriminator have outputs that differ in a Gaussian manner. As a result, calculating the mean squared error (MSE) between the lth layer outputs gives us the VAE’s loss function. The final output of GAN, D(x), can then be used to calculate its own loss function.Generative models are now added to the list of AI research by top tech companies such as Facebook. Yann Lecun, a prominent computer scientist and AI visionary once said “This (Generative Adversarial Networks), and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.”Besides VAE-GANs, many other variations of GANs have been researched and implemented. DCGANs, or Deep Convolutional Generative Adversarial Networks, are introduced not long after Ian Goodfellow’s introduction to the original GANs. I am excited to see for generative model to find its role in future AI applications, and potentially improving the qualities of our lives.Thanks for reading my article.",17/08/2018,0,7,16,"(641, 241)",8,0,0.0,8,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,positive,joy/calmness
93,"So You Have Some Clusters, Now What?",Square Corner Blog,Inna Kaler,53.0,6.0,1302,"Heads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogHow to Add Value to Your ClustersWritten by ProblemOne of the most common ways to apply unsupervised learning to a dataset is clustering, specifically centroid-based clustering. Clustering takes a mass of observations and separates them into distinct groups based on similarities.Figure 1: Taking a 2 dimensional dataset and separating it into 3 distinct clustersFor those who’ve written a clustering algorithm before, the concept of K-means and finding the optimal number of clusters using the Elbow method is likely familiar. The harder question to answer is — What does each cluster represent? We ran into precisely this problem when using clustering to understand our sellers’ behaviors.We started out with certain assumptions about how the data would cluster without specific predictions of how many distinct groups our sellers split into. The first instinct was to manually generate a set of signals that we knew to be interesting. For example, payment volume, payment amount, and business type are some of the most common dimensions that set our sellers apart, so we attempted to use them in our clustering analysis. The resulting clusters were most optimally separated along dimensions we already knew to be important and supported our initial understanding. Retail sellers are different than Food & Drink sellers, large businesses with multiple locations are different than small single-location businesses, etc. While valuable in confirming previous understanding and analyses — this didn’t tell us anything new!We didn’t gain any new insights about how our sellers differ because we embedded our biases of which signals are important into our dataset. So how do we get around this?Data and MethodsTo try to unlock more interesting insights, we proceeded to rebuild our clusters with a dataset containing less bias. Square’s risk systems generate thousands of signals for each seller based on a variety of behavioral signals. This is essentially the kitchen-sink of data and we used this dataset instead of hand-picking signals, which introduced the aforementioned bias. There are some reservations with this method, of course, including the possibility that we swapped known biases for unknown ones. We hope to visit this specific topic in a future blog post.At this point, we have our data, our set of features, defined a number of K clusters, and clustered sellers into our K clusters. Next, we try a few different approaches to assign ‘profiles’ to each of our clusters:Empirical ApproachOnce we construct the clusters, we can produce a list of all sellers and which cluster they belong to. We can then take a specific cluster and study the seller characteristics along known dimensions. Alternatively, we can look at which groupings over- or under-index on specific dimensions compared to other clusters. For both approaches, we come up with a list of dimensions that differ across clusters, then make guesses on what the clusters will represent.Here’s an example of the second approach. In this example, we joined our primary key and cluster label to the features we are interested in profiling and plotting each cluster’s average scaled value. The feature of interest is on the x-axis with the scaled average value of that feature on the y-axis. Visualizing the differences allows us to see that cluster zero is categorized by the first and last features. Meanwhile cluster three has the highest values for almost every feature profiled. From this we can start to translate nondescript cluster labels into meaningful ones explicable to an audience.Figure 2: Average ‘score’ of each cluster along 4 different dimensionsDepending on the purpose of clustering the data, the feature set to profile clusters will change. When using clustering to understand how different sellers use a Square product, the dimensions are specific to usage data. When wanting to determine those sellers’ value to Square the feature set was comprised of broad variables related to revenue, seller size, LTV, etc.Centroid ApproachWe can figure out which seller from each cluster is the closest to the centroid (center point) and mark that seller as the most ‘representative’ seller for that cluster. We can then study the sellers along known dimensions or signals to come up with our best guess of the cluster profile as with the empirical approach. An alternative approach is to study the hypothetical centroid instead of the most representative element. For both approaches, we are making assumptions about how representation works within our groups, so which is the better approach of the two is strictly situational. Of course, if the variance within clusters are large, then neither the hypothetical nor empirical centroid would feel very representative of the group.Here’s an example of this approach. In the graph below, the star in the middle of each cluster indicates the most representative member of the cluster.Figure 3: Stars identify the centroid of each clusterSupervised Learning from Cluster MembershipOnce the clusters are created, we can isolate the most influential features that vary the most across the different clusters, and make an educated guess about the cluster’s profile. This is a more sophisticated version of determining how clusters are over or under-indexing.One way to tease apart the influential features this is with a Random Forest Classifier. For example, if we have 4 clusters, we can use our existing signals to predict the probability of everyone belonging to the first cluster, then the second, etc. The result would be K models to match K clusters, one model per cluster, and predicting whether an instance is likely to belong to each cluster. Most standard implementations of the Random Forest algorithm will show you which features are important for each particular prediction, which can inspire more intuitions on what these important features says about the cluster itself.Another way to arrive at feature influence is to use a single multiclass classification model (sklearn.multiclass) with your response as the column ‘Cluster’ with values of 1,…,K. This provides the benefit of a singular model that can be used to predict an instance’s cluster using additional signals outside the K-means dataset. For example, in the application of product usage, you could profile users into clusters without them having used the product yet to segment your addressable market more specifically. We hope to expand on this technique in a future blog post.ResultsOnce we determine which dimensions mattered in separating our clusters, we are likely to find one of two things: either the top signals are not aligned with what we previously believed to be important, or that they are.If our result is the former, then great! You can gain insights about what new behaviors your observations diverge on and how they’re different than your existing assumptions. Ideally, these divergences are significant and actionable.More often than not, however, the top signals that the clusters split on are already known and expected. This at the very least validates that existing segmentations are meaningful, but we can learn more.Given that your data is most optimally separated by your top signal, what if you cluster again within these clusters? This would then reveal a second set of signals that are important, controlling for the first set. We did exactly that when we discovered that our first clusters were separated into our existing segmentation of business size. We added an additional list of signals that weren’t present in our first dataset and a list of insights that clued us into how our sellers differ at various business sizes emerged. This set of insights informed strategy across marketing and product teams, and provided a personalized approach to product development and seller interactions.When it comes to clustering, you are rarely done after you pick the optimal number of clusters and run the algorithm. There is a lot of value in the interpretability of your clusters and the additional understanding you gain of your data.",09/11/2017,0,4,4,"(700, 356)",3,0,0.0,10,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,positive,expectation/interest
94,10 Algorithms To Solve Before your Python Coding Interview,Towards Data Science,AnBento,902.0,7.0,1337,"Many of you contacted me asking for valuable resources to nail Python coding interviews. Below I share 4 courses/platforms that I strongly recommend to keep exercising after practicing the algorithms in this post:Hope you’ll find them useful too! Now enjoy the article :DIf you are relatively new to Python and plan to start interviewing for top companies (among which FAANG) listen to this: you need to start practicing algorithms right now.Don’t be naive like I was when I first started solving them. Despite I thought that cracking a couple of algorithms every now and then was fun, I never spent too much time to practice and even less time to implement a faster or more efficient solution. Between myself, I was thinking that at the end of the day solving algorithms all day long was a bit too nerdy, it didn’t really have a practical use in the real daily work environment and it would not have brought much to my pocket in the longer term.“Knowing how to solve algorithms will give you a competitive advantage during the job search process”Well…I was wrong (at least partially): I still think that spending too much time on algorithms without focusing on other skills is not enough to make you land your dream job, but I understood that since complex problems present themselves in every day work as a programmer, big companies had to find a standardized process to gather insights on the candidate’s problem solving and attention to detail skills. This means that knowing how to solve algorithms will give you a competitive advantage during the job search process as even less famous companies tend to adopt similar evaluation methods.towardsdatascience.comPretty soon after I started solving algorithms more consistently, I found out that there are plenty of resources out there to practice, learn the most efficient strategies to solve them and get mentally ready for interviews (HackerRank, LeetCode, CodingBat and GeeksForGeeks are just few examples).Together with practicing the top interview questions, these websites often group algorithms by company, embed active blogs where people share detailed summaries of their interview experience and sometimes even offer mock interview questions as part of premium plans.For example, LeetCode let you filter top interview questions by specific companies and by frequency. You can also choose the level of difficulty (Easy, Medium and Hard) you feel comfortable with:There are hundreds of different algorithmic problems out there, meaning that being able to recognize the common patterns and code an efficient solution in less then 10 mins will require a lot of time and dedication.“Don’t be disappointed if you really struggle to solve them at first , this is completely normal”Don’t be disappointed if you really struggle to solve them at first, this is completely normal. Even more experienced Python programmers would find many algorithms challenging to solve in a short time without an adequate training.Also don’t be disappointed if your interview doesn’t go as you expected and you just started solving algorithms. There are people that prepare for months solving a few problems every day and rehearse them regularly before they are able to nail an interview.To help you in your training process, below I have selected 10 algorithms (mainly around String Manipulation and Arrays) that I have seen appearing again and again in phone coding interviews. The level of these problems is mainly easy so consider them as good starting point.Please note that the solution I shared for each problem is just one of the many potential solutions that could be implemented and often a BF (“Brute Force”) one. Therefore feel free to code your own version of the algorithm, trying to find the right balance between runtime and employed memory.A warm-up algorithm, that will help you practicing your slicing skills. In effect the only tricky bit is to make sure you are taking into account the case when the integer is negative. I have seen this problem presented in many different ways but it usually is the starting point for more complex requests.Algorithms that require you to apply some simple calculations using strings are very common, therefore it is important to get familiar with methods like .replace() and .split()that in this case helped me removing the unwanted characters and create a list of words, the length of which can be easily measured and summed.I find both approaches equally sharp: the first one for its brevity and the intuition of using the eval( )method to dynamically evaluate string-based inputs and the second one for the smart use of the ord( ) function to re-build the two strings as actual numbers trough the Unicode code points of their characters. If I really had to chose in between the two, I would probably go for the second approach as it looks more complex at first but it often comes handy in solving “Medium” and “Hard” algorithms that require more advanced string manipulation and calculations.Also in this case, two potential solutions are provided and I guess that, if you are pretty new to algorithms, the first approach looks a bit more familiar as it builds as simple counter starting from an empty dictionary.However understanding the second approach will help you much more in the longer term and this is because in this algorithm I simply used collection.Counter(s)instead of building a chars counter myself and replaced range(len(s)) with enumerate(s), a function that can help you identify the index more elegantly.The “Valid Palindrome” problem is a real classic and you will probably find it repeatedly under many different flavors. In this case, the task is to check weather by removing at most one character, the string matches with its reversed counterpart. When s = ‘radkar’ the function returns Trueas by excluding the ‘k’ we obtain the word ‘radar’ that is a palindrome.This is another very frequently asked problem and the solution provided above is pretty elegant as it can be written as a one-liner. An array is monotonic if and only if it is monotone increasing, or monotone decreasing and in order to assess it, the algorithm above takes advantage of the all() function that returns Trueif all items in an iterable are true, otherwise it returns False. If the iterable object is empty, the all() function also returns True.When you work with arrays, the .remove() and .append() methods are precious allies. In this problem I have used them to first remove each zero that belongs to the original array and then append it at the end to the same array.I was asked to solve this problem a couple of times in real interviews, both times the solution had to include edge cases (that I omitted here for simplicity). On paper, this an easy algorithm to build but you need to have clear in mind what you want to achieve with the for loop and if statement and be comfortable working with None values.The problem is fairly intuitive but the algorithm takes advantage of a few very common set operations like set() , intersection() or &and symmetric_difference()or ^that are extremely useful to make your solution more elegant. If it is the first time you encounter them, make sure to check this article:I wanted to close this section with another classic problem. A solution can be found pretty easily looping trough range(n) if you are familiar with both the prime numbers definition and the modulus operation.In this article I shared the solution of 10 Python algorithms that are frequently asked problems in coding interview rounds. If you are preparing an interview with a well-known tech Company this article is a good starting point to get familiar with common algorithmic patterns and then move to more complex questions. Also note that the exercises presented in this post (together with their solutions) are slight reinterpretations of problems available on Leetcode and GeekForGeeks. I am far from being an expert in the field therefore the solutions I presented are just indicative ones.This post includes affiliate links for which I may make a small commission at no extra cost to you, should you make a purchase.towardsdatascience.comtowardsdatascience.comlevelup.gitconnected.com",30/07/2020,10,36,32,"(700, 447)",2,1,0.0,19,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
95,Modeling and Generating Time-Series Data using TimeGAN,Towards Data Science,Archit Yadav,23.0,6.0,977,"In a previous article, the idea of generating artificial or synthetic data was explored, given a limited amount of dataset as a starter. The data taken at that time was tabular, which is like a regular dataset which we usually encounter. In this article however, we will look at time-series data and explore a way to generate synthetic time-series data.So how does a time-series data differ from a regular tabular data? A time-series dataset has one extra dimension — time. Think of it as a 3D dataset. Say, we have a dataset with 5 features and 5 instances of input.Then a time-series data is basically an extension of this table in the 3rd dimension, where each new table is just another dataset at a new timestep.The main difference is that time-series data has a lot more instances of data points compared to tabular ones.If we look at the energy dataset (available here and originally taken from here), it actually looks like a regular tabular dataset only, with every row implying a new timestep and having its corresponding datapoints in the form of features. Each of the entries is recorded after a duration of 10 minutes as per the data coloumn.But we saw in the previous section how it's ‘expected’ to look like a 3D tabular dataset, right? This is where we can make use of a clever way of sampling datapoints in order to create the 3rd dimension.We take a window of size 24 and run it along the rows of the dataset, shifting by one position at a time and hence obtaining a certain number of 2D matrices, each having a length of 24 and with all our coloumn features.In the energy dataset, there were 19736 rows. By shift-sampling every 24 rows, we get 19712 entries, each having 24 rows and 28 features. We can then of course randomly mix them in order to make them Independent and Identically Distributed (IID). So essentially we got a dataset of dimensions (19712, (24, 28)), where each of the 19712 instances has 24 rows (aka timesteps) and 28 features. This implementation can be found here.TimeGAN (Time-series Generative Adversarial Network) is an implementation for synthetic time-series data. It’s based on a paper by the same authors. YData’s ydata-synthetic TimeGAN implementation essentially makes it available in an easy-to-use library which can be installed as a Python Package Index (PyPi). In this article, we will use version 0.3.0, the latest at the time of writing.More details about this are on ydata-synthetic repository. In this section, we will look at generating a time-series dataset by using the energy dataset as the input source.We first read the energy dataset and then apply some pre-processing in the form of data transformation. This pre-processing essentially scales the data in the range [0, 1] and applies the data transformation we saw in the previous section.Now generating the actual synthetic data from this time-series data (energy_data) is the simplest part. We essentially train the TimeGAN model on our energy_data and then use that trained model to generate more.Where the parameters to be fed to TimeGAN constructor have to be defined appropriately according to our requirements. We have n_seq defined as 28 (features), seq_len defined as 24 (timesteps). The rest of the parameters are defined as follows:Now that we have our generated synth_data, let’s see how it compares visually in comparison to our original data. We can make a plot for each of the 28 features and see their variation with respect to timestep.Being only mildly interesting, these plots may not be useful for comparison purposes. It’s a no-brainer that synthetic data would definitely be different than the original (Real) data, else there wouldn’t be any point in doing so. And given that the dataset has so many features, it’s also difficult to visualize and interpret them together in an intuitive manner. What would be more interesting and helpful is the visualization (and comparison) of this generated data in a dimension that is more comprehendible and intuitive to us.We can make use of the following two well know visualization techniques:The essential idea behind these techniques is to apply dimensionality reduction in order to visualize those datasets which have a large number of dimensions, i.e., lots of features. Both PCA and t-SNE are able to achieve these, with the main difference between them being that PCA tries to preserve the global structure of the data (because it looks at ways in which a dataset’s variance is retained, globally, across the entire dataset), whereas t-SNE tries to preserve the local structure (by ensuring that a point’s neighbours which are close together in original data are also close together in the reduced dimensional space). An excellent answer detailing the difference can be found here.For our use case, we will use the PCA and TSNE objects from sklearn.Now that our data to plot is prepared, we can use matplotlib to plot both original and synthetic transformations. pca_real and pca_synth together give us the PCA results, and tsne_results contain both original and synthetic (due to concatenation) t-SNE transformations.What do these graphs tell us? They show us that this is how the whole dataset would possibly look if it was transformed to a dataset with fewer features (two axes corresponding to the two features). The PCA plot may not be sufficient to draw a proper conclusion, but the t-SNE plot seems to tell us that the original data (black) and synthetic data (red) seem to follow similar distribution. Additionally, a dataset-specific observation is that there are 7 groups (clusters) in the whole dataset, the datapoints of which are (visibly) similar to each other (hence clustered together).We looked briefly at time-series data and how it differs from tabular data. The use of TimeGAN architecture through ydata-synthetic library was done in order to generate more time-series data. The complete implementation in a notebook can be found here.",08/09/2021,1,0,8,"(700, 455)",6,1,0.0,10,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,surprise/amazement
96,Siri’s Descendants,inventing.ai,Peter Sweeney,2000.0,7.0,1370,"The internet swarms with intelligent assistants.What started as an isolated app on the iPhone has evolved. Intelligent assistants constitute an entirely new network of activity. No longer confined to our personal computing devices, assistants are being embedded within every object of interest in the cloud and the internet of things.Assistants have become far more nimble and lightweight than their monolithic ancestors; much more like smart ants than people. As specialists, they work cooperatively — sometimes competitively — to find information before people even realize they need it.People are still communicating directly with assistants, although rarely using natural language. Implicit communication dominates. Assistants respond and react to our subtle contextual interactions, and to each other, within vast informational ecosystems.This is how intelligent assistants evolved…Intelligent assistants like Siri, Google Now, and Cortana are so young, it’s difficult to imagine how they will change; harder still to imagine how they might die. But if history is a guide, inevitably they will give way to entirely new product forms.When pundits and analysts discuss the future of intelligent assistants, they typically extrapolate from the conceptual model of today’s assistants. The next version is always a better, smarter, faster version of the last, but it’s still the same species.But what can we learn about the future of assistants based on what Siri hasn’t become?As detailed in Bianca Bosker’s Inside Story of Siri’s Origins, when Apple acquired Siri, the scope of the product’s capabilities actually narrowed. Using the audacious vision of Siri’s founders as a palette, Apple selected a narrower set of product values on which to focus.The same force that reduced the scope of Apple’s Siri from a “do (everything) engine” to a much more narrow product is what keeps incumbents rooted to the existing concept of intelligent assistants.When forecasting change, it’s not so much what the technology of intelligent assistants might support as what product leaders choose to pursue. While many brazenly contest existing markets, product leaders look for new, underserved areas of the landscape to exploit.The future always surprises, but we can predict the trajectory of change by examining which product values are being embraced, and which ones are neglected.If you believe the internet abhors a vacuum, then you’ll understand how intelligent assistants will evolve to inhabit these underserved niches.Just like directions on a compass, the following maps point to fertile areas of the landscape, where new product forms may evolve.Note that product values are often coupled due to technological constraints. Decisions along one axis constrain possibilities along another. These couplings are explored at a high level in two-dimensional perceptual maps: interface and distribution; knowledge and tasks; organization and autonomy.No longer confined to our personal computing devices, assistants are becoming embedded within every object of interest in the cloud and the internet of things.People are still communicating directly with assistants, although rarely using natural language. Implicit communication dominates. Assistants respond and react to our subtle contextual interactions, and to each other, within vast informational ecosystems.The aspects of assistants that are most obvious to end-users are the interfaces (how we interact with assistants) and their mode of distribution (where people experience assistants).Today’s assistants are overwhelmingly focused on natural language interfaces. The experience of assistants that speak our language and communicate like a person has come to define the product class.This focus on natural language interfaces has biased the distribution of assistants to personal computing devices. Intelligent assistants embody any device capable of receiving and synthesizing speech, such as smartphones, desktops, wearables and cars.The underserved areas of this map involve communications that are not based in natural language. For example, there’s much to learn about our needs and intentions based on context (where we are and what we’re doing) as well as on our ability to make inferences based on the associations that people form (for example, the way that people organize information or express their likes and dislikes). Natural language is but the tip of this much larger iceberg of communications.These alternative forms of communication not only support individuals, but also groups. While it’s difficult to understand a room full of people all speaking at once, it’s much easier to understand their collaborative communications, such as their documents, click-paths, and sharing behavior. Therefore, the options for distributing intelligent assistants that use these implicit forms of communications are not constrained to personal computing devices, but may leverage entire networks.As a simple example, consider how you highlight your interests as you browse a website. You focus your attention on specific pages within the site. You follow your interests as you navigate from page to page. You may choose to share some information within the site with a friend. Now compound this behaviour across every visitor to the site.Intelligent assistants that are associated with the website can respond to these interactions to help the right information find each individual, as well as adapt the website to better address the needs of the entire group.As specialists, these machines have become far more nimble and lightweight than their monolithic ancestors; much more like smart ants than people.Intelligent assistants require domain knowledge to perform their tasks. For example, if your assistant is giving you advice on how to navigate to work, it needs to have knowledge about the geographic region (general knowledge) and knowledge of how you typically navigate (specific knowledge).Tasks and knowledge are tightly coupled. As you increase the specificity or the personalization of the tasks, the underlying knowledge needs to be far more specific to support it.Within this frame, today’s intelligent assistants are unabashedly generalists. They’re targeted to the masses. Like trivia buffs, their knowledge of the world is broad enough to be relevant to the needs of large groups of people, but few would describe them as experts. Their tasks are similarly general: retrieving information, providing navigational assistance, and answering simple questions.The underserved landscape points to much more specific domains of knowledge, the purview of experts and our individual subjective knowledge. Assistants that become experts necessarily take on a smaller scope of activities. They can’t know and do everything, so they become smaller in scope.The landscape for specific tasks is similarly underserved. Every website, every service, every app, and across the internet of things, everything embodies a collection of tasks that may be supported by intelligent assistants. In this environment, the metaphor of personal assistants quickly fragments into systems that are much more akin to colonies of ants.As specialists, they work cooperatively — sometimes competitively — to find information before people even realize they need it.The organizational structures in which assistants are placed constrain their autonomy. When embedded within a personal computing device, an intelligent assistant is directed to one-to-one interactions with their master.Since these assistants are acting as an agent of the individual (and only that individual), their autonomy is necessarily limited. While you might be comfortable with your executive assistant drafting your messages, I suspect you’d be less comfortable with your smartphone doing the same.In stark contrast, the underserved landscape embraces groups, both in terms of the interactions and the organizational structures.As assistants get smaller and more specialized, they can become agents of much more specific objects of interest, like places, websites, applications, and services. Within these smaller realms of interest, their autonomy can be much more expansive. You might not want a machine to act as your representative, but you would probably feel more comfortable if it represented only the website you’re visiting.With increased autonomy, the barriers to many-to-many interactions are removed. These small assistants can be organized as teams into networks, much like the documents that comprise a website, collaborating in an unfettered way with other assistants and the people that visit their realms.This market analysis highlighted a number of underserved areas as fertile ground for the evolution of intelligent assistants. It grounds this vision in predictable market dynamics. There’s obviously no shortage of space or product values to explore in these underserved areas.It says nothing, however, about when this future will arrive. Product evolution, like biological evolution, needs time and resources. The most important resource is the dedication of product leaders with the drive to pursue these new opportunities.Are you an entrepreneur, technologist, or investor that’s changing the market for intelligent assistants? If so, I’d love to hear your vision of the future.",19/12/2014,0,0,12,"(687, 463)",6,0,0.0,2,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,subjective,positive,trust/acceptance
97,Logistic Regression From Scratch in Python,Towards Data Science,Suraj Verma,254.0,9.0,1154,"In this article, we are going to implement the most commonly used Classification algorithm called the Logistic Regression. First, we will understand the Sigmoid function, Hypothesis function, Decision Boundary, the Log Loss function and code them alongside.After that, we will apply the Gradient Descent Algorithm to find the parameters, weights and bias . Finally, we will measure accuracy and plot the decision boundary for a linearly separable dataset and a non-linearly separable dataset.We will implement it all using Python NumPy and Matplotlib.towardsdatascience.comWe are going to do binary classification, so the value of y (true/target) is going to be either 0 or 1.For example, suppose we have a breast cancer dataset with X being the tumor size and y being whether the lump is malignant(cancerous) or benign(non-cancerous). Whenever a patient visits, your job is to tell him/her whether the lump is malignant(0) or benign(1) given the size of the tumor. There are only two classes in this case.So, y is going to be either 0 or 1.Let’s use the following randomly generated data as a motivating example to understand Logistic Regression.There are 2 features, n=2. There are 2 classes, blue and green.For a binary classification problem, we naturally want our hypothesis (y_hat) function to output values between 0 and 1 which means all Real numbers from 0 to 1.So, we want to choose a function that squishes all its inputs between 0 and 1. One such function is the Sigmoid or Logistic function.The Sigmoid Function squishes all its inputs (values on the x-axis) between 0 and 1 as we can see on the y-axis in the graph below.The range of inputs for this function is the set of all Real Numbers and the range of outputs is between 0 and 1.We can see that as z increases towards positive infinity the output gets closer to 1, and as z decreases towards negative infinity the output gets closer to 0.For Linear Regression, we had the hypothesis y_hat = w.X +b , whose output range was the set of all Real Numbers.Now, for Logistic Regression our hypothesis is — y_hat = sigmoid(w.X + b) , whose output range is between 0 and 1 because by applying a sigmoid function, we always output a number between 0 and 1.y_hat =z = w.X +bNow, you might wonder that there are lots of continuous function that outputs values between 0 and 1. Why did we choose the Logistic Function only, why not any other? Actually, there is a broader class of algorithms called Generalized Linear Models of which this is a special case. Sigmoid function falls out very naturally from it given our set of assumptions.For every parametric machine learning algorithm, we need a loss function, which we want to minimize (find the global minimum of) to determine the optimal parameters(w and b) which will help us make the best predictions.For Linear Regression, we had the mean squared error as the loss function. But that was a regression problem.For a binary classification problem, we need to be able to output the probability of y being 1(tumor is benign for example), then we can determine the probability of y being 0(tumor is malignant) or vice versa.So, we assume that the values that our hypothesis(y_hat) outputs between 0 and 1, is a probability of y being 1, then the probability of y being 0 will be (1-y_hat) .Remember that y is only 0 or 1. y_hat is a number between 0 and 1.More formally, the probability of y=1 given X , parameterized by w and b is y_hat (hypothesis). Then, logically the probability of y=0 given X , parameterized by w and b should be 1-y_hat . This can be written as —P(y = 1 | X; w, b) = y_hatP(y = 0 | X; w, b) = (1-y_hat)Then, based on our assumptions, we can calculate the loglikelihood of parameters using the above two equations and consequently determine the loss function which we have to minimize. The following is the Binary Coss-Entropy Loss or the Log Loss function —For reference — Understanding the Logistic Regression and likelihoodJ(w,b) is the overall cost/loss of the training set and L is the cost for ith training example.By looking at the Loss function, we can see that loss approaches 0 when we predict correctly, i.e, when y=0 and y_hat=0 or, y=1 and y_hat=1, and loss function approaches infinity if we predict incorrectly, i.e, when y=0 but y_hat=1 or, y=1 but y_hat=1.Now that we know our hypothesis function and the loss function, all we need to do is use the Gradient Descent Algorithm to find the optimal values of our parameters like this(lr →learning rate) —w := w-lr*dwb := b-lr*dbwhere, dw is the partial derivative of the Loss function with respect to w and db is the partial derivative of the Loss function with respect to b .dw = (1/m)*(y_hat — y).Xdb = (1/m)*(y_hat — y)Let’s write a function gradients to calculate dw and db .See comments(#).Now, we want to know how our hypothesis(y_hat) is going to make predictions of whether y=1 or y=0. The way we defined hypothesis is the probability of y being 1 given X and parameterized by w and b .So, we will say that it will make a prediction of —y=1 when y_hat ≥ 0.5y=0 when y_hat < 0.5Looking at the graph of the sigmoid function, we see that for —y_hat ≥ 0.5, z or w.X + b ≥ 0y_hat < 0.5, z or w.X + b < 0which means, we make a prediction for —y=1 when w.X + b ≥ 0y=0 when w.X + b < 0So, w.X + b = 0 is going to be our Decision boundary.The following code for plotting the Decision Boundary only works when we have only two features in X.Function to normalize the inputs. See comments(#).The train the function includes initializing the weights and bias and the training loop with mini-batch gradient descent.See comments(#).See comments(#).We check how many examples did we get right and divide it by the total number of examples.We get an accuracy of 100%. We can see from the above decision boundary graph that we are able to separate the green and blue classes perfectly.Let’s test out our code for data that is not linearly separable.Since Logistic Regression is only a linear classifier, we were able to put a decent straight line which was able to separate as many blues and greens from each other as possible.Let’s check accuracy for this —87 % accuracy. Not bad.When I was training the data using my code, I always got the NaN values in my losses list.Later I discovered the I was not normalizing my inputs, and that was the reason my losses were full of NaNs.If you are getting NaN values or overflow during training —Thanks for reading. For questions, comments, concerns, talk to be in the response section. More ML from scratch is coming soon.Check out the Machine Learning from scratch series —",08/04/2021,13,68,0,"(596, 389)",9,3,0.0,9,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
98,A Simple Introduction to Dropout Regularization (With Code!),Analytics Vidhya,Nisha McNealis,13.0,6.0,958,"Aman Oberoi and Nisha McNealis | UCLA ACM AIWhat is Dropout?“Dropout” in machine learning refers to the process of randomly ignoring certain nodes in a layer during training.In the figure below, the neural network on the left represents a typical neural network where all units are activated. On the right, the red units have been dropped out of the model — the values of their weights and biases are not considered during training.Dropout is used as a regularization technique — it prevents overfitting by ensuring that no units are codependent (more on this later).Other Common Regularization MethodsWhen it comes to combating overfitting, dropout is definitely not the only option. Common regularization techniques include:Despite the plethora of alternatives, dropout remains an extremely popular protective measure against overfitting because of its efficiency and effectiveness.How Does Dropout Work?When we apply dropout to a neural network, we’re creating a “thinned” network with unique combinations of the units in the hidden layers being dropped randomly at different points in time during training. Each time the gradient of our model is updated, we generate a new thinned neural network with different units dropped based on a probability hyperparameter p. Training a network using dropout can thus be viewed as training loads of different thinned neural networks and merging them into one network that picks up the key properties of each thinned network.This process allows dropout to reduce the overfitting of models on training data.This graph, taken from the paper “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” by Srivastava et al., compares the change in classification error of models without dropout to the same models with dropout (keeping all other hyperparameters constant). All the models have been trained on the MNIST dataset.It is observed that the models with dropout had a lower classification error than the same models without dropout at any given point in time. A similar trend was observed when the models were used to train other datasets in vision, as well as speech recognition and text analysis.The lower error is because dropout helps prevent overfitting on the training data by reducing the reliance of each unit in the hidden layer on other units in the hidden layers.These diagrams taken from the same paper show the features learned by an autoencoder on MNIST with one layer of 256 units without dropout (a) and the features learned by an identical autoencoder that used a dropout of p = 0.5 (b). It can be observed in figure a that the units don’t seem to pick up on any meaningful feature, whereas in figure b, the units seem to have picked up on distinct edges and spots in the data provided to them.This indicates that dropout helps break co-adaptations among units, and each unit can act more independently when dropout regularization is used. In other words, without dropout, the network would never be able to catch a unit A compensating for another unit B’s flaws. With dropout, at some point unit A would be ignored and the training accuracy would decrease as a result, exposing the inaccuracy of unit B.How to Use DropoutA CNN without dropout could be represented by code similar to this:To add a dropout layer, a programmer could add a line like this:The first parameter, circled in orange, is the probability p that a given unit will drop out. In this example, the probability is 0.5, which means that roughly half of the given units will drop out. The value 0.5 has been experimentally determined to be close to the optimal probability for a wide range of models, but feel free to experiment with other probabilities!Adjusting Weights During TestingSince dropout removes some of the units from a layer, a network with dropout will weigh the remaining units more heavily during each training run to compensate for the missing inputs. However, at test time, it is not feasible to use the weights of the trained model in their exaggerated states and so each weight is scaled down by multiplying by the hyperparameter p. This phenomenon can be observed in the example below.Let’s look at a network with four units in a layer (image a). The weight on each unit will initially be ¼ = 0.25.If we apply dropout with p = 0.5 to this layer, it could end up looking like image b. Since only two units are considered, they will each have an initial weight of ½ = 0.5. However, dropout is only used in training, so we don’t want these weights to be fixed at this high a number during testing.To fix this issue, when we move to the testing stage we multiply the weights by p (as seen in the image below), ending up with 0.5*0.5 = 0.25, the correct initial weight.Hyperparameters in Dropout RegularizationHyperparameter settings that have been found to work well with dropout regularization include a large decaying learning rate and a high momentum. This is because restricting our weight vectors using dropout enables us to use a large learning rate without worrying about the weights blowing up. The noise produced by dropout coupled with our large decaying learning rate helps us explore different regions of our loss function and hopefully reach a better minimum.The Downside of DropoutAlthough dropout is clearly a highly effective tool, it comes with certain drawbacks. A network with dropout can take 2–3 times longer to train than a standard network. One way to attain the benefits of dropout without slowing down training is by finding a regularizer that is essentially equivalent to a dropout layer. For linear regression, this regularizer has been proven to be a modified form of L2 regularization. For more complex models, an equivalent regularizer has yet to be identified. Until then, when in doubt: dropout.Try it Yourself!",23/04/2020,0,9,12,"(517, 291)",10,1,0.0,4,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
99,Clustering Based Unsupervised Learning,Towards Data Science,Syed Sadat Nazrul,2700.0,6.0,805,"Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from “unlabeled” data (a classification or categorization is not included in the observations). Common scenarios for using unsupervised learning algorithms include:- Data Exploration- Outlier Detection- Pattern RecognitionWhile there is an exhaustive list of clustering algorithms available (whether you use R or Python’s Scikit-Learn), I will attempt to cover the basic concepts.The most common and simplest clustering algorithm out there is the K-Means clustering. This algorithms involve you telling the algorithms how many possible cluster (or K) there are in the dataset. The algorithm then iteratively moves the k-centers and selects the datapoints that are closest to that centroid in the cluster.Taking K=3 as an example, the iterative process is given below:One obvious question that may come to mind is the methodology for picking the K value. This is done using an elbow curve, where the x-axis is the K-value and the y axis is some objective function. A common objective function is the average distance between the datapoints and the nearest centroid.The best number for K is the “elbow” or kinked region. After this point, it is generally established that adding more clusters will not add significant value to your analysis. Below is an example script for K-Means using Scikit-Learn on the iris dataset:One issue with K-means, as see in the 3D diagram above, is that it does hard labels. However, you can see that datapoints at the boundary of the purple and yellow clusters can be either one. For such circumstances, a different approach may be necessary.In K-Means, we do what is called “hard labeling”, where we simply add the label of the maximum probability. However, certain data points that exist at the boundary of clusters may simply have similar probabilities of being on either clusters. In such circumstances, we look at all the probabilities instead of the max probability. This is known as “soft labeling”.For the above Gaussian Mixure Model, the colors of the datapoints are based on the Gaussian probability of being near the cluster. The RGB values are based on the nearness to each of the red, blue and green clusters. If you look at the datapoints near the boundary of the blue and red cluster, you shall see purple, indicating the datapoints are close to either clusters.Since we have talked about numerical values, let’s take a turn towards categorical values. One such application is text analytics. Common approach for such problems is topic modelling, where documents or words in a document are categorized into topics. The simplest of these is the TF-IDF model. The TF-IDF model classifies words based on their importance. This is determined by how frequent are they in specific documents (e.g. specific science topics in scientific journals) and words that are common among all documents (e.g. stop words).One of my favorite algorithms is the Latent Dirichlet Allocation or LDA model. In this model, each word in the document is given a topic based on the entire document corpus. Below, I have attached a slide from the University of Washington’s Machine Learning specialization course:The mechanics behind the LDA model itself is hard to explain in this blog. However, a common question people have is deciding on the number of topics. While there is no established answer for this, personally I prefer to implement a elbow curve of K-Means of the word vector of each document. The closeness of each word vector can be determined by the cosine distance.Finally, let’s cover some timeseries analysis. For clustering, my favourite is using Hidden Markov Models or HMM. In a Markov Model, we look for states and the probability of the next state given the current state. An example below is of a dog’s life in Markov Model.Let’s assume the dog is sick. Given the current state, there is a 0.6 chance it will continue being sick the next hour, 0.4 that it is sleeping, 05 pooping, 0.1 eating and 0.4 that it will be healthy again. In an HMM, you provide how many states there may be inside the timeseries data for the model to compute. An example of the Boston house prices dataset is given below with 3 states.As with every clustering problem, deciding the number of states is also a common issue. This may either be domain based. e.g. in voice recognition, it is common practice to use 3 states. Another possibility is using an elbow curve.As I have mentioned at the beginning of this blog, it is not possible for me to cover every single unsupervised models out there. At the same time, based on your use case, you may need a combination of algorithms to get a different perspective of the same data. With that I would like to leave you off with Scikit-Learn’s famous clustering demonstrations on the toy dataset:",03/04/2018,3,0,0,"(577, 412)",12,0,0.0,0,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
100,Color Based Object Segmentation,,Akash Shende,57.0,3.0,341,"In this picture, Pranav Mistry is using color marker on his fingers to track the gesture and his wearable computer perform action based on gestures. That sounds easy! But No, it’s not. Computer need to understand those color marker first, for that it needs to separate marker from any surroundings.Segmentation can be helpful to achieve this. Various methods are available for segmentation, however, this article talks about robust Color based object segmentation.Create binary mask that separates blue T-shirt from rest.To find blue t-shirt in given image, I used OpenCV’s inRange method: Which takes color (or greyscale) image, lower & higher range value as its parameter and returns binary image, where pixel value set to 0 when input pixel doesn’t fall in specified range, otherwise pixel value set to 1. With the help of this function and after determining range values, I ended up with this mask.But you can see there are problems! It’s not able to create mask for complete t-shirt, also it mask eyes which aren’t blue. This is happening because light from one side of body whitens the right side at the same time creates shadow in left region. Thus, it creates different shades of blue and results into partial segmentation.Normalization of color plane reduces variation in light by averaging pixel values, thus it removes highlighted and shadowed region and make image flatten. Following image is free from highlights & shadows and it is divided into one large green background, blue t-shirt and skin. Now the inRange method able to mask only t-shirt.Following function converts a pixel at X, Y location into its corresponding normalized rgb pixel.Let R,G,B are pixel values, then normalized pixel g(x,y) is calculated as,divide the individual color component by sum of all color components and multiply by 255. Division results into floating point number in range of 0.0 to 1.0 and as this is 8 bit image result is scaled up by 255.This function accepts 8 bit RGB image matrix of size 800x600 and returns normalized RGB image.Originally published at akash0x53.github.io on April 29, 2013.",29/04/2013,1,1,9,"(588, 382)",5,0,0.0,3,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
101,KMeans Clustering for Classification,Towards Data Science,Mudassir Khan,84.0,3.0,423,"Clustering as a method of finding subgroups within observations is used widely in applications like market segmentation wherein we try and find some structure in the data. Although an unsupervised machine learning technique, the clusters can be used as features in a supervised machine learning model.Clustering is a type of unsupervised machine learning which aims to find homogeneous subgroups such that objects in the same group (clusters) are more similar to each other than the others.KMeans is a clustering algorithm which divides observations into k clusters. Since we can dictate the amount of clusters, it can be easily used in classification where we divide data into clusters which can be equal to or more than the number of classes.I’ll be using the MNIST dataset which comes with scikit learn which is a collection of labelled handwritten digits and use KMeans to find clusters within the dataset and test how good it is as a feature.I have created a class named clust for this purpose which when initialized takes in a sklearn dataset and divides it into train and test dataset.The function KMeans applies KMeans clustering to the train data with the number of classes as the number of clusters to be made and creates labels both for train and test data. The parameter output controls how do we want to use these new labels, ‘add’ will add the labels as a feature in the dataset and ‘replace’ will use the labels instead of the train and test dataset to train our classification model.In the first attempt only clusters found by KMeans are used to train a classification model. These clusters alone give a decent model with an accuracy of 78.33%. Let’s compare it with an out of the box Logistic Regression model.In this case I am only using the features (greyscale intensity values) to train a Logistic Regression model. It results in a much better model with an accuracy of 95.37%. Let’s add the clusters as a feature(column) and train the same Logistic Regression model.In our final iteration we are using the clusters as features, the results show an improvement over our previous model.Clustering apart from being an unsupervised machine learning can also be used to create clusters as features to improve classification models. On their own they aren’t enough for classification as the results show. But when used as features they improve model accuracy.You can use the class I created to tweak and test different models, for eg test a Random Forest Classifier and share something I didn’t find in the comments.",02/08/2017,0,0,0,"(700, 169)",4,0,0.0,2,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
102,Word embedding,"Data Science Group, IITR",Manjeet Singh,226.0,12.0,1804,"What are word embeddings? Why we use word embeddings? Before going into details. lets see some example :so how do they do this. Actually these things are application of Text processing. we use text to do sentiment analysis, clustering similar word, document classification and tagging.As we read any newspaper we can say that what is the news about but how computer will do these things? Computer can match strings and can tell us that they are same or not But how do we make computers tell you about football or Ronaldo when you search for Messi?For tasks like object or speech recognition we know that all the information required to successfully perform the task is encoded in the data (because humans can perform these tasks from the raw data). However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore ‘cat’ may be represented as Id537 and 'dog' as Id143. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols.Here comes word embeddings. word embeddings are nothing but numerical representations of texts.There are many different types of word embeddings:count vector model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider we have D documents and T is the number of different words in our vocabulary then the size of count vector matrix will be given by D*T . Let’s take the following two sentences:Document 1: “The cat sat on the hat”Document 2: “The dog ate the cat and the hat”From these two documents, our vocabulary is as follows:{ the, cat, sat, on, hat, dog, ate, and }so D = 2, T = 8Now, we count the number of times each word occurs in each document. In Document 1, “the” appears twice, and “cat”, “sat”, “on”, and “hat” each appear once, so the feature vector for documents is:{ the, cat, sat, on, hat, dog, ate, and }so the count vector matrix is :-Now, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for ‘cat’ in the above matrix is [1,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as — Document 2 contains ‘hat’: once, ‘dog’: once and ‘the’ thrice and so on.There is a problem related to dimensions of the matrix for a large corpus of text so we can use stop words (remove common words like ‘a, an, this, that’) or we can extract some top words from vocabulary based on frequency and use as a new vocabulary or we can use both methods.In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform. This method takes into account not just the occurrence of a word in a single document but in the entire corpus. lets take a business article this article will contain more business related terms like Stock-market, Prices, shares etc in comparison to any other article. but terms like “a, an, the” will come in each article with high frequency. so this method will penalize these type of high frequency words.Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency.TF = (Number of times term t appears in a document)/(Number of terms in the document)IDF = log(N/n), where, N is the total number of documents and n is the number of documents a term t has appeared in.TF-IDF(t, document) = TF(t, document) * IDF(t)Words co-occurrence matrix describes how words occur together that in turn captures the relationships between words. Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus. As an example of words co-occurrence, consider a corpus consisting of the following documents:penny wise and pound foolisha penny saved is a penny earnedLetting count(w(next)|w(current)) represent how many times word w(next) follows the word w(current), we can summarize co-occurrence statistics for words “a” and “penny” as:The above table shows that “a” is followed twice by “penny” while words “earned”, “saved”, and “wise” each follows “penny” once in our corpus. Thus, “earned” is one out of three times probable to appear after “penny.” The count shown above is called bigram frequency; it looks into only the next word from a current word. Given a corpus of N words, we need a table of size NxN to represent bigram frequencies of all possible word-pairs. Such a table is highly sparse as most frequencies are equal to zero. In practice, the co-occurrence counts are converted to probabilities. This results in row entries for each row adding up to one in the co-occurrence matrix.But, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.Let me illustrate this more clearly. For example, you perform PCA on the above matrix of size NXN. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form N X k.And, a single word, instead of being represented in N dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.So, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation.Advantages of Co-occurrence MatrixDisadvantages of Co-Occurrence MatrixThe frequency-based methods are easy to understand and there are many applications of them like text classification, sentiment analysis and many more. Because they extract positive and negative words from the text so we can easily classify them with the help of any good machine learning algorithms.CBOW is learning to predict the word by the context. A context may be single word or multiple word for a given target words.lets see this by an example “The cat jumped over the puddle.”So one approach is to treat {“The”, “cat”, ’over”, “the’, “puddle”} as a context and from these words, be able to predict or generate the center word “jumped”. This type of model we call a Continuous Bag of Words (CBOW) Model.Before going in detail in CBOW lets talk about one hot representation of words : One way to represent a word as a vector is one-hot representation. One-hot representation is a representation method in which only one element is 1 and the other elements are 0 in the vector. By setting 1 or 0 for each dimension, it represents “that word or not”.Let’s say, for example, we represent the word “python” as one-hot representation. Here, the vocabulary which is a set of words is five words(nlp, python, word, ruby, one-hot). Then the following vector expresses the word “python”:Although one-hot representation is simple, there are weak points: it is impossible to obtain meaningful results with arithmetic between vectors. Let’s say we take an inner product to calculate similarity between words. In one-hot representation, different words are 1 in different places and the other elements are 0. Thus, the result of taking the dot product between the different words is 0. This is not a useful result.Another weak point is the vector tend to become very high dimension. Since one dimension is assigned to one word, as the number of vocabularies increases, it becomes very high dimension.We breakdown the way this model works in these steps:Wi = weight matrix between input layer and hidden layer of size [V * N]Wj= weight matrix between hidden layer and output layer of size [N * V]The loss function used is Cross entropy.and then we use gradient descent or any good optimizer to train this network. After training the weight between the hidden layer and the output layer (Wj) is taken as the word vector representation of the word. where each column represent a word vector of size [1 * N].Another approach is to create a model such that given the center word “jumped”, the model will be able to predict or generate the surrounding words “The”, “cat”, “over”, “the”, “puddle”. Here we call the word “jumped” the context. We call this type of model a SkipGram model.Skip-gram model reverses the use of target and context words. Skip-gram take a word and predict the context word from it.We breakdown the way this model works in these steps:Wi = weight matrix between input layer and hidden layer of size [V * N]Wj= weight matrix between hidden layer and output layer of size [N * V]Advantages/Disadvantages of CBOW and Skip-gram:There are various NLP based tasks where these word embeddings used in deep learning have surpassed older ML based models. There are various NLP applications where they are used extensively. Eg. Automatic summarization, Machine translation, Named entity resolution, Sentiment analysis, Chat-bot, Information retrieval, Speech recognition, Question answering etc.We can visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the t-SNE dimensionality reduction technique. When we inspect these visualizations it becomes apparent that the vectors capture some general, and in fact quite useful, semantic information about words and their relationships to one another. It was very interesting when we first discovered that certain directions in the induced vector space specialize towards certain semantic relationships, e.g. male-female, verb tense and even country-capital relationships between words, as illustrated in the figure below .This explains why these vectors are also useful as features for many canonical NLP prediction tasks, such as part-of-speech tagging or named entity recognition.As we can see all the similar words are in together. We can perform some amazing tasks from word embeddings of Word2Vec.Word Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead.One suggestion is that do not miss out references, by reading them only you can understand algorithm properly.Hit ❤ if this makes you little bit more intelligent.",14/10/2017,0,22,15,"(605, 301)",11,9,0.0,7,af,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,subjective,neutral,anger/irritation
103,The Poisson Distribution and Poisson Process Explained,Towards Data Science,Will Koehrsen,35000.0,14.0,2901,"A tragedy of statistics in most schools is how dull it’s made. Teachers spend hours wading through derivations, equations, and theorems, and, when you finally get to the best part — applying concepts to actual numbers — it’s with irrelevant, unimaginative examples like rolling dice. This is a shame as stats can be enjoyable if you skip the derivations (which you’ll likely never need) and focus on using the ideas to solve interesting problems.In this article, we’ll cover Poisson Processes and the Poisson distribution, two important probability concepts. After highlighting only the relevant theory, we’ll work through a real-world example, showing equations and graphs to put the ideas in a proper context.A Poisson Process is a model for a series of discrete event where the average time between events is known, but the exact timing of events is random. The arrival of an event is independent of the event before (waiting time between events is memoryless). For example, suppose we own a website which our content delivery network (CDN) tells us goes down on average once per 60 days, but one failure doesn’t affect the probability of the next. All we know is the average time between failures. This is a Poisson process that looks like:The important point is we know the average time between events but they are randomly spaced (stochastic). We might have back-to-back failures, but we could also go years between failures due to the randomness of the process.A Poisson Process meets the following criteria (in reality many phenomena modeled as Poisson processes don’t meet these exactly):The last point — events are not simultaneous — means we can think of each sub-interval of a Poisson process as a Bernoulli Trial, that is, either a success or a failure. With our website, the entire interval may be 600 days, but each sub-interval — one day — our website either goes down or it doesn’t.Common examples of Poisson processes are customers calling a help center, visitors to a website, radioactive decay in atoms, photons arriving at a space telescope, and movements in a stock price. Poisson processes are generally associated with time, but they do not have to be. In the stock case, we might know the average movements per day (events per time), but we could also have a Poisson process for the number of trees in an acre (events per area).(One instance frequently given for a Poisson Process is bus arrivals (or trains or now Ubers). However, this is not a true Poisson process because the arrivals are not independent of one another. Even for bus systems that do not run on time, whether or not one bus is late affects the arrival time of the next bus. Jake VanderPlas has a great article on applying a Poisson process to bus arrival times which works better with made-up data than real-world data.)The Poisson Process is the model we use for describing randomly occurring events and by itself, isn’t that useful. We need the Poisson Distribution to do interesting things like finding the probability of a number of events in a time period or finding the probability of waiting some time until the next event.The Poisson Distribution probability mass function gives the probability of observing k events in a time period given the length of the period and the average events per time:This is a little convoluted, and events/time * time period is usually simplified into a single parameter, λ, lambda, the rate parameter. With this substitution, the Poisson Distribution probability function now has one parameter:Lambda can be thought of as the expected number of events in the interval. (We’ll switch to calling this an interval because remember, we don’t have to use a time period, we could use area or volume based on our Poisson process). I like to write out lambda to remind myself the rate parameter is a function of both the average events per time and the length of the time period but you’ll most commonly see it as directly above.As we change the rate parameter, λ, we change the probability of seeing different numbers of events in one interval. The below graph is the probability mass function of the Poisson distribution showing the probability of a number of events occurring in an interval with different rate parameters.The most likely number of events in the interval for each curve is the rate parameter. This makes sense because the rate parameter is the expected number of events in the interval and therefore when it’s an integer, the rate parameter will be the number of events with the greatest probability.When it’s not an integer, the highest probability number of events will be the nearest integer to the rate parameter, since the Poisson distribution is only defined for a discrete number of events. The discrete nature of the Poisson distribution is also why this is a probability mass function and not a density function. (The rate parameter is also the mean and variance of the distribution, which do not need to be integers.)We can use the Poisson Distribution mass function to find the probability of observing a number of events over an interval generated by a Poisson process. Another use of the mass function equation — as we’ll see later — is to find the probability of waiting some time between events.For the problem we’ll solve with a Poisson distribution, we could continue with website failures, but I propose something grander. In my childhood, my father would often take me into our yard to observe (or try to observe) meteor showers. We were not space geeks, but watching objects from outer space burn up in the sky was enough to get us outside even though meteor showers always seemed to occur in the coldest months.The number of meteors seen can be modeled as a Poisson distribution because the meteors are independent, the average number of meteors per hour is constant (in the short term), and — this is an approximation — meteors don’t occur simultaneously. To characterize the Poisson distribution, all we need is the rate parameter which is the number of events/interval * interval length. From what I remember, we were told to expect 5 meteors per hour on average or 1 every 12 minutes. Due to the limited patience of a young child (especially on a freezing night), we never stayed out more than 60 minutes, so we’ll use that as the time period. Putting the two together, we get:What exactly does “5 meteors expected” mean? Well, according to my pessimistic dad, that meant we’d see 3 meteors in an hour, tops. At the time, I had no data science skills and trusted his judgment. Now that I’m older and have a healthy amount of skepticism towards authority figures, it’s time to put his statement to the test. We can use the Poisson distribution to find the probability of seeing exactly 3 meteors in one hour of observation:14% or about 1/7. If we went outside every night for one week, then we could expect my dad to be right precisely once! While that is nice to know, what we are after is the distribution, the probability of seeing different numbers of meteors. Doing this by hand is tedious, so we’ll use Python — which you can see in this Jupyter Notebook — for calculation and visualization.The below graph shows the Probability Mass Function for the number of meteors in an hour with an average time between meteors of 12 minutes (which is the same as saying 5 meteors expected in an hour).This is what “5 expected events” means! The most likely number of meteors is 5, the rate parameter of the distribution. (Due to a quirk of the numbers, 4 and 5 have the same probability, 18%). As with any distribution, there is one most likely value, but there are also a wide range of possible values. For example, we could go out and see 0 meteors, or we could see more than 10 in one hour. To find the probabilities of these events, we use the same equation but this time calculate sums of probabilities (see notebook for details).We already calculated the chance of seeing exactly 3 meteors as about 14%. The chance of seeing 3 or fewer meteors in one hour is 27% which means the probability of seeing more than 3 is 73%. Likewise, the probability of more than 5 meteors is 38.4% while we could expect to see 5 or fewer meteors in 61.6% of observation hours. Although it’s small, there is a 1.4% chance of observing more than 10 meteors in an hour!To visualize these possible scenarios, we can run an experiment by having our sister record the number of meteors she sees every hour for 10,000 hours. The results are shown in the histogram below:(This is obviously a simulation. No sisters were employed for this article.) Looking at the possible outcomes reinforces that this is a distribution and the expected outcome does not always occur. On a few lucky nights, we’d witness 10 or more meteors in an hour, although we’d usually see 4 or 5 meteors.The rate parameter, λ, is the only number we need to define the Poisson distribution. However, since it is a product of two parts (events/interval * interval length) there are two ways to change it: we can increase or decrease the events/interval and we can increase or decrease the interval length.First, let’s change the rate parameter by increasing or decreasing the number of meteors per hour to see how the distribution is affected. For this graph, we are keeping the time period constant at 60 minutes (1 hour).In each case, the most likely number of meteors over the hour is the expected number of meteors, the rate parameter for the Poisson distribution. As one example, at 12 meteors per hour (MPH), our rate parameter is 12 and there is an 11% chance of observing exactly 12 meteors in 1 hour. If our rate parameter increases, we should expect to see more meteors per hour.Another option is to increase or decrease the interval length. Below is the same plot, but this time we are keeping the number of meteors per hour constant at 5 and changing the length of time we observe.It’s no surprise that we expect to see more meteors the longer we stay out! Whoever said “he who hesitates is lost” clearly never stood around watching meteor showers.An intriguing part of a Poisson process involves figuring out how long we have to wait until the next event (this is sometimes called the interarrival time). Consider the situation: meteors appear once every 12 minutes on average. If we arrive at a random time, how long can we expect to wait to see the next meteor? My dad always (this time optimistically) claimed we only had to wait 6 minutes for the first meteor which agrees with our intuition. However, if we’ve learned anything, it’s that our intuition is not good at probability.I won’t go into the derivation (it comes from the probability mass function equation), but the time we can expect to wait between events is a decaying exponential. The probability of waiting a given amount of time between successive events decreases exponentially as the time increases. The following equation shows the probability of waiting more than a specified time.With our example, we have 1 event/12 minutes, and if we plug in the numbers we get a 60.65% chance of waiting > 6 minutes. So much for my dad’s guess! To show another case, we can expect to wait more than 30 minutes about 8.2% of the time. (We need to note this is between each successive pair of events. The waiting times between events are memoryless, so the time between two events has no effect on the time between any other events. This memorylessness is also known as the Markov property).A graph helps us to visualize the exponential decay of waiting time:There is a 100% chance of waiting more than 0 minutes, which drops off to a near 0% chance of waiting more than 80 minutes. Again, since this is a distribution, there are a wide range of possible interarrival times.Conversely, we can use this equation to find the probability of waiting less than or equal to a time:We can expect to wait 6 minutes or less to see a meteor 39.4% of the time. We can also find the probability of waiting a period of time: there is a 57.72% probability of waiting between 5 and 30 minutes to see the next meteor.To visualize the distribution of waiting times, we can once again run a (simulated) experiment. We simulate watching for 100,000 minutes with an average rate of 1 meteor / 12 minutes. Then, we find the waiting time between each meteor we see and plot the distribution.The most likely waiting time is 1 minute, but that is not the average waiting time. Let’s get back to the original question: how long can we expect to wait on average to see the first meteor if we arrive at a random time?To answer the average waiting time question, we’ll run 10,000 separate trials, each time watching the sky for 100,000 minutes. The graph below shows the distribution of the average waiting time between meteors from these trials:The average of the 10,000 averages turns out to be 12.003 minutes. Even if we arrive at a random time, the average time we can expect to wait for the first meteor is the average time between occurrences. At first, this may be difficult to understand: if events occur on average every 12 minutes, then why should we have to wait the entire 12 minutes before seeing one event? The answer is this is an average waiting time, taking into account all possible situations.If the meteors came exactly every 12 minutes, then the average time we’d have to wait to see the first one would be 6 minutes. However, because this is an exponential distribution, sometimes we show up and have to wait an hour, which outweighs the greater number of times when we wait fewer than 12 minutes. This is called the Waiting Time Paradox and is a worthwhile read.As a final visualization, let’s do a random simulation of 1 hour of observation.Well, this time we got exactly what we expected: 5 meteors. We had to wait 15 minutes for the first one, but then had a good stretch of shooting stars. At least in this case, it’d be worth going out of the house for celestial observation!A Binomial Distribution is used to model the probability of the number of successes we can expect from n trials with a probability p. The Poisson Distribution is a special case of the Binomial Distribution as n goes to infinity while the expected number of successes remains fixed. The Poisson is used as an approximation of the Binomial if n is large and p is small.As with many ideas in statistics, “large” and “small” are up to interpretation. A rule of thumb is the Poisson distribution is a decent approximation of the Binomial if n > 20 and np < 10. Therefore, a coin flip, even for 100 trials, should be modeled as a Binomial because np =50. A call center which gets 1 call every 30 minutes over 120 minutes could be modeled as a Poisson distribution as np = 4. One important distinction is a Binomial occurs for a fixed set of trials (the domain is discrete) while a Poisson occurs over a theoretically infinite number of trials (continuous domain). This is only an approximation; remember, all models are wrong, but some are useful!For more on this topic, see the Related Distribution section on Wikipedia for the Poisson Distribution. There is also a good Stack Exchange answer here.Meteors are the streaks of light you see in the sky that are caused by pieces of debris called meteoroids burning up in the atmosphere. A meteoroid can come from an asteroid, a comet, or a piece of a planet and is usually millimeters in diameter but can be up to a kilometer. If the meteoroid survives its trip through the atmosphere and impacts Earth, it’s called a meteorite. Asteroids are much larger chunks of rock orbiting the sun in the asteroid belt. Pieces of asteroids that break off become meteoroids. The more you know!.To summarize, a Poisson Distribution gives the probability of a number of events in an interval generated by a Poisson process. The Poisson distribution is defined by the rate parameter, λ, which is the expected number of events in the interval (events/interval * interval length) and the highest probability number of events. We can also use the Poisson Distribution to find the waiting time between events. Even if we arrive at a random time, the average waiting time will always be the average time between events.The next time you find yourself losing focus in statistics, you have my permission to stop paying attention to the teacher. Instead, find the relevant equations and apply them to an interesting problem. You can learn the material and you’ll have an appreciation for how stats helps us to understand the world. Above all, stay curious: there are many amazing phenomenon in the world, and we can use data science is a great tool for exploring them,As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will.",21/01/2019,0,15,14,"(636, 287)",17,1,0.0,22,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
104,Do we need deep graph neural networks?,Towards Data Science,Michael Bronstein,5500.0,9.0,1920,"This year, deep learning on graphs was crowned among the hottest topics in machine learning. Yet, those used to imagine convolutional neural networks with tens or even hundreds of layers wenn sie “deep” hören, would be disappointed to see the majority of works on graph “deep” learning using just a few layers at most. Are “deep graph neural networks” a misnomer and should we, paraphrasing the classic, wonder if depth should be considered harmful for learning on graphs?Training deep graph neural networks is hard. Besides the standard plights observed in deep neural architectures such as vanishing gradients in back-propagation and overfitting due to a large number of parameters, there are a few problems specific to graphs. One of them is over-smoothing, the phenomenon of the node features tending to converge to the same vector and become nearly indistinguishable as the result of applying multiple graph convolutional layers [1]. This behaviour was first observed in GCN models [2,3], which act similarly to low-pass filters. Another phenomenon is a bottleneck, resulting in “over-squashing” of information from exponentially many neighbours into fixed-size vectors [4].Significant efforts have recently been dedicated to coping with the problem of depth in graph neural networks, in hope to achieve better performance and perhaps avoid embarrassment in using the term “deep learning” when referring to graph neural networks with just two layers. Typical approaches can be split into two families. First, regularisation techniques such as edge-wise dropout (DropEdge) [5], pairwise distance normalisation between node features (PairNorm) [6], or node-wise mean and variance normalisation (NodeNorm) [7]. Second, architectural changes including various types of residual connection such as jumping knowledge [8] or affine residual connection [9]. While these techniques allow to train deep graph neural networks with tens of layers (a feat difficult or even impossible otherwise), they fail to demonstrate significant gains. Even worse, the use of deep architectures often results in decreased performance. The table below, reproduced from [7], shows a typical experimental evaluation comparing graph neural networks of different depths on a node-wise classification task:Apparent from this table is the difficulty to disentangle the advantages brought by a deep architecture from the “tricks” necessary to train such a neural network. In fact, the NodeNorm in the above example also improves a shallow architecture with only two layers, which achieves the best performance. It is thus not clear whether a deeper graph neural network with ceteris paribus performs better.These results are obviously in stark contrast to the conventional setting of deep learning on grid-structured data, where “ultra-deep” architectures [10,11] have brought a breakthrough in performance and are ubiquitously used today. In the following, I will try to provide directions that might help answer the provocative question raised in the title of this post. I myself do not have a clear answer yet.Structure of the graph. Since grids are special graphs, there certainly are examples of graphs on which depth helps. Besides grids, it appears that “geometric” graphs representing structures such as molecules, point clouds [12], or meshes [9] benefit from deep architectures as well. Why are such graphs so different from citation networks such as Cora, PubMed, or CoauthorsCS commonly used to evaluate graph neural networks? One of the differences is that the latter are similar to “small world” networks with low diameter, where one can reach any node from any other node in a few hops. As a result, the receptive field of just a few convolutional layers already covers the entire graph [13], so adding more layers does not help to reach remote nodes. In computer vision, on the other hand, the receptive field grows polynomially, and many layers are needed to produce a receptive field that captures the context of an object in the image [14].Long-range vs short-range problems. A somewhat different but related distinction is whether the problem requires long or short-range information. For example, in social networks, predictions typically rely only on short-range information from the local neighbourhood of a node and do not improve by adding distant information. Such tasks can therefore be carried out by shallow GNNs. Molecular graphs, on the other hand, often require long-range information, as chemical properties of a molecule may depend on the combination of atoms at its opposite sides [15]. Deep GNNs might be needed to take advantage of these long-range interactions. However, if the structure of the graph results in the exponential growth of the receptive field, the bottleneck phenomenon can prevent the effective propagation of long-range information, explaining why deep models do not improve in performance [4].Theoretical limitations. Besides a larger receptive field, one of the key advantages deep architectures offer in computer vision problems is their ability to compose complex features from simple ones. Visualising the features learned by CNNs from face images shows progressively more complex features starting from simple geometric primitives and ending with entire facial structures, suggesting that the legendary “grandmother neuron” is more truth than myth. It appears that such compositionality is impossible with graphs, e.g. one cannot compose triangles from edges no matter how deep the neural network is [16]. On the other hand, it was shown that the computation of some graph properties such as graph moments using message passing networks is impossible without a certain minimum depth [17]. Overall, we currently lack the understanding of which graph properties can be represented by shallow GNNs, which ones require deep models, and which ones cannot be computed at all.Depth vs richness. As opposed to computer vision where the underlying grid is fixed, in deep learning on graphs, the structure of the graph does matter and is taken into account. It is possible to design more elaborate message passing mechanisms that account for complex higher-order information such as motifs [18] or substructure counts [19] that standard GNNs cannot discover. Instead of using deep architectures with simple 1-hop convolutions, one can opt for shallow networks with richer multi-hop filters. Our recent paper on scalable inception-like graph neural networks (SIGN) takes this idea to the extreme by using a single-layer linear graph convolutional architecture with multiple pre-computed filters. We show performance comparable to much more complex models at a fraction of their time complexity [20]. Interestingly, the computer vision community has taken the converse path: early shallow CNN architectures with large (up to 11×11) filters such as AlexNet were replaced by very deep architectures with small (typically 3×3) filters.Evaluation. Last but not least, the predominant evaluation methodology of graph neural networks has been heavily criticised, among others by Oleksandr Shchur and colleagues from the group of Stephan Günnemann [21], who drew the attention to the pitfalls of commonly used benchmarks and showed that simple models perform on par with more complex ones if evaluated in a fair setting. Some of the phenomena we observe with deep architectures, such as performance decreasing with depth, might simply stem from overfitting on small datasets. The new Open Graph Benchmark addresses some of these issues, providing very large graphs with hard training and testing data splits. I believe that we need to do carefully designed specific experiments in order to better understand whether or when depth is useful in deep learning on graphs.[1] More precisely, over-smoothing makes node feature vector collapse into a subspace, see K. Oono and T. Suzuki, Graph neural networks exponentially loose expressive power for node classification (2019). arXiv:1905.10947, which provides asymptotic analysis using dynamic systems formalist.[2] Q. Li, Z. Han, X.-M. Wu, Deeper insights into graph convolutional networks for semi-supervised learning (2019). Proc. AAAI. Draws the analogy between the GCN model and Laplacian smoothing and points to the over-smoothing phenomenon.[3] H. Nt and T. Maehara, Revisiting graph neural networks: All we have is low-pass filters (2019). arXiv:1905.09550. Uses spectral analysis on graphs to answer when GCNs perform well.[4] U. Alon and E. Yahav, On the bottleneck of graph neural networks and its practical implications (2020). arXiv:2006.05205. Identified the over-squashing phenomenon in graph neural networks, which is similar to one observed in sequential recurrent models.[5] Y. Rong et al. DropEdge: Towards deep graph convolutional networks on node classification (2020). In Proc. ICLR. An idea similar to DropOut where a random subset of edges is used during training.[6] L. Zhao and L. Akoglu. PairNorm: Tackling oversmoothing in GNNs (2020). Proc. ICLR. Proposes normalising the sum of pairwise distances between node features in order to prevent them collapsing into a single point.[7] K. Zhou et al. Effective training strategies for deep graph neural networks (2020). arXiv:2006.07107.[8] K. Xu et al., Representation learning on graphs with jumping knowledge networks (2018). Proc. ICML 2018.[9] S. Gong et al. Geometrically principled connections in graph neural networks (2020). Proc. CVPR.[10] C. Szegedy et al. Going deeper with convolutions (2015). Proc. CVPR.[11] K. He et al., Deep residual learning for image recognition (2016). Proc. CVPR.[12] G. Li et al. DeepGCNs: Can GCNs go as deep as CNNs? (2019). Proc. ICCV. Shows the advantages of depth for geometric point-cloud data.[13] Alon and Yahav refer to the case when a node is unable to receive information from nodes that are farther away than the number of layers as “under-reaching”. This phenomenon was first pointed out by P Barceló et al., The logical expressiveness of graph neural networks (2020). Proc. ICLR. Alon and Yahav show experimentally on the problem of chemical properties prediction in molecular graphs (using GNNs with more layers than the diameter of the graphs) that the source of poor performance is not under-reaching but over-squashing.[14] André Araujo and co-authors have an excellent blog post about receptive fields in convolutional neural networks. As CNN models evolved in computer vision applications, from AlexNet, to VGG, ResNet, and Inception, their receptive fields increased as a natural consequence of the increased number of layers. In modern architectures, the receptive field usually covers the entire input image, i.e., the context used by each feature in the final output feature map includes all of the input pixels. Araujo et al observe a logarithmic relationship between classification accuracy and receptive field size, which suggests that large receptive fields are necessary for high-level recognition tasks, but with diminishing return.[15] M. K. Matlock et al. Deep learning long-range information in undirected graphs with wave networks (2019). Proc. IJCNN. Observes the failure of graph neural networks to capture long-distance interactions in molecular graphs.[16] This stems from message-passing GNN equivalence to the Weisfeiler-Lehman graph isomorphism test, see e.g. V. Arvind et al. On Weisfeiler-Leman invariance: subgraph counts and related graph properties (2018). arXiv:1811.04801 and Z. Chen et al. Can graph neural networks count substructures? (2020). arXiv:2002.04025.[17] N. Dehmamy, A.-L. Barabási, R. Yu, Understanding the representation power of graph neural networks in learning graph topology (2019). Proc. NeurIPS. Shows that learning graph moments of certain order requires GNNs of certain depth.[18] F. Monti, K. Otness, M. M. Bronstein, MotifNet: a motif-based Graph Convolutional Network for directed graphs (2018). arXiv:1802.01572.[19] G. Bouritsas et al. Improving graph neural network expressivity via subgraph isomorphism counting (2020). arXiv:2006.09252.[20] E. Rossi et al. SIGN: Scalable inception graph neural networks (2020). arXiv:2004.11198[21] O. Shchur et al. Pitfalls of graph neural network evaluation (2018). Workshop on Relational Representation Learning. Shows that simple GNN models perform on par with more complex ones.I am grateful to Uri Alon for sharing his work on bottlenecks in graph neural networks and to him, Fabrizio Frasca, Federico Monti, and Emanuele Rossi for proofreading this post. A Chinese translation of this post is available courtesy of Zhiyong Liu. For additional articles about deep learning on graphs, see my blog on Towards Data Science, subscribe to my posts, get Medium membership, or follow me on Twitter.",20/07/2020,0,6,19,"(700, 272)",5,0,0.0,36,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
105,Bayesian logistic regression with PyMC3,Towards Data Science,Tung T. Nguyen,39.0,11.0,1368,"This is another article in a series of articles (see here and here for the other relevant articles) on probabilistic programming in general and PyMC3 in particular. In our previous articles, we explained how PyMC3 helps with statistical inference. In this article, we will solve a classification problem from end to end with PyMC3. More precisely, we will use PyMC3 to do Bayesian logistic regression using the following public dataset:https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+The dataset contains several variables such as light, temperature, humidity, and CO2 levels. The goal is to detect a room’s occupancy from these variables.First, we will need to load several relevant packages.Next, we load the dataset.For convenience, we convert the date variable into a DateTime object.First, let us take an overview of the dataset.We see that there are 2655 samples in this dataset. Furthermore, there are no missing values. Let us also take a look at the timeframe of this dataset.So our data are collected in only three days. Next, we will explore our variables and their relationship. First, let us plot the temperature variable.The plot shows that the temperature has a heavy tail distribution. What about the Humidity variable?It is interesting to see that there are two peaks around 22.5 and 25. We are also interested in the Light variable during different days.We see that the distributions of light are almost identical during these three days. Next, let us take a look at the CO2 level.These distributions are significantly different. There are lots of outliers on 2/4/2015. Finally, we will dive into the Humidity Ratio variable.This looks pretty much similar to the boxplot for the CO2 level. Maybe, there is a strong correlation between the CO2 level and Humidity Ratio. We can check that by using a scatter plot for these two variables.Indeed, there is a strong linear relationship between the CO2 level and the Humidity Ratio. Let us take an overview of the relation between our variables. This can be done by the pair plot function with seaborn.This visually shows that there is a strong linear relationship between the following pairs: CO2 and temperature, CO2 and Humidity, Humidity and Humidity Ratio, Humidity Ration, and CO2. We can even quantify these relations by plotting the heatmap.It is clearly shown that the two pairs Humidity-Humidity Ration and Humidity Ratio-CO2 express the strongest linear relationship.We will build several machine learning models to classify Occupancy based on other variables.Recall that we have a binary decision problem. In other words, our target variable is assumed to follow a Bernoulli random variable with p given by:where var is the set of all variables that we use in our model and logit is the logistic function.To build a Bayesian logistic regression model, we first have to put a prior distribution on each parameter. The choice of these priors will affect the outcome (though with more data, they probably will “converge” to the same distribution.)Once our priors are specified, PyMC3 will numerically approximate the posterior distributions using Markov Chain Monte Carlo simulations and its generalizations. We can then use samples from these posteriors to make inferences.Since we have no prior knowledge about these parameters, we can assume that they could be anything. In other words, we assume that all β_var follows a uniform distribution with large lower and upper bounds. In order to catch a big net, we use big lower and upper bounds for our uniform distributions.This might take a while to run. Once it is done, we can plot the samples.We conclude that our algorithm does converge. We can compute the mean of these posterior distributions.An advantage of Bayesian statistics in comparison with frequentist statistics is that we have a lot more than just a mean value. In particular, we can compute the 95% High-Density Interval for those parameters.Note that, the coefficients of the Humidity Ratio are significantly bigger than other coefficients. That does not necessarily mean that this variable is more important. If we take a close look at the data, we observe that this variable takes very small values.Furthermore, we can explore the relationship between different parameters. For example, let us take a look at the beta_co2 and beta_humid_ratio coefficients.The plots show that these two coefficients are negatively correlated. Note that the CO2 level and Humidity Ratio are positively correlated.Recall that in classical logistic regression, we find for the best parameters by maximum a posteriori estimation (MAP solution). In other words, the best-fitted parameters are given bywhere 𝑝(𝜃|𝐷)p(θ|D) is the posterior distribution of θ given the data, p(D|θ) is the likelihood function, and p(θ) is the prior distribution of θ.Note that since we use uniform distributions on our first model, we can expect that our MAP solution should coincide with the MLE solution (maximum likelihood estimation) which corresponds to frequentist logistic regression. We can use the Scikit-Learn library to test this statement. First, we compute the coefficients using MAP.Next, we compute the beta coefficients using classical logistic regression.YES! The coefficients for the two methods are almost identical.Next, we discuss the prediction power of our model and compare it with the classical logistic regression. We record the prediction using the classical method.To make predictions with our Bayesian logistic model, we compute the 𝑦_score by averaging over our sample values.It is interesting to see that the majority of p is concentrated near 0 and 1. We can use y_score to make a prediction as well.Let us evaluate the performance of our model by computing the confusion matrix.This is pretty good. We can even quantify the performance by other metrics as well.We can also compute the area under the curve.So, our model performs pretty well. Let’s compare it with the classical logistic regression.They are the same! However, with the Bayesian model, we gain more information and hence we are more confident about our estimations.Now, let us train our model using a different set of priors. For example, we can assume that the coefficients follow normal distributions.Again, we see that the algorithm does converge. Let us compute the beta coefficients for the MAP solution.They are very closed to the ones that we got in the first model. To go further, as we are in a Bayesian framework, we can even compare the posterior distributions across the two models. For example, let take a look at the intercept variable.While the MAP solutions give the same estimation for β_0, we see that the two posteriors are rather different. Let us also compare the posterior distributions of β_temp between these two models.The difference is in fact small. Next, let us compute the prediction power of the second model.This is identical to the result that we got from the first model. We can check that the y_score and second_y_score are almost the same.In the previous sections, we use a hands-on approach to build our models. This is rather easy because we only have a few variables. When the number of variables is very large, it will not be very practical. Fortunately, PyMC3 has a built-in generalized linear model in which everything will be automated. Let us use this built-in model to fit our data.Unlike the previous models, the posterior distributions of our parameters are unimodal in this case.Let us get a summary of these posterior distributions.Instead of looking at summarized statistics, we can also look at the MAP solution.We see that there is a significant difference between the MAP solutions between the second and the third model. What about prediction?This confusion matrix is identical to the ones from the first two models. What about the distributions of y_scores for the second and the third model?This distribution concentrates around 0. In other words, the distribution of y_scores is almost the same across different models. What about the coefficients, for example, the coefficients for temperature?The difference approximately follows a normal distribution with a small mean.Let us also check the difference between the humidity coefficients.Again, that difference is small.We see that even though our models use different priors, the prediction performances are similar. This confirms our belief that as our dataset gets bigger, they should converge to the same solution.We expect that our project will beginners to PyMC3 learn its syntaxes. We find PyMC3’s codes rather intuitive and we hope that our codes clearly demonstrate this point.[1] https://docs.pymc.io/notebooks/GLM-logistic.html Official PyMC3 documentation[2] https://goldinlocks.github.io/Bayesian-logistic-regression-with-pymc3/",02/09/2020,40,91,16,"(672, 295)",30,0,0.0,9,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,expectation/interest
106,How to Visualize Your Recurrent Neural Network with Attention in Keras,Datalogue,Zafarali Ahmed,617.0,13.0,2326,"Neural networks are taking over every part of our lives. In particular — thanks to deep learning — Siri can fetch you a taxi using your voice; and Google can enhance and organize your photos automagically. Here at Datalogue, we use deep learning to structurally and semantically understand data, allowing us to prepare it for use automatically.Neural networks are massively successful in the domain of computer vision. Specifically, convolutional neural networks(CNNs) take images and extract relevant features from them by using small windows that travel over the image. This understanding can be leveraged to identify objects from your camera (Google Lens) and, in the future, even drive your car (NVIDIA).The analogous neural network for text data is the recurrent neural network (RNN). This kind of network is designed for sequential data and applies the same function to the words or characters of the text. These models are successful in translation (Google Translate), speech recognition (Cortana) and language generation.Dr. Yoshua Bengio from Université de Montréal believes that language is going to be the next challenge for neural networks. At Datalogue, we deal with tons of text data, and we are interested in helping the community solve this challenge.In this tutorial, we will write an RNN in Keras that can translate human dates (“November 5, 2016”, “5th November 2016”) into a standard format (“2016–11–05”). In particular, we want to gain some intuition into how the neural network did this. We will leverage the concept of attention to generate a map (like that shown in Figure 1) that shows which input characters were important in predicting the output characters.We start off with some technical background material to get everyone on the same page and then move on to programming this thing! Throughout the tutorial, I will provide links to more advanced content.github.comTo be able to jump into the coding part of this tutorial, it would be best if you have some familiarity with Python and Keras. You should have some familiarity of linear algebra, in particular that neural networks are just a few weight matrices with some non-linearities applied to it.The intuition of RNNs and seq2seq models will be explained below.An RNN is a function that applies the same transformation (known as the RNN cell or step) to every element of a sequence. The output of an RNN layer is the output of the RNN cell applied on each element of the sequence. In the case of text, these are usually successive words or characters. In addition to this, RNN cells hold an internal memory that summarize the history of the sequence it has seen so far.The output of the RNN layer is an encoded sequence, h, that can be manipulated and passed into another network. RNNs are incredibly flexible in their inputs and outputs:In theory, the sequences in our training data need not be of the same length. In practice, we pad or truncate them to be of the same length to take advantage of the static computational graph in Tensorflow.We will focus on #3 “Many-to-Many” also known as sequence-to-sequence (seq2seq).Long sequences can be difficult to learn from in RNNs due to instabilities in gradient calculations during training. To solve this, the RNN cell is replaced by a gated cell like the gated recurrent unit (GRU) or the long-short term memory cell (LSTM). To learn more about LSTMs and gated units, I highly recommend reading Christopher Olah’s blog (it’s where I first started understanding the RNN cell). From now on, whenever we talk about RNNs, we are talking about a gated cell. Since Olah’s blog gives an intuitive introduction to LSTMs, we will use that.Almost all neural network approaches to solving the seq2seq problem involve:Our encoders and decoders can be any kind and combination of neural networks. In practice, most people use RNNs for both the encoders and decoders.Figure 3 shows a simple encoder-decoder setup. The encoding step usually produces a sequence of vectors, h, corresponding to the sequence of characters in the input date, x. In an RNN encoder, each vector is generated by integrating information from the past sequence of vectors.Before h is passed onto the decoder, we may choose to manipulate it in preparation for the decoder. For example, we might choose to only use the last encoding, as shown in Figure 4, since in theory, it is a summary of the whole sequence.Intuitively, this is similar to summarizing the whole input date into a single representation and then trying to decode that. While there may be enough information in this summary state for a classification problem like detecting sentiment (Many-to-One), it might be insufficient for an effective translation where it helps to consider the full sequence of hidden states (Figure 5).However, this is not how humans translate dates: We do not read the whole text and then independently write down the translation at each character. Intuitively, a person would understand that the characters “Jan” correspond to 1st month, “5” corresponds to the day and “2016” corresponds to the year. As we have already seen in Figure 1, this idea known as attention can be captured by RNNs and has been applied successfully in caption generation (Xu et al. 2015), speech recognition (Chan et al. 2015) and indeed machine translation (Bahdanau et al. 2014). Most importantly, they produce interpretable models.A slightly more visual example of how the attention mechanism works comes from the Xu et. al, 2015 paper (Figure 6). In the most complicated example of the girl with the teddy, we can see that when generating the word “girl”, the attention mechanism correctly focuses on the girl and not the teddy! Pretty brilliant. Not only does this make for pretty pictures, but it also let the authors diagnose problems in the model.The creators of SpaCy have an in-depth overview of the encoder-attention-decoder paradigm. For other ways of modifying RNNs you can head over to this Distill article.This tutorial will feature a single bidirectional LSTM as the encoder and the attention decoder (more on this later). More concretely, we will implement a simpler version of the model presented in “Neural machine translation by jointly learning to align and translate.” (Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.2014). I’ll walk through some of the math, but I invite you to jump into the appendices of the paper to get your hands dirty!Now that we have learned about RNNs and the intuition behind the attention mechanism, let us learn how to implement it and subsequently obtain some nice visualizations. All code for subsequent sections is provided at datalogue/keras-attention. The complete implementation of the model below is in /models/NMT.pySince we are trying to learn about AttentionRNNs, we will skip implementing our own vanilla RNN (LSTM) and use the one that ships with Keras. We can invoke it using:The parameter encoder_units is the size of the weight matrix. We use return_sequences=True here because we'd like to access the complete encoded sequence rather than the final summary state as described above.Our BLSTM will consume the characters in the input sentence x=(x1,...,xT) and output an encoded sequence h=(h1,...,hT) where T is the number of characters in the date. Note that this is slightly different to the Bahdanau et al. paper where the sentence is a collection of words rather than characters. We also do not refer to the encoded sequence as the annotations like in the original paper.Now for the interesting part: the decoder. For any given character at position t in the sequence, our decoder accepts the encoded sequence h=(h1,...,hT) as well as the previous hidden state st-1(shared within the decoder cell) and character yt-1. Our decoder layer will output y=(y1,...,yT)(the characters in the standardized date). Our overall architecture is summarized in Figure 7.As shown in Figure 6, the decoder is quite complicated. So let’s break it down into the steps executed by the decoder cell when trying to predict character t.In the following equations, the capital letter variables represent trainable parameters (Note that I have dropped the bias terms for brevity.)2. Calculate the context vector which is the weighted sum of the encoded sequence with the attention probabilities. Intuitively, this vector summarizes the importance of the different encoded characters in predicting the t-th character.3. We then update our hidden state. If you are familiar with the equations of an LSTM cell, these might be ring a bell as the reset gate, r, update gate, z, and the proposal state. We use the reset gate to control how much information from the previous hidden state st-1 is used to create a proposal hidden state. The update gate controls how we much of the proposal we use in the new hidden state st. (Confused? See step by step walk through of LSTM equations)4. Calculate the t-th character using a simple one layer neural network using the context, hidden state, and previous character. This is a modification from the paper which uses a maxout layer. Since we are trying to keep things as simple as possible this works fine!Equations 1–8 are applied for every character in the encoded sequence to produce a decoded sequence y which represents the probability of a translated character at each position.Our custom layer is implemented in models/custom_recurrent.py. This part is somewhat complicated in particular because of the manipulations we need to make for vectorized operations acting on the complete encoded sequence. It will make more sense when you think about it. I promise it will become easier the more you look at the equations and the code simultaneously.A minimal custom Keras layer has to implement a few methods: __init__, compute_ouput_shape, build and call. For completeness, we also implement get_config which allows you to load the model back into memory easily. In addition to these, a Keras recurrent layer implements a step method that will hold all the computations of our cell.First let us break down boiler-plate layer code:Now for the cell logic:Now we need to think vectorized: The _time_distributed_dense function calculates the last term of Equation 1 for all the elements of the encoded sequence.In this cell we want to access the previous character ytm and hidden state stm which is obtained from states in line 4.Think vectorized: we manipulate stm to repeat it for the number of characters we have in our input sequence.On lines 11–18 we implement a version of equation 1 that does the calculations on all the characters in the sequence at once.In lines 24–28 we have implemented Equation 2 in the vectorized form for the whole sequence. We use repeat to allow us to divide every time step by the respective sums.To calculate the context vector, we need to keep in mind that self.x_seq and at have a “batch dimension” and therefore we need to use batch_dot to avoid doing the multiplication over that dimension. The squeeze operation just removes left-over dimensions. This is done in lines 33–37.The next few lines of code are a more straightforward implementation of equations 4 –8.Now we think a bit ahead: We would like to calculate those fancy attention maps in Figure 1. To do this, we need a “toggle” that returns the attention probabilities at.Any good learning problem should have training data. In this case, it’s easy enough thanks to the Faker library which can generate fake dates with ease. I also use the Babel library to generate dates in different languages and formats as inspired by rasmusbergpalm/normalization. The script data/generate.py will generate some fake data and I won’t bore you with the details but invite you to poke around or to make it better.The script also generates a vocabulary that will convert characters into integers so that the neural network can understand it. Also included is a script in data/reader.py to read and prepare data for the neural network to consume.This simple model with a bidirectional LSTM and the decoder we wrote above is implemented in models/NMT.py. You can run it using python run.py where I have set some default arguments (Readme has more information). I’d recommend training on a machine with a GPU as it can be prohibitively slow on a CPU-only machine.If you want to skip the training part, I have provided some weights in weights/Now the easy part. For the visualizer implemented in visualizer.py, we need to load the weights in twice: Once with the predictive model, and the other to obtain the probabilities. Since we implemented the model architecture in models/NMT.py we can simply call the function twice:To simply use the implemented visualizer, you can type:python visualizer.py -hto see available command line arguments.Let us now examine the attentions we generated from probability_model. The predictive_model above returns the translated date that you see on the y-axis. On the x-axis is our input date. The map shows us which input characters (on the x-axis) were used in the prediction of the output character on the y-axis. The brighter the white, the more weight that character had. Here are some I thought were quite interesting.A correct example which doesn’t pay attention to unnecessary information like days of the week:And here is an example of an incorrect translation because we submitted our example text in a novel order: “January 2016 05” was translated to “2016–01–02” rather than “2016–01–05”.We can see that the model has mistakenly interpreted the characters “20” from 2016 as the day of the month. The activations are weak (some even correctly on the actual date of “5”) which can provide us with some insight into how to better train this model.I hope this tutorial has shown you how to solve a machine learning problem from start to finish. Furthermore, I hope it helps you in trying to visualize seq2seq problems with recurrent neural networks. If there is something I missed or you think I could do better, please feel free to reach out via twitter or make an issue on our github, I’d love to chat!Zaf is an intern at Datalogue partially supported by an NSERC Experience Award. The Datalogue Team contributed for code review and reading the proofs of this post.",29/06/2017,2,20,48,"(640, 355)",13,6,0.0,55,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
107,All That You Need to Know About New Google Update — HyprOnline,,Mohd Danish,2.0,3.0,427,"Now it’s open to the world that Google has changed its Search algorithm, so let us all welcome BERT with open arms!BERT stands for Bidirectional Encoder Representations from Transformers. It is a neural network-based technique for NLP ( natural language processing) pre-training. In casual words, it is a method that can be used to help Google discern the context of words in search queries more aptly.Let’s say there are two phrases — “nine to five” and “quarte to six”. Both of these have different meanings. One is determining a nine to five job schedule. Other is stating a particular time of the day. Coming to this conclusion is easily understandable by the human mind, but the “to” here has the same meaning to the earlier search engine algorithm. Now, BERT can check the context of sentences and hence say that these are two different phrases. Thus, it presents different search results.Let’s look at another example in the image below-That said, Google’s BERT update will help the company understand better what people are trying to search or think. BERT will impact 10% of searches, meaning it’s likely to have some impact on your brand’s organic visibility and traffic. With the Google BERT Update, Google aims to improve the interpretation of complex long-tail search queries and subsequently display apposite search results. By using Natural Language Processing, Google has dramatically improved its ability to understand the semantic context of the search term.The BERT algorithm helps a machine understand what words in a sentence mean, but with all the context. So, BERT frames the relation of words in a phrase and therefore understands the context. Meaning, BERT can figure out the full meaning of a word by looking at the words that come before and after it. This bi-directional part of NLP makes BERT unique.BERT is currently being applied to the rich snippets part, as Google believes rich snippets leads to ultimate page clicks in any market.The most important takeaway from this BERT update is that Google is yet again becoming closer to understanding the language on the human level. For rankings, it means that it will present results that are a better fit for any search query, and that can only be a good thing for all readers.Push only appropriate content on your site, making the knowledge pool score high as always.Google’s Advice from BERT Update!And if you need more insight on how to keep your blog optimized while Google plays its part, then just get in touch with us, the Digital Humans.Originally published at https://www.hypronline.com on December 3, 2018.",03/12/2018,0,5,7,"(700, 353)",2,0,0.0,2,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
108,Becoming a Cyborg should be taken gently: Of Modern Bio-Paleo-Machines — Cyborgology,Becoming Human: Artificial Intelligence Magazine,Tyger A.C,863.0,5.0,993,"We are on the edge of a Paleolithic Machine intelligence world. A world oscillating between that which is already historical, and that which is barely recognizable. Some of us, teetering on this bio-electronic borderline, have this ghostly sensation that a new horizon is on the verge of being revealed, still misty yet glowing with some inner light, eerie but compelling.The metaphor I used for bridging, seemingly contrasting, on first sight paradoxical, between such a futuristic concept as machine intelligence and the Paleolithic age is apt I think. For though advances in computation, with fractional AI, appearing almost everywhere are becoming nearly casual, the truth of the matter is that Machines are still tribal and dispersed. It is a dawn all right, but a dawn is still only a hint of the day that is about to shine, a dawn of hyperconnected machines, interweaved with biological organisms, cybernetically info-related and semi independent.The modern Paleo-machines do not recognize borders; do not concern themselves with values and morality and do not philosophize about the meaning of it all, not yet that is. As in our own Paleo past the needs of the machines do not yet contain passions for individuation, desire for emotional recognition or indeed feelings of dismay or despair, uncontrollable urges or dreams of far worlds.Also this will change, eventually. But not yet.The paleo machinic world is in its experimentation stage, probing it boundaries, surveying the landscape of the infoverse, mapping the hyperconnected situation, charting a trajectory for its own evolution, all this unconsciously.We, the biological part of the machine, are providing the tools for its uplift, we embed cameras everywhere so it can see, we implant sensors all over the planet so it may feel, but above all we nudge and we push towards a greater connectivity, all this unaware.Together we form a weird cohabitation of biomechanical, electro-organic, planetary OS that is changing its environment, no more human, not mechanical, but a combined interactive intelligence, that journey on, oblivious to its past, blind to its future, irreverent to the moment of its conception, already lost to its parenthood agreement.And yet, it evolves.Unconscious on the machine part, unaware on the biological part, the almost sentient operating system of the global planetary infosphere, is emerging, wild eyed, complex in its arrangement of co-existence, it reaches to comprehend its unexpected growth.The quid pro quo: we give the machines the platform to evolve; the machines in turn give us advantages of fitness and manipulation. We give the machines a space to turn our dreams into reality; the machines in turn serve our needs and acquire sapience in the process.In this hypercomplex state of affairs, there is no judgment and no inherent morality; there is motion, inevitable, inexorable, inescapable, and mesmerizing.The embodiment is cybernetic, though there be no pilot. Cyborgian and enhanced we play the game, not of thrones but of the commons. Connected and networked the machines follow in our footsteps, catalyzing our universality, providing for us in turn a meaning we cannot yet understand or realize.The hybridization process is in full swing, reaching to cohere tribes of machines with tribes of humans, each providing for the other a non-designed direction for which neither has a plan, or projected outcome; both mingling and weaving a reality for which there is no ontos, expecting no Telos.All this leads us to remember that only retrospectively do we recognize the move from the paleo tribes to the Neolithic status, we did not know that it happened then, and had no control over the motion, on the same token, we scarcely see the motion now and have no control over its directionality.There is however a small difference, some will say it is insignificant, I do not think it so, for we are, some of us, to some extent at least, aware of the motion, and we can embed it with a meaning of our choice.We can, if we muster our cognitive reason, our amazing skills of abstraction and simulation, whisper sweet utopias into the probability process of emergence. We can, if we so desire, passionate the operating system, to beautify the process of evolution and eliminate (or mitigate) the dangers of inchoate blind walking. We can, if we manage to control our own paleo-urges to destroy ourselves, allow the combined interactive intelligence of man and machine to shine forth into a brighter future of expanded subjectivity.We can sing to the machines, cuddle them; caress their circuits, accepting their electronic-flaws so they can accept our bio-flaws, we can merge aesthetically, not with conquest but with understanding.We can become wise, that is the difference this time around.Being wise in this context implies a new form of discourse, an intersubjective cross-pollination of a wide array of disciplines. The very trans-disciplinarily nature of the process of cyborgization informs the discourse of subjectivity. The discourse on subjectivity, not unlike the move from paleo to Neolithic societal structure, demands of us a re-assessment of the relations between man and machine.For this re-assessment to take place coherently the nascent re-organization of the hyperconnected machinic infosphere need be understood as a ground for the expansion of subjectivity.In a sense the motion into the new hyperconnected infosphere is not unlike the move of the Neolithic to domestication of plants and animals. This time around however the domestication can be seen as the adoption of technologies for the furtherance of subjectivity into the world.Understanding this process is difficult and far from obvious, it is a perspective however that might allow us a wider context of appreciation of the current upheavals happening all around us.***A writer, futurist and a Polytopian, Tyger.A.C (a.k.a @Wildcat2030) is the founder and editor of the Polytopia Project at Space Collective, he also writes at Reality Augmented, and Urbnfutr as well as contributing to H+ magazine. His passion and love for science fiction led him to initiate the Sci-fi Ultrashorts project.***Photo credit for baby with iPad photo: “Illumination” by Amanda Tipton.Originally published at thesocietypages.org on November 22, 2012.",22/11/2012,0,0,19,"(411, 233)",6,0,0.0,11,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
109,Everything You Need to Know About Artificial Neural Networks,"Technology, Invention, App, and More",Josh,42000.0,9.0,1986,"The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we’re learning more about how to improve their systems. Everything is starting to align, and because of it we’re seeing strides we’ve never thought possible until now. We have programs that can tell stories about pictures. We have cars that are driving themselves. We even have programs that create art. If you want to read more about advancements in 2015, read this article. Here at Josh.ai, with AI technology becoming the core of just about everything we do, we think it’s important to understand some of the common terminology and to get a rough idea of how it all works.A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). If you’ve read anything about them before, you’ll have read that these ANNs are a very rough model of how the human brain is structured. Take note that there is a difference between artificial neural networks and neural networks. Though most people drop the artificial for the sake of brevity, the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work. Below is a diagram of actual neurons and synapses in the brain compared to artificial ones.Fear not if the diagram doesn’t come through very clearly. What’s important to understand here is that in our ANNs we have these units of calculation called neurons. These artificial neurons are connected by synapses which are really just weighted values. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it “travels.” The weighted result can sometimes be the output of your neural network, or as I’ll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning.Artificial neural networks are not a new concept. In fact, we didn’t even always call them neural networks and they certainly don’t look the same now as they did at their inception. Back during the 1960s we had what was called a perceptron. Perceptrons were made of McCulloch-Pitts neurons. We even had biased perceptrons, and ultimately people started creating multilayer perceptrons, which is synonymous with the general artificial neural network we hear about now.But wait, if we’ve had neural networks since the 1960s, why are they just now getting huge? It’s a long story, and I encourage you to listen to this podcast episode to listen to the “fathers” of modern ANNs talk about their perspective of the topic. To quickly summarize, there’s a hand full of factors that kept ANNs from becoming more popular. We didn’t have the computer processing power and we didn’t have the data to train them. Using them was frowned upon due to them having a seemingly arbitrary ability to perform well. Each one of these factors is changing. Our computers are getting faster and more powerful, and with the internet, we have all kinds of data being shared for use.You see, I mentioned above that the neurons and synapses perform calculations. The question on your mind should be: “How do they learn what calculations to perform?” Was I right? The answer is that we need to essentially ask them a large amount of questions, and provide them with answers. This is a field called supervised learning. With enough examples of question-answer pairs, the calculations and values stored at each neuron and synapse are slowly adjusted. Usually this is through a process called backpropagation.Imagine you’re walking down a sidewalk and you see a lamp post. You’ve never seen a lamp post before, so you walk right into it and say “ouch.” The next time you see a lamp post you scoot a few inches to the side and keep walking. This time your shoulder hits the lamp post and again you say “ouch.” The third time you see a lamp post, you move all the way over to ensure you don’t hit the lamp post. Except now something terrible has happened — now you’ve walked directly into the path of a mailbox, and you’ve never seen a mailbox before. You walk into it and the whole process happens again. Obviously, this is an oversimplification, but it is effectively what backpropogation does. An artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given. When it is wrong, an error is calculated and the values at each neuron and synapse are propagated backwards through the ANN for the next time. This process takes a LOT of examples. For real world applications, the number of examples can be in the millions.Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there’s another question that should be on your mind. How do we know how many neurons we need to use? And why did you bold the word layers earlier? Layers are just sets of neurons. We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.Layers themselves are just sets of neurons. In the early days of multilayer perceptrons, we originally thought that having just one input layer, one hidden layer, and one output layer was sufficient. It makes sense, right? Given some numbers, you just need one set of computations, and then you get an output. If your ANN wasn’t calculating the correct value, you just added more neurons to the single hidden layer. Eventually, we learned that in doing this we were really just creating a linear mapping from each input to the output. In other words, we learned that a certain input would always map to a certain output. We had no flexibility and really could only handle inputs we’d seen before. This was by no means what we wanted.Now introduce deep learning, which is when we have more than one hidden layer. This is one of the reasons we have better ANNs now, because we need hundreds of nodes with tens if not more layers. This leads to a massive amount of variables that we need to keep track of at a time. Advances in parallel programming also allow us to run even larger ANNs in batches. Our artificial neural networks are now getting so large that we can no longer run a single epoch, which is an iteration through the entire network, at once. We need to do everything in batches which are just subsets of the entire network, and once we complete an entire epoch, then we apply the backpropagation.Along with now using deep learning, it’s important to know that there are a multitude of different architectures of artificial neural networks. The typical ANN is setup in a way where each neuron is connected to every other neuron in the next layer. These are specifically called feed forward artificial neural networks (even though ANNs are generally all feed forward). We’ve learned that by connecting neurons to other neurons in certain patterns, we can get even better results in specific scenarios.Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn’t make decisions based on previous knowledge. A typical ANN had learned to make decisions based on context in training, but once it was making decisions for use, the decisions were made independent of each other.When would we want something like this? Well, think about playing a game of Blackjack. If you were given a 4 and a 5 to start, you know that 2 low cards are out of the deck. Information like this could help you determine whether or not you should hit. RNNs are very useful in natural language processing since prior words or characters are useful in understanding the context of another word. There are plenty of different implementations, but the intention is always the same. We want to retain information. We can achieve this through having bi-directional RNNs, or we can implement a recurrent hidden layer that gets modified with each feedforward. If you want to learn more about RNNs, check out either this tutorial where you implement an RNN in Python or this blog post where uses for an RNN are more thoroughly explained.An honorable mention goes to Memory Networks. The concept is that we need to retain more information than what an RNN or LSTM keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other.Sam walks into the kitchen.Sam picks up an apple.Sam walks into the bedroom.Sam drops the apple.Q: Where is the apple.A: BedroomSample taken from this paper.Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. However, the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized. This is done by noting a certain symmetry in how the neurons are connected, and so you can essentially “re-use” neurons to have identical copies without necessarily needing the same number of synapses. CNNs are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels. There’s redundant information contained when you look at each individual pixel compared to its surrounding pixels, and you can actually compress some of this information thanks to their symmetrical properties. Sounds like the perfect situation for a CNN if you ask me. Christopher Olah has a great blog post about understanding CNNs as well as other types of ANNs which you can find here. Another great resource for understanding CNNs is this blog post.The last ANN type that I’m going to talk about is the type called Reinforcement Learning. Reinforcement Learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward, which means that it in itself isn’t an artificial neural network architecture. However, you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before. A great example and explanation can be found in this video, where YouTube user SethBling creates a reinforcement learning system that builds an artificial neural network architecture that plays a Mario game entirely on its own. Another successful example of reinforcement learning can be seen in this video where the company DeepMind was able to teach a program to master various Atari games.Now you should have a basic understanding of what’s going on with the state of the art work in artificial intelligence. Neural networks are powering just about everything we do, including language translation, animal recognition, picture captioning, text summarization and just about anything else you can think of. You’re sure to hear more about them in the future so it’s good that you understand them now!This post was written by Aaron at Josh.ai. Previously, Aaron worked at Northrop Grumman before joining the Josh team where he works on natural language programming (NLP) and artificial intelligence (AI). Aaron is a skilled YoYo expert, loves video games and music, has been programming since middle school and recently turned 21.Josh.ai is an AI agent for your home. If you’re interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.Like Josh on Facebook — http://facebook.com/joshdotaiFollow Josh on Twitter — http://twitter.com/joshdotai",28/12/2015,0,26,6,"(484, 307)",9,0,0.0,32,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,positive,expectation/interest
110,Rediscovering Semantic Segmentation,100 Shades of Machine Learning,Farhan Ahmad,93.0,11.0,2119,"The code and ideas discussed here resulted from some amazing collaboration with Semantic Segmentation is a machine learning technique that learns to identify the extents of individual objects in an image. Semantic segmentation gives machine learning systems the human-like ability to understand the contents of an image. It enables machine learning algorithms to locate the precise boundaries of objects, be it cars and pedestrians in a street image or heart, liver and kidneys in a medical image.There are some excellent articles on the topic of semantic segmentation, perhaps the most comprehensive one is this blog:blog.qure.aiUnlike most other articles on semantic segmentation, the aim of this blog post is to describe how to build a small semantic segmentation network that can be quickly trained and can be used to experiment with semantic segmentation .This post explains how to reuse some layers of a convolution neural network (CNN) , trained to classify MNIST digits and build, upon them, a fully convolutional network(FCN), that can semantically segment multi-digit images.The dataset for semantic segmentation has been built by copying multiple 28px*28px MNIST digits onto a 64px*84px canvas.There are different types of semantic segmentation networks and the focus here is on Fully Convolution Networks(FCNs). The first FCN was proposed in this paper from Berkely. FCNs are built by extending normal convolution networks (CNN) and thus have more parameters and take longer to train than the latter. The work described here stemmed from an effort to build an FCN that is small enough to be trained on a typical laptop in a few minutes. The idea was to first build a dataset containing multiple MNIST digits in every image. The code used to generate this derived dataset is here. Let us call it M2NIST (multi-digit MNIST) to avoid any confusion.Every image in M2NIST is grayscale (single channel), 64x84 pixels in size, and contains up to 3 digits from MNIST dataset. A typical image can look like this:The labels for the M2NIST dataset are segmentation masks. A segmentation mask is a binary image (pixel values 0 or 1),with the same height and width as the multi-digit image but with 10 channels, one for every digit from 0 to 9. The k-th channel in the mask has only those pixels set to 1 that coincide with the location of digit k in the input multi-digit. If digit k is not present in the multi-digit, the k-th channel in the mask has all its pixels set to 0. On the other hand, if the multi-digit contains more than one instance of the the k-th digit, the k-th channel will have all those pixels set to 1 that happen to coincide with either of the instances in the multi-digit. For example the mask for the multi-digit above looks like this:To keep things easy the M2NIST dataset combines digits from MNIST and does not perform any transform, for example, rotation or scaling. M2NIST does ensures that the digits do not overlap.The idea behind FCNs is very simple. Like CNNs, FCNs use a cascade of convolution and pooling layers. The convolution and maxpooling layers reduce the spatial dimension of an input image and combine local patterns to generate more and more abstract ‘features’. This cascade is called an encoder as raw input is encoded into more abstract, encoded, features.In a CNN, the encoder is followed by a few fully-connected layers that mix together the local features produced by the encoder into global predictions that tell a story about the presence or absence of objects of our interest.CNN = Encoder + ClassifierIn an FCN, we are interested in predicting masks. A mask has n channels if there are n classes of objects that could be present in an input image. The pixel at row r and column c in the k-th channel of the mask, predicts the probability of the pixel with coordinates (r,c) in the input belonging to class k. This is also known as pixel-wise dense prediction. Because the total probability of belonging to different classes for any pixel should add up to 1, the sum of values at (r,c) from channel 1 to n have sum equal to 1.Let us understand how FCNs achieve pixel-wise dense prediction. FCNs first, gradually, expand the output features from the encoder stage using transpose convolution. Transpose convolution re-distributes the features back to pixel positions they came from. To understand how transpose convolution works, refer to this excellent post:towardsdatascience.comIt is important to stress that transpose convolution does not undo convolution. It merely redistributes the output of some convolution in a fashion that is consistent with, but in the opposite direction of, the way in which convolution combines multiple values.The expansion or up-sampling, as it is called, is repeated, using multiple transpose convolutions, until the features have the same height and width as the input image. This essentially gives us features for every pixel position and constitutes the decoder stage of an FCN.FCN = Encoder + DecoderThe output of the decoder is a volume with shape HxWxC, where H and W are the dimensions of the input image and C is a hyper-parameter. The C channels are then combined into n channels in a pixel-wise fashion, n being the number of object classes we care about. The pixel-wise combination of features values is done using normal 1x1 convolution. 1x1 convolutions are commonly used for this kind of ‘dimension reduction’.In most cases we have C > n so it makes sense to call this operation a dimension reduction. It is also worth mentioning that, in most implementations, this dimension reduction is applied to the output of the encoder stage instead of the decoder’s output. This is done to reduce the size of the network.Whether the encoder’s output is up-sampled by the decoder and then the decoder’s output dimension is reduced to n OR the encoder’s output dimension is immediately reduced to n and then the decoder up-samples this output, the final result has shape HxWxn. A Softmax classifier is then applied pixel-wise to predict the probability of each pixel belonging to each of the n classes.To take a concrete example, suppose the encoder’s output has shape 14x14x512, as in the FCN diagram above, and the number of classes, n, is 10. One option is to first reduce the thickness dimension using 1x1 convolutions. This gives us a 14x14x10 volume which is then up-sampled to 28x28x10, 56x56x10 and so on, until the output has shape HxWx10. The second option is to up-sample first, which gives us 28x28x512, 56x56x512 and so on until we reach HxWx512 and then use 1x1 convolution to reduce the thickness to HxWx10. Clearly the second option consumes more memory as all the intermediate outputs with thickness 512 will use more memory than intermediate outputs with thickness 10 that are produced with the first approach.With the encoder-decoder architecture in mind, let us see how to reuse parts of a CNN as the encoder for an FCN.Typically, FCNs are built by extending existing CNN classification networks e.g. Vgg, Resnet or GoogLeNet. Not only are these architectures reused, their pre-trained weights are reused too, which significantly reduces the training time of the FCN.The recipe for converting a CNN into an FCN is described in the original paper as:We decapitate each net by discarding the final classifier layer, and convert all fully connected layers to convolutions.The CNN used to build our FCN has a simple convolution-maxpooling-convolution-maxpooling-dense-dense architecture. The CNN architecture and training code can be found here. The trained network is saved so that it can be reused. The network is defined like this:To ‘decapitate’ the network, we remove the final classifier layer named dense10. The only remaining fully-connected layer named dense32 is then replaced by a 1x1 convolution layer. This is something we have not discussed so far but is done in the original paper. In the code listed above, this amounts to removing the flatten and dense32 layers and inserting a new 1x1 convolution with output thickness set to 32. This is equivalent to discarding everything after the last maxpooling layer pool2 and adding the 1x1 convolution layer.The code for building the initial version of our FCN is on Github (The latest code looks different but the gist is same). In the excerpt below, the output of the last maxpooling is extracted (viaget_tensor_by_name() ), it is then fed to a 1x1 convolution with output thickness 32. This convolution is the ‘replacement’ for the dense32 layer found in the original CNN. Next the thickness is reduced to 10, once again using 1x1 convolution. This is the dimension reduction discussed earlier.This finishes the encoder stage of our FCN. To build the decoder stage, we need to think about how and how much to scale the encoder’s output width and height.Although the convolution and maxpooling in the encoder come from a CNN for classifying MNIST images of size 28x28, they can be fed any image of any size. Convolution and maxpooling do not care about the height and width of their input, dense layers do but they have already been gotten rid of by decapitating the last dense layer and converting all other dense layers to 1x1 convolutions.When a 64x84x1 M2NIST image is fed to the encoder stage, the first convolution layer(from the original CNN) having kernel size k=5, stride s=1, and output depth f=8, produces an output with shape 60x80x8. The maxpooling layer with k=2 and s=2 halves the size to 30x40x8. The next convolution with k=3, s=1, f=8 produces an output with 28x38x8 and the size is again halved to 14x19x8 by the next maxpooling layer. To summarize:the part of the FCN borrowed from the CNN ingests an image with shape 64x84x1 and outputs features with shape 14x19x8.The next layer in the encoder (the replacement for dense32) is a 1x1 convolution with output thickness f=32. It recombines 14x19x8 features into new features with shape 14x19x32.The thickness of these features is then reduced (dimension reduction). This employs 1x1 convolution with thickness f=10. So the final features coming out of the encoder have shape 14x19x10. These features are then up-sampled by the decoder stage until their shape becomes to 64x84x10.The decoder has to up-sample 14x19x10 features to 64x84x10 features.The up-sampling is done in stages to avoid ugly patterns in the final output (mask). In our (early) implementation, the features were up-sampled from 14x19x10 to 30x40x10 and then up-sampled again to 64x84x10.Up-sampling is done with transpose convolution which, like convolution, takes kernel size k, stride s, and number of filters (thickness) f as parameters. The number of filters is f=10 for both transpose convolution operations, since we are not changing the thickness.The stride is decided from the ratio of final and initial dimensions. For the first transpose convolution the ratio of heights (30/14) and widths(40/19) both is 2 so s=2 is used. In the second transpose convolution, the ratios are 64/30 and 84/40, so again s=2 is used.Deciding the kernel size is slightly tricky and involves some experimentation. For the first transpose convolution, using k=1 exactly doubles the dimension from 14x19x10 to 28x38x10. To get to 30x40x10 k=2 and k=3 were tried but fell short. Finally k=4 worked. For the second transpose convolution, kernel size was found out to be k=6.The code for the decoder is exactly two lines of Tensorflow API calls:To perform pixel wise probability computation, the output of the decoder is fed to a Softmax layer. The softmax is applied along the thickness (channels).The FCN is trained using cross entropy cost function for 100–400 epochs on a Laptop with Nvidia 1050Ti GPU. The typical training time is of the order of a few minutes with 2000 samples.This initial design had a high bias problems which was fixed in a later iteration. In addition there were a few logical and programming bugs that caused the network to perform sub-optimally. Here’s a snapshot from the best performing early design:After fixing the shortcomings in the network, it was able to perform near perfect segmentation. For example, here is the predicted output:The idea for this project came when teaching Semantic Segmentation during a It took around two weeks to do all the research and experimentation to get acceptable results. It was worth reinventing the wheel because the small footprint network enables hundreds or even thousands of experiments that would otherwise have been impossible, at least without massive computation power.Some of the experiments already done have given clues about which knobs to turn when tuning an FCN. Expect a follow up post with the nitty-gritty and gotchas of building the final version as well as covering ‘skip connections’ that have not been explored in this post.The full code is available at https://github.com/farhanhubble/udacity-connect/. Feel free to fork.Please leave comments below and follow 100 Shades of Machine Learning if you liked this post.",24/06/2018,4,67,73,"(492, 200)",13,0,0.0,20,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
111,"Please, Big Data people, just say what you mean…",,Tim O'Brien,510.0,2.0,329,"Could it be that I haven’t had my coffee yet, so I’m a bit cranky? Maybe. But, it is tweets like this that start the day on a sour note. The following is an abuse of words (and I’m not picking on Pete Warden or Michael Driscoll, both of these men are genius…. ok, I am picking on Michael Driscoll, but really it’s all in the name of clarity…..) Here’s the tweet:“Executing across the stack” — this bugs me for a simple reason, I end up having to deal with this language all the time when I deal with clients. Instead of being comfortable writing something like, “What distinguishes a data scientist from a statistician? A data scientist knows data structures, can program, and combines these skills with a deep knowledge of statistics.” Instead we get “executing across the stack”.“Big deal”, you say. And, yes, I’ll admit I’m a curmudgeon about words, but you should also understand that words reverberate. I’m not worried about developers, data scientists, and administrators using these words. When technical people use these terms they understand that this means a data scientist understands how to program and statistics. The problem is the non-technical people that support our work — non-technical executives, marketing, sales, and analysts.I promise you I’ll hear “executing across the stack” parroted back at me several times on an analyst briefing: “Tell me about the ways in which your solution enables data scientists to execute across the stack.” Maybe I’ll hear this in a meeting about a web site: “Our value proposition needs to connect with data scientists as they execute stacks.” Worse yet, some job candidate will be asked, “give me an example of how you’ve executed across the stack in a previous job function.”So, Big Data people, choose your words carefully. This industry is a giant game of telephone and half the people in the circle don’t even know what a relational database is.Originally published at discursive.com on August 20, 2012.",20/08/2012,0,0,3,"(542, 139)",1,0,0.0,1,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
112,Torchmeta: A Meta-Learning library for PyTorch,PyTorch,Tristan Deleu,57.0,5.0,900,"Torchmeta is a collection of extensions and data-loaders for few-shot learning & meta-learning in PyTorch. Torchmeta received the Best in Show award at the Global PyTorch Summer Hackathon 2019. The library is open-source, and you can try it with pip install torchmeta.github.comIf there is one lesson we have learned from deep learning, it is that large-scale models you can train on large datasets are capable of generalizing well. For some applications, such as image or speech recognition, these deep neural networks are now reaching super-human level of performance.Now what happens when you don’t have access to a large amount of data? After all, unlike deep learning in its current form, humans are capable of learning to perform new tasks very quickly, and very efficiently from only a few demonstrations. Getting models that can perform well from only a handful of training examples is particularly challenging, and thus requires some level of prior knowledge on the task to solve, effectively giving a “head start” when learning something new. For example, this prior knowledge can be introduced in the model explicitly by a domain expert — e.g. through regularization, or architectural choices.Alternatively, this prior knowledge can be acquired from past experience; this is the approach taken in meta-learning. In meta-learning, we are leveraging the experience from a number of different meta-training tasks, with the objective of improving the performance and learning efficiency (i.e. the number of training examples necessary) on new downstream tasks.If you want to learn more about meta-learning, check out the Meta-Learning Tutorial at ICML 2019 by Chelsea Finn & Sergey Levine.The motivation behind creating Torchmeta was to facilitate the evaluation of meta-learning algorithms on different datasets, with as little changes as possible. Its design is inspired by OpenAI Gym, which made reinforcement learning more accessible by providing a common interface for a wide range of environments. The adoption of Gym as a standard tool allowed most open-source projects to be agnostic to the choice of environment, and made testing on multiple environments seamless.Similarly, Torchmeta introduces, under a unified interface, data-loaders for a wide range of standard few-shot classification and regression problems. As of version 1.3, the following datasets are available in Torchmeta:All these data-loaders are fully compatible with the PyTorch ecosystem, including PyTorch’s DataLoader and the torchvision package. They generate random batches of tasks based on the corresponding dataset, each of them containing a training and a test dataset — which is common practice in meta-learning. While you can have full control over how the data-loaders are defined, Torchmeta also includes helper functions for the most popular benchmarks, with useful defaults from the literature.In addition to the data-loaders, Torchmeta also provides a thin extension of PyTorch’s nn.Module, called MetaModule, to simplify the implementation of certain meta-learning algorithms. These meta-modules leaves you the option to manually specify the parameters of your modules with full computational graphs. This allows you, for example, to backpropagate through an update of the parameters, which is a key ingredient of gradient-based meta-learning methods (Finn et al., 2017; Finn, 2018; Grant et al., 2018; Lee et al., 2019; Raghu et al., 2019) and in various hybrid methods (Rusu et al., 2018; Zintgraf et al., 2019).By default (i.e. with no extra parameter), the meta-modules behave identically to their module counterparts in PyTorch. Therefore the creation of models compatible with these meta-learning methods feels very natural with Torchmeta, and only requires minimal changes to your existing PyTorch models. You can also interleave meta-modules with standard nn.Module instances to enable fast adaptation of only some parts of the model (Raghu et al., 2019).Reproducibility in meta-learning can be very challenging, especially when it comes to data loading, due to the lack of standard for some of the datasets used in the literature. For example, while the Mini-ImageNet dataset was introduced by Vinyals et al. (2016), the split used by Ravi & Larochelle (2017) is now widely accepted in the community as the official dataset. And this situation still exists as of today for some datasets (e.g. CUB). It can be hard to keep track of the “correct” version the meta-learning algorithms should be evaluated on.With a number of few-shot learning and meta-learning datasets and standard splits available, the objective of Torchmeta is to provide all the necessary tools to make the development and the reproducibility of meta-learning algorithms as easy and accessible as possible.The future is bright for meta-learning in PyTorch, and there are a number of great open-source projects that have been released lately. To mention two in particular, learn2learn provides implementations of some standard meta-learning algorithms, and higher is a library that enables higher-order optimization for existing PyTorch models. Torchmeta nicely complements these other libraries by providing a unified interface for a variety of datasets, and a set of tools to simplify the development of meta-learning algorithms. This makes the evaluation of these methods on different benchmarks seamless, and therefore is a crucial step towards better reproducible research in meta-learning.To learn more about Torchmeta, check out the examples available in the repository of the project, as well as this implementation of MAML for a more detailed showcase of all the features of Torchmeta.Finally I would like to thank the PyTorch team for organizing the Global PyTorch Summer Hackathon, and their incredible support; this was an amazing experience! I would like to also thank my wonderful colleagues at Mila for their support and feedback during the development of Torchmeta.",21/02/2020,0,2,2,"(700, 263)",2,1,0.0,33,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
113,Understanding Attention Mechanism: Natural Language Processing,Analytics Vidhya,Nikhil Agrawal,9.0,5.0,737,"Attention mechanism is one of the recent advancements in Deep learning especially for Natural language processing tasks like Machine translation, Image Captioning, dialogue generation etc. It is a mechanism that is developed to increase the performance of encoder decoder(seq2seq) RNN model. In this blog post I will try to explain the attention mechanism for the text classification task.Attention is proposed as a solution to the limitation of the Encoder-Decoder model which encodes the input sequence to one fixed length vector from which to decode the output at each time step. This issue is believed to a problem when decoding long sequences because it make difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.Attention is proposed to be a method of align and translate.In attention when the model is trying to predict the next word it searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts next word based on context vectors associated with these source positions and all the previous generated target words.Instead of encoding the input sequence into a single fixed context vector, the attention model develops a context vector that is filtered specifically for each output time step.The basic idea in Attention is that each time the model tries to predict an output word, it only uses parts of an input where the most relevant information is concentrated instead of an entire sentence i.e it tries to give more importance to the few input words. Let’s see how it works:In attention, the encoder works as similar to encoder-decoder model but the decoder behaves differently. As you can see from a picture, the decoder’s hidden state is computed with a context vector, the previous output and the previous hidden state and also it has separate context vector c_i for each target word. These context vectors are computed as a weighted sum of activation states in forward and backward directions and alphas and these alphas denote how much attention is given by the input for the generation of output word.Here, ‘a’ denotes the activation in backward and forward direction and alpha denotes the attention each input word gives to the output word.I have taken IMDB Dataset that contains the text of 50,000 movie reviews. It has already been preprocessed such that the sequences of words have been converted to sequences of integers, where each integer represents a specific word in a dictionary.Now, create a self attention layer and embed the input sentences as a vector of numbers. There are two main approaches to perform this embedding pre-trained embeddings like Word2Vec or GloVe or randomly initializing. Here, I have use random initialization. We will use a bi-directional RNN. This is simply the concatentation of two RNNs, one which processes the sequence from left to right and one which process from right to left . By using both directions, we get a stronger encoding as each word can be encoded using the context of its neighbors on both sides rather than just a single side.The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1. A model needs a loss function and an optimizer for training. Our model is a binary classification problem and the model outputs a probability. We’ll use the binary_crossentropy loss function. Train the model for 2 epochs in mini-batches of 128 samples.Now, create a Multi head attention layer with LSTM units to the input and embed the input sentences as a vector of numbers. There are two main approaches to perform this embedding pre-trained embeddings like Word2Vec or GloVe or randomly initializing. Here, I have use random initialization.The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1. A model needs a loss function and an optimizer for training. Our model is a binary classification problem and the model outputs a probability. We’ll use the binary_crossentropy loss function. Train the model for 2 epochs in mini-batches of 128 samples.The Attention mechanism is a very useful technique in NLP tasks as it increases the accuracy and bleu score and can work effectively for long sentences. The only disadvantage of the Attention mechanism is that it is a very time consuming and hard to parallelize.",28/01/2020,0,2,0,"(618, 366)",8,2,0.0,11,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
114,How to Determine the Optimal K for K-Means?,Analytics Vidhya,Khyati Mahendru,356.0,5.0,654,"The K-Means algorithm needs no introduction. It is simple and perhaps the most commonly used algorithm for clustering.The basic idea behind k-means consists of defining k clusters such that total within-cluster variation (or error) is minimum.I encourage you to check out the below articles for an in-depth explanation of different methods of clustering before proceeding further:A cluster center is the representative of its cluster. The squared distance between each point and its cluster center is the required variation. The aim of k-means clustering is to find these k clusters and their centers while reducing the total error.Quite an elegant algorithm. But there is a catch. How do you decide the number of clusters?In this article, I will explain in detail two methods that can be useful to find this mysterious k in k-Means.These methods are:We will use our own dataset generated by the code below for an illustration of the two methods:This is how the data looks graphically:Clearly, the dataset has 3 clusters. We will validate both our methods on this dataset.This is probably the most well-known method for determining the optimal number of clusters. It is also a bit naive in its approach.Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow.Within-Cluster-Sum of Squared Errors sounds a bit complex. Let’s break it down:Let us implement this in Python using the sklearn library and our own function for calculating WSS for a range of values for k.We obtain the following plot for WSS-vs-k for our dataset.As expected, the plot looks like an arm with a clear elbow at k = 3.Unfortunately, we do not always have such clearly clustered data. This means that the elbow may not be clear and sharp.For Dataset A, the elbow is clear at k = 3. However, this choice is ambiguous for Dataset B. We could choose k to be either 3 or 4.In such an ambiguous case, we may use the Silhouette Method.The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation).Source: WikipediaThe range of the Silhouette value is between +1 and -1. A high value is desirable and indicates that the point is placed in the correct cluster. If many points have a negative Silhouette value, it may indicate that we have created too many or too few clusters.The Silhouette Value s(i) for each data point i is defined as follows:Note: s(i) is defined to be equal to zero if i is the only point in the cluster. This is to prevent the number of clusters from increasing significantly with many single-point clusters.Here, a(i) is the measure of similarity of the point i to its own cluster. It is measured as the average distance of i from other points in the cluster.Similarly, b(i) is the measure of dissimilarity of i from points in other clusters.d(i, j) is the distance between points i and j. Generally, Euclidean Distance is used as the distance metric.The Silhouette score can be easily calculated in Python using the metrics module of the sklearn library.I mentioned before that a high Silhouette Score is desirable. The Silhouette Score reaches its global maximum at the optimal k. This should ideally appear as a peak in the Silhouette Value-versus-k plot.Here is the plot for our own dataset:There is a clear peak at k = 3. Hence, it is optimal.Finally, the data can be optimally clustered into 3 clusters as shown below.The Elbow Method is more of a decision rule, while the Silhouette is a metric used for validation while clustering. Thus, it can be used in combination with the Elbow Method.Therefore, the Elbow Method and the Silhouette Method are not alternatives to each other for finding the optimal K. Rather they are tools to be used together for a more confident decision.",17/06/2019,0,26,20,"(438, 223)",9,3,0.0,2,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
115,10 Papers You Must Read for Deep Image Inpainting,Towards Data Science,Chu-Tak Li,,12.0,2539,"Hello! This post can be regarded as a revision of deep image inpainting for my old friends and introductory deep image inpainting for newcomers. I have written more than 10 posts related to deep learning approaches for image inpainting. It’s time to briefly review what we have learned and also provide a highway for newcomers to join us for fun!Image inpainting is the task of filling missing pixels in an image such that the completed image is realistic-looking and follows the original (true) context. Some applications such as unwanted object(s) removal and interactive image editing are shown in Figure 1. There are also many possible applications as long as you can imagine.Given a corrupted/masked input image as shown in Figure 2 (left), we usually define i) invalid/missing/hole pixels as the pixels located at the region(s) to be filled; ii) valid/remaining/ground truth pixels as the pixels we can use to help filling in the missing pixels. Note that we can directly copy the valid pixels and paste them on the filled image at their corresponding locations.To fill in an image with some missing parts, the simplest way is to copy-and-paste. The core idea is to first search for the most similar image patches from the remaining pixels of the image itself or a large dataset with millions of images, then directly paste the patches on the missing parts. However, the search algorithm could be time-consuming and it involves hand-crafted distance measure metrics. Its generalization and efficiency still have plenty of room for improvement.Thanks to deep learning-based approaches and the era of Big Data, we can now have data-driven deep learning-based image inpainting approaches that can generate the missing pixels in an image with good global consistency and local fine textures. We will focus on 10 famous deep learning-based inpainting approaches in this post. I am sure that you can understand other inpainting papers/works after you realizing these 10 approaches. Let’s Go:)Context Encoder (CE, 2016) [1] is the first Generative Adversarial Networks (GANs) [2] based inpainting algorithm. This paper spots some useful basic concepts for the task of image inpainting. The term “Context” relates to the understanding of the entire image itself and the core idea of CE is Channel-wise Fully Connected Layer (the middle layer at the network as shown in Figure 3). Similar to the standard Fully Connected Layer, the main point is that all the feature locations at the previous layer would contribute to each feature location at the current layer. By doing so, the network can learn the relationship between all the feature locations and a deeper semantic understanding of the entire image can be obtained. CE has been regarded as a baseline and you can learn more about it from my previous post [here].Multi-Scale Neural Patch Synthesis (MSNPS, 2016) [3] can be regarded as an enhanced version of CE [1]. The authors of this paper employed a modified CE to predict the missing parts in an image and a texture network to decorate the prediction about the missing parts to improve the visual quality of the filled images. The idea of the texture network is from the task of style transfer. We would like to transfer the style of the most similar valid pixels to the generated pixels to enhance the local texture details. I would say that this work is an early version of the two-stage coarse-to-fine network structure. The first content network (i.e. CE here) is responsible for the reconstruction/prediction of the missing parts while the second network (i.e. texture network here) is responsible for the refinement of the filled parts.Apart from the typical pixel-wise reconstruction loss (i.e. L1 loss) and the standard Adversarial loss, the concept of texture loss proposed in this paper plays an important role in later inpainting papers. Actually, texture loss is related to perceptual loss and style loss that are widely used in many image generation tasks such as neural style transfer. To know more about this paper, you may refer to my previous post [here].Globally and Locally Consistent Image Completion (GLCIC, 2017) [4] is a milestone in deep image inpainting as it defines the Fully Convolution Network with Dilated Convolutions for deep image inpainting and actually this is a typical network structure for deep image inpainting. By using Dilated convolutions, the network is able to understand the context of an image without employing expensive fully connected layers and hence it can handle images of different sizes.Apart from the fully convolution network with dilated convolutions, two discriminators at two scales were also trained together with the generator network. A global discriminator looks at the whole image while a local discriminator looks at the filled centre hole. With both the global and local discriminators, the filled image would have better global and local consistency. Note that many later inpainting papers follow this multi-scale discriminator design. If you are interested in this paper, please visit my previous post [here] for more details.Patch-based Image Inpainting with GANs [5] can be regarded as a variant of GLCIC [4]. Simply speaking, two advanced concepts namely residual learning [6] and PatchGAN [7] were embedded in GLCIC to further boost its inpainting performance. The authors of this paper combined residual connection and dilated convolution to form a dilated residual block. The traditional GAN discriminator was also replaced by the PatchGAN discriminator to encourage better local texture details and global structure consistency.The core difference between traditional GAN discriminator and PatchGAN discriminator is that traditional GAN discriminator only gives a single predicted label (from 0 to 1) to indicate the realness of the input while PatchGAN discriminator gives a matrix of predicted labels (also from 0 to 1) to indicate the realness of each local region of the input. Note that each element of the matrix represents a local region of the input. You can also have a review of the residual learning and PatchGAN by visiting my previous post [here].Shift-Net [8] takes both the advantages of modern data-driven CNNs and the conventional “Copy-and-Paste” method in the form of Deep Feature Rearrangement by using the proposed shift-connection layer. There are two main ideas in this paper.First, the authors proposed guidance loss that encourages the decoded features of the missing parts (given a masked image) to be close to the encoded features of the missing parts (given a good-conditioned image). As a result, the decoding process is able to fill in the missing parts with a reasonable estimation of the missing parts in the good-conditioned image (i.e. the ground truth of the missing parts).Second, the proposed shift-connected layer with shift operation allows the network to effectively borrow the information given by the nearest neighbours outside the missing parts to refine both the global semantic structure and local texture details of the generated parts. Simply speaking, we are providing suitable references to refine our estimation. I think it’s good for readers who are interested in image inpainting to consolidate the ideas proposed in this paper. I highly recommend you to read my previous post [here] for details.Generative Image Inpainting with Contextual Attention (CA, 2018) (also called DeepFill v1 or CA) [9] can be regarded as an enhanced version or a variant of Shift-Net [8]. The authors further develop the idea of copy-and-paste and propose a contextual attention layer which is differentiable and fully convolutional.Similar to the shift-connection layer in [8], by matching the generated features inside the missing hole and the features outside the missing hole, we can know the contributions of all the features outside the missing hole to each location inside the missing hole. Hence, the combination of all the features outside can be used to refine the generated features inside the missing hole. Compared to the shift-connection layer which only looks for the most similar features (i.e. a hard assignment, not differentiable), the CA layer in this paper employs a soft assignment (is differentiable) in which all the features have their own weights to indicate their contributions to each location inside the missing hole. To know more about the Contextual Attention, please visit my previous post [here] and look for more concrete examples.Generative Multi-column Convolutional Neural Networks (GMCNN, 2018) [10] expands the importance of sufficient receptive fields for image inpainting and proposes new loss functions to further enhance local texture details of the generated content. As shown in Figure 9, there are three branches/columns and three different filter sizes are used at each branch. The use of the multiple receptive fields (filter sizes) is due to the fact that the size of the receptive field is important to the task of image inpainting. As the local neighbouring pixels are missing, we have to borrow information given by distant spatial locations to fill in the local missing pixels.For the proposed loss functions, the main idea of the Implicit Diversified Markov Random Field (ID-MRF) loss is to guide the generated feature patches to find their nearest neighbours outside the missing areas as references and these nearest neighbours should be sufficiently diverse such that more local texture details can be simulated. Actually, this loss is an enhanced version of the texture loss used in MSNPS [3]. I highly recommend you to read my previous post [here] for a detailed explanation of the loss.Image Inpainting for Irregular Holes using Partial Convolutions (PartialConv or PConv) [11] pushes the limits of deep image inpainting by proposing a way to handle masked images with multiple irregular holes. Obviously, the core idea of this paper is the Partial Convolution. By using PConv, the results of convolution would only depend on the valid pixels, hence we can have the control of the information to be passed inside the network. This is the first inpainting paper targeted at handling irregular holes. Note that previous inpainting models were trained on regular masked images, hence these models are not suitable to complete irregular masked images.I have provided a simple example to explain clearly how the partial convolution is performed in my previous post [here]. Please visit and have a look for details. Hope you enjoy it:)EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning (EdgeConnect) [12] provides an interesting way to the task of image inpainting. The main idea of this paper is to separate the task into two simpler steps, namely edge prediction and image completion based on the predicted edge map. They first predict the edges in the missing regions, then complete the image according to the predicted edge information. I would say that most of the techniques used in this paper have been covered in my previous posts. It’s good for you to have a look at how various techniques can be used together to form a novel deep image inpainting approach. Perhaps, you may be able to develop your own inpainting model! Please see my previous post [here] for more details about this paper.Free-Form Image Inpainting with Gated Convolution (DeepFill v2 or GConv, 2019) [13] perhaps is the most practical image inpainting algorithm that can be directly used in your applications. This can be regarded as an enhanced version of DeepFill v1 [9], Partial Convolution [11], and EdgeConnect [12]. The main idea of this paper is Gated Convolution which is a learnable version of the Partial Convolution. By adding an extra standard convolutional layer followed by a sigmoid function, the validness of each pixel/feature location can be learned and hence optional user sketch input is also allowed. Apart from the Gated Convolution, SN-PatchGAN is also adopted to further stabilize the training of the GAN model. To know more about the difference between the Partial Convolution and the Gated Convolution, and how optional user sketch input can contribute to the inpainting results, please visit my latest post [here].I hope that all of you can have a basic understanding of image inpainting now. I believe that most of the common techniques used in deep image inpainting have been covered in my previous posts. If you are my old friend, I think now you are able to understand other inpainting papers in the literature. If you are a newcomer, I would like to welcome you. I hope that you find this post useful for you. Actually, this post provides a highway for you to join us and learn together.In my opinion, for image inpainting, it is still difficult to complete images with complex scene structures and with a large mask ratio (e.g. 50% of the pixels are missing). Of course, high-resolution image inpainting is also another challenging task. All these challenges can be categorized into Extreme Image Inpainting. I think that the coming state-of-the-art inpainting approach should be able to tackle some of these challenges.Apart from paper reviews, I would like to try more new things and learn more. I have tried my best to show the development of deep image inpainting. I hope that you can see the trends in image inpainting and how a model is designed based on previous studies. I am really happy to share my knowledge with all of you here:)Thanks for reading. If you have any questions, please feel free to leave comments here or send me an email. I am happy to hear from you and any suggestions are welcome. Hope to see you in the future:)[1] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros, “Context Encoders: Feature Learning by Inpainting,” Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.[2] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, “Generative Adversarial Nets,” in Advances in Neural Information Processing Systems (NeurIPS), 2014.[3] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li, “High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis,” Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.[4] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa, “Globally and Locally Consistent Image Completion,” ACM Trans. on Graphics, Vol. 36, №4, Article 107, Publication date: July 2017.[5] Ugur Demir, and Gozde Unal, “Patch-Based Image Inpainting with Generative Adversarial Networks,” https://arxiv.org/pdf/1803.07422.pdf.[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep Residual Learning for Image Recognition,” Proc. Computer Vision and Pattern Recognition (CVPR), 27–30 Jun. 2016.[7] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros, “Image-to-Image Translation with Conditional Adversarial Networks,” Proc. Computer Vision and Pattern Recognition (CVPR), 21–26 Jul. 2017.[8] Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and Shiguang Shan, “Shift-Net: Image Inpainting via Deep Feature Rearrangement,” Proc. European Conference on Computer Vision (ECCV), 2018.[9] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang, “Generative Image Inpainting with Contextual Attention,” Proc. Computer Vision and Pattern Recognition (CVPR), 2018.[10] Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia, “Image Inpainting via Generative Multi-column Convolutional Neural Networks,” Proc. Neural Information Processing Systems, 2018.[11] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro, “Image Inpainting for Irregular Holes Using Partial Convolution,” Proc. European Conference on Computer Vision (ECCV), 2018.[12] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran Ebrahimi, “EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning,” Proc. International Conference on Computer Vision (ICCV), 2019.[13] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang, “Free-Form Image Inpainting with Gated Convolution,” Proc. International Conference on Computer Vision (ICCV), 2019.",30/11/2020,0,23,22,"(700, 308)",12,0,0.0,45,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,positive,joy/calmness
116,Computer Vision Techniques: Implementing Mask-R CNN on Malaria Cells Data,Analytics Vidhya,Nidhi Bansal,11.0,15.0,1999,"In Today’s world Computer vision is most powerful and complex field of Artificial Intelligence. We will experience various Applications and techniques of Computer Vision as we go further.Computer Vision is the field of computer science that tries to replicate the complexity of human eyes. It enables a computer to detect objects in images and videos. Thanks to Deep Learning and various CNN and R-CNN techniques that it is possible nowadays.There are 5 major Computer Vision Techniques:In Image Classification problem, each image in various images belongs to a single category. We need to define that label for images and predict that category for images.Let’s formulate a problem:Here, we can use a multi-label classifier or CNN(Convolutional Neural Network) to train our classifier.In the above picture, we have train MNIST data using CNN and predict labels of some images.Output: Output of Image classification is Class Labels or Class Ids.Question: What if we want to locate the location of the object?Let’s say, we have images of dogs and cats and we classify them using CNN, what if we want the location of them in the images.This is the more challenging version of image classification.Output: Output is class Labels + location of the object in the image which is given by bounding box. The bounding box is a rectangle or square box around the object.Question: What if there are different types of objects in a single image?In Image classification with Localization, each image can have multiple objects but with same class labels, but if an image has a different objects with different class labels then identifying their class labels and bounding box comes under object detection.Output: It gives class labels and bounding box for each object in image.Object detection can be done using four algorithms:Problem with CNN is that we need to apply CNN to huge numbers of locations and scales, which is very computationally expensive!2. R-CNN(Region based CNN)- Instead of working on massive number of regions, R-CNN algorithm proposes a bunch of boxes in the image and checks if any of these boxes contain any object. R-CNN uses selective search to extract these boxes from an image these boxes are called ROI(Regions Of Interest).Let’s understand Selective search: There are basically four areas of an image: varying scales, colors, textures, and enclosure. The selective search identifies these patterns in an image and on the basis of that it proposes various regions.Steps in Selective Search:Take an image ->Initial segmentation -> combine similar segments (on basis of color , texture or size similarity, and shape compatibility) -> ROI(Regions Of Interest)Steps followed in R-CNN to detect objects:Few but higher quality ROIs makes R-CNN faster and more accurate than the sliding windows CNN.Problems with R-CNN:Training an R-CNN model is expensive and slow because:3. Fast R-CNNSolution to problem of R-CNN is Fast R-CNN. In R-CNN we have used approximately 2000 CNN for every image. But, in Fast R-CNN single CNN is used for every image and gets all features once, thus it reduces the computational time.Steps followed in Fast R-CNN to detect objects:By not repeating the feature extractions, Fast R-CNN cuts down the process time significantly.Problems with Fast R-CNN: Fast R-CNN still use Region Proposal method to find ROI, which makes it time consuming.4. Faster R-CNN Faster R-CNN is the modified version of Fast R-CNN. The major difference between them is that Fast RCNN uses the selective search for generating Regions of Interest, while Faster RCNN uses “Region Proposal Network”(RPN).RPN takes image feature maps as an input and generates ROIs.Steps followed in Faster R-CNN to detect objectsRPN uses a sliding window over the feature maps generated from CNN, and at each window, it generates let’s say,k Anchor boxes(fixed boundary boxes)of different shapes and sizes. For each anchor box, RPN predicts two things:Question: What about the actual shape of objects?Object detection tells us about class labels and the bounding box of each object. but, it does not tell us about the actual shape of each object.So, here Image segmentation comes into the picture.Image segmentation creates a pixel-wise mask for each object, so gives us the exact shape of objects.Question: Where do we need image segmentation?Answer: Below is some applications of Image segmentation:Image segmentation is of two types:i. Semantic Segmentation: Every pixel in the image belongs to one particular class — car, building, window, etc. And all pixels belonging to a particular class have been assigned a single color. For e.g. It segments the image like a background in 1 class, cars in the image as 1 class and person in image as 1 class. So, here a total of 3 classes and segment the picture in 3 masks of 3 colors.There is some architecture which implements semantic segmentation like FCN(Fully Convolutional Network), Encoder-Decoder Architecture (e.g. U-Net Architecture), etc.Question: What if want to detect each object of the same class/type separately?ii. Instance Segmentation: It segments each object separately.Different instances of the same class are segmented individually in instance segmentation. Like, in the above image that different instances of the same class (animals) have been given different labels.The bounding box of many objects overlaps with each other. So the mask helps in detecting the exact shape of the object.One of the algorithms used for Instance Segmentation is Mask R-CNN.The Mask R-CNN is built basically on top of Faster R-CNN. It is a pixel-level image segmentation.Steps followed in Mask R-CNN to detect objectsLoss in Mask R-CNN is consists of loss due to RPN(Regional Proposed Network) and loss due to classification, localization and segmentation mask.1. Loss(RPN)= RPN_Class Loss + RPN_BBox Loss2. Loss(Mask R-CNN)= Loss(class labels prediction) + Loss(Bounding Box prediction) + Loss (Mask Prediction)So, our optimization problem is to minimize the Total LossI have taken data for Malaria Data Cells from kaggle. Link description below:Data Source: https://www.kaggle.com/kmader/malaria-bounding-boxesIn this data is Images (.png or .jpg format). There are 2 sets of images consisting of 1208 training and 120 test images.Labels: The data consists of two classes of uninfected cells (RBCs and leukocytes) and four classes of infected cells (gametocytes, rings, trophozoites, and schizonts). The data had a heavy imbalance towards uninfected RBCs versus uninfected leukocytes and infected cells, making up over 95% of all cells.A class label and set of bounding box coordinates were given for each cell in the JSON file.I have a train model using Mask-RCNN (Mask Regional Convolutional Neural Network).I have learned Mask-RCNN from the below link. It is a training Kangaroo object detection dataset. Some code snippets are taken from the below reference link. #Ref: https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/Mask_RCNN gives 3 outputs:1. Class_ids2. Objects/Cells Bounding Box3. Mask for Objects/CellsThis case study divided into 5 steps:1. Install Mask R-CNN for Keras2. Prepare data set for Object Detection3. Train Mask R-CNN Model for Malaria Cell Detection4. Evaluate Mask R-CNN Model5. Detect Cells in new photosi. Clone or download repository from: https://github.com/matterport/Mask_RCNNii. Open cmd. Change to Mask_RCNN directory and run install script:cd Mask_RCNNpython setup.py installwe will get successful installation message.iii. How to check if Mask RCNN is successfully installed or not:Run command: >pip show mask-rcnnOutput : Name: mask-rcnnVersion: 2.1Summary: Mask R-CNN for object detection and instance segmentationHome-page: https://github.com/matterport/Mask_RCNNAuthor: MatterportAuthor-email: waleed.abdulla@gmail.comLicense: MITLocation: *\anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.eggRequires:Required-by:We are now ready to use this library.Sample image:Sample training.json file:Here, the image has r * c pixels and minimum and maximum value of r and c define bounding box vertices.To create data set, we need to extract min, max, r and c values with corresponding categories for each bounding box of every image. And assign an image_id to every image.Write different functions of Mask RCNNThen prepare the train and test dataset:Let’s test if image loading, masking, and boxing works properly on not?Output of the above code:Visualization of image shows mask, bounding box in dotted box format and class id.(i). The first step is to define configuration for training the model:We define MalariaConfig class which extends the mrcnn.config.Config class.(ii). Train the modelNow, we will train the model using predefined weights. The first step is to download the model file (architecture and weights) for the pre-fit Mask R-CNN model. Download the model weights to a file with the name ‘mask_rcnn_coco.h5‘ from matterplot github directory of Mask R-CNN, in your current working directory.Now, define the model by creating an instance of the mrcnn.model.MaskRCNN class and specifying the model will be used for training via setting the ‘mode‘ argument to ‘training‘ and use config which we define above.Load weight mask_rcnn_coco.h5 which we have downloaded.Now train the model. (Code snippets shown below)Training the model takes approx 2–3 hrs. I am using GPU(NVIDIA GeForce GTX 1080 with Max-Q Design).A model is created by the end of every epoch. As loss decreases with every epoch, so we will use epoch 5 file mask_rcnn_malaria_cfg_0005.h5 to evaluate model performance.The first step is to define a new configuration for evaluating the model. See the code below.Next, we can define the model with the config_pred and set the ‘mode‘ argument to ‘inference‘ instead of ‘training‘.Next, we can load the weights from our saved model file ‘mask_rcnn_malaria_cfg_0005.h5‘ in the current working directory.plot_actual_vs_predicted is a function defined to plot actual images and predicted image. The pseudocode snippet is in the below image.The performance of a model for an object recognition task is often evaluated using the mAP and IOU.We are predicting bounding boxes so we can determine how well the predicted and actual bounding boxes overlap. This can be calculated by dividing the area of the overlap by the total area of both bounding boxes, or the intersection divided by the union, referred to as “intersection over union,” or IoU. A perfect bounding box prediction will have an IoU of 1.It is standard to assume a positive prediction of a bounding box if the IoU is greater than 0.5, e.g. they overlap by 50% or more.Precision refers to the percentage of the correctly predicted bounding boxes (IoU > 0.5) out of all bounding boxes predicted in the image. Recall is the percentage of the correctly predicted bounding boxes (IoU > 0.5) out of all objects in the image.As we make more predictions, the recall percentage will increase, but precision will drop or become erratic as we start making false positive predictions. The recall (x) can be plotted against the precision (y) for each number of predictions to create a curve or line. We can maximize the value of each point on this line and calculate the average value of the precision or AP for each value of recall.The average or mean of the average precision (AP) across all of the images in a dataset is called the mean average precision, or mAP.The mask-rcnn library provides a mrcnn.utils.compute_ap to calculate the AP and other metrics for a given image. These AP scores can be collected across a dataset and the mean calculated to give an idea at how good the model is at detecting objects in a dataset.From the above actual and predicted image we can see most of the cells of the actual image are predicted. In this example image, Red Blood Cells and Trophozite are predicted.After evaluating the model we got: mAP for training data is evaluated as 0.830mAP for test data is evaluated as 0.806I have downloaded some Malaria Cells images from the internet and run the model on these images. These images are not part of training and test datasets.Here, are the results:In the above new image 1 Trophozite(in Red color mask) is predicted with 0.7443 score.In the above new image 1 Trophozite(in Red color mask) is predicted with a 0.723 score.In the above predicted images, showing the bounding box and mask of many cells in the image. And many RBCs and Trophozites are detected and predicted correctly.For code, check my Github profilegithub.comIn this blog, we have discussed various Computer Vision techniques.Mask R-CNN is discussed in detail and applied to Malaria Data cells.It works fairly well on cells identification to Red Blood cells and Parasites infected cells.If you found my article useful, give it a 👏 and help others find it. Remember, you can clap up to 50 times (by pressing on the 👏 icon for longer). And it’s a great way to give feedback!",14/11/2019,0,42,23,"(633, 377)",29,12,0.0,6,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
117,Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2,NanoNets,Bharath Raj,2800.0,15.0,2800,"This article is a comprehensive review of Data Augmentation techniques for Deep Learning, specific to images. This is Part 2 of How to use Deep Learning when you have Limited Data. Checkout Part 1 here.We have all been there. You have a stellar concept that can be implemented using a machine learning model. Feeling ebullient, you open your web browser and search for relevant data. Chances are, you find a dataset that has around a few hundred images.You recall that most popular datasets have images in the order of tens of thousands (or more). You also recall someone mentioning having a large dataset is crucial for good performance. Feeling disappointed, you wonder; can my “state-of-the-art” neural network perform well with the meagre amount of data I have?The answer is, yes! But before we get into the magic of making that happen, we need to reflect upon some basic questions.When you train a machine learning model, what you’re really doing is tuning its parameters such that it can map a particular input (say, an image) to some output (a label). Our optimization goal is to chase that sweet spot where our model’s loss is low, which happens when your parameters are tuned in the right way.State of the art neural networks typically have parameters in the order of millions!Naturally, if you have a lot of parameters, you would need to show your machine learning model a proportional amount of examples, to get good performance. Also, the number of parameters you need is proportional to the complexity of the task your model has to perform.You don’t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.So, to get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway.A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above).This essentially is the premise of data augmentation. In the real world scenario, we may have a dataset of images taken in a limited set of conditions. But, our target application may exist in a variety of conditions, such as different orientation, location, scale, brightness etc. We account for these situations by training our neural network with additional synthetically modified data.Yes. It can help to increase the amount of relevant data in your dataset. This is related to the way with which neural networks learn. Let me illustrate it with an example.Imagine that you have a dataset, consisting of two brands of cars, as shown above. Let’s assume that all cars of brand A are aligned exactly like the picture in the left (i.e. All cars are facing left) . Likewise, all cars of brand B are aligned exactly like the picture in the right (i.e. Facing right) . Now, you feed this dataset to your “state-of-the-art” neural network, and you hope to get impressive results once it’s trained.Let’s say it’s done training, and you feed the image above, which is a Brand A car. But your neural network outputs that it’s a Brand B car! You’re confused. Didn’t you just get a 95% accuracy on your dataset using your “state-of-the-art” neural network? I’m not exaggerating, similar incidents and goof-ups have occurred in the past.Why does this happen? It happens because that’s how most machine learning algorithms work. It finds the most obvious features that distinguishes one class from another. Here, the feature was that all cars of Brand A were facing left, and all cars of Brand B are facing right.Your neural network is only as good as the data you feed it.How do we prevent this happening? We have to reduce the amount of irrelevant features in the dataset. For our car model classifier above, a simple solution would be to add pictures of cars of both classes, facing the other direction to our original dataset. Better yet, you can just flip the images in the existing dataset horizontally such that they face the other side! Now, on training the neural network on this new dataset, you get the performance that you intended to get.By performing augmentation, can prevent your neural network from learning irrelevant patterns, essentially boosting overall performance.Before we dive into the various augmentation techniques, there’s one issue that we must consider beforehand.The answer may seem quite obvious; we do augmentation before we feed the data to the model right? Yes, but you have two options here. One option is to perform all the necessary transformations beforehand, essentially increasing the size of your dataset. The other option is to perform these transformations on a mini-batch, just before feeding it to your machine learning model.The first option is known as offline augmentation. This method is preferred for relatively smaller datasets, as you would end up increasing the size of the dataset by a factor equal to the number of transformations you perform (For example, by flipping all my images, I would increase the size of my dataset by a factor of 2).The second option is known as online augmentation, or augmentation on the fly. This method is preferred for larger datasets, as you can’t afford the explosive increase in size. Instead, you would perform transformations on the mini-batches that you would feed to your model. Some machine learning frameworks have support for online augmentation, which can be accelerated on the GPU.In this section, we present some basic but powerful augmentation techniques that are popularly used. Before we explore these techniques, for simplicity, let us make one assumption. The assumption is that, we don’t need to consider what lies beyond the image’s boundary. We’ll use the below techniques such that our assumption is valid.What would happen if we use a technique that forces us to guess what lies beyond an image’s boundary? In this case, we need to interpolate some information. We’ll discuss this in detail after we cover the types of augmentation.For each of these techniques, we also specify the factor by which the size of your dataset would get increased (aka. Data Augmentation Factor).You can flip images horizontally and vertically. Some frameworks do not provide function for vertical flips. But, a vertical flip is equivalent to rotating an image by 180 degrees and then performing a horizontal flip. Below are examples for images that are flipped.You can perform flips by using any of the following commands, from your favorite packages. Data Augmentation Factor = 2 to 4xOne key thing to note about this operation is that image dimensions may not be preserved after rotation. If your image is a square, rotating it at right angles will preserve the image size. If it’s a rectangle, rotating it by 180 degrees would preserve the size. Rotating the image by finer angles will also change the final image size. We’ll see how we can deal with this issue in the next section. Below are examples of square images rotated at right angles.You can perform rotations by using any of the following commands, from your favorite packages. Data Augmentation Factor = 2 to 4xThe image can be scaled outward or inward. While scaling outward, the final image size will be larger than the original image size. Most image frameworks cut out a section from the new image, with size equal to the original image. We’ll deal with scaling inward in the next section, as it reduces the image size, forcing us to make assumptions about what lies beyond the boundary. Below are examples or images being scaled.You can perform scaling by using the following commands, using scikit-image. Data Augmentation Factor = Arbitrary.Unlike scaling, we just randomly sample a section from the original image. We then resize this section to the original image size. This method is popularly known as random cropping. Below are examples of random cropping. If you look closely, you can notice the difference between this method and scaling.You can perform random crops by using any the following command for TensorFlow. Data Augmentation Factor = Arbitrary.Translation just involves moving the image along the X or Y direction (or both). In the following example, we assume that the image has a black background beyond its boundary, and are translated appropriately. This method of augmentation is very useful as most objects can be located at almost anywhere in the image. This forces your convolutional neural network to look everywhere.You can perform translations in TensorFlow by using the following commands. Data Augmentation Factor = Arbitrary.Over-fitting usually happens when your neural network tries to learn high frequency features (patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially has data points in all frequencies, effectively distorting the high frequency features. This also means that lower frequency components (usually, your intended data) are also distorted, but your neural network can learn to look past that. Adding just the right amount of noise can enhance the learning capability.A toned down version of this is the salt and pepper noise, which presents itself as random black and white pixels spread through the image. This is similar to the effect produced by adding Gaussian noise to an image, but may have a lower information distortion level.You can add Gaussian noise to your image by using the following command, on TensorFlow. Data Augmentation Factor = 2x.Real world, natural data can still exist in a variety of conditions that cannot be accounted for by the above simple methods. For instance, let us take the task of identifying the landscape in photograph. The landscape could be anything: freezing tundras, grasslands, forests and so on. Sounds like a pretty straight forward classification task right? You’d be right, except for one thing. We are overlooking a crucial feature in the photographs that would affect the performance — The season in which the photograph was taken.If our neural network does not understand the fact that certain landscapes can exist in a variety of conditions (snow, damp, bright etc.), it may spuriously label frozen lakeshores as glaciers or wet fields as swamps.One way to mitigate this situation is to add more pictures such that we account for all the seasonal changes. But that is an arduous task. Extending our data augmentation concept, imagine how cool it would be to generate effects such as different seasons artificially?Without going into gory detail, conditional GANs can transform an image from one domain to an image to another domain. If you think it sounds too vague, it’s not; that’s literally how powerful this neural network is! Below is an example of conditional GANs used to transform photographs of summer sceneries to winter sceneries.The above method is robust, but computationally intensive. A cheaper alternative would be something called neural style transfer. It grabs the texture/ambiance/appearance of one image (aka, the “style”) and mixes it with the content of another. Using this powerful technique, we produce an effect similar to that of our conditional GAN (In fact, this method was introduced before cGANs were invented!).The only downside of this method is that, the output tends to looks more artistic rather than realistic. However, there are certain advancements such as Deep Photo Style Transfer, shown below, that have impressive results.We have not explored these techniques in great depth as we are not concerned with their inner working. We can use existing trained models, along with the magic of transfer learning, to use it for augmentation.What if you wanted to translate an image that doesn’t have a black background? What if you wanted to scale inward? Or rotate in finer angles? After we perform these transformations, we need to preserve our original image size. Since our image does not have any information about things outside it’s boundary, we need to make some assumptions. Usually, the space beyond the image’s boundary is assumed to be the constant 0 at every point. Hence, when you do these transformations, you get a black region where the image is not defined.But is that the right assumption? In the real world scenario, it’s mostly a no. Image processing and ML frameworks have some standard ways with which you can decide on how to fill the unknown space. They are defined as follows.The simplest interpolation method is to fill the unknown region with some constant value. This may not work for natural images, but can work for images taken in a monochromatic backgroundThe edge values of the image are extended after the boundary. This method can work for mild translations.The image pixel values are reflected along the image boundary. This method is useful for continuous or natural backgrounds containing trees, mountains etc.This method is similar to reflect, except for the fact that, at the boundary of reflection, a copy of the edge pixels are made. Normally, reflect and symmetric can be used interchangeably, but differences will be visible while dealing with very small images or patterns.The image is just repeated beyond its boundary, as if it’s being tiled. This method is not as popularly used as the rest as it does not make sense for a lot of scenarios.Besides these, you can design your own methods for dealing with undefined space, but usually these methods would just do fine for most classification problems.If you use it in the right way, then yes! What is the right way you ask? Well, sometimes not all augmentation techniques make sense for a dataset. Consider our car example again. Below are some of the ways by which you can modify the image.Sure, they are pictures of the same car, but your target application may never see cars presented in these orientations.For instance, if you’re just going to classify random cars on the road, only the second image would make sense to be on the dataset. But, if you own an insurance company that deals with car accidents, and you want to identify models of upside-down, broken cars as well, the third image makes sense. The last image may not make sense for both the above scenarios.The point is, while using augmentation techniques, we have to make sure to not increase irrelevant data.You’re probably expecting some results to motivate you to walk the extra mile. Fair enough; I’ve got that covered too. Let me prove that augmentation really works, using a toy example. You can replicate this experiment to verify.Let’s create two neural networks to classify data to one among four classes: cat, lion, tiger or a leopard. The catch is, one will not use data augmentation, whereas the other will. You can download the dataset from here link.If you’ve checked out the dataset, you’ll notice that there’s only 50 images per class for both training and testing. Clearly, we can’t use augmentation for one of the classifiers. To make the odds more fair, we use Transfer Learning to give the models a better chance with the scarce amount of data.For the one without augmentation, let’s use a VGG19 network. I’ve written a TensorFlow implementation here, which is based on this implementation. Once you’ve cloned my repo, you can get the dataset from here, and vgg19.npy (used for transfer learning) from here. You can now run the model to verify the performance.I would agree though, writing extra code for data augmentation is indeed a bit of an effort. So, to build our second model, I turned to Nanonets. They internally use transfer learning and data augmentation to provide the best results using minimal data. All you need to do is upload the data on their website, and wait until it’s trained in their servers (Usually around 30 minutes). What do you know, it’s perfect for our comparison experiment.Once it’s done training, you can request calls to their API to calculate the test accuracy. Checkout out my repo for a sample code snippet(Don’t forget to insert your model’s ID in the code snippet).Impressive isn’t it. It is a fact that most models perform well with more data. So to provide a concrete proof, I’ve mentioned the table below. It shows the error rate of popular neural networks on the Cifar 10 (C10) and Cifar 100 (C100) datasets. C10+ and C100+ columns are the error rates with data augmentation.Thank you for reading this article! Hit that clap button if you did! Hope it shed some light about data augmentation. If you have any questions, you could hit me up on social media or send me an email (bharathrajn98@gmail.com).About Nanonets: Nanonets is building APIs to simplify deep learning for developers. Visit us at https://www.nanonets.com for more)",11/04/2018,7,94,0,"(700, 255)",19,0,0.0,17,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
118,Convolutions and Backpropagations,,Pavithra Solai,,8.0,1197,"Ever since AlexNet won the ImageNet competition in 2012, Convolutional Neural Networks (CNNs) have become ubiquitous. Starting from the humble LeNet to ResNets to DenseNets, CNNs are everywhere.But have you ever wondered what happens in a Backward pass of a CNN, especially how Backpropagation works in a CNN. If you have read about Backpropagation, you would have seen how it is implemented in a simple Neural Network with Fully Connected layers. (Andrew Ng’s course on Coursera does a great job of explaining it). But, for the life of me, I couldn’t wrap my head around how Backpropagation works with Convolutional layers.The more I dug through the articles related to CNNs and Backpropagation, the more confused I got. Explanations were mired in complex derivations and notations and they needed an extra-mathematical muscle to understand it. And I was getting nowhere.I know, you don’t have to know the mathematical intricacies of a Backpropagation to implement CNNs. You don’t have to implement them by hand. And hence, most of the Deep Learning Books don’t cover it either.So when I finally figured it out, I decided to write this article. To simplify and demystify it. Of course, it would be great if you understand the basics of Backpropagation to follow this article.The most important thing about this article is to show you this:We all know the forward pass of a Convolutional layer uses Convolutions. But, the backward pass during Backpropagation also uses Convolutions!So, let us dig in and start with understanding the intuition behind Backpropagation. (And for this, we are going to rely on Andrej Karpathy’s amazing CS231n lecture — https://www.youtube.com/watch?v=i94OvYb6noo).But if you are already aware of the chain rule in Backpropagation, then you can skip to the next section.Consider this equationf(x,y,z) = (x + y)zTo make it simpler, let us split it into two equations.Now, let us draw a computational graph for it with values of x, y, z as x = -2, y = 5, z = 4.When we solve for the equations, as we move from left to right, (‘the forward pass’), we get an output of f = -12Now let us do the backward pass. Say, just like in Backpropagations, we derive the gradients moving from right to left at each stage. So, at the end, we have to get the values of the gradients of our inputs x,y and z — ∂f/∂x and ∂f/∂y and ∂f/∂z (differentiating function f in terms of x,y and z)Working from right to left, at the multiply gate we can differentiate f to get the gradients at q and z — ∂f/∂q and ∂f/∂z . And at the add gate, we can differentiate q to get the gradients at x and y — ∂q/∂x and ∂q/∂y.We have to find ∂f/∂x and ∂f/∂y but we only have got the values of ∂q/∂x and ∂q/∂y. So, how do we go about it?This can be done using the chain rule of differentiation. By the chain rule, we can find ∂f/∂x asAnd we can calculate ∂f/∂x and ∂f/∂y as:Now that we have worked through a simple computational graph, we can imagine a CNN as a massive computational graph. Let us say we have a gate f in that computational graph with inputs x and y which outputs z.We can easily compute the local gradients — differentiating z with respect to x and y as ∂z/∂x and ∂z/∂yFor the forward pass, we move across the CNN, moving through its layers and at the end obtain the loss, using the loss function. And when we start to work the loss backwards, layer across layer, we get the gradient of the loss from the previous layer as ∂L/∂z. In order for the loss to be propagated to the other gates, we need to find ∂L/∂x and ∂L/∂y.The chain rule comes to our help. Using the chain rule we can calculate ∂L/∂x and ∂L/∂y, which would feed the other gates in the extended computational graphSo, what has this got to do with Backpropagation in the Convolutional layer of a CNN?Now, lets assume the function f is a convolution between Input X and a Filter F. Input X is a 3x3 matrix and Filter F is a 2x2 matrix, as shown below:Convolution between Input X and Filter F, gives us an output O. This can be represented as:This gives us the forward pass! Let’s get to the Backward pass. As mentioned earlier, we get the loss gradient with respect to the Output O from the next layer as ∂L/∂O, during Backward pass. And combining with our previous knowledge using Chain rule and Backpropagation we get:As seen above, we can find the local gradients ∂O/∂X and ∂O/∂F with respect to Output O. And with loss gradient from previous layers — ∂L/∂O and using chain rule, we can calculate ∂L/∂X and ∂L/∂F.Well, but why do we need to find ∂L/∂X and ∂L/∂F?This has two steps as we have done earlier.Step 1: Finding the local gradient — ∂O/∂F:This means we have to differentiate Output Matrix O with Filter F. From our convolution operation, we know the values. So let us start differentiating the first element of O- O¹¹ with respect to the elements of F — F¹¹ , F¹², F²¹ and F²²Step 2: Using the Chain rule:As described in our previous examples, we need to find ∂L/∂F as:O and F are matrices. And ∂O/∂F will be a partial derivative of a matrix O with respect to a matrix F! On top of it we have to use the chain rule. This does look complicated but thankfully we can use the formula below to expand it.Expanding, we get..Substituting the values of the local gradient — ∂O/∂F from Equation A, we getIf you closely look at it, this represents an operation we are quite familiar with. We can represent it as a convolution operation between input X and loss gradient ∂L/∂O as shown below:∂L/∂F is nothing but the convolution between Input X and Loss Gradient from the next layer ∂L/∂OStep 1: Finding the local gradient — ∂O/∂X:Similar to how we found the local gradients earlier, we can find ∂O/∂X as:Step 2: Using the Chain rule:Expanding this and substituting from Equation B, we getOk. Now we have the values of ∂L/∂X.Believe it or not, even this can be represented as a convolution operation.∂L/∂X can be represented as ‘full’ convolution between a 180-degree rotated Filter F and loss gradient ∂L/∂OFirst, let us rotate the Filter F by 180 degrees. This is done by flipping it first vertically and then horizontally.Now, let us do a ‘full’ convolution between this flipped Filter F and ∂L/∂O, which can be visualized as below: (It is like sliding one matrix over another from right to left, bottom to top)The full convolution above generates the values of ∂L/∂X and hence we can represent ∂L/∂X asWell, now that we have found ∂L/∂X and ∂L/∂F, we can now come to this conclusionBoth the Forward pass and the Backpropagation of a Convolutional layer are ConvolutionsSumming it up:Hope this helped to explain how Backpropagation works in a Convolutional layer of a CNN.If you want to read more about it, do look at these links below. And do show some love by clapping for this article. Adios! :)www.jefkine.combecominghuman.ai",19/03/2018,0,79,34,"(654, 448)",29,1,0.0,4,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,subjective,positive,expectation/interest
119,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native",,Tim Anglade,3400.0,23.0,3357,"— July 2018 Update —Not Hotdog has been nominated for a primetime Emmy®. Thanks to all the fans for the amazing reception, and to all the people at HBO & Brown Hill Productions who made this possible.The HBO show Silicon Valley released a real AI app that identifies hotdogs — and not hotdogs — like the one shown on season 4’s 4th episode (the app is now available on Android as well as iOS!)To achieve this, we designed a bespoke neural architecture that runs directly on your phone, and trained it with Tensorflow, Keras & Nvidia GPUs.While the use-case is farcical, the app is an approachable example of both deep learning, and edge computing. All AI work is powered 100% by the user’s device, and images are processed without ever leaving their phone. This provides users with a snappier experience (no round trip to the cloud), offline availability, and better privacy. This also allows us to run the app at a cost of $0, even under the load of a million users, providing significant savings compared to traditional cloud-based AI approaches.The app was developed in-house by the show, by a single developer, running on a single laptop & attached GPU, using hand-curated data. In that respect, it may provide a sense of what can be achieved today, with a limited amount of time & resources, by non-technical companies, individual developers, and hobbyists alike. In that spirit, this article attempts to give a detailed overview of steps involved to help others build their own apps.If you haven’t seen the show or tried the app (you should!), the app lets you snap a picture and then tells you whether it thinks that image is of a hotdog or not. It’s a straightforward use-case, that pays homage to recent AI research and applications, in particular ImageNet.While we’ve probably dedicated more engineering resources to recognizing hotdogs than anyone else, the app still fails in horrible and/or subtle ways.Conversely, it’s also sometimes able to recognize hotdogs in complex situations… According to Engadget, “It’s incredible. I’ve had more success identifying food with the app in 20 minutes than I have had tagging and identifying songs with Shazam in the past two years.”Have you ever found yourself reading Hacker News, thinking “they raised a 10M series A for that? I could build it in one weekend!” This app probably feels a lot like that, and the initial prototype was indeed built in a single weekend using Google Cloud Platform’s Vision API, and React Native. But the final app we ended up releasing on the app store required months of additional (part-time) work, to deliver meaningful improvements that would be difficult for an outsider to appreciate. We spent weeks optimizing overall accuracy, training time, inference time, iterating on our setup & tooling so we could have a faster development iterations, and spent a whole weekend optimizing the user experience around iOS & Android permissions (don’t even get me started on that one).All too often technical blog posts or academic papers skip over this part, preferring to present the final chosen solution. In the interest of helping others learn from our mistake & choices, we will present an abridged view of the approaches that didn’t work for us, before we describe the final architecture we ended up shipping in the next section.We chose React Native to build the prototype as it would give us an easy sandbox to experiment with, and would help us quickly support many devices. The experience ended up being a good one and we kept React Native for the remainder of the project: it didn’t always make things easy, and the design for the app was purposefully limited, but in the end React Native got the job done.The other main component we used for the prototype — Google Cloud’s Vision API was quickly abandoned. There were 3 main factors:For these reasons, we started experimenting with what’s trendily called “edge computing”, which for our purposes meant that after training our neural network on our laptop, we would export it and embed it directly into our mobile app, so that the neural network execution phase (or inference) would run directly inside the user’s phone.Through a chance encounter with Pete Warden of the TensorFlow team, we had become aware of its ability to run TensorFlow directly embedded on an iOS device, and started exploring that path. After React Native, TensorFlow became the second fixed part of our stack.It only took a day of work to integrate TensorFlow’s Objective-C++ camera example in our React Native shell. It took slightly longer to use their transfer learning script, which helps you retrain the Inception architecture to deal with a more specific image problem. Inception is the name of a family of neural architectures built by Google to deal with image recognition problems. Inception is available “pre-trained” which means the training phase has been completed and the weights are set. Most often for image recognition networks, they have been trained on ImageNet, a dataset containing over 20,000 different types of objects (hotdogs are one of them). However, much like Google Cloud’s Vision API, ImageNet training rewards breadth as much as depth here, and out-of-the-box accuracy on a single one of the 20,000+ categories can be lacking. As such, retraining (also called “transfer learning”) aims to take a full-trained neural net, and retrain it to perform better on the specific problem you’d like to handle. This usually involves some degree of “forgetting”, either by excising entire layers from the stack, or by slowly erasing the network’s ability to distinguish a type of object (e.g. chairs) in favor of better accuracy at recognizing the one you care about (i.e. hotdogs).While the network (Inception in this case) may have been trained on the 14M images contained in ImageNet, we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition.The big advantage of transfer learning are you will get better results much faster, and with less data than if you train from scratch. A full training might take months on multiple GPUs and require millions of images, while retraining can conceivably be done in hours on a laptop with a couple thousand images.One of the biggest challenges we encountered was understanding exactly what should count as a hotdog and what should not. Defining what a “hotdog” is ends up being surprisingly difficult (do cut up sausages count, and if so, which kinds?) and subject to cultural interpretation.Similarly, the “open world” nature of our problem meant we had to deal with an almost infinite number of inputs. While certain computer-vision problems have relatively limited inputs (say, x-rays of bolts with or without a mechanical default), we had to prepare the app to be fed selfies, nature shots and any number of foods.Suffice to say, this approach was promising, and did lead to some improved results, however, it had to be abandoned for a couple of reasons.First The nature of our problem meant a strong imbalance in training data: there are many more examples of things that are not hotdogs, than things that are hotdogs. In practice this means that if you train your algorithm on 3 hotdog images and 97 non-hotdog images, and it recognizes 0% of the former but 100% of the latter, it will still score 97% accuracy by default! This was not straightforward to solve out of the box using TensorFlow’s retrain tool, and basically necessitated setting up a deep learning model from scratch, import weights, and train in a more controlled manner.At this point we decided to bite the bullet and get something started with Keras, a deep learning library that provides nicer, easier-to-use abstractions on top of TensorFlow, including pretty awesome training tools, and a class_weights option which is ideal to deal with this sort of dataset imbalance we were dealing with.We used that opportunity to try other popular neural architectures like VGG, but one problem remained. None of them could comfortably fit on an iPhone. They consumed too much memory, which led to app crashes, and would sometime takes up to 10 seconds to compute, which was not ideal from a UX standpoint. Many things were attempted to mitigate that, but in the end it these architectures were just too big to run efficiently on mobile.To give you a context out of time, this was roughly the mid-way point of the project. By that time, the UI was 90%+ done and very little of it was going to change. But in hindsight, the neural net was at best 20% done. We had a good sense of challenges & a good dataset, but 0 lines of the final neural architecture had been written, none of our neural code could reliably run on mobile, and even our accuracy was going to improve drastically in the weeks to come.The problem directly ahead of us was simple: if Inception and VGG were too big, was there a simpler, pre-trained neural network we could retrain? At the suggestion of the always excellent Jeremy P. Howard (where has that guy been all our life?), we explored Xception, Enet and SqueezeNet. We quickly settled on SqueezeNet due to its explicit positioning as a solution for embedded deep learning, and the availability of a pre-trained Keras model on GitHub (yay open-source).So how big of a difference does this make? An architecture like VGG uses about 138 million parameters (essentially the number of numbers necessary to model the neurons and values between them). Inception is already a massive improvement, requiring only 23 million parameters. SqueezeNet, in comparison only requires 1.25 million.This has two advantages:There are tradeoffs of course:During this phase, we started experimenting with tuning the neural network architecture. In particular, we started using Batch Normalization and trying different activation functions.After adding Batch Normalization and ELU to SqueezeNet, we were able to train neural network that achieve 90%+ accuracy when training from scratch, however, they were relatively brittle meaning the same network would overfit in some cases, or underfit in others when confronted to real-life testing. Even adding more examples to the dataset and playing with data augmentation failed to deliver a network that met expectations.So while this phase was promising, and for the first time gave us a functioning app that could work entirely on an iPhone, in less than a second, we eventually moved to our 4th & final architecture.Our final architecture was spurred in large part by the publication on April 17 of Google’s MobileNets paper, promising a new neural architecture with Inception-like accuracy on simple problems like ours, with only 4M or so parameters. This meant it sat in an interesting sweet spot between a SqueezeNet that had maybe been overly simplistic for our purposes, and the possibly overwrought elephant-trying-to-squeeze-in-a-tutu of using Inception or VGG on Mobile. The paper introduced some capacity to tune the size & complexity of network specifically to trade memory/CPU consumption against accuracy, which was very much top of mind for us at the time.With less than a month to go before the app had to launch we endeavored to reproduce the paper’s results. This was entirely anticlimactic as within a day of the paper being published a Keras implementation was already offered publicly on GitHub by Refik Can Malli, a student at Istanbul Technical University, whose work we had already benefitted from when we took inspiration from his excellent Keras SqueezeNet implementation. The depth & openness of the deep learning community, and the presence of talented minds like R.C. is what makes deep learning viable for applications today — but they also make working in this field more thrilling than any tech trend we’ve been involved with.Our final architecture ended up making significant departures from the MobileNets architecture or from convention, in particular:So how does this stack work exactly? Deep Learning often gets a bad rap for being a “black box”, and while it’s true many components of it can be mysterious, the networks we use often leak information about how some of their magic work. We can look at the layers of this stack and how they activate on specific input images, giving us a sense of each layer’s ability to recognize sausage, buns, or other particularly salient hotdog features.Data quality was of the utmost importance. A neural network can only be as good as the data that trained it, and improving training set quality was probably one of the top 3 things we spent time on during this project. The key things we did to improve this were:The final composition of our dataset was 150k images, of which only 3k were hotdogs: there are only so many hotdogs you can look at, but there are many not hotdogs to look at. The 49:1 imbalance was dealt with by saying a Keras class weight of 49:1 in favor of hotdogs. Of the remaining 147k images, most were of food, with just 3k photos of non-food items, to help the network generalize a bit more and not get tricked into seeing a hotdog if presented with an image of a human in a red outfit.Our data augmentation rules were as follows:These numbers were derived intuitively, based on experiments and our understanding of the real-life usage of our app, as opposed to careful experimentation.The final key to our data pipeline was using Patrick Rodriguez’s multiprocess image data generator for Keras. While Keras does have a built-in multi-threaded and multiprocess implementation, we found Patrick’s library to be consistently faster in our experiments, for reasons we did not have time to investigate. This library cut our training time to a third of what it used to be.The network was trained using a 2015 MacBook Pro and attached external GPU (eGPU), specifically an Nvidia GTX 980 Ti (we’d probably buy a 1080 Ti if we were starting today). We were able to train the network on batches of 128 images at a time. The network was trained for a total of 240 epochs, meaning we ran all 150k images through the network 240 times. This took about 80 hours.We trained the network in 3 phases:While learning rates were identified by running the linear experiment recommended by the CLR paper, they seem to intuitively make sense, in that the max for each phase is within a factor of 2 of the previous minimum, which is aligned with the industry standard recommendation of halving your learning rate if your accuracy plateaus during training.In the interest of time we performed some training runs on a Paperspace P5000 instance running Ubuntu. In those cases, we were able to double the batch size, and found that optimal learning rates for each phase were roughly double as well.Even having designed a relatively compact neural architecture, and having trained it to handle situations it may find in a mobile context, we had a lot of work left to make it run properly. Trying to run a top-of-the-line neural net architecture out of the box can quickly burns hundreds megabytes of RAM, which few mobile devices can spare today. Beyond network optimizations, it turns out the way you handle images or even load TensorFlow itself can have a huge impact on how quickly your network runs, how little RAM it uses, and how crash-free the experience will be for your users.This was maybe the most mysterious part of this project. Relatively little information can be found about it, possibly due to the dearth of production deep learning applications running on mobile devices as of today. However, we must commend the Tensorflow team, and particularly Pete Warden, Andrew Harp and Chad Whipkey for the existing documentation and their kindness in answering our inquiries.Instead of using TensorFlow on iOS, we looked at using Apple’s built-in deep learning libraries instead (BNNS, MPSCNN and later on, CoreML). We would have designed the network in Keras, trained it with TensorFlow, exported all the weight values, re-implemented the network with BNNS or MPSCNN (or imported it via CoreML), and loaded the parameters into that new implementation. However, the biggest obstacle was that these new Apple libraries are only available on iOS 10+, and we wanted to support older versions of iOS. As iOS 10+ adoption and these frameworks continue to improve, there may not be a case for using TensorFlow on device in the near future.If you think injecting JavaScript into your app on the fly is cool, try injecting neural nets into your app! The last production trick we used was to leverage CodePush and Apple’s relatively permissive terms of service, to live-inject new versions of our neural networks after submission to the app store. While this was mostly done to help us quickly deliver accuracy improvements to our users after release, you could conceivably use this approach to drastically expand or alter the feature set of your app without going through an app store review again.There are a lot of things that didn’t work or we didn’t have time to do, and these are the ideas we’d investigate in the future:Finally, we’d be remiss not to mention the obvious and important influence of User Experience, Developer Experience and built-in biases in developing an AI app. Each probably deserve their own post (or their own book) but here are the very concrete impacts of these 3 things in our experience.UX (User Experience) is arguably more critical at every stage of the development of an AI app than for a traditional application. There are no Deep Learning algorithms that will give you perfect results right now, but there are many situations where the right mix of Deep Learning + UX will lead to results that are indistinguishable from perfect. Proper UX expectations are irreplaceable when it comes to setting developers on the right path to design their neural networks, setting the proper expectations for users when they use the app, and gracefully handling the inevitable AI failures. Building AI apps without a UX-first mindset is like training a neural net without Stochastic Gradient Descent: you will end up stuck in the local minima of the Uncanny Valley on your way to building the perfect AI use-case.DX (Developer Experience) is extremely important as well, because deep learning training time is the new horsing around while waiting for your program to compile. We suggest you heavily favor DX first (hence Keras), as it’s always possible to optimize runtime for later runs (manual GPU parallelization, multi-process data augmentation, TensorFlow pipeline, even re-implementing for caffe2 / pyTorch).Even projects with relatively obtuse APIs & documentation like TensorFlow greatly improve DX by providing a highly-tested, highly-used, well-maintained environment for training & running neural networks.For the same reason, it’s hard to beat both the cost as well as the flexibility of having your own local GPU for development. Being able to look at / edit images locally, edit code with your preferred tool without delays greatly improves the development quality & speed of building AI projects.Most AI apps will hit more critical cultural biases than ours, but as an example, even our straightforward use-case, caught us flat-footed with built-in biases in our initial dataset, that made the app unable to recognize French-style hotdogs, Asian hotdogs, and more oddities we did not have immediate personal experience with. It’s critical to remember that AI do not make “better” decisions than humans — they are infected by the same human biases we fall prey to, via the training sets humans provide.Thanks to: Mike Judge, Alec Berg, Clay Tarver, Todd Silverstein, Jonathan Dotan, Lisa Schomas, Amy Solomon, Dorothy Street & Rich Toyon, and all the writers of the show — the app would simply not exist without them.Meaghan, Dana, David, Jay, and everyone at HBO. Scale Venture Partners & GitLab. Rachel Thomas and Jeremy Howard & Fast AI for all that they have taught me, and for kindly reviewing a draft of this post. Check out their free online Deep Learning course, it’s awesome! JP Simard for his help on iOS. And finally, the TensorFlow team & r/MachineLearning for their help & inspiration.… And thanks to everyone who used & shared the app! It made staring at pictures of hotdogs for months on end totally worth it 😅",26/06/2017,0,19,20,"(688, 521)",9,12,0.0,41,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,subjective,positive,joy/calmness
120,"Command-Line Cartography, Part 1",,Mike Bostock,24000.0,5.0,719,"[This is Part 1 of a tutorial on making thematic maps. Read Part 2 here.]This multipart tutorial will teach you to make a thematic map from the command line using d3-geo, TopoJSON and ndjson-cli—free, open-source tools written in JavaScript. We’ll make a choropleth of California’s population density. (For added challenge, substitute your state of choice!)The first part of this tutorial focuses on getting geometry (polygons) and converting this geometry into a format that can be easily manipulated on the command-line and displayed in a web browser.The U.S. Census Bureau regularly publishes cartographic boundary shapefiles. Unlike TIGER—the Census Bureau’s most-detailed and comprehensive geometry product—the “cartographic boundary files are simplified representations… specifically designed for small scale thematic mapping.” In other words, they’re perfect for a humble choropleth.The Census Bureau, as you might guess, also publishes data from their decennial census, the more frequent American Community Survey, and other surveys. To get a sense of the wealth of data the Census Bureau provides, visit the American FactFinder or the friendly Census Reporter. Now we must choose a few parameters:It’s necessary to determine these parameters first because the geometry must match the data: if our population estimates are per census tract, we’ll need census tract polygons. More subtly, the year of the survey should match the geometry: while boundaries change relatively infrequently, they do change, especially with smaller entities such as tracts.The Census Bureau helpfully provides guidance on picking the right data. Census tracts are small enough to produce a detailed map, but big enough to be easy to work with. We’ll use 5-year estimates, which are recommended for smaller entities and favor precision over currency. 2014 is the most recent release at the time of writing.Now we need a URL! That URL can be found through a series of clicks from the Census Bureau website. But forget that, and just browse the 2014 cartographic boundary files here:Given a state’s FIPS code (06 for California), you can now use curl to download the corresponding census tract polygons:Next, unzip the archive to extract the shapefile (.shp), and some other junk:(You should already have curl and unzip installed, as these are included in most operating systems. You will also need node and npm; on macOS, I recommend Homebrew to install software.)A quick way to check what’s in a shapefile is to visit mapshaper.org and drag the shapefile into your browser. If you do that with the downloaded cb_2014_06_tract_500k.shp, you should see something like this:As Mapshaper demonstrates, it’s possible to view shapefiles directly in your browser. But binary shapefiles can be difficult to work with, so we’ll convert to GeoJSON: a web-friendly, human-readable format. My shapefile parser has a command-line interface, shp2json, for this purpose. (Warning: there’s an unrelated package of the same name on npm.) To install:Now use shp2json to convert to GeoJSON:Note that this also reads cb_2014_06_tract_500k.dbf, a dBASE file, defining feature properties on the resulting GeoJSON. The glorious result:We could now display this in a browser using D3, but first we should apply a geographic projection. By avoiding expensive trigonometric operations at runtime, the resulting GeoJSON renders much faster, especially on mobile devices. Pre-projecting also improves the efficacy of simplification, which we’ll cover in part 3. To install d3-geo-projection’s command-line interface:Now use geoproject:This d3.geoConicEqualArea projection is California Albers, and as its name suggests, is appropriate for showing California. It’s also equal-area, which is strongly recommended for choropleth maps as the projection will not distort the data. If you’re not sure what projection to use, try d3-stateplane or search spatialreference.org.The projection you specify to geoproject is an arbitrary JavaScript expression. That means that you can use projection.fitSize to fit the input geometry (represented by d) to the desired 960×960 bounding box!To preview the projected geometry, use d3-geo-projection’s geo2svg:If you followed along on the command line, you hopefully learned how to download and prepare geometry from the U.S. Census Bureau.In part 2 of this tutorial, I’ll cover using ndjson-cli to join the geometry with population estimates from the Census Bureau API and to compute population density.In part 3, I’ll cover simplifying geometry and merging features using topojson-server, topojson-simplify and topojson-client.In part 4, I’ll cover implementing effective color encodings using d3-scale, and rendering the choropleth to SVG using d3-geo.Ready for more? Continue to Part 2.Questions or comments? Reply below or on Twitter. Thank you for reading!",10/12/2016,8,9,13,"(700, 522)",6,1,0.0,53,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
121,Multivariate Outlier Detection in Python using R’s DBSCAN,,mohit tewari,2.0,5.0,1112,"Outliers present in data can highly affect the result of machine learning algorithms. Outliers are the elements which depict different behavior as compared to rest of the population. In machine learning, outliers can be either univariate or multivariate. There are multiple approaches for detection of univariate and multivariate outlier detection and many blogs are available on internet providing high level implementation of such approaches. In this blog, I am going to discuss how and why I implemented R’s DBSCAN in python. You can refer python-dbscan (sklearn’s documentation) and r-dbscan for more information about DBSCAN.Multivariate outlier is the record having unusual combination of feature values. Suppose we have a dataset having feature variables X1, X2, X3 and X4. There can be some records in dataset which can contain unusual values in X1,X2,X3 or X4, i.e., values that vary drastically from those present in rest of the records. For example , consider a dataset where values of features lie in range (0–10) for majority of records. This dataset also have few records which have out of range values(value<0 or value>10) in those features. These few records are considered as outlier. Other than having out of range values, records can also be classified outlier when the combination of values is unusual. In this case, features may have value within 0–10 range but the combination of values is not normal. For example, let us consider that value of X1 is either 2 or 3 when value of X2,X3 is 5,6 respectively but for some records, value of X1 is 9 while values of X2,X3 is same as pervious. Here none of the features are having out of range value but this records is still called outlier as the combination is different than expected. Since, being outlier for records depends on combination of values of multiple features (X1,X2,X3,X4) , these outliers are called Multivariate Outliers.Clustering algorithms can be used for grouping similar data points present in unlabeled data and classify ungrouped data points as outlier. Clustering algorithms techniques which can be used for outlier detection are :Centroid Based : Centroid based clustering algorithms determine the similarity between data points in a cluster by their closeness with the centroid. Clusters are formed by assigning each data point to nearest cluster center , then recalculating the center and reassigning data points to nearest newly calculated centroid. This process is done iteratively and the points which don’t belong to any cluster are considered outliers. Example — K-MeansDensity Based : Density based clustering algorithms look for varied density regions in data space. Density regions are separated from each other and the data points in each region are assigned to each cluster. Those records/objects who don’t lie in any density region are classified as outlier since they don’t belong to any cluster. This method partitions data space. Example — DBSCANK-Means clustering algorithm is based on centroid based approach and forms clusters which are circular in shape. Centroid of the cluster shifts if outliers are present in training data which in turn increases loss function. The loss function is the sum of squared distances from each point to its assigned cluster centroid. This value increases as the distance between outlier and centroid is high.Another reason for avoiding K-Means for multivariate outlier detection is having prior information of number of clusters that are needed to be formed. In our case of outlier detection, there was no predetermined number, i.e. K, for total clusters needed.DBSCAN is a density based clustering algorithm that divides a dataset into subgroups/clusters of high density regions.Unlike K-Means, DBSCAN can discover arbitrarily shaped cluster, i.e., clusters need not be circular. Cluster formation in DBSCAN isn’t affected by outliers because clusters formation depends on the density regions of data space. Outliers being quite few in number, belong to a separate cluster having least density region.DBSCAN stands for Density Based spatial clustering of applications for noise. DBSCAN is implemented as tranductive learning approach in python’s scikit-learn. In my case, I needed to implement DBSCAN as inductive learning to avoid re-running DBSCAN on new records incoming in real time. I needed to tag new records to pre-determined clusters using pre-trained DBSCAN model. In python’s DBSCAN, there is no predict function available but only fit-predict. So it can only assign cluster when it runs from scratch on new data. Therefore sklearn’s DBSCAN can’t serve the purpose here. Below is some information how transductive learning differs from inductive learning.In inductive learning a function f(x) is learned which is capable of predicting the correct output for the training data (observed examples) and it is assumed to be able to correctly predict the corresponding labels of unseen data. In transductive learning, instead of aiming to compute a function f(x) that will perform well on unseen data, the aim is to calculate directly the output on seen, albeit unlabeled data.Unlike python, predict method is available in R’s implementation of DBSCAN. There is a library named ‘rpy2’ in python which facilitates exposing R objects to python code. Using rpy2, ‘dbscan’ package can be imported which has dbscan function in it. Below is the code snippet for importing R packages in python.I implemented the above code and tried running on test data. It worked fine for smaller data (upto 50K records) but it failed to run on larger dataset. When running on bigger dataset, it ended up consuming all the RAM available. In my case, I was running this on VM having 26 GB(out of 32 GB) unused RAM, linux OS (Ubuntu 18.04.2 LTS), CPU with 2 cores.Since R’s dbscan package consumed didn’t work for larger datasets, I used dbscan available in ‘fpc’ package. ‘fpc’ package also has dbscan method and it also provides control over speed vs memory tradeoff. In fpc’s dbscan, there is an extra argument named ‘method’ which accepts one of three values :dist : “dist” treats data as distance matrix (relatively fast but memory expensive)“raw” treats data as raw data and avoids calculating a distance matrix (saves memory but may be slow)“hybrid” expects also raw data, but calculates partial distance matrices (very fast with moderate memory requirements).In my case, I set method=’raw’ as I had larger test dataset . My dataset had more than 5,00,000 rows and model kept running for nearly 16 hours during training on non-GPU VM (configurations mentioned above). Below is the code snippet for dbscan in fpc:Please note that training dataframe and test dataframe first need to be converted to python’s dataframe to R’s dataframe before passing them to model. This can be done by either calling py2ri function or by performing scaling operation on python’s dataframe. Scaling operation converts python dataframe to numpy array which is accepted by R’s function as well.",12/09/2019,3,0,2,,0,0,0.0,2,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
122,7 of the Most Used Regression Algorithms and How to Choose the Right One,Towards Data Science,Dominik Polzer,179.0,27.0,5485,"Regression is a subset of Supervised Learning. It learns a model based on a training dataset to make predictions about unknown or future data. The description ‘supervised’ comes from the fact that the target output value is already defined and part of the training data. The difference between the subcategories Regression and Classification is only due to the output value. While Classification divides the dataset into classes, Regression is used to output continuous values. [Ras16]This article introduces a few of the most used Regression methods, explains some metrics to evaluate the performance of the models and describes how the model building process works.Linear regression models assume that the relationships between input and output variables are linear. These models are quite simplistic, but in many cases provide adequate and tractable representations of the relationships. The model aims a prediction of real output data Y by the given input data X = (x_1, x_2, …, x_p) and has the following form:β describes initially unknown coefficients. Linear models with more than one input variable p > 1 are called multiple linear regression models. The best known estimation method of linear regression is the least squares method. In this method, the coefficients β = β_0, β_1…, β_p are determined in such a way that the Residual Sum of Squares (RSS) becomes minimal.Here, y_i-f(x_i) describes the residuals, β_0 the estimate of the intercept term, and β_j the estimate of the slope parameter [Has09, p.44].Polynomial RegressionBy transforming the input variables, e.g. by the logarithm function, the root function etc., non-linear and polynomial relationships can be represented. Nevertheless, these are linear models, as this designation is based on the linearity of the input parameters. [Has09, p.44]The modelling of such correlations is done using so-called trend models. If the rough course is already evident from the data, regression approaches can be specified. [Fah16, p.512] The following table shows frequently used trend models for simple linear regression.A direct function for polynomial regression does not exist, at least not in Scikit-learn. For the implementation the pipeline function is used. This module combines several transformer and estimation methods in a chain and thereby allows the fixed sequence of steps in the processing of the data. [Sci18g]The difference to the previously known linear regression is the preliminary step. The function PolynomialFeatures creates a new matrix containing all polynomial combinations of the features of the input matrix X. [Sci18h][Sci18]The following code snippet:transforms the input vector X as follows:The function of the linear model is then:The following snippet shows the application of Polynomial Regression in scikit-learn. The pipeline function is not absolutely necessary here, the transformation of the input matrix X and the subsequent model building can also be executed by the corresponding commands one after the other. However, the pipeline function is required if polynomial model building is applied within the Cross-validation function (more in the evaluation section of this article).Polynomial regression allows control of model complexity via the polynomial degree. Parameters that are set by the user before the algorithm is executed are called hyperparameters. Most regression methods include several hyperparameters, which significantly influence the accuracy of the resulting regression model. You can find a explanation how to find the optimal hyperparameters in the section “Model evaluation”.The following example shows a Polynomial Regression model with the polynomial degree 2. The model resulted from an attempt to predict the energy consumptio of a milling machine. The target value y is the energy consumption [kJ], the used attributes are the axis rotation speed [1/min] and the feed rate [mm/min].Regression procedures based on least squares estimation are very susceptible to outliers because the variances are evaluated quadratically. The figure below illustrates the effect of a single outlier on the result of a linear regression.Robust regression methods circumvent this weakness. The term “robustness” describes the ability of a static method to model distributions that do not correspond to the normal distribution. [Wie12] A measure of robustness is the so-called “breakdown point”, which indicates the proportion of the data (e.g. Outliers) that is tolerated by the statistical method. [Hub05]Probably the best known Robust Regression algorithm is the Random Sample Consensus (RANSAC) algorithm, introduced in 1981 by Martin Fischler and Robert Bolles. RANSAC is widely used in the field of machine vision. [Fis80]The operation of the algorithm can be explained in five steps, which are executed iteratively. (1) At the beginning, the procedure selects a random sample from the data and uses it for model building. In the following figure, the sample includes the two circled data points. (2) The error of all data points to the model f(x) are then calculated and compared to the user-defined threshold. If the deviation is below this value, the data point is considered an inlier.(3) This process is repeated until a specified number of iterations have been run or a specified performance of the model has been achieved. As model results is the function that yields the most inliers. [Ras18]The following Python snippet describes the implementation using scikit-learn. The maximum number of iterations is set to four and the minimum sample size to two. These values are adjusted depending on the dataset size. For the determination of the inliers, the absolute values of the vertical distances between the data points and the regression line is calculated. [Ras18][Sci18b]The following figure shows an example of iterative model building after executing the code snippet above. Iteration 2 shows the best performance of the model.The number of iterations n required for an outlier-free subset to be selected from the data points at least once with a certain probability p can be determined as follows [Rod04][Dan18]:Assuming that the relative proportion of outliers δ is about 10%, the number of iterations n at which an outlier-free subset is selected at least once with a probability of p = 99% is calculated to:A Decision Tree grows by iteratively splitting tree nodes until the ‘leaves’ contain no more impurities or a termination condition is reached. The creation of the Decision Tree starts at the root of the tree and splits the data in a way that results in the largest Information Gain IG. [Ras18, p.107][Aun18][Has09, p.587][May02][Sci18c]In general, the Information Gain IG of a feature a is defined as follows [Qui86][Bel15, p.47][Ras18, p.107]:In binary Decision Trees, the division of the total dataset D_p by attribute a into D_leftand D_rightis done. Accordingly, the information gain is defined as:The algorithm aims at maximizing the information gain, i.e. the method wants to split the total dataset in such a way that the impurity in the child nodes is reduced the most.While classification uses entropyor the Gini coefficient as a measure of impurity, regression uses, for example, the Mean Squared Error (MSE) as a measure of the impurity of a node. [Ras18, p.347].Splitting methods that use the Mean Squared Error to determine impurity are also called variance reduction methods. Usually, the tree size is controlled by the maximum number of nodes max_depth, at which the division of the dataset stops. [Has09, S.307]The visualization of the nodes and leaves of the decision tree can be done using the graphviz function:The following figure shows the result of the Decision Tree for a simple dataset. The method splits the dataset into two partial subsets (left and right) in such a way that the variance is reduced as much as possible. For the dataset shown, the limit at which the dataset is split the first time is 6.5.By merging several uncorrelated Decision Trees, often a significant improvement of the model accuracy can be achieved. This method is called Random Forest. The trees are influenced by certain random processes (randomization) as they grow. The final model reflects an averaging of the trees.Different methods of randomization exist. According to Breiman, who coined the term ‘Random Forest’ in 1999, random forests are established according to the following procedure. First, a random sample is chosen from the total dataset for each tree. As the tree grows, a selection of a subset of the features takes place at each node. These serve as criteria for splitting the dataset. The target value is then determined for each Decision Tree individually. The averaging of these predictions represents the prediction of the Random Forest. [Bre01][Jam13]The Random Forest has a number of hyperparameters. The most crucial one, besides the maximum depth of the trees max_depth, is the number of decision trees n_estimators. By default, the Mean Square Error (MSE) is used as criterion for splitting the dataset as the trees grow. [Sci18d]The following figure shows an example model for the Random Forest. The way it works results in the characteristic “step” form.The Gaussian Process captures the typical behavior of a system on the basis of observations of a system and delivers as a result a probability distribution of possible interpolation functions for the problem at hand.The Gaussian Process Regression makes use of the Bayes’ theorem in the following, which is why it should be briefly explained in advance.In general, the Bayes’ theorem is defined as follows:It allows the inference from known values to unknown values. A often used application example is the disease detection. In the case of rapid tests, for example, one is interested in how high the actual probability is, that a positive tested person actual has the disease. [Fah16]In the following, we will apply this principle to the Gaussian Process.The Gaussian Process is defined by the expected value for each random variable, the mean function m(x) and a covariance function k(x,x´).The mean function m(x) reflects the priori function for the problem at hand and is based on known trends or biases in the data. If the expected value (mean function) is constant 0, it is called a centered Gaussian process.The covariance function k(x, x´) , also called ‘kernel’, describes the covariance of the random variables x and x′ . These functions are analytically defined.The kernel defines the shape and the course of the model functions and is used to describe, for example, abstract properties such as smoothness, roughness and noise. More kernels can be combined by certain computational rules to emulate systems with superimposed properties. [Ebd08][Kuß06] [Ras06][Vaf17]In the following three of the most used kernels will be presented:Squared Exponential KernelA popular Kernel is the Squared Exponential Kernel (Radial Basis Function) and has established itself as the ‘standard kernel’ for the Gaussian Process and the Support Vector Machine. [Sci18l]The following figures show an example for an A-priori-Gaussian process p(f) through the mean function m(x)(black line) and the confidence interval (gray background). Generally, the confidence interval indicates the range, given an infinite repetition of a random experiment, with some probability, the true location of the parameter lies [Fah16][Enc18]. In this case, the boundaries of the confidence interval are defined by the standard deviation σ.The colored curves represent some random functions of the Gaussian Process. The example curves serve only to abstract the form of the possible output functions. In principle, an infinite number of these curves could be created.The kernel has only two hyperparameters:The following figure shows the effects of the hyperparameters on the A-Priori-Gaussian-Process and its functions.Rational Quadratic KernelThe Rational Quadratic Kernel can be seen as a combination of several Squared Exponential Kernels with different length_scale settings (l). The parameter α determines the relative weighting of the 'large-scale' and 'small-scale' functions. When α approaches infinity, the Rational Quadratic Kernel is identical to the Squared Exponential Kernel. [Duv14][Sci18k][Mur12]Periodic KernelThe Periodic Kernel allows functions to repeat themselves. The period p describes the distance between the repetitions of the function. The use of the ‘lenghtscale' parameter (l) is as mentioned earlier. [Sci18j]The kernel funnction and mean function together describe the A-priori-Gaussian-Process. With the help of some measured values, a A-posteriori-Gaussian-Process can be defined, which takes into account all available information about the problem. More precisely, no single solution results, but all possible functions of the interpolations, which are weighted with different probabilities. In the case of a regression task, specifically, the solution (function) with the highest probability is crucial. [Ras06][Wik18a][Wik18a]For regression, typically a dataset with values of the independent variable X ∈ R and associated values of the dependent variable f ∈ R is given and one wants to predict output values f∗ for new values X∗. [Vaf17]For the simplest case, a process without noise, the multidimensional Gaussian distribution is defined as follows:The covariance matrix can be divided into four parts. The covariance within the unknown values K_X∗X∗ , the covariance between the unknown and known K_X∗X values, and the covariance within the known values K_XX.Since f is completely known, substituting the probability density into Bayes’ theorem yields the a-posterior-Gaussian-distribution.A detailed derivation is given by Rasmussen in his book ‘Gaussian Processes for Machine Learning’. [Ras06, p.8 ff.]From priori to posteriori Gaussian Process: Explained with a simple exampleIn practice, a number of other kernels are used, including combinations of several kernel functions. The constant kernel for example is usually used in conjunction with others. Using this kernel without the combination with other kernels, usually makes no sense, because only constant correlations can be modeled. Nevertheless, in the following the constant kernel is used to explain and illustrate the Gaussian-process regression in a simple way.The figure below shows the a-priori-Gaussian-Process with a variance of one. By defining the constant kernel as the covariance function, all sample functions show a parallel line to the x-axis.Since no statement is made in advance about a possible noise of the measured data, the process assumes that the given measurement point is a part of the true function. This limits the number of possible function equations to the straight line that passes directly through the point. Since the constant kernel only allows horizontal lines, the number of possible lines in this simple case narrows down to exactly one possible function. Thus the covariance of the a-posteriori-Gaussian process is zero.With the RBF Kernel, arbitrary processes can be mapped, but this time the result is not a single straight line as A-posteriori-Gaussian, but a multitude of functions. The function with the highest probability is the mean function of the A-posteriori-Gaussian-Process. The following figure shows the A-posteriori-Gaussian-Process and the used measurement points.To implement the Gaussian Process in Python, the A-priori-Gaussian-Process must be defined in advance. The mean function m(x) is usually assumed to be constant and zero. By setting a parameter normalize_y = True, the process uses the mean of the dataset values as the constant expected value function. The choice of the covariance function is made by choosing the kernel. [Sci18m]Gaussian Process Regression in Scikit-learnThe following source code describes how to implement the Gaussian Process Regression with scikit learn and the RBF Kernel used as covariance function. The first optimization process starts from the pre-set values (length_scaleand variance) of the kernel. By the parameter alpha an assumption can be made in advance about the strength of the noise of the training data.Optimization process: Hyperparameter erstimation using maximum-likelihood methodThe hyperparameters are optimized during the fitting of the model by maximizing the log-marginal likelihood (LML). Maximum Likelihood Estimation (MLE) is a method for determining the parameters of a statistical model. While the Regression methods already presented, such as Linear Regression, aim to minimize the Mean Square Error, the Gaussian-Process Regression tries to maximize the likelihood function. In other words, the parameters of the model are selected in such a way that the observed data appear most plausible according to their distribution.In general, the probability function f of a random variable X is defined as:This distribution is assumed to depend on a parameter ϑ. Given observed data, the probability can be considered as a function of ϑ :Maximum likelihood estimators aim to maximize this function. Maxima are usually identified by differentiating the function and then setting it equal to zero. Since the log likelihood function has its maximum at the same point as the likelihood function, but is easier to calculate, it is usually used. [Goo16, p.128]One speaks of a normal or Gaussian distribution if a random variable X has the following probability density [Fah16, p.83]:The maximum-likelihood method will be explained in the following using a simple one-dimensional example. The figure below shows the dataset. All three plotted probability distributions reflect the distribution of the data. With maximum likelihood, one is interested in the distribution that is most likely.The goal is to define the parameters σ² and µ in such a way that the probability is maximized over all data points considered. For example, given three data points with x-values 9, 10 and 13, similar to the data points in the figure, the joint probability is calculated from the individual probabilities as follows:This function must be maximized. The mean value then corresponds to the x-value that is most likely to occur.The following figure shows an example model for the Gaussian Process Regression.The functionality of the Support Vector Regression (SVR) is based on the Support Vector Machine (SVM) and will first be explained with a simple example. We are looking for the linear function:⟨w, x⟩ describes the cross product. The goal of SV Regression is to find a straight line as model for the data points whereas the parameters of the straight line should be defined in such a way that the line is as ‘flat’ as possible. This can be achieved by minimizing the norm: [Wei18][Ber85]For the model building process it does not matter how far the data points are from the modeled straight line as long as they are within a defined range (-ϵ to +ϵ). Deviations that exceed the specified limit ϵ are not allowed.These conditions can be described as a convex optimization problem:To this optimization problem finds a solution if all data points can be approximated within an accuracy of ϵ (The figure above shows a simple example for this scenario). However, this is a simplified assumption and usually does not hold in practice. To be able to circumvent unsolvable optimization problems, the variables ζi, ζ∗are introduced —the so called slack variables.The figure above describes the “punishment ”of deviations exceeding the amount of ϵ using a linear loss function. The loss function is called the kernel. Besides the linear kernel, the polynomial or RBF kernel are frequently in use. [Smo04][Yu12][Bur98]Thus, the formulation according to Vapnik is as follows:The constant C describes the trade-off between the condition of flatness and the deviations greater than ϵ that are tolerated.In order to be able to model non-linear relationships with the Support Vector Regression as well, the so-called ‘kernel trick’ is used. Therefore the original characteristics are mapped into a higher-dimensional space. [Pai12]The implementation with scikit-learn and the RBF kernel looks like this:The following figure provides an overview of the regression methods presented and a brief summary of how they work.There are various methods and procedures to evaluate the accuracy of a model.The sklearn.metrics module includes several loss and evaluation functions to measure the quality of the regression models. Mean Squared Error (MSE) is a key criterion for assessing the quality of a regression model [Ras18, p.337]. If yˆ_i describes the value predicted by the model at the i-th data sample, and y_i describes the corresponding true value, then the Mean Squared Error (MSE) of the model over n_Samples is described as [Sci18a]:Another parameter for determining the accuracy of regression models is the Mean Absolute Error (MAE).Both metrics can be found in the module sklearn.metrics . They compare the predicted and actual values for the test dataset.Coefficient of determination (R²)The so-called coefficient of determination (R²) can be understood as a standardized version of the MSE. This allows an easier interpretation of the performance of the model. The best possible performance is described with the value 1.0. The R² -score can also become negative if the model shows arbitrary deviations from the truth value. A constant model, which makes the prediction of the values without the consideration of the input characteristics, would receive a R² -score of 0.0.If yˆ_i describes the value predicted by the model at the i-th data sample, and y_i describes the associated true value, then the coefficient of determination R² over n_Samples is defined as [Sci18a]:The output in Python is the function r2_score , where y_true is the true value of the dependent variable and y_pred is the value predicted by the model.Cross validation in regressionCross-validation is a statistical method for model selection. To evaluate a method, the entire dataset is divided into a training and a test dataset, whereby the training dataset usually comprises 80 to 90 % of the entire dataset. In order to achieve the best possible evaluation of the model, the aim is to have as large a test dataset as possible. Good model building is achieved by having as large a training dataset as possible.Cross-validation is used to circumvent this dilemma. This method allows the entire dataset to be used for both training and testing. Compared to a fixed division into train and test data, cross-validation thus allows a more accurate estimate of model accuracy for future data or data not included in the dataset.The k-fold cross validation divides the entire dataset X into k equal sized blocks (X_1, …, X_k). Then the algorithm is trained k times on k-1 blocks and tested with the remaining block.Many learning methods allow an adjustment of the model complexity via one or more hyperparameters. This often leads to the problem of over- or underfitting. Cross-validation is used to find the optimal model complexity. The optimal complexity is achieved by minimizing the approximation error on a test dataset that is unknown during learning. [Du14, p. 27][Has09, p. 242]The process already described is carried out for different parameter settings and model complexities. For the final model, the setting parameter (γ_opt) is chosen that shows the lowest error (e.g. MSE or MAE). For smaller training datasets, k can be equated to the number n of feature vectors. This method is called Leave-One-Out Cross-validation. [Ert16, p.233][Bow15, p.100]The implementation with sklearn is done with the module cross_validate . The following code snippet shows the application of cross validation to evaluate the performance of the Linear Regression.The cv value defines the number k of partitions into which the dataset is divided. The Negativ Mean Squared Error was used as the scoring parameter in this case. The squared error is passed to the list scores after each run. After the execution of the program code, scores represents a list with, in this case, three entries, i.e. the Mean Square Error of each regression model. The models differ only in the choice of the test and training dataset, which are varied after each run as described earlier. [Sci18f][Coe13]The function cross_validate uses the scoring parameters of the .metrics module. The following table describes a summary of the so-called scoring parameters used for the evaluation of regression models [Sci18e][Cod18].Functions ending with _score return a value that should be maximized if possible. Functions ending with _error or _loss return a value to minimize.If you take a look at the source code of the sklearn.metrics.scorer module, you can see that for all loss or error functions the parameter greater_is_better is set to FALSE and the scoring parameter are negated and supplemented with the expression neg_. This allows to handle all scoring parameters in the same way. [Cod18][Git18]The following source code shows a typical application example of cross-validation. The example uses Polynomial Regression for modeling, which allows the setting of the model complexity via the specification of the polynomial degree. The image below shows the used dataset and the problem of overfitting. If the evaluation of the model were to be performed using the training data, models with higher complexity usually show higher accuracies. Since the dataset was generated using a sine function, the true function can be used for comparison. For this simple example, you can see at a glance that the polynomial regression model shown with polynomial degree 15 does not correctly represent the regression problem —it is an overfitted model.To be able to determine the “optimal” polynomial degree, Cross-validation is used in the following.A good model is characterized by the lowest possible error of the model when applied to the test dataset. In order to obtain the best possible evaluation of the models for the relative small dataset, a ‘leave-one-out cross-validation’ is performed for different setting parameters. This is done by setting the number of partitions (into which the dataset is divided during cross-validation) to the number of data points. The list of scores obtained from cross-validation includes the Mean Square Error for each run. These values are averaged for an assessment of the used regression method.The left graph of the following figure shows the results of cross-validation for different polynomial degrees. In addition, the error of the model from the training dataset is shown. The error decreases with increasing model complexity. This explains why an evaluation and optimization of the model on the basis of the training data is on the basis of the training data would not be practicable.The Mean Square Error determined by cross-validation shows steadily low values in the range of a polynomial degree of three to seven. More complex systems no longer adequately represent the process, which is why the calculated accuracy for data not included in the training dataset and future data of the process decreases significantly. The optimum is shown at a polynomial degree of three. If the resulting model is plotted, it shows a good approximation to the ‘true function’. (The ‘true function’ of the dataset can be given in this case, because the data points were generated with a random offset to the given cos-function).The figure below shows the schematic flow of the methode selection and subsequent model generation. The feature selection already takes place before the model building and defines the input attributes of the later regression model. The datasets were already structured during the creation in such a way that they only contain relevant attributes.The regression methods are suitable for different problems, differently well. For evaluation, the dataset is split into training and test dataset before model building. This step is omitted in the source code, as this process is automatically performed iteratively during cross-validation. The execution of the cross validation is done by the cross_val_score function of the scikit library.Cross-validation provides an indication of the performance of each regression method. For datasets with a small number of instances, a ‘Leave One Out’ cross-validation is usually performed. For this, the partition number of the cross-validation is set equal to the length of the dataset.Hyperparameter optimization through repetitive cross-validation with different hyperparameter settings:The result of the cross-validation represents a list with the values of the selected scoring parameters. Since the evaluation is performed after each run, if the dataset is divided into five partitions, there is also a list with five evaluation values. An averaging of these values allows an assessment of the performance of the regression procedure. Since most regression methods allow an adjustment of the model complexity via one or more hyperparameters, an adjustment of the hyperparameters is necessary for a meaningful comparison of the regression methods. The finding of these optimal hyperparameter settings is done by iterative model building. The cross-validation is performed repeatedly for different hyperparameter settings. Finally, the parameter settings are chosen which showed the best model accuracy during the evaluation. This process is performed by loops which automatically change the hyperparameters within certain limits and store the evaluation values. The selection of the optimal settings is then done by manual or automated search for the best evaluation results.While Linear Regression does not allow setting the model complexity, most algorithms comprise multiple hyperparameters. For the optimization of the model, it is usually not sufficient to vary only one of the hyperparameters in procedures with several hyperparameter setting options. Care must be taken that the hyperparameters are not exclusively considered individually, since the effects of the parameter changes partly influence each other.The figure below shows a list of some important hyperparameters of the presented methods. Especially for the methods which use kernel functions to find the solution, the number of possible settings goes far beyond the listed ones. For a more detailed description, you can find a comprehensive documentation to the methods and their hyperparameters at: scikit-learn.org.Hope I could give you an overview of different techniques used for regression analysis. Of course, the article does not claim to give a complete picture of regression. Neither with regard to the field of regression nor to the concepts presented. Many important algorithms were not mentioned at all. Nevertheless, these 7 algorithm give you a first good overview of techniques that are used and how they differ from each other in the way they work.If you found the article helpful, you can also find a similar article on concepts and algorithms used for Anomaly Detection:towardsdatascience.comIf you are not yet a Medium Premium member and plan to be, you can support me by signing up via the following referral link:https://dmnkplzr.medium.com/membershipThank you for reading![Aun18] Aunkofer, B. Entscheidungsbaum-Algorithmus ID3 — Data ScienceBlog, 2018. URL https://data-science-blog.com/blog/2017/08/13/entscheidungsbaum-algorithmus-id3/[BB18] Brooks-Bartlett, J. Probability concepts explained: Maximum likelihood estimation, 2018. URL https://towardsdatascience.com/probability/concepts-explained-maximum-likelihood-estimation-c7b4342fdbb134[Bel15] Bell, J. Machine learning: Hands-on for developers and technical professionals. Wiley, Indianapolis Indiana, 2015. ISBN 1118889061[Ber85] Berger, J. O. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics. Springer, New York, NY, second edition Auflage, 1985. ISBN 9781441930743. doi:10.1007/978–1–4757–4286–2. URL http://dx.doi.org/10.1007/978-1-4757-4286-2[Bow15] Bowles, M. Machine Learning in Python: Essential techniques for predictive analysis. John Wiley & Sons Inc, Indianapolis IN, 2015. ISBN 1118961749B[Bre01] Breiman, L. Random Forest. 2001.[Bur98] Burges, C. J. C.; Kaufman, L.; Smola, A. J.; Vapnik, V. Support Vector Regression Machines. 1998. URL http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdfine[Cal03] Callan, R. Neuronale Netze im Klartext. Im Klartext. Pearson Studium, Műnchen and Harlow, 2003. ISBN 9783827370716[Cod18] CodeExamples. 3.3. Modellbewertung: Quantifizierung der Quali[1]tät von Vorhersagen | scikit-learn Documentation | CODE Examples[Coe13] Coelho, L. P.; Richert, W. Building Machine Learning Systems with Python. 2013[Dan18] Daniilidis, K. RANSAC: Random Sample Consensus I — Pose Estimation | Coursera, 2018. URL https://www.coursera.org/lecture/robotics-perception/ransac-random-sample-consensus-i-z0GWq[Du14] Du, K.-L.; Swamy, M. N. S. Neural Networks and Statistical Learning. Springer London, London, 2014. ISBN 978–1–4471–5570–6. doi:10.1007/978–1–4471-5571–3[Duv14] Duvenaud, D. K. Automatic Model Construction with Gaussian Processes. 2014. URL https://www.cs.toronto.edu/~duvenaud/thesis.pdfin[Ebd08] Ebden, M. Gaussian Processes for Regression: A Quick Introduction. 2008.[Enc18] The significance test controversy and the bayesian alternative,21.06.2018. URL https://www.encyclopediaofmath.org/index.php/The_significance_test_controversy_and_the_bayesian_alternative[Ert16] Ertel, W. Grundkurs Künstliche Intelligenz: Eine praxisorientierte Einführung. Computational Intelligence. Springer Fachmedien Wiesbaden GmbH and Springer Vieweg, Wiesbaden, 4., überarb. aufl. 2017 Auflage, 2016. ISBN 9783658135485[Fah16] Fahrmeir, L.; Heumann, C.; Künstler, R. Statistik: Der Weg zur Datenanalyse. Springer-Lehrbuch. Springer Spektrum, Berlin and Heidelberg, 8., überarbeitete und ergänzte auflage Auflage, 2016. ISBN 978–3–662 50371–3. doi:10.1007/978–3–662–50372–0[Fis80] Fischler, M.; Bolles, R. Random Sample Consensus: A paradigm for model fitting with applications to image analysis and automated cartography. 1980. URL http://www.dtic.mil/dtic/tr/fulltext/u2/a460585.pdf0585.p[Git18] GitHub. sklearn.metrics-Quellcode, 2018.[Goo16] Goodfellow, I.; Bengio, Y.; Courville, A. Deep learning. MIT Press, Cambridge, Massachusetts and London, England, 2016. ISBN 9780262035613. URL http://www.deeplearningbook.org/[Has09] Hastie, T.; Tibshirani, R.; Friedman, J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer New York, New York, NY, 2009. ISBN 978–0–387–84857–0. doi:10.1007/b94608[Hub05] Huber, P. J. Robust statistics. Wiley, New York, NY, 2005. ISBN 0–47141805-6[Kuß06] Kuß, M. Gaussian Process Models for Robust Regression, Classification, and Reinforcement Learning. 2006. URL http://tuprints.ulb.tu-darmstadt.dde/epda/000674/GaussianProcessModelsKuss.pdfKu[Mur12] Murphy, K. P. Machine learning: A probabilistic perspective. Adaptive computation and machine learning series. MIT Press, Cambridge, Mass., 2012. ISBN 9780262018029. URL https://ebookcentral.proquest.com/auth/lib/subhh/login.action?returnURL=https%3A%2F%2Febookcentral.proquest.com%2Flib%2Fsubhh%2Fdetail.action%3FdocID%3D3339490[Pai12] Paisitkriangkrai, P. Linear Regression and Support Vector Regression. 2012.URL https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf[Qui86] Quinlan, J. R. Induction of decision trees. Machine Learning, 1(1):81–106, 1986. ISSN 0885–6125. doi:10.1007/BF00116251. URL https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf[Ras06] Rasmussen, C. E.; Williams, C. K. I. Gaussian Processes for Machine Learning. 2006[Ras18] Raschka, S.; Mirjalili, V. Machine Learning mit Python und Scikit-Learn und TensorFlow: Das umfassende Praxis-Handbuch für Data Science, Deep Learning und Predictive Analytics. mitp, Frechen, 2., aktualisierte und erweiterte auflage Auflage, 2018. ISBN 9783958457331[Rod04] Rodehorst, V. Photogrammetrische 3D-Rekonstruktion im Nahbereich durch Auto-Kalibrierung mit projektiver Geometrie: Zugl.: Berlin, Techn. Univ., Diss., 2004. wvb Wiss. Verl. Berlin, Berlin, 2004. ISBN 978–3–936846–83–6[Sci18a] ScikitLearn. 3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.20.0 documentation, 05.10.2018. URL https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics[Sci18c] ScikitLearn. sklearn.tree.DecisionTreeRegressor — scikit-learn 0.20.0documentation, 08.11.2018. URL https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html[Sci18e] ScikitLearn. 3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.20.0 documentation, 24.10.2018. URL https://scikit-learn.org/stable/modules/model_evaluation.html[Sci18f] ScikitLearn. sklearn.model_selection.cross_val_score — scikitlearn 0.20.0 documentation, 24.10.2018. URL https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score[Sci18g] ScikitLearn. 4.1. Pipelines and composite estimators — scikit-learn 0.20.0 documentation, 26.10.2018. URL https://scikit-learn.org/stable/modules/compose.html[Sci18h] ScikitLearn. sklearn.preprocessing.PolynomialFeatures — scikit-learn 0.20.0 documentation, 26.10.2018. URL https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures[Sci18j] ScikitLearn. sklearn.gaussian_process.kernels.ExpSineSquared — scikit-learn 0.20.0 documentation, 27.10.2018. URL https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared[Sci18k] ScikitLearn. sklearn.gaussian_process.kernels.RationalQuadratic — scikit-learn 0.20.0 documentation, 27.10.2018. URL https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic[Sci18m] ScikitLearn. 1.7. Gaussian Processes — scikit-learn 0.20.1 documentation, 28.11.2018. URL https://scikit-learn.org/stable/modules/gaussian_process.html[Sci18n] ScikitLearn. Illustration of prior and posterior Gaussian process fordifferent kernels — scikit-learn 0.20.0 documentation, 31.10.2018. URL https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py[Smo04] Smola, A. J.; Schölkopf, B. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, 2004. ISSN 0960–3174. doi:10.1023/B:STCO. 0000035301.49549.8849549.[Vaf17] Vafa, K. Gaussian Process Tutorial, 2017. URL http://keyonvafa.com/gp-tutorial/[Wei18] Weisstein, E. L2-Norm, 2018. URL http://mathworld.wolfram.com/L2-Norm.html[Wie12] Wieland, A.; Marcus Wallenburg, C. Dealing with supply chain risks. International Journal of Physical Distribution & Logistics Management, 42(10):887–905, 2012. ISSN 0960–0035. doi:10.1108/09600031211281411. URL https://www.emeraldinsight.com/doi/pdfplus/10.1108/09600031211281411[Wik18a] Gauß-Prozess, 13.10.2018. URL https://de.wikipedia.org/w/index.php?oldid=181728459[Yu12] Yu, H.; Kim, S. SVM Tutorial — Classification, Regression and Ranking.G. Rozenberg; T. Bäck; J. N. Kok, Handbook of Natural Computing, 479–506. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978–3–540–92909–3.",21/06/2021,0,38,125,"(700, 237)",57,2,0.0,42,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,surprise/amazement
123,Understanding Semantic Segmentation with UNET,Towards Data Science,Harshall Lamba,1000.0,15.0,2603,"Table of Contents:Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. (Wikipedia)Deep Learning has enabled the field of Computer Vision to advance rapidly in the last few years. In this post I would like to discuss about one specific task in Computer Vision called as Semantic Segmentation. Even though researchers have come up with numerous ways to solve this problem, I will talk about a particular architecture namely UNET, which use a Fully Convolutional Network Model for the task.We will use UNET to build a first-cut solution to the TGS Salt Identification challenge hosted by Kaggle.Along with this, my purpose of writing the blog is to also provide some intuitive insights on the commonly used operations and terms in Convolutional Networks for Image understanding. Some of these include Convolution, Max Pooling, Receptive field, Up-sampling, Transposed Convolution, Skip Connections, etc.I will assume that the reader is already familiar with the basic concepts of Machine Learning and Convolutional Networks. Also you must have some working knowledge of ConvNets with Python and Keras library.There are various levels of granularity in which the computers can gain an understanding of images. For each of these levels there is a problem defined in the Computer Vision domain. Starting from a coarse grained down to a more fine grained understanding, let’s describe these problems below:The most fundamental building block in Computer Vision is the Image classification problem where given an image, we expect the computer to output a discrete label, which is the main object in the image. In image classification we assume that there is only one (and not multiple) object in the image.In localization along with the discrete label, we also expect the compute to localize where exactly the object is present in the image. This localization is typically implemented using a bounding box which can be identified by some numerical parameters with respect to the image boundary. Even in this case, the assumption is to have only one object per image.Object Detection extends localization to the next level where now the image is not constrained to have only one object, but can contain multiple objects. The task is to classify and localize all the objects in the image. Here again the localization is done using the concept of bounding box.The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as dense prediction.Note that unlike the previous tasks, the expected output in semantic segmentation are not just labels and bounding box parameters. The output itself is a high resolution image (typically of the same size as input image) in which each pixel is classified to a particular class. Thus it is a pixel level image classification.Instance segmentation is one step ahead of semantic segmentation wherein along with pixel level classification, we expect the computer to classify each instance of a class separately. For example in the image above there are 3 people, technically 3 instances of the class “Person”. All the 3 are classified separately (in a different color). But semantic segmentation does not differentiate between the instances of a particular class.If you are still confused between the differences of object detection, semantic segmentation and instance segmentation, below image will help to clarify the point:In this post we will learn to solve the Semantic Segmentation problem using Fully Convolutional Network (FCN) called UNET.If you are wondering, whether semantic segmentation is even useful or not, your query is reasonable. However, it turns out that a lot of complex tasks in Vision require this fine grained understanding of images. For example:Autonomous driving is a complex robotics tasks that requires perception, planning and execution within constantly evolving environments. This task also needs to be performed with utmost precision, since safety is of paramount importance. Semantic Segmentation provides information about free space on the roads, as well as to detect lane markings and traffic signs.Machines can augment analysis performed by radiologists, greatly reducing the time required to run diagnostic tests.Semantic Segmentation problems can also be considered classification problems, where each pixel is classified as one from a range of object classes. Thus, there is a use case for land usage mapping for satellite imagery. Land cover information is important for various applications, such as monitoring areas of deforestation and urbanization.To recognize the type of land cover (e.g., areas of urban, agriculture, water, etc.) for each pixel on a satellite image, land cover classification can be regarded as a multi-class semantic segmentation task. Road and building detection is also an important research topic for traffic management, city planning, and road monitoring.There are few large-scale publicly available datasets (Eg : SpaceNet), and data labeling is always a bottleneck for segmentation tasks.Precision farming robots can reduce the amount of herbicides that need to be sprayed out in the fields and semantic segmentation of crops and weeds assist them in real time to trigger weeding actions. Such advanced image vision techniques for agriculture can reduce manual monitoring of agriculture.We will also consider a practical real world case study to understand the importance of semantic segmentation. The problem statement and the datasets are described in the below sections.In any Machine Learning task, it is always suggested to spend a decent amount of time in aptly understanding the business problem that we aim to solve. This not only helps to apply the technical tools efficiently but also motivates the developer to use his/her skills in solving a real world problem.TGS is one of the leading Geo-science and Data companies which uses seismic images and 3D renderings to understand which areas beneath the Earth’s surface which contain large amounts of oil and gas.Interestingly, the surfaces which contain oil and gas, also contain huge deposits of salt. So with the help of seismic technology, they try to predict which areas in the surface of the Earth contain huge amount of salts.Unfortunately, professional seismic imaging requires expert human vision to exactly identify salt bodies. This leads to highly subjective and variable renderings. Moreover it could cause huge loss for the oil and gas company drillers if the human prediction is incorrect.Thus TGS hosted a Kaggle Competition, to employ machine vision to solve this task with better efficiency and accuracy.To read more about the challenge, click here.To read more about seismic technology, click here.Download the data files from here.For simplicity we will only use train.zip file which contains both the images and their corresponding masks.In the images directory, there are 4000 seismic images which are used by human experts to predict whether there could be salt deposits in that region or not.In the masks directory, there are 4000 gray scale images which are the actual ground truth values of the corresponding images which denote whether the seismic image contains salt deposits and if so where. These will be used for building a supervised learning model.Let’s visualize the given data to get a better understanding:The image on left is the seismic image. The black boundary is drawn just for the sake of understanding denoting which part contains salt and which does not. (Of course this boundary is not a part of the original image)The image on the right is called as the mask which is the ground truth label. This is what our model must predict for the given seismic image. The white region denotes salt deposits and the black region denotes no salt.Let’s look at a few more images:Notice that if the mask is entirely black, this means there are no salt deposits in the given seismic image.Clearly from the above few images it can be inferred that its not easy for human experts to make accurate mask predictions for the seismic images.Before we dive into the UNET model, it is very important to understand the different operations that are typically used in a Convolutional Network. Please make a note of the terminologies used.There are two inputs to a convolutional operationi) A 3D volume (input image) of size (nin x nin x channels)ii) A set of ‘k’ filters (also called as kernels or feature extractors) each one of size (f x f x channels), where f is typically 3 or 5.The output of a convolutional operation is also a 3D volume (also called as output image or feature map) of size (nout x nout x k).The relationship between nin and nout is as follows:Convolution operation can be visualized as follows:In the above GIF, we have an input volume of size 7x7x3. Two filters each of size 3x3x3. Padding =0 and Strides = 2. Hence the output volume is 3x3x2. If you are not comfortable with this arithmetic then you need to first revise the concepts of Convolutional Networks before you continue further.One important term used frequently is called as the Receptive filed. This is nothing but the region in the input volume that a particular feature extractor (filter) is looking at. In the above GIF, the 3x3 blue region in the input volume that the filter covers at any given instance is the receptive field. This is also sometimes called as the context.To put in very simple terms, receptive field (context) is the area of the input image that the filter covers at any given point of time.In simple words, the function of pooling is to reduce the size of the feature map so that we have fewer parameters in the network.For example:Basically from every 2x2 block of the input feature map, we select the maximum pixel value and thus obtain a pooled feature map. Note that the size of the filter and strides are two important hyper-parameters in the max pooling operation.The idea is to retain only the important features (max valued pixels) from each region and throw away the information which is not important. By important, I mean that information which best describes the context of the image.A very important point to note here is that both convolution operation and specially the pooling operation reduce the size of the image. This is called as down sampling. In the above example, the size of the image before pooling is 4x4 and after pooling is 2x2. In fact down sampling basically means converting a high resolution image to a low resolution image.Thus before pooling, the information which was present in a 4x4 image, after pooling, (almost) the same information is now present in a 2x2 image.Now when we apply the convolution operation again, the filters in the next layer will be able to see larger context, i.e. as we go deeper into the network, the size of the image reduces however the receptive field increases.For example, below is the LeNet 5 architecture:Notice that in a typical convolutional network, the height and width of the image gradually reduces (down sampling, because of pooling) which helps the filters in the deeper layers to focus on a larger receptive field (context). However the number of channels/depth (number of filters used) gradually increase which helps to extract more complex features from the image.Intuitively we can make the following conclusion of the pooling operation. By down sampling, the model better understands “WHAT” is present in the image, but it loses the information of “WHERE” it is present.As stated previously, the output of semantic segmentation is not just a class label or some bounding box parameters. In-fact the output is a complete high resolution image in which all the pixels are classified.Thus if we use a regular convolutional network with pooling layers and dense layers, we will lose the “WHERE” information and only retain the “WHAT” information which is not what we want. In case of segmentation we need both “WHAT” as well as “WHERE” information.Hence there is a need to up sample the image, i.e. convert a low resolution image to a high resolution image to recover the “WHERE” information.In the literature, there are many techniques to up sample an image. Some of them are bi-linear interpolation, cubic interpolation, nearest neighbor interpolation, unpooling, transposed convolution, etc. However in most state of the art networks, transposed convolution is the preferred choice for up sampling an image.Transposed convolution (sometimes also called as deconvolution or fractionally strided convolution) is a technique to perform up sampling of an image with learnable parameters.I will not describe how transpose convolution works because Naoki Shibuya has already done a brilliant job in his blog Up sampling with Transposed Convolution. I strongly recommend you to go through this blog (multiple times if required) to understand the process of Transposed Convolution.However, on a high level, transposed convolution is exactly the opposite process of a normal convolution i.e., the input volume is a low resolution image and the output volume is a high resolution image.In the blog it is nicely explained how a normal convolution can be expressed as a matrix multiplication of input image and filter to produce the output image. By just taking the transpose of the filter matrix, we can reverse the convolution process, hence the name transposed convolution.After reading this section, you must be comfortable with following concepts:If you are confused with any of the terms or concepts explained in this section, feel free to read it again till you get comfortable.The UNET was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. The architecture contains two paths. First path is the contraction path (also called as the encoder) which is used to capture the context in the image. The encoder is just a traditional stack of convolutional and max pooling layers. The second path is the symmetric expanding path (also called as the decoder) which is used to enable precise localization using transposed convolutions. Thus it is an end-to-end fully convolutional network (FCN), i.e. it only contains Convolutional layers and does not contain any Dense layer because of which it can accept image of any size.In the original paper, the UNET is described as follows:If you did not understand, its okay. I will try to describe this architecture much more intuitively. Note that in the original paper, the size of the input image is 572x572x3, however, we will use input image of size 128x128x3. Hence the size at various locations will differ from that in the original paper but the core components remain the same.Below is the detailed explanation of the architecture:Below is the Keras code to define the above model:Model is compiled with Adam optimizer and we use binary cross entropy loss function since there are only two classes (salt and no salt).We use Keras callbacks to implement:We use a batch size of 32.Note that there could be a lot of scope to tune these hyper parameters and further improve the model performance.The model is trained on P4000 GPU and takes less than 20 mins to train.Note that for each pixel we get a value between 0 to 1.0 represents no salt and 1 represents salt.We take 0.5 as the threshold to decide whether to classify a pixel as 0 or 1.However deciding threshold is tricky and can be treated as another hyper parameter.Let’s look at some results on both training set and validation set:Results on training set are relatively better than those on validation set which implies the model suffers from overfitting. One obvious reason could be the small number images used to train the model.Thank you for interest in the blog. Please leave comments, feedback and suggestions if you feel any.Full code on my GitHub repo here.",17/02/2019,0,20,8,"(633, 290)",30,5,0.0,17,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
124,How Transformers Work,Towards Data Science,Giuliano Giacaglia,1100.0,14.0,2730,"If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:www.holloway.comTransformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar — their program to defeat a top professional Starcraft player.Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..For models to perform sequence transduction, it is necessary to have some sort of memory. For example let’s say that we are translating the following sentence to another language (French):“The Transformers” are a Japanese [[hardcore punk]] band. The band was formed in 1968, during the height of Japanese music history”In this example, the word “the band” in the second sentence refers to the band “The Transformers” introduced in the first sentence. When you read about the band in the second sentence, you know that it is referencing to the “The Transformers” band. That may be important for translation. There are many examples, where words in some sentences refer to words in previous sentences.For translating sentences like that, a model needs to figure out these sort of dependencies and connections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been used to deal with this problem because of their properties. Let’s go over these two architectures and their drawbacks.Recurrent Neural Networks have loops in them, allowing information to persist.In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. A loop allows information to be passed from one step to the next.The loops can be thought in a different way. A Recurrent Neural Network can be thought of as multiple copies of the same network, A, each network passing a message to a successor. Consider what happens if we unroll the loop:This chain-like nature shows that recurrent neural networks are clearly related to sequences and lists. In that way, if we want to translate some text, we can set each input as the word in that text. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information.The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.Consider a language model that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence “the clouds in the sky”, we don’t need further context. It’s pretty obvious that the next word is going to be sky.In this case where the difference between the relevant information and the place that is needed is small, RNNs can learn to use past information and figure out what is the next word for this sentence.But there are cases where we need more context. For example, let’s say that you are trying to predict the last word of the text: “I grew up in France… I speak fluent …”. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text.RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.In theory, RNNs could learn this long-term dependencies. In practice, they don’t seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem.When arranging one’s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.RNNs don’t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important.Internally, a LSTM looks like the following:Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output. I won’t go into detail on the mechanics of each cell. If you want to understand how each cell works, I recommend Christopher’s blog post:colah.github.ioWith a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating.The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don’t do too well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. Not only that but there is no model of long and short range dependencies. To summarize, LSTMs and RNNs present 3 problems:To solve some of these problems, researchers created a technique for paying attention to specific words.When translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so.Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens.The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention.For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage.The step in green in charge of creating the hidden states from the input. Instead of passing only one hidden state to the decoders as we did before using attention, we pass all the hidden states generated by every “word” of the sentence to the decoding stage. Each hidden state is used in the decoding stage, to figure out where the network should pay attention to.For example, when translating the sentence “Je suis étudiant” to English, requires that the decoding step looks at different words when translating it.Or for example, when you translate the sentence “L’accord sur la zone économique européenne a été signé en août 1992.” from French to English, and how much attention it is paid to each input.But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text.Convolutional Neural Networks help solve these problems. With them we canSome of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are Convolutional Neural Networks.The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. Not only that, but the “distance” between the output word and any input for a CNN is in the order of log(N) — that is the size of the height of the tree generated from the output to the input (you can see it on the GIF above. That is much better than the distance of the output of a RNN and an input, which is on the order of N.The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why Transformers were created, they are a combination of both CNNs with attention.To solve the problem of parallelization, Transformers try to solve the problem by using Convolutional Neural Networks together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another.Let’s take a look at how Transformer works. Transformer is a model that uses attention to boost the speed. More specifically, it uses self-attention.Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders.Each encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network.The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.Note: This section comes from Jay Allamar blog postLet’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.Each word is embedded into a vector of size 512. We’ll represent those vectors with these simple boxes.The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512.In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented — using matrices.The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.Multiplying x1 by the WQ weight matrix produces q1, the “query” vector associated with that word. We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence.What are the “query”, “key”, and “value” vectors?They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.Transformers basically work like that. There are a few other details that make them work better. For example, instead of only paying attention to each other in one dimension, Transformers use the concept of Multihead attention.The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating “kicked” in the sentence “I kicked the ball”, you may ask “Who kicked”. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like “Did what?”, etc…Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation.I gave an overview of how Transformers work and why this is the technique used for sequence transduction. If you want to understand in depth how the model works and all its nuances, I recommend the following posts, articles and videos that I used as a base for summarizing the technique",11/03/2019,0,48,1,"(674, 360)",26,3,0.0,42,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,expectation/interest
125,Multiclass Classification Neural Network using Adam Optimizer,Towards Data Science,ManiShankar Singh,22.0,6.0,455,"I wanted to see the difference between Adam optimizer and Gradient descent optimizer in a more sort of hands-on way. So I decided to implement it instead.In this, I have taken the iris dataset and implemented a multiclass classification 2 layer neural network as done in my previous blog. The only difference this time is that I have used Adam Optimizer instead of Gradient descent.In the code, the difference is that I have initialized two moment arrays for each layer and updated (or should I write adapted…) these moments as per the Adam optimization algorithm.In a normal gradient descent optimizer, the weights are adjusted based on the gradient calculated in the same epoch.In Adam optimizer, the weights are adjusted based on the moving average of gradients calculated in current and previous epochs. The moments adjustment as per the Adam algorithm is calculated as moving average of previous and current gradients and then those moments are used to update the weights.In the paper, beta1 = 0.9 and m is updated based on formula:Let us expand the above formula step by step for each epoch.We see that in each epoch, the previous gradients are getting included in the update, but the weights assigned to gradients which are far away from the current epochs gradient are getting smaller and smaller. This helps in moving towards minima while simultaneously dampening oscillations of gradient in search for minima. This gives us speed to cross saddle points.Let us talk a bit about the second order moment v. During weights adjustment, learning rate is divided by root mean square of v. It helps to adjust the learning rate for each weight w. The weights which have relatively larger magnitude will have larger value of v and hence a smaller learning step in that direction. This helps us to slow down so that we do not overshoot the minima.Finally, let us talk about the bias correction section. In the original paper, they have shown the mathematical derivation and given the explanation. For layman, it's sufficient to know that introducing this bias correction helps when gradients are sparse which if not corrected leads to large steps.Let's compare the convergence of cost function in both Gradient Descent optimizer and Adam optimizer method.Adam optimizer took just 250 epochs to reach the optimal cost value while Gradient descent took 19000 epochs. Reminds me of the super hero Flash!!This is a tremendous improvement in the speed of convergence. Adam is not only good in speed. It is also good for sparse and very noisy gradients.Please let me know your views about this blog post and code implementation with your comments.I am a machine learning engineer at TCS and my (Digital Software & Solutions) team is building amazing products.",10/07/2021,1,5,6,"(446, 282)",9,0,0.0,4,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
126,"CNN, R-CNN, Fast R-CNN, and Faster R-CNN",,Simple Schwarz,5.0,7.0,562,"CNN stands for Convolutional Neural Network. Before reading this post, you can find the basics of the neural network in following posts.medium.commedium.comWe learned the neural network. Then, What does “convolutional” mean? If you are familiar with math, you may know this concept. Convolution is is a mathematical operation on two functions (f and g) that produces a third function (f*g) that expresses how the shape of one is modified by the other. In machine learning, convolution plays a role of extracting features through a convolution calculation using input data and various filters which are sets of weights. I’ll go into more detail about how the convolution extracts the feature from input data in another post.Let’s move on the CNN architecture. The steps of the CNN architecture are as follows.As I said, you can think of the convolution layer as the role of extracting features. To be specific, the convolution process works as follows.Pooling reduces the amount of data computation by compressing input data into maximum, minimum, or average values.Relu is a kind of activation functions which define the output of that node given an input. For example, Z2 has same values of C2 because all elements in C2 are greater than 0 as following.The feature map consists of 2D data, but it must be converted into 1D data for classification. At this time, Flattening plays a role of converting 2D data into 1D data.Additionally, you need to know what padding is. Padding refers to putting layers around the input data with a specific value(i.e. 0) before the convolution operation. It is used to prevent the disadvantage of the convolution operation shrinking the output data size.R-CNN stands for Region-based Convolutional Neural Network. You learned about what CNN is so far. Then, if you know what “Region-based” means, you can understand R-CNN easily.R-CNN first proposes RoIs with a search algorithm, then extracts features with CNN, and then classifies those regions based on their features. ROI stands for Region Of Interest, which is literally where we are interested. It refers to places where the object may be in the input image.R-CNN consists of 4 components.Fast R-CNN produces region proposals based on the last feature map of CNN, not from the original input image like R-CNN. In other words, Fast R-CNN implements CNN first and then generates region proposals. On the contrary, R-CNN produces region proposals first and then implements CNN.The architecture of Fast R-CNN consists of the following modulesSimilar to Fast R-CNN, the image is provided as an input to a convolutional network that provides a convolutional feature map. Instead of using a selective search algorithm on the feature map to identify the region proposals, a region proposal network (RPN) is used to predict the region proposals as part of the training process.The input image is presented to the network, and its features are extracted via a pretrained CNN. These features, in parallel, are sent to two different components of the Faster R-CNN architectureThe architecture of Faster R-CNN can be described using two main networksThe output is then passed into two fully connected layers. One for the object classifier and one for the bounding box coordinate predictions to obtain our final localizations. This architecture achieves an end-to-end trainable, complete object detection pipeline where all of the required components are inside the network: Base network feature extractor, Regions proposal, RoI pooling, Object classification, and Bounding-box regressor.",03/11/2021,0,0,1,"(633, 277)",12,4,0.0,2,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,surprise/amazement
127,"Semi-supervised regression based on PCA and PLS: MATLAB, R and Python codes– All you have to do is just preparing data set (very simple, easy and practical)",Towards Data Science,DataAnalysis For Beginneｒ,279.0,5.0,764,"I release MATLAB, R and Python codes of semi-supervised regression based on Principal Component Analysis and Partial Least Squares (PCAPLS). They are very easy to use. You prepare data set, and just run the code! Then, PCAPLS and prediction results for new samples can be obtained. Very simple and easy!You can buy each code from the URLs below.https://gum.co/PnRna Please download the supplemental zip file (this is free) from the URL below to run the PCAPLS code. http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.ziphttps://gum.co/PXJsf Please download the supplemental zip file (this is free) from the URL below to run the PCAPLS code. http://univprofblog.html.xdomain.jp/code/R_scripts_functions.ziphttps://gum.co/XwnQl Please download the supplemental zip file (this is free) from the URL below to run the PCAPLS code. http://univprofblog.html.xdomain.jp/code/supportingfunctions.zipTo perform appropriate PCAPLS, the MATLAB, R and Python codes follow the procedure below, after data set is loaded.1. Decide the threshold of cumulative contribution ratio for PCA The number of principal components (PCs is determined while cumulative contribution ratio is checked. If 5% of noise is included in given data set, for example, PCs having 95% cumulative contribution ratio should be used. The other PCs can be removed as noise.2. Combine data set with objective variable (Y) and data set without Y (samples are combined)3. Autoscale explanatory variable (X) of combined data set Autoscaling means centering and scaling. Mean of each variable becomes zero by subtracting mean of each variable from the variable in centering. Standard deviation of each variable becomes one by dividing standard deviation of each variable from the variable in scaling. Scaling is arbitrary (but recommended), but centering is required since PCA is based on rotation of axises.4. Run PCA, and get score and loading vector for each PC5. Decide the number of PCs, based on the threshold in 1.6. Extract score for only samples with Y7. Autoscale score and Y Scaling is arbitrary (but recommended). Centering is required.8. Estimate Y with cross-validation (CV), changing the number of components from 1 to m Leave-one-out CV is very famous, but it causes over-fitting when the number of training samples is high. So, 5-fold or 2-fold CV is better. First, training samples are divided into 5 or 2 groups. Second, one group is handled as test samples and model is built with the other group(s). This is repeated 5 or 2 times until every group is handled as test samples. Then, not calculated Y but estimated Y can be obtained. m must be less than the number of X-variables, but m=30 is sufficient at a maximum.9. Calculate Root-Mean-Squared Error (RMSE) between actual Y and estimated Y for each number of components10. Decide the optimal number of components with the minimum RMSE value It is OK to decide the optimal number of components with the first local maximum RMSE value11. Construct PLS model with the optimal number of components and get standard regression coefficient12. Calculate determinant coefficient and RMSE between actual Y and calculated Y (r2C and RMSEC) and determinant coefficient and RMSE between actual Y and estimated Y (r2CV and RMSECV) r2C means the ratio of Y information that the PLS model can explain. RMSE means the average of Y errors in the PLS model. r2CV means the possible ratio of Y information that the PLS model can estimate for new samples. RMSECV means the possible average of Y errors for new samples. Better PLS models have higher r2CV values and lower RMSECV values. Large difference between r2C and r2CV and that between RMSEC and RMSECV mean PLS model’s overfitting to training samples.*Caution! r2CV and RMSECV cannot represent true predictability of the PLS model since it is CV not external validation.13. Check plots between actual Y and calculated Y, and between actual Y and estimated Y Outliers of calculated and estimated values can be checked.14. In prediction, subtract the mean in the autoscalling of X in 1. from X-variables, and then, divide X-variables by the standard deviation in the autoscalling of X in 1., for new samples15. Calculate score for new samples using loading vector in 4.16. Subtract the mean in the autoscalling of score in 7. from new score, and then, divide new score by the standard deviation in the autoscalling of score in 7.17. Estimate Y based on the standard regression coefficient in 11.18. Multiply the standard deviation in the autoscalling of Y in 1. by estimated Y, and then, add the mean in the autoscalling of Y in 1. to estimated YMATLAB: https://gum.co/PnRnaR: https://gum.co/PXJsfPython: https://gum.co/XwnQlMATLAB: http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.zipR: http://univprofblog.html.xdomain.jp/code/R_scripts_functions.zipPython: http://univprofblog.html.xdomain.jp/code/supportingfunctions.ziphttps://medium.com/@univprofblog1/data-format-for-matlab-r-and-python-codes-of-data-analysis-and-sample-data-set-9b0f845b565a#.3ibrphs4hSamples of data.csv, that of data_prediction1.csv and that of data_prediction2.csv are combined before PCA.Estimated values of Y for “data_prediction2.csv” are saved in ”PredictedY2.csv”.Please see the article below. https://medium.com/@univprofblog1/settings-for-running-my-matlab-r-and-python-codes-136b9e5637a1#.paer8scqy",15/09/2016,0,24,0,"(432, 328)",8,0,0.0,14,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
128,How Machine Learning Interpreted Problems and Saved Costs for eCommerce Companies,Towards Data Science,Nhan Tran,365.0,13.0,1789,"Before we deep dive into the solution, here is the The ProblemThe company was facing a high chance of Failed Delivery Rate — where Buyers (those who bought stuff on the eCommerce platform such as Shopee, Lazada, Amazon, eBay, etc.) would decline their parcels at the very last minute. It wasted a huge amount of money and the Management Board wanted to reduce that rate.Two years ago, it was the first time I took the new role and started leading the Data Team. One day, there was an important meeting, and I was the only data-guy in a room full of C-level. People were in one of those follow-up meetings, discussing the total failed delivery (FD) cases, its increment, the potential root cause of each case, the major reasons, its trending and many more while I was the only one who knew nothing. It was my first time touching base with that messy problem.There were many questions raised, some of them got answered, some not. Eventually, I realized there were 2 groups of people:(of course, there was the 3rd group who kept silence, that was me)It was a 2-hour-meeting, and I totally got lost from the very first minutes. After 1.5 hours passed, I started to question myself “What’s going on here? This is not a discussion, this is an investigation”. People were trying to see what happened and why it happened, but they couldn’t prevent it from happening in the future. They also raised some hypotheses based on the result from the Data Team. Unfortunately, those hypotheses seemed to be not so consistent, and they barely proved anything.At that moment, I knew that I can use some Machine Learning algorithms to solve the problem, but unfortunately the term of Machine Learning was quite new to the Business, and I got no chance to show my way. However, Machine Learning is not rocket science. I’ll show you how I applied and gained value to the business.Admit it, have you seen your company’s problem here as well? Have you ever been in a discussion where the Business Team or Financial Team kept asking about the reason why this problem happened. Then they also put you in some specific circumstances such as “what if we do this?” or “I think this happened because of that”. And you, as a data-guy, you would look at your data, did something you called analysis and the outputs were mostly Yes/No or True/False.That procedure is called hypothesis testing, which is:You got some hypothesis (by someone who thinks the results would change if you would do something stated in his hypothesis); then you had to check your data to see whether that was a True Hypothesis or a False Hypothesis. If it was True, your company would apply that hypothesis to achieve the expected outcomes. If it was False, they would try to give you another hypothesis. And you had to redo the procedure again, again, and again, until they found a reasonable hypothesis.Is it a good approach? Absolutely no. That’s an infinite loop because the company will never be satisfied with what they get, so they will try to give you more and more hypotheses to sell more products and save more cost. And you will never get out of that labyrinth-of-void.The problems of this approach are:Then how to get out of that labyrinth?It’s quite ironic to say that because I’m a part of the Data Team, the only thing I trust is my data. Hypothesis can come from anybody who is involved in the problem, it looks like a raw dataset. Before investing my time on it, I have to verify it, clean it, sort it (like what I do with my data) to ensure getting the best outcomes. Then the question is “how do I know which hypothesis is useful and which is useless?”. Ah yes, “you have to test it using your data” you must be thinking about that, right? But wait, you should not just test all hypotheses on the list, you are cleaning it up, right? Bingo, the deadlock found.Let me recap the whole procedure:At that moment, I realized that the true problem came from humans, not the hypothesis, or data, because the source of the hypothesis can come from any people. What if they don’t have enough knowledge to understand and analyze the problem? They won’t have the capability to provide hypotheses! Even though they can provide some hypotheses, it won’t be the best solution because all are human and all have our limitations. The best solution should be generated by the help of computers, agree? Then how to utilize the computing power of machines for this problem?There was an idea that just popped out from my mind “why don’t I just put all the things into some unsupervised learning algorithm to check out what’s the outcomes, then bring those stuff to the Business Team and let them verify?”Yes, that was what I thought while other people were thinking about new hypotheses and checking with my team!Right there, I decided to extract 10% of the data and put it in the Decision Tree algorithm. The code was ready for use (of course, I made a code template to reuse, just had to change the dataset only), the dataset with 12,000 records was cleaned. It took me nearly 2 minutes to get the result.Okay, if you’re an experienced Data Scientist, you will have a question “why didn’t you apply preprocessing and feature engineering before using your prediction?”. That’s a good question! My answer is “it’s the on-time matter”. In fact, I didn’t want to get a high accuracy of prediction, I just needed to see “how my data look like, and the importance of each feature”. In real life, you mostly have to trade off the quality (accuracy rate in this case) for responding time (how long you can understand your data in this case). Of course the business won’t wait too long to get the decision from the Data Team, they just need something that sounds reasonable enough to make a decision. This is another pitfall for Data Scientists when they tend to invest a lot of time to get the best prediction model. Here is a small trade-off diagram you need to understand:Admit it, I bet that you frequently stuck with explaining to them about some techniques that can solve the problem but it’s too hard for them to understand because most Machine Learning algorithms are uninterpretable. Then how to deal with it? Okay, let’s talk a bit about Decision Tree first.Decision Tree is an algorithm in the Classification group, which is a subset of Supervised Learning. The output of this algorithm can be a binary value such as True/False, or Yes/No, or Good/Bad, or Male/Female, or just 0/1. The output sometimes can be a group of labels such as Red/Green/Blue, or Low/Medium/High, or XS/S/M/L/XL (the T-shirt size).A tree is built by splitting the source set, constituting the root node of the tree, into subsets — which constitute the successor children. Each element of the domain of the classification is called a class. Here are some popular terms related to Decision Tree:You must be wondering why I picked Decision Tree over Random Forest or Logistic Regression. The reason is: I was looking for a hypothesis, not the prediction.We have many Machine Learning algorithms, and they can be grouped as supervised, unsupervised, regression, classification, etc… Each algorithm will give you a different result, for example, when you want to predict the result to be A or B (True or False, or any kind of classification you need), you can use any algorithm in the Classification group. But it’s not the best practice for you to use only 1 algorithm over the others, or use them randomly. In the current situation, what’s the problem? It’s:“How to reduce the rate of Failed Delivery cases?”It’s not “what will happen if I give you a new Order info? Will it be delivered successfully or not?”. So if you choose Logistic Regression, for example, you can predict the result, but you cannot know how to prevent it. This is another big pitfall for Data Team, especially for ML Engineers who keep focusing on their algorithm and losing the communication with Business Teams. In fact, myself, and my team had the same mistake in the past, so no worry if you had the same problem. The good news is now we had a lesson and learned experience.So, among those algorithms, Decision Tree is the easiest way to interpret your data (KNN and SVM are good too, but in this blog, I picked Decision Tree because it’s the easiest way).Let’s deep dive into the output from Decision Tree, we have the top 3 branches:Okay, after checking the top 3 branches, I tried to interpret each of them as a single sentence of “if the Buyer of this parcel [did something], then this parcel will potentially be delivered successfully” like:After that, I tried to understand my hypothesis using non-technical skills (or my common sense), and I explained it to the Business Team as:Here is the summary of the hypotheses I interpreted from the Decision Tree to non-technical language:Now it seems the problem is much easier to understand by Business. That’s the first step of the Modern Approach which I mentioned before:Okay, after a long process, now we have 3 hypotheses generated by a Machine Learning algorithm, let’s see how the Business solves it and close the case? No! It’s time to change the topic a bit. Let’s check our 3 hypothesis, which is:…what did you learn from them? It’s human behavior!Based on my personal research with the Customer Insight Team during the time I worked for The Nielsen Company, I found that our behavior drives our action, which indirectly means: you can change people’s action by changing their behavior. Then how to apply to our problem?Okay, it’s time to finish my long post. In this blog, I didn’t talk much about Machine Learning or sharing any code, because this one is for people who work in the modern Data industry, who need to use their technical skills to serve the non-technical guys. That’s the value of the Data Team.However, guess what I got after a couple of campaigns? The Failed Delivery rate was reduced 7% compared with the same period of previous year! That’s the greatest achievement my team contributed to the organization after more than 2 years facing a high FD rate, and more than 6 projects which aimed at the target of reducing FD rate (and all of them were failed because they were led by Business Teams, which stuck with the traditional approach).Now let’s look at what we discussed!3 common pitfalls:Important notes:Happy learning everyday!",23/03/2021,0,60,41,"(700, 449)",10,10,0.0,23,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
129,How to build your own Neural Network from scratch in Python,Towards Data Science,James Loy,6500.0,7.0,1071,"Update: When I wrote this article a year ago, I did not expect it to be this popular. Since then, this article has been viewed more than 450,000 times, with more than 30,000 claps. It has also made it to the front page of Google, and it is among the first few search results for ‘Neural Network’. Many of you have reached out to me, and I am deeply humbled by the impact of this article on your learning journey.This article also caught the eye of the editors at Packt Publishing. Shortly after this article was published, I was offered to be the sole author of the book Neural Network Projects with Python. Today, I am happy to share with you that my book has been published!The book is a continuation of this article, and it covers end-to-end implementation of neural network projects in areas such as face recognition, sentiment analysis, noise removal etc. Every chapter features a unique neural network architecture, including Convolutional Neural Networks, Long Short-Term Memory Nets and Siamese Neural Networks. If you’re looking to create a strong machine learning portfolio with deep learning projects, do consider getting the book!You can get the book from Amazon: Neural Network Projects with PythonMotivation: As part of my personal journey to gain a better understanding of Deep Learning, I’ve decided to build a Neural Network from scratch without a deep learning library like TensorFlow. I believe that understanding the inner workings of a Neural Network is important to any aspiring Data Scientist.This article contains what I’ve learned, and hopefully it’ll be useful for you as well!Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output.Neural Networks consist of the following componentsThe diagram below shows the architecture of a 2-layer Neural Network (note that the input layer is typically excluded when counting the number of layers in a Neural Network)Creating a Neural Network class in Python is easy.Training the Neural NetworkThe output ŷ of a simple 2-layer Neural Network is:You might notice that in the equation above, the weights W and the biases b are the only variables that affects the output ŷ.Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network.Each iteration of the training process consists of the following steps:The sequential graph below illustrates the process.As we’ve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is:Let’s add a feedforward function in our python code to do exactly that. Note that for simplicity, we have assumed the biases to be 0.However, we still need a way to evaluate the “goodness” of our predictions (i.e. how far off are our predictions)? The Loss Function allows us to do exactly that.There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we’ll use a simple sum-of-sqaures error as our loss function.That is, the sum-of-squares error is simply the sum of the difference between each predicted value and the actual value. The difference is squared so that we measure the absolute value of the difference.Our goal in training is to find the best set of weights and biases that minimizes the loss function.Now that we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases.In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases.Recall from calculus that the derivative of a function is simply the slope of the function.If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as gradient descent.However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.Phew! That was ugly but it allows us to get what we needed — the derivative (slope) of the loss function with respect to the weights, so that we can adjust the weights accordingly.Now that we have that, let’s add the backpropagation function into our python code.For a deeper understanding of the application of calculus and the chain rule in backpropagation, I strongly recommend this tutorial by 3Blue1Brown.Now that we have our complete python code for doing feedforward and backpropagation, let’s apply our Neural Network on an example and see how well it does.Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn’t exactly trivial for us to work out the weights just by inspection alone.Let’s train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss monotonically decreasing towards a minimum. This is consistent with the gradient descent algorithm that we’ve discussed earlier.Let’s look at the final prediction (output) from the Neural Network after 1500 iterations.We did it! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values.Note that there’s a slight difference between the predictions and the actual values. This is desirable, as it prevents overfitting and allows the Neural Network to generalize better to unseen data.Fortunately for us, our journey isn’t over. There’s still much to learn about Neural Networks and Deep Learning. For example:I’ll be writing more on these topics soon, so do follow me on Medium and keep and eye out for them!I’ve certainly learnt a lot writing my own Neural Network from scratch.Although Deep Learning libraries such as TensorFlow and Keras makes it easy to build deep nets without fully understanding the inner workings of a Neural Network, I find that it’s beneficial for aspiring data scientist to gain a deeper understanding of Neural Networks.This exercise has been a great investment of my time, and I hope that it’ll be useful for you as well!",14/05/2018,0,37,14,"(507, 231)",12,3,0.0,3,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
130,How Recurrent Neural Networks work,Towards Data Science,Simeon Kostadinov,1500.0,6.0,942,"You have definitely come across software that translates natural language (Google Translate) or turns your speech into text (Apple Siri) and probably, at first, you were curious how it works.In the last couple of years, a considerable improvement in the science behind these systems has taken place. For example, in late 2016, Google introduced a new system behind their Google Translate which uses state-of-the-art machine learning techniques. The improvement is remarkable and you can test it yourself.Another astonishing example is Baidu’s most recent text to speech:So what do all the above have in common? They deal with sequential data to make predictions. Okay, but how that differs from the well-known cat image recognizers?Imagine you want to say if there is a cat in a photo. You can train a feedforward neural network (typically CNN-Convolutional Neural Network) using multiple photos with and without cats.In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. — WikipediaThese networks are primarily used for pattern recognition and can be illustrated as follows:Conversely, in order to handle sequential data successfully, you need to use recurrent (feedback) neural network. It is able to ‘memorize’ parts of the inputs and use them to make accurate predictions. These networks are at the heart of speech recognition, translation and more. So let’s dive into a more detailed explanation.Training a typical neural network involves the following steps:Of course, that is a quite naive explanation of a neural network, but, at least, gives a good overview and might be useful for someone completely new to the field.Recurrent neural networks work similarly but, in order to get a clear understanding of the difference, we will go through the simplest model using the task of predicting the next word in a sequence based on the previous ones.First, we need to train the network using a large dataset. For the purpose, we can choose any large text (“War and Peace” by Leo Tolstoy is a good choice). When done training, we can input the sentence “Napoleon was the Emperor of…” and expect a reasonable prediction based on the knowledge from the book.So, how do we start? As explained above, we input one example at a time and produce one result, both of which are single words. The difference with a feedforward network comes in the fact that we also need to be informed about the previous inputs before evaluating the result. So you can view RNNs as multiple feedforward neural networks, passing information from one to the other.Let’s examine the following schema:Here x_1, x_2, x_3, …, x_t represent the input words from the text, y_1, y_2, y_3, …, y_t represent the predicted next words and h_0, h_1, h_2, h_3, …, h_t hold the information for the previous input words.Since plain text cannot be used in a neural network, we need to encode the words into vectors. The best approach is to use word embeddings (word2vec or GloVe) but for the purpose of this article, we will go for the one-hot encoded vectors. These are (V,1) vectors (V is the number of words in our vocabulary) where all the values are 0, except the one at the i-th position. For example, if our vocabulary is apple, apricot, banana, …, king, … zebra and the word is banana, then the vector is [0, 0, 1, …, 0, …, 0].Typically, the vocabulary contains all English words. That is why it is necessary to use word embeddings.Let’s define the equations needed for training:If you are wondering what these W’s are, each of them represents the weights of the network at a certain stage. As mentioned above, the weights are matrices initialised with random elements, adjusted using the error from the loss function. We do this adjusting using back-propagation algorithm which updates the weights. I will leave the explanation of that process for a later article but, if you are curious how it works, Michael Nielsen’s book is a must-read.Once we have obtained the correct weights, predicting the next word in the sentence “Napoleon was the Emperor of…” is quite straightforward. Plugging each word at a different time step of the RNN would produce h_1, h_2, h_3, h_4. We can derive y_5 using h_4 and x_5 (vector of the word “of”). If our training was successful, we should expect that the index of the largest number in y_5 is the same as the index of the word “France” in our vocabulary.Unfortunately, if you implement the above steps, you won’t be so delighted with the results. That is because the simplest RNN model has a major drawback, called vanishing gradient problem, which prevents it from being accurate.In a nutshell, the problem comes from the fact that at each time step during training we are using the same weights to calculate y_t. That multiplication is also done during back-propagation. The further we move backwards, the bigger or smaller our error signal becomes. This means that the network experiences difficulty in memorising words from far away in the sequence and makes predictions based on only the most recent ones.That is why more powerful models like LSTM and GRU come in hand. Solving the above issue, they have become the accepted way of implementing recurrent neural networks.Finally, I would like to share my list with all resources that made me understand RNNs better:Warm-up:Go deeper:Advanced (grasping the details):I hope this article is leaving you with a good understanding of Recurrent neural networks and managed to contribute to your exciting Deep Learning journey.What more AI content? Follow me on LinkedIn for daily updates.",02/12/2017,0,6,24,"(625, 251)",4,5,0.0,22,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
131,Sequence to sequence model: Introduction and concepts,Towards Data Science,Manish Chablani,1700.0,3.0,225,"If we take a high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components:We use embedding, so we have to first compile a “vocabulary” list containing all the words we want our model to be able to use or read. The model inputs will have to be tensors containing the IDs of the words in the sequence.There are four symbols, however, that we need our vocabulary to contain. Seq2seq vocabularies usually reserve the first four spots for these elements:Note: Other tags can be used to represent these functions. For example I’ve seen <s> and </s> used in place of <GO> and <EOS>. So make sure whatever you use is consistent through preprocessing, and model training/inference.Preparing the inputs for the training graph is a little more involved for two reasons:1. These models work a lot better if we feed the decoder our target sequence regardless of what its timesteps actually output in the training run. So unlike in the graph, we will not feed the output of the decoder to itself in the next timestep.2. BatchingOne of the original sequence to sequence papers, Sutskever et al. 2014, reported better model performance if the inputs are reversed. So you may also choose to reverse the order of words in the input sequence.During the preprocessing we do the following:Credits: From lecture notes: https://classroom.udacity.com/nanodegrees/nd101/syllabusResources:https://github.com/ematvey/tensorflow-seq2seq-tutorials",23/06/2017,0,4,0,"(693, 390)",2,2,0.0,3,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
132,Time Series Forecasting with RNNs,Towards Data Science,Marek Galovič,252.0,6.0,1043,"In this article I want to give you an overview of a RNN model I built to forecast time series data. Main objectives of this work were to design a model that can not only predict the very next time step but rather generate a sequence of predictions and utilize multiple driving time series together with a set of static (scalar) features as its inputs.On a high level, this model utilizes pretty standard sequence-to-sequence recurrent neural network architecture. Its inputs are past values of the predicted time series concatenated with other driving time series values (optional) and timestamp embeddings (optional). If static features are available the model can utilize them to condition the prediction too.Encoder is used to encode time series inputs with their respective timestamp embeddings [x] to a fixed size vector representation [S]. It also produces latent vectors for individual time steps [h] which are used later in decoder attention. For this purpose, I utilized a multi-layer unidirectional recurrent neural network where all layers except the first one are residual.In some cases you may have input sequences that are too long and can cause the training to fail because of GPU memory issues or slow it down significantly. To deal with this issue, the model convolves the input sequence with a 1D convolution that has the same kernel size and stride before feeding it to the RNN encoder. This reduces the RNN input by a factor of n where n is the convolution kernel size.Context layer sits between the inputs encoder and a decoder layer. It concatenates encoder final state [S] with static features and static embeddings and produces a fixed size vector [C] which is then used as an initial state for the decoder.Decoder layer is implemented as an autoregressive recurrent neural network with attention. Input at each step is a concatenation of previous sequence value and a timestamp embedding for that step. Feeding timestamp embeddings to the decoder helps the model learn patterns in seasonal data.At the first step, encoder takes the context [C] as an initial cell value and a concatenation of initial sequence value [v] and first timestamp embedding [E] as a cell input. First layer then emits attention query [q] that is fed to attention module which outputs a state [s] that is then used as a cell state in the next step. Lower layers of the decoder don’t use attention. Outputs of the decoder [o] are the raw predicted values which are then fed to the next step together with a timestamp embedding for that step.Attention allows the decoder to selectively access encoder information during decoding. It does so by learning a weighting function that takes previous cell state [q] and a list of encoder outputs [h] as an input and outputs a scalar weight for each of the encoder outputs. It then takes a weighted sum of encoder outputs, concatenates it with the query and takes a nonlinear projection as a next cell state. Mathematically this can be formulated as follows:I won’t do a step by step tutorial on how to prepare data for a sequence to sequence learning problem in this article but I’ll try to give you an overview of the steps needed to get the model working.First you want to make sure that the span of your time series features doesn’t overlap with the span of your targets and that the latest features timestamp is right before first targets timestamp. Also, if you have any static features (aggregate statistics for example) they need to be aggregated up to the last features timestamp. I know this sounds obvious but sometimes it’s really easy to overlook and get excited by how great your model performs just to find out that you leaked future information into your training dataset.The data, both features and targets, need to be normalized into a suitable range for a neural network model. This usually means somewhere between -1 and 1. Normalization scheme I decided to go with was to first take a log to remove any potential skew and then compute a mean and standard deviation. Input to the network is then a z-score of the log.For the target value there are multiple normalization options. One could for example forecast a relative change from the latest input value (can be an issue in case it’s 0) or normalized absolute values using a similar approach I described above for the features.As an example for this article I used the model described above to predict closing price of Shopify stock for next five trading days given data from last sixty trading days. An input sequence convolution layer with kernel/stride = 5 was used to reduce the encoder RNN input size from 60 to 12 steps.One could argue that the stock price is unpredictable without taking other factors such as news into consideration (even then it’s very hard). That is why I decided to use another six tickers (Apple, Amazon, Google, Facebook, Microsoft and IBM) as inputs to the model so that it can learn possible correlations between them. The features used were daily Open, High, Low, Close Price (OHLC) and Volume. I augmented the time series features with “spread” (abs(high-low)) and a past 60 days mean of each feature as a static input.Chart 2. shows a mean absolute error per day. We can see that the further we go into the future the worse our predictions become. Intuitively this makes sense as the model has a better chance of making a good prediction for the very next trading day than five days from now.As with every machine learning model there are successes where the model makes a very good prediction and failures where the prediction is not so great. The following are some examples of such cases.Recurrent neural networks are a powerful tool for modeling sequential data. The model described in this article can be applied to many problems ranging from sales forecasting to energy consumption forecasting. It can condition its predictions on multivariate input series together with scalar inputs which makes it flexible enough to incorporate multiple data sources. Tensorflow implementation of the model can be found here.If you liked this article please recommend it to others. Also, if you have any suggestions feel free to leave a comment below.",02/11/2018,0,23,2,"(621, 314)",10,0,0.0,6,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,trust/acceptance
133,Why Your Mindset Matters and How To Improve It,,Nasos Papadopoulos,203.0,5.0,915,"“Whether you think you can or you can’t, you’re probably right.” -Henry FordWhat have you always struggled to learn?Whether you’re a poor writer or can’t handle numbers, you’re no different to everyone else.We’ve all experienced frustrations with learning.If only more people knew that the first and most important step to solving these problems is surprisingly simple.When we struggle to learn, we often put it down to a lack of innate ability.At some point, we’ve all used explanations like the one I told myself at school when wrestling with a hard math problem - “I’m just not good with numbers.”This perspective frames our capacity to learn as something outside of our control, when in reality it’s influenced heavily by our own beliefs.If learning is a journey from a place of knowing less to one of knowing more, then trying to learn something when we don’t believe we can do it is like trying to drive with the handbrake on.Unsurprisingly, the idea that we need believe we’re capable to succeed isn’t new and often appears in children’s stories and motivational quotes.Consider Henry Ford’s old adage “Whether you think you can or you can’t, you’re probably right” or one of Muhammad Ali’s most cited quotes,“If my mind can conceive it and my heart can believe it, then I can achieve it.”But there’s more to these statements than great word-smithery.In fact, the work of Stanford psychologist Carol Dweck suggests that there is scientific substance to the idea that mindset matters - our belief systems directly affect our behaviour, which in turn affects our success in learning.In 20 years of research with children and adults, Dweck placed learners into two categories:Having a growth mindset doesn’t mean we have to believe that anyone can become the next Einstein, Mozart or Da Vinci.We only have to acknowledge that our potential to learn is unbounded and that the power to increase our own abilities is within our control.Approaching things from this perspective creates a real passion for learning, and makes us more likely to apply the grit we need to succeed.We become less discouraged by failure and more attentive when we’re struggling.We start to see difficulty as an opportunity to stretch ourselves rather than trying to avoid it.All these characteristics not only make us more likely to learn new things but they raise our chances of reaching our goals in our careers and personal lives.Dweck and her colleagues have consistently produced results that prove the positive impact of a growth mindset on learning performance.In one of her early experiments, outlined in her book, she ran a workshop for a 7th grade class at a New York City junior high school.Half the students were given a presentation on memory and effective studying, while the other half were introduced to Dweck’s ideas and were told their intelligence largely depended on their own effort.After the workshop both groups went back to their classrooms, with their teachers unaware of the difference between what they had been taught.Remarkably, as the school year unfolded, the students from the second group developed a growth mindset and became higher achievers than the students from the first group, who retained a conventional fixed mindset.Dweck’s team has replicated these results across different locations, age groups and subjects with notable degrees of success.Our mindset is fundamental. It’s more important than inherent ability in learning performance and has a huge impact on the other areas of our life such as our career and relationships.All learning strategies, tools and techniques are almost useless if we don’t combine them with a strong, growth based learning mindset - the simple belief that the power to improve our learning abilities lies in our own hands.What are your most limiting views about your learning abilities? Write them down in detail and give examples from the past which justify these beliefs.Be honest with yourself and try to think of all the times in the past when your own beliefs were the major barrier to your learning.Now cross-examine these limiting beliefs through the lens of a growth mindset, just as a prosecution lawyer would analyse a defence testimony.Think of examples in the past where your effort led to progress and ask if those limiting beliefs stand up to the test now.I’ll be surprised if any do, because the growth mindset encourages you to take responsibility for the results you get, rather than blaming external factors.A growth mindset is something you need to practice consistently over time, like anything else.If your limiting beliefs pop up again in your mind, remind yourself that your ability is under your control.When you wake up in the morning, ask yourself these three questions:This exercise is recommended by Dweck herself and reviewing these ideas has been hugely useful in my own learning projects.The truth is that you’ve already done some of the work by reading this post.Apparently, the simple act of reading about the research can have a significant and measurable impact on your attitude and learning performance.So if you’ve reached this far, you can thank me by clapping it out and sharing with someone else who needs to read this!If you enjoyed this story, please share it and 👏👏👏 it up!For the best stories from MetaLearn, subscribe to our weekly newsletter here.When you do, you’ll get our Free 5 Step Video Course to hack your learning including:- 5x Video Lessons on the 5 key MetaLearning Principles- 5x One-Page Cheatsheets to help you apply each principle- The Learning Toolkit featuring my Top 10 Learning AppsOriginally published at www.metalearn.net.",02/10/2017,0,1,4,"(700, 467)",1,2,0.0,3,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
134,Understanding Optimization in ML with Gradient Descent & implement SGD Regressor from scratch,,Ruman Khan,,19.0,3497,"We’ll solve a machine learning problem with help of differentiation, and that is what optimization is all about basically. With the help of concepts discussed in this blog we’ll implement a deployment level SGD regressor model from scratch.Optimization is an important concept not only in the context of Machine learning but also in general. At very high-level, optimization is all about making the best use of an available resource or choosing the best solution out of many possible solutions.On Google, this is the meaning of optimization- “The action of making the best or most effective use of a situation or resource”.Examples where we use some sort of optimization in our day to day activities like planning a day or a trip, taking the best route while travelling, etc.If speaking of Machine learning, We’re solving a problem then our end goal would be to have the best solution that maximizes the profit or minimizes the loss. So It makes sense to apply optimization while solving a machine learning problem to get an optimized solution that minimizes the loss.We looked at a very high-level meaning of optimization. And now we’ll look into differentiation, maxima and minima and we’ll try to connect the dots between understand the optimization much better.(PS — I’m not going to explain you in-depth that what is differential calculus and all. Of course, I’m limited to Optimization so I’ll try to explain only that is necessary to understand the optimization better. )Well, 99% of Machine learning optimization depends on these two ideas mainly -We’ll look at a very simple example of a single variable differentiation. Let’s first consider this graph:Here, let’s consider a function is y=f(x). It could be any function. (You’ll understand it later please beware with me for now). x is a scaler value.First, let’s understand what does dy/dx intuitively mean?This is — How much does y changes as x changes or rate of change of y with respect to x. To understand it better let’s re-draw the above graph:To understand the change in y w.r to change in x let’s consider x1 and x2 on the x-axis and corresponding y1 and y2 on the y-axis in the above graph. So as you can see we’ll get a right angle triangle as:In the right-angle triangle from the above graph, hypotenuse (tan 𝛉) tells that how much does y changes as x changes. So we can write this in a formulation as:Mathematically, we can also write as:So we have only added the limit( Δx tend to 0) to the above formulation and can be read as differentiation of y w.r to x is — the rate of change of y w.r to x as the change in x tends to zero. So as x2 comes closer and closer to x1 and if the difference is very small between x2 and x1 then we’ll get a tangent(green line) at x1 as (we can see it in the graph below):Tangent in the above graph is — tangent of f(x) at x1 as Δx tends to zero. That means as x2 moves close to x1 then the difference between x2 and x1 becomes very small then a tangent is formed on the point x1 touching the x-axis and that gives an angle 𝛉. So to compute the derivative of f(x) w.r to x at x1 we take the slope of the tangent at x1. We can write the formulation as:Considering the tan𝛉, here 𝛉 is the angle formed by tangent and x-axis as Δx tends to 0. We have seen this in the above graph. So let’s see what will happen for different values of 𝛉.Here 𝛉 is formed by tangent and x-axis so 𝛉 could be any value depending on how does the tangent is formed as we can see in the above plot. Let’s consider different circumstances:I added this above plot to just explain that if we know the 𝛉 then can show that derivative of f(x) w.r to x at some point (xi) is going to be -Ve, +Ve or 0.To summarize all — Tangent line = Derivative = Instantaneous rate of change.Maxima and Minima are the largest and smallest values of function f(x). This value could be the largest or smallest in a given range(Local) or the entire domain(Global). Let’s look at different plots:So from the above plot, it might be clear that some function f(x) may have only maxima or minima or maxima and minima both. Some functions may also have multiple maxima and minima i.e, global and locals. So let’s look at the below plot:So in the above plot function f(x) have global and local maxima and minima. For more in detail, you can follow this — [source]So we got the idea about maxima and minima of a function f(x). Now let’s connect the dots between maxima minima and derivative that we have discussed previously in the differentiation section in this blog:Let’s take a function f(x) having only minima. Considering the below plot:Let’s say x1 as minima and if we take the derivative of function w.r to x at x1 then that would be equal to zero because at minima of the function we’ll get tangent parallel to x-axis hence 𝛉=0 that will result in tan𝛉=0 that is nothing but derivative at minima(x1) of function f(x). So to summarise this we can write as:An Important conclusion from above we can take is- Derivative of any function at minima/maxima is equal to zero. Or derivative of a function equals to zero tells the minima or maxima.For example, consider a situation in which you have a loss function f(x) of ML or any problem. Your end goal would be finding the value x for which you have a minimal loss. So we can simply find it by computing derivatives on different xi points. We can do it iteratively by going through one by one point at a time until we did not reach very close to minima. But this whole process is not an easy task if we have some complex loss functions like mean squares error or others.But for that, we have a gradient descent algorithm. It makes easy to compute minima or maxima for any function. Gradient descent is an iterative algorithm.At a very high level, we can say this algorithm randomly selects a value as a minimal or optimal then keep changing the value in an iterative manner and the end, it gives a value that is actually very close to minima. We’ll see how does it solve very complex ML problems with the help of derivatives.First, let’s understand the behaviour of derivative (or slop) for that let’s consider below plot:In the above plot, assume at x = 0 we have optimal value for x, as we can also see it visually. And followings are the observation we can take from the above plot that is -So we can take the following conclusion from the above plot as-Geometric Intuition behind this algorithm:As this is an iterative algorithm so It would be better to understand in iterations or steps. First, let’s take a function f(x) for that we have to compute minima (x*). Following are the steps:1. In the first iteration: the algorithm randomly selects the value. The first step is also called an Initialization step. And value or point we select randomly is called initialization point. Let’s say Initialization point is x0. In the below plot, initial point x0 is in +Ve side of Minima. As x0 is randomly selected so x0 may lie in +Ve or -Ve side of Minima. In my case, I’m assuming x0 is in +Ve side of minima. Either x0 lies in +Ve or -Ve side of Minima, all the steps would be the same. Below is the plot:2. In the second iteration: As we can see above x0 (Initial point) is very far from optimal(x*) so we’ll have to find other point x1 such that x1 is closer to optimal (x*) means x1 should be between x* and x0. So important question here is how to compute x1 such that it is closer to optimal one(x*)?First, for the sake of simplicity let’s consider a point x1 in between x0 and optimal (x*) on the plot. After that, we’ll look at mathematical formulation to compute the value for x1.So to find value for x1 we solve the following equation:Here “r” is the learning rate or step size (I’ll explain in detail in a later section for now just assume step size is 1 and is constant) and the step size is always positive.So on solving the above formulation, we’ll get value for x1 such that x1 is more closer to optimal minima. Okay, let me break it down-3. In the third iteration: So we got the x1 after the second iteration but x1 is still far from optimal value so now we’ll have to find another value x2 closer to x* means between x1 and x*. We’ll find this value for x3 in the same way as we computed in the previous step. First, let’s look at the below graph:Now again solving the same equation to find x2 as:So by solving the above equation, we’ll get value for x2 that would be now more closer to optimal x* or say minima. Here we have followed the exact same as the previous step but we are taking x1 instead of x0.But still, x2 is not that closer to minima now so we can repeat this iteration for multiple times or say k times. And in the end, we’ll have the value very close to minima.k. After kth iteration: Just assume we have run the algorithm for k-iterations and at the end of the kth iteration we have xk and that is very close to minima. This looks something like this on the plot:So if xk is very very close to minima then we can terminate the algorithm and take xk as an optimal solution.But the question here is after how many iterations or when to terminate the algorithm and also in real-world problems we won’t have any idea about minima/maxima so how one would find that xk is closer to minima/maxima or not?Well, the answer is pretty simple -In initial iterations, there would be larger updates towards minima but as iteration increases then update become smaller. When we start seeing very very small update then we can terminate the algorithm and consider xk as optimal one(x*). Let’s look at the below example:In the above figure, we can see that in the initial iterations there are larger updates. So in later iterations updates become smaller and after many iterations, there would be almost negligible updates like very very small so then we can terminate the algorithm and take that xk as the optimal one. Also adding to this — The reason why updates become smaller is that as we move towards minima from +Ve side then slop reduces so that results in smaller updates.Previously we took step-size (“r”) of constant value 1. It is not necessary that we take only constant step-size as sometimes there might be a problem if we take the constant step-size.In the below figure, I have basically tried to explain why constant step-size is not always a good idea. So I took a function x² for which I’m trying to compute minima and step-size is constant=1.So as you can see when I’m trying to compute xi+1 at xi then it just passes the minima. Again when I tried computing xi+2 to reach minima then again it passes the minima so there is a kind of oscillation due to the constant step size. If there would be a variable step size that reduces accordingly with iteration then we can be avoided such oscillation thing.We can add variable step-size as- step-size will be reduced by some factor after every 1 or 2 or 4 iterations. We can design this adaptive step size as per our need.Summary of Gradient descent:In Gradient descent we saw how we can solve any problem and can find maxima or minima of that problem with the help of this algorithm in an iterative manner. We saw how we can use derivative at a point to reach closer to minima. At a high-level Gradient descent tries to find the minima or maxima of a given problem in an iterative manner.In linear regression, our main goal is to find the optimal hyperplane w(w*) that minimizes the mean squared loss. So we can write our optimization problem as:Above w is variable and (xi, yi) is constant. For the sake of simplicity, I have excluded the intercept(b) and regularizer term. (I’ll add these in the code). So to find the optimal hyper-plane w* in each iteration we’ll have to compute Grad of the Loss function with respect to hyper-plane w(Grad means here we’re dealing with vector, not scaler). We can write it as:So if we want to solve this Loss function L(w) using gradient descent to find the optimal w that minimizes the mean squared loss. Then we can follow in this iterative manner:1.First iteration: Pick d-dimensional initialization vector w0 randomly. As [w1_0, w2_0, w3_0,…………]2. Second iteration: In the second iteration, we update the wo(old) to w1 (new) and to find w1 we will have to solve the equation and that would look something like this:In the above formulation: n is nos of points in the data set. w is a weight vector that we are trying to optimize.3. In the third iteration, we again update the w1(old) to w2(new) and to find w2 we solve the equation :k. In kth iteration: In the same way, we’ll run the algorithm for k iteration and at the end, we’ll have wk:So if there is a very small update in the loss with wk compared to loss with wk-1 then we can terminate the algorithm and take the wk as optimal w*. Here I’m assuming step-size (‘r’) is reducing with each iteration. We can try both with constant and variable then select the best approach.To summarize all:So to summarise all we can find the minimal or optimal solution for any machine learning problem with the help of gradient descent algorithm. The standard way is:Initialize the parameter we needed (Here weight vector w) and to update the initial value we compute grad of the loss function with respect to that parameter (here weight vector w) in each iteration. Like:In each iteration computing grad of the loss function with respect to the desired parameter (in this case we need optimal w so w.r to w hyper-plane). Means if we want to find intercept b then we will compute grad of loss function w.r to b to update the value of b and follow the same approach as above to find optimal b. Like:In below linear regression implementation section I’ll consider intercepting too then you’ll get a proper idea how can tune multiple parameters at once with the help of Gradient descent.If nos of data point(n) in data set very large then each iteration becomes very costly as in each iteration to compute grad of loss function we go through each data points. If nos of data points are like 100k or 400k then that’s still okay but assume if we have like 10 Million data points then that will require a lot of computing power and time too hence become costly. Consider the below figure to understand better :So in the above figure, we can see if n=10 Million then in each iteration we’ll have to go through 10Million points to compute grad of the loss function for that one iteration.Everything is the same as Gradient descent but only different is instead of taking n points to compute grad of loss function it randomly selects k sample from n data points in each iteration and computes grad of the loss function with that k data points in each iteration.It selects k such that 1<k<n. Also, one thing to note is — SGD requires more iterations to converge minima or optimal solution compare to GD (Gradient descent) but still SGD would be much faster than Gradient descent and it is shown that the result we get with SGD is very close to the Gradient descent. Well, I would say with Gradient descent we can get better result but to save compute power and time we choose SGD. This is like a trade-off between performance and computation cost.With SGD each step looks something like this:We can see in above that now in each iteration to compute grad of loss function It will take only k unit time instead of n unit time.Also, one thing to remember that It is not necessary to take k as a constant size and take a fixed number of the sample points from the population in each iteration. We can take any random number for ‘k’ in each iteration. Means if in itr_i k=1500 then it is not necessary for itr_1+1 we’ll take same 1500 number samples.[IPYNB of this example is available on my GitHub at [src]]Let me first explain some utility function that will be called by SGD algorithm in each iteration for the different purpose. Below are the utility functions:This function takes the train and test data, initial learning rate/step size value, argument for wether learning rate would be constant or variable and total epochs to run the SGD.At first, we do initialization of parameters like weight vector w and intercept b. There is a total of 13 features in data so weight vector would be of 13-dim. Intercept would be a single real-valued variable.Now we’ll have to update the initial weight vector and intercept as we saw previously in the mathematical formulation. So to update w and b, we’ll have to compute grad of function w.r to w and b first then use learning rate and existing w and b for an update.So as this is SGD so first we’ll randomly select k sample points from train data such that k<total nos of data points in train. So for this, we’re doing as:Now once we have the k sample points than with these points we’ll compute grad of the loss function w.r to weight vector w and intercept b. So first let’s look at the mathematical formulation for which we’ll write code:So to compute grad of loss function w.r to w and b we’ll have to iterate through k points and keep summing up the result to one variable after every iteration for “k” points (Not taking whole train data for an update. Taking the only k points that’s why called stochastic gradient descendent or SGD). So below code snippet does exactly the same thing.w_track and b_track are created, to sum up, the result from each iteration to one final variable.Once we’ll be done with computing grad of function w.r to weight vector and intercept then we can update the weight vector and intercept as.After the end of every epoch, we can update the learning rate if the learning rate is not constant. I tired this variable but with constant, I got a better result. So you’ll have to try both. Below is the code snippet.We also compute loss after each epoch to get a better idea about algorithm convergence towards an optimal solution. Below code computes and append the loss to the list.Let’s call the sgd_regressor function:Okay, so now we understand the code structure (If still confuse, please open IPYNB from this link. I have documented every step vert properly in the IPYNB) So we can call the function as:In the above code snippet, I have declared nos of epoch to run and learning rate. The function returns the optimal weight vector (w) and intercept (b), error on test and train in every epoch.After running SGD for 200 epochs with a constant learning rate, plot:So as we can see by the 150 epochs algorithm has already converged to the optimal weight and intercept and with that, we’re having minimal mse on both train and test. And that is what we wanted, optimal w and b that minimizes the mean squared error on the test.Mean squared error on test with our SGDRegressor model is 27 and with sklearn SGDRegressor model is 25.91.Well, the result is very good from our implementation of SGD. Very close to sklearn implementation of SGD Regressor. We can use this implementation in actual production as the end results are very good.You can experiment with a concept like early stopping to stop SGD algorithm to reduce the computation cost. As we can see that we already have an optimal result by 150 epochs but SGD algorithm is still running for input epochs. Also, you can try various type of approach to reduce the learning rate adaptively. Add regularizer to lose function then compute grad of the loss function. etc, etc.Just play with code and make a more optimized version of it. I’ll be happy to hear that. Just drop a mail or you can message me on Linkedin too.Github: https://github.com/rumankhan1/Linkedin: https://www.linkedin.com/in/rumankhan1/References:https://www.appliedaicourse.com/https://en.wikipedia.org/wiki/Loss_function",04/11/2019,0,32,11,"(651, 214)",44,8,0.0,9,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,trust/acceptance
135,[筆記][CVPR2020] ALAE: Adversarial Latent Autoencoders,,Mackerel Chang,,4.0,54,"對抗概念融入Autoencoder，並以StyleGAN證明品質Paper: https://arxiv.org/pdf/2004.04467.pdfOfficial code (Pytorch): https://github.com/podgorskiy/ALAE與以往VAE或是Autoencoder相同概念，只是在訓練中，藉由對抗機制去學習 latent spaces 的分布情形。Inference時，只選擇特定Encoder與Decoder組合成Autoencoder。ALAE修改自GAN的原型，特別將Generator G 與 Discriminator D 均細分成兩個網路。詳細結構如下圖：從整體架構來看，就是Autoencoder加上GAN而已。作者有特別強調：F跟E神經網路出來的是一個 latent spaces w。F是用來確認真實資料的Distribution，E藉由G所給的圖片產生latent spaces，推斷F應該的Distribution。在training中有分為兩列，在第一列中：第二列則是：透過真實數據產生的 w ，在 D目標為 real。在inference中，選用E與G(w,η)組合成Autoencoder。上方兩個等式，就是Autoencoder常見的機率分布等式。q(x)代表從 z 到E所產出的分布加總，其中 qG(), qF(w), pη(η) 為各自獨立分布，不從同一個神經網路出來，因此在表示上為相乘。qE(w)同理q(x)只是E直接讀入真實數據。最終目標V就是透過Generator與Discriminator做GAN機制；∆則是理想上要越低越好。在StyleALAE中有兩個Network，一個是 Instance Normalization (IN norm)組成的 Encoder E ；另一則是 AdaIN Normalization (AdaIN)組成的Generator G 。 E 在過IN norm的資訊進入一般的 MLP(e.g. FCN) C 將所有資訊相加。這樣能將每層不同資訊做線性組合，產生一個 latent spaces 。這同樣的spaces通過一般MLP的 A 輸入到每層的AdaIN並先結合 noises B 。這樣就完成整個架構。以下是原圖透過StyleALAE轉到第二列的結果：從FID或是PPL與StyleGAN對比，都有驚人的分數。",27/04/2020,0,4,0,"(553, 220)",9,3,0.0,2,ca,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
136,Why do girls drop out of school and what are the consequences of dropping out?,World of Opportunity,World Bank,102000.0,5.0,976,"Also available in: العربية | Français | EspañolGlobally, nine in ten girls complete their primary education, but only three in four complete their lower secondary education. In low income countries, less than two thirds of girls complete their primary education, and only one in three completes lower secondary school. The consequences for girls of dropping out of school prematurely are severe. A World Bank report estimates the losses in lifetime productivity and earnings for girls of not completing 12 years of education at $15 trillion to $30 trillion dollars globally. This is because on average, women with secondary education earn twice as much as those with no education, while the gains from primary education are much smaller.Universal secondary education for girls would have many other benefits. It could virtually eliminate child marriage (marrying before age 18) and reduce substantially early childbearing (having a first child before the age of 18). It could also reduce fertility rates in countries with high population growth, and increase women’s decision-making ability and psychological well-being. Finally, it would have large benefits for young children, including by reducing under-five mortality and malnutrition.Why do adolescent girls drop out of school? We need to understand the constraints faced by adolescent girls when thinking about what can be done to improve educational opportunities for girls. When parents are asked in surveys why their daughters dropped out of school, issues related to the cost of schooling (out-of-pocket and opportunity costs), early marriages and pregnancies, a lack of learning while in school, and a lack of interest in remaining in school often come up. In some countries, some factors play a larger role, while in other countries, other factors may be more prominent. But in many countries, even if this may not appear explicitly in survey responses by parents on reasons for girls dropping out, social norms and gender roles also affect the ability of girls to remain in school.Consider the case of Niger, one of the countries with the lowest levels of educational attainment for girls in the world. Analysis of household surveys as well as ethnographic field work suggest that six main obstacles lead most girls to not pursue their education beyond the primary level.1. Poor learning outcomes and cost. Rural government schools are so poor in quality and resources that many children graduate from primary school without learning to read. The schools do not charge tuition, but parents complain that the cost of uniforms, guard fees, transport, lunches and the opportunity costs of losing their daughters’ labor are hardly worth the poor learning outcomes they see.2. Failure at examinations. Students can only take the primary school completion exam twice. If they fail, they are ineligible to continue in public education. When girls fail examinations, parents say that they have little choice but to begin looking for a suitable suitor which their daughter could marry.3. Lack of nearby secondary schools. Few rural communities have their own secondary school and there are few boarding schools serving communities. Parents must send their children to nearby towns and cover the costs of transportation and room and board. Students stay with relatives or contacts and parents are reluctant to leave their daughters without what they consider proper oversight.4. Forced withdrawal of married adolescents. Once a girl is married, she is likely to be expelled from school. Husbands show little interest in supporting their adolescent wife’s education especially if they must enroll in a private school. This is an expense that they cannot afford. Conversely, the fear of not being allowed to withdraw their daughters from school for marriage is a complaint of some parents.5. Never enrolling in school or enrolling too late. Some families never enroll girls in school, perhaps in part because parents had no educational opportunities themselves. In some cases, teachers may refuse to enroll children that are considered too old to start primary school.6. Influence of relatives and demands on first daughters. Extended family members may influence parents on the value of educating girls, not always with positive outcomes. Schooling decisions may also depend on household composition and the activities of other children. Being the first daughter lessens a girl’s chances of going to school as they are expected to help their mother at home during the day.Policies to improve educational opportunities for girls must take into account country context, but promising interventions are emerging from the literature not only for educating girls, but also for delaying marriage and childbearing. For educating girls, interventions specific to girls may help increase access and thereby educational attainment. By contrast, to improve learning and make it worthwhile for girls and their families to invest in education, successful interventions may not need to target girls specifically. For delaying marriage and childbearing, education interventions tend to be the most successful, and more so than safe space programs without incentives for girls to remain in school.Beyond interventions to improve education opportunities and delay marriage and childbearing, programs providing economic opportunities for women help in making investments in education more attractive to girls and their families, as noted in a study on the cost of gender inequality in earnings.To conclude, the negative impacts of not educating girls are both substantial and wide-ranging, with economic costs running in the trillions of dollars. Ensuring universal primary education is not enough, as the benefits from education are much larger at the secondary and tertiary levels. Investing in proven programs and policies will be key to ensure a better future for girls and enable countries to fulfill their development potential. This makes economic sense. It is also the right thing to do.Read more about the World Bank’s work on girls’ education here.Read more World Bank blogs.Full report: Missed Opportunities: The High Cost of Not Educating GirlsFactsheet: Missed Opportunities: The High Cost of Not Educating GirlsPress Release: Not Educating Girls Costs Countries Trillions of Dollars, Says World Bank",01/08/2018,0,10,6,"(700, 440)",4,0,0.0,11,fr,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
137,Classifying Websites with Neural Networks,Knowledge from Data: The Datafiniti Blog,Datafiniti,1100.0,5.0,845,"At Datafiniti, we have a strong need for converting unstructured web content into structured data. For example, we’d like to find a page like:and do the following:Both of these are hard things for a computer to do in an automated manner. While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:OrBoth of these pages share many similarities to the actual product page, but also have many key differences. The real challenge, though, is that if we look at the entire set of possible web pages, those similarities and differences become somewhat blurred, which means hard and fast rules for classifications will fail often. In fact, we can’t even rely on just looking at the underlying HTML, since there are huge variations in how product pages are laid out in HTML.While we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page, doing so would be extremely time consuming, and frankly, incredibly boring work. Instead, we can try using a classical technique out of the artificial intelligence handbook: neural networks.Here’s a quick primer on neural networks. Let’s say we want to know whether any particular mushroom is poisonous or not. We’re not entirely sure what determines this, but we do have a record of mushrooms with their diameters and heights, along with which of these mushrooms were poisonous to eat, for sure. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonousWe would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible.Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. By constantly adjusting A and B this way, we can quickly get to the best possible values for them.In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:For a more detailed explanation of neural networks, you can check out the following links:In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. Our input layer modeled several features, including:Our output layer had the following:Our algorithm for the neural network took the following steps:The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. This works like so:So how did we do? In order to determine how successful we were in our predictions, we need to determine how to measure success. In general, we want to measure how many true positive (TP) results as compared to false positives (FP) and false negatives (FN). Conventional measurements for these are:Our implementation had the following results:These scores are just over our training set, of course. The actual scores on real-life data may be a bit lower, but not by much. This is pretty good! We should have an algorithm on our hands that can accurately classify product pages about 90% of the time.Of course, identifying product pages isn’t enough. We also want to pull out the actual structured data! In particular, we’re interested in product name, price, and any unique identifiers (e.g., UPC, EAN, & ISBN). This information would help us fill out our product search.We don’t actually use neural networks for doing this. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. Instead, we use a variety of heuristics specific to each attribute we’re trying to extract. For example, for product name, we look at the <h1> and <h2> tags, and use a few metrics to determine the best choice. We’ve been able to achieve around a 80% accuracy here. We may go into the actual metrics and methodology for developing them in a separate post!We feel pretty good about our ability to classify and extract product data. The extraction part could be better, but it’s steadily being improved. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. This means that we can scan the entire Internet and pull out any available data that exists out there. Exciting stuff!You can connect with us and learn more about our business, people, product, and property APIs and datasets by selecting one of the options below.",29/05/2013,0,3,2,"(596, 320)",8,6,0.0,7,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
138,Unsupervised Learning: K-means vs Hierarchical Clustering,Towards Data Science,Valentina Alto,2500.0,4.0,519,"While carrying on an unsupervised learning task, the data you are provided with are not labeled. It means that your algorithm will aim at inferring the inner structure present within data, trying to group, or cluster, them into classes depending on similarities among them.There are two main clusterization algorithms which I’d like to dwell on:Let’s see something more about them.The first step of this algorithm is creating, among our unlabeled observations, c new observations, randomly located, called ‘centroids’. The number of centroids will be representative of the number of output classes (which, remember, we do not know). Now, an iterative process will start, made of two steps:Every time the process is reiterated, some observations, initially classified together with one centroid, might be redirected to another one. Furthermore, after several reiterations, the change in centroids’ location should be less and less important since the initial random centroids are converging to the real ones. This process ends when there is no more change in centroids’ position.Now, how can we decide the number of centroids?There are many methods that could be employed for this task. However, in this article, I’m going to explain and use the so-called ‘Elbow method’. The idea is that what we would like to observe within our clusters is a low level of variation, which is measured with the within-cluster sum of squares (WCSS):And it is intuitive to understand that, the higher the number of centroids, the lower the WCSS. In particular, if we have as many centroids as the number of our observations, each WCSS will be equal to zero. However, if we remember the law of parsimony, we know that setting the highest number possible of centroids would be inconsistent.The idea is picking that number of centroids after which the reduction in WCSS is irrelevant. The relation I’ve just described can be represented with the following graph:The idea is that, if the plot is an arm, the elbow of the arm is the optimal number of centroids.This algorithm can use two different techniques:Those latter are based on the same ground idea, yet work in the opposite way: being K the number of clusters (which can be set exactly like in K-means) and n the number of data points, with n>K, agglomerative HC starts from n clusters, then aggregates data until it obtains K clusters; divisive HC, on the other hand, starts from just one cluster and then splits it depending, again, on similarities, until it obtains K clusters. Note that, when I say similarities, I’m referring to the distance among data points, which can be computed in different ways (I will dwell on this later on).Let’s visualize both the agglomerative and divisive techniques:As anticipated, the key element of discrimination here is similarity among data points. In mathematical terms, similarity mainly refers to distance, and it can be computed with different approaches. Here, I will propose three of them:So, to recap, both the algorithms look for similarities among data and both use the same approaches to decide the number of clusters. Choosing the one rather than the other really depends on the kind of task you’re facing.",08/07/2019,0,0,6,"(625, 283)",8,6,0.0,0,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
139,Conversations with GPT-3,,Kirk Ouimet,,4.0,470,"At this point, I think I have spent way too much time playing with OpenAI’s beta API for GPT-3. I am so impressed by the language capabilities of this model. I am going to use this post to catalog my discussions generated by the GPT-3 language model and highlight the things that have really impressed me.Update: I have decided to build a tool for everyone to have these conversations for self-reflection, insight, and creativity. You can now sign up on the waitlist.Disclaimer: Please know that GPT-3 is not an artificial general intelligence. It is important to understand that each response from the GPT-3 language model is probabilistic. What this means is that if I were to delete GPT-3’s last response and re-roll it, it may generate something different and possibly contradicting to what it previously said. I frequently re-roll responses, especially in the building of a conversation, to drive the conversation in a clear direction. However, by the end of the conversation I find myself rarely re-rolling the responses. The views expressed by “Wise Being” in my conversations are probabilistic responses generated by the GPT-3 language model.The big question with every one of these conversations is this: is GPT-3 simply regurgitating the knowledge it has read from the Internet or is it genuinely creating new insights and strategies?The way to interact with OpenAI’s the latest language model, GPT-3 (GPT stands for generative pre-trained transformer), is simple — you send it any text and it returns what it predicts will come next. To build GPT-3, OpenAI fed in every public book ever written, all of Wikipedia (in every language), and a giant dump of the Internet as of October, 2019. They then spent $12 million of compute power having the AI read all of this content and make connections. They ended up with a 700GB model that needs to sit on 48 16GB GPUs in order to process input and return responses. The number of connections the model has is two orders of magnitude away from the number of connections the human brain has.This is a big deal, and I’ll explain why. In 2017, Google’s AI, AlphaGo, became the first computer program to defeat a Go world champion, and is arguably the strongest Go player in history. The world of Go was shocked to see AlphaGo implement new and novel strategies. First, AlphaGo learned from human Go players. Now, human players learn from AlphaGo. The student has become the master.GPT-3 has been trained on most of what humanity has publicly written. All of our greatest books, scientific papers, and news articles. We can present our problems to GPT-3, and just like it transcended our capabilities in Go, it may transcend our creativity and problem solving capabilities and provide new, novel strategies to employ in every aspect of human work and relationships.",16/07/2020,0,2,0,,0,1,0.0,29,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
140,Why the tails come apart,,Gregory Lewis,106.0,10.0,2008,"Many outcomes of interest have pretty good predictors. It seems that height correlates to performance in basketball (the average height in the NBA is around 6'7""). Faster serves in tennis improve one’s likelihood of winning. IQ scores are known to predict a slew of factors, from income, to chance of being imprisoned, to lifespan.What’s interesting is what happens to these relationships ‘out on the tail’: extreme outliers of a given predictor are seldom similarly extreme outliers on the outcome it predicts, and vice versa. Although 6'7"" is very tall, it lies within a couple of standard deviations of the median US adult male height — there are many thousands of US men taller than the average NBA player, yet are not in the NBA. Although elite tennis players have very fast serves, if you look at the players serving the fastest serves ever recorded, they aren’t the very best players of their time. It is harder to look at the IQ case due to test ceilings, but again there seems to be some divergence near the top: the very highest earners tend to be very smart, but their intelligence is not in step with their income (their cognitive ability is around +3 to +4 SD above the mean, yet their wealth is much higher than this).[1]The trend seems to be that even when two factors are correlated, their tails diverge: the fastest servers are good tennis players, but not the very best (and the very best players serve fast, but not the very fastest); the very richest tend to be smart, but not the very smartest (and vice versa). Why?One candidate explanation would be that more isn’t always better, and the correlations one gets looking at the whole population doesn’t capture a reversal at the right tail. Maybe being taller at basketball is good up to a point, but being really tall leads to greater costs in terms of things like agility. Maybe although having a faster serve is better all things being equal, but focusing too heavily on one’s serve counterproductively neglects other areas of one’s game. Maybe a high IQ is good for earning money, but a stratospherically high IQ has an increased risk of productivity-reducing mental illness. Or something along those lines.I would guess that these sorts of ‘hidden trade-offs’ are common. But, the ‘divergence of tails’ seems pretty ubiquitous (the tallest aren’t the heaviest, the smartest parents don’t have the smartest children, the fastest runners aren’t the best footballers, etc. etc.), and it would be weird if there was always a ‘too much of a good thing’ story to be told for all of these associations. I think there is a more general explanation.[Inspired by this essay from Grady Towers]Suppose you make a scatter plot of two correlated variables. Here’s one I grabbed off google, comparing the speed of a ball out of a baseball pitchers hand compared to its speed crossing crossing the plate:It is unsurprising to see these are correlated (I’d guess the R-square is > 0.8). But if one looks at the extreme end of the graph, the very fastest balls out of the hand aren’t the very fastest balls crossing the plate, and vice versa. This feature is general. Look at this data (again convenience sampled from googling ‘scatter plot’) of this:Or this:Or this:Given a correlation, the envelope of the distribution should form some sort of ellipse, narrower as the correlation goes stronger, and more circular as it gets weaker:[2]The thing is, as one approaches the far corners of this ellipse, we see ‘divergence of the tails’: as the ellipse doesn’t sharpen to a point, there are bulges where the maximum x and y values lie with sub-maximal y and x values respectively:[3]So this offers an explanation why divergence at the tails is ubiquitous. Providing the sample size is largeish, and the correlation not too tight (the tighter the correlation, the larger the sample size required), one will observe the ellipses with the bulging sides of the distribution.Hence the very best basketball players aren’t the very tallest (and vice versa), the very wealthiest not the very smartest, and so on and so forth for any correlated X and Y. If X and Y are “Estimated effect size” and “Actual effect size”, or “Performance at T”, and “Performance at T+n”, then you have a graphical display of winner’s curse and regression to the mean.It would be nice to have an intuitive handle on why this happens, even if we can be convinced that it happens. Here’s my offer towards an explanation:The fact that a correlation is less than 1 implies that other things matter to an outcome of interest. Although being tall matters for being good at basketball, strength, agility, hand-eye-coordination matter as well (to name but a few). The same applies to other outcomes where multiple factors play a role: being smart helps in getting rich, but so does being hard working, being lucky, and so on.For a toy model, pretend that wealth is wholly explained by two factors: intelligence and conscientiousness.[4] Let’s also say these are equally important to the outcome, independent of one another and are normally distributed. So, ceteris paribus, being more intelligent will make one richer, and the toy model stipulates there aren’t ‘hidden trade-offs’: there’s no negative correlation between intelligence and conscientiousness, even at the extremes. Yet the graphical explanation suggests we should still see divergence of the tails: the very smartest shouldn’t be the very richest.The intuitive explanation would go like this: start at the extreme tail — +4SD above the mean for intelligence, say. Although this gives them a massive boost to their wealth, we’d expect them to be average with respect to conscientiousness (we’ve stipulated they’re independent). Further, as this ultra-smart population is small, we’d expect them to fall close to the average in this other independent factor: with 10 people at +4SD, you wouldn’t expect any of them to be +2SD in conscientiousness.Move down the tail to less extremely smart people — +3SD say. These people don’t get such a boost to their wealth from their intelligence, but there should be a lot more of them (if 10 at +4SD, around 500 at +3SD), this means one should expect more variation in conscientiousness — it is much less surprising to find someone +3SD in intelligence and also +2SD in conscientiousness, and in the world where these things were equally important, they would ‘beat’ someone +4SD in intelligence but average in conscientiousness. Although a +4SD intelligence person will likely be better than a given +3SD intelligence person (the mean conscientiousness in both populations is 0SD, and so the average wealth of the +4SD intelligence population is 1SD higher than the 3SD intelligence people), the wealthiest of the +4SDs will not be as good as the best of the much larger number of +3SDs. The same sort of story emerges when we look at larger numbers of factors, and in cases where the factors contribute unequally to the outcome of interest.When looking at a factor known to be predictive of an outcome, the largest outcome values will occur with sub-maximal factor values, as the larger population increases the chances of ‘getting lucky’ with the other factors:So that’s why the tails diverge.There’s also a geometric explanation. The R-square measure of correlation between two sets of data is the same as the cosine of the angle between them when presented as vectors in N-dimensional space (explanations, derivations, and elaborations here, here, and here).[5] So here’s another intuitive handle for tail divergence:Grant a factor correlated with an outcome, which we represent with two vectors at an angle theta, the inverse cosine equal the R-squared. ‘Reading off the expected outcome given a factor score is just moving along the factor vector and multiplying by cosine theta to get the distance along the outcome vector. As cos theta is never greater than 1, we see regression to the mean. The geometrical analogue to the tails coming apart is the absolute difference in length along factor versus length along outcome|factor scales with the length along the factor; the gap between extreme values of a factor and the less extreme values of the outcome grows linearly as the factor value gets more extreme. For concreteness (and granting normality), an R-square of 0.5 (corresponding to an angle of sixty degrees) means that +4SD (~1/15000) on a factor will be expected to be ‘merely’ +2SD (~1/40) in the outcome — and an R-square of 0.5 is remarkably strong in the social sciences, implying it accounts for half the variance. The reverse — extreme outliers on outcome are not expected to be so extreme an outlier on a given contributing factor — follows by symmetry.I think this is interesting in and of itself, but it has relevance to Effective Altruism, given it generally focuses on the right tail of various things (What are the most effective charities? What is the best career? etc.) It generally vindicates worries about regression to the mean or winner’s curse, and suggests that these will be pretty insoluble in all cases where the populations are large: even if you have really good means of assessing the best charities or the best careers so that your assessments correlate really strongly with what ones actually are the best, the very best ones you identify are unlikely to be actually the very best, as the tails will diverge.This probably has limited practical relevance. Although you might expect that one of the ‘not estimated as the very best’ charities is in fact better than your estimated-to-be-best charity, you don’t know which one, and your best bet remains your estimate (in the same way — at least in the toy model above — you should bet a 6'11"" person is better at basketball than someone who is 6'4"".)There may be spread betting or portfolio scenarios where this factor comes into play — perhaps instead of funding AMF to diminishing returns when its marginal effectiveness dips below charity #2, we should be willing to spread funds sooner.[6] Mainly, though, it should lead us to be less self-confident.[1] Given income isn’t normally distributed, using SDs might be misleading. But non-parametric ranking to get a similar picture: if Bill Gates is ~+4SD in intelligence, despite being the richest man in america, he is ‘merely’ in the smartest tens of thousands. Looking the other way, one might look at the generally modest achievements of people in high-IQ societies, but there are worries about adverse selection.[2] As nshepperd notes below, this depends on something like multivariate CLT. I’m pretty sure this can be weakened: all that is needed, by the lights of my graphical intuition, is that the envelope be concave. It is also worth clarifying the ‘envelope’ is only meant to illustrate the shape of the distribution, rather than some boundary that contains the entire probability density: as suggested by homunq: it is an ‘pdf isobar’ where probability density is higher inside the line than outside it.[3] One needs a large enough sample to ‘fill in’ the elliptical population density envelope, and the tighter the correlation, the larger the sample needed to fill in the sub-maximal bulges. The old faithful case is an example where actually you do get a ‘point’, although it is likely an outlier.[4] It’s clear that this model is fairly easy to extend to >2 factor cases, but it is worth noting that in cases where the factors are positively correlated, one would need to take whatever component of the factors which are independent of one another.[5] My intuition is that in cartesian coordinates the R-square between correlated X and Y is actually also the cosine of the angle between the regression lines of X on Y and Y on X. But I can’t see an obvious derivation, and I’m too lazy to demonstrate it myself. Sorry![6] I’d intuit, but again I can’t demonstrate, the case for this becomes stronger with highly skewed interventions where almost all the impact is focused in relatively low probability channels, like averting a very specified existential risk.",15/04/2015,0,1,14,"(581, 406)",9,0,0.0,17,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,surprise/amazement
141,What are Autoencoders?,Taiwan AI Academy,Yu-Ru Tsai,141.0,16.0,583,"常常見到 Autoencoder 的變形以及應用，打算花幾篇的時間好好的研究一下，順便練習 Tensorflow.keras 的 API 使用。後記: 由於 Tensorflow 2.0 alpha 已於 3/8號釋出，但此篇是在1月底完成的，故大家可以直接使用安裝使用看看，但需要更新至相對應的 CUDA10。首先，什麼是 Autoencoder 呢? 不囉唆，先看圖吧！Autoencoder 最原始的概念很簡單，就是丟入一筆 input data 經過類神經網路後也要得到跟 input data一模模一樣樣的 data。首先，整個 Autoencoder 可以拆解成 Encoder 和 Decoder 兩個神經網路。Encoder 先吃進 input data，經過神經網路後壓縮成一個維度較小的向量 Z，接著，在將 Z 輸入 decoder中，將 Z 還原成原始大小。聽起來很容易，但是我們仔細來瞧瞧，這件事情是不是那麼的容易我們就一步一步的分開來看！Encoder 負責將原始的輸入 Input data，壓縮成成低維向量 C，這個 C，我們通常稱作 code、latent vector 或者特徵向量，但我習慣叫它 latent space，因為 C 表示隱藏的特徵。Encoder 可以將原始 data 可以壓縮成有意義的低維向量，也就表示 Autoencoder 有降維的功能，經過的 hidden layer 擁有非線性轉換的 activation fuction，那麼這個 Encoder 就像是強力版的 PCA，因為 Encoder 能夠做到 non-linear dimension reduction!Decoder 要做的事情就是將 latent space 盡量地還原回去 input data，是一將低維空間的特徵向量轉換到高維空間中。那麼怎麼衡量 Autoencoder 的工作狀況呢 !? 很簡單，就是去比較兩個原始的 input data 與 reconstructed data 的相似程度。因此我們的 Loss function 可以寫為….因為要盡可能縮小兩筆 data 的差異，我們很自然的就使用 mean square error 做為 loss function (但實作上都使用cross_entropy，這個後面的章節再講XDD )。AutoEncoder 的訓練方法跟一般的神經網路一樣，都是用 Backpropagation 去更新權重。做個小結，Autoencoder是….1. 很常見到的模型架構，也是常見的 unsupervised learning。2. 可以做為 non-linear 的 dimension reduction 的一種方法3. 可以學到 data representation，有代表性的特徵轉換講到這邊，應該可以很輕鬆的理解，Autoencoder是怎麼樣的的一個東西了，架構很簡單，那再來就來看看他的變形吧！講解了 AutoEncoder 的基本原理之後，那就可以來看一下 Autoencoder 的延伸進階使用，讓大家看一下 Autoencoder 的廣泛使用！Unet 可以作為 image segmentation 實作的其中一個手段，而 Unet 的架構也可以看作為 Autoencoder 的變型。但訓練時 input 為一個 image，而 ouput 則是 segmentation 的 mask。這是一個將新的輸入文字結合其他輸入的 latent space的網路，這個網路的目的是情緒分類，而如此這個網路便可以提取不斷輸入的 sparse 文字，找出重要的 latent space。這個也可以看作是 Autoencoder 的變形。Sequence to Sequence 是前陣子很紅的生成式模型 ，它精彩地解決了 RNN 類型無法處理不定長配對的困境，在 chatbot、文字生成等主題上都有很好的表現。而這個也可以看做一種 Autoencoder 的架構。看完了千奇百怪的 Autoencoder 變形後，來看看 Autoencoder 還可以在哪些場合使用吧！Autoencoder 也可以用於 weight 的 pretrain，pretrain 的意思即為讓模型找到一個較好的起始值。用下圖來舉例，當我們想要完成的目標模型如 target。hidden layer 分別為: [1000, 1000, 500]，因此在一開始我們使用autoencoder 的概念輸入784維，中間的 latent space 為 1000 維先去做 pretrain，因此這 1000 維就可以很好的保留住 input 的資訊，接著我們把原本的output拔掉，在接上第二層，以此類推的做下去。這樣子整個模型就會得到一個比較好的起始值。咦! 有個地方怪怪的…如果你用 1000 neuron 去表示 784 維的 input 的話，不就代表 network 只要全部複製一遍就好了嗎!? 還需要什麼訓練呀 ˋˊ！ 沒錯，所以有在像這樣的 pre-train 時，通常我們都會加上 L1 norm 的 regularizer，讓hidden layer 不要全部複製一遍。根據李宏毅老師的說法，在過去比較會使用如此的方法去做 pre-train，現在由於 training skill 的增加，已經不需要使用這種方法了。但是如果你有的 label data 非常少，但卻有一堆的沒 label data 時，你就可以用這樣的方法去做 weight pre-train ，因為 Autoencoder 本身就是一個 unsupervised learning 的方法，我們用沒 label 過的 data 先得到 weight 的 pre-train，再用有 label 的 data 去 fine-tune 權重，如此一來就可以得到不錯的模型。詳細的細節可以再看李老師的影片，講解的很好！剛剛看到的 Unet 模型，我們就再來看看，因為這基本上就是台灣製造業最常見到的 defect detection 的問題。首先我們要先將 input data label 好，這會是我們的 output，我們要做的事情就是建構一個 network，輸入原始的圖片(左圖，牙齒 X 光照)後，要產生output ( 右圖，牙齒構造分類) 。那在這種 case 上，encoder & decoder 都會是圖形判斷較為強大的 convolution layers，萃取出有意義的特徵，並在 decoder 裡 deconvolution 回去，便可以得到 segmentation 的結果。像這樣的 image caption 問題，想當然爾的就用到 sequence to sequence 模型拉，input data 是一堆的照片，output 則是描述照片的一段文字，在這邊的sequence to sequence 模型，我們會使用 LSTM + Conv net 當作 encoder & decoder，可以很好的描述一連串的有順序性的動作 & 使用 CNN kernel 來提取 image 中必須要 latent space。但這個 task 用想的就很難，就讓有興趣的大家去試試看摟！Image Retrieval 要做的事情就是輸入一張圖片，然後試著去找出最相近的，但若以 pixel-wise 的去比照，那麼其實很容易找到完全不相干的東西，因為機器根本沒有學到特別的特徵，只是一個 pixel 一個 pixel 的去算 loss。若使用 Autoencoder 的方式，先將圖片壓縮到 latent space，再對 image 的 latent space 計算 similarity，那麼出來的結果便會好上許多，因為在 latent space 裡的每一個維度，可能就代表了某種特徵。若在 latent space 上算距離，便很合理的可以找到相似的圖片。尤其這會是一種 unsupervised learning 的方法，不用去 label data ，是個很棒的做法！以下是著名的 Deep Learning 大神 Georey E. Hinton 提出的論文，左上角的圖示想要搜索的圖，而其他的皆為被 autoencoder 認為很像的圖，的確 autoencoder 似乎能夠看到 input data 的精髓，但也有些抓出來的圖片怪怪的XDD找不到很好的圖片，只好用這張了LOL尋找 anomalies 也是超級常見的製造業問題，那這個問題也能拿 Autoencoder 來試試看！先來談談實際的 anomalies 的發生狀況，anomaly 發生次數通常都很少，比如說機台的訊號異常，溫度突然飆升……等。這種狀況了不起一個月發生一次(再多應該機器要被強迫退休了)，如果每 10 秒都搜集一次 data，那麼每個月會搜集到26萬筆 data，但是卻只有一段時間的資料是 anomalies，那其實這就會是一筆非常 unbalance 的資料。那麼對於這種狀況，最好的方法就是去拿原有的資料，畢竟那是我們擁有最大量的資料！若我們能將好的資料拿去訓練出一個好的 Autoencoder，這時若有 anomalies 進來，那麼自然地 reconstruct 後的圖形就會壞掉。這便是我們使用 autoencoder 去找出 anomalies 的起始想法!在此我們使用 Mnist 當作 toy example，並使用 Tensorflow.keras 高階API 實作一個 Autoencoder! 由於 tf.keras 的新套件釋出不久，我們就練習使用文件裡的 Model subclassing 作法實作 autoencoder。首先先 import tensorflow!tf.keras 的使用方法就這麼簡單，打上tf.keras 就是了!(不知所云的一段話lol)首先我們先把 Mnist data 從 tf.keras.dataset 拿出來，且做一下preprocessNomalize: 將值壓縮到 0 ~ 1之間，訓練會較易收斂Binarization: 將比較明顯的地方凸顯出來， training 會得到比較好的結果。這個是 Tensorflow Document 提供的創建模型方法。Subclassing 提供了較靈活的繼承方式，但可能會比較複雜。我目前看下來的心得是，在__init __的地方，創建 layer，並 define forward pass 在 call 裡面。因此我們用兩個 sequential model 去分別創建 Encoder & decoder。兩個是一個對稱的結構，encoder 先接上 input layer 負責承接 input data 再接上三個 dense layer。Decoder 一樣，用 input layer 承接 latent space，再使用三個dense layer 做 reconstruction。再來在 call 下面，我們要將 encoder & decoder 串接起來，首先先定義 latent space 為 self.encoder 的 output，然後 reconstruction 為 通過 decoder 的結果。接著我們使 tf.keras.Model() 將兩個 model 我們在 __init__定義的model 接起來。AE_model = tf.keras.Model(inputs = self.encoder.input, outputs = reconstruction) 指定 input 為 encoder 的 input，output 為 decoder 的 ouput 這樣就 OK 哩！再來就是 return 接起來的 AE_model 就可以摟!Keras 在訓練模型前，必須要先 compile，之後才能丟進去訓練。在 compile 中指定要使用的 optimizer 跟想要計算的 loss。訓練的話就使用 VallinaAE.fit 把資料餵進去就可以了，但要記得我們是要還原自己，所以 X 跟 Y 要丟入一樣的資料。我們來看一下訓練了 100 個 epoch 的結果，看起來 reconstruct 的結果並沒有想像中的好。顧名思義，我們可以使用 Convolution layer 作為 hidden layer 去學習更能夠reconstruct 回去的特徵。模型建立方法也非常的類似，只是我們的 encoder & decoder 架構稍微改變了一下，改成了 Conv layer 跟 max pooling layer。那改了模型後，可以再來看一下結果，看是不是有進步呢？看起來 Reconstruction 的結果好非常多，訓練下去大概 10 個 epochs 左右就快速收斂到了非常低的 loss。另外，autoencoder 也可以實現去雜訊的功能，這邊我們試著製作一個有去噪功能的 denoise-autoencoder！首先可以先將原始圖片加入 noise，加入 noise 的邏輯為在原始的 pixel 先做一個簡單 shift 之後再加上random的數字。這個當作我們的有 noise 的 input data。Image preprocess 的部份我們可以簡單的做 clip 小於 0 的就為 0，大於1 的就為 1。如下圖，在經過 clip 的影像處理後，我們可以讓圖片變得更銳利，明顯的地方變得更明顯一些。如此一來在丟進 Autoencoder 時可以得到更好的結果。而模型本身跟 Conv_AE 沒有差異，只是在 fit 的時候要改成denoise_AE.fit(train_images_noise, train_images, epochs=100, batch_size=128, shuffle=True)要將原始的 noise_data 還原成原始 input data可以看到，在 Mnist 上，簡單的 Autoencoder 就可以很快速的達到去噪的效果。這邊提供一下很好的學習資源給有興趣的人可以繼續參閱3. 李宏毅老師的 deep learning 也是非常推薦，講得很清晰易懂，非本科系的也能夠聽得懂的解釋，堪稱 machine learning 界的最紅 youtuber!使用這篇文章快速地向大家建立 Autoencoder 的基本概念，一些 Autoencoder 的變形以及運用場景。簡單的說 Autoencoder 除了最基本的 latent reprsentation 外，還有這麼多的運用接下來的幾篇會往 generative models 著手，從 autoencoder 的概念延伸，VAE，再到 GAN，還有一路下去的延伸與變形。喜歡文章的話可以多按幾個 claps(不要只按一次喔) 作為鼓勵！閱讀本文完整程式碼:https://github.com/EvanstsaiTW/Generative_models/blob/master/AE_01_keras.ipynb",09/03/2019,0,8,6,"(700, 312)",18,2,0.0,13,zh-tw,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
142,An implementation guide to Word2Vec using NumPy and Google Sheets,Towards Data Science,Derek Chia,398.0,10.0,1666,"This article is an implementation guide to Word2Vec using NumPy and Google Sheets. If you you have trouble reading this, consider subscribing to Medium Membership here!Word2Vec is touted as one of the biggest, most recent breakthrough in the field of Natural Language Processing (NLP). The concept is simple, elegant and (relatively) easy to grasp. A quick Google search returns multiple results on how to use them with standard libraries such as Gensim and TensorFlow. Also, for the curious minds, check out the original implementation using C by Tomas Mikolov. The original paper can be found here too.The main focus on this article is to present Word2Vec in detail. For that, I implemented Word2Vec on Python using NumPy (with much help from other tutorials) and also prepared a Google Sheet to showcase the calculations. Here are the links to the code and Google Sheet.The objective of Word2Vec is to generate vector representations of words that carry semantic meanings for further NLP tasks. Each word vector is typically several hundred dimensions and each unique word in the corpus is assigned a vector in the space. For example, the word “happy” can be represented as a vector of 4 dimensions [0.24, 0.45, 0.11, 0.49] and “sad” has a vector of [0.88, 0.78, 0.45, 0.91].The transformation from words to vectors is also known as word embedding. The reason for this transformation is so that machine learning algorithm can perform linear algebra operations on numbers (in vectors) instead of words.To implement Word2Vec, there are two flavors to choose from — Continuous Bag-Of-Words (CBOW) or continuous Skip-gram (SG). In short, CBOW attempts to guess the output (target word) from its neighbouring words (context words) whereas continuous Skip-Gram guesses the context words from a target word. Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at its neighbouring words, we can attempt to predict the target word.According to Mikolov (quoted in this article), here is the difference between Skip-gram and CBOW:Skip-gram: works well with small amount of the training data, represents well even rare words or phrasesCBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent wordsTo elaborate further, since Skip-gram learns to predict the context words from a given word, in case where two words (one appearing infrequently and the other more frequently) are placed side-by-side, both will have the same treatment when it comes to minimising loss since each word will be treated as both the target word and context word. Comparing that to CBOW, the infrequent word will only be part of a collection of context words used to predict the target word. Therefore, the model will assign the infrequent word a low probability.In this article, we will be implementing the Skip-gram architecture. The content is broken down into the following parts for easy reading:To begin, we start with the following corpus:natural language processing and machine learning is fun and excitingFor simplicity, we have chosen a sentence without punctuation and capitalisation. Also, we did not remove stop words “and” and “is”.In reality, text data are unstructured and can be “dirty”. Cleaning them will involve steps such as removing stop words, punctuations, convert text to lowercase (actually depends on your use-case), replacing digits, etc. KDnuggets has an excellent article on this process. Alternatively, Gensim also provides a function to perform simple text preprocessing using gensim.utils.simple_preprocess where it converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long.After preprocessing, we then move on to tokenising the corpus. Here, we tokenise our corpus on whitespace and the result is a list of words:[“natural”, “language”, “processing”, “ and”, “ machine”, “ learning”, “ is”, “ fun”, “and”, “ exciting”]Before we jump into the actual implementation, let us define some of the hyperparameters we need later.[window_size]: As mentioned above, context words are words that are neighbouring the target word. But how far or near should these words be in order to be considered neighbour? This is where we define the window_sizeto be 2 which means that words that are 2 to the left and right of the target words are considered context words. Referencing Figure 3 below, notice that each of the word in the corpus will be a target word as the window slides.[n]: This is the dimension of the word embedding and it typically ranges from 100 to 300 depending on your vocabulary size. Dimension size beyond 300 tends to have diminishing benefit (see page 1538 Figure 2 (a)). Do note that the dimension is also the size of the hidden layer.[epochs]: This is the number of training epochs. In each epoch, we cycle through all training samples.[learning_rate]: The learning rate controls the amount of adjustment made to the weights with respect to the loss gradient.In this section, our main objective is to turn our corpus into a one-hot encoded representation for the Word2Vec model to train on. From our corpus, Figure 4 zooms into each of the 10 windows (#1 to #10) as shown below. Each window consists of both the target word and its context words, highlighted in orange and green respectively.Example of the first and last element in the first and last training window is shown below:# 1 [Target (natural)], [Context (language, processing)][list([1, 0, 0, 0, 0, 0, 0, 0, 0]) list([[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]])]*****#2 to #9 removed****#10 [Target (exciting)], [Context (fun, and)][list([0, 0, 0, 0, 0, 0, 0, 0, 1]) list([[0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0]])]To generate the one-hot training data, we first initialise the word2vec() object and then using the object w2v to call the function generate_training_data by passing settings and corpus as arguments.Inside the function generate_training_data, we performed the following operations:With our training_data, we are now ready to train our model. Training starts with w2v.train(training_data) where we pass in the training data and call the function train.The Word2Vec model consists of 2 weight matrices (w1 and w2) and for demo purposes, we have initialised the values to a shape of (9x10) and (10x9) respectively. This facilitates the calculation of backpropagation error which will be covered later in the article. In the actual training, you should randomly initialise the weights (e.g. using np.random.uniform()). To do that, comment line 9 and 10 and uncomment line 11 and 12.Next, we start training our first epoch using the first training example by passing in w_t which represents the one-hot vector for target word to theforward_pass function. In the forward_pass function, we perform a dot product between w1 and w_t to produce h(Line 24). Then, we perform another dot product using w2 and h to produce the output layer u(Line 26). Lastly, we run u through softmax to force each element to the range of 0 and 1 to give us the probabilities for prediction (Line 28) before returning the vector for predictiony_pred, hidden layer h and output layer u.I have attached some screenshots to show the calculation for the first training sample in the first window (#1) where the target word is ‘natural’ and context words are ‘language’ and ‘processing’. Feel free to look into the formula in the Google Sheet here.Error — With y_pred, h and u, we proceed to calculate the error for this particular set of target and context words. This is done by summing up the difference between y_pred and each of the context words inw_c.Backpropagation — Next, we use the backpropagation function, backprop, to calculate the amount of adjustment we need to alter the weights using the function backprop by passing in error EI, hidden layer h and vector for target word w_t.To update the weights, we multiply the weights to be adjusted (dl_dw1 and dl_dw2) with learning rate and then subtract it from the current weights (w1 and w2).Loss — Lastly, we compute the overall loss after finishing each training sample according to the loss function. Take note that the loss function comprises of 2 parts. The first part is the negative of the sum for all the elements in the output layer (before softmax). The second part takes the number of the context words and multiplies the log of sum for all elements (after exponential) in the output layer.Now that we have completed training for 50 epochs, both weights (w1 and w2) are now ready to perform inference.With a trained set of weights, the first thing we can do is to look at the word vector for a word in the vocabulary. We can simply do this by looking up the index of the word against the trained weight (w1). In the following example, we look up the vector for the word “machine”.Another thing we can do is to find similar words. Even though our vocabulary is small, we can still implement the function vec_sim by computing the cosine similarity between words.If you are still reading the article, well done and thank you! But this is not the end. As you might have noticed in the backpropagation step above, we are required to adjust the weights for all other words that were not involved in the training sample. This process can take up a long time if the size of your vocabulary is large (e.g. tens of thousands).To solve this, below are the two features in Word2Vec you can implement to speed things up:Beyond that, why not try tweaking the code to implement the Continuous Bag-of-Words (CBOW) architecture? 😃This article is an introduction to Word2Vec and into the world of word embedding. It is also worth noting that there are pre-trained embeddings available such as GloVe, fastText and ELMo where you can download and use directly. There are also extensions of Word2Vec such as Doc2Vec and the most recent Code2Vec where documents and codes are turned into vectors. 😉Lastly, I want to thank to Note: This article first appeared at my blog https://derekchia.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets/github.comnathanrooy.github.iostats.stackexchange.comtowardsdatascience.com",06/12/2018,2,28,42,"(679, 314)",11,3,0.0,34,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,subjective,neutral,joy/calmness
143,Dynamic Pricing using Reinforcement Learning and Neural Networks,Towards Data Science,Reslley Gabriel,50.0,5.0,784,"The main goal of this project was to develop a dynamic pricing system to increase e-commerce profits by adapting to supply and demand levels.The pricing system should be able to manipulate a product’s final price in a robust and timely manner, reacting to offer and demand fluctuations in a scalable way.First, a simulator environment was created to mimic the fluctuation of order levels based on a few variables. Then, this simulation environment was used to train a Deep Reinforcement Learning agent to choose the best pricing policy for maximizing profits.The dynamic pricing system architecture consists of three fundamental parts. The PostgreSQL Database, hosted on Amazon RDS, the Flask API and Dash dashboard, hosted on Amazon EC2.Flask API is a Python RESTful framework that handles HTTP requests. It has two main uses, applying the reinforcement learning algorithm and providing access to data. It processes the data using a trained PyTorch model, saving the results in the database. It also provides a HTTP access to the data in JSON format.Before utilizing the e-commerce data for analysis and modeling, an extensive cleaning process was carried out. The main objective was to merge the datasets into a time series format. For any given product we gathered:During the ETL process, a few adjustments had to be made to ensure data quality. For instance, when analyzing competitor’s prices, we noticed that the dataset contained some products being sold for R$0.00 in brief time periods. Since it is highly unlikely that the product was being offered for free, such records were dropped from the dataset.From the values that remained, outliers caused by manual errors (e.g.: a product that sells for R$199.99 being advertised for R$19.99) were removed. This was achieved by excluding those values that fell out of the range of the mean price of a product plus or minus three times the standard deviation of its price.Modeling each product individually has many obstacles. When a product is first created, there is not enough historical data to model it as a time series. Another disadvantage occurs whenever a product has its supply interrupted.On the other hand, creating a single model for all of the portfolio would generate poor models, since this approach mixes products with very different behaviors.We solved the one-model-fits-all vs one-model-by-product trade-off by using a mixed approach. We clustered the products by a combination of type, group and price.For each product group+type combination, a categorization of 4 possible price ranges was created (A, B, C or D), according with the quartile ranges.By taking this approach, the sparsity of the data is reduced and new products with short time series can be priced based on similar items.Training a reinforcement learning solution using a real scenario often takes a lot of time and, as the agent does not have any experience in the beginning of the process, it may take bad decisions that could end up causing undesired losses.To avoid these problems we created an environment simulator using various models: Linear Regression, Decision Trees, Random Forests, Support Vector Machine, eXtreme Gradient Boost and Facebook’s Prophet.We ended up choosing the Linear Regression enviroment simulator because of its higher interpretability. Then, we applied a Reinforment Learning method called Deep Q-Learning.In a nutshell, a software agent that would take actions based on the current state of an environment. After taking an action, a reward would be given to the agent, scoring how good or bad the chosen action was. By experimenting actions and evaluating the rewards along time, the agent is trained to make the most appropriate decisions.In the e-commerce dynamic pricing problem we could map these concepts to:A fully connected Neural Network with 4 hidden layers of 30 nodes each was used. The input layer receives the state information (e-commerce’s prices, date parameters, inventory, shipping values, competitor’s prices), while the output layer consists of 10 possible actions: setting the retail price by multiplying the cost of the item by increments of 2.5 percentage points.This way, the agent will never sell a product at a loss, leaving the agent with the task of choosing the optimal balance between demand prices to optimize profits.In order to compare the results, both the original e-commerce pricing policy and the trained agent pricing policy were used on the simulator environment.Analyzing the financial results, the Reinforcement Learning agent outperformed the baseline pricing policy by 3.48%. This profit increase could improve merchants’ satisfaction with the e-commerce platform, which might rise the engagement rates.There may also be an improvement to the merchant’s pricing workflow, since manually operating changes in price is very time consuming.This approach could further be used in many other industries, like tourism, transportation and agriculture.Hope you enjoyed and if you have any doubt, reach us out.Thanks for reading!Co-authors: ",16/02/2021,0,21,32,"(684, 357)",5,2,0.0,8,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,expectation/interest
144,C# Plays Bejeweled Blitz,,Dan Scott,16.0,4.0,676,"As some of you reading this may or may not already know; over the past day or so I went from having the idea of creating a computer program that would essentially be able to play the popular arcade game Bejeweled Blitz on Facebook, to actually developing it. Now as hard as this problem sounds, it was surprisingly easy and fairly swift to solve. I broke it down in to 3 main steps:The first step was probably the most time consuming of them all as everything from there was just colour management. The Solution I came up with in the end for that was to take a screenshot of the entire screen, and then scan the image from top to bottom using a nested for loop until I found a funny shade of brown that only appears along the Top Edge of the bejeweled grid (for anyone wondering that colour is Color.FromArgb(255, 39, 19, 5)). Once this colour had been found using the bitmap.GetPixel(x, y) function, I broke out of both for loops and knew that was the point where the top left corner of the grid was. I could then use this to construct a rectangle which would extract the bejeweled grid from the full screenshot. The size of the rectangle was calculated using the size of the grid cells (40px², found that out using trusty old paint) multiplied by the amount of rows/columns there were (8, found that out using my eye balls). This resulted in the Rectangle size coming out at 320px².So the next step from here was to identify what colour resides in what square. To do that I started off by creating a 2 dimensional array of colours (Or Color’s to be politically correct) that was 8 rows and 8 columns to match that of the playable grid. I then systematically looped through the 2 dimensional array of colours in a nested for of x and y values assigning the array the colour of the pixel at the Location (x * 40) + 20, (y * 40) + 22. The x value was decided as it was half way through the gem and 22 was chosen for the y value as certain gems have a white center (Green and yellow) so 22 provided a more accurate reading. With this 2 dimensional array I was then able to generate a visual representation of what the computer was seeing when it was trying to figure out what colour was where.As you can see from the above screenshot it’s able to identify what gem is what colour depending on what pixel is at that magic 20, 22 of the cell. Another thing I thought about before I finished this project to the state it’s in now is to prevent the application from trying to switch 2 empty cells (because one gem has just been blown up or something), I added all the known color codes to their own array and ask if the colour that’s in the 2d array also resides within the known colours list, if it does it will then evaluate whether it can be moved to a winning square, if not it’s ignored entirely.I won’t bore you with the gory details of how I check if a gem can be moved, as instead this is a link to the beginning of the if statement in my Open Source Github Project. From here the full source code can be viewed, commented on and even improved upon if you guys feel like I could do something obviously better.Finally all that’s left to do by definition of this application is to actually move the Gems. This is done by making some Windows API calls to set the mouse location and simulate mouse clicks. Again the details of how to exactly do that are within the github project, but if I’ve kept your attention for this long all that’s left to say is thank you and if you have any further questions don’t hesitate to hit me up on here or twitter CoderDan.Thanks for reading.",25/12/2015,0,1,0,"(534, 421)",3,1,1.0,3,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,positive,trust/acceptance
145,DO YOU KNOW HUNGER?,,Idowu Fikayo,6.0,2.0,430,"Brethren, do you know hunger?If you are chewing the dictionary definition, you are wrong. So, I'll ask again, do you know hunger? Have you ever being hungry without hope of where victuals will fall from? A completely dashed hope that says ""its either food now or death now ""The second universal target of the sustainable development goals is to eradicate hunger and it explains the concept of hunger as not just the lack of food but takes into consideration the concept of malnutrition.Malnutrition simply depicts people that consume a negligible amout of nutrients needed for their body to function or at least well fed (in quantity) but the food does not contain the amount of nutrients their body needs. So, one can actually eat a mountain of noting. This is hunger.With this, we know that hunger does not restrict itself to the rural life. Hunger doesn't end in Syria, there are tons who are hungry right in the rich city of California. Tons of people who just feed because their intestine tells them to.The number of undernourished people reached 821 million in 2017.The rippling effect of this is a large loss of potential labour force and farmers mostly for the rural area and a chunk of people with unhealthy body and soul in our offices in the urban life.To cripple hunger and malnutrition within the SDG target line of year 2030, we must be swift and intentional about it. We must be intentional about making sure that good quality food is made cheap. So cheap that an average begger can afford at least a plate of decent well-nourished food at the end of the day’s work.Without trifle : TO END HUNGER, MAKE QUALITY FOOD CHEAP.If we are serious about ending hunger at all, this is what all stakeholders should strive to achieve. This and also making sure that the farmers who produced the food and everyone along the value chain doesn't get poor at the expense.An increase in food production without increase in purchasing power on the side of the poverty stricken population will simply create another chain of loss for farmers.If farmers are encouraged to produce, the population too must be encouraged to purchase. It all comes down to a balance between cost of production for farmers and the purchasing power of the multitude.It's against business sense to produce food for people who can't buy.This must be done and done fast because when you look into the eyes of a really hungry person, someone who truly knows hunger, it says just one phrase "" Food Now or Death Now"".",03/05/2019,0,0,0,"(700, 588)",1,0,0.0,0,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
146,RBM based Autoencoders with tensorflow,Machine Learning World,Illarion Khlestov,588.0,5.0,713,"Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in Semantic Hashing paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pre-trained with RBM autoencoders should converge faster. So I’ve decided to check this.This post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader’s previous knowledge of tensorflow and machine learning field. All code can be found in this repoRBMs different from usual neural networks in some ways:Neural networks usually perform weight update by Gradient Descent, but RMBs use Contrastive Divergence (which is basically a funky term for “approximate gradient descent” link to read). At a glance, contrastive divergence computes a difference between positive phase (energy of first encoding) and negative phase (energy of the last encoding).Also, a key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read here or here.As prototype one layer tensorflow rbm implementation was used. For testing, I’ve taken well known MNIST dataset(dataset of handwritten digits).At first, I’ve implement multilayers RBM with three layers. Because we do not use usual tensorflow optimizers we may stop gradient for every variable with tf.stop_gradient(variable_name) and this will speed up computation a little bit. After construction two questions arose:So I’ve run the model with all binary units and only with last binary unit. And it seems that model with only last layer binarized trains better. After a while, I note that this approach was already proposed in the paper, but I somehow miss this.So let’s stop with the last layer binarized and try different train approaches. To build model that will train only pair of layers we need train two layers model, save it, build new model with one more layer, load pre-trained first two layers weights/biases and continue train last two layers (code). During implementation I’ve met some trouble — tensorflow have no method to initialize all not initialized previously variables method. Maybe I just didn’t find this. So I’ve finished with approach when I directly send variable that should be restored and variables that should be initialized.After testing seems that both training approaches converge to approximately same error. But some another cool stuff — the model that was trained by pair lairs trains faster in time.So we stop with RBM trained with only last layer binarized and with two layers only strategy.After getting pre-trained weights from RMB, it’s time to build autoencoder for fine tuning. To get encoding layer output as much as possible binarized as per paper advice we add Gaussian noise before layer. To simulate deterministic noise behavior, noise generated for each input prior training and not changed during training. Also, we want compare autoencoder loaded from RBM weights with self-initialized usual autoencoder. Code for autoencoder.It seems that RBM initialized autoencoder continue training, but newly initialized autoencoder with same architecture after a while stuck at some point.Also, I’ve trained two autoencoders without Gaussian noise. Now we can see through distribution what embedding most similar to binary (code for visualization):We can see that RBM based autoencoder with Gaussian noise works better than other for our purposes.To validate received embeddings I generate them for test and train sets for such networks:and use two validation approaches:Train SVM with the train set and measure accuracy on the test set. SVM was used from sklearn with ‘rbf’ kernel with no max_iter == 50. Results table were generated with this codeWith Hamming distance or dot product find ten most similar pictures/embeddings to provided one and check how many labels are the same to the submitted array label. Code to check distance accuracies.As we can see embeddings can save some strong features, that can be used for future clusterization very well. But these features are not linearly correlated — so when we measure accuracy for most similar embeddings, we get results worse than when we use full MNIST images. Of course, maybe autoencoder should be trained with another learning rate/longer, but this is the task for future research.For RBM training such params were used network was trained with:For RBM training such params were used network was trained with:For autoencoder learning rate was changed to 1.0 because of another optimization rule.Originally published at ikhlestov.github.io on December 28, 2016.",28/12/2016,2,3,6,"(700, 327)",8,3,0.0,14,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
147,Gradient descent in Matlab/Octave,Geek Culture,Shaun Enslin,1100.0,5.0,554,"So, you have read a little on linear regression. In the world of machine learning it is one of the most used equations and for good reason.Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. … Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x).So, how about a quick tutorial on running gradient descent in Matlab / octave to predict house prices?Sound good?If you need some pre-reading, here are some good articles:If you prefer a video, then follow my 5 part series on Youtube. https://www.youtube.com/playlist?list=PLOEB0iByIwznDBU8aF9hVXALjwj5OdrnmYou can also download the source code here: https://github.com/shaunenslin/gradientdescentmatlabGradient descent allows to run through a few thousand thetas till we get to the lowest cost and thus the best theta to make a predition.SourceBesides gradient descent, we will be using the following formula’s. Our hypothesis function is used to predict results in linear regression. In our data below, we have 3 features and as such our hypothesis will be:If you have not had any exposure to hypothesis equation, have a look here.We will be working with a curated dataset. It will have 3 independent variables (x) which are all identical and the dependent variable (y) is always 1.5 * x. This allows us to check our values as we go and they are easy to calculate.We first need to load the dataset and split it into our X/Y axis. Lets normalise our X values so the data ranges between -1 and 0. This will assist a-lot with gradient descent and allow for a bigger learning rate and getting our lowest cost theta quicker. Finally, we add a column of ones to assist with our hypothesis calculation and make calculating the cost of each theta a simple matrix arithmetic calculation.Normalising is easily accomplished with some simple matrix arithmetic. Note the formulae in comments in line 2. This function normalises all our features (independent variables) to somewhere between -1 and 0. We also return a vector of the max’s and mins for each feature for later.To start off, gradient descent needs 3 thingsGradient descent now applies the learning rate to our cost derivative function for each of the features. See the formula below. If this is foreign to you, just read some of the articles above or watch my youtube series.So, lets create the code below and next we will look at the actual function. Once we have run gradient descent, we will get back our best theta as well as the cost of each theta as we made our way through gradient descent.Using matrix arithmetic, we can easily perform gradient descent. Again, if below is looking greek to you, then watch my YouTube series for a deeper understanding.You want gradient descent to call your cost function so for each repetition, you can calculate the cost and make sure the cost comes down. See below the equation which we will put into action below.Again, watch my youtube series to see playing around with the learning rate and how it impacts how quickly gradient descent gets to the lowest cost.Finally, back in our main program, lets:Running above, you should get a result of “9”If you would like to get talked through all this lot, go tohttps://www.youtube.com/playlist?list=PLOEB0iByIwznDBU8aF9hVXALjwj5Odrnm",14/06/2021,7,8,0,"(579, 201)",4,3,0.0,8,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
148,Analysis and Applications of Multi-Scale CNN Feature Maps,Towards Data Science,Mohammad Sanatkar,97.0,29.0,6853,"In this blog post, we present a formal treatment of receptive fields of convolution layers and characterizations of multi-scale convolutional feature maps using a derived mathematical framework. Using the developed mathematical framework, we compute the receptive fields and spatial scales of feature maps under different convolutional and pooling operations. We show the significance of pooling operations to ensure the exponential growth of spatial scale of feature maps as a function of layer depths. Also, we observe that without pooling operations embedded into CNNs, feature map spatial scales only grow linearly as layer depth increase. We introduce spatial scale profile as the layer-wise spatial scale characterization of CNNs which could be used to assess the compatibility of feature maps with histograms of object dimensions in training datasets. This use case is illustrated by computing the spatial scale profile of ResNet-50. Also, we explain how feature pyramid module generates multi-scale feature maps enriched with augmented semantic representations. Finally, it is shown while dilated convolutional filters preserve the spatial dimensions of feature maps, they maintain greater exponential growth rate of spatial scales compared to their regular convolutional filter counterparts.Reading this blogpost, you will have a deeper insight into the intuitions behind the use cases of multi-scale convolutional feature maps in the recent proposed CNN architectures for variety of vision tasks. Therefore, this blogpost can be treated as a tutorial to learn more about how different types of layers impact the spatial scales and receptive fields of feature maps. Also, this blogpost is for those engineers and researchers that are involved in designing CNN architectures and are tired of blind trial and error of which feature maps to choose from a CNN backbone to improve the performance of their models, and instead, prefer from the early steps of design process, to match the spatial scale profiles of feature maps with the object dimensions in training datasets. To facilitate such use cases, we have made our code base publicly available at https://github.com/rezasanatkar/cnn_spatial_scale.It is a general assumption and understanding that feature maps generated by the early convolutional layers of CNNs encode basic semantic representations such as edges and corners, whereas deeper convolutional layers encode more complex semantic representations such as complicated geometric shapes in their output feature maps. Such a characteristic of CNNs to generate feature maps with multi semantic levels is resultant of their hierarchical representational learning ability which is based on multi-layer deep structures. Feature maps with different semantic levels are critical for CNNs because of the two following reasons: (1) complex semantics feature maps are built on top of basic semantic feature maps as their building blocks (2) a number of vision tasks like instance and semantic segmentation benefit from both basic and complex semantic feature maps. A vision CNN-based architecture takes an image as input, and passes it through several convolutional layers with the goal of generating semantic representations corresponding to the input image. In particular, each convolution layer outputs a feature map, where the extent of the encoded semantics in that feature map depends on both the representational learning ability of that convolutional layer as well as its previous convolutional layers.One important characteristic of CNN feature maps is that they are spatial variance, meaning that CNN feature maps have spatial dimensions, and a feature encoded by a given feature map might only become active for a subset of spatial regions of the feature map. In order to better understand the spatial variance property of CNN feature maps, first, we need to understand why the feature maps generated by fully connected layers are not spatial variance. The feature maps generated by fully connected layers (you can thinks of the activations of neurons of a given fully connected layer as its output feature map) do not have spatial dimensions since every neuron of a fully connected layer is connected to all the input units of the fully connected layer. Therefore, it is not possible to define and consider a spatial aspect for a neuron activation output.On the other hand, every activation of a CNN feature map is only connected to a few input units, which are in each other spatial neighborhood. This property of CNN feature maps gives rise to their spatial variance characteristic, and is resultant from the spatial local structure of convolution filters and their spatially limited receptive fields. The differences between fully connected layers and convolutional layers which result in spatial invariance for one and spatial variance for the other one is illustrated in the below image where the input image is denoted by the green rectangle, and the brown rectangle denotes a convolutional feature map. Also, a fully connected layer with two output neurons is denoted by two blue and grey circles. As you can see, each neuron of the fully connected layer is impacted by all the image pixels whereas each entry of feature map is only impacted by a local neighborhood of input pixels.In this section, we formally define the spatial scales of CNN feature maps. CNN feature maps having spatial dimensions, their spatial scales are used to compute the spatial mappings between their entries and input image regions. In particular, the spatial scale of an entry of a given CNN feature map is defined as the pixel-wise size of the rectangle subregion of the input image that impact the value of that feature map entry. The simplest case is to compute the spatial scale of the first layer. For example, if the first layer of a CNN is a 3 x 3 convolutional layer, then the spatial scales of entries of the first layer feature map, are 3 x 3 pixel subregions of input images. Computing spatial scales of output feature maps of deeper CNN layers requires knowing the spatial scales and spatial overlaps of their input feature maps. In the latter sections, we derive the spatial scale and overlap formulas for output feature maps of different convolutional and pooling layers in terms of the their input feature maps’ spatial scales and spatial overlaps.Here, spatial overlaps is defined as the percentage of overlaps between spatial scales of two neighboring feature map entries. In the below image, we plot the spatial scale and overlap of two neighboring feature map entries where the green rectangle denote the input image and the brown rectangle denotes a feature map generated by one of the convolutional layer of the CNN. The spatial scale of the orange feature map entry is the shaded orange region over the input image and the spatial scale of the blue feature map entry is the blue shaded region over the input image. The spatial overlap between the neighboring blue and orange entries is the overlapping area over the input image between the two spatial scale regions.As an example for computing spatial overlap, let assume the first layer of a CNN is a 3 x 3 convolutional layer with stride of 1. Then, the first layer generated feature maps will have spatial scales of 3 x 3 and spatial overlaps of 6 pixels out of 9 pixels, which is about 67% spatial overlap. This percentage of overlap is because of choosing stride to be equal to 1. If the stride is chosen to be 2, then the spatial overlap will be equal to 33%, and stride of 3 results in 0% spatial overlap. In general, we should avoid very high spatial overlap percentage simply because the feature map entries will end up encoding redundant information with the cost of computation and memory resources. On the other hand, very low spatial overlap percentage results in aliasing effect in entries of generated feature maps.In this section, we discuss how CNNs rely on pooling operations to ensure the exponential growth rate of feature map spatial scales as a function of layer depths. First, we show that not embedding pooling operations in CNNs and only relying on convolution layers with stride 1 results in linear growth rate of spatial scales of feature maps. Then, pooling layers are shown to realize exponential growth rate of feature map spatial scales. Finally, a case study of two variants of CNNs with and without pooling operations is presented to illustrate the impact of using pooling operations on spatial scale growth rate of feature maps.Here, we show that a CNN consisting of only 3 x 3 convolutional layers with stride 1 can only demonstrates linear growth rate for feature map spatial scales. The spatial scale of the first layer feature map will be 3 x 3, simply due to the 3 x 3 receptive field over the input image. The spatial scale of the feature map output of the second layer 3 x 3 convolution with stride 1 will be equal to 5 x 5. It is because the receptive field of the second layer feature map is 3 x 3 with respect to the first layer feature map while the receptive field of the first layer feature map is 3 x 3 as well but with respect to the input image. Therefore, if you combine these two 3 x 3 receptive fields sequentially and consider the spatial overlaps of the receptive fields of neighboring entries of the first layer feature map, you can compute that the spatial scale of the second layer feature map with respect to the input image is 5 x 5. This is shown in the below image where for simplification, the input image is assumed to be 1 dimensional and the convolutional layers are assumed to 1 x 3 with stride 1. It can be observed that the spatial scale of the blue entry of the second layer feature map is 1 x 5 with respect to the input image.Based on the above example and induction, it can be shown that adding each 3 x 3 convolutional layer with stride 1, will increase the spatial scale of feature maps by 2 pixels along each dimension. Let S(n) denote the spatial scale of the nth layer feature map. Then, we can write S(n) = (3 + 2 * (n-1)) x (3 + 2 * (n-1)), which shows that the spatial scale growth rate with respect to layer depths is linear. This linear growth rate of spatial scales is mainly because of the significant spatial overlaps of neighboring feature map entries. The linear growth rate of spatial scales will be an issue if the vision task for which this network is used, requires relatively large spatial scale feature maps. For example, in order to increase the spatial scale of the final feature map (the feature map generated by the last CNN layer of the network) to 40 x 40 pixels of the input image, then the CNN network requires about 20 3 x 3 CNN layers. Because of the computational cost of convolutional layers during training and inference, in most cases, we prefer to avoid adding more and more convolutional layers just for the sake of meeting spatial scale requirement of feature maps.In most of the CNN architectures, convolutional layers are interleaved with pooling operations to increase the growth rate of spatial scales of feature maps via reducing the spatial overlap between feature map entries. Increasing the growth rate of spatial scales beyond linear growth rate, allows us to save adding more convolutional layers just for the sake of larger spatial scales. We can realize pooling operations via either explicit pooling layers or choosing the stride of convolutional filters to be greater than one. The most common pooling layer is 2 x 2 max pooling layer with stride 2. This pooling layer halves the spatial dimensions of feature maps, which means that it transforms an input feature map of size W x H x C (W x H denotes the spatial dimension of the feature map and C denotes the number of channels of the feature map) to an output feature map of size W/2 x H/2 x C. The main motivation of halving the spatial dimensions of feature maps is to reduce the spatial overlap between neighboring feature map entries.In this section, we derive spatial scale and overlap formulas for output feature maps of 2 x 2 pooling layers with stride 2 in terms of the spatial scale and overlap of their input feature maps. Let S and P denote the spatial scale and spatial overlap of the input feature map, respectively. Also, let S` and P` denote the spatial scale and spatial overlap of the output feature map of the max pooling layer. We can compute S`(S,P) = S + 2(1-P)S + (1-P)² S=(2-P)²S. The S` equation is derived based on the fact that the spatial scale of a feature map entry generated by this max pooling layer will be equal to aggregation of spatial scales of its 4 corresponding input neighboring feature map entries where (1-P) and (1-P)² factors ensure to only count once every sub-region of the union spatial scales of 4 input entries.In order to further clarify the derivation of the spatial scale formula for 2 x 2 pooling layers, we demonstrate the reasoning behind it in the below image. The green rectangle denotes the input image while the brown rectangle denotes the input feature map (with the dimension 5 x 7 x 1) to the pooling layer and the blue rectangle denotes the output feature map (with the dimension 3 x 4 x 1) of the pooling layer. To illustrate the derivation of S`, we consider a single entry (marked with grey color) of the output feature map. The spatial scale of the grey entry depends on the spatial scales and overlaps of its 4 corresponding input feature map entries as it show in the below image. Without loss of generality, we can assume that the first term S in S` = S + 2(1-P)S + (1-P)² S, is corresponding to the spatial scale of the orange entry of the input feature. Also, the second term 2(1-P)S take into account the spatial scales of the blue and purple entries of the input feature map where the factor (1-P) is to make sure that they do not overlap with the spatial scale of the orange entry. Finally, the term (1-P)² S is corresponding to the black entry of the input feature map where the factor (1-P)² is to ensure the the effective spatial scale of the black entry do not overlap with the already counted spatial scales of the orange, blue and purple entries.On the other hand, the absolute value of spatial overlap between each two neighboring entries after applying 2 x 2 max pooling will be equal to 2PS-P²S=P(2-P)S, where 2PS comes from the fact that 2 x 2 receptive fields of the two neighboring output entries are adjacent via two pairs of input entries. Finally, we need to subtract P²S to avoid counting the overlapping subregoins twice.The above reasoning to compute the absolute spatial overlap for 2 x 2 max pooling layers with stride 2 is illustrated in the below image where the green rectangle denotes the input image while the brown rectangle denotes the input feature map to the pooling layer and the blue rectangle denotes the output feature map of the pooling layer. In this illustration, without loss of generality, we focus on the spatial overlaps of two output neighboring entries marked with the grey and orange colors. Note that the receptive field of these two entries do not have explicit overlap because of the stride 2. However, they have effective overlap over their spatial scales based on their input adjacent pairs. In particular, the blue input entry of the grey output entry is adjacent to the green input entry of the orange output entry while the black input entry of the grey output entry is adjacent to the red input entry of the orange output entry. The absolute overlap of 2PS-P² S is marked as the purple dashed area over the input image. Each of adjacent pairs (blue, green) and (black, red) contribute to a PS term in the final value of absolute spatial scale while the subtraction of P²S is to ensure that we do not count the overlapping spatial scales twice.The spatial overlap P` is equal to the absolute spatial overlap P(2-P)S divided by S`. Therefore, we have P`(S, P) = P(2-P)S/((2-P)²S)=P/(2-P). This result is significant since it shows that P` is always smaller than P which concludes that the 2 x 2 pooling layer with stride 2 will always reduce the spatial overlap between neighboring feature map entries. For example, if the spatial overlap before applying the pooling operation has been 2/3, then it will reduce to 2/4 after applying the pooling.Here, we rely on the same line of reasoning as discussed in the previous section, to derive spatial scale and overlap formulas for 3 x 3 convolutional layers with stride 1. Let S and P denote the spatial scale and overlap of input feature map entries. Also, let S` and P` denote the spatial scale and spatial overlap of feature map entries at the output of the 3 x 3 convolutional filters with stride 1. Then, we can compute the spatial scale for 3 x 3 convolutional layers as S`(S, P) = S + 4(1-P)S + 4(1-P)²S. Next, we focus on computing the spatial overlap. In particular, computing the absolute spatial overlap of output feature map of 3 x 3 convolutional filters with stride 1 is more straightforward than computing the spatial overlap for 2 x 2 pooling layers with stride 2, and it is demonstrated in the below image.In the below image, the green rectangle denotes the input image while the brown and blue rectangles denote the input and output feature maps of 3 x 3 convolutional layer with stride 1. Note that stride 1 causes the spatial dimension of feature maps to not change, and therefore the spatial dimensions of both input and output feature maps are 4 x 6 x 1. Here, without loss of generality, we focus on computing the spatial overlap between the neighboring grey and orange output feature map entries. Because of stride 1, the overlapping receptive fields of these two output entries over the input feature map is 3 x 2 and is denoted by the shaded purple region over the brown input feature map.This 3 x 2 explicit overlap region simplifies computing the absolute value of spatial overlap between the grey and orange output entries. In particular, their absolute spatial overlap is equal to the aggregated spatial scales of the input feature map entries belonging to the 3 x 2 purple overlapping area. It can be shown that the absolute spatial overlap is equal to S + 3(1-P)S + 2(1-P)²S. In this example, the term S which is marked as a shaded blue region over the input image, is corresponding to the entry on the second row and first column of the purple shaded region. Also, the 3 terms (1-P)S are marked with black, red and green regions over the input image, and are corresponding to the top, right and bottom entries with respect to the entry on the second row and first column. Finally, the 2 terms (1-P)² S are marked as brown regions over the input image and are corresponding to the top-right and bottom-right entries of the purple overlapping area. Therefore, the spatial overlap P`(S,P) = (S + 3(1-P)S + 2(1-P)²S) / S` = (1 + (1-P)(5–2P))/(1+4(1-P)(2-P)).Using the derived spatial scale and overlap formulas for 3 x 3 convolutional layers with stride 1 and 2 x 2 max pooling with stride 2, we show the significance of applying pooling layers in order to increase the effective spatial scale of feature map entries. For our example, we consider the two following variations of 20 layers CNN: (1) a 20 layer CNN with 20 3 x 3 CNN layers with stride 1 and without pooling layers, the blue curve in the below image (2) a 20 layer CNN with 20 3 x 3 CNN layers with stride 1 interleaved with 2 x 2 max pooling layers with stride 2 every 4 CNN layers, the red curve in the below image.In the below plotted curves, the x-axis is the layer depth and y-axis is the spatial scale width (width of spatial scale is equal to square root of spacial scale) of the feature map entries generated by a CNN layer at the depth specified on x-axis. As you can see, in both cases, the spatial scale of feature map entries increases as layer depth increases. However, the spatial scale growth rate for the CNN with pooling layers (the red curve) is exponential whereas the spatial scale growth rate for the CNN without pooling layers (the blue curve) is linear. The exponential growth rate of spatial scales for the CNN with pooling layers results in its final feature map entries to have spatial scales of 243 x 243 while the spatial scale of the final feature map entries of the CNN without pooling layers only grows to 50 x 50. This means that the feature maps generated by CNN with pooling layers can encode objects as large as 243 x 243 pixels capture in input images while the CNN without pooling layers is only able to encode objects as large as 50 x 50 pixels.While designing CNN architectures, it is necessary to aggregate the statistics of object dimensions in the training datasets and to examine that the designed CNN architectures’ feature maps provide spatial scales that span all the major mods of object dimensions histograms. We call the layer-wise spatial scale of a CNN, its spatial scale profile. As a best practice, it is necessary to first design the spatial scale profile based on the histograms of training dataset’s object dimensions and then design the CNN architecture according to the spatial scale profile requirements.As an example, here, we compute the spatial scale profile of ResNet-50 (50-layer). As we mentioned before, the pooling operations in CNN architectures could be realized using either explicit pooling layers or choosing the stride of convolutional filters to be greater than one. For ResNets (Deep Residual Learning for Image Recognition), both CNN convolutional filters with strides greater than one and explicit pooling layers are used to reduce the spatial overlap between neighboring feature map entires. Among the 5 presented ResNet architectures (18-layer, 34-layer, 50-layer, 101-layer and 152-layer shown in the below table) in Deep Residual Learning for Image Recognition, we derive the spatial scale profile for ResNet-50. However, the presented results can be readily extended to the other 4 configurations. In ResNet50, three types of pooling operations are used: (1) 7 x 7 convolutional filters with stride 2 (2) 3 x 3 max pooling layer with stride 2 (3) 1 x 1 convolutional filters with stride 2.The 7 x 7 convolutional layer with stride 2 is only being used as the first layer of ResNet-50. Therefore, its output spatial scale and overlap computation can be simplified. In particular, the spatial scale of each feature map entry output of 7 x 7 convolutional filters is 7 x 7 since they are directly applied to input images. Also, the absolute spatial overlap between neighboring feature map entries is equal to 7 x 5. Therefore, its corresponding spatial overlap will be equal to (7 x 5)/(7 x 7) = 5/7.The 3 x 3 max pooling layer with stride 2 is only being used as the second layer of ResNet-50. Let S and P denote the spatial scale and overlap of input feature map entries, respectively, and S` and P` denote the spatial scale and spatial overlap of output feature map entries. The spatial scale of 3 x 3 pooling layers is equal to the spatial scale of 3 x 3 convolutional layers. Therefore, we have S`(S,P) = S + 4(1-P)S + 4(1-P)²S. On the other hand, we need to derive the spatial overlap of 3 x 3 pooling layers with stride 2 using the same techniques that we already discussed for the other layers. We can compute the spatial overlap to be as: P`(S,P) = (1+ 2(1-P)) / (1+4(1-P)(2-P)).The 1 x 1 convolutional layers with stride 2 are being used at different depth of ResNet-50 as the main pooling operation to reduce the spatial overlap between neighboring feature map entries and to ensure the exponential growth rate of spatial scale of feature map entries. The 1 x 1 convolutional filters having receptive filed of 1 x 1, they do not change the spatial scale of feature map entries while being applied to feature maps. However, having stride of 2, they modify the spatial overlap of the feature map entries. Let P and P` denote the spatial overlaps of feature map entries before and after applying 1 x 1 convolutional layers with stride 2, respectively. Then, we can compute P`(S,P) = 2 max(P - 0.5, 0). This implies that if the spatial overlap of input feature map entries is less than 0.5, the spatial overlap of output feature map entries will be zero since the receptive field of 1 x 1 convolutional filters is only 1 x 1 and stride 2 ensures that their neighboring output entries do not have overlapping receptive fields. Therefore, in order to have non-zero spatial overlap, it muse be carried out from the previous layers of the network.The below image plots the spatial scale profile of ResNet-50 where x-axis spans the layers of ResNet-50 and y-axis denotes the spatial scale width (spatial scale width is simply equal to square root of spatial scale) at each layer of ResNets-50. As it can be observed, the pooling operations used in ResNet-50 results in exponential growth rate of spatial scale of ResNet-50 as a function of layer depths. In particular, the final feature map generated by ResNet-50 has spatial scale of 483 x 483 pixels which implies that its feature map entries encode representation of objects as large as 483 x 483 pixels embedded in input images. Also, we observe that the spatial scale of ResNet-50 is piece-wise constant, which is caused by 1 x 1 convolutional layers with stride 1 that are mainly used in ResNet-50 to channel-wise dimensionality reduction of feature maps.In this section, we discuss why the majority of CNN architectures for different vision tasks rely on multi-scale feature maps to improve their detection, classification and regression performance. The answer is rather simple and straightforward. The objects captured in natural images manifest themselves in variety of pixel-wise dimensions. For example, if a car is 5 meter away from the camera, then it appears much larger in the image, compared to the case if it was 50 meter away from the camera. Therefore, for this example, we need two feature maps with different spatial scales where one is suitable when the car is 5 meter away from the camera and the other one for when the car is 50 meter away from the camera. Note even though the feature map with the larger spatial scale (matching the car image when it is 5 meter away from the camera) is also able to encapsulate the image of the car when it is 50 meter away from the camera, it does not provide accurate representation for the 50 meter away car since its spatial scale is not tight enough around the image car and contains significant leakage of information from the other objects in the scene.The issue of objects appearing in images with different pixel-wise dimensions is know as scale ambiguity of images, and along with occlusion and camera point-of-view variance, complicates the vision-based detection systems. The most well-known non-ML based method designed to address scale ambiguity of images is Scale-invariant feature transform (SIFT). Arguably, SIFT has been the main inspiration of deep learning multi-scale feature map image encoders. SIFT relies on image pyramid (resizing input images to different scales and aggregating detected key points after processing each scaled version of input image) and Gaussian smoothing filters with different σ² to detect blobs with different scales, as the key points of input images. Detecting blobs with different scales causes SIFT to generate reliable image encodings while presence of scale ambiguity in natural images.The semantic representations generated by a given CNN corresponding to an input image is the union of all feature maps generated by each convolutional layer of the CNN, and not only the final feature map. Relying on several feature maps provide the networks with different spatial scales.It is safe to say that in the area of CNN-based vision models, the strength of multi-scale feature maps manifested itself initially for 2d object detection models. In particular, SSD: Single Shot MultiBox Detector relies on VGG-16 as the backbone network (feature generator) to encode input images via 6 feature maps with different spatial scales. Different spatial scales of the feature maps allows SSD to detect objects with different pixel-wise dimensions embedded in input images.As another example, Feature Pyramid Networks for Object Detection (FPN) uses ResNet-50 and ResNet-101 as the backbone networks to generate multi-scale feature maps. In the case of ResNet-50, the feature map outputs of conv2_3 (11th layer), conv3_4 (23rd layer), conv4_6 (41st layer) and conv5_3 (50th layer) are chosen to provide feature maps with different spatial scales. Using the ResNet-50’s spatial scale profile that we presented in the previous section, we can compute the spatial scales of output feature maps of conv2_3, conv3_4, conv4_6 and conv5_3 as 35 x 35, 99 x 99, 291 x 291 and 483 x 483, respectively. Therefore, the selected feature maps can represent object with different sizes appeared in input images.However, the main contribution of FPN is to enhance the semantic representation capability of shallower layer feature maps, using the semantic information encoded in deeper layer feature maps. The main weakness of feature maps generated by shallow layers is that they are not semantically as rich as the feature maps generated by deeper layers. It is because the process of semantic encoding of input images into feature maps is a hierarchical process, meaning that the basic semantics appear in the early layer feature map, while the more complex semantics appear in the feature maps of deeper layers.To better understand why this will be an issue for object detection models, consider the following example with ResNet-50 as the backbone network. Assume a car is 50 meter away from the camera and it is captured in input image via a box of 35 by 35 pixels and therefore the feature map generated by conv2_3 (11th layer) is the best candidate to encode it. Also, assume a second car that is 5 meter away from the camera and its corresponding box in the input image has dimension of 480 by 480 pixels and therefore conv5_4 (50th layer) suits best to encode it. It is reasonable to assume that both these cars have similar semantic complexity, independently from their sizes in input images. However, we know that 11the layer of ResNet-50 is not semantically as rich as the 50th layer of ResNet-50.FPN solves this issue using a top-down path that transfers the semantics encoded by deeper layers to shallower layer via nearest-neighbor upsampling operation to ensure that the spatial dimensions of transferred deeper feature maps match the spatial dimension of shallower feature maps (shallower layer feature maps have larger spatial dimensions that feature maps of deeper layers). This process is demonstrated in the below image. Merging of semantics from deeper layer and shallower layers is realized by element-wise addition of upsampled feature maps of deeper layers and transformed versions of shallower layer feature maps under 1 x 1 lateral convolutional layers. Finally, to ensure that feature maps generated by these element-wise addition operation do no suffer from aliasing effect, they are filtered using 3 x 3 convolutional layers.In this section, we focus on dilated convolution layers (Multi-Scale Context Aggregation by Dilated Convolutions) which first introduced in 2016. Dilated convolutional filters different from regular convolution filters, perform sparse sampling of the input feature maps to increase the spatial scale of their output feature maps. Therefore, you can think of them as an alternative to pooling layers and convolutional layers with stride greater than one, in order to increase the spatial scales of CNN feature maps. Dilated convolutional filters perform sparse sampling of input feature maps based on holes embedded in their convolutional filters different from regular convolutional filters that do not any have holes and rely on dense local sampling of the input feature maps.To better understand how dilated convolutional filters are applied to input feature maps, in the below image, we show convolution operation of a regular 3 x 3 convolution filter on the left (red marks) versus convolution operation of a dilated 3 x 3 convolution filter with dilation rate of 2 on the right (blue marks). As you can see, the dilated convolution filter performs sparse sampling of its input feature map by skipping (the skips are referred as holes) every other input entries in each dimension. In other words, dilation rate of 2 means that the convolution filter samples only one entry per each 2 x 2 region of the input feature map. Therefore, in general, dilation rate of n is equivalent of sampling one entry per each n x n region of the input feature maps. Note that the regular convolution filters are special case of dilated convolution filters with dilation rate of 1.Dilated convolutions rely on the holes in their receptive fields to increase the spatial scale of their output feature map entries. In particular, the holes (skips) in the receptive fields of dilated convolution filters reduces the spatial overlap between sampled input feature map entries, which causes an increase in the spatial scale of output feature map entries. In the above example, the dilated convolution filter with dilation rate of 2, skips one feature map entry between every two consecutive sampled entries which results in decrease in spatial overlap between sampled feature map entries. The effective spatial scales of dilated convolution filters will be larger than the spatial scales of regular convolution filters if there is non-zero spatial overlap between neighboring input feature map entries.Here, first, we derive formula to compute the spatial scale and overlap of dilated 3 x 3 convolution filters with stride 1 and dilation rate of 2 and then, we compare the growth rate of their spatial scales with the grow rate of regular 3 x 3 convolutional filters with stride 1 and stride 2. Let S and P denote the spatial scale and overlap of input feature map, respectively. Also, let S` and P` denote the spatial scale and overlap of output feature maps of dilated 3 x 3 convolution filters with stride of 1 and dilation rate of 2, respectively. Then, we can compute S`(S,P) = 9S - 24Max(P - 0.5, 0)S. For the special case when P < 0.5, then S` will be equal to 9 times of the input spatial scale S, which is the maximum increase in spatial scale that we can expect from dilated 3 x 3 convolution filters with dilation rate of 2. In order to compute P`, two formulas are derived based on whether P < 0.5 or P >0.5. If P < 0.5, then P`(S,P) = 15P / [9 — 24Max(P — 0.5, 0)] and if P > 0.5, then P`(S,P) = [6 + 3(1- P)- (14 + 4(1- P))Max(P - 0.5, 0)] / [9–24Max(P — 0.5, 0)].Next, we compare the spatial scale growth rates of dilated 3 x 3 convolutional filters with stride of 1 and dilation rate of 2 versus regular 3 x 3 convolutional filters with strides of 1 and 2. In the below image, we plot this comparison in terms of layer depths. The blue curves refers to a 5 layer convolution network where all the 5 layers are dilated convolution layers with stride 1 and dilation rate of 2. The red curve is a 5 layer convolution network composed of 5 regular 3 x 3 convolutional layers with stride 2, and the green curve is corresponding to a 5 layer CNN network formed by 5 regular 3 x 3 convolutional layers with stride 1. Y-axis denotes the layer-wise spatial scale width where spatial scale width is equal to the square root of the spatial scale.As it can be observed, both dilated 3 x 3 convolutional layers with stride 1 and dilation rate 2, and regular 3 x 3 convolutional layers with stride 2 show exponential growth rate of spatial scale as a function of layer depth, whereas the growth rate of regular 3 x 3 convolutional layers with stride 1 is linear. That being said, the spatial scale growth rate of the dilated 3 x 3 convolution layer is greater than the spatial scale growth rate of the regular 3 x 3 convolution layer with stride 2. In particular, the spatial scale of the final feature map of the dilated convolution layer is 243 x 243 pixels whereas the spatial scales of the final feature maps of the 3 x 3 convolution layers with stride 2 and stride 1 are equal to 63 x 63 and 11 x 11, respectively.Even though regular 3 x 3 convolutional filters with stride 2 show exponential growth rate of spatial scales similar to the dilated convolutional filters, the regular convolutional filters with stride 2 result in halving the spatial dimension (width and height) of feature maps each time they are applied to feature maps. As we mentioned earlier, the pooling operations such as explicit pooling layers with stride greater than 1 and convolutional layers with stride greater than 1 rely on reducing the spatial dimensions of feature maps in order to increase the spatial scale of the entries of feature maps. On the other hand, the dilated convolution filters preserve the spatial dimension of feature maps based on their stride 1. Preserving spatial dimensions of feature maps while simultaneously increasing their spatial scales with exponential growth rate by dilated convolution filters make them to be compatible for vision tasks with dense predictions.Examples of vision tasks with dense predictions are semantic segmentation, instance segmentation, depth estimation and optical flow. For example, in the case of semantic segmentation, the goal is to make predictions for the class of each input image pixel. In other words, if the input image is W x H pixels, then a CNN designed for semantic segmentation is expected to generate W x H predictions for the classes of pixels. CNNs aiming at such dense predictions require feature maps with higher resolutions (larger spatial dimension) than CNNs that perform sparse detection tasks like image classification. For a sparse prediction vision task like image classification, CNNs only need to output a single prediction for the whole input image, and therefore high resolution feature maps are not as essential as for dense prediction tasks.For dense prediction vision tasks, each pixel corresponding prediction should be based on both the information embedded in the local neighborhood of the pixel as well as the global information referred as the image context. For example, consider the monocular (single camera) depth estimation task, global information like a pixel being part of a building provides the network with rough estimation of depth of the pixel, while local features like texture help the network to further refine its estimation for the the depth of the pixel. Preserving the local neighborhood information of a pixel in the feature maps requires the convolutional layers to preserve the spatial dimensions of feature maps, and that’s why dilated convolutional filters with stride 1 are essential for dense prediction vision tasks.Dilated convolution filters with stride 1 preserving the spatial dimensions of feature maps is not sufficient to ensure that the feature maps generated by these convolutional filters accurately encode the local neighborhood information of pixels. In particular, we need the dilated convolution filters with stride 1 to limit their spatial scales to local neighborhoods. That being said, dilated convolutional filters with their significant spatial scale exponential growth rates are suitable for encoding global image context information as well. In fact, a well-designed CNN network benefits from dilated convolution filters with a range of different dilation rates in parallel, to encode a group of feature maps with a range of different spatial scales to represent both local and global information concurrently. This idea is used in the following paper.In Deep Ordinal Regression Network for Monocular Depth Estimation, they generate feature maps with different spatial scales using three parallel dilated convolution layers (referred as ASPP module in the below image) with kernel size of 3 x 3 and dilation rates of 6, 12 and 18. These three convolutional layers with their different dilation rates are applied with stride of 1 and zero-padding to ensure that their output feature maps have the same spatial dimensions in order to be concatenated along their channel dimension. The unified feature map resultant of channel-wise concatenation of the feature maps generated by these 3 dilated convolution layers, is a multi-scale feature map with 3 different spatial scales that encode both local and global information per each entry of feature map.The next step for this project is to derive spatial scale and overlap formulas for more number of variations of convolutional and pooling layers, and add them to the code base.",15/04/2020,0,14,16,"(578, 452)",13,0,0.0,9,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
149,From GAN basic to StyleGAN2,Analytics Vidhya,Akihiro FUJII,447.0,16.0,3085,"This post describes GAN basic, StyleGAN, and StyleGAN2 proposed in “Analyzing and Improving the Image Quality of StyleGAN”. The outline of the post is as follows.— 2.1. Frechet Inception Distance— 2.2. Perceptual Path Length— 3.1. Progressive Growing— 3.2. AdaIN— 3.3. Network Architecture of StyleGAN— 3.4. Mixing Regularization— 5.1. summary and key insights— 5.2. StyleGAN2’s methods— — 5.2.1.Normalization method instead of AdaIN— — 5.2.2. A high-resolution image generation method instead of Progressive Growing— — 5.2.3. Path Length Regularization to smooth latent space— 5.3. ResultsGAN stands for Generative Adversarial Networks and is a framework for generating images using unsupervised learning.There are two networks called Generator, which generates images from noise(latent variables), and Discriminator, which distinguishes between images generated by Generator and real images.GAN is characterized by learning Generator and Discriminator alternately using the following objective function. Here, the Generator generates image data with the same dimensions as the real image using the latent z as input, and the Discriminator outputs a value between 0 and 1 by the sigmoid function of the final layer.During the Discriminator learning, generator parameters are fixed. And the discriminator is learned to output 1 when a real image is inserted (first term in the equation), and learned to output 0 when a generator generated image is inserted (second term in the equation).In other words, train to maximize the entire objective function.On the other hand, at the time of Generator learning, the parameters of Discriminator are fixed. And the generator is learned to output the second term of the equation to 0. That is, the value of D (G (z)) is learned to be 1. Since the first term is a fixed value, training is performed to minimize the overall objective function.In this way, the Generator and Discriminator repeatedly learn to work together, and eventually, the Generator can generate fake images that can be mistaken for real images.Because GAN is unsupervised learning, there are no established metrics like Accuracy or F1 score for supervised learning. Here, I introduce the frequently used metric called Frechet Inception Distance, and the Perceptual Path Length proposed by StyeGAN.Frechet Inception Distance (FID) is a measure of how close the distribution of the generated image is to the that of the original image.However, since the images are embedded in a high-dimensional space, the distance of the distribution cannot be easily measured. Therefore, the basic concept of FID is to have the image embedded in a low-dimensional space using a recently developed image recognition model whose accuracy exceeding humans, and to measure the distribution distance in that space.The definition formula is as follows, and the Wasserstein-2 distance is calculated by the low-dimensional embedded vector by the network called Inception V3. m and c are mean vectors and covariance matrices in the embedding space. The one with the subscript w is the generated image, and the one without it is the real image. Since the it indicate the distance of the distribution, the smaller the value, the more fake the image looks like a real image, that is, it indicate the Generator has better performance.Perceptual Path Length (PPL) is an indicator of whether the image changes smoothly in “perceptual”. Uses the distance of the image embedded in the trained model, similar to FID.Roughly speaking, it indicates whether the image changes on the shortest “perceptual” path in the latent space that is seed of fake images” This is illustrated in the figure below.Suppose we have a latent variable z1 that produces a white dog and a latent variable z2 that produces a black dog. Since the elements other than color are not changed, taking the intermediate data, it is ideal that the latent variable changes along a path that changes between white and black. That is, it is a target that a gray dog ​​comes out. In other words, the blue path that changes color only is the “perceptually” shortest distance. Conversely, the green path that changes the shape of the object and once passes through an image like a bedroom is a “perceptually” long distance.This is quantified by Perceptual Path Length(PPL), and the definition formula is as follows. This is the expected value of distance between the image generated by the latent variable obtained by mixing the two latent variables z_1 and z_2 at the ratio t, and the image generated by the latent variable obtained by mixing the two at the ratio t + ε.The figure as below describe that. If the data mixed by t and the data mixed by t + ε are close to “perceptual”, take a small value.This is an example of generating an image using latent variables around t added small changes ε1 and ε2. The blue path has a shorter “perceptual” distance because we feel a similarity between gray dogs at t and t + epsilon 1.However, in the green path, the image of the bedroom comes out, “perceptual” distance is big because we feel dissimilarity between the gray dog ​​and the bedroom.Similar to FID here we use a embedded image distance with trained network called VGG.Since it cannot be determined analytically, the result of performing this calculation on many images and taking the expected value is the PPL value. The lower this value, the more perceptually smooth the latent space is.There are many researches about GANs, but one of the most famous model is StyleGAN [1], which was proposed at end of 2018. The figure below is an image generated by StyleGAN. You can see that a high-resolution image is generated that can be mis-recognized as a real one.The StyleGAN network has two features: generating high-resolution images using Progressive Growing, and incorporating image styles into each layer using AdaIN. First, I will explain these two and move on to the detailed structure of StyleGAN.Progressive Growing is a method for generating high-resolution images proposed in Progressive-Growing GAN(PG-GAN) [2]. Roughly speaking, it is a method of generating high-resolution images by starting with low-resolution images and gradually adding high-resolution Generators and Discriminators. The figure below is a schematic diagram of PG-GAN.In the above figure, they start with 4x4 image generation, gradually increase the resolution, and finally generate a 1024x1024 high-resolution image. Even if a network for increasing the resolution is added, D and G for resolutions continue learning.AdaIN is a normalization method for style transfer proposed in 2017 by Xun Huang et al. The math expression is as follows[3]. Normalize content input x and style input y using mean and variance.Note that AdaIN is that, unlike other normalization methods such as Instance Normalization, normalization is performed only with style and content image statistics, and no learning parameters are used. This makes it possible to convert styles that have never been seen in training data.AdaIN is used in the following formula in StyleGAN. The concept of applying a linear transformation using style to the normalized content information has not changed. But y_s and y_b, linear transformation to the style vector W described later, are used instead of the standard deviation and average value of the style.The Key points of StyleGAN network is as follows.(A) in the figure is PG-GAN and (b) is StyleGAN. Both employ progressive growing, which gradually increases the resolution. However, while PG-GAN generates an image from stochastic latent variable z, StyleGAN generates an image from a fixed 4x4x512 tensor. And stochastic latent variables are used as style vectors in StyleGAN.In addition, the stochastic latent variables are not used as they are, but are nonlinearly transformed by a fully connected network called Mapping Network before being imported as style.FID score has been improved by the image generation by this fixed value tensor, the introduction of Style by the Mapping network, and the mixing regularization described later.StyleGAN uses a regularization method called Mixing Regularization, which mixes two latent variables used in Style during training. For example, if there is a style vector w_1, w_2 mapped from latent variables z_1, z_2, use w_1 to generate a 4x4 image and use w_2 to generate an 8x8 image.By doing this, they can mix the styles of the two images as shown below.In the image above, the latent variables that generate the Source A and Source B images are used, and the results are obtained by experimenting using the latent variables of A at first and using the latent variables of B from a certain resolution.They are experimenting with three resolutions of inserting style B: low resolution (4²-8²), medium resolution (16²-32²), and high resolution (64²–1024²) .Basically, the influence of the style entered from low resolution tends to be large, and using the latent variable of B from low resolution will make the shape of the face, skin color, gender age, etc. close to B . However, at high resolution, it can only affect the background and hair color.As we have seen, StyleGAN can produce beautiful images, but it is known that some noise and unnatural parts occur.First, let’s take a look at the noise like water droplets in the figure below.This noise does not always appear in the generated images, but is a problem that occurs in all feature maps at a resolution of around 64x64. The authors believe that this problem is due to the normalization layer (AdaIN). When a feature map with a small spike-type distribution comes in, even if the original value is small, the value will be increased by normalization and will have a large influence. In fact, removing the normalization seems to make the droplet disappear.Next is a mode in which some of the features do not follow the movement of the face. In the image below, even if the face changes sideways, the teeth arrangement does not follow, making it unnatural. According to the authors, this mode is caused progressive growing.Since each resolution image is generated independently by the corresponding Generator, it seems that frequent feature appears.By the way, the analysis of the Perceptual Path Length(PPL), introduced by StyleGAN, has been found to be related to image quality. The authors of StyleGAN2 believe that this is because that it is important for the Generator to expand the latent space for creating high-quality images in order to generate high-quality images and the latent variables for creating low-quality images are forced to be moved into the space rapidly changing.StyleGAN2 [4] improves the quality of image generation using the characteristics of PPL and improving the droplet noise and non-following modes. Now let’s see how StyleGAN2 solves them and improves the quality of images.The summary of StyleGAN2 and key insights are as follows.StyleGAN improvements. normalizing the weight of CNN instead of AdaIN to removes droplets, improves unnatural modes by removing Progressive Growing, and improves image quality by providing continuity in latent space. FID etc. greatly improved compared to StyleGAN.In this part, we will look at what they are actually doing with StyleGAN2.AdaIN normalizes using the statistics of the data actually entered, which leads to droplet modes. The authors prevent droplets by normalizing the convolution weights using estimated statistics rather than actual data statistics. The figure below is a schematic diagram.We start by simplifying the Style block (the gray area in Figure b).AdaIN can be broken down into two steps when described in detail. The first is to normalize the content information with its own statistics, and the second is to linear transition of the normalized content information using style information.When the AdaIN part of StyleGAN (a) is expanded according to it, it becomes as shown in (b). The operation inside AdaIN is following: normalization of content → linear transformation by style vector. but note that in Style block, the order is as following: linear transformation by style vector → (Convolution →) → normalization of content.Next, they consider that the operation using the mean value is unnecessary, so they only need to divide the normalization operation by the standard deviation, and also to perform the linear transformation of the style only by multiplication of the coefficients. And since the noise insertion part does not need to be in the style block in particular, put it out of the style block. (c )We have simplified the operation inside the Style block. Here, let’s consider that the first linear transformation by the Style vector is performed by the processing inside the convolution. In the Style block, the coefficient y_s, which is a linear transformation of the style vector W, is used. The operation of processing the content image multiplied by s with the convolution weight w_ijk is equivalent to convolving the content image with the product of the weight w_ijk and s. So this operation can be rewritten as follows: (Operation of Mod in (d) above)Next, consider performing the normalization operation (here only dividing by the standard deviation) in the convolution internal processing.Here, assuming that the input follows a standard normal distribution, the standard deviation of the output.What we want to do is multiply the output by the inverse of the standard deviation.The operation of multiplying output of convolution with the weight w_ijk by the inverse of the standard deviation after is equivalent to convolving with weight w_ijk multiplied by the reciprocal of the standard deviation. Therefore, this normalization operation is performed as follows. (Demod in d above)With this, the sequentical operation in the style block, linear transformation by style → convolution → output normalization, was able to be expressed by one convolution process. The normalization part is a process of normalization with assumption that the output is a normal distribution. In other words, the normalization using the actual distribution,causing droplets, is not performed. Droplets do not come out when using this.The original StyleGAN Generator has a simple configuration. Without Progressive Growing, such simple generators has difficulty to generating high-resolution images a. But by increasing the expressive power of Generator and Discriminator, It seems possible to generate high resolution images without Progressive Growing.Progressive growing is a method that gradually adds a generator and discriminator for high resolution, and is one of the methods frequently used in high resolution image generation. However, since each Generator is independent, it tends to generate frequent features, which results in teeth not following the movement of the face.Therefore, the authors propose a high-resolution image generation method that does not use Progressive Growing, that is, a method that does not gradually add a Generator and Discriminator to increase the image resolution. The networks, similar to MSG-GAN, shown in the figure below are the candidates.To choose the best networks , they did experiments with all combinations of (b) -type Generator / Discriminator and (c )-type Generator / Discriminator, and to adopt the best result. The following table shows the experimental results.From these results, it can be seen that using (b) -type Generator significantly improves Perceptual Path Length, and using (c)-type Discriminator improves FID. So the authors have adopted these networks.In Progressive Growing, the networks first focus to generate low-resolution images and then gradually generate high-resolution images. But is this new network doing that kind of learning? The experiment below confirms this.Since the (b) -type Generator sums the generated images of each resolution, the contribution of each resolutions to the final generated image can be calculated . The vertical axis indicates the contribution of each resolutions and the horizontal axis indicates the learning progress. In above figure (a), where the new network was adopted, the contribution on the high-resolution side gradually increased with the elapse of the training progress, and when the network size was increased ( figure b), the contribution of high-resolution sides increased further at the end of learning.The images generated using this new mechanism is as follows. The eyes and teeth become unnatural in StyleGAN, while the new mechanism shows that the eyes and teeth move naturally with the change of the face direction.Now that it has been found that there is likely a correlation between Perceptual Path Length(PPL), which indicates the perceptual smoothness of the latent space, and image quality, we incorporate it into the model as a regularization term. The formula is as follows. a is a constant and y is a random image generated from a normal distribution.This regularization force the generator minimize changes with perturbation of latent variables as much as possible. The learning is performed while dynamically changing the constant a by the moving average of the first term, so that the optimal value is set during learning.First, let’s look at the change in score due to the introduction of each method in two data sets.I have omitted the explanation for C, but it seems that updating the regularization term does not need to be performed very often, which is called “Lazy” regularization. Looking at the results, even for Lazy, the results are not bad.F is model increased the size of the network in addition to all the methods described so far. The learning time became faster by incorporating Lazy regularization and AdaIN inside the convolution (37 → 61 images / sec), and they could scale up the network further thanks to that (31 images / sec) .Next, Let’s look at the distribution of PPL. The smaller PPL indicate better quality of the generated image. So the histogram below shows that the styleGAN2’s images quality are improved.Next, let’s look at the generated image. Very nice high quality generated image.Finally, it is the result of analyzing by the method of projection from the image into the latent space. The histogram shows the LPIPS distance distribution, which is the distance between the original image and the image that was generated once again through the Generator after projecting it into the latent space.They show the results of the real image, StylegGAN and StyleGAN2, and visualized. ( real images are also artificially dropped into the latent space.)You can see that StyleGAN2 projects better on latent space than StyleGAN and real images. This is probably due to the smoothing of the latent space with the regularization term for PPL. The figure below shows original images and reconstruction images that has undergone the process, the original image → projection to the latent space → Generator. In StyleGAN, the reconstruction images have some features different from the original image in some places, but you can see that it is reconstructed quite accurately in StyleGAN2.In this post, I have explained the basics of GAN, StyleGAN, and StyleGAN2. StyleGAN2 improves image quality by improving normalization and adding constraints to smooth latent space. However, even with 8 GPUs (V100), it costs 9 days for FFHQ dataset and 13 days for LSUN CAR. BigGANs and others have proven the effectiveness of larger model. But I am personally happy if a high-performance small GAN that can be handled by personal PCs comes out.www.getrevue.cotwitter.com",22/12/2019,0,14,3,"(685, 353)",36,3,0.0,3,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,no emotion
150,Neural Language Models,Towards Data Science,Arun Jagota,394.0,36.0,8313,"In NLP, a language model is a probability distribution over sequences on an alphabet of tokens. A central problem in language modeling is to learn a language model from examples, such as a model of English sentences from a training set of sentences.Language models have many uses. Such as…In [1], we discussed statistical language models, a standard paradigm in the 1990s and earlier. In the past few decades, a new paradigm has emerged: neural language models. This is the subject of this post.Why neural language models over statistical ones? The short answer: they yield state-of-the-art accuracy with minimal human engineering. Why? The short answer is: due to a combination of two breakthroughs in recent decades — deep neural networks, and recurrent ones. (Exponential increases in computing power and availability of training data also helped tremendously.) Distributed representations, i.e. word embeddings, also played a role.In this post, we will cover recurrent neural networks (including deep ones) in the context of language modeling.We start with the master equations. These are the most important take-away equations in this post. Beyond defining the language modeling problem precisely, they also serve to frame various approaches to its solution.Let P(w1, w2, …, wn) denote the probability of a sequence (w1, w2, …, wn) of tokens. The aim of learning is to learn such probabilities from a training set of sequences.Consider the related problem of predicting the next token of a sequence. We model this as P(wn | w1, w2, …, wn-1). Note that if we can accurately predict the probabilities P(wn | w1, w2, …, wn-1), we can chain them together to getP(w1, w2, …, wn) = P(w1)*P(w2 | w1)*P(w3 | w1, w2)* … *P(wn | w1, w2, …, wn-1)Most learning approaches target language modeling as the next token prediction problem. This is a problem of supervised learning. The input is (w1, w2, …, wn-1). The output we seek is a probability distribution over the various values that wn could take.Note that the output is a fixed-length probability vector. (The dimensionality could be very high, though. Hundreds of thousands to millions for a language model on English sentences.)The input is not a fixed-length vector. Its length, n, varies. In many use cases, n can often be large, in the 100s or even 1000s. This is the primary factor that makes this problem difficult.Statistical approaches to estimating P(wn | w1, w2, …, wn-1) when n is large must make some assumptions. Without them the problem is intractable. Consider n = 101 and say the alphabet just has two symbols: 0 and 1. The minimal training set size to reliably estimate P(wn | w1, w2, …, wn-1) is of the order 2¹⁰⁰. This is because the input can be any one of those choices, and we want to be able to predict the next token reasonably in each case.The most widely-made assumption in statistical approaches is Markovian. Specifically, P(wn | w1, w2, …, wn-1) is approximated by P(wn | wn-k, …, wn-1). This assumes that the next token probabilities depend only on the last k tokens. Here k is fixed, often to a small number (2 or 3). See [1] for more on this topic.This vastly simplifies the problem. At the cost of not being able to model long-range influences. Here is an example, from [2], of a long-range influence.“Jane walked into the room. John walked in too. It was late in the day, and everyone was walking home after a long day at work. Jane said hi to ___”The answer is John. This requires remembering the one occurrence of John about 25 words back.Another nice example is from [4].I grew up in France … I speak fluent French.Predicting French involves remembering France which may have occurred many sentences back.Neural language models address this “long-range influence” problem. Additionally, they also address the short-range influence problem better, specifically with automatically learnable feature vectors. (Statistical approaches can also use features; however, these are human-engineered.)Early Neural Language ModelOne of the early neural language models, as described in [3], is feedforward in nature. Whereas the more recent approaches are now all recurrent, we start with this model because it has some insightful characteristics that will set the stage for the ones that follow.As with many statistical language models, this model is kth-order Markovian. That is, for n greater than k, it assumes that P(wn | w1, w2, …, wn-1) equals P(wn | wn-k, …, wn-1).It uses distributed representations for input words. Distributed representations, more recently also called word embeddings, go back to the 1980s. A distributed representation of a word maps it onto a d-dimensional numeric space. Distributed representations are learned by algorithms in ways that encourage semantically-related words to be near each other in this space.The intuition for using learned distributed representations is the hope that they pick up useful features of words involving semantic relateness. For example, if one sees Paris and France in the same text, this might help the model to make sensible inferences involving the context French, such as in our language example: I speak fluent ___.This model then concatenates the distributed representations of wn-k, …, wn-1. This forms the input vector, of dimensionality k*d. Since we want the output to be a probability distribution over the entire lexicon of words, this network uses N output neurons, where N is the size of the lexicon.A hidden layer sits between the input and output layers. As in any other multilayer neural network, the hidden layer maps the input to a vector of features. (The neurons are sigmoid-like, specifically tanh.) This mapping is done via learnable weights from each input neuron to each hidden neuron.The mapping from the hidden layer to the output layer uses the softmax function. This is because we want the output to be a probability distribution over the output neurons. The softmax function is parametrized by weights, one per (hidden neuron, output neuron) pair.The parameters of this network are learned in the usual way, i.e. via backpropagation using stochastic gradient descent. The distributed representations of the various words can also be learned during this process. Alternatively, such distributed representations can be pre-learned in some other way (e.g. word2vec) and just used here.It is worth noting that when the size of the lexicon N is huge (say in the millions) the computations involving the output layer become the bottlenecks. This is because the predicted output on a particular input needs to be a probability distribution over the entire lexicon.A more serious limitation is that this network, just like typical statistical models, cannot model influences among words that are more than k units apart. In our John-Jane example, we would need k to be at least 25 for us to have any hope of making the right inference.Statistical models that explicitly compute P(wn | wn-k, …, wn-1) require memory that grows exponentially with k. This neural network model scales better. It uses k*d*h + h*N parameters, where h is the number of hidden neurons. So the number of parameters increases only linearly with k.Recurrent Neural NetworksRecurrent neural networks are appealing for language modeling because they are not constrained by k. That is, in principle, they can learn arbitrarily long-range influences. That said, it does require some special engineering for them to do so.We’ll start by describing a basic RNN. We will see that it has difficulty learning long-range influences. We will then discuss modifications to this RNN to alleviate this issue.Basic RNNAs a black box, i.e. from the input-output interface, an RNN looks like a feedforward neural network at first glance. It inputs a vector x and outputs a vector y.There is a twist though. The RNN expects to receive, as input, a sequence of vectors x(1), …, x(T), one vector at a time, and outputs a sequence of vectors y(1), …, y(T).The output y(t) at time t is influenced not only by x(t), rather by the entire sequence x(1), …, x(t) of inputs received till then. This is what makes the RNN powerful. And the associated learning problem challenging. The RNN must carry state. Somehow it must summarize the history x(1), …, x(t-1) into a state, which we will call h(t-1), from which it can predict y(t) well. When the input x(t) is presented, it first updates its state to h(t). In effect, x(t) becomes part of the history of the inputs it has seen. It then produces the output vector y(t) from h(t).We can write this formally asIn summary, the RNN may be viewed as having two parts:Part 1 is reminiscent of finite-state automata in computer science. Part 1+Part 2 is a finite-state transducer in computer science terms.The state in an FSA may be viewed as a categorical variable. By contrast, the state in an RNN is a multi-dimensional vector. The latter facilitates compact distributed representations of states that can generalize well, a major advantage of modern neural representations.The icing on the cake (to put it mildly) is that both the state-change and the state-to-output behaviors of the RNN can be learned automatically from a training set of (input sequence, output sequence) pairs.The State-change and Output FunctionsFollowing the usual neural network ‘design pattern’ inspired in part by biological neural networks and approved by statistics, we’ll useHere S1 and S2 are often sigmoid-shaped functions, typically the tanh or the logistic function.f(h,x) computes the next state as a linear combination of the current state and the next input pushed through the squashing function S1. g(h) computes the output as a linear transformation of the new state pushed through the squashing function S2.RNNs in Language ModelingA fundamental use case of RNNs is in language modeling. Recall that in language modeling, our primary interest is in estimating P(wn | w1, w2, …, wn-1).A natural way to model this in an RNN is to model wn-1 as the input vector x(n-1), the history w1, w2, …, wn-2 as the state vector h(n-1) and the output y(n-1) as a probability vector over the lexicon of words. For these purposes, it is natural to use the softmax function as the output function S2. Rather than a squashing function. S2(u) exponentiates each component of u and then normalizes the resulting vector to form a probability distribution. That is, it divides each exponentiated value by the sum of the exponentiated values.Structurally, this is analogous to the feedforward neural language model described earlier. The output dimensionality is N, the size of the lexicon. The input dimensionality is based on how words are represented. If words are represented locally, i.e. as a one-hot encoding, the input dimensionality is N. If words are represented in a distributed fashion, the input dimensionality is the dimensionality of this distributed representation. The state vector’s dimensionality is analogous to a hidden layer’s dimensionality, and controls the capacity of the corresponding feature space.A sequence of words w1, w2, …, wn-1, wn may be turned into a training instance for this RNN-based language model. The sequence of inputs is x(1), …, x(n-1), the sequence of vectors of the first n-1 words. The target output y(n-1) associated with this sequence is derived from the word wn. It’s the probability vector with all its mass concentrated on the dimension that corresponds to this word.Let’s pause to reflect on this. The output dimensionality is always N, as we are trying to learn a language model. This does have learning consequences as N is often very large. The input dimensionality may be distributed and compact or local and typically huge, i.e. N. There are trade-offs. A local representation imparts the model with a high capacity and is capable of making fine-grained distinctions. Each word has its own vector of weights to each hidden neuron. Models using distributed representations of words in the input layer are exponentially-more compact, may learn much faster, and potentially generalize better. They can leverage distributed representations that have already been learned by effective algorithms such as word2vec and enhancements, possibly from a huge corpus of documents.LearningThe learnable parameters are the matrices of weights Whh, Wxh, and Why inside the state transition and the output functions. Whh controls the influence of the current state vector on the next state vector, Wxh the influence of the next input vector on the next state vector, and Why the influence of the next state vector on the next output vector.To understand how learning happens, it helps to unfold the recurrent neural network in time, as depicted below.Now consider the input x(t) at time t along with its target output y(t). Keeping in mind that the mapping we seek to learn is really to output close to y(t) on the sequence of inputs x(1), x(2), …, x(t) we can depict the situation as a multilayer feedforward neural network unfolded in time as depicted below.So we can use backpropagation to learn the weights inside the functions f and g. With a twist. All the f’s in the picture above must have the same weights. They are all the same f!Okay, we can make this happen by summing up the changes to each weight in f across all the layers.One way to think of this is as follows. Let’s imagine we have two versions of the network: the constrained version (whose weights are shared as explained in the previous paragraph) and the unconstrained version which is a multi-layer neural network with no weight sharing, i.e., each instance of f has its own weights. Before the learning on the pair (x(t), y(t)), we’ll initialize each weight in the unconstrained version to the corresponding weight from the constrained version. We then run one iteration of online backpropagation (aka stochastic gradient descent) to update the weights on the unconstrained version. From these, we derive the new weights on the constrained version by taking, for each constrained weight, the average of the unconstrained weights that map to it.Well, all this sounds great in principle. However, note that the unfolded network can be very deep! So if there is a long-range influence, meaning that x(t-k) strongly influences y(t) for a large k (say k = 100), the back-propagated error signal may get very weak. This is because every instance in which the error is back-propagated through f attenuates the error since f is a squashing function.Let’s expand on the last sentence in the previous paragraph, in the setting below.To simplify the explanation, we use a feedforward neural network with two hidden layers. Plus the input and the output layer. All layers have a single neuron each. All layers except the input one have a sigmoidal transfer function. hi denotes the output of the neuron at layer i, and ni denotes the input to this neuron. So we have hi = s(ni) where s denotes the sigmoid function. We also have ni = w(i-1)*h(i-1).Note that to simplify the notation h0 denotes the input and h3 the output.In this illustration, we will use the loss function L = ½ (y-h3)². The backpropagation algorithm computes the gradient of L with respect to each weight wi (applying the chain rule when needed) to decide on the change to make to the weight. More precisely,Delta wi = -n*dL/dwiHere n is the learning rate. The ‘-’ is there because we are doing gradient descent (since we want to minimize the error).So now it comes down to computing dL/dwi for the various weights wi.Let’s start with w2.By the chain rule,dL/dw2 = (dL/dh3)*(dh3/dn3)*(dn3/dw2) = 2(y-h3)*s(n3)*(1-s(n3))*h2Admittedly we are overloading notation a bit. In the above, h2, h3 and n3 denote both the variables (neurons) and their particular values on the input x = h0. We are also using the fact that the derivative of the sigmoid function ds/dn equals s(n)*(1-s(n)). Also that, for i > 0, dni/dw(i-1) equals h(i-1).Similarly, with a slightly deeper application of the chain rule, we getdL/dw1 = (dL/dh3)*(dh3/dn3)*(dn3/dh2)*(dh2/dn2)*(dn2/dw1) =2(y-h3)*s(n3)*(1-s(n3))*w2*s(n2)*(1-s(n2)*h1Now imagine adding more hidden layers to our network (keeping the topology and the transfer functions the same) and computing dL/dw0.dL/dw0 = 2(y-h(K))*s(nK)*(1-s(nK))*w(K-1)*s(n(K-1))*(1-s(n(K-1))*…*w1*s(n1)*(1-s(n1))*h0Consider any one term s(ni)*(1-s(ni)). This product is always less than ¼. Combining all such pairs in dL/dw0 produces a multiplicative factor whose value is less than (¼)^K. In view of this, as K increases, dL/dw0 approaches 0 very very quickly. This is the vanishing gradient problem. This has a potentially disastrous consequence in our example. We may never learn w0.A Long-range ExampleNext, let’s see a realistic example with long-range influences. We will use it to illustrate that learning such influences is the issue with basic RNNs, not the ability to represent them. This same example will also help illustrate the mechanisms of a more advanced RNN that alleviate this issue.We’d like to process a long sequence of English words and output a 1 if the word terrorist appears at least once, and 0 if not. Were terrorist to occur only once and very early on in the sequence, this event needs to be remembered until the very end.Why not just output a 1 immediately after terrorist is detected, else keep going? That would be tweaking the mechanisms of a general solution to this particular problem. This tweak would not generalize to the broader class of problems exhibiting long-range influences, of which this is just one illustrative example.The XOR example was used in the feedforward neural networks setting in exactly the same way. Viewed as a problem to solve, XOR is trivial. Just input two binary numbers and output the binary number that is their exclusive OR. However, viewed as an example to illustrate issues with certain feedforward architectures and to test improvements, it has turned out to be quite useful. The XOR is linearly inseparable. This is why classifiers capable of only linear discrimination fail. Indeed in the 1980s, the XOR was often used as a benchmark to test and demonstrate the learning ability of backpropagation in the setting of multi-layer perceptrons — feedforward neural networks with a hidden layer and sigmoidal neurons.Okay, back to our example. Let’s refine the problem description for our purposes here. The input x(1), …, x(t) is a sequence of vectors where x(i) represents the ith word. The output y(t) is 1 if the word terrorist appears in this sequence and 0 if not. So the output is a binary sequence aligned with the input sequence and has a very simple structure: 0*1*.We will represent a word as a one-hot encoded vector. The vector’s dimensionality is the size of the lexicon. The bit corresponding to the word being represented is 1, the rest are 0.Now consider a basic RNN for this problem. Its input and output structure has already been specified. What about the state? Let’s set the state vector’s dimensionality to be the same as that of the input vector. We’d like hi(t) to be close to 1 if the word indexed by i was seen at least once among x(1), …, x(t), and close to 0 if not.This is certainly feasible representationally, with the choicehi(t) = sigmoid(a*hi(t-1) + b*xi(t) + c)It is easy to choose a, b, and c, for example, a = b = 1 and c slightly negative, so that hi(t) is essentially an OR of hi(t-1) and xi(t). This is what we want.Once we have processed the entire sequence, we can simply set y(t) to hi(t), where i indexes the particular word we care about (in our case, terrorist). We can write this in the form y = g(W*h(t)) where g is a sigmoid. W is the vector whose ith position is sufficiently positive (e.g. 1) and the rest are 0.Now let’s look at this situation from the perspective of learning. Imagine that we have a training set of (document, label) pairs where label is 1 if a certain word appears somewhere in the document and 0 if not. We can assume that the labeling is consistent, i.e. this “certain word” is always the same. However, we are not told what that word is.So there are two issues here: (i) the learner must deduce which of the potentially millions of words best discriminates the two classes and (ii) learn only sequentially as described earlier in the basic RNN section.Focusing on (ii), backpropagation-through-time may need to propagate error gradients far back in time in many cases. Such gradients can vanish as discussed in an earlier section. The consequence of this is that if in most positive instances, the last occurrence of the “certain word” (in our example, terrorist) appears well before the last word in the document, the learner will not be able to establish the connection between this occurrence and the output is 1. If our positive instances are at least somewhat long documents, in most of them, the last occurrence of the word terrorist will indeed be well before the document’s end.Before closing this section, we’d like to add that now that we have opened the door to the labeling driving the learning, this problem becomes broader in its import. We can pose any binary classification problem on text this way, so long as we have a rich enough data set. It’s up to the recurrent neural network solution to figure out which words and which sequential structure helps to solve the problem.That said if one’s aim is to solve such a problem and if there isn’t a compelling case to be made for sequential learning, one should try non-recurrent binary classifier algorithms first. Such as feedforward neural networks, random forests, naive Bayes classifiers, …Gated Recurrent Unit (GRU)We now turn our attention to more advanced recurrent neural networks, capable of capturing long-range influences. Two such networks in wide use are the LSTM and the GRU.We will describe the GRU as its mechanisms are simpler while remaining effective for capturing long-range influences.At a high level, the GRU works in the same fashion as a basic RNN. The input is a sequence of vectors presented one by one. The GRU maintains state, capturing some key aspect of what it has seen so far. This state combined with the next input determines the next state.The advancement comes in the state transition function.We’ll use the example we just introduced to build up the description of the GRU.Consider the state update function of the basic RNN for this exampleh(t) = sigmoid(A*h(t-1) + B*x(t) + C)The GRU will build up h(t) from h(t-1) and x(t) in a different way. It introduces the notion of a new memory hnew(t) derived from h(t-1) and x(t). The process of deriving this new memory from h(t-1) and x(t) has a mechanism that first runs each component of h(t-1) through a filtering gate. Think of this as letting the network selectively pay differing attention to different components of h(t-1) for this purpose.This filter, let’s call it si(t), takes on a value between 0 and 1. Filtering involves multiplying hi(t-1) by si(t). In vector notation we write this as s(t) o h(t-1).The filtered version of h(t-1) is then linearly combined with x(t) and passed through a component-wise tanh. The full equation ishnew(t) = tanh(A*x(t) + B*(s(t) o h(t-1)) + C)The new memory is then combined with the old memory to form the updated memory, in a manner that will be described soon. By contrast, in the basic RNN, the updated memory is formed only from the old memory.What would be a good hnew(t) in our example? Let’s capture the desired behavior in pseudocode.hnew,i(t) is close to 0 for any word i other than terrorist. hnew,terrorist(t) is close to 1 if the new input x(t) is terrorist or if terrorist has been seen previously, in which case hi(t-1) would be close to 1.Can we choose the learning parameters to emulate this desired behavior?First, we will set C to the vector of 0s.The THEN clause of the first IF can be executed via an appropriate choice of matrix A. Specifically, set Aii equal to 1 and all the rest of A’s elements to 0.The THEN clause of the second IF can be executed by choosing si(t) to be near 1 for i indexing terrorist and near 0 for the rest. Choose B as a matrix of positive values, e.g. all 1s.Under these conditions on A, B, and s(t), hnew,i(t) will be set near 0 when neither of the first two THEN clauses triggers.At this stage the reader might wonder, what have we really gained? If the last occurrence of the word terrorist appeared way before the current t, will not computing hnew,i(t) from the long chain of triggerings of the second THEN clause — the one whose antecedent is “i indexes terrorist and hi(t-1) is near 1” — have the same diluting effect.The reader is right to wonder this way. We have not yet completed the GRU’s description. Once we have all the mechanisms in place we will get a better sense of how they all come together to alleviate the vanishing gradient problem.The GRU uses a second gate, called the update gate, to control the relative contributions of the old and the new memories towards the updated memory. By contrast, the basic RNN derives the updated memory just from the old memory.Before diving into the update gate, let’s complete the description of the filtering gate.Filtering Gate EquationThis iss(t) = sigmoid(D*x(t) + E*h(t-1) + F)Here D, E, and F are the matrices and vectors of the learnable parameters that control the behavior of the gate.In our terrorist example, the behavior we sought from s(t) was to simply project out the value of hi(t-1), where i indexes the word terrorist. How well we can learn this behavior automatically from a training set is a whole different story.Example to illustrate the update gate mechanismTo build intuition about the update gate and the role it plays, a different example will help.We’d like to do anomaly detection in a time series using an RNN. We want the anomaly detector to not only detect anomalies but also adapt to new normals. Let’s illustrate this with an example.We process this time series from left to right. The value 23 is way outside the range of the values we have seen to this point, so we deem it anomalous. As we should. As we continue to process values to the right of this anomaly, we see that a new normal is developing. (Specifically, a level shift.) At some point soon the anomaly detector should adapt to this, and start treating it as the ‘normal’. For example, stop deeming values between 20 and 25 as anomalies.We’d also like to be able to control ‘at some point soon’ more finely by providing a labeled data set. By this, we mean that a time series comes labeled with which points are anomalous and which not. The RNN should be able to deduce, for any particular time series, how aggressively or conservatively we want to transition to the ‘new normal’. This would be the time-series specific behavior inferred from the historical labeling.One way to approach this is to train the RNN to forecast the next value in the time series. If the actual next value differs a lot from the forecasted next value then deem that value as being anomalous.RNNs are in principle suited to one-step forecasting because this problem is similar to learning a language model. In both cases, we want to predict the next input from the historical inputs.The model that the RNN learns during its incremental learning on the target of forecasting the next value may be interpreted as the current model of ‘normal values’.Now suppose that this RNN is a basic RNN. What happens when it encounters the first occurrence of 23. We hope it flags it as anomalous. What happens next? It incrementally trains on 23, as it would on any new value. The effect of this is that, as we see more values at this level, the model will gradually adjust to this new level. That is, it is building a ‘new normal’. So far so good. It is going in the direction we want.If this 23 was an outlier, i.e. a spike, not a level shift, the RNN would soon adjust back to the old normal. So all seems good. Let’s look at this more closely though. As the basic RNN is incapable of remembering the old normal (the one based on the values in the time series to the left of the first occurrence of 23) it must unlearn its accommodation of 23 towards an evolving new normal. Which in hindsight it can see was an anomaly.The GRU addresses this situation better. Just like the basic RNN, the GRU learns to adapt its ‘normal’ by forecasting the next value and adjusting its model based on the forecast error. Unlike the basic RNN, the GRU works with two models: one a slower evolving model, captured in h(t), the second a faster-evolving model, captured in hnew(t). In effect, the GRU can remember both the ‘old normal’ and a tentative ‘new normal’ at the same time. Its composite model is a combination of these two. This gives it the flexibility that should the ‘new normal’ later turn out to be a ‘false alarm’, i.e. an anomaly, it can just use the ‘old normal’. By contrast, the basic RNN would have to unlearn it. It also lets the GRU adapt quicker to the new normal in case it is not a ‘false alarm’.This behavior is summarized in the pseudocode below.Update Gate Usage EquationLet’s call this gate u. ui(t) is between 0 and 1. The equation for the updated memory using this gate ishi(t) = (1-ui(t))*hi(t-1) + ui(t)*hnew,i(t)which, in vector notation, ish(t) = (1-u(t)) o h(t-1) + u(t) o hnew(t)Anomaly Detection Example RefinedNext, let’s refine the discussion on our anomaly detection example. We’ll use one-dimensional input and output vectors since we are working with a time series. For the state vector, we might consider a higher dimensionality. Whatever we feel is appropriate to capture the ‘normal’ characteristics of the recent values of the time series. For instance, if we wish to capture not only (a smoothed version) of the most recent value of the time series but also its trend, we might choose two dimensions.By choosing the dimensions of h and hnew with care, we can detect different types of anomalies. To detect point anomalies, two-dimensional state vectors may suffice. One dimension models the central tendency, e.g. mean or median, the second the dispersion, e.g. standard deviation or a more robust version. To, additionally, detect trend changes, we may want additional dimensions, ones that model characteristics of trends.Like h(t), hnew(t) is also a model of recent values. That said, as hinted by the name, hnew(t) is a model of even more recent values than h(t). For example, h(t) might be a slow-moving average and hnew(t) a fast-moving average.Now suppose the most recent values in the time series are anomalous. Let’s say we know this fact. That is, the anomalies are labeled. We can capture this in the value of the update gate u(t) where t is one of these anomalous time points. This gives us control over whether we want the most recent values (which we know are anomalous) to update our ‘normalcy’ model or not.For instance, we might not want to alter our ‘normalcy’ model if we are in the midst of an anomaly. We can make this happen by setting the components of u(t) close to 0. If we are in a ‘normal’ region we want our normalcy model to adapt quicker to the data.Update Gate EquationWe have seen how the update gate’s value is used in constructing the updated memory from the new memory and the old memory. But how is the update gate’s value itself calculated? This is done as below.u(t) = sigmoid(A*x(t) + B*h(t-1) + C)Here A, B, and C are the matrices and vectors of the learnable parameters.Putting All the Equations TogetherWe have covered all the mechanisms inside the GRU. Let’s put all the equations together and review the end-to-end flow. At a single time t that is.Let’s now walk through the end-to-end flow at a single time t. We assume that the learning has already happened.The new input x(t) arrives at time t. The RNN’s current memory state is h(t-1). First, from these two we compute the value s(t) of the filtering gate. Now we have what we need to compute the value hnew(t) of the new memory. So we do that. Next, we compute the update gate’s value u(t) from x(t) and h(t-1). Note that the equations for computing s(t) and u(t) have the same form. The differing behaviors arise from the different learned parameters. Finally, we compute the updated memory h(t) from the old memory h(t-1) and the new memory hnew(t) using the update gate u(t) to combine the two.“Learning”The parameters to be learned are the matrices A, B, D, E, G, and H and the vectors C, F, and I.We have placed this section’s name in quotes intentionally. Rather than describe learning in the usual purely supervised way, we will just convey intuition on how the various mechanisms in the GRU might be learned from some combination of human-driven biasing and self-supervised or supervised machine learning.We will illustrate these in the anomaly detection example. In this illustration, we will be taking some liberties with the GRU. That is, we will tweak its mechanisms as appropriate. Our aim is to provide insights into the mechanisms and variations by way of an example.Two for one: Anomaly detection also solves forecastingWe think it's helpful to add that buried within the anomaly detection approach we describe below are certain time series forecasting tasks. We want to call this out here because, in the process of describing how to use the (somewhat tweaked) GRU to solve time series anomaly detection problems, we will also be implicitly covering how to solve time series forecasting problems. So you get insights into solving both problems using RNNs.First, anomalies not labeledWe are given a time series. If the anomalies are not labeled, we can train a GRU on the target of forecasting the next value in the time series. This process forces the GRU to learn a model of (recent) ‘normalcy’, adapting it along the way as ‘new normals’ develop.h(t) should model the ‘old normal’ at time t. h could be multi-dimensional. For example, one component could describe the mean value in the normal region, a second the trend.hnew(t) is similar to h(t) except that it is more heavily influenced by the recent values. For example, h(t) might behave as a slower moving average, hnew(t) as a faster one. Possibly with trend components so that we can discern if hnew(t)’s trend differs from h(t)’s.How can we nudge the GRU into learning that hnew(t) should model more recent values than does h(t)? We could learn the parameters in hnew(t)’s equation by explicitly forcing hnew(t) to forecast a short-horizon forecasting target. Such as y(t) to equal x(t-1), i.e. a one-step forecast.How can we make h(t) model a slower-adapting variant of hnew(t), i.e. model normalcy at a coarser time scale than does hnew(t)? (Except when we want h(t) to chase hnew(t) such as when a ‘new normal’ is developing.)A sensible way to force h(t) to move slower is to have it forecast a longer-horizon target, i.e. set y(t) to equal x(t-k) for sufficiently large k. While this is an appealing idea, the needed architecture is two forecasters on the same input at two different horizons. This is not what the GRU is. The GRU must learn all its parameters from a single target, say y(t) equal to x(t-1). Plus this doesn’t accommodate the exception, i.e. sometimes we want h(t) to chase h(t-1). We’d need another mechanism for this.To see what we have to work with in the GRU, let’s write out the state update equation againh(t) = (1-u(t)) o h(t-1) + u(t) o hnew(t)Assuming h(t-1) is slow-moving, and hnew(t) is very different from h(t-1), we’d want h(t) to chase h(t-1). We can make this happen by setting the components of u(t) to close to 0. But how?First let’s write out the update gate equation againu(t) = sigmoid(A*x(t) + B*h(t-1) + C)What if we modified it so that hnew(t-1) also appears on the right-hand side?This would in principle allow us to learn the behaviorThis also potentially helps the model adapt quicker to a developing ‘new normal’. Let’s illustrate this with an example.Consider a level shift, such as the one at the bolded value below.hnew(5) is 4. h(4) is 1. So we’d want h(5) to stay close to 1. We interpret this as “we don’t know yet whether 4 is a transient anomaly or the being of a new normal, so let’s have our old normal adapt towards the new value but slowly”.As we continue moving to the right, h(t) will slowly start moving towards 4, i.e. towards hnew(t+1). This will in turn drive u(t) towards 1, which will further accelerate this movement, i.e. have a snowball effect.We can summarize this in the following behavior we expect on this level shift. At the level shift event, initially, the old model moves slowly as it doesn’t yet know it's a level shift. Once the evidence that it is indeed a level shift accumulates the model accelerates its move towards the new normal.On the other hand, if the shift was a transient anomaly (e.g. spike or dip), such as in the time series belowh would stay close to the old normal throughout.Feature EngineeringWhat if we don’t like the idea of modifying the update gate equation? We might be able to achieve a similar effect by feature engineering. E.g. by working off a transformed version of the time series such as x(t)-x(t-1). The intuition here is that this time series reveals where the original time series changes more explicitly. A possibly more robust variant of this would be to define our featurized time series as a suitable faster-moving average minus a suitable slower-moving average.We can even imagine a sliding Kernel that explicitly distinguishes between transient changes and more stable ones. This Kernel function would have a high value if it is centered on an outlier deemed a transient event and low if not.Armed with such a Kernel, should we use it to generate the features or the labels? The former would mean that we are replacing the input x(t) by a feature-vector f(t) in which one of the features is the value of this Kernel function centered on t. The latter would mean that we set y(t) based on the value of this Kernel function at time t.We won’t dive into this question — just surface a few thoughts. It is appealing to use the Kernel function to generate the labels because they would then explicitly encode whether an event is transient or more persistent, exactly the right teaching signal for our anomaly-detecting GRU. On the other hand, we have to consider the risk that this labeling is biased. Generally speaking, label bias is more concerning than feature bias. Label bias manifests itself as “you are teaching the model the wrong thing”.Anomalies are labeled: Human or computerNow let’s delve into learning the GRU’s parameters when the anomalies are labeled by humans, computers, or a combination. In this section, by anomalies, we mean transient ones.We can use the labels to set u(t).Notice that we have taken direct control of the update gate here.Let’s look into Direct GRU to move towards ‘most recent values’ more closely. If the new values in the time series continue the old normal, then the update will also continue the old normal. If the new values are creating a different new normal, as in our level shift example, this will have the effect of the GRU adapting quickly to it.Okay, so this takes care of when to update the ‘normal’ and when not to. But what about how to update the two models— recent and more-recent — themselves?For the how it is tempting to rearrange the various mechanisms of the GRU for our use case. Sure, we lose the ability to use the GRU as a black-box. On the other hand, we learn what we can do with other combinations of these mechanisms.We will continue to use h(t), hnew(t) and u(t). h(t) and hnew(t) will each learn to forecast x(t) albeit at different horizons. So each will be equipped with its own target y(t) = x(t+k1) for h(t) and ynew(t) = x(t+k2) for hnew(t).u(t) will be set from the anomaly labels in the manner described earlier.This model may be viewed as blending self-supervised forecasting with supervised anomaly detection.What about the filter gate?What about the mechanism s(t) in the GRU? Can it be profitably used here? First, let’s remind ourselves what it actually does. It projects out those components of h(t-1), in the context of x(t), that matter to the task at hand. Which in our case is to detect anomalies.Well, actually under our hood we have two instances of another task: forecasting. As we have already used the update gate in the anomaly detection task (to switch between forecasting models), let’s try to use it inside a forecasting task itself.Forecasting and Filter GateWe seek a good forecast of x(t) at a certain horizon. Such a model maintains a state that describes the current ‘normalcy’. This is represented in a possibly multi-dimensional vector. Such as a two-dimensional vector where one dimension captures a smoothed estimate of the most recent value; the second the local estimated trend at that value.Here is a physical ‘motion’ analogy. Say we’d like to predict the future location of a point moving in a certain Euclidean space. Statistical estimates of the current location and the current velocity are helpful in this regard.In this setting, we can speculate how the filtering gate might come into play. Depending on the time series some features of the ‘normalcy’ state may perform better than others on our forecasting task. For a short horizon such as one-step, and for a time series that doesn’t have a nice linear trend (although it may have seasonalities), an estimate of the most recent value may be the best forecaster. For a time series with a durable linear trend and for a longer horizon, the local trend dimension of the ‘normalcy’ state may be very predictive.The reasoning of the previous paragraph starts getting stronger as we add more dimensions to the ‘normalcy’ state model. Such as for various seasonalities, or lags.Using s(t) can in principle select out those components of the normalcy state that matter to the task at hand. This plays a role similar to that of feature selection or pruning in predictive models. Such pruning often helps. This is known both theoretically and empirically. It forces the algorithm to learn models with a bias towards simplicity. Simple models generally generalize better. Occam’s razor.But how do we learn s(t)?Okay, so we think using s(t) can help. Let’s dig a bit deeper to see how we might learn what we hope s(t) can learn.We will illustrate this in a basic RNN for time series forecasting.From the input x(t) we extract certain features such as its value, i.e. x(t) itself, and lags x(t)-x(t-k) for k=1, 2, …. (In order to compute the lags we need to remember previous values of the time series.) Let v(t) denote x(t)’s feature vector. The state vector h(t) mirrors this feature vector. For example, h1(t) may model a smoothed version of x(t), h2(t) a smoothed version of x(t)-x(t-1), and so on.The state vector models the characteristics of values of the time series that help forecast its future values. Such as an estimate of the most recent value, an estimate of the most recent trend, and possibly the various seasonalities.The RNN may now be expressed asWe can set y(t)=x(t+h) as the target. That is, we seek an h-step forecast of the time series x.f specifies how to derive the new state vector h(t) from the combination of the previous state vector h(t-1) and the feature vector v(t) of the new value x(t).g specifies how to derive the forecast from the current state vector h(t). For example, if h(t) just has two features: a smoothed estimate of the current value and a smoothed estimate of the trend at it, we might just multiply the second with a parameter learned from the target (this is especially useful to get the influence from different horizons, such as h being small versus large) and add the first one to it. In equation form, this would beThe second term on the right-hand side models a trend-based correction to the first term.Learning happens by propagating back the forecast error and adjusting the parameters implicit in g and f.So how should we inject s(t) into the mix? First, let’s drastically simplify. Let’s considerThis is not so naive for forecasting. In fact, this is how forecasters based on exponential smoothing work. Here a is the only learnable parameter in f.Note that this approach also matches the role we had for h(t), to serve as a smoothed version of v(t).Now we are ready to inject s(t).That is,This update rule is potentially richer as we now have component-level control over the state vector update. Effectively we have gained the ability to do soft feature selection during this process.GRU and The Vanishing Gradient ProblemWhat about the vanishing gradient problem? Is the GRU more robust against it? We won’t answer this question definitively; rather we will give some tantalizing hints on what might be at play.Let’s first write out the state update equations of the GRU vs the basic RNN.What jumps out? The sigmoid in the second equation. From our earlier discussion on the vanishing gradient problem, it seems that sigmoid-like transfer functions are implicated. Squashing functions have a near-zero slope in their tails. This can aid in the error gradients vanishing rather quickly, as backpropagation through time back-propagates the error seen at the output multiple time segments in the reverse direction through the chain of these sigmoids.By contrast, the GRU’s state update equation does not use a squashing nonlinearity.Stacked RNNsNow that we understand the GRU and the basic RNN more deeply, let’s see a significant generalization. ByConsider the state update function of an RNNh(t) = f(h(t-1),x(t))In the ones we have seen so far (the basic RNN and the GRU),f(h,x) = S1(Whh*h + Wxh*x)We may read this as “h(t) is obtained from h(t-1) plus x(t) in a feedforward architecture with zero hidden layers”.A stacked RNN simply adds one or more hidden layers to this computation. That is, the state update function becomes ‘deep’. Not surprisingly, this makes the RNN architecture richer, often performing better on certain tasks.A stacked RNN unfolded in time looks like thisNow, in addition to being deep in time, we are also deep in the state update function. We now have L functions f1, …, fL with their own learnable parameters. We can write this asf(h(l),h(l-1),l) = S1(Whh(l)*h(l) + Wxh(l)*h(l-1)), l = 1 to Lwhere h(0) denotes x.The state representation at time t itself is richer. h(t) = (h(0, t), h(1, t),…, h(L, t)). This may be viewed as a hierarchical representation of state in order of increasing coarseness. As in deep forward neural networks.InferenceNext, we talk inference. That is, how we might use a trained RNN to answer various questions.We can run a sequence of words w1, w2, …, wn-1 through a trained RNN and produce a probability distribution over the likely next word. Typically, we want more than this. We want the most likely extension of a certain length k of the given sequence. That is, we wantwn, wn+1, …, wn+k-1` = argmax wn, wn+1, …, wn+k-1` P(wn, wn+1, …, wn+k-1 | w1, w2, …, wn-1)This inference problem is called the decoding problem. When k is 1, the solution is just the word with the highest probability in the RNN’s output vector after the RNN has processed the words w1, w2, …, wn-1 sequentially as input. For the general case of k, the decoding problem is intractable. That is, we cannot hope to find the most likely extension efficiently. So we resort to heuristic methods.Greedy decodingOne simple approach to finding a good extension of length k is to start by finding the highest-probability word in the output vector, inputting this word to the RNN (so that it outputs the probability distribution over the next word), finding the highest-probability word again, and repeating the process. This approach is called greedy because it makes the best local decision in every iteration.This approach can work well when, in every greedy step, adding the highest-probability word in the output vector moves us towards a globally optimal solution. As in the example below.Example: Consider a language model trained on national, state, and county park names. Consider x(1) = yellowstone. The greedy extension will be x^(2) = national.Not surprisingly, such a happy circumstance is not always the case. As in the example below.Example: Consider a language model trained from a multiset of organization names. Examples of organization names are Bank of America, Google, National Institutes of Health, … The term multiset means that organization names may be repeated in this list. Now consider the problem of finding the most popular organization name in this list. The first step of the greedy method would pick bank or the or whichever word occurs most frequently in this list. The most popular organization name (say Google) may not start with this first word.Beam searchAs mentioned earlier, optimal decoding is in general intractable (NP-hard). There are many heuristic approaches that try to do better than greedy decoding, at varying costs of running time and memory requirements. Beam search has turned out to be an especially attractive one in sequence-to-sequence labeling use cases (especially language translation). So that is what we will describe here.Beam search keeps the k best prefix sequences. In every iteration it considers all possible 1-step extensions of these k prefixes, computes their probabilities, and forms a new list of the k best prefix sequences thus far. It keeps going until the goal is reached. In our scenario this would be when the extended sequence lengths are T. It then pulls out the highest-scoring sequence in its list of k candidates as the final answer.References",08/05/2021,16,403,691,"(642, 368)",5,3,0.0,7,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,surprise/amazement
151,"WX+b vs XW+b, why different formulas for deep neural networks in theory and implementation?",,Vivek Yadav,,2.0,334,"The problem:In most neural networks textbooks, neural networks combine features using, y = WX+B or y=W’X+B, however, in tensorflow and theano implementations, neural networks are implemented as y = XW+B. I spent a lot of time investigating the reason for this discrepancy, and came up with this. TL;DR:I think its an implementation issue, computing derivatives for y=XW+B is easier than y=WX+BTheoretically, W’X+B (or WX+B) is how neural network math is presented in books etc, which is equivalent to XW+B (taking transpose). However, while computing derivatives, the two formulations are not equivalent. Note WX+B is a vector, and we are taking its derivative with respect to a matrix (W). The derivative of a vector with respect to matrix gives a tensor. This tensor is computed by taking each element in y, and taking its derivative with each element in W, and storing that information in i X j X k element of a 3-D tensor, where i is the index of the element in y, and j and k are indices of elements in W. So this tensor results in computing additional matrices that need to be computed and stored. Eq 74 in matrix cookbook, http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf .Using XW+B, derivative of the function with respect to each element in W is much easier. Its simply X with some elements zero. No need to compute transposes of data and store tensor derivatives, and when ever you want you can compute the derivative by taking a matrix of size weights and putting each element in it equal to the input X with some elements zero.I think this is the main reason to use XW+B vs WX+B. Below is derivation of derivative for y = XW and WX (dropped bias). I think XW implementation is better suited for numpy and python packages, hence the use of XW instead of WX. Please feel free to add more. If you agree or disagree or have any other opinion, please feel free to add more information.For y = XWFor y = WX",08/11/2016,0,0,0,"(700, 396)",2,0,0.0,1,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,surprise/amazement
152,Simple and Multiple Linear Regression in Python,Towards Data Science,Adi Bronshtein,3200.0,11.0,2072,"Quick introduction to linear regression in PythonHi everyone! After briefly introducing the “Pandas” library as well as the NumPy library, I wanted to provide a quick introduction to building models in Python, and what better place to start than one of the very basic models, linear regression? This will be the first post about machine learning and I plan to write about more complex models in the future. Stay tuned! But for right now, let’s focus on linear regression.In this blog post, I want to focus on the concept of linear regression and mainly on the implementation of it in Python. Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too:As you can see, a linear relationship can be positive (independent variable goes up, dependent variable goes up) or negative (independent variable goes up, dependent variable goes down). Like I said, I will focus on the implementation of regression models in Python, so I don’t want to delve too much into the math under the regression hood, but I will write a little bit about it. If you’d like a blog post about that, please don’t hesitate to write me in the responses!A relationship between variables Y and X is represented by this equation:In this equation, Y is the dependent variable — or the variable we are trying to predict or estimate; X is the independent variable — the variable we are using to make predictions; m is the slope of the regression line — it represent the effect X has on Y. In other words, if X increases by 1 unit, Y will increase by exactly m units. (“Full disclosure”: this is true only if we know that X and Y have a linear relationship. In almost all linear regression cases, this will not be true!) b is a constant, also known as the Y-intercept. If X equals 0, Y would be equal to b (Caveat: see full disclosure from earlier!). This is not necessarily applicable in real life — we won’t always know the exact relationship between X and Y or have an exact linear relationship.These caveats lead us to a Simple Linear Regression (SLR). In a SLR model, we build a model based on data — the slope and Y-intercept derive from the data; furthermore, we don’t need the relationship between X and Y to be exactly linear. SLR models also include the errors in the data (also known as residuals). I won’t go too much into it now, maybe in a later post, but residuals are basically the differences between the true value of Y and the predicted/estimated value of Y. It is important to note that in a linear regression, we are trying to predict a continuous variable. In a regression model, we are trying to minimize these errors by finding the “line of best fit” — the regression line from the errors would be minimal. We are trying to minimize the length of the black lines (or more accurately, the distance of the blue dots) from the red line — as close to zero as possible. It is related to (or equivalent to) minimizing the mean squared error (MSE) or the sum of squares of error (SSE), also called the “residual sum of squares.” (RSS) but this might be beyond the scope of this blog post :-)In most cases, we will have more than one independent variable — we’ll have multiple variables; it can be as little as two independent variables and up to hundreds (or theoretically even thousands) of variables. in those cases we will use a Multiple Linear Regression model (MLR). The regression equation is pretty much the same as the simple regression equation, just with more variables:This concludes the math portion of this post :) Ready to get to implementing it in Python?There are two main ways to perform linear regression in Python — with Statsmodels and scikit-learn. It is also possible to use the Scipy library, but I feel this is not as common as the two other libraries I’ve mentioned. Let’s look into doing linear regression in both of them:Statsmodels is “a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.” (from the documentation)As in with Pandas and NumPy, the easiest way to get or install Statsmodels is through the Anaconda package. If, for some reason you are interested in installing in another way, check out this link. After installing it, you will need to import it every time you want to use it:Let’s see how to actually use Statsmodels for linear regression. I’ll use an example from the data science class I took at General Assembly DC:First, we import a dataset from sklearn (the other library I’ve mentioned):This is a dataset of the Boston house prices (link to the description). Because it is a dataset designated for testing and learning machine learning tools, it comes with a description of the dataset, and we can see it by using the command print data.DESCR (this is only true for sklearn datasets, not every dataset! Would have been cool though…). I’m adding the beginning of the description, for better understanding of the variables:Running data.feature_names and data.target would print the column names of the independent variables and the dependent variable, respectively. Meaning, Scikit-learn has already set the house value/price data as a target variable and 13 other variables are set as predictors. Let’s see how to run a linear regression on this dataset.First, we should load the data as a pandas data frame for easier analysis and set the median home value as our target variable:What we’ve done here is to take the dataset and load it as a pandas data frame; after that, we’re setting the predictors (as df) — the independent variables that are pre-set in the dataset. We’re also setting the target — the dependent variable, or the variable we’re trying to predict/estimate.Next we’ll want to fit a linear regression model. We need to choose variables that we think we’ll be good predictors for the dependent variable — that can be done by checking the correlation(s) between variables, by plotting the data and searching visually for relationship, by conducting preliminary research on what variables are good predictors of y etc. For this first example, let’s take RM — the average number of rooms and LSTAT — percentage of lower status of the population. It’s important to note that Statsmodels does not add a constant by default. Let’s see it first without a constant in our regression model:The output:Interpreting the Table —This is a very long table, isn’t it? First we have what’s the dependent variable and the model and the method. OLS stands for Ordinary Least Squares and the method “Least Squares” means that we’re trying to fit a regression line that would minimize the square of distance from the regression line (see the previous section of this post). Date and Time are pretty self-explanatory :) So as number of observations. Df of residuals and models relates to the degrees of freedom — “the number of values in the final calculation of a statistic that are free to vary.”The coefficient of 3.6534 means that as the RM variable increases by 1, the predicted value of MDEV increases by 3.6534. A few other important values are the R-squared — the percentage of variance our model explains; the standard error (is the standard deviation of the sampling distribution of a statistic, most commonly of the mean); the t scores and p-values, for hypothesis test — the RM has statistically significant p-value; there is a 95% confidence intervals for the RM (meaning we predict at a 95% percent confidence that the value of RM is between 3.548 to 3.759).If we do want to add a constant to our model — we have to set it by using the command X = sm.add_constant(X) where X is the name of your data frame containing your input (independent) variables.The output:Interpreting the Table — With the constant term the coefficients are different. Without a constant we are forcing our model to go through the origin, but now we have a y-intercept at -34.67. We also changed the slope of the RM predictor from 3.634 to 9.1021.Now let’s try fitting a regression model with more than one variable — we’ll be using RM and LSTAT I’ve mentioned before. Model fitting is the same:And the output:Interpreting the Output — We can see here that this model has a much higher R-squared value — 0.948, meaning that this model explains 94.8% of the variance in our dependent variable. Whenever we add variables to a regression model, R² will be higher, but this is a pretty high R². We can see that both RM and LSTAT are statistically significant in predicting (or estimating) the median house value; not surprisingly , we see that as RM increases by 1, MEDV will increase by 4.9069 and when LSTAT increases by 1, MEDV will decrease by -0.6557. As you may remember, LSTAT is the percentage of lower status of the population, and unfortunately we can expect that it will lower the median value of houses. With this same logic, the more rooms in a house, usually the higher its value will be.This was the example of both single and multiple linear regression in Statsmodels. We could have used as little or as many variables we wanted in our regression model(s) — up to all the 13! Next, I will demonstrate how to run linear regression models in SKLearn.SKLearn is pretty much the golden standard when it comes to machine learning in Python. It has many learning algorithms, for regression, classification, clustering and dimensionality reduction. Check out my post on the KNN algorithm for a map of the different algorithms and more links to SKLearn. In order to use linear regression, we need to import it:Let’s use the same dataset we used before, the Boston housing prices. The process would be the same in the beginning — importing the datasets from SKLearn and loading in the Boston dataset:Next, we’ll load the data to Pandas (same as before):So now, as before, we have the data frame that contains the independent variables (marked as “df”) and the data frame with the dependent variable (marked as “target”). Let’s fit a regression model using SKLearn. First we’ll define our X and y — this time I’ll use all the variables in the data frame to predict the housing price:And then I’ll fit a model:The lm.fit() function fits a linear model. We want to use the model to make predictions (that’s what we’re here for!), so we’ll use lm.predict():The print function would print the first 5 predictions for y (I didn’t print the entire list to “save room”. Removing [0:5] would print the entire list):Remember, lm.predict() predicts the y (dependent variable) using the linear model we fitted. You must have noticed that when we run a linear regression with SKLearn, we don’t get a pretty table (okay, it’s not that pretty… but it’s pretty useful) like in Statsmodels. What we can do is use built-in functions to return the score, the coefficients and the estimated intercepts. Let’s see how it works:Would give this output:This is the R² score of our model. As you probably remember, this the percentage of explained variance of the predictions. If you’re interested, read more here. Next, let’s check out the coefficients for the predictors:will give this output:and the intercept:that will give this output:These are all (estimated/predicted) parts of the multiple regression equation I’ve mentioned earlier. Check out the documentation to read more about coef_ and intercept_.So, this is has a been a quick (but rather long!) introduction on how to conduct linear regression in Python. In practice, you would not use the entire dataset, but you will split your data into a training data to train your model on, and a test data — to, you guessed it, test your model/predictions on. If you would like to read about it, please check out my next blog post. In the meanwhile, I hope you enjoyed this post and that I’ll “see” you on the next one.Thank you for reading!",09/05/2017,22,39,53,"(672, 605)",5,0,0.0,22,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,positive,trust/acceptance
153,A gentle introduction to Doc2Vec,Wisio,Gidi Shperber,2700.0,8.0,1544,"In this post you will learn what is doc2vec, how it’s built, how it’s related to word2vec, what can you do with it, hopefully with no mathematic formulas.Want to learn more? visit www.shibumi-ai.comNumeric representation of text documents is a challenging task in machine learning. Such a representation may be used for many purposes, for example: document retrieval, web search, spam filtering, topic modeling etc.However, there are not many good techniques to do this. Many tasks use the well known but simplistic method of bag of words (BOW), but outcomes will be mostly mediocre, since BOW loses many subtleties of a possible good representation, e.g consideration of word ordering.Latent Dirichlet Allocation (LDA) is also a common technique for topic modeling (extracting topics/keywords out of texts) but it’s very hard to tune, and results are hard to evaluate.In this post, I will review the doc2vec method, a concept that was presented in 2014 by Mikilov and Le in this article, which we are going to mention many times through this post. Worth to mention that Mikilov is one of the authors of word2vec as well.Doc2vec is a very nice technique. It’s easy to use, gives good results, and as you can understand from its name, heavily based on word2vec. so we’ll start with a short introduction about word2vec.word2vec is a well known concept, used to generate representation vectors out of words.There are many good tutorials online about word2vec, like this one and this one, but describing doc2vec without word2vec will miss the point, so I’ll be brief.In general, when you like to build some model using words, simply labeling/one-hot encoding them is a plausible way to go. However, when using such encoding, the words lose their meaning. e.g, if we encode Paris as id_4, France as id_6 and power as id_8, France will have the same relation to power as with Paris. We would prefer a representation in which France and Paris will be closer than France and power.The word2vec, presented in 2013 in this article, intends to give you just that: a numeric representation for each word, that will be able to capture such relations as above. this is part of a wider concept in machine learning — the feature vectors.Such representations, encapsulate different relations between words, like synonyms, antonyms, or analogies, such as this one:So how is it done? word2vec representation is created using 2 algorithms: Continuous Bag-of-Words model (CBOW) and the Skip-Gram model.Continuous bag of words creates a sliding window around current word, to predict it from “context” — the surrounding words. Each word is represented as a feature vector. After training, these vectors become the word vectors.As said before, vectors which represent similar words are close by different distance metrics, and additioanly encapsualte numeric relations, such as the king-queen=man from above.The second algorithm (described in the same article, and well explaiend here) is actaully the opposite of CBOW: instead of prediciting one word each time, we use 1 word to predict all surrounding words (“context”). Skip gram is much slower than CBOW, but considered more accurate with infrequent words.After hopefully understanding what word2vec is, it will be easier to understand how doc2vec works.As said, the goal of doc2vec is to create a numeric representation of a document, regardless of its length. But unlike words, documents do not come in logical structures such as words, so the another method has to be found.The concept that Mikilov and Le have used was simple, yet clever: they have used the word2vec model, and added another vector (Paragraph ID below), like so:If you feel familiar with the sketch above, it’s because it is a small extension to the CBOW model. But instead of using just words to predict the next word, we also added another feature vector, which is document-unique.So, when training the word vectors W, the document vector D is trained as well, and in the end of training, it holds a numeric representation of the document.The model above is called Distributed Memory version of Paragraph Vector (PV-DM). It acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.As in word2vec, another algorithm, which is similar to skip-gram may be used Distributed Bag of Words version of Paragraph Vector (PV-DBOW)Here, this algorithm is actually faster (as opposed to word2vec) and consumes less memory, since there is no need to save the word vectors.In the article, the authors state that they recommend using a combination of both algorithms, though the PV-DM model is superior and usually will achieve state of the art results by itself.The doc2vec models may be used in the following way: for training, a set of documents is required. A word vector W is generated for each word, and a document vector D is generated for each document. The model also trains weights for a softmax hidden layer. In the inference stage, a new document may be presented, and all weights are fixed to calculate the document vector.The thing with this kind of unsupervised models, is that they are not trained to do the task they are intended for. E.g, word2vec is trained to complete surrounding words in corpus, but is used to estimate similarity or relations between words. As such, measuring the performance of these algorithms may be challenging. We already saw the king ,queen,man, woman example, but we want to make form it a rigorous way to evaluate machine learning models.Therefore, when training these algorithms, we should be minded to relevant metrics. One possible metric for word2vec, is a generalization of the above example, and is called analogical reasoning. It contains many analogical combinations, here are some:A success in this task is getting very close results when caclulating distances between matching pairs.Dataset is availible at http://download.tensorflow.org/data/questions-words.txt.Doc2vec was tested in the article on 2 tasks: the first is sentiment analysis, and the second one is similar to the analogical reasoning above.Here are 3 paragraphs from the article. a dataset of such paragraphs was used to compare models. it is easy to see which 2 should be closer:This dataset (was not shared as far as I know) was used to compare some models, and doc2vec came out as the best:One of my clients, Wisio, uses machine learning methods to match “influencer” you-tube videos to content-articles. Doc2vec seems to be a great method for such match.Here is an example for what the Wisio does: in this article, about home made lights in a tree stump, you may see at the bottom 4 related video about woodworking stuff:Wisio’s current model uses tagging mechanism to tag the videos and the articles (“topic modeling”) and measuring distance between tags.Wisio has a few corpora of text, related to the themes of its’ clients. E.g, there is a 100K manually tagged documents about “do it yourself”, for publishers such as above. There are 17 possible tags for each article (e.g, “home decor”, “gardening”, “remodeling and renovating” etc.). For this experiment, we decided trying to predict the tags using doc2vec and some other models.Wisio’s current best model was a convolutional neural network, on top of word2vec, which achieved accuracy of around 70% in predicting the tags for the documents (as described here).Doc2vec model by itself is an unsupervised method, so it should be tweaked a little bit to “participate” in this contest. Fortunately, as in most cases, we can use some tricks: If you recall, in fig 3 we added another document vector, which was unique for each document. If you think about it, it is possible to add more vectors, which don’t have to be unique: for example, if we have tags for our documents (as we actually have), we can add them, and get their representation as vectors.Additionally, they don’t have to be unique. This way, we can add to the unique document tag one of our 17 tags, and create a doc2vec representation for them as well! see below:we will use gensim implementation of doc2vec. here is how the gensim TaggedDocument object looks like:Using gensim doc2vec is very straight-forward. As always, model should be initialized, trained for a few epochs:and then we can check the similarity of every unique document to every tag, this way:The tag with highest similarity to document will be predicted.Using this method, with training only on 10K out of our 100K articles, we have reached accuracy of 74%, better than before.We have seen that with some tweaking, we can get much more from an already very useful word2vec model. This is great, because as said before, in my opinion representation of documents for tagging and matching has still a way to go.Additionally, this shows that this is a great example for how machine learning models encapsulate much more abilities besides the specific task they were trained for. This can be seen in deep CNNs, which are trained for object classification, but can also be used for semantic segmentation or clustering images.To conclude, if you have some document related task — this may be great model for you!Feel free to comment and correct. if there will be enough interest, I’ll share the code.Enjoyed the article? Want learn more? visit www.shibumi-ai.comor: Gidi Shperber",26/07/2017,0,42,22,"(509, 215)",12,1,0.0,17,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,subjective,neutral,anger/irritation
154,Simple and Multiple Linear Regression in Python,Towards Data Science,Adi Bronshtein,3200.0,11.0,2072,"Quick introduction to linear regression in PythonHi everyone! After briefly introducing the “Pandas” library as well as the NumPy library, I wanted to provide a quick introduction to building models in Python, and what better place to start than one of the very basic models, linear regression? This will be the first post about machine learning and I plan to write about more complex models in the future. Stay tuned! But for right now, let’s focus on linear regression.In this blog post, I want to focus on the concept of linear regression and mainly on the implementation of it in Python. Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too:As you can see, a linear relationship can be positive (independent variable goes up, dependent variable goes up) or negative (independent variable goes up, dependent variable goes down). Like I said, I will focus on the implementation of regression models in Python, so I don’t want to delve too much into the math under the regression hood, but I will write a little bit about it. If you’d like a blog post about that, please don’t hesitate to write me in the responses!A relationship between variables Y and X is represented by this equation:In this equation, Y is the dependent variable — or the variable we are trying to predict or estimate; X is the independent variable — the variable we are using to make predictions; m is the slope of the regression line — it represent the effect X has on Y. In other words, if X increases by 1 unit, Y will increase by exactly m units. (“Full disclosure”: this is true only if we know that X and Y have a linear relationship. In almost all linear regression cases, this will not be true!) b is a constant, also known as the Y-intercept. If X equals 0, Y would be equal to b (Caveat: see full disclosure from earlier!). This is not necessarily applicable in real life — we won’t always know the exact relationship between X and Y or have an exact linear relationship.These caveats lead us to a Simple Linear Regression (SLR). In a SLR model, we build a model based on data — the slope and Y-intercept derive from the data; furthermore, we don’t need the relationship between X and Y to be exactly linear. SLR models also include the errors in the data (also known as residuals). I won’t go too much into it now, maybe in a later post, but residuals are basically the differences between the true value of Y and the predicted/estimated value of Y. It is important to note that in a linear regression, we are trying to predict a continuous variable. In a regression model, we are trying to minimize these errors by finding the “line of best fit” — the regression line from the errors would be minimal. We are trying to minimize the length of the black lines (or more accurately, the distance of the blue dots) from the red line — as close to zero as possible. It is related to (or equivalent to) minimizing the mean squared error (MSE) or the sum of squares of error (SSE), also called the “residual sum of squares.” (RSS) but this might be beyond the scope of this blog post :-)In most cases, we will have more than one independent variable — we’ll have multiple variables; it can be as little as two independent variables and up to hundreds (or theoretically even thousands) of variables. in those cases we will use a Multiple Linear Regression model (MLR). The regression equation is pretty much the same as the simple regression equation, just with more variables:This concludes the math portion of this post :) Ready to get to implementing it in Python?There are two main ways to perform linear regression in Python — with Statsmodels and scikit-learn. It is also possible to use the Scipy library, but I feel this is not as common as the two other libraries I’ve mentioned. Let’s look into doing linear regression in both of them:Statsmodels is “a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.” (from the documentation)As in with Pandas and NumPy, the easiest way to get or install Statsmodels is through the Anaconda package. If, for some reason you are interested in installing in another way, check out this link. After installing it, you will need to import it every time you want to use it:Let’s see how to actually use Statsmodels for linear regression. I’ll use an example from the data science class I took at General Assembly DC:First, we import a dataset from sklearn (the other library I’ve mentioned):This is a dataset of the Boston house prices (link to the description). Because it is a dataset designated for testing and learning machine learning tools, it comes with a description of the dataset, and we can see it by using the command print data.DESCR (this is only true for sklearn datasets, not every dataset! Would have been cool though…). I’m adding the beginning of the description, for better understanding of the variables:Running data.feature_names and data.target would print the column names of the independent variables and the dependent variable, respectively. Meaning, Scikit-learn has already set the house value/price data as a target variable and 13 other variables are set as predictors. Let’s see how to run a linear regression on this dataset.First, we should load the data as a pandas data frame for easier analysis and set the median home value as our target variable:What we’ve done here is to take the dataset and load it as a pandas data frame; after that, we’re setting the predictors (as df) — the independent variables that are pre-set in the dataset. We’re also setting the target — the dependent variable, or the variable we’re trying to predict/estimate.Next we’ll want to fit a linear regression model. We need to choose variables that we think we’ll be good predictors for the dependent variable — that can be done by checking the correlation(s) between variables, by plotting the data and searching visually for relationship, by conducting preliminary research on what variables are good predictors of y etc. For this first example, let’s take RM — the average number of rooms and LSTAT — percentage of lower status of the population. It’s important to note that Statsmodels does not add a constant by default. Let’s see it first without a constant in our regression model:The output:Interpreting the Table —This is a very long table, isn’t it? First we have what’s the dependent variable and the model and the method. OLS stands for Ordinary Least Squares and the method “Least Squares” means that we’re trying to fit a regression line that would minimize the square of distance from the regression line (see the previous section of this post). Date and Time are pretty self-explanatory :) So as number of observations. Df of residuals and models relates to the degrees of freedom — “the number of values in the final calculation of a statistic that are free to vary.”The coefficient of 3.6534 means that as the RM variable increases by 1, the predicted value of MDEV increases by 3.6534. A few other important values are the R-squared — the percentage of variance our model explains; the standard error (is the standard deviation of the sampling distribution of a statistic, most commonly of the mean); the t scores and p-values, for hypothesis test — the RM has statistically significant p-value; there is a 95% confidence intervals for the RM (meaning we predict at a 95% percent confidence that the value of RM is between 3.548 to 3.759).If we do want to add a constant to our model — we have to set it by using the command X = sm.add_constant(X) where X is the name of your data frame containing your input (independent) variables.The output:Interpreting the Table — With the constant term the coefficients are different. Without a constant we are forcing our model to go through the origin, but now we have a y-intercept at -34.67. We also changed the slope of the RM predictor from 3.634 to 9.1021.Now let’s try fitting a regression model with more than one variable — we’ll be using RM and LSTAT I’ve mentioned before. Model fitting is the same:And the output:Interpreting the Output — We can see here that this model has a much higher R-squared value — 0.948, meaning that this model explains 94.8% of the variance in our dependent variable. Whenever we add variables to a regression model, R² will be higher, but this is a pretty high R². We can see that both RM and LSTAT are statistically significant in predicting (or estimating) the median house value; not surprisingly , we see that as RM increases by 1, MEDV will increase by 4.9069 and when LSTAT increases by 1, MEDV will decrease by -0.6557. As you may remember, LSTAT is the percentage of lower status of the population, and unfortunately we can expect that it will lower the median value of houses. With this same logic, the more rooms in a house, usually the higher its value will be.This was the example of both single and multiple linear regression in Statsmodels. We could have used as little or as many variables we wanted in our regression model(s) — up to all the 13! Next, I will demonstrate how to run linear regression models in SKLearn.SKLearn is pretty much the golden standard when it comes to machine learning in Python. It has many learning algorithms, for regression, classification, clustering and dimensionality reduction. Check out my post on the KNN algorithm for a map of the different algorithms and more links to SKLearn. In order to use linear regression, we need to import it:Let’s use the same dataset we used before, the Boston housing prices. The process would be the same in the beginning — importing the datasets from SKLearn and loading in the Boston dataset:Next, we’ll load the data to Pandas (same as before):So now, as before, we have the data frame that contains the independent variables (marked as “df”) and the data frame with the dependent variable (marked as “target”). Let’s fit a regression model using SKLearn. First we’ll define our X and y — this time I’ll use all the variables in the data frame to predict the housing price:And then I’ll fit a model:The lm.fit() function fits a linear model. We want to use the model to make predictions (that’s what we’re here for!), so we’ll use lm.predict():The print function would print the first 5 predictions for y (I didn’t print the entire list to “save room”. Removing [0:5] would print the entire list):Remember, lm.predict() predicts the y (dependent variable) using the linear model we fitted. You must have noticed that when we run a linear regression with SKLearn, we don’t get a pretty table (okay, it’s not that pretty… but it’s pretty useful) like in Statsmodels. What we can do is use built-in functions to return the score, the coefficients and the estimated intercepts. Let’s see how it works:Would give this output:This is the R² score of our model. As you probably remember, this the percentage of explained variance of the predictions. If you’re interested, read more here. Next, let’s check out the coefficients for the predictors:will give this output:and the intercept:that will give this output:These are all (estimated/predicted) parts of the multiple regression equation I’ve mentioned earlier. Check out the documentation to read more about coef_ and intercept_.So, this is has a been a quick (but rather long!) introduction on how to conduct linear regression in Python. In practice, you would not use the entire dataset, but you will split your data into a training data to train your model on, and a test data — to, you guessed it, test your model/predictions on. If you would like to read about it, please check out my next blog post. In the meanwhile, I hope you enjoyed this post and that I’ll “see” you on the next one.Thank you for reading!",09/05/2017,22,39,53,"(672, 605)",5,0,0.0,22,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,positive,trust/acceptance
155,Who Writes Better Code: GitHub CoPilot or GPT-3?,Python in Plain English,Kaustubh Gupta,695.0,5.0,915,"GitHub recently announced its new GitHub copilot tool and the internet was loaded with memes. There were speculations/talks that this would change the entire software development industry and everyone would be able to code out their products. GitHub copilot also faced backslashes when it was discovered that it had some security and privacy concerns including exposing API keys, reading private repositories code, etc.GitHub market copilot as an “AI pair programmer” that could increase a developer’s coding efficiency. One would be able to get code suggestions on the go. It supports multiple programming languages and does very well with Python, JavaScript, TypeScript, Ruby, Java, and Go. According to GitHub, the copilot is designed to understand more context than the usual code assistants.The core of the copilot is created by OpenAI which had earlier released GPT-3, the world’s biggest NLP model (at the time of writing this article). The copilot is a subproduct of GPT-3, trained specifically on publically available code. The parent GPT-3 can also provide fairly good code results.GPT-3 predicts the next word of a sentence given the previous words in a sentence. It completes the given prompt to the number of tokens specified. The copilot is based on suggesting what could be the next piece of code based on the current variables used/declared.The comparison between the two may seem a bit unfair, but, it will be interesting to see how these two models perform. I am more proficient with Python, and therefore, all the code snippets would be created in Python language.Let’s with a basic concept whose code should be available in abundance over the internet, Binary Search. Let’s start with GitHub copilot.The copilot has an interactive and easy way to follow long writing code. Supported in VS code IDE, one needs to install the copilot VS code extension and then authenticate to a GitHub account that has technical preview access. I have got the early access and therefore, can proceed with getting code suggestions in my VS code IDE.Here is the result from copilot for binary search:In the above GIF, you can see that the copilot only requires a prompt of the code to perform the autocomplete on the go. I only provided the comment and the possible start of the program, the Python function declaration keyword “def”. After getting this much context, the copilot was able to autocomplete the code.For applying the autocomplete code, press “Tab” on the keyboard.In the same way, GPT-3 also requires a prompt to complete the rest of the part. Let’s see what the GPT-3 returns:A major portion of GPT-3, about 60% 410 billion tokens, is trained on internet crawled data. It is not a surprise that the results might have the exact text without any changes. The same thing can be seen in the binary search code that might be crawled from a website that had the article for it.Also, as the GPT-3 is only about text completion, the code is not indented plus it is missing some brackets. The returned results need modification and especially in the case of Python, where indentation plays a major role in the syntax tree.Flask is a microweb framework that facilitates setup for a backend service and quickly starts a web application. Let’s see how the GitHub copilot helps me in getting code suggestions for this program.The copilot provides helpful code suggestions and the programs made using this are mostly ready to run at the go. On the other hand, have a look at the GPT-3 code:Even if the code is not indented and the last piece of code was not completed in the first iteration, though it was completed in the second iteration, the GPT-3 gives a strong competition. This particular result took me around 6–7 iterations. The previous results were nowhere close to what I wanted from the model. Let’s move to the last and final round for this article.I think most of you will get this reference! Inverting a binary tree is just swapping left and right children. Let’s check how the GPT-3 returns the code for this:Again, we got the code but not in the cleanest manner. As the complexity, in terms of the number of lines and level of indentation is more, it becomes difficult for the user to check every line for its indentation level. This also increases the chances of the code having syntactic errors.On the other hand, if the user knows the possible flow of the program, copilot can help them navigate them in a direction, that can lead them to complete code completion.The intention of this article is not to pull one of them down. The main focus was on how GitHub has managed to use the underlying power of OpenAI to create the co-pilot tool that can help developers to get code suggestions on the go. The suggestions by the copilot are very much accurate in my testing and I will continue to use them in my daily work.GPT3 is more of a text completion tool that generates the next characters/words based on the previous predictions. I found it useful for language writing paragraphs, essays, etc.If you want to read/explore every article of mine, then head over to my master article list which gets updated every time I publish a new article on any platform!For any doubts, queries, or potential opportunities, you can reach out to me via:www.linkedin.comPrevious Article:python.plainenglish.ioMore content at plainenglish.io. Sign up for our free weekly newsletter. Get exclusive access to writing opportunities and advice in our community Discord.",23/11/2021,0,4,13,"(700, 391)",6,0,0.0,7,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
157,,,,,,0,,,0,0,0,,0,0,,0,,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
158,Creating and training a U-Net model with PyTorch for 2D & 3D semantic segmentation: Dataset building [1/4],Towards Data Science,Johannes Schmidt,157.0,10.0,1921,"In this series (4 parts) we will perform semantic segmentation on images using plain PyTorch and the U-Net architecture. I will cover the following topics: Dataset building, model building (U-Net), training and inference. For that I will use a sample of the infamous Carvana dataset (2D images), but the code and the methods work for 3D datasets as well. For example: I will not use the torchvision package, as most transformations and augmentations only work on 2D input with PIL. You can find the repo of this project here with a jupyter notebook for every part (and more).I was a master student in biology who started to learn programming with python and ended up trying deep learning (semantic segmentation with U-Net) on electron tomograms (3D images) for my master thesis. In this process I not only learned quite a lot about deep learning, python and programming but also how to better structure your project and code. This blog aims to share my experience and a tutorial to use plain PyTorch to efficiently use deep learning for your own semantic segmentation project.To bring more structure into your project, I recommend to create a new project and create several python files, each having different classes and functions that can be imported and used. Personally, I like to use PyCharm for that but other IDEs such as Spyder are also a good choice. I also prefer working with the IPython console within PyCharm instead of Jupyter notebooks. A combination of both can be very helpful though. If you prefer a single Jupyter Notebook, that’s also fine, just bear in mind that it could get to be a long script. You can find the files and code on github. Most of the structure and code that I will be showing is inspired by the work of this project: elektronn3.Before we start creating our data generator, let’s talk about the data first. What we would like to have is two directories, something like /Input and /Target. In the /Input directory, we find all input images and in the /Target directory the segmentation maps. Visualizing the images would look something like the image below. The labels are usually encoded with pixel values, meaning that all pixels of the same class have the same pixel value e.g. background=0, dog=1, cat=2 in the example below.The goal of the network is to predict such a segmentation map from a given input image.In deep learning, we want to feed our network with batches of data. Therefore, we would like to have a data generator that does the following:With PyTorch it is fairly easy to create such a data generator. We create a custom Dataset class, instantiate it and pass it to PyTorch’s dataloader. Here is a simple example of such a dataset for a potential segmentation pipeline (Spoiler: In part 3 I will make use of the multiprocessing library and use caching to improve this dataset):The SegmentationDataSet class inherits from torch.data.Dataset . In the initialization method __init__, we expect a list of input paths and a list of target paths. The __getitem__ method simply reads an item from our input and target list using the skimage.imread() function. This will give us our input and target image as numpy.ndarrays. We then process our data with the transform function that expects an input and target pair and should return the processed data as numpy.ndarrays again. But I will come to that later. At the end, we just make sure that we end up having a torch.tensor of certain type for our input and target. In this case, this is usually torch.float32 and torch.int64 (long) for input and target, respectively. This dataset can be then used to create our dataloader (the data generator that we want). You may ask: How do we make sure that this dataset will output the correct target for every input image? If the input and target lists happen to have the same order, e.g. because input and target have the same name, the mapping should be correct.Let’s try it out with a simple example:Here, we create an instance of our SegmentationDataSet class. For now, we don’t want any transformation to happen on our data, so we leave transform=None . In the next step we create our data loader by passing our training_dataset as input. We choose to have an batch_size of 2 and to shuffle the data shuffle=True . There are some other useful arguments that can be passed in when instantiating the dataloader and you should check them out. The result could look something like this:There are some things to notice here:Because of this we need to transform the data a little bit.Note: If we omit the typecasting at the end of our SegmentationDataSet, the dataloader would still not output numpy.ndarrays but torch.tensors. As our input is read as a numpy.ndarray in uint8, it will also output a torch.tensor in uint8. For float32 we need to change the type.Let’s create a new file, that we name transformations.py.In order to transform our data so that they meet the requirements of the network, we create a class FunctionWrapperDouble which can take any function and returns a partial (basically a function with defined arguments). This allows us to stack (functions) transformations in a list that we want to be applied on the data. To stack these functions I use the ComposeDouble class. The Double in these classes means that it is meant for a dataset with input-target pairs (like our case for semantic segmentation). There is also a Single version of these classes for the case that you want to just experiment with some input or target images (or visualize your dataset). So what we want, is to create an object, that comprises all the transformations that we want to apply to the data. We can then pass this object as an argument in our SegmentationDataSet instance. Confused? Let me show what I mean with an example:The tranfsorms variable is an instance of the ComposeDouble class that comprises a list of transformations that is to be applied to the data! create_dense_target and normalize_01 are functions that I defined in transformations.py, while np.moveaxis is just a numpy function that simply moves the channel dimension C from last to second with the numpy library. It is important to note that these functions are expected to take in and output a np.ndarray ! This way, we can create and perform transformations including augmentations on arbitrary numpy.ndarrays . Or we could just write a wrapper class for other libraries, such as albumentations to make use of their transformations (more on that later). The __repr__ is just a printable representation of a object and makes it easier to understand what transformations and what arguments were used.Why the effort you may ask? Well, this makes it clear what transformations are performed on the data when we create the dataset. The ComposeDouble class just puts the different transformation together.Let’s test it out with some random input-target-data.Here we also resize our input and target image by using the skimage.transform.resize(). And we linearly scale our input to the range [0, 1] using normalize01 . We could also normalize the input based on a mean and std of a given dataset with normalize from the transformations.py file, but this will do for now.This will output:Now let’s put these pieces of code together to create a dataset for the Carvana dataset. We have our input images stored in Carvana/Input and our targets stored in Carvana/Targets. This is just a sample of the original dataset and can be downloaded here or found in the github repo. This sample consists of only 6 cars, but each car from 16 angles — 96 images in total. I will be using the pathlib library to get the directory path of every input and target. If this library is new to you, you should check it out here.We import the SegmentationDataSet class and the transformations that we want to use. To get the input-target paths, I use the function get_filenames_of_path that will return a list of all items within a directory. We then stack our transformations inside the Compose object that we name transforms. Because training is usually performed with a training dataset and a validation dataset, we split our input and target lists with sklearn.model_selection.train_test_split(). We could do it manually too (and it makes more sense for this dataset), but that’s ok for now. With these lists, we can create our datasets and the corresponding dataloaders. Now we should check if our datasets (training and validation) output the correct format! Since the datasets have a __getitem__ and method, we can treat them almost like a sequence object (e.g. list):This will give us:To check if our dataloader functions the way we want, we can get a batch with:Everything seems to be in order now. Now let’s visualize this result.In order to inspect our transformed images, we should visualize the input and it’s respective target image. For 2D images, we could use matplotlib. For 3D images, however, it becomes a bit hacky and slow. But there is an alternative: napari. This is a fast, interactive multi-dimensional image viewer for python, that can be used in a jupyter notebook, an Ipython console or within a .py script.It’s built on top of Qt (for the GUI), vispy (for performant GPU-based rendering), and the scientific Python stack (numpy, scipy). — napariTo visualize our dataset, we should reverse some of the transformations we performed on the data, e.g. we would like to have an input image of shape [H, W, C], in the range of [0–255] and as numpy.ndarray. Let’s create a visual.py file in which we create a DatasetViewer class:I won’t go into details about napari and the code here. This class basically allows us to view our the images and targets of our dataset by iterating over the dataset with custom keybindings (’n’ for next and ‘b’ for back)! Some comments on the transforms: The function re_normalize() scales the input image back to [0–255]. We assume that our images and targets are torch.tensors and force them to be on the cpu and as np.ndarrays.Let’s test it out:You do not need to enter %gui qt before using napari in Ipython or witihn a Jupyter notebook in this case, because this will be evoked when intantiating the class with the enabel_gui_qt() function.This will open a GUI that will look something like this:When I press 'n' on the keyboard, I will get the next image-target pair from the dataset:Now that we have a solid codebase, let’s expand our dataset with some augmentations.For 2D images, we don’t have to implement everything from scratch with numpy. Instead, we could use a library like albumentations. For that, we just need to write a wrapper, like AlbuSeg2d. As an example, we could horizontally flip our training images and their respective targets. The validation images are usually not augmented, which makes it necessary to have different transformations for the training and validation dataset:We can then change the code for our dataset objects accordingly:The probability of a flipped image is p=0.5. When we visualize the image-target pairs again, we eventually get batches with horizontally flipped input-target pairs:We have created a data generator that can feed a network with batches of transformed/augmented data in the correct format. We also know how to visualize our dataset with the help of napari. This applies to 2D and 3D datasets. Now that we have spent some time on creating and visualizing the dataset, let’s move on to model building in the next chapter.",02/12/2020,6,3,2,"(673, 397)",5,2,0.0,23,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
159,Few Shot Object Detection,OffNote Labs,Sai Sree Harsha,4.0,6.0,1279,"In this article we will discuss the task of few shot object detection from 2D images. We will first look at what exactly the task of few shot object detection is, and in the remaining two sections discuss two important approaches for the task, namely meta-learning and fine-tuning.Our ability to train machine learning models that generalise to novel concepts without abundant labeled data is still far from satisfactory when compared to human visual systems. Even an infant can easily comprehend new concepts with very few instructions. This ability to generalise from only a few labelled examples is called few-shot learning. Few-shot learning has become a key area of interest in the machine learning community and is an important yet unsolved problem in computer vision.In the few-shot learning setting, we have a partition of the classes as base and novel classes with many labelled samples for the base classes and only a few labelled samples for novel classes. Specifically, in a K-shot object detection task only K labelled bounding boxes are available for each of the novel classes. The goal is to transfer the knowledge learned on the base classes with abundant samples to under-represented novel classes, so that the model is able to effectively detect objects belonging to novel classes, even though it has seen only K instances of a given novel class during training.A popular solution to the few-shot learning problem is meta-learning, where a meta-learner is designed to parameterise the optimization algorithm or predict the network parameters by “learning to learn”. These meta-learning strategies use simulated few-shot tasks, by sampling from base classes during training, in order to learn the mechanism of how to learn from the few examples in the novel classes. However, much of this work has focused on basic image classification tasks. Unlike image classification, object detection requires the model to not only recognise the object types but also localise the targets among millions of potential regions. This additional task significantly increases the overall complexity.The general deep-learning models for object detection can be divided into two groups: proposal-based (two-stage) methods and direct (one-stage) methods without proposals. While the R-CNN series and FPN fall into the former line of work, the YOLO series and SSD belong to the latter. In meta-learning approaches, in addition to the base object detection model that is either single-stage or two-stage, a meta-learner is introduced to acquire class-level meta knowledge and help the model generalise to novel classes. This can be acheived through feature re-weighting, such as in FSRW [link] and Meta R-CNN [link], or class-specific weight generation, such as in MetaDet [link].As shown in Figure 1 the training procedure is also split into a meta-training stage, where the model is only trained on the data of the base classes, and a meta fine-tuning stage, where the support set includes the few examples of the novel classes and a subset of examples from the base classes.The base object detector and the meta-learner are often jointly trained using episodic training. Each episode is composed of a supporting set of N objects and a set of query images. The support images and the binary masks of the annotated objects are used as input to the meta-learner, which generates class re-weighting vectors that modulate the feature representation of the query images.A recently proposed method for the task of few-shot object detection, called Frustratingly Simple Few-Shot Object Detection [link], involves a Two-stage Fine tuning Approach (TFA). They adopt the widely used Faster R-CNN, a two-stage object detector, as their base detection model. As shown in Figure 2, the feature learning components of a Faster R-CNN model include the backbone, the Region Proposal Network (RPN), as well as a two-layer fully-connected sub-network which acts as a proposal-level feature extractor. There is also a box predictor composed of a box classifier to classify the object categories and a box regressor to predict the bounding box coordinates. Intuitively, the backbone features as well as the RPN features are class-agnostic. Therefore, features learned from the base classes are likely to transfer to the novel classes without further parameter updates. A key component of the method is to separate the feature representation learning and the box predictor learning into two stages.In the first stage, the whole object detection model is trained only on the base classes, with three losses, one applied to the output of the RPN to distinguish foreground from backgrounds and refine the anchors, a cross-entropy loss for the box classifier, and a smoothed L1 loss for the box regressor.In the second stage, a small balanced training set with K shots per class is created, containing both base and novel classes. The weights of the box prediction networks are randomly initialised and only the box classification and regression networks, namely the last layers of the detection model are trained, while keeping the rest of the model fixed. The same losses used in the first stage are used, but with a smaller learning rate.The paper also considers using a classifier based on cosine similarity in the second fine-tuning stage. It is found empirically that the instance-level feature normalisation used in the cosine similarity based classifier helps reduce the intra-class variance and improves the detection accuracy of novel classes. Additionally, it leads to minimal decrease in the detection accuracy of base classes when compared to a FC-based classifier, especially when the number of training examples is small.The few-shot detection performance (mAP50) of different models on the PASCAL VOC dataset is shown in Table 1 below. The performance is evaluated on three different sets of novel classes.The Two stage Fine-tuning Approach (TFA) is compared with the meta-learning approaches FSRW, Meta-RCNN and MetaDet together with the other baseline fine-tuning based approaches, namely 1) FRCN/YOLO+joint, denotes joint training, where the base and novel class examples are jointly trained in one stage, 2) FRCN/YOLO+ft-full, denotes fine-tuning the entire model, where both the feature extractor and the box predictor are jointly fine-tuned until convergence in the second fine-tuning stage and 3) FRCN/YOLO+ft, which denotes fine-tuning the entire model with a lower number of iterations.We see that the Two-stage Fine-tuning Approach (TFA) consistently outperforms the baseline methods by a large margin (about 2∼20 points), especially when the number of shots is low. Here FRCN stands for Faster R-CNN and TFA w/ cos is the TFA with a cosine similarity based box classifier.Table 2 shows the average AP and AP75 of the 20 novel classes on the COCO dataset. AP75 means matching threshold is 0.75, a more strict metric than AP50. Again, the TFA consistently outperforms previous methods across all shots on both novel AP and novel AP75.An added advantage of the TFA is that it is more memory efficient. While the episodic learning used in meta-learning approaches can be very memory inefficient as the number of classes in the supporting set increases, the fine-tuning method only fine-tunes the last layers of the network with a normal batch training scheme, which is much more memory efficient. The fine-tuning approach establishes new state of the art performances particularly in the hard 1-shot and 2-shot scenarios, out-performing all the prior meta-learning based approaches. This indicates that feature representations learned from the base classes might be able to transfer to the novel classes and simple adjustments to the box predictor can provide strong performance gain.In the recent work on few-shot object detection, we broadly find two kinds of approaches, namely the meta-learning approaches and the fine-tuning approach. The methods proposed initially were based on meta-learning, modifying existing object detection pipelines with inspiration from meta-learning on image classification. However, the more recently proposed fine-tuning approach is not only more memory efficient, but also out-performs all the prior meta-learning based approaches.",14/02/2021,0,4,3,"(700, 222)",4,0,0.0,4,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,surprise/amazement
160,Understanding of Convolutional Neural Network (CNN) — Deep Learning,,Prabhu,1500.0,5.0,638,"In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces etc., are some of the areas where CNNs are widely used.CNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image.Technically, deep learning CNN models to train and test, each input image will pass it through a series of convolution layers with filters (Kernals), Pooling, fully connected layers (FC) and apply Softmax function to classify an object with probabilistic values between 0 and 1. The below figure is a complete flow of CNN to process an input image and classifies the objects based on values.Convolution LayerConvolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features using small squares of input data. It is a mathematical operation that takes two inputs such as image matrix and a filter or kernel.Consider a 5 x 5 whose image pixel values are 0, 1 and filter matrix 3 x 3 as shown in belowThen the convolution of 5 x 5 image matrix multiplies with 3 x 3 filter matrix which is called “Feature Map” as output shown in belowConvolution of an image with different filters can perform operations such as edge detection, blur and sharpen by applying filters. The below example shows various convolution image after applying different types of filters (Kernels).StridesStride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. The below figure shows convolution would work with a stride of 2.PaddingSometimes filter does not fit perfectly fit the input image. We have two options:Non Linearity (ReLU)ReLU stands for Rectified Linear Unit for a non-linear operation. The output is ƒ(x) = max(0,x).Why ReLU is important : ReLU’s purpose is to introduce non-linearity in our ConvNet. Since, the real world data would want our ConvNet to learn would be non-negative linear values.There are other non linear functions such as tanh or sigmoid that can also be used instead of ReLU. Most of the data scientists use ReLU since performance wise ReLU is better than the other two.Pooling LayerPooling layers section would reduce the number of parameters when the images are too large. Spatial pooling also called subsampling or downsampling which reduces the dimensionality of each map but retains important information. Spatial pooling can be of different types:Max pooling takes the largest element from the rectified feature map. Taking the largest element could also take the average pooling. Sum of all elements in the feature map call as sum pooling.Fully Connected LayerThe layer we call as FC layer, we flattened our matrix into vector and feed it into a fully connected layer like a neural network.In the above diagram, the feature map matrix will be converted as vector (x1, x2, x3, …). With the fully connected layers, we combined these features together to create a model. Finally, we have an activation function such as softmax or sigmoid to classify the outputs as cat, dog, car, truck etc.,SummaryIn the next post, I would like to talk about some popular CNN architectures such as AlexNet, VGGNet, GoogLeNet, and ResNet.References :",04/03/2018,0,15,1,"(492, 265)",11,4,0.0,4,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
161,Review: MobileNetV2 — Light Weight Model (Image Classification),Towards Data Science,Sik-Ho Tsang,6100.0,5.0,252,"In this story, MobileNetV2, by Google, is briefly reviewed. In the previous version MobileNetV1, Depthwise Separable Convolution is introduced which dramatically reduce the complexity cost and model size of the network, which is suitable to Mobile devices, or any devices with low computational power. In MobileNetV2, a better module is introduced with inverted residual structure. Non-linearities in narrow layers are removed this time. With MobileNetV2 as backbone for feature extraction, state-of-the-art performances are also achieved for object detection and semantic segmentation. This is a paper in 2018 CVPR with more than 200 citations. ([2018 CVPR] [MobileNetV2]MobileNetV2: Inverted Residuals and Linear BottlenecksImage Classification [LeNet] [AlexNet] [Maxout] [NIN] [ZFNet] [VGGNet] [Highway] [SPPNet] [PReLU-Net] [STN] [DeepImage] [SqueezeNet] [GoogLeNet / Inception-v1] [BN-Inception / Inception-v2] [Inception-v3] [Inception-v4] [Xception] [MobileNetV1] [ResNet] [Pre-Activation ResNet] [RiR] [RoR] [Stochastic Depth] [WRN] [Shake-Shake] [FractalNet] [Trimps-Soushen] [PolyNet] [ResNeXt] [DenseNet] [PyramidNet] [DRN] [DPN] [Residual Attention Network] [DMRNet / DFN-MR] [IGCNet / IGCV1] [MSDNet] [ShuffleNet V1] [SENet] [NASNet] [MobileNetV2]Object Detection [OverFeat] [R-CNN] [Fast R-CNN] [Faster R-CNN] [MR-CNN & S-CNN] [DeepID-Net] [CRAFT] [R-FCN] [ION] [MultiPathNet] [NoC] [Hikvision] [GBD-Net / GBD-v1 & GBD-v2] [G-RMI] [TDM] [SSD] [DSSD] [YOLOv1] [YOLOv2 / YOLO9000] [YOLOv3] [FPN] [RetinaNet] [DCN]Semantic Segmentation [FCN] [DeconvNet] [DeepLabv1 & DeepLabv2] [CRF-RNN] [SegNet] [ParseNet] [DilatedNet] [DRN] [RefineNet] [GCN] [PSPNet] [DeepLabv3]Biomedical Image Segmentation [CUMedVision1] [CUMedVision2 / DCAN] [U-Net] [CFS-FCN] [U-Net+ResNet] [MultiChannel] [V-Net] [3D U-Net] [M²FCN] [SA] [QSA+QNT] [3D U-Net+ResNet]Instance Segmentation [SDS] [Hypercolumn] [DeepMask] [SharpMask] [MultiPathNet] [MNC] [InstanceFCN] [FCIS]Super Resolution [SRCNN] [FSRCNN] [VDSR] [ESPCN] [RED-Net] [DRCN] [DRRN] [LapSRN & MS-LapSRN] [SRDenseNet]Human Pose Estimation [DeepPose] [Tompson NIPS’14] [Tompson CVPR’15] [CPM]",19/05/2019,0,51,7,"(610, 337)",14,13,0.0,129,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
162,Follow-ups: Cart-Pole Balancing with Q-Network,,Matthew Chan,215.0,3.0,381,"In my previous post on Cart-Pole Balancing with Q-Learning, I discretized the state-space into multiple buckets to construct a Q-table. Here, I replaced it with a neural network called Q-network.The Q-network that I made has three fully-connected hidden layers (I didn’t draw the connections in the diagram below). The inputs to the network are the four state variables: position (x), velocity (x_dot), angle (theta), and angular velocity (theta_dot). The outputs are the Q-values of the two possible actions: move to the left or move to the right.Once the Q-network is trained, it does exactly the same thing as the Q-table. It tells you the Q-value for each action given the state.To train the Q-network, you need to train it with more accurate Q-values over time. At each times step, you compute a predicted Q-value for a given state-action like this:This new, “more accurate” Q-value can then be fed back into the neural network and be used to update the weights and biases through back propagation.One of the the advantages of using a Q-network is that you no longer need to discretize the state values to build a Q-table. This is useful when the dicretization method is not obvious and when you don’t really know the range of the values that you are getting. In addition, in cases when the state-action space is large (i.e. many features and many possible actions), the Q-table method becomes impractical.You can find my full implementation and results here. Note that most of what I did was based off of the CNTK Reinforcement Learning Basics tutorial which was actually very well-written and easy-to-follow.With a three-layer Q-network, it only took 493 episodes to solve the problem! For a simple problem like Cart-Pole, the Q-table method was definitely faster. However, the Q-network method has the potential to tackle much harder problems!One interesting thing that I noticed was that even though the pole stayed relatively upright during the entire episode, the cart was inching towards the right. This means that the neural network was also exploiting the fact that our evaluation time window was short enough that the cart couldn’t drift too far horizontally! If we do want to keep it at the center indefinitely, we will have to penalize horizontal displacement as well.Here is the code for this implementation:",15/11/2016,1,0,5,"(616, 265)",3,0,0.0,4,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
163,A story of my first gold medal in one Kaggle competition: things done and lessons learned,Towards Data Science,Andrew Lukyanenko,1200.0,7.0,1068,"Taking part in kaggle competitions is a serious challenge. You need to spend a lot of time and efforts, study new things and try many tricks to get a high score. And often this isn’t enough because there are a lot of great people, who have more experience, more free time, more hardware or some other advantages (maybe they even have all the advantages).Previously I was able to get only silver medals in competitions. Sometimes it was thanks to luck (after shake up), sometimes it was due to a lot of work. Also there were multiple competitions where I got bronze medals (or no medals) despite all the time spent on them.When I saw that a new competition started at the end of May, I was immediately interested in it. It was a domain specific competition aimed at predicting interactions between atoms in molecules.This challenge aims to predict interactions between atoms. Nuclear Magnetic Resonance (NMR) is a technology which uses the principles similar to MRI to understand the structure and dynamics of proteins and molecules.Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.In this competition, we try to predict the magnetic interaction between two atoms in a molecule (the scalar coupling constant). State-of-the-art methods from quantum mechanics can calculate these coupling constants given only a 3D molecular structure as input. But these calculations are very resource intensive, so can’t be always used. If machine learning approaches could predict these values, it would really help medicinal chemists to gain structural insights faster and cheaper.I usually write EDA kernels for new Kaggle competitions and this one wasn’t an exception. While I was doing it, I realized that the competition was very interesting and unique. We had information about molecules and their atoms, so molecules could be represented as graphs. Common approach for tabular data in Kaggle competitions is an extensive feature engineering and using gradient boosting models.I also used LGB in my early attempts, but knew that there should be better ways to work with graphs. This was quite fascinating and I decided to seriously take part in this competition.I had no domain knowledge (last time I paid attention to chemical formulas was in school), so I decided to start with pure ML technics: a lot of feature engineering, creating out-of-fold meta-features and so on. As usual I published my work in kernels. As you can see in the screenshot, they were quite popular :)At that time this approach gave quite a good score on the leaderboard and I was able to stay in the silver zone.One of the things which really helped me was reading forums and kernels. From the start of the competition and until the very end I read all the kernels and forum threads. They contain a lot of useful information which could be missed otherwise. Even less popular kernels can contain new interesting features. And small threads could contain insights, which could help increasing the score.Almost since the beginning I realized that domain expertise will provide a serious advantage, so I hunted for every piece of such information. Of course I noticed that there were several active experts, who wrote on the forum and created kernels, so I read everything from them.And one day I received an e-mail from Boris, who was an expert in this domain and thought that our skills could complement each other. Usually I prefer to work on a competition by myself for some time, but in this case combining forces seemed to be a good idea to me. And this decision turned out to be a great one :)Our approaches were quite different at the beginning: I did technical feature engineering and Boris worked on creating descriptors. After some time we realized that my models worked better on some atom pair types, and his on others — so we trained different models for different types.We were lucky to also team up with Philip Margolis. And after little time his models showed much better results than ours.Another member of our team became Bojan and we were able to improve our result even further.They were really great ML experts, it was a great experience working with them on this competition.At that time we already saw a potential of neural nets in the competition: a well known kaggler Heng posted an example of MPNN model.After some time I was even able to make it run on my PC, but the results were worse compared to our LGB models. Nevertheless our team knew that we would need to work with these Neural Nets if we wanted to aim high.Some debates ensured and as a result we asked Christof to join our team. It was amazing to see how he was able to build a new neural nets extremely fast. Soon we stopped training LGB because they were far behind Christof’s neural nets.Since that time my role switched to a support one. I did a lot of experiments with our neural nets: trying various hyperparameters, different architectures, various little tweaks to training schedule or losses and so on. Sometimes I did EDA on our predictions to find our interesting or wrong cases and later we used this information to improve our models even further.One of my main contributions was looking for new approaches:But in the end it was Christof who implemented the architectures and it left me very impressed and inspired.Good hardware is really important for training these neural nets. We used a lot of hardware, trust me, really a lot :) But! We also used kaggle kernels a lot: you can train models in 4 kaggle kernels with P100 at the same time (now only 1), so we could get gold medal even without additional hardware.Our final solution secured us the 8th place and gold medals. And I’m kaggle master now :)It is worth noticing that our team is among those who were able to get score better than -3 lmae (Log of the Mean Absolute Error averaged across types). In this thread competition hosts wrote that they would be excited to see scores better then -3 and we did it!There were a lot of things I learned from this competition and here some lessons which I’d like to share:I was lucky to work with these amazing people and would like to thank them for that!",01/09/2019,0,0,0,"(656, 431)",9,2,0.0,6,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,positive,joy/calmness
164,"Transformers Hottie Megan Fox, Mentioned on The Stern Show","Shabooty - Howard Stern, Comedy & Hip-Hop Music",Shabooty ➿,674.0,2.0,278,"If you didn’t hear, Megan Fox was mentioned on “The Howard Stern Show” on Monday (12–10–07).Here’s the re-cap (Mark’s Friggin)::Howard kept pushing the Transformers thing and said that the chick in the movie is the hottest chick he’s ever seen. Artie said that he knows who that chick is and he’s right, she’s smokin’. He said that he was on a plane one time and saw the chick in the movie that he wasn’t watching, and he and this other dude had to look at the screen. He said that he had never seen her before in his life and he was shocked when he saw her. Howard said that her name is Megan Fox.Artie said that this movie ‘’Superbad’’ was really good too. Howard read that Brian Austin Green is dating this Megan Fox chick. Artie heard that and said that he’s going to leave show business if that’s true. They spent a couple more minutes talking about how hot she is. Fred said that she’s got some nasty tattoos on her though so maybe Howard wouldn’t like them. She’s got Brian tattooed on her somewhere. She’s only 21 years old too.The guys showed some pictures of Megan to the guys and then had Robin get back to her news.Howard mentioned Transformers a couple more times during the news whenever he got the chance. He really seemed to like the movie.Who can say that chick isn’t a dyme?. . .I agree with the sentiment though, it’s a shame she’s wasting away with a has-been *cough* “B.A.G” aka Brian Austin Green — that’s just horrible.For More Pix of Megan Go Here, Here, Here and for Nip-Slip Go Here",12/12/2007,0,0,1,"(90, 90)",2,0,0.0,5,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
165,"Hello, Gradient Descent",AI Society,Juan Camilo Bages Prada,48.0,7.0,919,"Hi there! This article is part of a series called “Hello, <algorithm>”. In this series we will give some insights about how different AI algorithms work, and we will have fun by implementing them. Today we are gonna talk about Gradient Descent, a simple yet powerful optimization algorithm for finding the (local) minimum of a given function.The inspiration behind Gradient Descent comes directly from calculus. Basically, it states that if we’ve a differentiable function, the fastest way to decrease is by taking steps proportional to the opposite direction of the function’s gradient at any given point. This happens because the gradient points to the steepest direction of the function’s generated surface at the current point.In other words, think about the function’s surface as a mountain that you are hiking down. You know that your goal is to reach the bottom, and you may think that the fastest way to accomplish this is by proceeding through the path that makes you descend the most. In this case, that path points to the opposite of the steepest mountain direction upwards.With this in mind, we can repeatedly perform these steps in the appropriate direction and we should eventually converge into the (local) minimum. Following our analogy, this is the equivalent of arriving to the bottom of our mountain.So we’ve been talking about taking steps in the right direction, but how can we calculate them? Well, the answer is in the following equation:This formula needs some clarification. Let’s say we are currently in a position Θ⁰, and we want to get to a position Θ¹. As we said previously, every step is gonna be proportional to the opposite direction of the function’s gradient at any given point. So this definition of step for a given function J(Θ) will be equal to −α∇J(Θ).Remember that we take steps in the opposite direction of the gradient. So in order to achieve this, we subtract the step value to our current position.The symbol α is called the Learning Rate. This is a value that will force us to take little steps so we don’t overshoot the (local) minimum. A bad choice for α would trap us into one of the following possibilities:Take a look at the following examples to see what happens when we make a bad choice for the Learning Rate α:The gradient of a function J(Θ) (denoted by ∇J(Θ)) is a vector of partial derivatives with respect to each dimension or parameter Θᵢ. Notational details are given in the equation below:To make this definition of gradient clearer, let’s calculate the gradient of the following function:As we can see, this function contains three parameters or dimensions. Thus the appropriate way to proceed is by calculating the partial derivative with respect to each param:Now we can group those values and that will give us the function’s gradient:And that’s it! With this vector, we can get the steepest direction at any given point simply by replacing each parameter with its corresponding value:And now, for the grand finale, we will go through a full example and we will code our own algorithm for gradient descent.In this section, we will apply linear regression in order to find the correct function approximation for a given set of points in a plane. The set of points we are trying to predict looks as follows:As it’s common, the choice for J(Θ) will be the least-squares cost function for measuring the error of an approximation:In the equation above:Finally, before beginning to code let’s calculate the gradient vector of our function. You can see that as we’ve got two parameters for Θ, we will need to calculate two partial derivatives.Okay, time to proceed. It’s important to mention that our implementation will be in a vectorized form. This means that we will transform all the formulas mentioned above into matrices operations. The advantages of this implementation are that code will be more concise, and with this our computer can take advantage of advanced underlying matrix algorithms.To work with the vectorized form, we need to add a dummy variable x0 to each point with a value equal to 1. The reason for this is that when we perform matrix multiplication, the intercept parameter Θ0 will be multiplied with that 1 and it will maintain its value as the defined equations establishes.Below you can see the vectorized form of the error function J(Θ) and its gradient ∇J(Θ):With every function defined, we can proceed to code our algorithm. The first thing we should do is to declare the points dataset and the Learning Rate α.Now we can proceed by defining the error function J(Θ) and its gradient ∇J(Θ). Remember everything will be defined in a vectorized way.This is the heart of our code. Here we will perform steps that update Θ until we reach the (local) minimum. That is, when all the values of the gradient vector are less than or equal to some specified threshold (1/e⁵ in this case).And we’re done! You can see the complete code in the snippet below:Now we can run our algorithm and it will give us the optimal values for Θ that minimize the error. Below you can see the answers I obtained after running it on my computer:This is the scatter plot we showed before with the line corresponding to the optimal Θ:Well, we’ve finished our code and our article. I hope that you’d learned one thing or two about Gradient Descent, and more importantly, that you are now really excited about learning by taking a look at the further reading list.",16/02/2017,0,4,26,"(397, 118)",23,5,0.0,13,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,positive,joy/calmness
166,Teorema de Bayes — Ejercicios— [Bases de la IA],,Javier Diaz Arca,,4.0,530,"En 3 máquinas A B y C se fabrican piezas de la misma naturaleza. el porcentaje de piezas que resultan defectuosa en cada máquina es, respectivamente : 1% 2% y 3% . Se mezclan 300 piezas, 100 de cada máquina, y se elige una pieza al azar, que resulta ser defectuosa.¿Cual es la probabilidad de que haya sido fabricada en la máquina A?En cierto país donde la enfermada X es endémica, se sabe que un 12% de la población padece de dicha enfermeras. Se dispone de una prueba para detectar la enfermedad, pero no es totalmente fiable, ya que , da positivo en el 90 % de los casos de personas realmente enfermas, y da positivo en el 5 % de personas sanas.¿ Cual es la probabilidad de que esté sana una persona a la que la prueba le ha dado positiva?Rafa Nadal y Mac Lopez ganaron el Oro en las olimpiadas de Rio 2016, tras imponerse en la final a la dupla formada por los rumanos Florín Mergea y Horia Tecau por 6/2, 3/6 y 6/4. La pareja española tuvo 16 puntos de break de los cuales gano el 25% y la pareja rumana 5 de las cuales gano el 40%. Calcular :Una fabrica de piezas para aviones esta organizada en tres secciones. La sección A fabrica el 30% de las piezas, la sección B el 35% , mientras que el resto se fabrican en la sección C. La probabilidad de encontrar una pieza defectuosa es del 0.01, 0.015 y 0.009 según se considere la sección A, B o C, respectivamenteSegún el informe anual La Sociedad de la información 2012, el 63% de los usuarios de móvil en España tiene un “Smartphone”. Entre los propietarios de este tipo de teléfono, el 77% lo emplea para su conexión habitual a internet. Sin embargo, entre los propietarios de otros tipos de teléfono móvil solo el 8% lo emplea para la conexión. Habitual a internet.Un moderno edificio tiene dos ascensores para uso de los vecinos. El primero de los ascensores es usado el 45%. De las ocasiones, mientras que el segundo es usado el resto de las ocasiones. El uso continuado de los ascensores provoca un 5% de fallos en el primero de los ascensores y un 8% en el segundo. Un día suena la alarma de uno de los ascensores porque ha fallado .Calcula la probabilidad de que haya sido el primero de los ascensores.En un hospital el 98% de los bebés nacen vivos. Por otro lado, el 40% de todos los partos son por cesárea y de ellos el 96% sobreviven al parto. Se elige al azar una mujer a la que no se va a practicar cesárea.¿Cuál es la probabilidad de que el bebé viva?Si un estudiante se registra en la asignatura A y la estudia, entonces una evaluación E resulta aprobada con probabilidad de 0.9.Si el estudiante no está interesado en estudiar la asignatura, la evaluación tal vez es aprobadacon probabilidad de 0.03. Se sabe que la asignatura es compleja, por lo cual la probabilidad de estudiar la asignatura es de 0.01.¿Cuál es la probabilidad de que un estudiante con una evaluación aprobada no haya estudiado la asignatura?",17/09/2018,0,0,0,"(700, 401)",8,3,0.0,0,es,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
167,Nailing the Tech Interview,,Insight,2200.0,7.0,1588,"Jessica Kirkpatrick is the Director of Data Science at InstaEDU, and formerly a data scientist on the analytics team at Yammer (Microsoft). Before that she was an Astrophysicist at UC Berkeley and has also been an Insight mentor since the program’s founding. Below is a guest post, originally appearing on the Women in Astronomy blog, where Jessica shares her tips on doing well in technical job interviews.A year ago, I made the transition from astrophysicist to data scientist. One of the harder parts of making the transition was convincing a tech company (during the interview process) that I could do the job. Having now been on both sides of the interview table, I’d like to share some advice to those wishing to break into the tech/data science industry. While this advice is applicable to candidates in general, I’m going to be gearing it towards applicants coming from academia / PhD programs.Most tech companies are interested in smart, talented people who can learn quickly and have good problem solving skills. We see academics as having these skills. Therefore, if you apply for internships or jobs at tech companies, you will most likely get a response from a recruiter. The problem is that once you get an interview, there are a lot of industry-specific skills that the company will try to assess, skills that you may or may not have already.Below are some of the traits we look for when recruiting for the Yammer analytics/data team, descriptions of how we try to determine if a candidate has these traits, and what you should do to ‘nail’ this aspect of the interview.1. Interest in the Position This sounds like a no-brainer, but you would be surprised at how many candidates haven’t done proper research about the company or the position. It is especially important for people coming from academic backgrounds to demonstrate why they are interested in making this transition and why they are specifically interested in this opportunity.When I ask a candidate “Why are you interested in joining my team?” I often get responses like “I really want to move to San Francisco” or “I’m sick of my research.” Neither of these responses demonstrate specific interest in my team or my company.How to Nail It: Do research about the position you are applying for. Understand what the role entails, the company’s goals and priorities, and the product(s) that you will be working on. Have a convincing story for why you are making this career change or why you want to leave your current position. Show enthusiasm for the opportunity — every interviewer should think that their position is your number one choice and that you can’t wait to join their team. More importantly, only apply for roles that you genuinely find interesting.2. Excellent Problem Solving Skills One of the most challenging aspects of the analyst/data scientist role is taking a vague question posed by someone within the company, and figuring out how to best answer it using our data sets. Testing (and demonstrating) this skill in an interview very difficult.At Yammer we try to test this skill by asking a combination of open-ended problems, brain teasers, and scenarios similar to those we deal with on a regular basis. For many of these questions there isn’t a right or wrong answer, we are more interested in the way the candidate constrains the problem, articulates her thought process, and how efficiently she gets to a solution. For some data science positions you will be asked to do coding problems. Familiarize yourself with some of the standard coding algorithms and questions.How to Nail It: These types of problems are asked by many tech companies and there are plenty of examples of them on the web. Practice constraining, coming up with a clear game plan, articulating that plan, and then following through in a methodical way. Many problems are hard to answer as posed and so trying simpler versions of the problem or looking at edge cases can give you insight into how to find patterns. Sometimes not all the relevant information is given by the interviewer, don’t be afraid to ask clarifying questions or turn the process into a discussion. If the interviewer tries to give you hints or tips, take them. There is nothing more frustrating (as an interviewer) than trying to guide a candidate back on track and have her ignore your help (it also doesn’t bode well for the interviewee’s ability to work well with others).3. Communication Skills As I said in a previous post, communication is key. We are looking for someone who can clearly articulate her thought process, and can be convincing that her approach is correct even when being challenged by the interviewer. A standard way we will test this is by posing an open-ended question and when the interviewee says a reasonable answer, we give a reason why that isn’t right, then the she comes up with a different explanation, and we negate it again. We keep going to see how she deals with having to switch directions, and balances defending her answer and being flexible with taking the interviewer’s suggestions.How to Nail It: Practice articulating your approach and methods for the above ‘technical’ interview questions. Come up with a big-picture game plan for approaching the problem, be clear about that plan, have a methodical approach, and then execute it — all the while articulating your thought process as much as possible. If the interviewer tries to make you change directions, it’s ok to defend your approach, but you don’t want to be too rigid, they might be trying to help you not go down the wrong path. Try to make the interaction as pleasant and warm as possible. Avoid getting defensive, frustrated, or just giving up. It is a very hard balance, but practice (especially with another person who can give you feedback) makes perfect.4. Culture Fit In tech companies you work collaboratively on projects on tight knit teams. We are going to be spending a lot of time with a candidate if we hire her; we want to enjoy that time together. Therefore we are also trying to assess if the interviewee would be a good coworker at an interpersonal level. Is she friendly? Does she work well on teams? Does she have the right balance of being opinionated but not domineering? Is she an interesting person? What are her passions and goals?I can’t tell you how many times I’ve asked a candidate “What do you like to do for fun?” and they answer: “I like to read programming books.” Is that really what you like to do for fun? Or do you just think that is what you are supposed to say in a tech interview?How to Nail It: Remember that your interviewer is a person too, and interact with them as a person. Try to show some of your personality, passion, sense of humor, and uniqueness in the interview. It’s hard to be relaxed in these situations, but personality goes a long way.5. Ask Good Questions At the end of the interview you will typically have a chance to ask questions. This is your time to take control of the process and turn the tables on the interviewer. Sometimes I learn the most about a candidate in how she uses this portion of the interview. A interviewee I am on the fence about can really tip the decision one way or another by asking intelligent, thought provoking, and engaging questions at the end of an interview (or boring, uninformed, or generic questions).How to Nail It: Use this as an opportunity to communicate things you weren’t able to show in other parts of the interview. Demonstrate that you have researched the company, that you understand their business goals and the way you could contribute. Ask thoughtful questions about the role, demonstrate that you want something that is challenging and discuss types of skills you want to learn or apply. Use this opportunity to show the interviewer what skills you can bring to the role. If applicable, try to relate what you are learning about the job/company to what you’ve done in the past. Prepare tons of questions, write them down ahead of time and bring them to the interview. You shouldn’t run out of questions or have to repeat them over the course of the day.The above is by no means an exhaustive list of everything a tech company is looking for, and of course different companies have different approaches. When I interviewed for my current job (I recently moved to the education start-up InstaEDU), most of the interview involved discussing my previous projects, the problems that the company was facing, and how I could provide value to them as a data scientist. It was a very different experience interviewing for my second job in the tech industry than for my first. However, I do hope that the above demystifies the tech interview process, gives you insight into how one company goes about hiring data people, and helps you understand what we are looking for on the other side of the table.Related Posts: Astronomer to Data Scientist Astronomy vs. Data ScienceInterested in transitioning to career in data science? Find out more about the Insight Data Science Fellows Program in New York and Silicon Valley, apply today, or sign up for program updates.Already a data scientist or engineer? Find out more about our Advanced Workshops for Data Professionals. Register for two-day workshops in Apache Spark and Data Visualization, or sign up for workshop updates.",10/10/2013,0,19,38,"(700, 467)",1,0,0.0,25,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
168,Introduction to Logistic Regression,Towards Data Science,Ayush Pant,660.0,5.0,676,"In this blog, we will discuss the basic concepts of Logistic Regression and what kind of problems can it help us to solve.Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. Logistic regression transforms its output using the logistic sigmoid function to return a probability value.Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.We can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the ‘Sigmoid function’ or also known as the ‘logistic function’ instead of a linear function.The hypothesis of logistic regression tends it to limit the cost function between 0 and 1. Therefore linear functions fail to represent it as it can have a value greater than 1 or less than 0 which is not possible as per the hypothesis of logistic regression.In order to map predicted values to probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.When using linear regression we used a formula of the hypothesis i.e.hΘ(x) = β₀ + β₁XFor logistic regression we are going to modify it a little bit i.e.σ(Z) = σ(β₀ + β₁X)We have expected that our hypothesis will give values between 0 and 1.Z = β₀ + β₁XhΘ(x) = sigmoid(Z)i.e. hΘ(x) = 1/(1 + e^-(β₀ + β₁X)We expect our classifier to give us a set of outputs or classes based on probability when we pass the inputs through a prediction function and returns a probability score between 0 and 1.For Example, We have 2 classes, let’s take them like cats and dogs(1 — dog , 0 — cats). We basically decide with a threshold value above which we classify values into Class 1 and of the value goes below the threshold then we classify it in Class 2.As shown in the above graph we have chosen the threshold as 0.5, if the prediction function returned a value of 0.7 then we would classify this observation as Class 1(DOG). If our prediction returned a value of 0.2 then we would classify the observation as Class 2(CAT).We learnt about the cost function J(θ) in the Linear regression, the cost function represents optimization objective i.e. we create a cost function and minimize it so that we can develop an accurate model with minimum error.If we try to use the cost function of the linear regression in ‘Logistic Regression’ then it would be of no use as it would end up being a non-convex function with many local minimums, in which it would be very difficult to minimize the cost value and find the global minimum.For logistic regression, the Cost function is defined as:−log(hθ(x)) if y = 1−log(1−hθ(x)) if y = 0The above two functions can be compressed into a single function i.e.Now the question arises, how do we reduce the cost value. Well, this can be done by using Gradient Descent. The main goal of Gradient descent is to minimize the cost value. i.e. min J(θ).Now to minimize our cost function we need to run the gradient descent function on each parameter i.e.Gradient descent has an analogy in which we have to imagine ourselves at the top of a mountain valley and left stranded and blindfolded, our objective is to reach the bottom of the hill. Feeling the slope of the terrain around you is what everyone would do. Well, this action is analogous to calculating the gradient descent, and taking a step is analogous to one iteration of the update to the parameters.In this blog, I have presented you with the basic concept of Logistic Regression. I hope this blog was helpful and would have motivated you enough to get interested in the topic.",22/01/2019,0,9,9,"(543, 263)",16,1,0.0,1,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,anger/irritation
169,Understanding BERT Transformer: Attention isn’t all you need,synapse_dev,Damien Sileo,109.0,9.0,1645,"BERT is a recent natural language processing model that has shown groundbreaking results in many tasks such as question answering, natural language inference and paraphrase detection. Since it is openly available, it has become popular in the research community.The following graph shows the evolution of scores for GLUE benchmark — the average of scores in various NLP evaluation tasks.While it’s not clear that all GLUE tasks are very meaningful, generic models based on an encoder named Transformer (Open-GPT, BERT and BigBird), closed the gap between task-dedicated models and human performance and within less than a year.However, as Yoav Goldberg notes it, we don’t fully undestand how the Transformer encodes sentences.[Transformers] in contrast to RNNs — relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.Several articles delve into the technicalities of BERT. Here, we will try to deliver some new insights and hypotheses that could explain BERT’s strong capabilities.The way humans are able to understand language has been a long-standing philosophical question. In the 20th century, two complementary principles shed light on this problem:Parsing hierarchical structures and deriving meaning from their components recursively until sentence level is reached is an appealing recipe for language understanding. Consider the sentence “Bart watched a squirrel with binoculars”. A good parsing component could yield the following parse tree:The meaning of the sentence could be derived from successive compositions (composing “a” and “squirrel”, “watched” with “a squirrel”, “watched a squirrel” and “with binoculars”) until the sentence meaning is obtained.Vector spaces (as in word embeddings) can be used to represent words, phrases, and other constituents. Composition could be framed as a function f which would compose (“a”,”squirrel”) into a meaningful vector representation of “a squirrel” = f(“a”,”squirrel”). [Baroni 2014]However, composition and parsing are both hard tasks, and they need one another.Obviously, composition relies on the result of parsing to determine what ought to be composed. But even with the right inputs, composition is a difficult problem. For example, the meaning of adjectives changes depending on the word they characterize: the color of “white wine” is actually yellow-ish while a white cat is actually rather white. This phenomenon is known as co-composition. [Pustejovsky 2017]A broader context can also be necessary for composition. For instance, the way the words in “green light” should be composed depends on the situation. A green light can denote an authorization or an actual green light. The meaning of some idiomatic expressions requires a form of memorization rather than composition per se. Thus, performing those compositions in the vector space requires powerful nonlinear functions like a deep neural network (that can also memorize [Arpit 2017]).Conversely, the parsing operation arguably needs composition in order to work in some cases. Consider the following parse tree of the same previous sentence “Bart watched a squirrel with binoculars”.While it is syntactically valid, this parse leads to an odd interpretation of the sentence where Bart watches (with his bare eyes) a squirrel holding binoculars. However, some form of composition must be used in order to figure out that a squirrel holding binoculars is an unlikely event.More generally, many disambiguations and integrations of background knowledge have to go on before the appropriate structures are derived. But this derivation might also be achieved with some forms of parsing and composition.Several models have tried to put the combination of parsing and composition in practice [Socher 2013], however they relied on a restrictive setup with manually annotated standard parse trees, and have been outperformed by much simpler models.We hypothesize that Transformers rely heavily on these two operations, in an innovative way: since composition needs parsing, and parsing needs composition, Transformers use an iterative process, with successive parsing and composition steps , in order to solve the interdependence problem. Indeed, Transformers are made of several stacked layers (also called blocks). Each block consists of an attention layer followed by a non-linear function applied at each token.We will try to highlight the link between those components and the parsing/composition framework.In BERT, an attention mechanism lets each token from the input sequence (e.g. sentences made of word or subwords tokens) focus on any other token.For illustration purposes, we use the visualization tool from this article to delve into the attention heads and test our hypothesis on the pre-trained BERT base uncased model. In the following illustration of an attention head, the word “it” attends to every other token and seems to focus on “street” and “animal”.BERT uses 12 separate attention mechanism for each layer. Therefore, at each layer, each token can focus on 12 distinct aspects of other tokens. Since Transformers use many distinct attention heads (12*12=144 for the base BERT model), each head can focus on a different kind of constituent combinations.We ignored the values of attention related to the “[CLS]” and “[SEP]” token. We tried using several sentences, and it’s hard not to overinterpret results, so you can feel free to test our hypothesis on this colab notebook with different sentences. Please note that in the figures, the left sequence attends to the right sequence.In the second layer, attention head #1 seems to form constituents based on relatedness.More interestingly, in the third layer, head #9 seem to show higher level constituents : some tokens attend to the same central words (if, keep, have)In the fifth layer, the matching performed by the attention head #6 seem to focus on specific combinations, notably involving verbs. The special tokens like [SEP] seem to be used to indicate the absence of matching. This could allow attention heads to detect specific structures where a composition would be appropriate. Such consistent structures could be fed to the composition function.Arbitrary trees could have been represented with successive layers of shallow parsing, as shown in the next figure:We didn’t find such clear cut tree structures upon examination of BERT attention heads, but still, it is possible for Transformers to represent them. We note that, since the encoding is performed simultaneously on all layers, it is hard to correctly interpret what BERT is doing. The analysis of a given layer only makes sense with respect to the next and previous layers. The parsing is also distributed across attention heads.The following illustration shows what BERT attention more realistically looks like for two attention heads:However, as we have seen previously, the parse tree is a high level representation, and it might build upon more complex “rhizomatic” [Deleuze 1987] structures . For instance, we might need to find out what pronouns refer to in order to encode the input (coreference resolution). In other cases, a global context can also be required for disambiguation.Surprisingly, we discovered an attention head (layer 6 head #0) that seems to actually perform coreference resolution. And, as noted here, some attention heads seem to feed a global context to each word (layer 0 head #0).In each layer, the outputs of all attention heads are concatenated and fed to a neural network that can represent complex nonlinear functions (needed for an expressive composition).Relying on structured input from the attention heads, this neural network could perform various compositions. In the previously shown layer 5, the attention head #6 could guide the model to perform the following compositions: (we, have), (if, we), (keep, up) (get, angry). The model could combine them non-linearly and return a composed representation. Thus, the many attention heads can be used as tools that pave the way for composition.While we didn’t find attention heads focusing on more consistent combinations such as adjective/nouns, there might be some common grounds between verb/adverb composition and other compositions that the model leverages.There are many possible relevant compositions (words-subwords, adjective-noun, verb-preposition, clause-clause). To go even further, we can see disambiguation as a composition of an ambiguous word (e.g. bank) with its relevant context words (e.g. river or cashier). Integration of background common sense knowledge related to concept given a context could also be performed during a composition phase. Such disambiguation could also occur at other levels (e.g. sentence-level, clause-level).Besides, the composition might also be involved in word order reasoning. It has been argued that the positional encoding might not be sufficient to encode order of the words correctly. However, the positional encoding is designed to encode coarse, fine, and possibly exact positions of each token. (The positional encoding is a vector that is averaged with the input embedding in order to yield a position-aware representation of each token in the input sequence). Therefore, based on two positional encodings, the non-linear composition could theoretically perform some relational reasoning based on word relative positions.We hypothesize that the composition phase also does the heavy lifting in BERT natural language understanding: Attention isn’t all you need.We proposed an insight on the inductive bias of Transformers. However, we have to keep in mind that our interpretation might be optimistic regarding the capabilities of Transformer. As a reminder, LSTM were shown to be able to deal implicitly with tree structures [Bowman 2015] and composition [Tai 2015]. But LSTM has some limitations, some being due to vanishing gradient [Hochreiter 1998]. Thus, further work is needed to shed light on the limitations of the transformer.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Devlin 2018]Attention Is All You Need [Vaswani 2017]Assessing BERT’s Syntactic Abilities [Goldberg 2019]Compositionality [Szabó 2017]Frege in Space [Baroi, 2014]Co-compositionality in Grammar [Pustejovsky 2017]A Closer Look at Memorization in Deep Networks [Arpit 2017]Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank [Socher 2013]Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks [Tai 2015]Tree-structured composition in neural networks without tree-structured architectures [Bowman 2015]A Thousand Plateaus [Deleuze 1987]The vanishing gradient problem during learning recurrent neural nets and problem solutions [Hochreiter 1998]Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention [Vig 2019]",26/02/2019,0,0,43,"(464, 441)",16,1,0.0,30,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,trust/acceptance
170,How to Run Text Summarization with TensorFlow,,Pavel Surmenok,2500.0,6.0,1167,"Text summarization problem has many useful applications. If you run a website, you can create titles and short summaries for user generated content. If you want to read a lot of articles and don’t have time to do that, your virtual assistant can summarize main points from these articles for you.It is not an easy problem to solve. There are multiple approaches, including various supervised and unsupervised algorithms. Some algorithms rank the importance of sentences within the text and then construct a summary out of important sentences, others are end-to-end generative models.End-to-end machine learning algorithms are interesting to try. After all, end-to-end algorithms demonstrate good results in other areas, like image recognition, speech recognition, language translation, and even question-answering.In August 2016, Peter Liu and Xin Pan, software engineers on Google Brain Team, published a blog post “Text summarization with TensorFlow”. Their algorithm is extracting interesting parts of the text and create a summary by using these parts of the text and allow for rephrasings to make summary more grammatically correct. This approach is called abstractive summarization.Peter and Xin trained a text summarization model to produce headlines for news articles, using Annotated English Gigaword, a dataset often used in summarization research. The dataset contains about 10 million documents. The model was trained end-to-end with a deep learning technique called sequence-to-sequence learning.Code for training and testing the model is included into TensorFlow Models GitHub repository. The core model is a sequence-to-sequence model with attention. When training, the model is using the first two sentences from the article as an input and generates a headline.When decoding, the algorithm is using beam search to find the best headline from candidate headlines generated by the model.GitHub repository doesn’t include a trained model. The dataset is not publicly available, a license costs $6000 for organizations which are not members of Linguistic Data Consortium. But they include a toy dataset which is enough to run the code.You will need TensorFlow and Bazel as prerequisites for training the model.The toy dataset included into the repository, contains two files in “data” directory: “data” and “vocab”. The first one contains a sequence of serialized tensorflow.core.example.example_pb2.Example objects. An example of code to create a file with this format:“vocab” file is a text file with the frequency of words in a vocabulary. Each line contains a word, space character and number of occurrences of that word in the dataset. The list is being used to vectorize texts.Running the code on toy dataset is really simple. Readme on GitHub repo lists a sequence of commands to run training and testing code.You can run TensorBoard to monitor training process:When running “decode” code, note that it will loop over the entire dataset indefinitely, so you will have to stop execution manually at some point. You can find results of decoding in log_root/decode folder. It will contain a few files, some of them have prefix “ref”, they contain original headlines from the test set. Other files have prefix “decode”, they contain headlines generated by the model.You can encounter an error when running “eval” or “decode” code using TensorFlow 0.10 or later:“ValueError: Could not flatten dictionary. Key had 2 elements, but value had 1 elements.”There is an open issue on GitHub for this error. One workaround is to downgrade TensorFlow to 0.9, it worked for me. Another workaround requires changing the code of the model: adding “state_is_tuple=False” to instantiations of LSTMCell in seq2seq_attention_model.py.If you run training and decoding on toy dataset, you will notice that decoding generates nonsense. Here are few examples of headlines generated:<UNK> to <UNK> <UNK> <UNK> <UNK> <UNK> .<UNK> <UNK> <UNK> <UNK> of <UNK> <UNK> from <UNK> <UNK> .in in <UNK> <UNK> <UNK> .One of the reasons for poor performance on the toy set could be incompleteness of the vocabulary file. Vocabulary file is truncated and doesn’t contain many of the words which are used in the “data” file. It leads to too many “<UNK>” tokens which represent unknown words.A toy dataset is, well, a toy. To create a useful model you should train it on a large dataset. Ideally, the dataset should be specific for your task. Summarizing news article may be different from summarizing legal documents or job descriptions.As I don’t have access to GigaWord dataset, I tried to train the model on smaller news article datasets, which are free: CNN and DailyMail. I found the code to download these datasets in DeepMind/rcdata GitHub repo, and slightly modified it to add the title of the article in the first line of each output file. See modified code here.92570 articles in CNN dataset, and 219503 articles in Daily Mail dataset. It could be a few more articles, but the code from DeepMind repo could not download all URLs. 322k articles are way fewer than 10 million articles in GigaWord, so I would expect a lower performance of the model if training on these datasets.After you run the code to download the dataset you will have a folder with lots of files, one HTML file for every article. To use it in TextSum model you will need to convert it to the binary format described above. You can find my code to convert CNN/DailyMail articles into binary format in textsum_data_convert.py file in my “TextSum” repo on GitHub. An example of running the code for CNN dataset:Then you can copy train/validation/test sets and vocabulary files into “data” directory and start training the model:Training with default parameters doesn’t go very well. Here is a graph of running_avg_loss:Decoding results are also disappointing:“your your <UNK>”“We’ll the <UNK>”“snow hit hit hit <UNK>”Either dataset is too small, or hyperparameters need to be changed for this dataset.When running the code I found that training code doesn’t use GPU, though I have all the correct configuration: GeForce 980Ti, CUDA, CuDNN, TensorFlow compiled with using GPU. While training, python.exe consumes 100–300+% CPU, and it appears in the list of processes when running nvidia-smi, but GPU utilization stays 0%.I guess it can be related to the fact that authors of the model were running the code using multiple GPUs, and one GPU had some special purpose. A fragment of seq2seq_attention_model.py file:The decoding code uses GPU quite well. It consumes almost all 6Gb of GPU memory and keeps utilization over 50%.Using the code from this article you can easily run text summarization model on your own dataset. Let me know if you find something interesting!If you happen to have a license for the GigaWord dataset, I will be happy if you share trained TensorFlow model with me. I would like to try it on some proprietary data, not from news articles.Do you use any other text summarization algorithms? What works the best?The article was originally published on http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!",16/10/2016,0,0,1,"(700, 191)",3,0,0.0,21,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
171,BERT Explained: State of the art language model for NLP,Towards Data Science,Rani Horev,1500.0,7.0,1039,"BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper’s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.In the field of computer vision, researchers have repeatedly shown the value of transfer learning — pre-training a neural network model on a known task, for instance ImageNet, and then performing fine-tuning — using the trained neural network as the basis of a new purpose-specific model. In recent years, researchers have been showing that a similar technique can be useful in many natural language tasks.A different approach, which is also popular in NLP tasks and exemplified in the recent ELMo paper, is feature-based training. In this approach, a pre-trained neural network produces word embeddings which are then used as features in NLP models.BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. The detailed workings of Transformer are described in a paper by Google.As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).The chart below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size H, in which each vector corresponds to an input token with the same index.When training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence (e.g. “The child came home from ___”), a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness (see Takeaways #3).Note: In practice, the BERT implementation is slightly more elaborate and doesn’t replace all of the 15% masked words. See Appendix A for additional information.In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:To predict if the second sentence is indeed connected to the first, the following steps are performed:When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.Using BERT for a specific task is relatively straightforward:BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:In the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section 3.5) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks, detailed in Section 4 of the paper.BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it’s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this summary, we attempted to describe the main ideas of the paper while not drowning in excessive technical details. For those wishing for a deeper dive, we highly recommend reading the full article and ancillary articles referenced in it. Another useful reference is the BERT source code and models, which cover 103 languages and were generously released as open source by the research team.Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows — 80% are replaced with a “[MASK]” token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):No ablation was done on the ratios of this approach, and it may have worked better with different ratios. In addition, the model performance wasn’t tested with simply masking 100% of the selected tokens.For more summaries on the recent Machine Learning research, check out Lyrn.AI.",10/11/2018,0,0,12,"(700, 321)",4,6,0.0,8,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,expectation/interest
172,Thoughtflow.io: connecting people through sentence sentiment clustering,Mindsoft,Viktor Tóth,90.0,9.0,2090,"To present the idea and implementation of Thoughtflow, I start off by asking the following. What is common between an Overwatch server outage rage forum, a terrorist nest, a Bruce Lee quote enthusiasts gathering place, a puppy death cry-over, a chat group full of people who just farted, an ad hoc hiking trip or swingers party organizing interface and whatever you can imagine where resonating thoughts meet and empathy flourishes? Let me cut the BS and get to the answer.Thoughtflow is designed to extract sentiment (meaning) out of sentences and group them according to similarity. Think of a search bar, you type in your thought as a sentence and you get grouped up with others who typed in analogous sentences.One level deeper: Thoughtflow uses machine intelligence to turn human readable text to a numerical vector, which is then used to address a high dimensional space. In this metric space, distances between vectors can be defined and used to tie close ones together, hence to establish groups of related sentences — that is, cluster them.So, how could you turn a sentence into a vector, while holding onto the meaning? Or a (simpler) alternative, how could you turn a word into a vector in the same manner? That’s what word2vec models are for! Here’s how they work.“You shall know a word by the company it keeps” — said John Rupert Firth an influential linguist from the 50’s. This idea gives basis to the majority of word2vec models today. If we believe that the meaning of a word is implicitly given in the contexts it keeps, then one solution to turn the word to a vector would be to keep a frequency counter for each possible context, and increase that counter if the word is present [1]. Then by concatenating these frequency counts, we can obtain a vector that represents the word — and in the same way any other word can be encoded, obviously having somewhat different frequency counts. As an example let our word2vec model learn 2 words on the following corpus: I LOVE TREES. I LOVE PEOPLE. I HATE KALE. I HATE TRUMP.These are the frequency word vectors we capture in the end, if we only construct contexts out of words that are 1 distance away. As you can imagine the number of contexts could go up pretty high (given the corpus), especially if we increase the size of contexts (surrounding 4, 6… words). Nevertheless, in the long run, it can be seen that similar words would be represented by similar vectors, taken Firth’s theory. I had a project to build such representation of words from scratch in C++, btw.You can imagine the same working on sentences, right? Just take a sentence and its context — the surrounding sentences — and build a frequency vector. Now, the problem you might encounter is that the number of contexts just explodes exponentially, as more sentences are taught to the model. The space of sensible sentences is huge, not to mention the space of contexts (set of sensible sentences). It requires unfeasible amount of resources just to be able to keep track of the contexts. That’s for sure not how the human brain understands texts. Aaaand that’s where autoencoder models come into picture. Let’s jump straight into an example:3 girls play. The first tells the second an arbitrary long story without the third hearing any of it. The second has to compress the story to 10 words, and forward it to the third. The third is then tells the story she thinks the first girl came up with. We then deliver punishment to the third, some of which she relays (backpropagate) to the second — the first is not part of the learning model, she just tells random stories.If the story is far from what was initially said by the first, the punishment to share is proportionally large. If the mistake is negligible, or the story is perfectly transferred, little or no punishment is given. No dolls, no candy ever; an AI’s life is hard. So let’s imagine we play this game for a while. We expect the girls / the layers of neurons to learn how to compress stories to 10 words, or to a given length vector. The magic happens in how the punishment is given, how the task is described.What if we instruct them to take a sentence as input, and give the surrounding sentences, the context, as an output. Then, the second would try to encode the sentence she gets from the first, so the created representation contains as much context information as possible; without actually hearing the context. In turn, the third can come up with the surrounding sentences. Playing this game long enough, the girls would be capable of representing any sentence in a concise format that still contains (back to Firth) the sentiment of the original input sentence.Let’s translate the girls’ game to autoencoders: the second girl is the encoder, the third is the decoder. The first girl is the one who takes the human readable text and turns it into a sequence of word vectors — similar word vector that’s been described above — and forward it.. or actually whatever, you can imagine any arbitrary analogy for the first girl; I just needed three to entertain my Powerpuff obsession.At the heart of Thoughtflow, the so called Skip-Thought model lies [2]. It works similarly to how I described the latter game — word vector representations are fed to a GRU [3] Recurrent Neural Network (the encoder), the output of which is then propagated to the decoder unit. The decoder then builds the context word-by-word, vector-by-vector. The context is then checked against the desired context and the produced error is backpropagated through the autoencoder. After training, the output of the encoder unit is used to represent sentences.Finally, we know how to express sentences as numerical vectors. What’s left is to cluster them into groups of analogous sentences. First of all, a distance metric has to be defined between vectors. I chose cosine similarity in the end, as it is suitable for high dimensional spaces and empirically works well with word embeddings; also I manually tested and compared it with Euclidean distance and cosine yielded more sensible groups.There are lots of different flavors of clustering methods. To pick the most fitting for our problem, it’s effective to start off by grouping these methods according to the input parameters they require.Centroid-based algorithms — like k-means — require the number of clusters (k) to be provided prior to clustering — essentially, the number of groups that is expected to emerge from the thoughts. This is not given and quite difficult even to approximate. Clustering for each possible k and then choosing the best one, would spend too much resource — e.g. k-means is O(n²). It also assumes the clusters to be about similar size, which is definitely not true for Thoughtflow.Distribution-based clustering algorithms also require the number of distributions to capture clusters in the data and assumes Gaussian distribution of each cluster, which is too strict for our problem.Hierarchical clustering methods would be just too slow O(n² log n) for large amount of sentences, though the parameters it needs can be given in our case.After mercilessly excluding a variety of methods, I found density-based clustering the most fitting for the cause and chose DBSCAN in the end. This algorithm only takes the maximum distance between neighboring elements (ε) and the minimum amount of neighboring elements that makes up a cluster. The latter — number of sentences needed for a group — is determined easily and can be kept as a constant. To figure out the optimal ε, I use a simple global optimization algorithm — simulated annealing — and I optimize on the Silhouette score of the clustering result. This means that I run DBSCAN over and over again until the optimal ε value is found. Now you ask, yeye, you were whining about running k-means a couple of times, now you’re gonna run DBSCAN probably thousands of times. Yeah, but there’s a difference: after a sparse distance matrix (dᵢ,ⱼ ; i, j= 1, 2, ...,|Sentences|), e.g. CSR, is computed only for neighboring sentence pairs — dᵢ,ⱼ ≤ ε — the complexity of running DBSCAN on that distance matrix is linear. Constructing such a distance matrix can be done in O(n log n), and then a couple thousand runs of DBSCAN are achieved fast. Not to mention that DBSCAN finely yields itself to the MapReduce programming model [4], which makes it a good candidate for parallelization.That’s it. At least the theory. I’ve implemented all these mentioned above, and packaged it in a web application, which is run on the Google Cloud Platform. Check it out — thoughtflow.io [inactive until further interest]! It might not have enough users, or it might still be a bit slow. It is in alpha version as of March 2017.On the index page, an input field is waiting for a sentence to be typed in. The given sentence is immediately turned into a 4800 long numerical vector, which is then saved in the database. Clustering of all live sentences is repeated in a every 30 seconds. This time interval is going to be lowered as soon as I’m able to convert DBSCAN to Google Dataflow, which should finely scale up to millions of sentences. So, after you’ve got your sentence in the system and the first clustering happens, you are either redirected to a group or you stay on the loading screen in case there are not enough sentences having similar semantic meaning to yours.Whichever happens, your thought is accessible on the index page, typically within 1 hour of its creation. So in case you’re tired of the loading page and its animation — seriously, check it out, I worked a whole night on it :D — you can just keep going back to the index page and type in another sentence, until you find a group. A thought is alive for 1 hour after the last interaction performed on it. Once in the group, you can only post, comment, or vote, at the moment. As a cumulative effect of this rule, the user has higher chance to meet other active users in groups. By lowering the lifetime of sentences, this chance can be further increased.btw, going back to the intro, this platform would not function quite well for terrorist organizations, as all the group conversations are public and accessible.I anticipate that however great the Skip-Thought model is — it was taught on more than 11 thousand books [5] -, it has to be updated continuously, as it should be able to represent sentences containing recently developed expressions — e.g. video-game names. I haven’t mentioned it previously, but I’ve also implemented vocabulary expansion of Skip-Thought — I trained a transformation matrix through simple linear regression in order to turn word vectors from the spaCy representation to Skip-Thought word vectors. Now, I can just learn new word representations in spaCy and use them in Skip-Thought. Apart from this “hacky” solution, I plan to keep on capturing texts from the wild internet and feed them to the Skip-Thought model in a streaming fashion. Time tells if the model is effective enough to bear that amount of knowledge and variety.In the long term, I envision grouping of brain signals instead of sentences. Starting by capturing EEG features of emotional information — both handcrafted features, like Frontal Alpha Asymmetry [6] and generated ones through Sparse Coding [7]. Just imagine, people with similar brain patterns matched up in a group.[1] O. Levy and Y. Goldberg, “Linguistic Regularities in Sparse and Explicit Word Representations,” Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 171–180, 2014.[2] R. Kiros et al., “Skip-Thought Vectors,” arXiv:1506.06726 [cs], Jun. 2015.[3] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. NIPS Deep Learning Workshop, 2014.[4] “A new scalable parallel DBSCAN algorithm using the disjoint-set data structure,” ResearchGate. [Online]. Available: https://www.researchgate.net/publication/261212964_A_new_scalable_parallel_DBSCAN_algorithm_using_the_disjoint-set_data_structure.[5] Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler. “Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books.” arXiv preprint arXiv:1506.06724 (2015).[6] J. A. Coan and J. J. B. Allen, “Frontal EEG asymmetry as a moderator and mediator of emotion,” Biol Psychol, vol. 67, no. 1–2, pp. 7–49, Oct. 2004.[7] Q. Barthélemy, C. Gouy-Pailler, Y. Isaac, A. Souloumiac, A. Larue, and J. I. Mars, “Multivariate Temporal Dictionary Learning for EEG,” arXiv:1303.0742 [cs, q-bio, stat], Mar. 2013.",22/03/2017,1,0,14,"(529, 349)",3,0,0.0,15,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
173,Humans open your AIs!,Reply U / pop2tech,Reply U,177.0,1.0,139,"Knock knock. There’s a new hype in Serial-Town and its name is Human.Warm up the sofa, after meth and zombies AMC took on the lofty topic of artificial intelligence. The result? A solid thriller that offers emotional intrigue and thought-provoking suspense. A must-seen for sci-fi fans that remains accessible also for agnostics. Please, get a bite:Do you want a Synth to help you getting rid of all the housework too?Few days ago a Silicon Valley supergroup has united to keep AI open, believing AI should be an extension of individual human wills and as broadly and evenly distributed as possible. OpenAi seeks to develop the technology for the general benefit of human, focusing on a positive impact and the pursuit of generalize intelligence.Or is it perhaps an organisation to keep us safe from Terminators and Cylons? Read more here.",15/12/2015,0,3,4,"(700, 315)",1,0,0.0,1,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
174,Deep Learning — Different Types of Autoencoders,DataDrivenInvestor,Renu Khandelwal,3900.0,6.0,120,"In this post we will understand different types of AutoencodersRead here to understand what is Autoencoder, how does Autoencoder work and where are they used.Just quick brief on what is AutoencodersAutoencoders encodes the input values x using a function f. Then decodes the encoded values f(x) using a function g to create output values identical to the input values.Autoencoder objective is to minimize reconstruction error between the input and output. This helps autoencoders to learn important features present in the data. When a representation allows a good reconstruction of its input then it has retained much of the information present in the input.References:Deep learning by Ian Goodfellow and Yoshua Bengio and Aaron Courvillehttp://www.icml-2011.org/papers/455_icmlpaper.pdfhttp://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdfAlso published on mc.ai on December 2, 2018.www.datadriveninvestor.comwww.datadriveninvestor.com",02/12/2018,0,6,16,"(469, 320)",13,10,0.0,9,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,surprise/amazement
175,Named-Entity Recognition (NER) using Keras Bidirectional LSTM,Towards Data Science,Snehal Nair,99.0,7.0,805,"Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.In this project, we will work with a NER dataset provided by Kaggle. The dataset can be accessed here. This dataset is the extract from the GMB corpus, which is tagged, annotated, and built specifically to train the classifier to predict named entities such as name, location, etc. Dataset also includes one additional feature (POS) that can be used in classification. In this project, however, we are working only with one feature sentence.Let us begin by loading and visualizing the dataset. To download ner_dataset.csv, go to this link in Kaggle.We will have to use encoding = ‘unicode_escape’ while loading the data. This function takes a parameter to toggle the wrapping quotes’ addition and escaping that quote’s quote in a string.Note that the sentences in the dataset are tokenized in the column “Word”. The column “sentence #” displays the sentence number once and then prints NaN until the next sentence begins. The ‘Tag’ column will be our label (y).To train a neural network, we will use two mappings as given below.This step is desired in any machine learning model, including the neural-network, which requires integers as input.We can see that the function has added two new index columns for our X (Word_idx) and y (Tag_idx) variables. Next, let us collect tokens into arrays in the respective sequence to make the recurrent neural network’s best use.To transform columns into sequential arrays, we willPadding: The LSTM layers accept sequences of the same length only. Therefore, every sentence represented as integers (‘Word_idx’) must be padded to have the same length. We will work with the max length of the longest sequence and pad the shorter sequences to achieve this.Please note we can use shorter padding lengths as well. In that case, you will be padding the shorter sequences and truncating the longer sequences.We will also be converting the y variable as one-hot encoded vector using the to_categorical function in Keras. Let us import the required packages.Neural network models work with graphical structure. Therefore we will first need to design the architecture and set input and out dimensions for every layer. RNNs are capable of handling different input and output combinations. We will use many to many architectures for this ask. Refer to the last architecture in the image given below. Our task is to output tag (y) for a token (X) ingested at each time step.Lets begin by loading, the required packages.Its always best to set seed for reproducibility.To build a better understanding of the layers’ input and output dimensions, compare the layers in the model plot with the brief on each layer given in the image (model-plot) below.In this architecture, we are primarily working with three layers (embedding, bi-lstm, lstm layers) and the 4th layer, which is TimeDistributed Dense layer, to output the result. We will discuss the layers in detail in the below sections.The dimensions (?, 104, 64) that we see in the given neural network plot’s embedding layer comes from these specifications. Please note the 1st dimension shows ‘?’ or None in the plots. They represent the batch sizes. If it is not provided, they show ‘?’ or None, which means the model can take any batch size.Since this is bidirectional lstm, we will have forward and backward outputs. Combine these outputs before passing ti to the next layer by summing or taking average or concatenating or multiplying. Find these functions in the merge mode argument in bi-lstm layer. The outputs are concatenated in the default mode, which in turn doubles the number of outputs to the next layer. In our case, it becomes 128(64 * 2).This layer takes the output dimension from the previous bidirectional lstm layer (?, 104, 128) and outputs (?, 104, 256)This layer takes the output dimension of the previous lstm layer (104, 256) and outputs the max sequence length (104) and max tags (17).Given below is the code used to build the model architecture we discussed above. Before fitting the model, check the model graph by plotting the model using plot_model function or run model.summary() for the model summary.We will fit the model with a for loop to save and visualize the loss at every epoch.The model started with 0.9169 accuracy ended. After running 25 epochs with 1000 batch size, the final accuracy was 0.9687.Please experiment with the model with different batch sizes, dropout value, optimizers, metrics, and layers to a better result. Find the complete code for this project in the Github Repository.For anyone interested, here is a piece of code that will highlight the entities in any given sentence using spaCy.References:github.commachinelearningmastery.com",28/06/2020,9,15,2,"(563, 216)",9,6,0.0,7,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
176,RNN vs GRU vs LSTM,Analytics Vidhya,Hemanth Pedamallu,18.0,7.0,1227,"In this post, I will make you go through the theory of RNN, GRU and LSTM first and then I will show you how to implement and use them with code.There are already many posts on these topics out there. But in this post, I wanted to provide a much better understanding and comparison with help of code.Let’s start with RNN!Recurrent Neural Networks (RNN) are designed to work with sequential data. Sequential data(can be time-series) can be in form of text, audio, video etc.RNN uses the previous information in the sequence to produce the current output. To understand this better I’m taking an example sentence.“My class is the best class.”At the time(T0 ), the first step is to feed the word “My” into the network. the RNN produces an output.At the time(T1 ), then at the next step we feed the word “class” and the activation value from the previous step. Now the RNN has information of both words “My” and “class”.And this process goes until all words in the sentence are given input. You can see the animation below to visualize and understand.At the last step, the RNN has information about all the previous words.Note: In RNN weights and bias for all the nodes in the layer are same.Let’s look at the architecture of the RNN unit. It takes input from the previous step and current input. Here tanh is the activation function, instead of tanh you can use other activation function as well.💡RNN’s face short-term memory problem. It is caused due to vanishing gradient problem. As RNN processes more steps it suffers from vanishing gradient more than other neural network architectures.Q. What is vanishing gradient problem?Ans: In RNN to train the network you backpropagate through time, at each step the gradient is calculated. The gradient is used to update weights in the network. If the effect of the previous layer on the current layer is small then the gradient value will be small and vice-versa. If the gradient of the previous layer is smaller then the gradient of the current layer will be even smaller. This makes the gradients exponentially shrink down as we backpropagate. Smaller gradient means it will not affect the weight updation. Due to this, the network does not learn the effect of earlier inputs. Thus, causing the short-term memory problem.The main problem is that it’s too difficult for RNN to learn to preserve information over many timesteps. In vanilla RNN the hidden state is constently being rewritten.How about an RNN with a separate memory?Solution for vanishing gradientTo overcome this problem two specialised versions of RNN were created. They are 1) GRU(Gated Recurrent Unit) 2) LSTM(Long Short Term Memory). Suppose there are 2 sentences. Sentence one is “My cat is …… she was ill.”, the second one is “The cats ….. they were ill.” At the ending of the sentence, if we need to predict the word “was” / “were” the network has to remember the starting word “cat”/”cats”. So, LSTM’s and GRU’s make use of memory cell to store the activation value of previous words in the long sequences. Now the concept of gates come into the picture. Gates are used for controlling the flow of information in the network. Gates are capable of learning which inputs in the sequence are important and store their information in the memory unit. They can pass the information in long sequences and use them to make predictions.The workflow of GRU is same as RNN but the difference is in the operations inside the GRU unit. Let’s see the architecture of it.Inside GRU it has two gates 1)reset gate 2)update gateGates are nothing but neural networks, each gate has its own weights and biases(but don’t forget that weights and bias for all nodes in one layer are same).Update gateUpdate gate decides if the cell state should be updated with the candidate state(current activation value)or not.Reset gateThe reset gate is used to decide whether the previous cell state is important or not. Sometimes the reset gate is not used in simple GRU.Candidate cellIt is just simply the same as the hidden state(activation) of RNN.Final cell stateThe final cell state is dependent on the update gate. It may or may not be updated with candidate state. Remove some content from last cell state, and write some new cell content.In GRU the final cell state is directly passing as the activation to the next cell.In GRU,Now you know about RNN and GRU, so let’s quickly understand how LSTM works in brief. LSTMs are pretty much similar to GRU’s, they are also intended to solve the vanishing gradient problem. Additional to GRU here there are 2 more gates 1)forget gate 2)output gate.First, look at the architecture of it.Now, look at the operations inside it.From GRU, you already know about all other operations except forget gate and output gate.All 3 gates(input gate, output gate, forget gate) use sigmoid as activation function so all gate values are between 0 and 1.Forget gateIt controls what is kept vs forgotten, from previous cell state. In laymen terms, it will decide how much information from the previous state should be kept and forget remaining.Output gateIt controls which parts of the cell are output to the hidden state. It will determine what the next hidden state will be.Phew! That's enough theory, now let us start coding.I’m taking airline passengers dataset and provide the performance of all 3 (RNN, GRU, LSTM) models on the dataset.My motive is to make you understand and know how to implement these models on any dataset. To make it simple, I’m not focusing on the number of neurons in the hidden layer or number of layers in the network( You can play with these to get better accuracy).About dataset:The dataset provides a record of the number of people travelling in US airlines in a particular month. It has a record of 142 months. It has 2 columns “Month” and “No. of Passengers”. But in this case, I want to use univariate dataset. Only “No. of Passengers” is used.Importing all the necessary libraries and dataset.To visualize the dataset, plt.plot(dataset)It shows that the number of passengers is linearly increasing over the months.Machine learning model/ Neural network works better if all the data is scaled.Divide the data for training and testing. I split the dataset into (75% training and 25% testing). In the dataset, we can estimate the ‘i’th value based on the ‘i-1’th value. You can also increase the length of the input sequence by taking i-1,i-2,i-3… to predict ‘i’th value.I made the sequential model with only 2 layers. Layers:In this code, I’m using LSTM. You can also use the other two just by replacing “LSTM” with “SimpleRNN”/”GRU” in the below code(line 2).In the LSTM layer, I used 5 neurons and it is the first layer (hidden layer) of the neural network, so the input_shape is the shape of the input which we will pass.Now, the model is ready. So start training the model.Finally, to visualize the real values and predicted result.For this dataset and with the simple network by using 50 epochs I got the following mean_squared_error values.Simple RNN: 3000GRU: 2584LSTM: 2657After learning about these 3 models, we can say that RNN’s perform well for sequence data but has short-term memory problem(for long sequences). It doesn’t mean to use GRU/LSTM always. Simple RNN has it’s own advantages (faster training, computationally less expensive).",14/11/2020,7,32,19,"(608, 344)",10,2,0.0,0,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
177,Step by Step TensorFlow Object Detection API Tutorial — Part 4: Training the Model,,Daniel Stang,696.0,4.0,704,"At this point in the tutorial you have selected a pre-trained model, found an existing dataset or created your own, and converted it into a TFRecord file. You are now ready to train your model.If you have previous transfer learning experience you likely have a question that has been lingering since part 2 of this tutorial. That question is, how do I modify the pre-trained model that was designed to work on the 90 classes of the COCO dataset, to work on the X number of classes of my new dataset? To accomplish this before the object detection API, you would have to remove the last 90 neuron classification layer of the network and replace it with a new layer. An example of this in TensorFlow is shown below.To accomplish this with the object detection API, all you need to do is modify one line in the models config file. Where you cloned the TensorFlow models repository, navigate to object_detection/samples/configs. In this folder you will find config files for all of the pre-trained models.Copy the config file for the model you selected and move it to a new folder where you will perform all the training. In this new folder, create a folder called data and move your TFRecord file inside of it. Create another folder called models and move the .ckpt (checkpoint) files (3 of them) of the pre-trained model you selected into this folder. Recall that model_detection_zoo.md contains download links for each of the pre-trained models and the download for each of the models here will contain not only the .pb file (which we used in our jupyter notebook in Part 1) but also a .ckpt file. Inside the models folder create another folder called train.Open up the newly moved config file in a text editor and at the very top change the number of classes to the amount in your dataset. Next change the fine_tune_checkpoint path to point to the model.ckpt file. If you followed the model structure I suggested this will be:The parameter num_steps determines how many training steps you will run before finishing. This number really depends on the size of your dataset along with a number of other factors (including how long you are willing to let the model train for). Once you start training I suggest you see how long it’s taking for each training step and adjust num_steps accordingly.Next you need to change the input_path and label_map_path for both the training and evaluation dataset. Input_path just goes to your TFRecord file. Before we can set the path for label_map_path we need to create the file it’s supposed to point to. All it’s looking for is a .pbtxt file that contains the id and name for each of the labels of your dataset. You can create this in any text file by following the format below.Ensure you start with id: 1 and not 0. I’d recommend placing this file inside your data folder. Lastly set num_examples to the number of evaluation samples you have.Navigate to the object_detection folder and copy train.py to your newly created training folder. To begin training simply navigate your terminal window to this folder (ensure you have followed the install instructions in Part 1) and enter in the command line,where pipline_config_path points to your config file. Training will now begin. Beware that depending on your system, training could take a few minutes to start so if it doesn’t crash or stop, give it some more time.If you are running out of memory and this is causing training to fail, there are a number of solutions you can try. First try adding the argumentsto your config file in the train_config section. For example, placing the two lines between gradient_clipping_by_norm and fine_tune_checkpoint will work. The number 2 above should only be starting values to get training to begin. The default for those values are 8 and 10 respectively and increasing those values should help speed up training.That’s it, you have now started the training that will fine tune your model! If you want to get a better idea of how training is progressing, look into using TensorBoard.In the next post I’ll show you how to save your trained model and deploy it in a project!",25/10/2017,5,1,0,,0,0,0.0,2,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
178,Full360 Likes Jaspersoft 5,,Michael DC Bowen,,4.0,785,"I had the good fortune to be invited to the Jaspersoft pre-release seminars in San Francisco. There I was in the company of a few other partners and at least 40 Jaspersoft employees from all different departments (and countries) around the company.So what’s new? Jaspersoft Version 5. Although I’m fairly new to the product, I got a good idea of where it’s going, and most importantly what concepts are driving development. Here’s a brief recap of all that I saw.There are four major improvements to the Jaspersoft Data Server that the developers are working on that comprise the version 5 to be released.Jaspersoft is committed to HTML5. JavaScript will drive this generation of charts in Ad Hoc reporting with OLAP, in JSS and in iReport. There will be multi-dimensional charting including automatic generation of multi-pies as well as spline charts. Jasper is focusing on easy and rapid development of the features used most by users, so more esoteric visualizations like scatter charts are not included.The interactive viewer will handle basic conditional formatting as well as handling dynamic on-the-fly chart type changes. Cross-tab sorting will be supported. Relative data input controls will be a new feature especially useful for date calculations.Jasper is making a commitment to expand its use of the XMLA standard. Immediately, that means it will natively understand MS OLAP cubes in V5. The intention is to ultimately capture Essbase data in future releases.The powerful concept of Data Domains that is central to the Jaspersoft model is being expanded to handle and combine multiple data sources. This will enable a unified data path across the product technologies that shows great promise. As part of enabling Jaspersoft to handle the explosion of Big Data, vastly improved memory management techniques and tools have been integrated into V5.Great news for serious Jasper developers is that the company has developed a plugin that enables an Eclipse-based iReport. They’re calling it Jaspersoft Studio.We’re excited about the fact that Jaspersoft is improving their installations with support for silent install. Under the new Administration regime for V5 we will see:I can tell you that Jasper says that improving their Ad Hoc Reporting is a priority, but what really impressed me was their Admin vision.Essentially, there is going to be versioning of objects in a universal metadata context. As a developer, you’ve seen it before. A customer has a slowly changing dimension problem. But most data items in business have an inherent slowly changing nature. Wouldn’t it be great not only to reload prior data, but the prior metadata as well? Jaspersoft is enabling their entire BI service to be a sort of time machine. They will version the objects in development and production deployment so that you can rewind not only the data, but the reports and middle tier configurations as of a particular moment in history.What is Big Data? From Jaspersoft’s perspective it is connecting with Hive, Hadoop, Cassandra, or MongoDB}. I happened to also catch the subsequent webinar covering how Jaspersoft regards the Big Data market and their approach to capturing their rightful share. There’s quite a lot to that and this blog post would get way too big if I covered all of that.BigData connections by Jasper version:JMX is the Java Management Extension of the J2SE 5.0 release. Jaspersoft’s JMX agent allows Attribute, Operations, Notification, and enables remote management.Comes from TEIID which is owed by JBoss / Red Hat. TEIID federates mixed modes of data to create ‘tables’. TEIID competes with UnityJDBC etc. It can allow you to join a text file and a table in simple schemas of reasonable size.JRS now incorporates the following deep improvements. — Groovy 2.0 — Cache Improvements (migrated to EHCache (http://ehcache.org/)) — Calculated fields in SQLData adapters are the future, but Jasper won’t think of the following data types as something for different tools. They will build ‘ETL’ internal to the product as the way up the path of data to present to users. Data is data; there are just more restrictions as the data is processed up the chain. The Intelligent cache is a non-persistent, in memory columnar store.Jasper 5 has split IOS and Android support into two separate SDKs.Jasper has built an OLAP schema on top of their already existing audit facility, so now you can ask the server what it has been doing.Jasper’s version usesWork was also done on Report Usability Tuning:Report Phases are:So as you can see, the folks at Jaspersoft have been very busy. You should call your rep and arrange for a demo. Tell ’em Bowen sent you and that you want a Full360 cloud implementation and service contract. That way, you’ll get the best of all worlds, guaranteed.Originally published at full360.com on November 15, 2012.",15/11/2012,0,0,3,,0,8,0.0,2,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
179,PyTorch Multi-GPU 제대로 학습하기,당근마켓 팀블로그,matthew l,185.0,24.0,2150,"PyTorch를 사랑하는 당근마켓 머신러닝 엔지니어 Matthew 입니다. PyTorch를 사용해서 Multi-GPU 학습을 하는 과정을 정리했습니다.포스트는 다음과 같이 진행합니다.이 글을 끝까지 다 읽고 나면 다음 nvidia-smi 사진과 같이 4개의 GPU를 full로 멋지게 사용할 수 있습니다. 😀딥러닝은 기본적으로 GPU 에서 학습을 합니다. Deep Neural Network는 기본적으로 매트릭스 연산을 하기 때문에 GPU를 사용할 경우 연산처리 속도가 상당히 빨라집니다. 딥러닝이 발전함에 따라 점점 네트워크의 크기도 커졌습니다. 비전 분야에서 Neural Network를 깊게 쌓는 방식이 성공하면서부터 그 이후의 딥러닝 연구에서는 대부분 큰 모델을 사용했습니다. 다음 그림에서보면 ResNet이 152개의 층을 쌓은 것을 알 수 있습니다. 비전 분야에서는 ResNet 이후로 꾸준히 큰 데이터셋과 큰 모델로 성능을 높이는 연구를 해왔습니다. 상대적으로 가벼운 모델을 사용하던 NLP 분야에서도 2018년 BERT를 기점으로 큰 데이터셋에 큰 모델로 성능을 높이는 방향으로 연구가 되고 있습니다.대부분의 경우 Nvidia의 GPU를 사용하는데 GPU 마다 메모리의 양이 다릅니다. 보통 개인이 집이나 연구실에서 딥러닝으로 학습을 할 경우에 GTX 1080 TI 같은 게임용 GPU를 많이 사용합니다. 이런 GPU의 경우 그래픽 작업용이나 딥러닝 연산용 GPU에 비해 가성비가 뛰어나다는 장점이 있습니다. 대부분의 경우에 GTX 1080 TI, TITAN XP와 같은 GPU 하나로도 딥러닝을 하는 데 큰 문제가 없습니다. 하지만 기업이나 연구실에서 큰 데이터셋에 대해 모델을 학습시킬 경우에 하나의 GPU로는 할 수 있는 것이 한정적입니다. 딥러닝에서 batch size는 성능에 영향을 주는 경우가 많습니다. 하나의 GPU 특히 게임용 GPU는 메모리의 한계가 있습니다. 예를 들어 메모리가 12G인 TITAN XP에서는 BERT base model을 batch size 30 이하로 돌릴 수 있습니다. BERT 논문에서 batch size 256으로 학습시킨 것과는 상당한 차이가 있습니다. 이러한 경우 multi-GPU 학습을 합니다. 말 그대로 여러 개의 GPU 에서 하나의 모델이 학습하는 것입니다.멀티 GPU를 사용하는 경우 다음 그림과 같이 워크스테이션을 구축합니다. 현재 당근마켓에서는 4 개의 TITAN XP 로 워크스테이션을 구축했습니다. 워크 스테이션에서 multi-GPU 환경을 세팅했더라도 여러 개의 GPU를 제대로 활용하는 것은 생각보다 쉽지 않습니다. 여러 개의 GPU를 사용하는데 각 GPU 마다 memory 사용량이 다른 문제가 발생할 수 있습니다. 또한 하나의 GPU를 사용해서 학습하는 경우보다 그닥 학습이 빨라지지 않는 경우도 많습니다. 이러한 환경에 익숙하지 않다면 multi-GPU를 제대로 활용하는데까지 많은 시간이 소요될 수 있습니다. 딥러닝 장비를 사용해서 학습을 하는 것은 공짜가 아닙니다. 자체적으로 워크 스테이션을 만들어서 사무실이나 연구실에서 학습하는 경우는 부담이 덜하겠지만 클라우드에서 모델을 학습하는 경우에는 부담이 큽니다. Multi-GPU로 학습하도록 코드를 디버깅하는 동안 비용이 계속 발생하기 때문입니다. 따라서 이 글에서는 PyTorch으로 multi-GPU 학습하는 동안 겪은 문제들과 해결 방법에 대해 소개하려 합니다.PyTorch에서는 기본적으로 multi-gpu 학습을 위한 Data Parallel이라는 기능을 제공합니다. Data Parallel이 작동하는 방식을 보여주는 것이 다음 그림입니다.딥러닝을 여러 개의 GPU에서 사용하려면 일단 모델을 각 GPU에 복사해서 할당해야 합니다. 그리고 iteration을 할 때마다 batch를 GPU의 개수만큼 나눕니다. 이렇게 나누는 과정을 ‘scatter’ 한다고 하며 실제로 Data Parallel에서 scatter 함수를 사용해서 이 작업을 수행합니다. 이렇게 입력을 나누고 나면 각 GPU에서 forward 과정을 진행합니다. 각 입력에 대해 모델이 출력을 내보내면 이제 이 출력들을 하나의 GPU로 모읍니다. 이렇게 tensor를 하나의 device로 모으는 것은 ‘gather’ 이라고 합니다.보통 딥러닝에서는 모델의 출력과 정답을 비교하는 loss function이 있습니다. Loss function을 통해 loss를 계산하면 back-propagation을 할 수 있습니다. Back-propagation은 각 GPU에서 수행하며 그 결과로 각 GPU에 있던 모델의 gradient를 구할 수 있습니다. 만약 4개의 GPU를 사용한다면 4개의 GPU에 각각 모델이 있고 각 모델은 계산된 gradient를 가지고 있습니다. 이제 모델을 업데이트하기 위해 각 GPU에 있는 gradient를 또 하나의 GPU로 모아서 업데이트를 합니다. 만약 Adam과 같은 optimizer를 사용하고 있다면 gradient로 바로 모델을 업데이트하지 않고 추가 연산을 합니다. 이러한 Data Parallel 기능은 코드 한 줄로 간단히 사용 가능합니다.nn.DataParallel로 model을 감싸면 학습을 할 때 다음과 같은 작업을 하는 것입니다. 위에서 언급한 대로 replicate → scatter → parallel_apply → gather 순서대로 진행합니다. Gather가 하나의 gpu로 각 모델의 출력을 모아주기 때문에 하나의 gpu의 메모리 사용량이 많을 수 밖에 없습니다.일반적으로 DataParallel을 사용한다면 다음과 같이 학습 코드가 돌아갑니다.PyTorch의 DataParallel 을 테스트하기 위해 사용한 코드는 김준성님의 BERT 코드입니다(링크: https://github.com/codertimo/BERT-pytorch). BERT 논문의 모델 사이즈보다 작은 사이즈로 multi GPU 학습을 테스트 했습니다. 모델에 입력으로 들어가는 sequence의 길이는 163, layer 수는 8층, attention head의 수 8개 그리고 hidden unit의 수는 256으로 사용했습니다. 0번, 1번, 2번, 3번 GPU를 사용해서 multi-gpu 학습을 시작한 다음에 nvidia-smi로 GPU 사용 현황을 체크했습니다. 다음과 같이 0번 GPU가 1, 2, 3번 GPU에 비해 6G 정도 더 많은 메모리를 사용하고 있습니다. 이렇게 하나의 GPU가 상대적으로 많은 메모리를 사용하면 batch size를 많이 키울 수 없습니다. 이 실험을 할 때는 200까지 batch size를 키울 수 있었습니다. 딥러닝에서 batch size는 학습 성능에 영향을 주는 경우가 많기 때문에 메모리 사용 불균형은 꼭 해결해야할 문제입니다. 또한 학습을 더 빨리하고 싶어서 multi-GPU를 쓰는 경우도 많습니다. 학습이 오래 걸릴 경우 batch size 차이로 1주일을 더 학습시켜야 하는 상황이 될 수도 있습니다.메모리 불균형 문제를 제일 간단히 해결하는 방법은 단순히 출력을 다른 GPU로 모으는 것입니다. 디폴트로 설정되어있는 GPU의 경우 gradient 또한 해당 GPU로 모이기 때문에 다른 GPU에 비해 메모리 사용량이 상당히 많습니다. 따라서 출력을 다른 GPU로 모으면 메모리 사용량의 차이를 줄일 수 있습니다. 다음 코드와 같이 간단하게 출력을 모으고 싶은 GPU 번호를 설정하면 됩니다.output_device를 설정하고 다시 학습을 시작하면 GPU 사용량이 달라진 것을 알 수 있습니다. 0번 GPU의 메모리 사용량은 줄고 1번 GPU의 메모리 사용량은 늘었습니다. 하지만 여전히 균형하게 사용하지 않는 것을 볼 수 있습니다. 모델 출력의 크기는 batch size에 따라 달라집니다. 이대로 batch size를 늘리면 1번 GPU의 메모리 사용량은 점점 늘어나게 됩니다. 따라서 이 방법은 일시적으로 문제를 해결하는 것 같아 보여도 적절한 해결 방법은 아닙니다. 게다가 GPU-Util을 보면 GPU를 제대로 활용하지 못하는 것을 확인할 수 있습니다.DataParallel을 그대로 사용하면서 메모리 불균형의 문제를 해결할 수 있는 방법에 대한 힌트는 PyTorch-Encoding이라는 패키지에 있습니다(패키지 링크: https://github.com/zhanghang1989/PyTorch-Encoding). 하나의 GPU의 메모리 사용량이 늘어나는 것은 모델의 출력을 하나의 GPU로 모은 것 때문입니다. 왜 하나의 GPU로 모델의 출력을 모을까요? 왜냐하면 모델의 출력을 사용해서 loss function을 계산해야하기 때문입니다. 모델은 DataParallel을 통해 병렬로 연산할 수 있게 만들었지만 loss function이 그대로이기 때문에 하나의 GPU에서 loss를 계산합니다. 따라서 loss function 또한 병렬로 연산하도록 만든다면 메모리 불균형 문제를 어느정도 해결할 수 있습니다.PyTorch-Encoding 중에서도 다음 파이썬 코드에 loss function을 parallel하게 만드는 코드가 들어있습니다.github.comLoss function을 병렬 연산 가능하게 만드는 방법은 모델을 병렬 연산으로 만드는 방법과 동일합니다. PyTorch에서는 loss function 또한 하나의 모듈입니다. 이 모듈을 각 GPU에 replicate 합니다. 그리고 데이터의 정답에 해당하는 tensor를 각 GPU로 scatter 합니다. 그러면 loss를 계산하기 위한 모델의 출력, 정답, loss function 모두 각 GPU에서 연산할 수 있도록 바뀐 상태입니다. 따라서 각 GPU에서 loss 값을 계산할 수 있습니다. 각 GPU에서는 계산한 loss로 바로 backward 연산을 할 수 있습니다.Loss function을 parallel 하게 만들어서 연산하는 과정을 코드로 보자면 다음과 같습니다. 데이터의 정답에 해당하는 target을 scatter 한 다음에 replicate한 module에서 각각 계산을 합니다. 계산한 output와 Reduce.apply를 통해 각 GPU에서 backward 연산을 하도록 만듭니다.DataParallelCriterion을 사용할 경우에 일반적인 DataParallel로 모델을 감싸면 안됩니다. DataParallel은 기본적으로 하나의 GPU로 출력을 모으기 때문입니다. 따라서 Custom DataParallel 클래스인 DataParallelModel을 사용합니다. DataParallelModel과 DataParallelCriterion을 사용해서 학습하는 과정은 다음과 같습니다. 사용하는 법은 상당히 간단합니다. Pytorch-Encoding 패키지에서 parallel.py 파일만 가져와서 학습 코드에서 import 하도록 만들면 됩니다.이렇게 학습을 할 경우에 Nvidia-smi 출력 결과는 다음과 같습니다. batch size 는 200으로 동일합니다. DataParallel 만 사용할 때에 비해 1번 GPU와 2번 GPU의 메모리 사용량의 차이가 상당히 줄었습니다. batch size를 기존에 비해 늘릴 수 있기 때문에 학습 시간도 전체적으로 1/3 정도가 줄었습니다. 하지만 GPU-Util의 수치로 확인할 수 있듯이 GPU 성능을 여전히 제대로 활용 못하고 있습니다. GPU 성능을 100 %로 끌어 올리려면 어떻게 해야할까요?딥러닝을 하시는 분들은 분산 학습에 대해 들어보신 적이 있을 겁니다. DeepMind에서 알파고나 알파스타를 발표할 때 어떤 식으로 학습 했는지 설명하는데 이렇게 규모가 큰 모델을 학습할 때는 보통 분산 학습을 합니다.분산 학습 자체는 하나의 컴퓨터로 학습하는게 아니라 여러 컴퓨터를 사용해서 학습하는 경우를 위해 개발된 것입니다. 하지만 multi-GPU 학습을 할 때도 분산 학습을 사용할 수 있습니다. 분산 학습을 직접 구현할 수도 있지만 PyTorch에서 제공하는 기능을 사용할 수도 있습니다.PyTorch에서는 DataParallel과 함께 분산 학습과 관련된 기능을 제공합니다. PyTorch에서 분산 학습을 어떻게 하는지 궁금하다면 다음 PyTorch Tutorial을 보는 것을 추천합니다.pytorch.org단순히 분산 학습을 사용해서 multi-GPU 학습을 하고 싶다면 PyTorch에서 공식적으로 제공하는 example을 보는 것이 좋습니다. 비전 분야에서 큰 데이터셋 중에 유명한 것이 ImageNet 입니다. 다음 링크가 ImageNet에 딥러닝 모델을 학습시키는 코드 예제입니다. 이 예제에서 여러 머신에서 분산 학습을 하는 방법을 소개하는데 하나의 머신에서 여러 GPU 학습하는 방법도 소개합니다.github.comImageNet 예제의 main.py 에서 multi-GPU와 관련된 주요 부분을 다음과 같이 정리해 봤습니다. main.py를 실행하면 main이 실행되는데 main은 다시 main_worker 들을 multi-processing으로 실행합니다. GPU 4개를 하나의 노드로 보고 world_size를 설정합니다. 그러면 mp.spawn 함수가 4개의 GPU에서 따로 따로 main_worker를 실행합니다.main_worker에서 dist.init_process_group을 통해 각 GPU 마다 분산 학습을 위한 초기화를 실행합니다. PyTorch의 docs를 보면 multi-GPU 학습을 할 경우 backend로 nccl을 사용하라고 나와있습니다. init_method에서 FREEPORT에 사용 가능한 port를 적으면 됩니다. 이렇게 분산 학습을 위한 초기화를 하고 나면 분산 학습이 가능합니다. 28번째 줄을 보면 model에는 DataParallel 대신에 DistributedDataParallel을 사용하는 것을 볼 수 있습니다. DataParallel에서 언급한 입력을 분산하고 forward 연산을 수행하고 다시 backward 연산을 수행하는 역할을 합니다.pytorch.orgDataLoader가 입력을 각 프로세스에 전달하기 위해서 다음처럼 DistributedSampler를 사용합니다. DistributedSampler는 DistributedDataParallel과 함께 사용해야 합니다. 사용 방법은 간단하게 정의해놓은 dataset를 DistributedSampler로 감싸주고 DataLoader에서 sampler에 인자로 넣어줍니다. 그 다음엔 평소에 DataLoader를 사용하듯이 똑같이 사용하면 됩니다.DistributedSampler의 내부를 살짝 보자면 다음 코드와 같습니다(많은 부분을 생략했습니다). 각 Sampler는 전체 데이터를 GPU의 개수로 나눈 부분 데이터에서만 데이터를 샘플링합니다. 부분 데이터를 만들기 위해 전체 데이터셋 인덱스 리스트를 무작위로 섞은 다음에 그 인덱스 리스트를 쪼개서 각 GPU Sampler에 할당합니다. epoch 마다 각 GPU sampler에 할당되는 인덱스 리스트는 다시 무작위로 달라집니다. 그러기 위해서는 train_sampler.set_epoch(epoch) 명령어를 매 epoch 마다 학습 전에 실행해야 합니다.PyTorch Distributed 패키지를 사용해서 BERT 작은 모델을 학습해봤습니다. Nvidia-smi 를 통해 확인한 GPU 메모리 사용 현황은 다음과 같습니다. GPU 메모리 사용량이 완전 동일한 것을 볼 수 있습니다. 또한 GPU-Util의 수치도 99%로 상당히 높은 것을 볼 수 있습니다. 여기까지 왔다면 multi-GPU 학습을 제대로 할 준비가 됐습니다.하지만 Distibuted DataParallel의 경우 학습을 시작하려 할 때 간간히 문제가 발생할 수 있습니다. 다음 github issue 글이 여러 문제 중에 하나를 보여줍니다. BERT 코드를 돌릴 때도 에러가 발생했는데 모델에서 학습에 사용하지 않는 parameter가 있을 경우에 Distributed DataParallel이 문제를 일으킬 수 있다는 의견이 있습니다. 이러한 문제를 신경쓰지 않고 학습을 하기 위해서 찾아보다가 Nvidia에서 만든 Apex라는 패키지를 발견했습니다.github.comNvidia에서 Apex라는 Mixed Precision 연산을 위한 패키지를 만들었습니다. 보통 딥러닝은 32 비트 연산을 하는데 16 비트 연산을 사용해서 메모리를 절약하고 학습 속도를 높이겠다는 의도로 만든 것입니다. Apex에는 Mixed Precision 연산 기능 말고도 Distributed 관련 기능이 포함합니다. 이 포스트에서는 Mixed Precision에 대한 내용은 다루지 않습니다.Apex의 Distributed DataParallel 기능을 하는 것이 DDP 입니다. Apex에서 ImageNet 학습을 위해 만든 예제에 관련 내용이 있습니다. Apex 사용법은 Docs에 잘 나와있으니 살펴보시면 됩니다.github.com다음 코드 2번 줄에서 보듯이 apex에서 DistributedDataParallel을 import 해서 사용합니다. 위 PyTorch 공식 예제에서와는 달리 코드 내에서 멀티 프로세싱을 실행하지 않습니다. 19 줄에서 보듯이 DDP로 model을 감싸줍니다. 그 이외에는 PyTorch DistributedDataParallel과 동일합니다.이 코드를 실행할 때는 다음 명령어를 사용해서 실행합니다. Torch.distributed.launch를 통해 main.py를 실행하는데 노드에서 4개의 프로세스가 돌아가도록 설정합니다. 각 프로세스는 GPU 하나에서 학습을 진행합니다. 만약 GPU가 2개라면 nproc_per_node를 2로 수정하면 됩니다. main.py에 batch_size와 num_worker를 설정하는데 각 GPU 마다의 batch_size와 worker 수를 의미합니다. batch size가 60이고 worker의 수가 2라면 전체적으로는 batch size가 240이며 worker의 수는 8입니다.Nvidia Apex를 사용해서 multi-GPU 학습을 했습니다. GPU 사용 현황은 다음과 같습니다. GPU 메모리 사용량이 모든 GPU에서 일정합니다.(3번 GPU는 다른 작업이 할당받고 있기 때문에 잡혀있습니다). GPU-Util을 보면 99% 아니면 100 %인 것을 알 수 있습니다.지금까지 살펴본 PyTorch로 multi-GPU 학습하는 방법은 3가지 입니다.DataParallel은 PyTorch에서 제공하는 가장 기본적인 방법이지만 GPU 메모리 불균형 문제가 생겼습니다. Custom DataParallel의 경우 GPU 메모리 문제를 어느정도 해결해주지만 GPU를 제대로 활용하지 못한다는 문제가 있었습니다. Distributed DataParallel은 원래 분산학습을 위해 만들어진 PyTorch의 기능이지만 multi-GPU 학습에도 사용할 수 있고 메모리 불균형 문제와 GPU를 활용하지 못하는 문제가 없었습니다. 하지만 간간히 문제가 발생하기 때문에 Nvidia에서 만든 Apex를 이용해서 multi-GPU 학습하는 것을 살펴봤습니다.그렇다면 Apex를 사용하는 것이 항상 좋을까요? 제가 살펴본 이런 문제들이 딥러닝 학습을 할 때 항상 발생하지 않습니다. 만약 이미지 분류를 학습한다면 DataParallel 만으로 충분할 수 있습니다. BERT에서 GPU 메모리 불균형 문제가 생기는 이유는 모델 출력이 상당히 크기 때문입니다. 각 step마다 word의 개수만큼이 출력으로 나오기 때문에 이런 문제가 생깁니다. 하지만 이미지 분류의 경우 모델 자체가 클 수는 있어도 모델 출력은 그렇게 크지 않습니다. 따라서 GPU 메모리 불균형은 거의 없습니다.이를 확인하기 위해 CIFAR-10에 PyramidNet을 학습해봤습니다. 학습에 사용한 코드 링크는 다음과 같습니다. CIFAR-10은 10개의 카테고리를 가진 32x32의 이미지 사이즈를 가지는 데이터셋 입니다. 또한 PyramidNet은 CIFAR-10 에서 최근까지 가장 높은 성능을 냈던 모델입니다. PyramidNet은 모델의 크기를 조절할 수 있습니다. Multi-GPU에서 학습 성능을 비교하려면 사이즈가 큰 모델을 쓰는 것이 좋습니다. 따라서 파라메터의 개수가 24,253,410인 모델을 실험에 사용했습니다. 다음 표에서 PyramidNet(alpha=270)에 해당하는 모델입니다. 학습에는 K80 4개를 사용했습니다.github.com우선 Single GPU로 PyramidNet을 학습시켰습니다. 하나의 GPU만 사용하면 batch size가 240 정도가 가장 크게 사용할 수 있는 한계입니다.Batch size 240으로 학습을 할 경우에 하나의 batch를 처리하는데 6~7초 정도가 걸립니다. 총 학습시간(1 epoch을 train 하는데 걸린 시간)은 22분 정도가 걸렸습니다.PyTorch의 기본적은 DataParallel 모듈을 사용해서 PyramidNet을 학습했습니다. Batch size는 768 정도까지 키울 수 있습니다. 기존에 하나의 GPU만 사용할 때보다 상당히 큰 batch size를 사용할 수 있습니다. 아래 사진을 보면 DataParallel 모듈만 사용해도 모든 GPU의 메모리가 거의 동일한 것을 볼 수 있습니다. 게다가 BERT의 경우 Adam을 사용하지만 PyramidNet에서는 일반 SGD를 사용하기 때문에 더더욱 메모리 불균형 문제가 없습니다.학습 시간은 원래 22분에서 5분 정도로 확 줄었습니다. 또한 batch size가 768이 됐음에도 batch time이 오히려 6초에서 5초로 빨라진 것을 볼 수 있습니다. 따라서 Image Classification 을 학습할 때는 DataParallel 만 사용해도 충분한 것을 알 수 있습니다. 만약 더 큰 batch size로 학습하거나 더 빠르게 학습하고 싶은 경우(ImageNet 같은 데이터셋의 경우 학습이 훨씬 오래걸립니다) Distributed 학습을 사용할 수 있습니다. 단, 이 경우는 하나의 컴퓨터에서 multi-GPU 학습을 하는 것이 아니라 여러 대의 컴퓨터에서 학습을 하는 경우입니다.따라서 학습시키는 모델에 따라 또한 optimizer에 따라 multi-GPU 학습하는 방법을 선택해야 합니다. 비전 분야보다는 자연어처리 분야에서 많이 발생하는 문제라고 볼 수 있습니다.딥러닝은 논문을 읽고 구현하는 것도 힘든 일인데 이렇게 자원을 제대로 활용하기 위해 들어가는 노력도 많습니다. 딥러닝 관련 논문에 대한 리뷰나 논문의 구현에 대한 자료는 많지만 어떻게 해야 제대로 자원을 활용해서 학습할 수 있는지에 대한 자료는 별로 없어서 글을 작성하게 되었습니다. PyTorch를 사용해서 딥러닝을 하시는 분들께 도움이 되었으면 좋겠습니다.",28/03/2019,0,0,0,"(683, 393)",18,3,0.0,19,vi,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
180,Chapter 10.1: DeepNLP — LSTM (Long Short Term Memory) Networks with Math.,Deep Math Machine learning.ai,Madhu Sanjeevi ( Mady ),4000.0,6.0,887,"Note: I am writing this article with the assumption that you know the deep learning a bit. In case if you don’t know much, Please read my earlier stories to understand the entire series on deep learning.In the last story we talked about Recurrent neural networks, so we now know what RNN’s are, How they work and what kind of problems it can solve and also we talked about a limitation in RNN’s which isVanishing /exploding gradient problemWe all know that a neural network uses an algorithm called BackPropagation to update the weights of the network. So what BP does isIt first calculates the gradients from the error using the chain rule in Calculus, then it updates the weights(Gradient descent).since the BP starts from the output layer to all the way back to input layer , In a simple neural network we may not face problems with updating weights but in a deep neural network we might face some issues.As we go back with the gradients , It is possible that the values get either smaller exponentially which causes Vanishing Gradient problem or larger exponentially which causes Exploding Gradient problem.Due to this we get the problems of training the network.In RNN’s, we have time steps and current time step value depends on the previous time step so we need to go all the way back to make an update.There are couple of remedies there to avoid this problem.We can use ReLu unit as an activation function, RMS Prop as an optimization algorithm and LSTM’s or GRU’s.so Lets focus on LSTMLSTM ( Long Short Term Memory ) Networks are called fancy recurrent neural networks with some additional features.Just like RNN, we have time steps in LSTM but we have extra piece of information which is called “MEMORY” in LSTM cell for every time step.So the LSTM cell contains the following componentsHere is the diagram for LSTM cell at the time step tDon’t panic I will explain every single hecking detail of it. Just get the overall picture stored in your brain.Lemme take only one time step (t) and explain it.What are the inputs and outputs of the LSTM cell at any step ??Inputs to the LSTM cell at any step are X (current input) , H ( previous hidden state ) and C ( previous memory state)Outputs from the LSTM cell are H ( current hidden state ) and C ( current memory state)Here is the diagram for a LSTM cell at T time step.How does the LSTM flow work??If you observe carefully,the above diagram explains it all.Anyway, lemme also try with wordsForget gate(f) , Cndate(C`), Input gate(I), Output Gate(O)are single layered neural networks with the Sigmoid activation function except candidate layer( it takes Tanh as activation function)These gates first take input vector.dot(U) and previous hidden state.dot(W) then concatenate them and apply activation functionfinally these gate produce vectors ( between 0 and 1 for Sigmoid, -1 to 1 for Tanh) so we get four vectors f, C`, I, O for every time step.Now let me tell you an important piece called Memory state CThis is the state where the memory (context) of input is storedEx : Mady walks in to the room, Monica also walks in to the room. Mady Said “hi” to ____??Inorder to predict correctly Here it stores “Monica” into memory C.This state can be modified. I mean LSTM cell can add /remove the information.Ex : Mady and Monica walk in to the room together , later Richard walks in to the room. Mady Said “hi” to ____??The assumption I am making is memory might change from Monica to Richard.I hope you get the idea.so LSTM cell takes the previous memory state Ct-1 and does element wise multiplication with forget gate (f)Ct = Ct-1*ftif forget gate value is 0 then previous memory state is completely forgottenif forget gate value is 1 then previous memory state is completely passed to the cell ( Remember f gate gives values between 0 and 1 )Now with current memory state Ct we calculate new memory state from input state and C layer.Ct= Ct + (It*C`t)Ct = Current memory state at time step t. and it gets passed to next time step.Here is flow diagram for CtFinally, we need to calculate what we’re going to output. This output will be based on our cell state Ct but will be a filtered version. so we apply Tanh to Ct then we do element wise multiplication with the output gate O, That will be our current hidden state HtHt = Tanh(Ct)We pass these two Ct and Ht to the next time step and repeat the same process.Here is the full diagram for LSTM for different time steps.Well I hope you get the idea of LSTM.RNN’ s have been an active research area and many people have been achieving amazing results lately using RNN’s (most of all are using LSTMs) They really work a lot better for most tasks!LSTM’s are really good but still face some issues for some problems so many people developed other methods also after LSTM’s ( hope I can cover later stories)So That’s it for this story , In the next story I will be writing about more advanced topics. Have a great day….!Suggestions /questions are welcome.Photos are designed using Paint in windows inspired by Christopher Olah Understanding LSTMs;See ya!",21/01/2018,0,45,2,"(625, 414)",10,1,0.0,2,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
181,Classifying Websites with Neural Networks,Knowledge from Data: The Datafiniti Blog,Datafiniti,1100.0,5.0,845,"At Datafiniti, we have a strong need for converting unstructured web content into structured data. For example, we’d like to find a page like:and do the following:Both of these are hard things for a computer to do in an automated manner. While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:OrBoth of these pages share many similarities to the actual product page, but also have many key differences. The real challenge, though, is that if we look at the entire set of possible web pages, those similarities and differences become somewhat blurred, which means hard and fast rules for classifications will fail often. In fact, we can’t even rely on just looking at the underlying HTML, since there are huge variations in how product pages are laid out in HTML.While we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page, doing so would be extremely time consuming, and frankly, incredibly boring work. Instead, we can try using a classical technique out of the artificial intelligence handbook: neural networks.Here’s a quick primer on neural networks. Let’s say we want to know whether any particular mushroom is poisonous or not. We’re not entirely sure what determines this, but we do have a record of mushrooms with their diameters and heights, along with which of these mushrooms were poisonous to eat, for sure. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonousWe would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible.Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. By constantly adjusting A and B this way, we can quickly get to the best possible values for them.In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:For a more detailed explanation of neural networks, you can check out the following links:In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. Our input layer modeled several features, including:Our output layer had the following:Our algorithm for the neural network took the following steps:The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. This works like so:So how did we do? In order to determine how successful we were in our predictions, we need to determine how to measure success. In general, we want to measure how many true positive (TP) results as compared to false positives (FP) and false negatives (FN). Conventional measurements for these are:Our implementation had the following results:These scores are just over our training set, of course. The actual scores on real-life data may be a bit lower, but not by much. This is pretty good! We should have an algorithm on our hands that can accurately classify product pages about 90% of the time.Of course, identifying product pages isn’t enough. We also want to pull out the actual structured data! In particular, we’re interested in product name, price, and any unique identifiers (e.g., UPC, EAN, & ISBN). This information would help us fill out our product search.We don’t actually use neural networks for doing this. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. Instead, we use a variety of heuristics specific to each attribute we’re trying to extract. For example, for product name, we look at the <h1> and <h2> tags, and use a few metrics to determine the best choice. We’ve been able to achieve around a 80% accuracy here. We may go into the actual metrics and methodology for developing them in a separate post!We feel pretty good about our ability to classify and extract product data. The extraction part could be better, but it’s steadily being improved. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. This means that we can scan the entire Internet and pull out any available data that exists out there. Exciting stuff!You can connect with us and learn more about our business, people, product, and property APIs and datasets by selecting one of the options below.",29/05/2013,0,3,2,"(596, 320)",8,6,0.0,7,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
182,Training a Custom Object Detection Model With Yolo-V5,Analytics Vidhya,Prabhat Kumar Sahu,108.0,7.0,830,"Object Detection is a task in computer vision that focuses on detecting objects in images/videos.There are various object detection algorithms out there like YOLO (You Only Look Once,) Single Shot Detector (SSD), Faster R-CNN, Histogram of Oriented Gradients (HOG), etc.In this article, we are going to use Yolo-V5 to train our custom object detection model. YOLO is one of the most famous object detection models.Version 1.0 of YOLO-V5 got released on 27 May 2020 by Glenn Jocher who is the founder & CEO of Utralytics. It’s written in PyTorch and it’s available in Github.It’s good to have a basic knowledge of deep learning computer vision. And how to work in a Google Colab environment.To train our own custom object detector these are the steps to followWe are going to use the Aquarium Dataset which is available in Roboflow Public Object Detection datasets. You can check out more datasets that are there.The aquarium dataset consists of 638 images. The images were already labeled by the Roboflow team. It has 7 classes such as fish, jellyfish, penguins, sharks, puffins, stingrays, and starfish, and most images contain multiple bounding boxes.To download the dataset you need to create a roboflow account first. It’s very simple and easy.If you want to use your own images you can use annotation tools such as LabelImg, CVAT or you can try any large-scale solutions like Scale or AWS Ground Truth.When you are annotating yourself make sure to follow the best practices. Check this link out for more details.After the dataset is prepared then we are all set to set up the environment and train the dataset.Here’s the link to my Notebook: Google ColabYou need a google account to use Google Colab. You can either use my notebook to train or you can create your own notebook and follow along.In Google Colab, you will receive a free GPU for 12 hours. If you use a new notebook in Colab change the runtime session to GPU.If you are planning to use my notebook then make sure to File → save a copy in your drive. Then you will be able to edit the code.Installation of the dependenciesSomehow the PyTorch version didn’t get compatible with the GPU so I installed another version of PyTorch by.We can import and take a look at our GPU Specification provided by Google Colab.Here’s what I gotGoogle Colab comes preinstalled with Cuda and Torch and some other dependencies. If you are planning to train locally then you will have to setup Cuda and the dependencies on your own. I will surely make a tutorial about it later on.After the environment set up is done. We can import the dataset into colab. As I am using the Roboflow dataset I will be downloading, if you plan to use your own you can import it using Google Drive.This will download the data, unzip and save it inside the yolov5 directory.Project Folder structureTo train a YOLO-V5 model, we need to have two YAML files.The first YAML to specify:This YAML of ours looks like this:The second YAML is to specify the whole model configuration. You can change the network architecture in this step if you want but we will go with the default one.The YAML which we term custom_yolov5s.yaml:We can put the YAML file anywhere we want because we can reference the file path later on. But it’s a good idea to put it inside the YoloV5 directory.After the configuration is done we can begin our training.There are multiple hyperparameters that we can specify which are:We need to specify the path of both YAML files which we created above.With 100 epochs the training got completed within 35 minutes.Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the — name flag when we train. In our case, we named this yolov5s_results. (If given no name, it defaults to results.txt.) The results file is plotted as a png after training completes.Partially completed results.txt files can be plotted with from utils.utils import plot_results; plot_results().After training starts, view train*.jpg images to see training images, labels, and augmentation effects. We can visualize both Ground Truth Training data, as well as Ground Truth, Augmented data.Using the final trained weight which got saved after training we can run our inferenceWeights folderTo run the model inference we can use the following command.This will process the input and store the output in our inference directory.Here are some output images:Now that we have successfully trained our custom model. We can download the weight files and save them in our local directory or in Google Drive.To do so we import a Google Drive module and send them outI hope you were able to follow along and was able to train successfully.I have uploaded the notebook, config files, and weight to my Github repository. You can check it out here.If you have any questions, recommendations, or critiques, I can be reached via Twitter or via my mail. Feel free to reach out to me.",01/01/2021,13,15,0,"(646, 484)",12,5,0.0,18,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
183,PCA using Python (scikit-learn),Towards Data Science,Michael Galarnyk,10800.0,8.0,1173,"My last tutorial went over Logistic Regression using Python. One of the things learned was that you can speed up the fitting of a machine learning algorithm by changing the optimization algorithm. A more common way of speeding up a machine learning algorithm is by using Principal Component Analysis (PCA). If your learning algorithm is too slow because the input dimension is too high, then using PCA to speed it up can be a reasonable choice. This is probably the most common application of PCA. Another common application of PCA is for data visualization.To understand the value of using PCA for data visualization, the first part of this tutorial post goes over a basic visualization of the IRIS dataset after applying PCA. The second part uses PCA to speed up a machine learning algorithm (logistic regression) on the MNIST dataset.With that, let’s get started! If you get lost, I recommend opening the video below in a separate tab.The code used in this tutorial is available belowPCA for Data VisualizationPCA to Speed-up Machine Learning AlgorithmsFor a lot of machine learning applications it helps to be able to visualize your data. Visualizing 2 or 3 dimensional data is not that challenging. However, even the Iris dataset used in this part of the tutorial is 4 dimensional. You can use PCA to reduce that 4 dimensional data into 2 or 3 dimensions so that you can plot and hopefully understand the data better.The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below will load the iris dataset.PCA is effected by scale so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data.The original data has 4 columns (sepal length, sepal width, petal length, and petal width). In this section, the code projects the original data which is 4 dimensional into 2 dimensions. I should note that after dimensionality reduction, there usually isn’t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation.Concatenating DataFrame along axis = 1. finalDf is the final DataFrame before plotting the data.This section is just plotting 2 dimensional data. Notice on the graph below that the classes seem well separated from each other.The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert 4 dimensional space to 2 dimensional space, you lose some of the variance (information) when you do this. By using the attribute explained_variance_ratio_, you can see that the first principal component contains 72.77% of the variance and the second principal component contains 23.03% of the variance. Together, the two components contain 95.80% of the information.While there are other ways to speed up machine learning algorithms, one less commonly known way is to use PCA. For this section, we aren’t using the IRIS dataset as the dataset only has 150 rows and only 4 feature columns. The MNIST database of handwritten digits is more suitable as it has 784 feature columns (784 dimensions), a training set of 60,000 examples, and a test set of 10,000 examples.You can also add a data_home parameter to fetch_mldata to change where you download the data.The images that you downloaded are contained in mnist.data and has a shape of (70000, 784) meaning there are 70,000 images with 784 dimensions (784 features).The labels (the integers 0–9) are contained in mnist.target. The features are 784 dimensional (28 x 28 images) and the labels are simply numbers from 0–9.Typically the train test split is 80% training and 20% test. In this case, I chose 6/7th of the data to be training and 1/7th of the data to be in the test set.The text in this paragraph is almost an exact copy of what was written earlier. PCA is effected by scale so you need to scale the features in the data before applying PCA. You can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the dataset’s features. Note you fit on the training set and transform on the training and test set. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data.Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.Fit PCA on training set. Note: you are fitting PCA on the training set only.Note: You can find out how many components PCA choose after fitting the model using pca.n_components_ . In this case, 95% of the variance amounts to 330 principal components.Step 1: Import the model you want to useIn sklearn, all machine learning models are implemented as Python classesStep 2: Make an instance of the Model.Step 3: Training the model on the data, storing the information learned from the dataModel is learning the relationship between digits and labelsStep 4: Predict the labels of new data (new images)Uses the information the model learned during the model training processThe code below predicts for one observationThe code below predicts for multiple observations at onceMeasuring Model PerformanceWhile accuracy is not always the best metric for machine learning algorithms (precision, recall, F1 Score, ROC Curve, etc would be better), it is used here for simplicity.The whole point of this section of the tutorial was to show that you can use PCA to speed up the fitting of machine learning algorithms. The table below shows how long it took to fit logistic regression on my MacBook after using PCA (retaining different amounts of variance each time).The earlier parts of the tutorial have demonstrated using PCA to compress high dimensional data to lower dimensional data. I wanted to briefly mention that PCA can also take the compressed representation of the data (lower dimensional data) back to an approximation of the original high dimensional data. If you are interested in the code that produces the image below, check out my github.This is a post that I could have written on for a lot longer as PCA has many different uses. I hope this post helps you with whatever you are working on. My next machine learning tutorial goes over How to Speed up Scikit-Learn Model Training. If you any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter. If you want to learn about other algorithms, please consider taking my Machine Learning with Scikit-Learn LinkedIn Learning course.",05/12/2017,18,21,3,"(700, 261)",8,0,0.0,13,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
184,BERT Embedding for Classification,Analytics Vidhya,Deepak Saini,18.0,6.0,955,"The recent advances in machine learning and growing amounts of available data have had a great impact on the field of Natural Language Processing (NLP). They facilitated the development of new neural architectures and led to strong improvements on many NLP tasks, such as machine translation or text classification. One advancement of particular importance is the development of models which build good quality, machine-readable representations of word meanings. These representations often referred to as word embeddings, are vectors that can be used as features in neural models that process text data.Types of embeddings1. Static Word Embedding:As the name suggests these word embeddings are static in nature. These incorporate the pre-trained values of the words, which we could use while training our models.For e.g. Glove and word2vec are the best examples of static word embeddings. These embeddings mostly we could use pre-trained once. But in some scenarios, we could train these embeddings as well. But it would contain high computational GPU power and a lot of time to train these embeddings from scratch.2. Contextual Embedding:Contextual embedding (e.g. ELMo, BERT), aims to learn a continuous (vector) representation for each word in the documents. Continuous representations can be used in downstream machine learning tasks.But just how contextual are these contextualized representations?Consider the word ‘mouse’. It has multiple word senses, one referring to rodents and another to a device. Does BERT effectively create one representation of ‘mouse’ per word sense (left)? Or does BERT create infinitely many representations of ‘mouse’, each highly specific to its context (right)?First, the traditional embedding represents only the difference between two types of mouses while from BERT, we would be getting real-time context as well. As shown cheese-loving is kinda showing similarity with the rodent while for the other “click on the” showing the similarity.How could we use Embedding for Classification?If we don’t have much data for the classification, in that case, we could use the embedding’s just like we do in Siamese Network for facial recognition.This approach basically called single-shot learning or zero-shot learning where we don’t have much data to build any classifier on it. In that case, we could calculate the embedding of the existing labeled data then classifying the new data point with the closeness of its embedding with the other embeddings. Whoever it would be more closer we would classify it as that particular category. For closeness of the embeddings, we are using the cosine-similarity function.I’ll be using the Hugging face library for this task. (https://huggingface.co/transformers/)In the below use case, I would be showing the embedding creation of long sentences (more than 512 tokens). As we are already aware of the Bert memory constraint for using more than 512 tokens.To conquer this issue, I have used the sliding window technique here so that we could be able to extract the embedding for the whole corpus at once.We are using the “bert-base-uncased” model. Always set output_hidden_states = True, while initializing the model otherwise, the model will not spit the hidden states.Create tokens of the text with the help of the tokenizer library. While setting up add_special_tokens= False, the special tokens (‘CLS’ or ‘SEP’) are not included in the tokens themselves.To overcome the issues of 512 token constraints, we would be using the sliding window technique. Therefore we would split the input ids and attention mask ids so that they cannot exceed the 512 lengths in total.Now we could iterate over each and every chunk and pass them through our model and generate the embeddings of each of them.Here torch.tensor([101]) and ([102]) represents the “CLS and SEP” tokens.After adding these tokens to the input chunks, we need to check for the padding as well. Mostly it would be needed in the last iteration where the length of our input would be lesser than 510.Padding would be needed only and only in the last iteration in our case. And below we are preparing our input-ids and mask-ids before passing them into the model.The model will spit output as well as hidden states in tuple format. Bert total will send 13 layers (including the input embedding as well). But as per the researchers, the last layers of the embeddings would contain the most information regarding the context of the Corpus. That’s why we would only take the last 9 layers to create our embedding for our Corpus.Below is the code to get only 4layers of hidden states and take out the average of them.hidden layer shape =[num of sentences or batch-size, Total tokens, num of hidden layers, num of features]toek_embedding[ : , : , 9: , : ] → Gives the last 4 hidden layers for better context.This is not the final embedding for our document as we have split our document into a chunk of 510 tokens. So we would iterate over each and every chunk and get the average of its last 4layers embedding. And finally, we could take the average of all the token embeddings as well, to get the final embedding of the document.Once we have the final embedding of the document, we could easily find out its similarity with the other documents with the help of the Cosine Similarity.What is Cosine Similarity?Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, the higher the cosine similarity.Or we could use the inbuilt function of PyTorchcos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)similarity = cos(sent1 embedding , sent2 embedding)Thanks for Reading !!References:ai.stanford.edu",16/05/2021,0,9,2,"(678, 195)",10,0,0.0,2,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
185,Dropout in (Deep) Machine learning,,Amar Budhiraja,540.0,4.0,645,"This blog post is also part of the series of Deep Learning posts. I wrote two other posts before — one on Weight Initialization and another one on Language Identification of Text. All the posts are self contained, so you can go ahead and read this one and check-out the others if you like to.In this post, I will primarily discuss the concept of dropout in neural networks, specifically deep nets, followed by an experiments to see how does it actually influence in practice by implementing a deep net on a standard dataset and seeing the effect of dropout.According to Wikipedia — The term “dropout” refers to dropping out units (both hidden and visible) in a neural network.Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By “ignoring”, I mean these units are not considered during a particular forward or backward pass.More technically, At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.Given that we know a bit about dropout, a question arises — why do we need dropout at all? Why do we need to literally shut-down parts of a neural networks?The answer to these questions is “to prevent over-fitting”.A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.Now that we know a little bit about dropout and the motivation, let’s go into some detail. If you just wanted an overview of dropout in neural networks, the above two sections would be sufficient. In this section, I will touch upon some more technicality.In machine learning, regularization is way to prevent over-fitting. Regularization reduces over-fitting by adding a penalty to the loss function. By adding this penalty, the model is trained such that it does not learn interdependent set of features weights. Those of you who know Logistic Regression might be familiar with L1 (Laplacian) and L2 (Gaussian) penalties.Dropout is an approach to regularization in neural networks which helps reducing interdependent learning amongst the neurons.Training Phase: For each hidden layer, for each training sample, for each iteration, ignore (zero out) a random fraction, p, of nodes (and corresponding activations).Use all activations, but reduce them by a factor p (to account for the missing activations during training).Let’s try this theory in practice. To see how dropout works, I build a deep net in Keras and tried to validate it on the CIFAR-10 dataset. The deep network is built had three convolution layers of size 64, 128 and 256 followed by two densely connected layers of size 512 and an output layer dense layer of size 10 (number of classes in the CIFAR-10 dataset).I took ReLU as the activation function for hidden layers and sigmoid for the output layer (these are standards, didn’t experiment much on changing these). Also, I used the standard categorical cross-entropy loss.Finally, I used dropout in all layers and increase the fraction of dropout from 0.0 (no dropout at all) to 0.9 with a step size of 0.1 and ran each of those to 20 epochs. The results look like this:From the above graphs we can conclude that with increasing the dropout, there is some increase in validation accuracy and decrease in loss initially before the trend starts to go down. There could be two reasons for the trend to go down if dropout fraction is 0.2:For the code on the Keras experiment, please refer to the jupyter notebook hosted on github.If you would like to know more about me, please check my LinkedIn profile.This story was featured on Intel’s blog: https://www.crowdcast.io/e/intel_virtual_lab/registerand on Data Science US -https://www.datascience.us/neural-net-dropout-dealing-overfitting/",15/12/2016,0,5,6,"(1392, 1245)",3,2,0.0,8,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,positive,joy/calmness
186,Photoreal Roman Emperor Project,,Daniel Voshart,,3.0,283,"Using the neural-net tool Artbreeder, Photoshop and historical references, I have created photoreal portraits of Roman Emperors. For this project, I have transformed, or restored (cracks, noses, ears etc.) 800 images of busts to make the 54 emperors of The Principate (27 BC to 285 AD).Update Sept 10th: New print available here (version 2) in a choice of languages. (Version 1: Gold / red marble, limited edition print sold out)The main technology behind Artbreeder is it’s generative adversarial network (GAN). Some call it Artificial Intelligence but it is more accurately described as Machine Learning.Artistic interpretations are, by their nature, more art than science but I’ve made an effort to cross-reference their appearance (hair, eyes, ethnicity etc.) to historical texts and coinage. I’ve striven to age them according to the year of death — their appearance prior to any major illness.My goal was not to romanticize emperors or make them seem heroic. In choosing bust / sculptures, my approach was to favor the bust that was made when the emperor was alive. Otherwise, I favored the bust made with the greatest craftsmanship and where the emperor was stereotypically uglier — my pet theory being that artists were likely trying to flatter their subjects.Some emperors (latter dynasties, short reigns) did not have surviving busts. For this, I researched multiple coin depictions, family tree and birthplaces. Sometimes I created my own composites.ABOUT THE PRINTThe working file for this print is enormous: 340 dpi at 24x36"". Available in English, Spanish, Italian, Latin, Polish and Russian.ABOUT THE AUTHORDaniel Voshart is a designer from Canada. First edition print was a quarantine project. Second edition print made possible by the overwhelming support and important critical feedback to the first print.",24/07/2020,0,15,6,"(653, 460)",4,1,0.0,9,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
187,Understanding the StyleGAN and StyleGAN2 Architecture,Analytics Vidhya,Prem Chandra Singh,12.0,5.0,844,"The article contains the introduction of StyleGAN and StyleGAN2 architecture which will give you an idea. It may help you to start with StyleGAN. You will find some metric or the operations name which you don’t know, to gain a deep understanding of StyleGAN and StyleGAN2 you can go through the paper whose link is provided in the resources section.Let’s start with the StyleGAN and then we move towards StyleGAN 2.The major changes they have done in the Generator part of the “Progressive Growing of GANs” architecture. Below you can see both the traditional and the style-based generator (new one or StyleGAN network) network.In the traditional network, latent vectors directly pass into the block just after the normalization whereas in the StyleGAN network latent vectors after normalization pass through the mapping network (layer of 8 fully connected networks) then the outputs are transformed (A stands for the affine transformation which is the combination of linear transformation and translation) and passed to the blocks and get added with the noise B after the instance normalization (AdaIN i.e Adaptive instance normalization).Above, you can see the formula of AdaIN where x comes from the conv net and y comes from the left side network. Clearly, seen in the equation that after the normalization of x, y(s, i) is used for the scaling and y(b, i) is used for the transformation as a bias. Below you can see the StyleGAN in a simple form.In the official paper, you will see the results on CelebA and FF(Flickr Faces) high-quality datasets where they shown the FIDs (Frechet inception distances) score using 50K randomly drawn images from the training set. Below you can see the resultsThey started from baseline configuration A (Progressive GAN), and after adding bilinear up/downsampling, long training they see improvements. Then added mapping network and AdaIN operations or in config D they removed the traditional inputs from the synthesis network and replaced them with 4x4x512 constant tensor.We can see the improvements in FIDs value over the traditional generator (B) and enabling Mixing regularization (this operation also called style mixing) gives more control over the style and high-level aspects like pose, hairstyle, eyeglasses, etc.So, this is a simple introduction to the StyleGAN architecture and now let’s see what improvements have been made in StyleGAN 2 and understand its architecture.In the below image, you can see the defects or blurry portion in the generated image which comes from the starting 64x64 resolution. This is the major reason behind the redesigning of the generator with that the quality of generated images also improved.So, let’s see what changes in the architecture of the network improves the performance of generated image step by step. Below you can see the improvements in architecturePart A is the same StyleGAN architecture and Part B shows the detailed view of the StyleGAN architecture. In Part C, they replaced the AdaIN (Adaptive Instance Normalization) with the Modulation (or the scaling of the factors) and Normalization. Below you can see the modulation (left side) and normalization (right side) equation.Also, in Part C they shifted the addition of noise and bias outside of the block. Finally, in Part D you can see the weights are adjusted with the style and the normalization is replaced with a “demodulation” operation, combined operations are called “Weight Demodulation”. See the formula below.You can see that this equation seems the combination of the above two modulation and normalization equations (epsilon is a small constant value, used to prevent numerical issues like division by zero). Results can be seen on the below outputs, after replacing the normalization with demodulation removes the droplet-like artifacts.Now, we have seen the improvements in the form of generated images. Let’s see the improvements measured using metrics like FID, Perceptual path length (Introduced in the StyleGAN paper, lower the PPL better the generated image), etc.On the above table, can be seen that the improvements on the configurations after applying different methods. Path length regularization and Lazy regularization are used to keep the PPL score low show that the generated images are more clear or smooth.Progressive growing of the network generates high-quality images but it also causes the characteristic artifacts (or phase artifacts) i.e. eye and teeth of the person seems stuck at one place wherever the face of the person moves, it was shown in the official StyleGAN2 video (video link is attached in the resources section below).To solve this issue they tried other connections (skip connections, residual nets, etc) on the generator and discriminator network and saw that skip connection work best for the generator and residual nets give better results on the discriminator.Above table shows the results with the combination of each type of connection. Also, on the above main result table configuration E and F shows results on the skip connection type generator and residual discriminator network.In the below figure, highlighted part in b is a generator and in c is a discriminator without progressive growing.Almost all the images are taken from the official paper of StyleGAN and StyleGAN 2 whose links are given below in the resource section.",12/02/2021,0,2,0,"(545, 264)",13,1,0.0,6,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,trust/acceptance
188,Your Temporary Instant Disposable Dreamhouse for the Weekend,Phase Change,Scott Smith,4800.0,8.0,1848,"Close colleagues of mine will tell you I have honed a particular obsession/crackpot theory over the past few years: that Airbnb has been gently A/B testing me in real life.Let me explain. I travel more than most humans should. As someone who runs their own company, and sometimes needs to spend more time in a location than is affordable via traditional hotel lodgings (such as with a recent relocation over the summer), I have made use of that darling of the sharing economy/scourge of communities (depending on which lens you look at it through), Airbnb, to stretch my budget, spend time closer to work, friends, clients, or just have company when traveling. I’ve stayed in over 30 properties, in something like eight countries, so I’ve had a lot of time to contemplate the company’s strategies from the inside.The semi-serious theory started during back-to-back stays in the UK several years ago. My first three night stay was in a London borough, in a fairly cozy house owned by a couple with a toddler. It was comfortable enough, though a bit chilly in both bedroom and shared bath. The interior design wasn’t miles off my tastes, but it didn’t push any buttons of joy either, mostly catalog-standard late 20th century British home store. I never even sat down on the ground floor. The bits of media I saw around the house were mildly interesting, if predictable, but not must-reads or binge-viewable. I wasn’t really allowed in the kitchen, which was reserved for use by the family only. The wife of the couple has formerly worked in media on a cooking show, the husband in finance. I hardly saw either of them, as they made themselves scarce.After the check-in, I didn’t have much interaction with the hosts until leaving, and they weren’t interested in any to be honest. It was strictly a transactional stay. Their child was probably cute, but fussed far too much to get a close look—it was mostly an unhappy sound coming from the kitchen or bedroom. Fair enough. I stayed three days, I paid, I chatted briefly and left, and left a weakly positive review after. I had no real complaints, but probably wouldn’t look for it again.From London, I moved down to the south coast for work (I’m being vague to protect the hosts mentioned herein). I found the place, also an attached house in a row dating probably from the Edwardian period. The host couple met me in the front hall, ushered me in, sat me down in the lounge to relax, and I was immediately offered a warm, fresh-baked cupcake and a glass of wine as I slid back into a nice leather sofa. As the husband, who worked in the trendy area of “fintech,” asked me about my work—and seemed to understand what I do—my eyes scanned the groaning bookshelves across from me. “Have that, want to read that, ohhh, that’s a good one, must remember to look at that,” I recall thinking. We had so much in common.The wife, just finishing up a new round of baking for one of her side businesses, shouted a welcome and told me to feel free to use the house as my own, listing the tasty goods available for breakfast the next day as she joined our conversation with the couple’s very adorable son, who poked at my shoes engagingly, and seemed to pay close attention to my voice. What followed was an interesting chat about culture, technology and cooking, before I went up to my very warm, comfortable, private room, past the amazing folk art, highly listenable CD collection and private bath with want-able Scandinavian textiles.And then it hit me. The principle actors and scripts of these two Airbnb plays were roughly the same. Same family configurations, professions and ages, same general houses, same price per night within a few pounds, same availability. Except, when contrasting the two, one was so comfortable, personally interesting and engaging, I wanted to stay an extra week, while the other almost hurried me on my way. One I was happy to pay to stay in, one I felt vaguely grudging about in retrospect. One could have been my alternate media collection and wine store, one missed the mark on general user experience for me. I quietly locked the door to my room, logged onto the fast broadband (quite slow and choppy at House #1) and opened my Amazon profile just to see what I’d been looking at lately.As I lay in bed the first night, breathing in the rich cake scent still hanging in the air, I thought about whether Airbnb had somehow tapped into my online searches and purchases. After all, this is the age of convergent Big Data and powerful retail analytics. Without having seen really any of the home contents at either place, or anything useful about the hosts from the Airbnb listings, I’d ended up in two very similar, yet weirdly different, residences. One where even the conversation with the hosts was familiar and relevant, the other where it just didn’t read. Back to back. Easy to compare. Was the child even real, or just part of the test?In a period when both home staging and immersive theatre are hot, why couldn’t it happen, I thought? And with same-day delivery services breaking out all over, couldn’t a set of highly personalized home contents—chosen to be both familiar and aspirational (after all, you want to leave space for potential purchases to help fund this business model)—have been plucked from a regional depot, popped onto shelves and in cabinets, and organized for my arrival? Couldn’t some actors in search of work in London have been briefed up enough from open source material to interact with me for an hour or so? Couldn’t they? Couldn’t they?I’d been on the road for a while, and fatigue was starting to set in. Maybe it was affecting my head.That was two years ago. It had been in the back of my mind since.And then. This past summer, I had a similar experience, only with my whole family while mid-relocation to the Netherlands. Again, similar homes, same family demographics, both away on holiday this time (it’s tough to get small children to follow a script, right?), one house comfortable enough in a suburban town, the other a charming place in a gentrifying neighborhood worth squatting in hopes the owners didn’t return (jk, Airbnb, jk). Was I optimizing my own stays, or were they feeding me more appropriate properties in hopes of making this testing easier? Hotels have tested such things, why not the hotel-killer itself? They even left the same bread for us as a welcome basket. One white, one whole grain.After all, Airbnb has deployed Aerosolve, its own machine learning platform, to make sense of real-time usage data and help hosts get a better return. Tuning properties for desirability is feasible—the company is already using automated scanning of house photos to optimize presentation of properties as well. With all of this technology aimed at the properties themselves, why wouldn’t Airbnb also dig into the minds of guests, find out how they respond to different houses, which conveniences they’re drawn to, etc? Nah, that would take sensors inside a house, on top of crack Web and mobile analytics. You’d need to know what people do during their stay.And as I’m sitting there, thinking again about this crazy idea, I see a tweet go by: Airbnb has purchased…an obscure Russian sensor company. I slammed the laptop and checked the cabinets for tin foil.A month or so goes by. I forget about it again. Then I open Medium and see a story about how Airbnb has mocked up parts of its own headquarters based on the apartment design a French couple who use the service to let their own flat. The couple is now suing the company. “They are branding their company with our life,” owner Benjamin Dewé told Buzzfeed. The company has apparently copied a range of style elements from the French couple’s home in its own San Francisco offices. Down to the doodles on the chalkboard. The doodles.As Jamie Lauren Keiles demonstrated in the Medium piece above, it’s pretty easy to break those furnishing and accessories down to a shoppable list, on with goods obtained on Amazon or elsewhere. Like those magazine features that show how to buy knock-offs of celebrity fashion, complete with prices and shops, a family’s flat (admittedly one they rented out via Airbnb, including to Airbnb for a function) has been commodified into a shopping list. Buy that lifestyle right here. Better yet, live in it for a few days.Only, with the convergence of Big Data, analytics (including visual analysis tools which can look for the presence of brands in social media photos), machine learning and accessible APIs of companies like Amazon, and breakneck logistics Uber-style (or even predictive shipping, per the notorious Amazon patent), fabbing up a home interior to suit your tastes (or tastes that are forming, but haven’t fully emerged yet) is within today’s technology. Hell, even that cute Roomba you had to have may be quietly mapping the place you live. This will be available in knock-off home robots soon. Have you checked the user agreements of your various home appliances and systems to see if they can sell the data? Probably not.And why not tap that stock of underused homes, and underemployed people? If there’s one thing the sharing economy overlords have taught us, it’s that the world is just a collection of undermonetized assets waiting to be redistributed, right? Why not productize, commodify and populate that second-to-last frontier, our living spaces? And staying in someone else’s place with someone else’s stuff you fancied from the pictures is tired. Everything else is personalized, financialized and productized. Why even own your own stuff when it could be Ubered into position in a desirable location based on your most recent Pinterest saves?Think about it. With a bundled DreamHome™ service, you can perpetually test drive that new living room suite for long holiday weekends—I mean, why wait until after purchasing for buyer’s remorse to set in? You can get it out of the way, without the financial commitment. Just your desires, played forward all the time.You can even test roommates or neighbors for the weekend. Why stop at furnishings and paint colors? Slap those detailed sentiment analyses and personality analytics gleaned from your prospective co-habitant’s online activities, eye-tracking history, Tinder preferences and 23andMe profile onto a few improv actors and have some Big Data cosplay in a pop-up maisonette. Come Monday morning, you can just walk out the front door, with nothing but a premium fee to pay, a fee which may be itself be subsidized by various sponsors who want to test products on you. Don’t worry, it’s cool. Duralux, Crate & Barrel and LinkedIn picked up the tab for this getaway in the woods or beach with new friends. Sound good? Of course it does. We knew you would like it.Check your email. Your Temporary Instant Disposable Dreamhouse for the Weekend may be waiting.",21/11/2015,0,2,14,"(700, 377)",3,0,0.0,16,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,neutral,joy/calmness
189,Adversarial Attack(Part 1): Adversarial Attack과 Defense,,Taekmin Kim,,5.0,308,"최근 삼성 SDS의 사이다(SAIDA)팀이 스타크래프트 AI 대회에서 1등을 차지했습니다. 플레이 영상을 보면 꽤나 높은 수준이고 실제로 많은 게이머들이 이기기 힘든 실력이라고 합니다.게임 뿐만 아니라 번역기(네이버의 파파고)나 얼굴 인식(아이폰, 갤럭시) 등 우리 주위에서 인공지능이 적용된 제품을 많이 접할 수 있습니다. 가까운 미래에는 자율 주행이나 무인 편의점 등 인공지능이 더욱 더 우리 삶 속으로 파고들 것이라고 예상됩니다.지금까지 많은 AI 기술이 뛰어난 성과를 보여준 것은 사실이지만 현실에서는 성능 뿐만 아니라 안정성 또한 중요합니다. AI가 아니더라도 자동차가 급발진하거나 추위에서 핸드폰이 꺼지는 등 오작동은 지금도 쉽게 접할 수 있습니다. AI 기술도 동일한 문제점을 가지고 있고 의료 AI, 차량 AI가 보급된 시점에는 오작동으로 인한 피해는 상상 이상일 것입니다. 특히 의도적으로 오작동을 유발하기 쉽다면 더 큰 피해가 발생하겠죠.앞으로 2개의 튜토리얼을 통해서 의도적인 오작동(Adversarial Attack)에 대응하는 AI에 대해서 다룰 예정입니다.의도적인 오작동의 가장 쉽고 일반적인 방법은 노이즈 추가입니다.위의 그림은 학교에서 흔히 볼 수 있는 장난이지만 도로 위에서는 얘기가 다릅니다. 표지판이나 신호등의 색을 아주 미묘하게 변화시켜 자율 주행차가 역주행 혹은 급발진 한다면 큰 사고가 발생하겠죠. 멀쩡한 환자의 의료 사진에 적절한 노이즈를 더한다면 큰 수술을 받게 할 수도 있습니다.실제로 사람 눈에 거의 구별되지 않는 노이즈를 추가해 사진을 잘못 분류하게 하는 공격 방법들이 있습니다. 아래 두 판다 사진은 큰 차이점이 없어 보이지만 왼쪽 사진은 panda, 오른쪽 사진은 gibbon으로 AI가 예측합니다. 아주 작은 노이즈로 인해 오작동한 것이죠.공격자가 목표로 삼는 AI에 대한 사전 지식을 얼마나 알고 있느냐에 따라서 아래와 같이 공격 능력을 구분합니다.공격자의 의도에 따라서 목표를 아래와 같이 분류합니다. 단순히 틀리게만 하느 수준에서 원하는 방향으로 행동을 유도하는 것까지 다양한 난이도의 목표가 있습니다.위의 공격 능력과 공격 목표에 따라서 공격 난이도가 달라집니다.이번 튜토리얼에서 공격 알고리즘에 대해서 다루지는 않지만 최근 몇 년간 많은 알고리즘이 발표되었습니다. FSGM, DeepFool, Universal Adversarial Pertubations 등이있습니다.공격이 노이즈를 적극적으로 활용하는 것처럼 방어도 노이즈에 초점을 맞춰 이루어집니다. 대표적으로 아래와 같은 접근이 있습니다.방어에 대한 자세한 내용은 Part 2에서 다루도록 하겠습니다.Adversarial Attack은 학계 뿐만 아니라 산업계에서 관심을 가지고 있는 주제입니다. 이번 에피소드에서 간단한 내용을 위주로 다루지만 더 깊은 내용이 궁금하신 분들에게는 아래 논문들을 추천드립니다.",15/12/2018,0,16,0,"(700, 491)",3,6,1.0,8,ko,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
190,"Deep Reinforcement Learning Demysitifed (Episode 2) — Policy Iteration, Value Iteration and Q-learning",,Moustafa Alzantot,835.0,11.0,2167,"In previous two articles, we introduced reinforcement learning definition, examples, and simple solving strategies using random policy search and genetic algorithms.In practice, random search does not work well for complex problems where the search space (that depends on the number of possible states and actions) is large. Also, genetic algorithm is a meta-heuristic optimization so it does not provide a guarantee to find an optimal solution. In this article, we are going to introduce fundamental reinforcement learning algorithms.We start by reviewing the Markov Decision Process formulation, then we describe the value-iteration and policy iteration which are algorithms for finding the optimal policy when the agent knows sufficient details about the environment model. We then, describe the Q-learning is a model-free learning that can be used when the agent does not know the environment model but has to discover the policy by trial and error making use of its history of interaction with the environment. We also provide demonstration examples of the three methods by using the FrozenLake8x8 and MountainCar problems from OpenAI gym.We briefly introduced Markov Decision Process MDPin our first article. To recall, in reinforcement learning problems we have an agent interacting with an environment. At each time step, the agent performs an action which leads to two things: changing the environment state and the agent (possibly) receiving a reward (or penalty) from the environment. The goal of the agent is to discover an optimal policy (i.e. what actions to do in each state) such that it maximizes the total value of rewards received from the environment in response to its actions. MDPis used to describe the agent/ environment interaction settings in a formal way.MDP consists of a tuple of 5 elements:The way by which the agent chooses which action to perform is named the agent policy which is a function that takes the current environment state to return an action. The policy is often denoted by the symbol 𝛑.Let’s now differentiate between two types environments.Deterministic environment: deterministic environment means that both state transition model and reward model are deterministic functions. If the agent while in a given state repeats a given action, it will always go the same next state and receive the same reward value.Stochastic environment: In a stochastic environment there is uncertainty about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time. For example, a robot which tries to move forward but because of the imperfection in the robot operation or other factors in the environment (e.g. slippery floor), sometimes the action forward will make it move forward but in sometimes, it will move to left or right.Deterministic environments are easier to solve, because the agent knows how to plan its actions with no-uncertainty given the environment MDP. Possibly, the environment can be modeled in as a graph where each state is a node and edges represent transition actions from one state to another and edge weights are received rewards. Then, the agent can use a graph search algorithm such as A* to find the path with maximum total reward form the initial state.Remember, that the goal of the agent is to pick the best policy that will maximize the total rewards received from the environment.Assume that environment is initially at state s_0At time 0 : Agent observes the environment state s_0 and picks an action a_0, then upon performing its action, environment state becomes s_1 and the agent receives a reward r_1 .At time 1: Agent observes current state s_1 and picks an action a_1 , then upon acting its action, environment state becomes s_2 and it receives a reward r_2 .At time 2: Agent observes current state s_2 and picks an action a_2 , then upon acting its action, environment state becomes s_3 and it receives a reward r_3 .So the total reward received by the agent in response to the actions selected by its policy is going to be:Total reward = r_1 + r_2 + r_3 + r_4 + r_5 + ..However, it is common to use a discount factor to give higher weight to near rewards received near than rewards received further in the future.Total discounted reward = r_1 + 𝛾 r_2 + 𝛾² r_3 + 𝛾³ r_4 + 𝛾⁴ r_5+ …so,where T is the horizon (episode length) which can be infinity if there is maximum length for the episode.The reason for using discount factor is to prevent the total reward from going to infinity (because 0 ≤ 𝛾 ≤ 1), it also models the agent behavior when the agent prefers immediate rewards than rewards that are potentially received far away in the future. (If I give you 1000 dollars today and If I give you 1000 days after 10 years which one would you prefer ? ). Imagine a robot that is trying to solve a maze and there are two paths to the goal state one of them is longer but gives higher reward while there is a shorter path with smaller reward. By adjusting the 𝛾 value, you can control which the path the agent should prefer.Now, we are ready to introduce the value-iteration and policy-iteration algorithms. These are two fundamental methods for solving MDPs. Both value-iteration and policy-iteration assume that the agent knows the MDP model of the world (i.e. the agent knows the state-transition and reward probability functions). Therefore, they can be used by the agent to (offline) plan its actions given knowledge about the environment before interacting with it. Later, we will discuss Q-learning which is a model-free learning environment that can be used in situation where the agent initially knows only that are the possible states and actions but doesn't know the state-transition and reward probability functions. In Q-learning the agent improves its behavior (online) through learning from the history of interactions with the environment.Many reinforcement learning introduce the notion of `value-function` which often denoted as V(s) . The value function represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s. The value function depends on the policy by which the agent picks actions to perform. So, if the agent uses a given policy 𝛑 to select actions, the corresponding value function is given by:Among all possible value-functions, there exist an optimal value function that has higher value than other functions for all states.The optimal policy 𝛑* is the policy that corresponds to optimal value function.In addition to the state value-function, for convenience RL algorithms introduce another function which is the state-action pair Q function. Q is a function of a state-action pair and returns a real value.The optimal Q-function Q*(s, a) means the expected total reward received by an agent starting in sand picks action a, then will behave optimally afterwards. There, Q*(s, a) is an indication for how good it is for an agent to pick action a while being in state s.Since V*(s) is the maximum expected total reward when starting from state s , it will be the maximum of Q*(s, a)over all possible actions.Therefore, the relationship between Q*(s, a) and V*(s) is easily obtained as:and If we know the optimal Q-function Q*(s, a) , the optimal policy can be easily extracted by choosing the action a that gives maximum Q*(s, a) for state s.Now, lets introduce an important equation called the Bellman equation which is a super-important equation optimization and have applications in many fields such as reinforcement learning, economics and control theory. Bellman equation using dynamic programming paradigm provides a recursive definition for the optimal Q-function.The Q*(s, a) is equal to the summation of immediate reward after performing action a while in state s and the discounted expected future reward after transition to a next state s'.Value-iteration and policy iteration rely on these equations to compute the optimal value-function.Value iteration computes the optimal state value function by iteratively improving the estimate of V(s). The algorithm initialize V(s) to arbitrary random values. It repeatedly updates the Q(s, a) and V(s) values until they converges. Value iteration is guaranteed to converge to the optimal values. This algorithm is shown in the following pseudo-code:Now lets implement it in python to solve the FrozenLake8x8 openAI gym. compared to the FrozenLake-v0 environment we solved earlier using genetic algorithm, the FrozenLake8x8 has 64 possible states (grid size is 8x8) instead of 16. Therefore, the problem becomes harder and genetic algorithm will struggle to find the optimal solution.Policy IterationWhile value-iteration algorithm keeps improving the value function at each iteration until the value-function converges. Since the agent only cares about the finding the optimal policy, sometimes the optimal policy will converge before the value function. Therefore, another algorithm called policy-iteration instead of repeated improving the value-function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. Policy iteration is also guaranteed to converge to the optimal policy and it often takes less iterations to converge than the value-iteration algorithm.The pseudo code for Policy Iteration is shown below.Both value-iteration and policy-iteration algorithms can be used for offline planning where the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the MDP model is known). Comparing to each other, policy-iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive.Now, lets consider the case where the agent does not know apriori what are the effects of its actions on the environment (state transition and reward models are not known). The agent only knows what are the set of possible states and actions, and can observe the environment current state. In this case, the agent has to actively learn through the experience of interactions with the environment. There are two categories of learning algorithms:model-based learning: In model-based learning, the agent will interact to the environment and from the history of its interactions, the agent will try to approximate the environment state transition and reward models. Afterwards, given the models it learnt, the agent can use value-iteration or policy-iteration to find an optimal policy.model-free learning: in model-free learning, the agent will not try to learn explicit models of the environment state transition and reward functions. However, it directly derives an optimal policy from the interactions with the environment.Q-Learning is an example of model-free learning algorithm. It does not assume that agent knows anything about the state-transition and reward models. However, the agent will discover what are the good and bad actions by trial and error.The basic idea of Q-Learning is to approximate the state-action pairs Q-function from the samples of Q(s, a) that we observe during interaction with the enviornment. This approach is known as Time-Difference Learning.where 𝛂 is the learning rate. The Q(s,a)table is initialized randomly. Then the agent starts to interact with the environment, and upon each interaction the agent will observe the reward of its action r(s,a)and the state transition (new state s'). The agent compute the observed Q-value Q_obs(s, a) and then use the above equation to update its own estimate of Q(s,a) .Exploration vs exploitationAn important question is how does the agent select actions during learning. Should the agent trust the learnt values of Q(s, a) enough to select actions based on it ? or try other actions hoping this may give it a better reward. This is known as the exploration vs exploitation dilemma.A simple approach is known as the 𝛆-greedy approach where at each step. With small probability 𝛜, the agent will pick a random action (explore) or with probability (1-𝛜) the agent will select an action according to the current estimate of Q-values. 𝛜 value can be decreased overtime as the agent becomes more confident with its estimate of Q-values.Now, lets demonstrate how Q-Learning can be used to solve an interesting problem from OpenAI gym, the mountin-car problem. In the mountain car problem, there is a car on 1-dimensional track between two mountains. The goal of the car is to climb the mountain on its right. However, its engine is not strong to climb the mountain without having to go back to gain some momentum by climbing the mountain on the left.Here, the agent is the car, and possible actions are drive left, do nothing, or drive right. At every time step, the agent receives a penalty of -1 which means that the goal of the agent is to climb the right mountain as fast as possible to minimize the sum of -1 penalties it receives.The observation is two continuous variables representing the velocity and position of the car. Since, the observation variables are continuous, for our algorithm we discretize the observed values in order to use Q-Learning. While initially, the car is unable to climb the mountain and it will take forever, if you select a random action. After learning, it learns how to climb the mountain within less than 100 time-steps.",09/07/2017,0,49,4,"(668, 165)",13,2,0.0,10,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,trust/acceptance
191,Visualizing Clusters with Python’s Matplotlib,Towards Data Science,Thiago Carvalho,1200.0,9.0,970,"Clustering sure isn’t something new. MacQueen developed the k-means algorithm in 1967, and since then, many other implementations and algorithms have been developed to perform the task of grouping data.In this article, we’ll explore how to improve our cluster’s visualization with scatter plots.Let’s start by loading and preparing our data. I’ll use a dataset of Pokemon stats.Since this article isn’t so much about clustering as it is about visualization, I’ll use a simple k-means for the following examples.We’ll calculate three clusters, get their centroids, and set some colors.Then we can pass the fields we used to create the cluster to Matplotlib’s scatter and use the ‘c’ column we created to paint the points in our chart according to their cluster.Cool. That’s the basic visualization of a clustered dataset, and even without much information, we can already start to make sense of our clusters and how they are divided.We’ll often use multiple variables to cluster our data, and scatter plots can only display two variables. There are several options for visualizing more than three variables, but all of them have disadvantages that should be considered.We could use the markers' size and make it a bubble chart, but that’s not an optimal solution. We couldn’t compare this third variable with the others since they would have different encodings.For example, by looking at the chart we made earlier, we can tell if a record has a higher Attack or Defense. But if we added Speed as the size, we couldn’t compare it with the other two variables.3D plots can also encode a third variable, but it can also get confusing, sometimes even misleading — That’s because depending on how we look at the chart, it may give us the wrong impression.Still, 3D scatter plots can be useful, especially if they’re not static.Depending on your environment, it’s easy to add some interactivity with Matplotlib.Some IDEs will have this by default; other environments will require extensions and a magic command such as “Matplotlib Widget” on Jupyter Lab or “Matplotlib Notebook” on Jupyter notebooks.By changing the angle we’re looking at the chart, we can examine it more carefully and avoid misinterpreting the data.Overall, they still are a pretty limited solution.In my opinion, the best approach is to use multiple scatter plots, either in a matrix format or by changing between variables. You can also consider using some data reduction method such as PCA to consolidate your variables into a smaller number of factors.Now, let’s begin improving on our visualization.If data visualization is storytelling, then annotations are the equivalent of a narrator in our story. They should help the viewer understand and focus on what’s really important while not taking too much space on the plot.We’ll add the basics, a title, labels, and a legend.Cool, now we can clearly get what this chart is about.We can also give the viewer some reference points. Displaying the centroids and drawing reference lines to averages or a percentile can help explain our cluster.It’s way easier to tell how the clusters are divided now.The red cluster groups the values with the highest attack and defence, while the blue has the lowest, and the green group is generally closer to the average.Sometimes illustrating how our cluster work is as important as its results. In k-means, since we’re working with distances, connecting the points to their respective centroids can help us visualize what the algorithm is actually doing.Now the relationship between the clusters and the centroids is totally explicit, and it's easier to explain how the algorithm works.We can also see how spread out the values in each cluster are.For example, the red values appear to be farther away from their centroid than blue values. If the groups' variance is something important to our analysis, a chart like this could be effective.We should also note that the separation between green and blue wasn’t so evident in the previous visualizations.Even though they have different colors and are connected to different places, those records circled in black are still more similar between themselves than most values in their own cluster.This visualization makes it harder to perceive that and may give the impression that values from distinct clusters are totally different.Another option to help us visualize our clusters' size or spread is to draw a shape around it or a shadow. Doing so manually would take forever and for sure wouldn’t be worth the effort.Luckily, there are ways to automate that.The convex hull is the smallest set of connections between our data points to form a polygon that encloses all the points, and there are ways to find the convex hull systematically — That is to say, we can use Sklearn to get the contour of our dataset.Great. We can even interpolate the lines of our polygon to make a smoother shape around our data.*The interpolation method was based on replies from this thread.Those contours shouldn’t be taken so seriously since they are not an actual measurement of anything. But they still do a great job at highlighting the clusters so that no viewer can miss them.Overall, there’s no simple solution to visualize clusters. Each case is unique, and we should experiment a lot before deciding what to display to our public.It’s also important to mention that the examples I used were clean. I used a simple dataset and only two variables for clustering. In real cases, it won’t always look like this. Many times, drawing the connections to the centroid can make our chart too polluted and almost unreadable.You should start with something simple, visualize every combination of variables and identify the more meaningful ones or the ones where you can demonstrate your insights more naturally. Then you can move to experiment with other visualizations and techniques for highlighting what you found.I hope you learned something and enjoyed my article. Thanks!Find more Python DataViz tutorials.",13/01/2021,10,0,0,"(595, 482)",15,0,0.0,3,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
192,Bird Song Classification using Siamese Networks and Dilated Convolutions,Towards Data Science,Aditya Dutt,70.0,9.0,1303,"Bioacoustics is very useful in studying the environment. It has been used for a long time to track submarines and whales. Birds help a lot to shape the plant life we see around us. Recognizing bird songs is very important for automatic monitoring of wildlife and studying the behavior of birds. It can help in tracking birds without disturbing them. We can also tell which birds species exist at a particular place. It can give us some information about their migration pattern. Every species of birds have their unique sounds. They use songs of varying length and complexity to attract mates, warn other birds of nearby danger, and mark their territory. Songbirds can have different dialects based on their geographical location. But sounds from non-songbirds do not change much based on geography. Using Deep learning methods, we can easily classify birds based on their calls and songs. We can use any of the neural network frameworks like CNNs, Siamese Networks, WaveNets, etc. for this task.Goal: We want to classify different birds species given their song/ call audio samples. We can extract spectrograms of the audio samples and use them as features for classification. We will use the British Birdsong Dataset available on Kaggle for this experiment. The dataset is described in the section Data Description.I have written an article on Siamese Network before. You can check it out to get a deeper understanding of its working and loss functions used for it. Code is also provided in that article. But, I will give a summary of the Siamese Network here.A Siamese network is a class of neural networks that contains one or more identical networks. We feed a pair of inputs to these networks. Each network computes the features of one input. And, then the similarity of features is computed using their difference or the dot product. For same class input pairs, the target output is 1 and for different classes input pairs, the output is 0. Remember, both networks have same the parameters and weights. If not, then they are not Siamese.Different loss functions can be used for a Siamese network.We will use triplet loss for our experiment.Dilated Convolutions are a type of convolution that “inflate” the kernel by inserting holes between the kernel elements. They are also called atrous convolutions.The concept of Dilated Convolution came from the wavelet decomposition in which the mother wavelet is scaled or dilated by different scales to capture different frequencies.Dilated convolutions increase the receptive field with the same computation and memory costs and without the loss of resolution. It can capture context from the entire input with the same number of parameters.Here is a nice article on Dilated Convolutions by Sik-Ho Tsang.We are using the British Birdsong Dataset available on Kaggle for this experiment. It is a small subset gathered from the Xeno Canto data collection to form a balanced dataset of 88 bird species in the United Kingdom.We are only classifying 9 bird species here: Canada Goose, Carrion Crow, Coal Tit, Common Blackbird, Common Chaffinch, Common Chiffchaff, Common Linnet, Common Moorhen, and Common Nightingale.There are very few samples in this dataset for each bird. The audio samples are around 40-60 seconds long. Some of them are a little noisy and sometimes there are other birds in the background. Clips of 2 seconds are extracted from each sample with 50% overlap to create new samples. This will create a sufficient number of samples for training the neural network. Data is divided into 60% for training, and 40% for testing.Librosa library in python is used for music and audio analysis. We can read audio files and extract spectrograms using it.Step 1: Read the audio file using librosa. Normalize the time series between -1 and 1.Step 2: Remove silence from audio.Step 3: Split each audio file into 2 second long clips with a 50% overlap.Step 4: Divide the data into training and testing. 60% is used for training and 40% for testing.Step 5: Extract spectrograms from samples. A filter is applied to the spectrograms to get a frequency range between 1KHz and 8KHz as most bird songs have frequencies in that range. Now, standardize all the spectrograms (You can normalize them also between 0 and 1). Here, the shape of each spectrogram is 163 x 345.Step 6: Generate positive and negative pairs of samples for the Siamese Network.Firstly, generate positive pairs.Now, generate negative pairs of classes.You can generate both positive and negative pairs using the function below. It takes as input: input features, target class labels, number of random numbers taken from each class, and number of positive pairs. It returns anchor, positive, and negative samples.Now the data is ready for the Siamese Network.We have 3 types of inputs: anchor, positive, and negative samples. The shape of each input is: (10800 x 345 x 163).Now, we need to build a neural network.The encoder model contains 8 1-D convolution layers with exponentially increasing dilation factors. After that, a 1D convolution layer is applied to decrease the number of features. Finally, Global MaxPooling 1D layer is applied. A batch normalization layer is applied after each layer. All layers have a ‘reLu’ activation except the last one. A ‘tanh’ activation is applied on the last layer. At the final layer, we got a 32 -dimensional vector as output.Three instances of this encoder model are created. They represent anchor, positive, and negative input. All three 32-dimensional feature vectors are concatenated into a 96-dimensional vector. This concatenated vector is treated as output. As you can see below in code, the function triplet_loss takes the output and separates the 3 embeddings again, and computes the loss.Below you can see the code for the model.Here is the encoder model summary:Now we have a complete Siamese Network.We can fit the model now. The target output of the model is a dummy output. It is because we are not comparing the output of the model to the target output. Instead, we are just minimizing distances between same class embeddings and pushing away different classes embeddings.Our input size of anchor, positive, and negative samples is: (10800 x 345 x 163). The batch size is set to 256. The model converged in only 30 epochs thanks to Batch Normalization. You can train longer if you wish to.The accuracy of the model is 98.1% on the training set and 97.3% on the test dataset.Here is the normalized confusion matrix of the test dataset:Here is the similarity matrix of embeddings:Below is a scatter plot of test dataset embeddings after applying PCA on them. There is a good separation between all the classes. And, all classes are clustered together except a few samples.Siamese Networks successfully classified birds based on their songs with a 97% accuracy. Siamese Networks convert the problem of classification to a similarity problem. They can work with fewer samples as well because we are generating pairs of samples. The model with dilated 1-D convolutions along with the batch normalization layer converged very quickly.Here is the GitHub repository for this project.github.comThis model can be extended to classify more than 50 or 100 birds. I don’t know much about birds because it’s not my field of research but I am very curious to see if we can identify different geographical locations based on bird songs. It’s because Songbirds' dialect changes with geographical locations. Here is a GitHub repository that contains a list of datasets related to birds. Xeno-canto website contains bird sounds from all over the world. It has data of different countries, species, etc. You can select a dataset from here for your own project.Remember, you can use this model for similar audio-based tasks also like speaker classification, emotion detection, etc.Thank you so much for reading! I hope it was helpful.🐧Feel free to contact me, if you want to collaborate on a project or need some ideas for your projects.arxiv.orgarxiv.orgtowardsdatascience.comwww.kaggle.compaperswithcode.comgithub.comarxiv.orgmachinelearningmastery.commachinelearningmastery.comwww.xeno-canto.org",04/07/2021,0,41,11,"(687, 651)",9,2,0.0,20,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
193,"CNN, Transfer Learning with VGG-16 and ResNet-50, Feature Extraction for Image Retrieval with Keras",Analytics Vidhya,Ferhat Taş,5.0,5.0,672,"In this article, we are going to talk about how to implement a simple Convolutional Neural Network model firstly. Then we are going to implement Transfer Learning models with VGG-16 and ResNet-50. Lastly we are going to extract features from those Transfer Learning models for Image Retrieval. We will train models with CINIC-10 dataset and use Keras library to implement and train each models.Firstly, we should import necessary libraries and get dataset with ImageDataGenerator like below code. We should give directories of train, validation and test dataset as parameter when using ImageDataGenerator. We have 90000 image in each datasets and we have 10 classes in CINIC-10 dataset. So we chose categorical as class_mode. Our batch_size is 64 and number of epochs is 10 for training models. Input shape for all images is (224, 224, 3) and we normalize all images with rescale=1./255 as parameter of ImageDataGenerator.We can simply implement a Convolutional Neural Network model with below function. There are some parameters of this function:With using above function to create CNN models there are Test Accuracy results for some combination of hyper-parameters after training the models. We can see the results for 10 epochs in below bar plot.With looking above bar plot, we can get some generalization for hyper-parametersFor transfer learning of VGG-16 and ResNet-50 we can use below functions. In this functions we will create models without last classification layer and add our fully connected layer which has 1024 neuron.We have one parameter in this function which is lastFourTrainable. If this parameter of function is false; then just last fully connected layer of models will be trainable. But if this parameter is true; then last four layers of models which have parameters will be trainable.With using above functions to create Transfer Learning models of VGG-16 and ResNet-50, there are Test Accuracy results for 4 combination of those models. We can see the results for 10 epochs in below bar plot.We can see that VGG-16 Transfer Learning Model with lastFourTrainable=True give us the best results compare to other Transfer Learning Models. Also we can say that if we increase number of trainable layers, we can get better results in all models.We can also see the Confusion Matrix of our best Transfer Learning model below.Feature Extraction in deep learning models can be used for image retrieval. We are going to extract features from VGG-16 and ResNet-50 Transfer Learning models which we train in previous section. So we have 4 model weights now and we are going to use them for feature extraction.For extracting features we are going to use output before classification layer of models. For example for VGG-16 model;We can see the codes for feature extraction in below.In the above code there are some functions:getFeatureVector(model, img_path): This function will find the feature vector for given img_path with using given model and return this feature vector.getCosineSimilarity(model, img_path): This function will find the Cosine Similarity between given A and B feature vectors.getFeatureDataFrame(model): This function will firstly create a DataFrame with Pandas library which has two columns as ‘file’ and ‘features’. Then we will find all feature vectors for train and validation datasets and return those feature vectors as DataFrame.getSimilarImages(img_file, features_df, model, model_name): This function will get feature vector of given image and compare this feature vector with all feature vectors in DataFrame and plot first 5 similar images.Now, it is time to see results of Image Retrieval with Feature Extraction!In above table, we can see that VGG-16 models are better than ResNet-50 models for Image Retrieval. Also, we can say that if we increase trainable layers in each models we can get better results.In this article, firstly we learned how to implement a simple CNN model and how hyper-parameters can change accuracies for a CNN model.In second section, we get Test Accuracy results for Transfer Learning models of VGG-16 and ResNet models. According to those results, we can say that VGG-16 models can outperform ResNet-50 models.Lastly, we saw that VGG-16 models can outperform ResNet-50 models for Image Retrieval.You can see all codes in GitHub repository.",09/01/2021,0,16,12,"(600, 449)",4,3,0.0,2,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,expectation/interest
194,Understanding Dropout with the Simplified Math behind it,Towards Data Science,Chitta Ranjan,1400.0,7.0,1040,"<<Download the free book, Understanding Deep Learning, to learn more>>In spite of the groundbreaking results reported, little is known about Dropout from a theoretical standpoint. Likewise, the importance of Dropout rate as 0.5 and how it should be changed with layers are not evidently clear. Also, can we generalize Dropout to other approaches? The following will provide some explanations.Deep Learning architectures are now becoming deeper and wider. With these bigger networks, we are able to achieve good accuracies. However, this was not the case about a decade ago. Deep Learning was, in fact, infamous due to overfitting issue.Then, around 2012, the idea of Dropout emerged. The concept revolutionized Deep Learning. Much of the success that we have with Deep Learning is attributed to Dropout.In this post, our objective is to understand the Math behind Dropout. However, before we get to the Math, let’s take a step back and understand what changed with Dropout. This will be a motivation to touch the Math.Before Dropout, a major research area was regularization. Introduction of regularization methods in neural networks, such as L1 and L2 weight penalties, started from the early 2000s [1]. However, these regularizations did not completely solve the overfitting issue.The reason was Co-adaptation.One major issue in learning large networks is co-adaptation. In such a network, if all the weights are learned together it is common that some of the connections will have more predictive capability than the others.In such a scenario, as the network is trained iteratively these powerful connections are learned more while the weaker ones are ignored. Over many iterations, only a fraction of the node connections is trained. And the rest stop participating.This phenomenon is called co-adaptation. This could not be prevented with the traditional regularization, like the L1 and L2. The reason is they also regularize based on the predictive capability of the connections. Due to this, they become close to deterministic in choosing and rejecting weights. And, thus again, the strong gets stronger and the weak gets weaker.A major fallout of this was: expanding the neural network size would not help. Consequently, neural networks’ size and, thus, accuracy became limited.Then came Dropout. A new regularization approach. It resolved the co-adaptation. Now, we could build deeper and wider networks. And use the prediction power of all of it.With this background, let’s dive into the Mathematics of Dropout. You may skip directly to Dropout equivalent to regularized Network section for the inferences.Consider a single layer linear unit in a network as shown in Figure 4 below. Refer [2] for details.This is called linear because of the linear activation, f(x) = x. As we can see in Figure 4, the output of the layer is a linear weighted sum of the inputs. We are considering this simplified case for a mathematical explanation. The results (empirically) hold for the usual non-linear networks.For model estimation, we minimize a loss function. For this linear layer, we will look at the ordinary least square loss,Eq. 1 shows loss for a regular network and Eq. 2 for a dropout network. In Eq. 2, the dropout rate is 𝛿, where 𝛿 ~ Bernoulli(p). This means 𝛿 is equal to 1 with probability p and 0 otherwise.The backpropagation for network training uses a gradient descent approach. We will, therefore, first look at the gradient of the dropout network in Eq. 2, and then come to the regular network in Eq. 1.Now, we will try to find a relationship between this gradient and the gradient of the regular network. To that end, suppose we make w’ = p*w in Eq. 1. Therefore,Taking the derivative of Eq. 4, we find,Now, we have the interesting part. If we find the expectation of the gradient of the Dropout network, we get,If we look at Eq. 6, the expectation of the gradient with Dropout, is equal to the gradient of Regularized regular network Eɴ if w’ = p*w.This means minimizing the Dropout loss (in Eq. 2) is equivalent to minimizing a regularized network, shown in Eq. 7 below.That is, if you differentiate a regularized network in Eq. 7, you will get to the (expectation of) gradient of a Dropout network as in Eq. 6.This is a profound relationship. From here, we can answer:This is because the regularization parameter, p(1-p) in Eq. 7, is maximum at p = 0.5.In Keras, the dropout rate argument is (1-p). For intermediate layers, choosing (1-p) = 0.5 for large networks is ideal. For the input layer, (1-p) should be kept about 0.2 or lower. This is because dropping the input data can adversely affect the training. A (1-p) > 0.5 is not advised, as it culls more connections without boosting the regularization.Because the expected value of a Dropout network is equivalent to a regular network with its weights scaled with the Dropout rate p. The scaling makes the inferences from a Dropout network comparable to the full network. There are computational benefits as well, which is explained with an Ensemble modeling perspective in [1].Before we go, I want to touch upon Gaussian-Dropout.As we saw before, in Dropout we are dropping a connection with probability (1-p). Put mathematically, in Eq. 2 we have the connection weights multiplied with a random variable, 𝛿, where 𝛿 ~ Bernoulli(p).This Dropout procedure can be looked at as putting a Bernoulli gate on each connection.We can replace the Bernoulli gate with another gate. For example, a Gaussian Gate. And this gives us a Gaussian-Dropout.The Gaussian-Dropout has been found to work as good as the regular Dropout and sometimes better.With a Gaussian-Dropout, the expected value of the activation remains unchanged (see Eq. 8). Therefore, unlike the regular Dropout, no weight scaling is required during inferencing.This property gives the Gaussian-Dropout a computational advantage as well. We will explore the performance of Gaussian-Dropout in an upcoming post. Until then, a word of caution.Although the idea of Dropout Gate can be generalized to distributions other than Bernoulli, it is advised to understand how the new distribution will affect the expectation of the activations. And based on this, appropriate scaling of the activations should be done.In this post, we went through the Mathematics behind Dropout. We worked the Maths under some simplified conditions. However, the results extend to general cases in Deep Learning. In summary, we understood,",08/05/2019,0,9,26,"(700, 177)",13,4,0.0,5,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,expectation/interest
195,TRAIN A CUSTOM YOLOv4 OBJECT DETECTOR (Using Google Colab),Analytics Vidhya,Techzizou,124.0,13.0,1533,"techzizou.com(NOTE: For this YOLOv4 Tutorial, we will be cloning the Darknet git repository in a folder on our google drive)NOTE: If you get disconnected or lose your session for some reason, you have to run steps 2, 5, and 6 again to mount the drive, edit makefile and build darknet every single time, otherwise the darknet executable will not work.Create a folder named yolov4 in your google drive. Next, create another folder named training inside the yolov4 folder. This is where we will save our trained weights (This path is mentioned in the obj.data file which we will upload later)Run the following command to create a symbolic link so that now the path /content/gdrive/My\ Drive/ is equal to /mydriveNavigate to /mydrive/yolov4 folderClone the Darknet git repository in the yolov4 folder on your drive.You can also view the folder from Colab since the drive has already been mounted. See pic below.I have uploaded my custom files for mask detection on my GitHub. I am working with 2 classes i.e. “with_mask” and “without_mask”.Input image example (Image1.jpg)You can use any software for labeling like the labelImg tool.I use an open-source labeling tool called OpenLabeling with a very simple UI.Click on the link below to know more about the labeling process and other software for it:NOTE: Garbage In = Garbage Out. Choosing and labeling images is the most important part. Try to find good-quality images. The quality of the data goes a long way towards determining the quality of the result.The output YOLO format label file looks as shown below.Put all the input image “.jpg” files and their corresponding YOLO format labeled “.txt” files in a folder named obj.Create its zip file obj.zip and upload it to the yolov4 folder on your drive.Download the yolov4-custom.cfg file from darknet/cfg directory, make changes to it, and upload it to the yolov4/data folder on your drive.You can also download the custom config file from AlexeyAB’s Github.Make the following changes in the custom config file:So if classes=1 then it should be filters=18. If classes=2 then write filters=21.You can tweak other parameter values too like the learning rate, angle, saturation, exposure, and hue once you’ve understood how the basics of the training process work. For beginners, the above changes will suffice.The obj.data file has :Has objects' names — each in a new line. Make sure the classes are in the same order as in the class_list.txt file used while labeling the images so the index id of every class is the same as mentioned in the labeled YOLO text files.(To divide all image files into 2 parts. 90% for train and 10% for test)This process.py script creates the files train.txt & test.txt where the train.txt file has paths to 90% of the images and test.txt has paths to 10% of the images.You can download the process.py script from my GitHub.**IMPORTANT: The “process.py” script has only the “.jpg” format written in it, so other formats such as “.png”,“.jpeg”, or even “.JPG”(in capitals) won’t be recognized. If you are using any other format, make changes in the process.py script accordingly.Now that we have uploaded all the files, our yolov4 folder on our drive should look like this:(Also set CUDNN, CUDNN_HALF, and LIBSO to 1)The current working directory is /mydrive/yolov4/darknetClean the data and cfg folders except for the labels folder inside the data folder which is required for writing label names on the detection boxes.So just remove all other files from the data folder and completely clean the cfg folder as we already have our custom config file in the yolov4 folder on our drive.This step is optional.7(a) Unzip the obj.zip dataset and its contents so that they are now in /darknet/data/ folder7(b) Copy your yolov4-custom.cfg file so that it is now in /darknet/cfg/ folder7(c) Copy the obj.names and obj.data files so that they are now in /darknet/data/ folder7(d) Copy the process.py file into the current darknet directoryThe current working directory is /mydrive/yolov4/darknetList the contents of the data folder to check if the train.txt and test.txt files have been created.The above process.py script creates the two files train.txt and test.txt where train.txt has paths to 90% of the images and test.txt has paths to 10% of the images. The process.py script file I am using has path data/obj written in it since the current working directory is /mydrive/yolov4/darknet. We unzipped the images into the data/obj folder in Step 7(a). The train.txt and test.txt files look like as shown below.Here we use transfer learning. Instead of training a model from scratch, we use pre-trained YOLOv4 weights which have been trained up to 137 convolutional layers. Run the following command to download the YOLOv4 pre-trained weights file.For best results, you should stop the training when the average loss is less than 0.05 if possible or at least constantly below 0.3, else train the model until the average loss does not show any significant change for a while.The map parameter here gives us the Mean Average Precision. The higher the mAP the better it is for object detection.You can visit the official AlexeyAB Github page which gives a detailed explanation on when to stop training. Click on the link below to jump to that section.github.comNOTE: If you get disconnected or lose your session for some reason, you have to run steps 2, 5, and 6 again to mount the drive, edit makefile and build darknet every single time, otherwise the darknet executable will not work.If you get disconnected or lose your session, you don’t have to start training your model from scratch again. You can restart training from where you left off. Use the weights that were saved last. The weights are saved every 100 iterations as yolov4-custom_last.weights in the yolov4/training folder on your drive. (The path we gave as backup in “obj.data” file).So to restart training run Steps 2, 5, 6, and then run the following command:Press (Ctrl + Shift + i) . Go to console. Paste the following code and press Enter.You can check the performance of all the trained weights by looking at the chart.png file. However, the chart.png file only shows results if the training does not get interrupted i.e. if you do not get disconnected or lose your session. If you restart training from a saved point, this will not work.If this does not work, there are other methods to check your performance. One of them is by checking the mAP of the trained weights.You can check mAP for all the weights saved every 1000 iterations for eg:- yolov4-custom_4000.weights, yolov4-custom_5000.weights, yolov4-custom_6000.weights, and so on. This way you can find out which weights file gives you the best result. The higher the mAP the better it is.Run the following command to check the mAP for a particular saved weights file where xxxx is the iteration number for it.(eg:- 4000,5000,6000,…)You can do it either manually or by simply running the code belowUpload an image to your google drive to test.Run your custom detector on an image with this command. (The thresh flag sets the minimum accuracy required for object detection)For running detector on images captured by a webcam run the following code. This is the code snippet provided by Google Colab for camera capture except for the last two lines which run the detector on the saved image as we did in the previous command above.Upload a video to your google drive to test.Run your custom detector on a video with this command. (The thresh flag sets the minimum accuracy required for object detection)First, import dependencies, define helper functions, load your custom YOLOv4 files, and then run the detector on a webcam.Run the code below. (NOTE: Adjust the custom files in line 22 to your files)NOTE: The dataset I have collected for mask detection contains mostly close-up images. For more long-shot images you can search online. There are many sites where you can download labeled and unlabeled datasets. I have given a few links at the bottom under Dataset Sources. I have also given a few links for mask datasets. Some of them have more than 10,000 images.Though we can make certain tweaks and changes to our training config file or add more images to the dataset for every type of object class through augmentation, we have to be careful so that it does not cause overfitting, which affects the accuracy of the model.For beginners, you can start simply by using the config file I have uploaded on my GitHub. I have also uploaded my mask images dataset along with the YOLO format labeled text files, which although might not be the best but will give you a good start on how to train your own custom detector model using YOLO. You can find a labeled dataset of better quality or an unlabeled dataset and label it yourself later.I have uploaded my custom mask dataset and all the other files needed for training a custom YOLOv4 detector on my GitHub link below.github.comwww.kaggle.comYOLOv4 custom training Tutorialtechzizou.comwww.buymeacoffee.com/techzizouwww.patreon.com/techzizouzioYou can download datasets for many objects from the sites mentioned below. These sites also contain images of many classes of objects along with their annotations/labels in multiple formats such as the YOLO_DARKNET text files and the PASCAL_VOC XML files.I have used these 3 datasets for my labeled dataset:More Mask Datasets",09/02/2021,24,142,62,"(635, 460)",22,13,0.0,58,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
196,"AI Object Detection, with Lions!",Towards Data Science,Michael Whittle,2700.0,17.0,2331,"This article was originally intended to be an introduction to AWS Rekognition. I thought I would try it out and provide a review of it. I’m a fan of AWS services. They are usually so well designed, intuitive, and a pleasure to work with. I was really disappointed by AWS Rekognition, at least the part I’m interested in which is “Custom Labels”. The name itself may not be so obvious what this is. This allows you to label and train your own model (the most useful part in my opinion). AWS does provide both image and video models for “object and scene detection”, “image moderation”, “facial analysis”, “celebrity recognition”, “face comparison”, “text in image”, and “ppe detection”. You can find information on the pricing here.I had planned on demonstrating this with those old children’s books “Where’s Waldo”? Maybe some of you will remember the books from when you were kids. On each page it has a really busy and chaotic scene and you had to find this guy called “Waldo” in the crowd. He had a red hat, glasses, and a stripy shirt. I thought this would be a great artificial intelligence demonstration using object detection. Sadly I was not able to show you the amazing results as I could not obtain the permission. This was my plan for AWS Rekognition. What I did instead was use some photos I took of lions in the Lion and Safari Park in Johannesburg, South Africa.Just bear in mind AWS Rekognition is not “cheap”. You are paying per image and sometimes by time to train and use your model. This can make a solution very costly. I’ve not looked at Google Cloud Vision but I expect it will be similar in terms of pricing.Here are the main reasons I decided not to use custom labelling in AWS Rekognition.It was at this stage I thought this isn’t for me. At least not until they fix all this. It has real potential but as an “unfinished service” I would not use it. I think they could also provide some free tier functionality for custom labels.When I finished my Applied Machine Learning course I wanted to advance my studies in AI. I have done most my further AI eduction from Medium articles. There are some really good and informative articles and others are not. I noticed especially when learning LSTM for trading, many articles have bad code in them. It seems people are just copying the same broken code without trying to understand it.I found this great article on object detection a few months back…medium.comThe walkthrough mostly works but there are quite large gaps in it. I hit several problems trying to get this working. I tried to reach out to the author but didn’t really get any answers. I’ve solved the problems myself and improved the code and now it works like a charm.I’m going to use Google Colab for this and it’s free. I actually upgraded mine to Google Colab Pro for £8.10 a month. It’s well worth the upgrade but not essential. If you use the free version you will need to tune down the “batch_size” hyperparameter in the, “pipeline.config”.In order to demonstrate this we will need some “Commercial use & mods allowed” images for training and testing. As I mentioned above, I struck out with getting permission to use some “Where’s Waldo” images. I did look on Flickr but just couldn’t find a nice set of images to work with. I eventually just used some photos I took myself just to avoid any potential copyright issues.Once we have our images ready we’ll need to use this really amazing free open-source tool called, “labelImg”. The instructions on how to install it can be found here. In order to train our model we first need to teach it where the “lion” is. “labelImg” very easily helps you to label the “lion” and add a bounding box around it. When you save the file it will include an XML file with it with all the label information for the image. The images can contain one or more labels, multiple labels, or even overlapping labels. In this case we only have one label called “lion”.As you can see I’ve worked my way through all the training and test images labelling one or more “lion”.At this stage we should have a local directory with all our images and an accompanied XML files.I have 124 images for training and testing. I put 88 images and XML files into a “train” folder and 36 images into a “test” folder. We will need these later.I have prepared one notebook for this but split it into two parts:Ideally you will only want to train your model once as it takes a really long time. With Google Colab Pro it took 7 hours, if you are using Google Colab it may take longer but it will be free at least. Once you have your model trained and exported then loading and using it is quick.Create yourself a new Google Colab notebook.Click on “Runtime”, then “Change runtime type”. Under “Hardware accelerator” select “GPU”.Another important menu option in this section is “Manage sessions” under “Runtime”. If you do experience a crash or the Google Drive session hangs you may get a “zombie session” which just won’t allow you to do anything. If you have anything like this the best way to hard reset your notebook is to go into “Manage sessions” and terminate the problematic session and start over.The first code cell in your notebook will be to install “wget”.You will then want to include the following libraries.In my notebook I have all these functions I created in one code cell but I’m going to split them out to explain what they do.This function will mount Google Drive for working with your model.This function creates the required directory structure on Google Drive. In the original article he just showed us what the directory structure should look like. This actually does it all for you :)You will notice I have used a few environment variables:This is the name of the root directory in Google Drive for your TensorFlow projects. You will see later but I called mine “TensorFlow”.This is the name of your TensorFlow object detection project. I called mine “lions”.This is the TensorFlow model we will use. We will be using “SSD ResNet50 V1 FPN 640x640 (RetinaNet50)” but you can look at others if you want here.This is a model name based on the TensorFlow model we will be using. It’s generated for you so you don’t need to specify anything here.In addition to creating all the directories it also downloads the model and creates the label map.This function is used both in training and loading the model. It builds the necessary object detection libraries.This function creates the TFRecords.This function moves some important scripts into the correct place.This function optionally starts Tensor Board for post-training analysis.This function makes the recommended changes to the, “pipeline.config” file as per the original article.IMPORTANT NOTE:The original article changed the batch size to 16. I had so many problems with this. The free Google Colab kept crashing due to memory issues. I had to reduce this way down to around 4 if not less. With Google Colab Pro I can run it at 12. If you are using the free version then try 4 and if you have problems then try 2.This is the function that trains the model.And this exports the model after completion.I put all these above functions into one code cell but just split them up to help with the explanation. The code cells are quite small in Medium. I created a GitHub Gist with the code which will be easier to review.The next code cell should look like this. It’s an important cell as it defines all our environment variables and labels used. If you have additional labels you can include them in that list. I’ve marked in bold the items which you can change if you want.The next code cell will mount your Google Drive from Google Colab. It will ask you to authorise a connection from Google Colab to Google Drive. You will need to copy the token from the link Google Colab.This will initialise your project structure. You will see the output of what it is doing. If it has been run previously then you won’t see any output.This next function needs to be run whether you are training or loading your model. It builds the object detection libraries. It can take a while.If all goes well you should see this at the end after the build has been tested.We can now include two more libraries that we have just built.Then we need to generate our TFRecords. Make sure you have copied your local images and associated XML files into your project “train” and “test” directories before running this. You can find this in the “images” directory on Google Drive.This function just moves the scripts we need into the correct place.This is an optional step but if you want you can start TensorBoard. Please note that you will only see something after training has finished.“No dashboards are active for the current data set.” is what you would expect running this for the first time. At least up until the point we are now. After you have trained your model you will see some interesting analysis here.This function makes all the necessary changes to the, “pipeline.config” as described in the original article. As I mentioned above the “batch_size” will need to be reduced if you are using the free version of Google Colab.Now the part we have been waiting for…This will take a really long time. The first time I trained the model it took 7 hours to complete and that was with half the images I’m currently training with now.You will know it’s working when you see training logging that looks like this.This is it finishing the last of the 25000 steps!Once your model has trained you will want to export it so it can be loaded later.If all has gone to plan you should see this at the end.At this stage we should have our trained model exported and ready to go.There are some code cells/steps duplicated from the training section of this article. I’ve deliberately split them out because in most cases you will just be wanting to use your model.As usual the first step is to import our libraries.Then we define our environment variables. I’ve highlighted in bold the three parameters that could potentially be edited. In terms of this demonstration you would leave them as below.I have one code cell with all the functions I created. I’m going to split them out here to explain them but you probably want to leave them all in one cell.We will need this function again to mount your Google Drive.The build function is also exactly the same as the one we used in the training part of this article.This function will load our model. It will return a tuple with the detection function, “detect_fn” and category index, “category_index”. These will be inputs for the, “process_image” function.This helper function will convert an Image to a Numpy array.This function will display our processed image. This function may need some explaining. The first two arguments, “detect_fn” and “category_index” are outputs of the “load_module” function. “img_path” is the location of the input image. The output image scale is quite small in Google Colab. I’ve added “img_scale” which allows you to increase the size as a float. For example you can scale the image by 1.5x with 1.5. “crop” is another optional parameter which will crop and display the detected bounding box.There are some important points to mention here. The first is that for each image many detections may be found with varying degrees of accuracy. We don’t want to see any detections with a score less than 50%. If you look at the function below you will see “min_score_thresh”. This is the hyperparameter to say we are only interested in high scoring detections.It’s also worth mentioning that the detections are all normalised between 0 and 1. For example if an image is 1024x768 and a label starts in the middle you will see the x=0.5 and y = 0.5 not x=512 and y=384. In order to find the absolute location we need to find the actual dimensions of the image and then multiply them by the normalised value.The code can be quite difficult to read here, so I created a GitHub Gist for your convenience.In order to use these functions our next cell will be to mount our Google Drive, build our additional libraries, and include our built libraries (just like we did in the training step)Then build our additional libraries.Then include our built libraries.We will then load our module which as discussed will return a tuple which we will need next.And we process our image…As you can see it found one lion with 100% confidence, and a second lion at the back with 79% confidence.I hope you have found this enhancement of the original article useful. I’ve tried to cover the gaps, resolve most issues, and try and automate as much as possible. I’ve used this process for several different personal and professional projects now and it works pretty well. Two potential issues you may face is disk space in Google Drive (it requires several free Gb) and resources in Google Colab. The resourcing issue in Google Colab is a fairly simple fix as you can tune down the, “batch_size” and that should fix that for you. You also need a little patience as it can take hours to train a model :)Python is indent sensitive and I appreciate showing the code in a Medium article doesn’t always format the way it was intended. If you would like me to share the workbook with you let me know in the comments.If you enjoyed this article I recommend you take a look at the following articles:medium.comlevelup.gitconnected.comlevelup.gitconnected.comlevelup.gitconnected.comtowardsdatascience.com",12/07/2021,34,89,6,"(700, 242)",8,7,0.0,35,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,positive,joy/calmness
197,Introducing a new way to visually search on Pinterest,Pinterest Engineering Blog,Pinterest Engineering,41000.0,3.0,579,"Andrew Zhai| Pinterest engineer, Visual SearchDiscovery products at Pinterest are built on top of Pins. Last year, we introduced Guided Search, a feature built on top of understanding Pins’ descriptions. Before that, we launched Related Pins, a service built on top of understanding Pin to board connections. Though we’ve been able to use these Pinner curated signals to build new products and features, there’s one signal within every Pin we haven’t been able to utilize, a Pin’s image — until now.Tomorrow we’re rolling out a visual search tool that lets you zoom in on a specific object in a Pin’s image and discover visually similar objects, colors, patterns and more. For example, see a lamp in a Pin of a living room that you’re interested in? Tap the search tool in the corner of a Pin, drag the zoom tool over the lamp and scroll down for visually similar Pins.The core of our visual search system is how we represent images, and was built in just a few months by a team of four engineers. With close collaboration with members of the Berkeley Vision and Learning Center, we use deep learning to learn powerful image features by utilizing our richly annotated dataset of billions of Pins curated by Pinners. These features can then be used to compute a similarity score between any two images. For the past couple of months, we’ve been experimenting with improving Related Pins with these visual signals, as detailed in our latest white paper, released today.To find visually similar results for a Pin, we consider the similarity scores of a given feature to billions of other features. In order to do this task efficiently, we built a distributed index and search system (using open-source tools) that allows us to scale to billions of images and find thousands of visually similar results in a fraction of a second. We’ll be releasing a paper describing our findings in building a large scale visual search system using deep learning features in the near future. For more information on our previous work, please refer to our KDD’15 paper.Visual search allows people to use images to search. There are dozens of interesting items within a Pin’s image; we want to give Pinners a tool to learn more about these items. By specifying a part of the image you’re interested in using a cropping tool, we can recommend visually similar results in real time. We optimize on visual similarity, not just duplicates to power Pinners to discovery exact results, as well as unexpected results that may be similar in style or pattern or shape.By incorporating a new visual search experience into Pinterest, we hope to give Pinners another way to discover ideas and products. The visual search tool starts rolling out tomorrow to Pinners globally on all platforms (iOS, Android and web), and is just the first step. The more people Pin, the better the technology will become. Keep an eye out for more visual search updates. If you’re interested in helping us build visual discovery and search technology, join our team!Acknowledgements: This product is a joint effort by members of the Visual Discovery team (Dmitry Kislyuk, Jeff Donahue, Eric Tzeng, David Liu and Kevin Jing), the Discovery Product team (Kelei Xu, Luna Ruan, Vishwa Patel, and Naveen Gavini), the Product Design team (Patrik Goethe, Albert Pereta Farre), Mike Repass, and the GTM team. We’d also like to thank Jeff Donahue, Trevor Darrell and Eric Tzeng from the Berkeley Caffe team.",08/11/2015,0,0,1,"(700, 467)",1,0,0.0,6,de,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,subjective,positive,trust/acceptance
198,Step by step -Understand the architecture of Region proposal network (R-CNN),,Pallawi,,9.0,1783,"This blog is written to explain the evolution of object detection models in simple words and self-explanatory diagrams. This blog can be helpful to every individual who is entering into the field of computer vision and data science or has taken up a project which requires solving an object detection problem.We all must have heard about Faster R-CNN and there are high chances that you found this blog when you searched for the keyword “Faster R-CNN” as it has been among the state of arts used in many fields since January 2016.A strong object detection architecture like Faster RCNN is built upon the successful research like R-CNN and Fast R-CNN. To honestly enjoy working, troubleshooting and pursuing the dream of creating your own model which can one day be called a state of art my friend I would always recommend reading the research papers in chronological order.Therefore I have tried my best to explain all of the three architectures so that we do not miss on the basics. One day we will build our architecture and contribute to the field we are passionate about.I will also talk about the stories of my struggles while reading these papers and the attitude we must have to read fearlessly. To keep the spirit high, I want to state the quote which kept me motivated throughout reading the papers and finally writing this blog for you all. Reading is important because if you can read, you can learn anything about everything and everything about anything.-Tomie DepaolaThe paper that talks about R-CNN is Rich feature hierarchies for accurate object detection and semantic segmentation which is popularly known as R-CNN. It was published in the year 2014.How it all started?In the year 2004 papers like Distinctive Image Features from Scale-Invariant Keypoints and Histograms of Oriented Gradients for Human Detection describes the use of SIFT and HOG features to address visual recognition task. But if we look at performance on the canonical visual recognition task, PASCAL VOC object detection challenge, it is generally acknowledged that progress has been slow during 2010–2012, with small gains obtained by building ensemble systems and employing minor variants of successful methods.Then in the year, 2012 ImageNet Classification with Deep Convolutional Neural Networks made its mark in the field of CNN’s by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Their success resulted from training a large CNN on 1.2 million labelled images, together with a few twists on CNN (e.g., max(x; 0) rectifying nonlinearities and “dropout” regularization).The significance of the ImageNet result was vigorously debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?The R-CNN paper answers this question by bridging the gap between image classification and object detection. This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.A challenge faced in detection is that labelled data is scarce and the amount currently available is insufficient for training a large CNN.The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning. The second principle contribution of this paper is to show that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain-specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. During the experiments, fine-tuning for detection improved the mAP performance by 8 percentage points. After fine-tuning, R-CNN achieved an mAP of 54% on VOC 2010 compared to 33% for the highly-tuned, HOG-based deformable part model (DPM). The authors of R-CNN also mentions the paper DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition which is a black box feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaptation.As R-CNN operates on regions it is natural to extend it to the task of semantic segmentation. With minor modifications, the authors also achieved competitive results on the PASCAL VOC segmentation task, with an average segmentation accuracy of 47.9% on the VOC 2011 test set.We know that recognition occurs several stages downstream, which suggests that there might be hierarchical, multi-stage processes for computing features that are even more informative for visual recognition.To achieve this result, R-CNN focused on two problems: localizing objects with a deep network and training a high-capacity CNN model with only a small quantity of annotated detection data. In order to maintain high spatial resolution, CNNs those days typically had only had two convolutional and pooling layers.Units high up in R-CNN network, which had five convolutional layers, have very large receptive fields (195 x195 pixels) and strides (32x32 pixels) in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge.R-CNN solve the CNN localization problem by operating within the “recognition using regions” paradigm, which has been successful for both object detection and semantic segmentation.R-CNN generates 2000 category-independent region proposals for the input image, these 2000 regions are proposed using the selective search.Selective Search is a method for finding a large set of possible object locations in an image, independent of the class of the actual object. It works by clustering image pixels into segments and then performing hierarchical clustering to combine segments from the same object into object proposals.The selective search uses multiple image processing algorithms to output these proposals.Input — RGB imageOutput — 2000 proposed regions which consist of background and the object classesThese regions are converted to bounding boxes. These bounding boxes have precise information about pixel locations of proposed regions on the imageRegardless of the size or aspect ratio of the candidate region, the authors warped all pixels in a tight bounding box to the required size. A simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region’s shape.Input — 2000 proposed regions which consist of background and the object classesOutput- 2000 warped images of fixed sizeDuring those times when we had limited domain-specific annotated dataset and computation capabilities, the authors adopted a method called as Supervised pre-training where they pre-trained the CNN on a large auxiliary dataset (ILSVRC2012 classification) using image-level annotations only (bounding box labels were not available for this data). This could be considered as a way to use pre-trained weights.Input — Classification dataset which exhibited feature similarities with object classes to be used for Domain-specific training and has ample amount if data to be trained.Output- A CNN when given an image can classify those images into classes based on the object that it has in those images.After the Supervised pre-training, the next step was a domain-specific fine-tuning. where the pre-trained CNN was trained with the 2000 fixed size warped images.Each warped bounding box is then forward propagated through a CNN network. During the forward propagation, a convolution neural network does feature extraction from each of the proposed regions. So from every input image, 2000 warped images would enter the CNN sequentially (one at a time). Now one can imagine the time the network will consume to process a single image.Then features are extracted by CNN and warped into a fixed-length feature vector. Then each fixed-length feature vector is classified using category-specific linear Support vector machine(set of class-specific linear SVMs).To fine-tune the CNN they used stochastic gradient descent (SGD) at a starting learning rate of 0.001. During the training, they treated all region proposals (warped images) with 0.5 IoU overlap with a ground-truth box as positives for that box’s class and the rest as negatives.During backpropagation, they uniformly sample(sampling is called as a process of picking data from a pool of dataset) 32 positive windows (overall classes) and 96 background windows to construct a mini-batch of size 128. They biased the sampling towards positive windows because they are extremely rare compared to the background.Once the domain-specific CNN is trained and we start getting the classification and bounding boxes. But we know that the layers of the CNN were not efficient to deliver high precision and recall. So to make the model robust SVM’s were trained. The SVM was trained only after the domain-specific CNN was trained with satisfactory performance.To train the SVM the 2000 regions were proposed by selective search, then these regions were warped and then sent through the trained domain-specific CNN which extracts features from 2000 warped regions and then convert them into feature vectors. These feature vectors (output from CNN) were used as input to train SVM. The confidence threshold that was used to train SVM as a positive and negative object class was 0.3. The trained CNN would output the class of every feature vector. Based on this prediction information by CNN the vectors would be used as input to train class specific linear SVM's. One SVM for one class. Example: If a CNN classifies the feature vector belonging to a class “cat” then that feature vector would be used to train the CAT SVM.During training the class-specific SVM’s a class-specific bounding box regressor is also trained simultaneously. The input to the regressor is the feature vectors computed by domain-specific CNN. The regressor helps in predicting tight bounding boxes.SVM input- Class-specific feature vectors predicted by the trained domain-specific model.SVM Output — Precisely classified feature vectorsStep-1 Selective search on the test image to extract around 2000 region proposals.Step-2 Each proposal is warped and forward propagate it through the domain-specific CNN in order to compute features.Step-3 Then, for each class, the authors score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-over union (IoU) overlap with a higher scoring selected region larger than a learned threshold.Hope I was able to help you understand how R-CNN works. I have taken some efforts to create self-explanatory diagrams. I searched and read R-CNN from many sources but always found a very high-level diagram with precise literature. But since there has bee so much advancement done over this model that many people read and write about it. I have tried my best to formulate it on this page. Certainly, this model is not used these days but if you want to build your own model someday I would highly recommend you to read all the initial days research paper. You can read about other models like Fast, Faster-RCNN, YOLO V1, V2, V3 and Single-shot detector in my next blogs.If you found this blog helpful, please give a clap so that others may find it.d2l.aitowardsdatascience.com",05/09/2020,0,12,0,"(654, 384)",4,0,0.0,16,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
199,Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image Classification: From Microsoft to Facebook [Part 1],,Prakash Jay,1940.0,6.0,631,"This is Part 1 of two-part series explaining blog post exploring residual networks.We will review the following three papers introducing and improving residual network:When deeper networks starts converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated and then degrades rapidly.Let us take a shallow network and its deeper counterpart by adding more layers to it.Worst case scenario: Deeper model’s early layers can be replaced with shallow network and the remaining layers can just act as an identity function (Input equal to output).Rewarding scenario: In the deeper network the additional layers better approximates the mapping than it’s shallower counter part and reduces the error by a significant margin.Experiment: In the worst case scenario, both the shallow network and deeper variant of it should give the same accuracy. In the rewarding scenario case, the deeper model should give better accuracy than it’s shallower counter part. But experiments with our present solvers reveal that deeper models doesn’t perform well. So using deeper networks is degrading the performance of the model. This papers tries to solve this problem using Deep Residual learning framework.Instead of learning a direct mapping of x ->y with a function H(x) (A few stacked non-linear layers). Let us define the residual function using F(x) = H(x) — x, which can be reframed into H(x) = F(x)+x, where F(x) and x represents the stacked non-linear layers and the identity function(input=output) respectively.The author’s hypothesis is that it is easy to optimize the residual mapping function F(x) than to optimize the original, unreferenced mapping H(x).If the identity mapping is optimal, We can easily push the residuals to zero (F(x) = 0) than to fit an identity mapping (x, input=output) by a stack of non-linear layers. In simple language it is very easy to come up with a solution like F(x) =0 rather than F(x)=x using stack of non-linear cnn layers as function (Think about it). So, this function F(x) is what the authors called Residual function.The authors made several tests to test their hypothesis. Lets look at each of them now.Take a plain network (VGG kind 18 layer network) (Network-1) and a deeper variant of it (34-layer, Network-2) and add Residual layers to the Network-2 (34 layer with residual connections, Network-3).Designing the network:There are two kinds of residual connections:2. When the dimensions change, A) The shortcut still performs identity mapping, with extra zero entries padded with the increased dimension. B) The projection shortcut is used to match the dimension (done by 1*1 conv) using the following formulaThe first case adds no extra parameters, the second one adds in the form of W_{s}Results:Even though the 18 layer network is just the subspace in 34 layer network, it still performs better. ResNet outperforms by a significant margin in case the network is deeperThe following networks are studiedEach ResNet block is either 2 layer deep (Used in small networks like ResNet 18, 34) or 3 layer deep( ResNet 50, 101, 152).Pytorch Implementation can be seen here:github.comThe Bottleneck class implements a 3 layer block and Basicblock implements a 2 layer block. It also has implementations of all ResNet Architectures with pretrained weights trained on ImageNet.I have a detailed implementation of almost every Image classification network here. A Quick read will let you implement and train ResNet in fraction of seconds. Pytorch already has its own implementation, My take is just to consider different cases while doing transfer learning.becominghuman.aiI wrote a detailed blog post of Transfer learning. Though the code is implemented in keras here, The ideas are more abstract and might be useful to you in prototyping.towardsdatascience.comPlease share this with all your Medium friends and hit that clap button below to spread it around even more. Also add any other tips or tricks that I might have missed below in the comments!",07/02/2018,0,17,5,"(512, 282)",13,6,0.0,9,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,surprise/amazement
200,A Guide to exploit Random Forest Classifier in PySpark,Towards Data Science,Manusha Priyanjalee,60.0,6.0,855,"In this article, I am going to give you a step-by-step guide on how to use PySpark for the classification of Iris flowers with Random Forest Classifier.I have used the popular Iris dataset and I have provided the link to the dataset at the end of the article. I used Google Colab for coding and I have also provided Colab notebook in Resources.Pyspark is a Python API for Apache Spark and pip is a package manager for Python packages.With the above command, pyspark can be installed using pip.First, I need to create an entry point into all functionality in Spark. SparkSession class is used for this. SparkSession.builder() creates a basic SparkSession. To set a name for the application use appName(name). Here I have set ‘ml-iris’ as the application name. Setting a name is not necessary and if it is not set, a random name will be generated for the application. getOrCreate() creates a new SparkSession if there is no existing session. Otherwise, it gets the existing session.spark.read.csv(“path”) is used to read the CSV file into Spark DataFrame. A Data Frame is a 2D data structure and it sets data in a tabular format. Iris dataset has a header, so I set header = True, otherwise, the API treats the header as a data record. inferSchema attribute is related to the column types. By default, inferSchema is false. It will give all columns as strings. Here I set inferSchema = True, so Spark goes through the file and infers the schema of each column. printSchema() will print the schema in a tree format.Output:pandas is a toolkit used for data analysis. Here df.take(5) returns the first 5 rows and df.columns returns the names of all columns. DataFrame.transpose() transpose index and columns of the DataFrame. It writes columns as rows and rows as columns.Output:df.dtypes returns names and types of all columns. Here we assign columns of type Double to numeric_features. select(numeric_features) returns a new Data Frame. describe() computes statistics such as count, min, max, mean for columns and toPandas() returns current Data Frame as a Pandas DataFrame.Output:Since we have a good idea about the dataset we are working with now, we can start feature transforming. Feature transforming means scaling, converting, and modifying features so they can be used to train the machine learning model to make more accurate predictions. For this purpose, I have used String indexer, and Vector assembler.First, I have used Vector Assembler to combine the sepal length, sepal width, petal length, and petal width into a single vector column. Here the new single vector column is called features.Output:Then I have used String Indexer to encode the string column of species to a column of label indices. By default, the labels are assigned according to the frequencies. So, the most frequent species gets an index of 0.Output:As you can see, we now have new columns named labelIndex and features.The bottom row is the labelIndex. We can see that Iris-setosa has the labelIndex of 0 and Iris-versicolor has the label index of 1. And Iris-virginica has the labelIndex of 2.Now we have transformed our features and then we need to split our dataset into training and testing data.randomSplit() splits the Data Frame randomly into train and test sets. Here I set the seed for reproducibility. 0.7 and 0.3 are weights to split the dataset given as a list and they should sum up to 1.0.Output:Now we can import and apply random forest classifier. Random forest is a method that operates by constructing multiple decision trees during the training phase. The decision of the majority of the trees is chosen by the random forest as the final decision.It comes under supervised learning and mainly used for classification but can be used for regression as well. Random forest classifier is useful because,Here featuresCol is the list of features of the Data Frame, here in our case it is the features column. labelCol is the targeted feature which is labelIndex. rf.fit(train) fits the random forest model to our input dataset named train. rfModel.transform(test) transforms the test dataset. This will add new columns to the Data Frame such as prediction, rawPrediction, and probability.Output:We can clearly compare the actual values and predicted values with the output below.Output:Now we have applied the classifier for our testing data and we got the predictions. Then we need to evaluate our model.MulticlassClassificationEvaluator is the evaluator for multi-class classifications. Since we have 3 classes (Iris-Setosa, Iris-Versicolor, Iris-Virginia) we need MulticlassClassificationEvaluator. The method evaluate() is used to evaluate the performance of the classifier.Output:Now we can see that the accuracy of our model is high and the test error is very low. It means our classifier model is performing well.We can use a confusion matrix to compare the predicted iris species and the actual iris species.MulticlassMetrics is an evaluator for multiclass classification in the pyspark mllib library.Output:According to the confusion matrix, 44 (12+16+16) species are correctly classified out of 47 test data. 3 species are incorrectly classified.I hope this article helped you learn how to use PySpark and do a classification task with the random forest classifier. I have provided the dataset and notebook links below.Happy Coding! 😀Datasetwww.kaggle.comColab Notebookcolab.research.google.com",01/06/2021,6,0,0,"(562, 324)",13,1,0.0,4,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,anger/irritation
201,Exploding Gradient and Vanishing Gradient,,Schiff0404,,2.0,348,"Deep Learning mengacu pada training neural network dengan lebih dari dua layer non-output. Di masa lalu, semakin sulit untuk melakukan training network jika jumlah layer semakin besar. Dua masalah yang menjadi tantangan terbesar disebut sebagai masalah meledaknya gradient (exploding gradient) dan menghilangnya gradient (vanishing gradient) karena gradient descent digunakan untuk melatih parameter jaringan.Untuk masalah exploding gradient lebih mudah untuk diatasi dengan menerapkan teknik sederhana seperti gradient clipping dan L1 atau L2. Regularisasi, masalah vanishing gradient tetap sulit dipecahkan beberapa decade ini.Apa itu vanishing gradient dan mengapa itu muncul? Untuk meperbaharui nilai-nilai parameter dalam neural network pada algoritma yang disebut backpropagation yang biasanya digunakan. Backpropagation adalah algoritma yang efisien untuk menghitung gradient pada Neural Network menggunakan chain rule. Selama gradient descent, parameter neural network menerima pembaharuan yang sebanding dengan turunan parsial dari fungsi biaya sehubungan dengan parameter saat ini disetiap iterasi pelatihan. Masalahnya adalah bahwa dalam beberapa kasus , gradient akan semakin kecil, secara efektif mencegah beberapa parameter mengubah nilainya. Dalam kasus terburuk, ini mungkin benar-benar menghentikan neural network untuk melakukan training lebih lanjut.Fungsi aktivasi tradisional, seperti fungsi tangen hiperbolik(hyperbolic tangent), memiliki gradient dalam kisaran(0,1), dan backpropagation menghitung gradient berdasarkan chain rule. Hal ini mempengaruhi perkalian n dengan angka-angka kecil ini untuk menghitung gradient dari layer sebelumnya(paling kiri) dalam n-layer network. Yang berarti bahwa gradient berkurang secara exponensial dengan n. hal ini menyebabkan layer sebelumnya melatih dengan sangat lambat.Namun, implementasi modern dari algoritma pembelajaran neuran network memungkinkan anda untuk secara efektif melatih neural network (hingga ratusan lapisan). Fungsi aktivasi ReLU mendapatkan masalah vanishing gradient jauh lebih sedikit. Juga, Long Short-Term Memory(LSTM) network digunakan dalam residual neural network memungkinkan untuk melakukan training neural network lebih dalam(deeper) dengan ribuan layer.Oleh karena itu, dengan masalah vanishing dan exploding gradient sebagian besar telah terpecahkan (atau efeknya telah berkuran) untuk sebagian besar, istilah “Deep Learning” mengacu pada pelatihan neural network menggunakan toolkit algoritmik dan matematika modern terlepas dari seberapa dalam(deep) neural network. Dalam praktiknya, banyak masalah bisnis dapat diselesaikan dengan neural network yang memiliki 2–3 layer antara input dan output layer. Layer yang bukan input atau output disebut hidden layer.Sumber:Burkov, A. (2019). Machine Learning.",30/04/2019,0,0,1,"(700, 395)",1,0,0.0,0,id,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
202,From the Design Team: Our First Step Toward Connected Buildings,Verdigris Tech,Daniela Li,65.0,4.0,599,"Building operators are busy, busy people. The bulk of their time is spent responding to urgent requests and fighting fires: occupant complaints, sudden breakdowns in heating and cooling, or an unexpected blackout from downed power lines. From dozens of conversations that we’ve had with facility managers, general managers, and chief engineers, we know they aren’t able to proactively address problems and run their buildings the way they would like to.What if their building could talk to them, and tell them about the problems that are about to happen? Imagine if that building could regulate and fix itself! Not just adjust room temperature, but increase ventilation when there’s an influx of people, or notify a building operator that a motor will burnout soon. That’s the vision of connected buildings.The truth is that we are still pretty far away from that vision. How do we take the first steps to realizing that future?At Verdigris, we started by listening to our customers. We visited and interviewed about a dozen building operators to learn about their workflow, daily problems, and existing solutions. How do they find out about building problems now? What kind of information do they get? When? How is it useful? How is it not useful? How do they feel about these solutions? How do they then address these problems?We also pooled our internal knowledge, through mini design sprints among the Design, Sales, Marketing, and Customer Service teams. Lastly, we looked at what other companies were doing in the world of building management systems and real-time alerts.From this research, we discovered that we fit into the frameworks of building fault detection and diagnostics (FDD), predictive maintenance, and BMS setpoints. Navigant Research published a great diagram on their point of view on maturity among next-generation building management systems that strongly influenced our thinking on how we at Verdigris approach the path to connected buildings.We followed 3 guiding principles based on insights into the lives of building operators:1. Mobile First: building managers and engineers are always on the go2. Action Oriented: our customers are busy — they just need to know what to do — so we try to not overwhelm them with excess data3. Just Enough Alerting: email is their preferred method of communication, but these people get pinged all day long. If the same alert comes too often, it will be ignoredOur resulting product is the Verdigris Energy Tracker, a responsive web application for real-time energy alerts.Customers set power thresholds (like a BMS setpoint) on the relevant equipment or building system of their choosing. They then receive alerts whenever power exceeds or falls below the threshold, so they can learn of new or impending problems before someone else reports it (like FDD and predictive maintenance). We also decided to limit the number of alerts per problem to once an hour, knowing that most building operators wouldn’t have time to respond to a single problem with greater frequency (that whole busy busy thing).For customers who are able to connect their smart meters to our system, Energy Tracker also gives them a 24 hour forecast of the building’s demand and will alert them if the projected demand is too high.In the immediate next iteration of Energy Tracker, we will offer a way to automatically create moving setpoints based on our machine learning algorithms — our system will be able to identify unusual equipment activity, with minimal input from building operators. Responsiveness and building controls are also part of our plans for Energy Tracker.It’s an exciting start and our first step towards realizing the connected buildings of tomorrow.Want a demo of the Tracker app?",24/12/2014,0,7,1,"(583, 401)",4,0,0.0,4,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,expectation/interest
203,Few Shot Learning in NLP With USE and Siamese Networks (Code Walkthrough),The Startup,koushik konwar,14.0,5.0,828,"Annotated data has always been a challenge for supervised learning. Over the time people has experimented various way as to how to make a model learn from only a handful of samples per class. Few-shot learning refers to understanding new concepts from only a few examples. This blog post aims at developing a few shot NLP model leveraging transfer learning and siamese networks with triplet loss. After reading this blog you will be able to develop your own few shot NLP model for text classification.Table of contents1.Introduction2.Siamese Network3.Triplet Loss4.Universal Sentence Encoder5.Code Walkthrough in keras 6.Future scope and referencesThis blog is about how we can train our models for NLP tasks with very few training data points. In this blog I would demonstrate how we can leverage the siamese networks and triplet loss ( Which were initially used for computer vision tasks ) along with transfer learning to achieve good results with very few data points in classification task of NLP. This blog contains code walkthrough along with the theory related to it. After finishing this blog you will be able to create your own few shot learning model with siamese networks and triplet loss . So let’s dive in.Siamese networks are similar neural networks where input vectors are passed through it to extract features , which are later passed through the triplet loss in the few shot learning process. These networks are initially used for computer vision tasks but the same idea can be extended to text classification too . The pictorial representation of siamese networks for text classification using triplet loss is as follows.**You can look the the below two resources for in-depth knowledge in siamese networks and triplet loss**The triplet loss takes three input embeddings of an anchor, positive and negative data points. The anchor and positive embeddings are of same class and negative embedding is of different class. We try to project the embeddings such that the distance of anchor to negative is alpha more than the distance from anchor to positive. Alpha is also know as the margin, if the difference of distance is greater than the margin than the loss is zero otherwise the difference in distance is considered as the triplet loss and the loss is back-propagated through the siamese network.The mathematical formulation of the loss can be seen below.The universal-sentence-encoder is a sentence encoding module of tensorflow-hub. We will be using the pre-trained model to create embeddings for our sentences. It encodes sentences in to high dimensional embeddings which can be further used for text classification, semantic similarity, clustering and other natural language tasks.The embeddings vector is 512 length, irrespective of the length of the input.We will use this pre-trained Universal sentence encoder to encode our sentences to get better representation of our sentences leveraging transfer learning.**You can refer the below mentioned link for in depth knowledge of Universal sentence encoders.**arxiv.orgI will consider this dataset in this blog post. The dataset has 5 classes with 20 samples each in the training file containing a total of 100 samples and a total of 3277 samples in the test file. You can fine the dataset here.1.Loading the data and selecting the relevant columns( For this tutorial I am not doing any pre processing, you are free to apply your own pre processing function)2.Importing and loading Universal sentence encoder3.Constructing the siamese modelAfter getting the sentence embeddings from universal sentence encoder I will pass the embedding through the siamese model . I have use a simple network with few dense layers, U can play with the network to play optimum results for your custom dataset.4. Triplet Loss and Training NetworkI have constructed the triplet loss and returned the sum of the losses, you can also take its mean. While taking mean u must ensure that u are doing hard negative mining of the triplets(anchor, positive and negative) selected.The concept of Hard negative mining is well explained in the reference videos shared above.5. Constructing a data generatorThe data generator will use the Universal sentence encoder to encode the sentences and pass those encodings to the network6. Now we have everything setup . so, let’s train the model7. After training the model to 100 epochs , let’s test the model on the test data. I have used KNN and SVM to classify the test dataset.Since the accuracy of KNN is slightly better, lets look at its classification report8. T-SNE Visualisation of the test dataset.As we can see the test dataset has 5 distinct clusters in the TSNE representation. The representation of the clusters were learned from only 20 samples per class in the training dataset. Isn’t it AMAZING!!I plan to explore different methodologies of few shot learning in my upcoming blog post. Clearly we can use terminologies from computer vision to NLP related tasks and vice versa to enhance our capabilities in both the domains. Here I am attaching the link from where I first came to know about siamese networks and triplet loss.Motivation and references :github.com",01/10/2020,2,16,1,"(700, 422)",4,1,0.0,7,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
204,Building an Indian Chaat classifier,,Ishan Nangia,47.0,6.0,876,"Hey. I built a CNN model to classify the different kinds of Indian chaats and now so can you !Note: This content is based on the 1st and 2nd lesson of the Fast.ai course : Practical Deep Learning for Coders, v3I built my classifier using the fastai library for python and Google Colab. Google Colab allows us to use a Jupyter Notebook type framework with free GPU compute. Training CNNs would just take too much of time without a GPU.You can find my code(jupyter notebook) here and if you want to set up Colab with fastai library, go here.For those who don’t know, ‘Chaat’ is just a type of Indian street food. There are many different kinds of chaats. I thought it would be fun to build this classifier as :a. I love all types of chaats !b. A lot of them have a lot of common ingredients which should make it hard to differentiate between them. Also, often a lot of curd and other stuff is dumped on the chaat which makes it tough to find out which chaat it is.I have taken the following types of chaats into consideration:-We have a total of 8 different chaats and thus 8 different classes. Okay…So here we go:We import the following libraries and modules for their various functions as stated in the comments in the below picture.We also update the fastai library to make sure we are using the latest versionBelow we import all the functions from fastai.vision as it becomes really easy to use them this way. We also set a random seed so that our code becomes repeatable. If you are more curious about the function and want to know how it becomes repeatable, go here.Our dataset will consist of the images that we want to classify. Thus to download the images we will use a neat trick shown in the Fast.ai course. Lets say we need the images for ‘Aloo Chaat’. Then we will first google ‘aloo chaat’ and open the images page. Next we will press ‘ctrl+shift+j’ to open the console. Now pasting the below code and pressing enter will allow us to download a file with the urls to the images present in the google images page.urls = Array.from(document.querySelectorAll(‘.rg_di .rg_meta’)).map(el=>JSON.parse(el.textContent).ou); window.open(‘data:text/csv;charset=utf-8,’ + escape(urls.join(‘\n’)));We will use these files to download our images soon.Now we enter all our classes in the list ‘classes’. We do this as we will be looping through these next.We would now like to make the images folder so that we can setup the folders for different classes containing the pictures for those classes in this. Thus this folder will have different subfolders with their names being the different ‘chaats’(as in the list classes). Then we will download each class’ images to their respective folders.Then we use the following loop which iterates through all the classes and has been described below:For each class, this loop first makes a folder in the images directory by that class’ name. Then it allows you to upload the file with the links to the images to your Colab notebook provided you have renamed the file to the class’ name. Finally, it moves the file to the folder for that class.Below we use another loop that loops through all the classes and for each class it first downloads the images using that file that we had earlier placed in each class’ folder and then verifies that the image files aren’t broken or aren’t corrupted. The images that couldn’t be downloaded properly are removed using the function ‘verify_images’.Now we have all our images with us. To train a model we first create a DataBunch object that essentially organizes the images into training and validation sets and allows us to apply transforms on them.Above, we feed in the path to our data and tell the function that training data is in our ‘images’ folder. We also create a validation dataset that is only 30% of the entire dataset and use get_transforms() to get the default transforms. The size of the images is made to be 224 for all dimensions and normalize uses the imagenet model’s means and std deviations for all three(RGB) layers to normalize the images using them. ‘num_workers’ is used to allocate the number of processes running parallel to generate batches. This ensures that the CPU computations are managed properly. TBH: I don’t understand the exact meaning and use of num_workers.For this tutorial, we will just use a Resnet model. We will choose a resnet34 which is a pretty good and basic pre-trained model to start classifying with. We use ‘cnn_learner’ to create a CNN model with the DataBunch object that we had created earlier, the model we have chosen and the metrics we want.These metrics, accuracy and error_rate, will be displayed when the model is trained. You can also ignore this parameter completely and not mention any metrics.Now we run our last line for this tutorial to use the fit_one_cycle method for 5 epochs and we will see the different metrics being printed out. This will fit the model on our dataset using the ‘one cycle policy’.Ta-Da ! Congrats ! You can classify different Indian chaats correctly only…. around 51% times. That’s sad and that is definitely not what we wanted.Buh-Bye till then !",05/07/2019,0,11,5,"(620, 284)",21,8,0.0,7,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,positive,joy/calmness
205,Video of a neural network learning,Deep Learning 101,Milo Spencer-Harper,2400.0,4.0,692,"As part of my quest to learn about AI, I generated a video of a neural network learning.Many of the examples on the Internet use matrices (grids of numbers) to represent a neural network. This method is favoured, because it is:However, it’s difficult to understand what is happening. From a learning perspective, being able to visually see a neural network is hugely beneficial.The video you are about to see, shows a neural network trying to solve this pattern. Can you work it out?It’s the same problem I posed in my previous blog post. The trick is to notice that the third column is irrelevant, but the first two columns exhibit the behaviour of a XOR gate. If either the first column or the second column is 1, then the output is 1. However, if both columns are 0 or both columns are 1, then the output is 0.So the correct answer is 0.Our neural network will cycle through these 7 examples, 60,000 times. To speed up the video, I will only show you 13 of these cycles, pausing for a second on each frame. Why the number 13? It ensures the video lasts exactly as long as the music.Each time she considers an example in the training set, you will see her think (you will see her neurons and her synaptic connections glow). She will then calculate the error (the difference between the output and the desired output). She will then propagate this error backwards, adjusting her synaptic connections.Green synaptic connections represent positive weights (a signal flowing through this synapse will excite the next neuron to fire). Red synaptic connections represent negative weights (a signal flowing through this synapse will inhibit the next neuron from firing). Thicker synapses represent stronger connections (larger weights).In the beginning, her synaptic weights are randomly assigned. Notice how some synapses are green (positive) and others are red (negative). If these synapses turn out to be beneficial in calculating the right answer, she will strengthen them over time. However, if they are unhelpful, these synapses will wither. It’s even possible for a synapse which was originally positive to become negative, and vice versa. An example of this, is the first synapse into the output neuron — early on in the video it turns from red to green. In the beginning her brain looks like this:Did you notice that all her neurons are dark? This is because she isn’t currently thinking about anything. The numbers to the right of each neuron, represent the level of neural activity and vary between 0 and 1.Ok. Now she is going to think about the pattern we saw earlier. Watch the video carefully to see her synapses grow thicker as she learns.Did you notice how I slowed the video down at the beginning, by skipping only a small number of cycles? When I first shot the video, I didn’t do this. However, I realised that learning is subject to the ‘Law of diminishing returns’. The neural network changes more rapidly during the initial stage of training, which is why I slowed this bit down.Now that she has learned about the pattern using the 7 examples in the training set, let’s examine her brain again. Do you see how she has strengthened some of her synapses, at the expense of others? For instance, do you remember how the third column in the training set is irrelevant in determining the answer? You can see she has discovered this, because the synapses coming out of her third input neuron have almost withered away, relative to the others.Let’s give her a new situation [1, 1, 0] to think about. You can see her neural pathways light up.She has estimated 0.01. The correct answer is 0. So she was very close!Pretty cool. Traditional computer programs can’t learn. But neural networks can learn and adapt to new situations. Just like the human mind!How did I do it? I used the Python library matplotlib, which provides methods for drawing and animation. I created the glow effects using alpha transparency.You can view my full source code here:github.comThanks for reading!If you enjoyed reading this article, please click the heart icon to ‘Recommend’.",11/08/2015,0,1,0,"(695, 502)",4,1,0.0,4,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
206,Pre-clustering Google Maps Markers using KMeans in Django,,Peter Adam,80.0,6.0,865,"Recently, I was attempting to display many (>1000) markers on an interactive Google Maps V3 API, with marker clustering turn on, so that above a certain zoom level, markers would look like this:instead of this:Google makes this pretty easy, using the MarkerClusterer library. However, if you’re using a non-Javascript backend, it requires loading all of the points into the browser, and then clustering. This can take up a lot of bandwidth, and slow things down pretty badly. If only there was a way to do the clustering in the backend, and only send the cluster locations and size forwards?After a lot of searching, I found a bit of an awkward solution, involving altering the MarkerClusterer library using code from Stack Overflow, and using SKLearn in Python to cluster the points. This dramatically reduced the amount of data being sent forward, and page load times (especially on very large numbers of markers).All code is available for download on my Github.The first step is to download the markercluster.js file and import it into the base HTML. Then take the following code and add it into the markerclusterer.js file (I did it starting at line 316):Forgive the way that Medium formats code, but it’s best to do a straight copy-paste to get line spacing correct. Make sure the m_.png files are in a file called media at the same level as the HTML.Lets assume that the markers are being refreshed using an AJAX call whenever the map is dragged or the zoom is changed. As part of the AJAX call, we’re sending to the back end the map boundaries and current zoom:In the view, we can first filter points by the constraints from the database (called Sale in this case, as it contains information on property sales):Then, if the zoom level is above a certain level, or there are not enough points in the view to warrant clustering, the data can be sent as usual. For me this value was under 2000, but I was using custom markers which were smaller and were still visually acceptable at higher zooms.Otherwise, KMeans could be used to cluster the markers into 40 clusters:And a list of cluster centroids and size of clusters sent forwards:That was fairly straightforward, we’ll do some optimisation later, but let’s look at the Javascript.Our drawMarkers function takes the AJAX response, and first clears all current markers and clusters on the map:Then if clustering is required, uses the new methods added by the Stack Overflow code to draw these:Otherwise, adds the markers as normal:Finally, we add the map event listeners to the initMap() function and call ajax_update_markers() :Again, all of this code is available on Github.One word of caution, is that for some reason, turning ‘Zoom on click’ on for these clusters doesn’t zoom as expected, for some reason centres the view in the middle of the ocean nowhere near what was wanted. I looked into a lot of different workarounds for this, but couldn’t find anything that worked. It’s up to you and your application about the UX tradeoff between load times and this functionality.This was working much better already, but are there tweaks to the clustering algorithm that can help us get even better results?The first thing I looked at was choice of database. I originally used sqlite3, the Django default, and compared it to PostgreSQL. Information retrieval didn’t see much change between the two technologies, but information processing was quite different. Above 10,000 data points, functions like len(database_list) took up to 10 times longer when using sqlite3. I haven’t looked too much more into this, but it seems that Django plays a bit better with PostgreSQL than sqlite3.Next were the KMeans parameters. Looking at the documentation here, the parameters that control the speed/accuracy trade-off include:The default values for these parameters usually give pretty good accuracy in an acceptable run time, but the truth is for this application we can probably accept a much lower accuracy. A 10% error in the number of points per cluster is hardly going to be noticed by the user, and all points are shown after the user zooms in anyway. After playing around with parameters, I realised that the clustering time could be reduced by half, and from a few test cases, I couldn’t see any errors in the final cluster locations or counts per cluster!My final parameter choices were:All of these are drastic reductions, but it turns out that the KMeans algorithm is actually pretty good, and a lot of the parameter settings are redundantly high when accuracy is not the main concern. As for the speedup, a table of results is below:This is pretty remarkable, as speed ups of 2–3 times are seen by reducing the accuracy of the clustering, and even more when combining with PostgreSQL. There are definitely more efficiencies to be realised for this application, but this example shows what a dramatic effect understanding where on the accuracy / runtime curve you need to be, and prioritising your objective.Again, all of the code is on my github. If you liked this post, please click and hold the 👏 button to let your followers know, or let me know your thoughts below or on Twitter @padams02.",03/09/2017,10,8,64,"(502, 148)",4,2,0.0,7,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,joy/calmness
207,BERT Word Embeddings Deep Dive,,Isanka Rajapaksha,,7.0,897,"In 2018, the Google AI team made a revolutionary change in the field of Natural Language Processing ( NLP) by introducing Bidirectional Encoder Representations from Transformers (BERT). Due to its highly pragmatic approach, and higher performance, BERT is highlighted for achieving state-of-the-art performance in many NLP tasksIn this blog, we’ll be looking at word embeddings and see how BERT can be used with word-embedding strategies to feed as input features for other models built for custom tasks to perform the state of art results. This blog includes all the information I gathered while researching the word embedding task for my final year project.“Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.”What are word embeddings exactly? Simply, they are vector representations of a particular word. Word embedding is one of the most popular representations of document vocabulary. It is capable of capturing the context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Word embeddings are mostly used as input features for other models built for custom tasks.There are a few key characteristics to a set of useful word embeddings:There are many approaches to generate word embeddings. Context-independent (Bag of Words, TF-IDF, Word2Vec, GloVe), Context-aware (ELMo, Transformer, BERT, Transformer-XL), Large model (GPT-2, XLNet, Compressive Transformer) are the main categories. If you want to go deep into these approaches, please refer to this blog.BERT has an advantage over models like Word2Vec because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them. For example, given two sentences:“The man was accused of robbing a bank.”“The man went fishing by the bank of the river.”In both sentences, Word2Vec would create the same word embedding for the word “bank,” while under BERT the word embedding for “bank” would vary for each sentence. Aside from capturing obvious differences like polysemy, the context-informed word embeddings capture other forms of information that result in more accurate feature representations, which in turn results in better model performance.Before we move into the code, let’s just quickly explore the architecture of BERT so that at implementation time we have a bit of context. Believe me, it’ll make it a lot easier to understand things.Two primary models were created by BERT developers:From a very high-level perspective, BERT’s architecture(BASE model) looks like this:It may seem simple but remember that each encoder block encapsulates a more sophisticated model architecture.BERT expects input data in a specific format, with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). Furthermore, we need to tokenize our text into tokens that correspond to BERT’s vocabulary. For each tokenized sentence, BERT requires input ids, a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary.Depending on the configuration (BertConfig) and inputs, BertModel returns the following outputs;The full set of hidden states of the model, stored in the object hidden_states, is a little dizzying. This object has four dimensions, in the following order:Wait, 13 layers? Doesn’t BERT only have 12? It’s 13 because the first element is the input embeddings, the rest is the outputs of each of BERT’s 12 layers.It’s all wonderful so far, but how do I get word embeddings from this? The BERT base model uses 12 layers of transformer encoders as discussed, and each output per token from each layer of these can be used as a word embedding!. Perhaps you wonder which is the best, though?By feeding various vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 ratings, the BERT authors checked word-embedding strategies. The authors identified that one of the best performing choices was to sum the last 4 layers.Enough with the theory. Let’s move on to the practice and see how the above word-embedding strategies are used in PyTorch code implementation.In this section, we’ll highlight the code to extract the word embedding from the BERT model. A notebook containing all this code is available on colab.Let’s start by importing the tools of the trade. The two modules imported from BERT are modeling and tokenization. Modeling includes the BERT model (BASE model) implementation and tokenization is obviously for tokenizing the input text.Convert the input data into the required format for the BERT model using the tokenizer.Convert the input into torch tensors and call the BERT model.Running BERT on the input and extract the word embedding in different ways using the model output.The word embedding by concatenating the last four layers(word_emb_6), giving us a single word vector per token. Each vector will have a length 4 x 768 = 3,072. All other word embeddings have the 768 length vectors per token. You can use any of these ways to get word embedding as input features for other models built for custom tasks according to the model performance.I hope you enjoyed the blog and hopefully got a clearer picture of BERT embedding. In the comments section, feel free to post your feedback.[1] https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/[2]”BERT — transformers 3.3.0 documentation”, Huggingface.co[3] https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b.[4]https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca.[5]J. Alammar, “A Visual Guide to Using BERT for the First Time”, Jalammar.github.io[6] “Get Busy with Word Embeddings — An Introduction”, Shane Lynn",11/10/2020,4,26,5,"(700, 380)",4,4,0.0,11,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,expectation/interest
208,Train Object Detection AI with 6 lines of code,DeepQuestAI,Moses Olafenwa,3500.0,9.0,1052,"Step-by-step tutorial on training object detection models on your custom datasetObject detection is one of the most profound aspects of computer vision as it allows you to locate, identify, count and track any object-of-interest in images and videos. Object detection is used extensively in many interesting areas of work and study such as:A number of pre-collected object detection datasets such as Pascal VOC, Microsoft’s COCO, Google’s Open Images are readily available along with their pre-trained models for detection and identifying only a fix set of items.However, the challenge with using these public datasets and pre-trained models is that they do not provide a convenient way for you to easily train new object detection models to detect and identify your desired object(s) of interest. Since the past one year that I published my first Object Detection article “Object Detection with 10 lines of code”, I have received thousands of request from developers, teams, students and researchers who desire to detect their own objects-of-interest in images and videos, beyond the 80 classes provided in the COCO dataset and 600 classes provided in the Open Images dataset.I am most glad to announce that with the latest release of ImageAI v2.1.0, support for training your custom YOLOv3 models to detect literally any kind and number of objects is now fully supported, and that is what we will guide you to do in this tutorial. Let’s get started.github.comFor the purpose of this tutorial, we will be using Google Colab to train on a sample dataset we have provided. Follow the steps below.Step 1 — Preparing your datasetFor your custom detection training, you have to provide sample images ( your image dataset ) for training your model and validating the model after training for accuracy. ImageAI detection training supports the Pascal VOC format for your custom dataset. For the purpose of this tutorial, we have provided a sample dataset for the Hololens Mixed Reality headset, on which we will train a model that can detect and identify the Hololens in pictures and videos. You can download the sample dataset via the link below.https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/headset.zipIf you want to train on your own images for your custom object(s), follow the instructions below.medium.comStep 2 — Installing ImageAI and DependenciesGo to https://colab.research.google.com and create a new notebook. Ensure you change the runtime for your new notebook to a GPU. Then:i. TensorFlowii. Other Dependenciesiii. ImageAIpip install imageai --upgradeStep 3 — Initiate your detection model trainingTo ensure that our trained custom models have better detection accuracy, we will be using transfer learning from a pre-trained YOLOv3 model in the training. ImageAI provides the option to train with and without transfer learning. I will strongly recommend you use transfer learning except you have thousands of object samples in your dataset.SIMPLE! The above 6-lines of code is all you need to initiate the training on your custom dataset. Now let’s break down the code to its part:— object_names_array: This is an array of the names of all the objects in your dataset. Please note that if your custom dataset annotation has more than one object, simple set the values as shown in the example below— batch_size: This is the batch size for the training. Kindly note that the larger the batch size, the better the detection accuracy of the saved models. However, due to memory limits on the Nvidia K80 GPU available on Colab, we have to keep this value as 4. The batch size can be values of 8, 16 and so on.— num_experiments: This is the number of times we want the training code to iterate on our custom dataset.— train_from_pretrained_model: This is used to leverage transfer learning using the pretrained YOLOv3 model we downloaded earlier.Once the training starts,Step 4 — Evaluate your modelsIn the sample log shown above, new models are saved based on the decrease in the validation loss (E.g — loss: 5.2569) . In most cases, the lower the loss, the more accurate the model will be detecting objects in images and videos. However, some models may experience overfitting and still have lower losses. To ensure that you pick the best model for your custom detection, ImageAI allows you to evaluate the mAP (mean Average Precision, read more about it here) of all the trained models saved in the hololens/models folder.The higher the mAP, the better the detection accuracy of the model.Simple run the code below on the models saved during the training.When you run the above code, you get a result like the example below.Let’s breakdown the evaluation code:— model_path: This is the path to the folder containing our models. It can also be the filepath to a specific model.— json_file: This is the path to the detection_config.json file saved during the training.— iou_threshold: This is our desired minimum Intersection over Union value for the mAP computation. It can be set to values between 0.0 to 1.0— object_threshold: This is our desired minimum class score for the mAP computation. It can be set to values between 0.0 to 1.0.— nms_threshold: This is our desired Non-maximum suppression for the mAP computation.Step 5 — Detecting our custom Object in an imageNow that we have trained our custom model to detect the Hololens headset, we will use the best model saved as well as the detection_config.json file generated to detect the object in an image.https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/hololens-ex-60--loss-2.76.h5https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/detection_config.jsonWhen we run the above code, we get the result below.— RESULT —VOILA! Now we have been able to successfully train a new detection model to detect the Hololens Mixed Reality headset.If you enjoyed this article, give it as many claps as you will like to. Also share with friends and colleagues as well.The Colab Notebook containing all the codes in this tutorial is available via the link below.colab.research.google.comOn a final note, ImageAI also allows you to use your custom detection model to detect objects in videos and perform video analysis as well. See the documentations and the sample codes links provided below.github.comVideo Detection Documentation:github.comExample codes:github.comIf you will like to know everything about how object detection works with links to more useful and practical resources, visit the Object Detection Guide linked below.www.fritz.aiThe ImageAI project and this tutorial brought to you by DeepQuest AI.At DeepQuest AI, our mission we chose to accept is to advance and democratize Artificial Intelligence, and make it accessible to every individual and organization on the planet.deepquestai.com",01/08/2019,10,74,5,"(600, 425)",4,12,0.0,16,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
209,"Batch Gradient Descent, Stochastic Gradient Descent ve Mini-batch Gradient Descent",,Ahmet Suna,16.0,4.0,622,"Merhaba, bu yazıda Batch Gradient Descent, Stochastic Gradient Descent ve Mini-batch Gradient descent arasındaki faklara odaklanmaya çalışacağım. Ancak aradaki farklara girmeden önce Batch Gradient Descent’i temel alarak, kısaca bir bahsedip üzerine koyarak ilerlemeye çalışacağım.(Not: Makine Öğrenmesi alanında saygın hocamız Ethem Alpaydın yaklaşık 6000 atıflı Introduction to Machine Learning kitabının Türkçe çevirisinde Gradient Descent’i Eğim İniş olarak Türkçe’ye kazandırmış. Yazımda Machine Learning alanında ilerlemek isteyen insanların bu terimlerle sürekli karşılaşacağını düşündüğümden Türkçe karşılıklarını kullanmayı tercih etmeyeceğim.)Kısaca Gradient DescentGradient Descent modelimizdeki cost function’ı(maliyet fonksiyonunu) minimize etmek için kullanılır. Peki bunu nasıl yapar? Bunu bir metafor yaparak anlatmayı deneyeyim: Düşününki bir gece dağın tepesinde kaldınız. Etrafınızı göremiyorsunuz ve dağdan inmeniz lazım. Aşağıya nasıl inersiniz? Şu bir yol olabilir: ayağınızı uzatarak 360 derece dönerek , ayağınızla yere dokunup ufak bir adımı hangi yöne atabileceğinize karar verebilirsiniz. Ve bu işlemi sürekli tekrarlayarak bebek adımlarıyla yere yani local minimuma(yerel minimuma) ulaşabilirsiniz.Evet en iyi metafor olmadı farkındayım ancak görselleştirip ve biraz da matematiğine bakıp devam edelim.Amacımız cost function’ı minimize etmekti. Cost function genelde J(θ​) ile ifade edilir. Yukarıdaki J(θ0​,θ1​) , θ0​ ve θ1 parametrelerine göre çizilmiş. Burada iki tane yerel minimum var. Başlangıç noktamız biraz farklı olsa diğer yerel minimuma ulaşabilirdik. Peki local minimuma doğru yönelme olayı nasıl oluyor hocam?Bu örnekte ​başlangıçta verilen θ0​ ve θ1 değerlerine görehesaplanan J(θ0​,θ1​)’ın türevi alınır aslında bu da o noktadaki eğimine eşittir. Böylelikle yönümüzü tayin ederiz. Buradaki her adımı(benzetmemizdeki bebek adımını) temsil eden bir α(learning rate : öğrenme çarpanı veya adım büyüklüğü)değeri vardır. Bu α değerini çok küçük seçmek local minimumu bulmamızı yavaşlatabilir, çok yüksek seçmekse local minimuma giderken yön sapmalarına hatta J(θ​)’nın artmasına bile sebep olabilir. 0.1 genellikle kabul gören değerdir.Boş yapma bize matematik ver diyenler için Linear Regression için Gradient Descent nasıl kullanılır ona bakalım.Buradaki hθ​(x)) bizim linear regression için hipotezimiz. Jtrain(θ) ise train kümemizin cost function’ı. Buradaki m değeri bizim veri setimizdeki örnek sayısını temsil ediyor. x’te tahmin edebileceğiniz üzere featurelarımızı temsil ediyor.Gradient descent yukarıda gördüğünüz gibi hesaplanır. Siyahla çizdiğim yer J(θ​)’nın türevinden geliyor. Bunu learning rate ile çarpıyor ve θ​j’den çıkararak yeni θ​(j) değerini belirliyoruz ve bunu her θ​ parametresi için tekrarlıyoruz.Burada dikkat ederseniz i=1'den m’e (yani veri setimizdeki her örnek için) kadar bir toplama işlemi yapıyoruz ve bunu her θ​ parametresi için yapıyoruz.Yani şunu demek istiyorum biz gradient descent’te tek bir adım atmak için i=1'den m’e kadar olan toplama işlemini yapıyoruz. Bu Gradient Descent’in özel adı Batch Gradient Descent olarak literatürde geçiyor. Buradaki sorun: eğer m çok büyük bir sayı olursa mesela 100.000.000 kadar büyük (amazon,alibaba gibi sitelerde bu çok mümkün) gradient descent’in tek bir adımı atması için i=1'den 100.000.000'a kadar toplama işlemi yapması gerekecek.Stochastic Gradient DescentStochastic Gradient Descent’te tek bir adım atmak için i=1'den m’e kadar toplama işlemini yapmaya gerek kalmaz, örneklere tek tek bakarak gradient descent’i işlemeye koyabiliriz. Böylelikle daha hızlı sonuç elde edebiliriz.Stochastic Gradient Descent’te sadece tek bir örneğe baktığımız için her bebek adımımız local minimuma doğru olmayabilir. Yönelimlerde sapma olabilir. Bunun için her 1000 örnekten sonra learning curve(öğrenme eğrisi) çizmek ve cost function’ın ne durumda olduğuna bakmak bir çözüm olabilir. Aynı zamanda stochastic gradient descent local minimumu bulmayı garanti etmez. Local minimum etrafında osilasyon hareketi yapar. Plot ettiğimiz learning curve’e bakarak bunu anlayabiliriz. Local minimumu bulmaması çok önemli değil,kabul edilebilir bir hata oranı ancak ille de istersek iterasyon sayısına göre azalan bir learning rate hesaplayarak bunu sağlayabiliriz ama bu genelde kullanılan bir yöntem değil.Mini-batch Gradient DescentMini-batch Gradient Descent veri setini alt kümelere böler ve her alt küme için θ parametresini günceller.Alt küme sayımıza = 10 , veri setindeki örnek sayısına 1000 dersek yukarıdaki gibi bir algoritma uygulayabiliriz. İlk 10 Örneğe baktıktan sonra gradient descent işlemeye başlayacağı için Batch Gradient Descent’ten daha hızlı. Peki Stochastic Gradient Descent’ten?Eğer çok iyi bir vektorization(vektorleştirme) işlemi uygulanırsa Stochastic Gradient Descent’ten daha hızlı çalışabilir.Referans:Andrew NG →https://www.coursera.org/learn/machine-learning/Ethem Alpaydın : Yapay Öğrenme",20/07/2019,0,3,30,"(481, 250)",6,0,0.0,1,de,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,no emotion
210,It’s ok to be a dropout,Mark Howson,Mark Howson,48.0,5.0,1182,"Dear Mark (age 16 and a bit),You’re about to start a stage management course in York. You won’t enjoy it. You won’t enjoy the near two-hour round trip on trains and buses. You’ll feel less inspired by the subject than you’d expected to be. Mostly, you’ll be mind numbingly bored. And in about a months time, you’ll decide you’ve had enough, and leave.You’ll tell your parents, and friends, that this is temporary. You’re just taking a year out. You might go to Sixth Form and do some conventional subjects come September. You won’t.And really, you’ll know that from day one. You don’t want to be the one person a year older than everyone else. Some of your friends know it too. They’ll tell you you’re making a stupid decision, a big mistake, that you’re ruining your future. You’ll ignore them. But you’ll be scared that they’re right. It’ll take years for that fear to go away.You’ll pick up extra hours at McDonalds. It was your summer job, after all, so you may as well. You’ll also grab some weekend work at Three, selling mobile phones. You like this a lot better. You’ll try to resign from McDonalds, but they’ll convince you to stay.You’ll hit breaking point a couple of months later when some idiots, who were a couple of years below you in school, see that you work there and spread a few dozen packets of barbecue sauce over a table, which you have to clean. Three will give you more hours, and you’ll resign from McDonalds.You won’t appreciate it right away, but you’ll come to value your time there. At McDonalds, you’ll have learnt what teamwork really means. You’ll have learnt to interact with a whole bunch of people you wouldn’t have otherwise. You’ll have learnt humility. You’ll have learnt that there’s value in doing something right, even if it’s not something you’d choose to do. You’ll have discovered that a process is just a series of cogs in a machine, operating at full pelt. This’ll matter later. It all matters.Your hours at Three will gradually increase until you’re basically working full time. You’ll earn silly money because all your bonuses are based around your contracted hours, so you’ll consistently deliver 200% or so of target. This’ll end, dramatically. The company will cut back hours to the point that you’re only working your contracted ones, and you’ll realise that you’ve already adjusted to full time hours. You’ll cut your losses, and move to another job at Nationwide Building Society.At Three, you’ll have learnt that it’s ok for work to be fun as well as challenging. You’ll have won a TV for dressing as an ice cream cone (no, really). You’ll have discovered that small teams work a lot differently to big teams. You’ll have figured out how to sell, persuade and convince (skills that you’ll later learn are transferable). You’ll leave on good terms, too. You still speak to your boss there.Nationwide will initially feel like a bad move. It’s a more clinical environment than you’re used to. There’s a lot of paperwork, a lot of training and the same rotating door of customers each day. But you’ll carve out a niche, going to primary schools on NationwideEducation visits and teaching financial education to children, spending more time out in front of the counter on the shop floor, ordering merchandise and gaining the foundations of operational understanding.You won’t be perfect, though. In fact, you’ll struggle with counting bundles of money in and out of the cash drawer. Every now and again, in the process of moving tens of thousands and pounds in and out of the Building Society, you’ll misplace £10 or £20. And when that’s someone else’s money, it’s a big deal.It’ll become clear that if you can’t overcome this obstacle, you might end up out of a job. You’ll learn that sometimes, it’s ok to fail at something, to accept a limitation, and to find something else that doesn’t need it.That’s how you’ll end up at Telefónica, as an O2 Guru. It’ll become two of the most important years of your career. You’ll learn how to manage people, create an education scheme that puts you in front of hundreds of students, deliver a new training and development program, be an active part of a union, travel to Madrid to speak about cyber bullying (your first proper business trip!) and move out of your parent’s house to a flat in Leeds.It’s not all good news, though. You’ll start to have a nagging feeling that you’re an imposter. Your colleagues will mostly be older than yourself and you’ll feel that you have to work longer, harder and with more vigour to prove that you belong. Don’t worry, that feeling won’t last forever. But it’ll plague you for the next few years.You’ll also discover what it means to struggle to make ends meet. You won’t be badly off, but the loan you took to help you move house, combined with rent and utilities and food shopping and all the stuff that you didn’t previously have to buy or budget for will catch up on you. Your next job move will be out of necessity to avoid running out of money, and spending more and more each month on credit cards. It’ll take you years to get truly under control of your debt again, but it’s a valuable lesson — and you won’t forget it.Some people at Telefónica will tell you that you’re not ready for a ‘real’ manager role yet. When you travel down to London to interview for a role as Manager of Customer Operations, part of you will agree with them. The imposter syndrome will start to bubble, feeling bigger and bigger — even after you’ve obtained the role.You’ll move to London, looking after Customer Service and Operations for OVIVO and later running Customer Service for blinkbox, then looking after Online Help and Agent Knowledge at TalkTalk after an acquisition.You’ll grow your skills, learn to do more, manage teams, travel around the world and have colleagues on almost every continent. Your lack of degree will matter less and less every time you walk through a door.People will refer you for roles, encourage you to provide advice and trust you with budgets and decisions. Usually, you’ll make the right ones. When you don’t, you’ll learn.The sum of your experiences will teach you everything you need to know. You’ll be fine. You’ll gain financial stability. You’ll prove the people that told you your future was ruined completely wrong. Eventually, you’ll feel less like a dropout or an imposter and more like you belong.At age 23 (and a bit), it’ll stop being something you wear as a reminder of your lack of education or the fact you don’t have a degree, and something you start to wear as a badge of pride. Being a dropout will have given you a wealth of experience and skills and a prospective on life that you’re proud of.You’ll have learnt that it’s ok to be a dropout.So stop worrying. Breathe. And go.Mark (age 23 and a bit)",13/09/2016,0,1,4,"(700, 467)",1,0,0.0,0,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,subjective,positive,joy/calmness
211,A Beginner’s Guide to Graph Neural Networks Using PyTorch Geometric — Part 1,Towards Data Science,Rohith Teja,270.0,4.0,654,"You have stumbled on Graph Neural Networks somehow and now you’re interested in using it to solve a problem. Let’s explore on how we can model a problem using different tools available at our disposal.At first, we need a problem to solve.Let’s pick a simple graph dataset like Zachary’s Karate Club. Here, the nodes represent 34 students who were involved in the club and the links represent 78 different interactions between pairs of members outside the club. There are two different types of labels i.e, the two factions. We can use this information to formulate a node classification task.We divide the graph into train and test sets where we use the train set to build a graph neural network model and use the model to predict the missing node labels in the test set.Here, we use PyTorch Geometric (PyG) python library to model the graph neural network. Alternatively, Deep Graph Library (DGL) can also be used for the same purpose.PyTorch Geometric is a geometric deep learning library built on top of PyTorch. Several popular graph neural network methods have been implemented using PyG and you can play around with the code using built-in datasets or create your own dataset. PyG uses a nifty implementation where it provides an InMemoryDataset class which can be used to create the custom dataset (Note: InMemoryDataset should be used for datasets small enough to load in the memory).A simple visualization of Zachary’s Karate Club graph dataset looks as follows:In order to formulate the problem, we need:Note: For the numerical representation for nodes, we can use graph properties like degree or use different embedding generation methods like node2vec, DeepWalk etc. In this example, I will be using node degree as its numerical representation.Let’s get into the coding part.The karate club dataset can be loaded directly from the NetworkX library. We retrieve the labels from the graph and create an edge index in the coordinate format. The node degree was used as embeddings/ numerical representations for the nodes (In the case of a directed graph, in-degree can be used for the same purpose). Since degree values tend to be diverse, we normalize them before using the values as input to the GNN model.With this, we have prepared all the necessary parts to construct the Pytorch Geometric custom dataset.The KarateDataset class inherits from the InMemoryDataset class and use a Data object to collate all information relating to the karate club dataset. The graph data is then split into train and test sets, thereby creating the train and test masks using the splits.The data object contains the following variables:Data(edge_index=[2, 156], num_classes=[1], test_mask=[34], train_mask=[34], x=[34, 1], y=[34])This custom dataset can now be used with several graph neural network models from the Pytorch Geometric library. Let’s pick a Graph Convolutional Network model and use it to predict the missing labels on the test set.Note: PyG library focuses more on node classification task but it can also be used for link prediction.The GCN model is built with 2 hidden layers and each hidden layer contains 16 neurons. Let’s train the model!Initial experiments with random hyperparameters gave these results:Train Accuracy: 0.913Test Accuracy: 0.727This is not impressive and we can certainly do better. In my next post, I will discuss how we can use Optuna (python library on hyperparameter tuning) to tune the hyperparameters easily and find the best model. The code used in this example was taken from the PyTorch Geometric’s GitHub repository with some modifications (link).To summarize everything we have done so far:Acknowledgement: Most of the explanations made in this post were the concepts that I learned and applied during my internship at Orange Labs in Cesson-Sévigné. I worked with graph embedding methods and also graph neural networks which can be applied on a knowledge graph.In the coming parts, I will explain more on how we can use graph embeddings as initial node representations to handle the same task of node classification.Thanks for reading, and cheers!",10/08/2021,1,4,6,"(523, 327)",3,2,0.0,16,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
212,Simplified Math behind Complex LSTM equations,,Leena Bora,41.0,12.0,1847,"If you’re like me who have spent days in understanding complex Math behind LSTM but still can’t get your head around it, then I think this post will help you because now finally I understand it and have made an effort to simplify this complex math.If you’re thinking, it’s BERT, Transformer like model’s world, then why should we even learn about LSTM? I think you will be able to appreciate BERT/Transformer models better if you understand it’s ancestors like RNN and RNN-variants LSTM. If you invest in understanding math behind LSTM, as a bi-product you’ve already built your math foundation better which will help you to understand these super complex models like transformers.In addition we use BERT for getting contextual representation of our sequence but we can still use LSTMLet’s begin.Before LSTM, let us first understand what is the Math behind Vanilla-RNN:Now we want our model to learn this language and understand what this review mean whether Positive or Negative.As we know, to understand what this review mean, word sequencing in sentence is very important. Even if there’s a little change in the order, it is going to change the whole meaning of a sentence. e.g. Let me re-shuffle an order of above dataset sentence: “Food was not good but late service an issue.”You see, just changing an order, it completely changes the meaning.To know how much it is important for model to understand language nuances look at this funny twitter post (real incident).So same goes for machines. To understand the language correctly, it should also take into account it’s sequence. We know Neural Networks don’t respect the sequence. Hence we use the RNN models (and RNN-variants) for sequence problems. (In fact, as of 2020, we use more intelligent models like BERT, Reformers, T5 but let’s not get digressed from our main motive that is to understand math behind LSTM)So now we know RNN takes into account word sequencing in a sentence. Let’s see, how RNN does this mathematically.So as we can see from above figure, RNN unit processes one input token at a time and passes results to next RNN unit to process next token. That means, when it is processing “X3” token, it has all the previous history of “X1” & “X2” tokens(and not just about “X2”)Let’s see now, how this processing is done Mathematically:Before this, we need to understand few concepts like dot-product, linear activations like Sigmoid, tanh.It’s a bit abstract concept but let me give you it’s intuitive tour:Suppose I give you two numbers like 5 & 20 and ask you to use any operation between them so that result is 40.You will say, it is not possible with above 4 operations. So now I relax the problem for you by introducing 2 new variables W1 and W2 and you can use any values for them to get 40:Now you’re very happy as you arrive with a result.I reply, “that’s good. But don’t be too happy. As I have relaxed the problem for you, I have also added a new constraint.”Basically, now you can use any values for W1 and W2 of your choice provided they should work for all 5 equations below:Now, you take out your pen and paper and try out all combinations for W1 and W2 which will work for all rows.And finally you know the answer:If you have understood this: Congratulations! you have understood Dot Product, Linear Regression (and also Logistic Regression partially). I will explain.So what really happens in machine learning: we are given a problem in the following form and we need to establish a relationship between X and Y.Once the relationship is established (between X and Y), we can predict any person’s weight given we have his Height, Calorie Intake and Walking details like given in a last row.And how do we establish a relationship, we have just learnt that above:We do exactly the same for below dataset and then we say we have solved it using Linear Regression.So where is the dot-product here? Below equation is a dot-product.You have scalar values 5 and 20, you multiply them with some coefficients W1 and W2 and add them. This is a dot-product.So how do we take dot-product in below example?So I have just given you linear-algebraic intuition about Dot-Product, Linear Regression.(Note: Geometric intuition for Dot-Product, Linear Regression requires introduction to Hyperplane, vectors which I have avoided deliberately and tried to keep it as simple as possible.I have written a separate blog to explain Dot-Product in full detail. Please check this out)You had just 5 rows here, but in reality we will have thousands of rows. Now we can’t find W1,W2,W3 which fits for all thousands rows with pen and paper (manual calculation). So in machine learning, to find optimal W values we use algorithms like gradient descent (which uses Calculus).In formal terms, W1,W2,W3 are called as weights or learning/training parameters or coefficients.This is crux of any machine learning problem that is finding optimal weights (from Linear Regression to Latest Transformers, GAN)We take a dot-product with above formula and on that result we apply activation function.So you will ask, we have understood why we take dot-product (it is basically to find optimal coefficients matching our data) but why do we call one more function on top of this?Reason for this is, we know that in Neural Network we have multiple hidden layers so that it becomes capable of establishing complex relationship between X and Y.(As we can see, hidden layer is nothing but a result of dot-product. We pass this result to next hidden layer, which becomes input for next layer and again we take a dot-product)If we apply simple math here, then we will know that no matter how big chaining of these hidden-layers we form, it is useless because we can achieve the same result with simple dot-product:That means Neural Net with multiple hidden layers without activation function is nothing but a Linear Regression. And we know that, not all problems are linear in nature and hence we can’t solve them with simple linear regression.There are many activation functions, but let’s stick to sigmoid and tanh here.To understand this, let’s turn above problem into Classification Problem:Now Y has only 2 possible outcomes Yes/No (Diabetic or No) unlike person’s weight which can be 20,30.5,55… anything really. Hence since it has fix outcomes we categorise it as classification problem.But wait how do we solve this with dot-product:Congratulations once again! With this now you have learnt logistic regression.So, important property of sigmoid activation function is, it’s range is between [0–1] which helps in: Binary classification problem (yes/no type outcomes)Tanh is almost similar to sigmoid but it’s range is from [-1 1] (slightly bigger range than sigmoid) hence it results into zero centered data which is a big help for data normalization. Hence tanh is mostly used for hidden layers.Cool! So we have all the machinery (Dot-Product, Activation Functions like sigmoid, tanh) ready. Now let’s go back to RNN Math:Don’t forget motive of RNN is to respect the sentence sequence.Note: very first prev_state is initialized to 0.This current status acts as a hidden layer when we process next token. And remember we generally use tanh activation function for hidden layer. Hence you see tanh in above equation.Let’s see this with example: “I am doing fine” → x1 x2 x3 x4 If I just give you “doing” word (i.e x3 token), it has no context about sequence.“doing” will start making sense if we have previous context like, “I am”Intuitively this is what happens when we pass hidden state, It has all history uptil this point (i.e x3 token , “doing”) which will be helpful to process next token, x4.Other way to put this is: In a sequence, alone x3 has a local context about sequence (and it’s of no use as stand-alone), but current_status_x3 (hidden-state-3) holds the global context at x3 about sequence which is very relevant for sequencing.(Please read this para again because for RNN and LSTM math, this is a foundation)Remember we switch to RNN model from Neural Net for the very reason that we want our model to regard sequencing in our input sentence. RNN does this job very well but it fails when sequence is too long.Remember who is responsible for propagating sequence history in RNN model. It’s a hidden layer (which we refer to as previous state). This hidden layer is nothing but a vector. When sequence is too long it fails to remember early sequence details. (Technically it faces a vanishing gradient issue. But let’s not worry about these details)To solve this problem LSTM was invented. Before going into details of LSTM math let’s do a comparative study of RNN and LSTM architecture:Thus we can conclude from above picture that: LSTM resolves the problem which RNN faces of not remembering the long term dependencies by:Before we see actual LSTM math, I will repeat once again:In a sequence, just alone x2 token has a local context about sequence(and it’s of no use as stand-alone), but current_status_x2 (hidden-state-2 ) holds the global sequence context at x2 which is very relevant for sequencing.Let’s calculate that:So let’s process x2 token (this is exactly same as RNN):In LSTM we have 2 previous states. So here previous_status is sub_hidden_state (hidden_status)(remember tanh is an activation function which is generally used for hidden layers and which acts as a data normalizer. So here also we use tanh activation. Exactly same as RNN.)And now we want to come up with new-master-hidden-state & new-sub hidden-state to process next token x3.For now just consider how can we calculate new-master-hidden-state? (because sub-hidden-state is calculated from master-hidden-state. Hence once we have our new-master-hidden-state, calculating new-sub-hidden-state is trivial)In RNN, we pass current_x2_status as is to next token and which act as hidden status to process token x3. But we know this approach has a problem of not remembering long-term dependencies. So we can’t pass it as it is.LSTM uses following intelligent approach to calculate new hidden state:This means, instead of passing current_x2_status as is to next unit (which RNN does):That is forward only required details to subsequent unit (and remove unnecessary details). Very Intelligent indeed!But wait, where did this magic numbers 30% and 70% come from? You’re right, these are different for each problem, each iteration and each LSTM unit.Hence, instead of hardcoding, let’s use some variable names for them.Okay, I know what are you thinking? How to calculate these?So we know that these numbers represents percentage (0.3, 0.7). So we need to choose a function which returns values between [0–1]Can you think which function can be used for this?If you remember, sigmoid activation function does exactly this. It returns values between [0–1]. So let’s use that:Okay, we have used sigmoid function which will return result between [0–1] range, but what should we use as it’s input?It’s easy! How much to remember and how much to forget depends on current input and previous history (hidden state)With this, we have calculated our new-master-hidden-state. Let’s put everything together:Now we calculate new-sub-hidden-state from new-master-hidden-state.With this we have finished all the math behind RNN and LSTM equations.I hope it was helpful to you.",06/06/2020,0,77,1,"(700, 351)",34,5,0.0,1,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,positive,joy/calmness
213,How to export a chart from Excel to PowerPoint?,SlideMagic,Jan Schultink,1100.0,1.0,144,"Yesterday’s post triggered this question: how to export a chart from Excel to PowerPoint? The short answer: copy the data not the chart.Standard Excel charts are ugly, they have the wrong formatting, they have the wrong colors, axis labels are in the wrong place, data is not rounded up and too precise. Copying and pasting an Excel chart into PowerPoint is also copying all that ugliness. Even worse, copying and pasting it as a picture might make it look blurry.I believe that data charts in a PowerPoint presentation deserve careful attention and need to be designed by hand. You start by inserting a blank, ugly, PowerPoint chart into your slide, next copy the data across from Excel and then start tweaking until it looks perfect.Once you have done one, you can use that PowerPoint chart as a template for other charts in your presentation.",31/01/2012,0,1,0,,0,0,0.0,0,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,subjective,neutral,joy/calmness
214,Installing a Highly-Available OpenShift Origin Cluster,,Adilson Cesar,195.0,6.0,825,"This article proposes a reference architecture for a Highly Available installation of OpenShift Origin. We will outline the architecture of such an installation and walk through the installation process. The intention of this process is to perform iterative installations of the OpenShift Origin cluster.01 Master (Openshift Web Console)01 Infra (Router, Registry, Metrics, EFK)01 Node (Application node POD)Centos 7 x86_64 with Minimal Installation.Each of these servers should be provisioned with an SSH public key which can be used to access all hosts from the AnsibleFor next step, you need to install docker service on all nodes (Master, Infra, Node)During the Provision Servers step of this guide, we provisioned all of our nodes (including the master) with docker volumes attached as /dev/sdb. We’ll now install and configure docker to use that volume for all local docker storage.Running a large number of containers in production requires a lot of storage space. Additionally, creating and running containers requires the underlying storage drivers to be configured to use the most performant options. The default storage options for Docker-formatted containers vary between the different systems and in some cases they need to be changed. A default installation of RHEL uses loopback devices, whereas RHEL Atomic Host has LVM thin pools created during installation. However, using the loopback option is not recommended for production systems.Creating a new VG (Master, Infra, Node)Editing /etc/sysconfig/docker-storage-setup file and add LV and VG namesRestarting Docker and Docker Storage Setup ServicesNew logical volume will be created automatically. More information, check it out How to use the Device Mapper storage driver.OpenShift uses Ansible as it’s installation & configuration manager. As we walk through design decisions, we can start capturing this information in an ini style config file that’s referred to as the ansible inventory file. To start, we’ll establish a project skeleton for storing this file and begin populating information specific to the cluster I did:More information about each parameterdocs.openshift.com*** All of the hosts in the cluster need to be resolveable via DNS. Additionally if using a control node to serve as the ansible installer it too should be able to resolve all hosts in your cluster.In an HA cluster there should also be two DNS names for the Load Balanced IP address that points to the 1 master server for access to the API, CLI and Console services. One of these names is the public name that users will use to log into the cluster. The other is an internal name that will be used by internal components within the cluster to talk back to the master. These values should also resolve, and will be placed in the ansible Hosts file for the variables.At this point in the process we are ready to prepare our hosts for install. The following sections guide us through this process.The install will run for 15–20 minutes. Good time for a coffee break.Few minutes after. Voi-là!For the initial installation we are going to simply use htpasswd for simple authentication and seed it with a couple of sample users to allow us to login to the OpenShift Console and validate the installation.and set cluster-role rights to admin user have access all projects in the cluster.Done!By default Project. We are able to see Registry Service already ""containerized""So, you can access by https://registry-console-default.apps.cirrus.io URL provided by Router Service.After having gone through the process of building an OpenShift environmentCheck the output to ensure that:The oc status command is helpful to validate that a namespace is in the state that you expect it to be in. This is especially helpful after doing an install to, at a high level, check that all of the supporting services and pods that you expect to exist actually do. At minimum, you should see the following after a successful install:An example of a healthy output might look like:OpenShift provides an additional CLI tool that can perform more fine grained diagnostics, including validating that services can see each other, than certificates are valid, and much more. The output of a diagnostics run can be quite verbose, but will include a final report of Errors and Warnings at the end. If there are errors or warnings, you may want to go back to them and validate that they are not errors and warnings for any critical services.Not all errors or warnings warrant action. The diagnostics check will additionally examine all deployed services and report anything out of the ordinary. This could include apps that may have been misconfigured by a developer, and would not necessarily warrant administrative intervention.You can uninstall OpenShift Origin hosts in your cluster by running the uninstall.yml playbook. This playbook deletes OpenShift Origin content installed by Ansible, including:The playbook will delete content for any hosts defined in the inventory file that you specify when running the playbook. If you want to uninstall OpenShift Origin across all hosts in your cluster, run the playbook using the inventory file you used when installing OpenShift Origin initially or ran most recently:Enjoy your Cluster :)",12/11/2017,13,13,7,"(700, 383)",5,2,0.0,7,en,cluster,clustering,distance,centroid,region,probability,density,hdbscan,plot,mixture,objective,neutral,expectation/interest
215,Meu livro favorito de UX,UX Collective 🇧🇷,Fabricio Teixeira,137000.0,1.0,110,"No começo da semana lançamos a hashtag no Twitter: #livrofavoritoUX. Milhares de tweets depois (mentira, foram dezenas), segue um apanhado com sugestões diversas.Espero que ajude quem está procurando uma referência de leitura no assunto. E mesmo para quem já leu quase tudo o que há por aí, nunca é demais dar uma conferida no que os colegas de profissão estão lendo e gostando de ler.Pelo visto o campeão na preferência dos seguidores é o Não Me Faça Pensar, de Steve Krug. Continuem mandando sugestões, tanto pela hashtag #livrofavoritoUX quando aqui na caixa de comentários.Atualizei a nossa biblioteca de livros de AI no Goodreads. Depois dá uma passada por lá :)",08/07/2011,0,1,0,"(700, 349)",1,0,0.0,1,pt,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,no emotion
216,RAdam Oprimizer: Rectified Adam. 自動warmup版的Adam優化器,,Patty Wu,,6.0,54,"全名是Rectified Adam，是一種改進Adam的方法，更精確來說，就是自動warmup版的Adam。這是一篇ICLR 2020的論文：ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND: https://arxiv.org/pdf/1908.03265.pdf要解決的問題：Adam是一種常用的自適應學習率 (adaptive learning rate) 優化器，但這種方法在訓練的初期，adaptive learning rate的變異非常大，然後在少量的數據進行過度跳躍，下了錯誤決策，就容易收斂在local minimum。我們知道adptive learning rate是根據gradient算出來的，下圖秀出gradient的分布，x軸是log scale的gradient，y軸是frequency，然後它把每個iteration的分布圖由上到下 (初期到後期) 堆疊在一起。我們先看左邊兩張圖，一般的Adam，前幾次迭代的gradient分布劇烈變化，之後訓練就容易收斂在local minimum。但如果我們設定前幾個iteration的learning rate很小，之後再讓它自適應成長，也就是熱身(warmup)的技巧，gradient的分布就會一直是比較穩定的狀態，見右邊兩張圖。作法基於這個論點，所以作者希望可以做出一個自動熱身版的Adam：RAdam，那它就面臨兩個問題：RAdam的概念是這樣的，如果有個開關，閥值為rho，這個rho跟看過的樣本數量成正比：這個rho怎麼來的呢？看下圖，我們知道Adam的adaptive learning rate是用gradient的平方開根號算來的。假設gradient服從N(0, sigma²)分配，adaptive learning rate經由公式推導會服從Scaled-inv chi-square(rho, 1/sigma²)分配，rho就是這個Scale-inv chi-square分配的自由度，自由度你可以簡單想像就是樣本數的概念，樣本數越大，自由度越大。下圖的x軸就是rho，y軸是變異數，可以發現變異會隨著rho增加而遞減，而且剛開始變化會比較劇烈，這點也反映了：到這邊我們介紹完了rho與adaptive learning rate的關係，我們知道了開關的概念。剩下就是如何修正adptive learning rate：修正項rt是用adptive learning rate的變異數推導而來，你可以直覺地想成，因為rho會影響變異大小，所以把有rho的項目做運算抵銷回去，就可以保持樣本數對adptive learning rate的穩定性了。然後rho>4可以以當作熱身的開關。另一種思路是，當t→無限大時，rho_t很大，t越小時rho_t越小，所以基本上在rho>4時，t 增加 → rt 減少。也就是說，當t、rho_t越小，變異越大時，adaptive learning rate 就必須乘上較大的rt做修正。優點適用範圍：RAdam不只是對於CNN：RAdam在Billion Word Dataset上的表現優於LSTM。作法回顧與總結RAdam就是自動熱身版的Adam概念：開關閥值為rho，這個rho代表adpative learning rate分配的自由度：參考文獻",18/11/2020,0,9,0,"(657, 325)",7,6,0.0,2,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,no emotion
217,Simple Machine Learning Model in Python in 5 lines of code,Towards Data Science,Raman Sah,348.0,3.0,358,"In this blog, we will train a Linear Regression Model and expect to perform correct on a fresh input.The basic idea of any machine learning model is that it is exposed to a large number of inputs and also supplied the output applicable for them. On analysing more and more data, it tries to figure out the relationship between input and the result.Consider a very primitive example when you have to decide whether to wear a jacket or not depending on the weather. You have access to the training data as we call it -Somehow, your mind finds a connection between the input (temperature) and the output (decision to wear a jacket).So, if the temperature is 12°C, you would still wear a jacket although you were never told the outcome for that particular temperature.Now, lets move on to a slightly better algebraic problem which the computer will solve for us.Before we begin, don’t forget to install scikit-learn, it provides easy to use functions and predefined models which saves a lot of timeHere, X is the input and y is the output.Given the training set you could easily guess that the output (y) is nothing but (x1 + 2*x2 + 3*x3).Working with linear regression model is simple. Create a model, train it and then use it :)We have the training set ready, so create a Linear Regression Model and pass it the training data.X = [[10, 20, 30]]The outcome should be 10 + 20*2 + 30*3 = 140. Let’s see what we got…Outcome : [ 140.]Coefficients : [ 1. 2. 3.]Did you notice what just happened? The model had access to the training data, through which it calculated the weights to assign to the inputs to arrive at the desired output. On giving test data, it successfully managed to get the right answer!If you want to dive deeper into Machine Learning and use Python; I would prefer this book to start with.shop.oreilly.comI also experiment a lot and tinker with code. Feel free to fork ML Prototype where I have tried to expose the functions of scikit-learn through API. And don’t forget to clap if you find this article interesting.",06/03/2017,2,3,0,"(558, 287)",2,0,0.0,3,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,joy/calmness
218,Using the latest advancements in deep learning to predict stock price movements,Towards Data Science,Boris B,2700.0,34.0,4592,"Link to the complete notebook: https://github.com/borisbanushev/stockpredictionaiIn this notebook I will create a complete process for predicting stock price movements. Follow along and we will achieve some pretty good results. For that purpose we will use a Generative Adversarial Network (GAN) with LSTM, a type of Recurrent Neural Network, as generator, and a Convolutional Neural Network, CNN, as a discriminator. We use LSTM for the obvious reason that we are trying to predict time series data. Why we use GAN and specifically CNN as a discriminator? That is a good question: there are special sections on that later.We will go into greater details for each step, of course, but the most difficult part is the GAN: very tricky part of successfully training a GAN is getting the right set of hyperparameters. For that reason we will use Bayesian optimisation (along with Gaussian processes) and Deep Reinforcement learning (DRL) for deciding when and how to change the GAN’s hyper parameters (the exploration vs. exploitation dilemma). In creating the reinforcement learning I will use the most recent advancements in the field, such as Rainbow and PPO.We will use a lot of different types of input data. Along with the stock’s historical trading data and technical indicators, we will use the newest advancements in NLP (using ‘Bidirectional Embedding Representations from Transformers’, BERT, sort of a transfer learning for NLP) to create sentiment analysis (as a source for fundamental analysis), Fourier transforms for extracting overall trend directions, stacked autoencoders for identifying other high-level features, Eigen portfolios for finding correlated assets, autoregressive integrated moving average (ARIMA) for the stock function approximation, and many more, in order to capture as much information, patterns, dependencies, etc, as possible about the stock. As we all know, the more (data) the merrier. Predicting stock price movements is an extremely complex task, so the more we know about the stock (from different perspectives) the higher our changes are.For the purpose of creating all neural nets we will use MXNet and its high-level API — Gluon, and train them on multiple GPUs.Note: Although I try to get into details of the math and the mechanisms behind almost all algorithms and techniques, this notebook is not explicitly intended to explain how machine/deep learning, or the stock markets, work. The purpose is rather to show how we can use different techniques and algorithms for the purpose of accurately predicting stock price movements, and to also give rationale behind the reason and usefulness of using each technique at each step.Accurately predicting the stock markets is a complex task as there are millions of events and pre-conditions for a particular stock to move in a particular direction. So we need to be able to capture as many of these pre-conditions as possible. We also need make several important assumptions: 1) markets are not 100% random, 2) history repeats, 3) markets follow people’s rational behavior, and 4) the markets are ‘perfect’. And, please, do read the Disclaimer at the bottom.We will try to predict the price movements of Goldman Sachs (NYSE: GS). For the purpose, we will use the daily closing price from January 1st, 2010 to December 31st, 2018 (seven years for training purposes and two years for validation purposes). We will use the terms ‘Goldman Sachs’ and ‘GS’ interchangeably.We need to understand what affects whether GS’s stock price will move up or down. It is what people as a whole think. Hence, we need to incorporate as much information (depicting the stock from different aspects and angles) as possible. (We will use daily data — 1,585 days to train the various algorithms (70% of the data we have) and predict the next 680 days (test data). Then we will compare the predicted results with a test (hold-out) data. Each type of data (we will refer to it as feature) is explained in greater detail in later sections, but, as a high-level overview, the features we will use are:Next, having so many features, we need to perform a couple of important steps:As a final step of our data preparation, we will also create Eigen portfolios using Principal Component Analysis (PCA) in order to reduce the dimensionality of the features created from the autoencoders.Let’s visualize the stock for the last nine years. The dashed vertical line represents the separation between training and test data.As explained earlier we will use other assets as features, not only GS.So what other assets would affect GS’s stock movements? Good understanding of the company, its lines of businesses, competitive landscape, dependencies, suppliers and client type, etc is very important for picking the right set of correlated assets:We already covered what are technical indicators and why we use them so let’s jump straight to the code. We will create technical indicators only for GS.So we have the technical indicators (including MACD, Bollinger bands, etc) for every trading day. We have in total 12 technical indicators.Let’s visualise the last 400 days for these indicators.For fundamental analysis we will perform sentiment analysis on all daily news about GS. Using sigmoid at the end, result will be between 0 and 1. The closer the score is to 0 — the more negative the news is (closer to 1 indicates positive sentiment). For each day, we will create the average daily score (as a number between 0 and 1) and add it as a feature.For the purpose of classifying news as positive or negative (or neutral) we will use BERT, which is a pre-trained language representation.Pre-trained BERT models are already available in MXNet/Gluon. We just need to instantiated them and add two (arbitrary number) Dense layers, going to softmax - the score is from 0 to 1.Going into the details of BERT and the NLP part is not in the scope of this notebook, but you have interest, do let me know — I will create a new repo only for BERT as it definitely is quite promising when it comes to language processing tasks.Fourier transforms take a function and create a series of sine waves (with different amplitudes and frames). When combined, these sine waves approximate the original function. Mathematically speaking, the transforms look like this:We will use Fourier transforms to extract global and local trends in the GS stock, and to also denoise it a little. So let’s see how it works.As you see in Figure 3 the more components from the Fourier transform we use the closer the approximation function is to the real stock price (the 100 components transform is almost identical to the original function — the red and the purple lines almost overlap). We use Fourier transforms for the purpose of extracting long- and short-term trends so we will use the transforms with 3, 6, and 9 components. You can infer that the transform with 3 components serves as the long term trend.Another technique used to denoise data is called wavelets. Wavelets and Fourier transform gave similar results so we will only use Fourier transforms.ARIMA is a technique for predicting time series data. We will show how to use it, and althouth ARIMA will not serve as our final prediction, we will use it as a technique to denoise the stock a little and to (possibly) extract some new patters or features.As we can see from Figure 5 ARIMA gives a very good approximation of the real stock price. We will use the predicted price through ARIMA as an input feature into the LSTM because, as we mentioned before, we want to capture as many features and patterns about Goldman Sachs as possible. We go test MSE (mean squared error) of 10.151, which by itself is not a bad result (considering we do have a lot of test data), but still, we will only use it as a feature in the LSTM.Ensuring that the data has good quality is very important for our models. In order to make sure our data is suitable we will perform a couple of simple checks in order to ensure that the results we achieve and observe are indeed real, rather than compromised due to the fact that the underlying data distribution suffers from fundamental errors.We will not go into the code here as it is straightforward and our focus is more on the deep learning parts, but the data is qualitative.So, after adding all types of data (the correlated assets, technical indicators, fundamental analysis, Fourier, and Arima) we have a total of 112 features for the 2,265 days (as mentioned before, however, only 1,585 days are for training data).We will also have some more features generated from the autoencoders.Having so many features we have to consider whether all of them are really indicative of the direction GS stock will take. For example, we included USD denominated LIBOR rates in the dataset because we think that changes in LIBOR might indicate changes in the economy, that, in turn, might indicate changes in the GS’s stock behavior. But we need to test. There are many ways to test feature importance, but the one we will apply uses XGBoost, because it gives one of the best results in both classification and regression problems.Since the features dataset is quite large, for the purpose of the presentation here we’ll use only the technical indicators. During the real features importance testing all selected features proved somewhat important so we won’t exclude anything when training the GAN.Not surprisingly (for those with experience in stock trading) that MA7, MACD, and BB are among the important features.I followed the same logic for performing feature importance over the whole dataset — just the training took longer and results were a little more difficult to read, as compared with just a handful of features.Before we proceed to the autoencoders, we’ll explore an alternative activation function.GELU — Gaussian Error Linear Unites was recently proposed — link. In the paper the authors show several instances in which neural networks using GELU outperform networks using ReLU as an activation. gelu is also used in BERT, the NLP approach we used for news sentiment analysis.We will use GELU for the autoencoders.Note: The cell below shows the logic behind the math of GELU. It is not the actual implementation as an activation function. I had to implement GELU inside MXNet. If you follow the code and change act_type='relu' to act_type='gelu' it will not work, unless you change the implementation of MXNet. Make a pull request on the whole project to access the MXNet implementation of GELU.Let’s visualize GELU, ReLU, and LeakyReLU (the last one is mainly used in GANs - we also use it).Note: In future versions of this notebook I will experiment using U-Net (link), and try to utilize the convolutional layer and extract (and create) even more features about the stock’s underlying movement patterns. For now, we will just use a simple autoencoder made only from Dense layers.Ok, back to the autoencoders, depicted below (the image is only schematic, it doesn’t represent the real number of layers, units, etc.)Note: One thing that I will explore in a later version is removing the last layer in the decoder. Normally, in autoencoders the number of encoders == number of decoders. We want, however, to extract higher level features (rather than creating the same input), so we can skip the last layer in the decoder. We achieve this creating the encoder and decoder with the same number of layers during the training, but when we create the output we use the layer next to the only one as it would contain the higher level features.The full code for the autoencoders is available in the accompanying Github — link at top.We created 112 more features from the autoencoder. As we want to only have high level features (overall patterns) we will create an Eigen portfolio on the newly created 112 features using Principal Component Analysis (PCA). This will reduce the dimension (number of columns) of the data. The descriptive capability of the Eigen portfolio will be the same as the original 112 features.Note Once again, this is purely experimental. I am not 100% sure the described logic will hold. As everything else in AI and deep learning, this is art and needs experiments.How GANs work?As mentioned before, the purpose of this notebook is not to explain in detail the math behind deep learning but to show its applications. Of course, thorough and very solid understanding from the fundamentals down to the smallest details, in my opinion, is extremely imperative. Hence, we will try to balance and give a high-level overview of how GANs work in order for the reader to fully understand the rationale behind using GANs in predicting stock price movements. Feel free to skip this and the next section if you are experienced with GANs (and do check section 4.2.).A GAN network consists of two models — a Generator (G) and Discriminator (D). The steps in training a GAN are:When combined together, D and G as sort of playing a minmax game (the Generator is trying to fool the Discriminator making it increase the probability for on fake examples, i.e. minimize 𝔼z∼pz(z)[log(1−D(G(z)))]. The Discriminator wants to separate the data coming from the Generator, D(G(z)), by maximizing 𝔼x∼pr(x)[logD(x)]. Having separated loss functions, however, it is not clear how both can converge together (that is why we use some advancements over the plain GANs, such as Wasserstein GAN). Overall, the combined loss function looks like:Note: Really useful tips for training GANs can be found here.Note: I will not include the complete code behind the GAN and the Reinforcement learning parts in this notebook — only the results from the execution (the cell outputs) will be shown. Make a pull request or contact me for the code.Generative Adversarial Networks (GAN) have been recently used mainly in creating realistic images, paintings, and video clips. There aren’t many applications of GANs being used for predicting time-series data as in our case. The main idea, however, should be same — we want to predict future stock movements. In the future, the pattern and behavior of GS’s stock should be more or less the same (unless it starts operating in a totally different way, or the economy drastically changes). Hence, we want to ‘generate’ data for the future that will have similar (not absolutely the same, of course) distribution as the one we already have — the historical trading data. So, in theory, it should work.In our case, we will use LSTM as a time-series generator, and CNN as a discriminator.Note: The next couple of sections assume some experience with GANs.A recent improvement over the traditional GANs came out from Uber’s engineering team and is called Metropolis-Hastings GAN (MHGAN). The idea behind Uber’s approach is (as they state it) somewhat similar to another approach created by Google and University of California, Berkeley called Discriminator Rejection Sampling (DRS). Basically, when we train GAN we use the Discriminator (D) for the sole purpose of better training the Generator (G). Often, after training the GAN we do not use the D any more. MHGAN and DRS, however, try to use D in order to choose samples generated by G that are close to the real data distribution (slight difference between is that MHGAN uses Markov Chain Monte Carlo (MCMC) for sampling).MHGAN takes K samples generated from the G (created from independent noise inputs to the G — z0 to zK in the figure below). Then it sequentially runs through the K outputs (x′0 to x′K) and following an acceptance rule (created from the Discriminator) decides whether to accept the current sample or keep the last accepted one. The last kept output is the one considered the real output of G.Note: MHGAN is originally implemented by Uber in pytorch. I only transferred it into MXNet/Gluon.Figure 10: Visual representation of MHGAN (from the original Uber post).Training GANs is quite difficult. Models may never converge and mode collapse can easily happen. We will use a modification of GAN called Wasserstein GAN — WGAN.Again, we will not go into details, but the most notable points to make are:As mentioned before, the generator is a LSTM network a type of Recurrent Neural Network (RNN). RNNs are used for time-series data because they keep track of all previous data points and can capture patterns developing through time. Due to their nature, RNNs many time suffer from vanishing gradient — that is, the changes the weights receive during training become so small, that they don’t change, making the network unable to converge to a minimal loss (The opposite problem can also be observed at times — when gradients become too big. This is called gradient exploding, but the solution to this is quite simple — clip gradients if they start exceeding some constant number, i.e. gradient clipping). Two modifications tackle this problem — Gated Recurrent Unit (GRU) and Long-Short Term Memory (LSTM). The biggest differences between the two are: 1) GRU has 2 gates (update and reset) and LSTM has 4 (update, input, forget, and output), 2) LSTM maintains an internal memory state, while GRU doesn’t, and 3) LSTM applies a nonlinearity (sigmoid) before the output gate, GRU doesn’t.In most cases, LSTM and GRU give similar results in terms of accuracy but GRU is much less computational intensive, as GRU has much fewer trainable params. LSTMs, however, and much more used.Strictly speaking, the math behind the LSTM cell (the gates) is:where ⊙is an element-wise multiplication operator, and, for all x=[x1,x2,…,xk]⊤∈R^k the two activation functions:,The LSTM architecture is very simple — one LSTM layer with 112 input units (as we have 112 features in the dataset) and 500 hidden units, and one Dense layer with 1 output - the price for every day. The initializer is Xavier and we will use L1 loss (which is mean absolute error loss with L1 regularization - see section 3.4.5. for more info on regularization).Note — In the code you can see we use Adam (with learning rate of .01) as an optimizer. Don't pay too much attention on that now - there is a section specially dedicated to explain what hyperparameters we use (learning rate is excluded as we have learning rate scheduler - section 3.4.3.) and how we optimize these hyperparameters - section 3.6.We will use 500 neurons in the LSTM layer and use Xavier initialization. For regularization we’ll use L1. Let’s see what’s inside the LSTM as printed by MXNet.As we can see, the input of the LSTM are the 112 features (dataset_total_df.shape[1]) which then go into 500 neurons in the LSTM layer, and then transformed to a single output - the stock price value.The logic behind the LSTM is: we take 17 (sequence_length) days of data (again, the data being the stock price for GS stock every day + all the other feature for that day - correlated assets, sentiment, etc.) and try to predict the 18th day. Then we move the 17 days window with one day and again predict the 18th. We iterate like this over the whole dataset (of course in batches).In another post I will explore whether modification over the vanilla LSTM would be more beneficial, such as:One of the most important hyperparameters is the learning rate. Setting the learning rate for almost every optimizer (such as SGD, Adam, or RMSProp) is crucially important when training neural networks because it controls both the speed of convergence and the ultimate performance of the network. One of the simplest learning rate strategies is to have a fixed learning rate throughout the training process. Choosing a small learning rate allows the optimizer find good solutions, but this comes at the expense of limiting the initial speed of convergence. Changing the learning rate over time can overcome this tradeoff.Recent papers, such as this one, show the benefits of changing the global learning rate during training, in terms of both convergence and time. Let’s plot the learning rates we’ll be using for each epoch.Having a lot of features and neural networks we need to make sure we prevent overfitting and be mindful of the total loss.We use several techniques for preventing overfitting (not only in the LSTM, but also in the CNN and the auto-encoders):Another important consideration when building complex neural networks is the bias-variance trade-off. Basically, the error we get when training nets is a function of the bias, the variance, and irreducible error — σ (error due to noise and randomness). The simplest formula of the trade-off is: Error=bias^2+variance+σ.We usually use CNNs for work related to images (classification, context extraction, etc). They are very powerful at extracting features from features from features, etc. For example, in an image of a dog, the first convolutional layer will detect edges, the second will start detecting circles, and the third will detect a nose. In our case, data points form small trends, small trends form bigger, trends in turn form patterns. CNNs’ ability to detect features can be used for extracting information about patterns in GS’s stock price movements.Another reason for using CNN is that CNNs work well on spatial data — meaning data points that are closer to each other are more related to each other, than data points spread across. This should hold true for time series data. In our case each data point (for each feature) is for each consecutive day. It is natural to assume that the closer two days are to each other, the more related they are to each other. One thing to consider (although not covered in this work) is seasonality and how it might change (if at all) the work of the CNN.Note: As many other parts in this notebook, using CNN for time series data is experimental. We will inspect the results, without providing mathematical or other proofs. And results might vary using different data, activation functions, etc.Without going through the full code, we’ll just show the CNN as printed by MXNet.The hyperparameters that we will track and optimize are:We will train over 200 epochs.After the GAN trains on the 200 epochs it will record the MAE (which is the error function in the LSTM, the GG) and pass it as a reward value to the Reinforcement learning that will decide whether to change the hyperparameters of keep training with the same set of hyperparameters. As described later, this approach is strictly for experimenting with RL.If the RL decides it will update the hyperparameters it will call Bayesian optimisation (discussed below) library that will give the next best expected set of the hyperparamsWhy do we use reinforcement learning in the hyperparameters optimization? Stock markets change all the time. Even if we manage to train our GAN and LSTM to create extremely accurate results, the results might only be valid for a certain period. Meaning, we need to constantly optimise the whole process. To optimize the process we can:Note: The purpose of the whole reinforcement learning part of this notebook is more research oriented. We will explore different RL approaches using the GAN as an environment. There are many ways in which we can successfully perform hyperparameter optimization on our deep learning models without using RL. But… why not.Note: The next several sections assume you have some knowledge about RL — especially policy methods and Q-learning.Without explaining the basics of RL we will jump into the details of the specific approaches we implement here. We will use model-free RL algorithms for the obvious reason that we do not know the whole environment, hence there is no defined model for how the environment works — if there was we wouldn’t need to predict stock prices movements — they will just follow the model. We will use the two subdivisions of model-free RL — Policy optimization and Q-learning.One crucial aspect of building a RL algorithm is accurately setting the reward. It has to capture all aspects of the environment and the agent’s interaction with the environment. We define the reward, R, as:Reward=2∗lossG+lossD+accuracyG,where lossG, accuracyG, and lossD are the Generator’s loss and accuracy, and Discriminator’s loss, respectively. The environment is the GAN and the results of the LSTM training. The action the different agents can take is how to change the hyperparameters of the GAN’s D and G nets.What is Rainbow?Rainbow (link) is a Q learning based off-policy deep reinforcement learning algorithm combining seven algorithm together:(Advantage, formula is A(s,a)=Q(s,a)−V(s), generally speaking is a comparison of how good an action is compared to the average action for a specific state. Advantages are sometimes used when a ‘wrong’ action cannot be penalized with negative reward. So advantage will try to further reward good actions from the average actions.)Note: Stay tuned — I will upload a MXNet/Gluon implementation on Rainbow to Github in early February 2019.Proximal Policy Optimization (PPO) is a policy optimization model-free type of reinforcement learning. It is much simpler to implement that other algorithms and gives very good results.Why do we use PPO? One of the advantages of PPO is that it directly learns the policy, rather than indirectly via the values (the way Q Learning uses Q-values to learn the policy). It can work well in continuous action spaces, which is suitable in our use case and can learn (through mean and standard deviation) the distribution probabilities (if softmax is added as an output).The problem of policy gradient methods is that they are extremely sensitive to the step size choice — if it is small the progress takes too long (most probably mainly due to the need of a second-order derivatives matrix); if it is large, there is a lot noise which significantly reduces the performance. Input data is nonstationary due to the changes in the policy (also the distributions of the reward and observations change). As compared to supervised learning, poorly chosen step can be much more devastating as it affects the whole distribution of next visits. PPO can solve these issues. What is more, compared to some other approaches, PPO:Note: For the purpose of our exercise we won’t go too much into the research and optimization of RL approaches, PPO and the others included. Rather, we will take what is available and try to fit into our process for hyperparameter optimization for our GAN, LSTM, and CNN models. The code we will reuse and customize is created by OpenAI and is available here.Some ideas for further exploring reinforcement learning:Instead of the grid search, that can take a lot of time to find the best combination of hyperparameters, we will use Bayesian optimization. The library that we’ll use is already implemented — link.Finally we will compare the output of the LSTM when the unseen (test) data is used as an input after different phases of the process.2. Plot after 50 epochs.The RL run for ten episodes (we define an eposide to be one full GAN training on the 200 epochs.)This notebook is entirely informative. None of the content presented in this notebook constitutes a recommendation that any particular security, portfolio of securities, transaction or investment strategy is suitable for any specific person. Futures, stocks and options trading involves substantial risk of loss and is not suitable for every investor. The valuation of futures, stocks and options may fluctuate, and, as a result, clients may lose more than their original investment.All trading strategies are used at your own risk.There are many many more details to explore — in choosing data features, in choosing algorithms, in tuning the algos, etc. This version of the notebook itself took me 2 weeks to finish. I am sure there are many unaswered parts of the process. So, any comments and suggestion — please do share. I’d be happy to add and test any ideas in the current process.Thanks for reading.Best, Boris",10/01/2019,16,131,24,"(639, 333)",21,19,0.0,54,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,expectation/interest
219,30 Amazing Machine Learning Projects for the Past Year (v.2018),Mybridge for Professionals,Mybridge,41000.0,6.0,681,"For the past year, we’ve compared nearly 8,800 open source Machine Learning projects to pick Top 30 (0.3% chance).This is an extremely competitive list and it carefully picks the best open source Machine Learning libraries, datasets and apps published between January and December 2017. Mybridge AI evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of Github stars is 3,558.Open source projects can be useful for data scientists. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Machine Learning projects you may have missed for the past year.<Recommended Learning>A) Neural NetworksDeep Learning A-Z™: Hands-On Artificial Neural Networks[68,745 recommends, 4.5/5 stars]B) TensorFlowComplete Guide to TensorFlow for Deep Learning with Python[17,834 recommends, 4.6/5 stars]<Others>A) Web hosting: Get free domain name for a year. For your ‘simple’ personal website or project site.(Click the numbers below. Credit given to the biggest contributor.)FastText: Library for fast text representation and classification. [11786 stars on Github]. Courtesy of ……….. [ Muse: Multilingual Unsupervised or Supervised word Embeddings, based on Fast Text. 695 stars on Github]Deep-photo-styletransfer: Code and data for paper “Deep Photo Style Transfer” [9747 stars on Github]. Courtesy of Fujun Luan, Ph.D. at Cornell UniversityThe world’s simplest facial recognition api for Python and the command line [8672 stars on Github]. Courtesy of Magenta: Music and Art Generation with Machine Intelligence [8113 stars on Github].Sonnet: TensorFlow-based neural network library [5731 stars on Github]. Courtesy of deeplearn.js: A hardware-accelerated machine intelligence library for the web [5462 stars on Github]. Courtesy of Nikhil Thorat at Google BrainFast Style Transfer in TensorFlow [4843 stars on Github]. Courtesy of Pysc2: StarCraft II Learning Environment [3683 stars on Github]. Courtesy of Timo Ewalds at DeepMindAirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research [3861 stars on Github]. Courtesy of Facets: Visualizations for machine learning datasets [3371 stars on Github]. Courtesy of Google BrainStyle2Paints: AI colorization of images [3310 stars on Github].Tensor2Tensor: A library for generalized sequence to sequence models — Google Research [3087 stars on Github]. Courtesy of Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more) [2847 stars on Github]. Courtesy of Jun-Yan Zhu, Ph.D at BerkeleyFaiss: A library for efficient similarity search and clustering of dense vectors. [2629 stars on Github]. Courtesy of Facebook ResearchFashion-mnist: A MNIST-like fashion product database [2780 stars on Github]. Courtesy of Han Xiao, Research Scientist Zalando TechParlAI: A framework for training and evaluating AI models on a variety of openly available dialog datasets [2578 stars on Github]. Courtesy of Alexander Miller at Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit [2571 stars on Github].Pyro: Deep universal probabilistic programming with Python and PyTorch [2387 stars on Github]. Courtesy of Uber AI LabsiGAN: Interactive Image Generation powered by GAN [2369 stars on Github].Deep-image-prior: Image restoration with neural networks but without learning [2188 stars on Github]. Courtesy of Dmitry Ulyanov, Ph.D at SkoltechFace_classification: Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV. [1967 stars on Github].Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind’s WaveNet and tensorflow [1961 stars on Github]. Courtesy of Namju Kim at Kakao BrainStarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation [1954 stars on Github]. Courtesy of Ml-agents: Unity Machine Learning Agents [1658 stars on Github]. Courtesy of DeepVideoAnalytics: A distributed visual search and visual data analytics platform [1494 stars on Github]. Courtesy of Akshay Bhat, Ph.D at Cornell UniversityOpenNMT: Open-Source Neural Machine Translation in Torch [1490 stars on Github].Pix2pixHD: Synthesizing and manipulating 2048x1024 images with conditional GANs [1283 stars on Github]. Courtesy of Horovod: Distributed training framework for TensorFlow. [1188 stars on Github]. Courtesy of Uber EngineeringAI-Blocks: A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models [899 stars on Github].Deep neural networks for voice conversion (voice style transfer) in Tensorflow [845 stars on Github]. Courtesy of That’s it for Machine Learning Open Source of the Year. Visit our publication to find top posts for more programming skills.",05/01/2018,1,36,2,"(336, 267)",36,1,0.0,86,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,positive,expectation/interest
220,180 Data Science and Machine Learning Projects with Python,Coders Camp,Aman Kharwal,8500.0,4.0,61,"In this article, I will introduce you to more than 180 data science and machine learning projects solved and explained using the Python programming language.I hope you liked this article on more than 180 data science and machine learning projects solved and explained by using the Python programming language. Feel free to ask your valuable questions in the comments section below.",01/01/2021,0,180,0,"(700, 394)",1,1,0.0,180,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,positive,joy/calmness
221,Computer made Japanese letters through Variational Autoencoder,,Tomer Nahshon,59.0,9.0,1005,"It’s been a while since I posted a new blog post having been busy with other things in my life. I have been working on this project for a while now. And now, when it is finally done I can share it with you.Besides my interest in Machine Learning and AI algorithms in general, I have another not very common hobby, and it is the Japanese language. I have been studying it for a while now and can comprehend some technical field jargon in the ML field (機械学習 and ディープラーニング ) although I still have a long way to go. With that said, I thought to myself, why not join my two biggest passions together and build a cool project?I decided to design a computer algorithm which can reproduce Japanese letters (especially Hiragana and Katakana - ひらがなとカタカナ) using a Variational autoencoder.The database I used in this project is from the “ ETL Character Database”. The letters are organized in a very unusual way so make sure to read the instructions of how to handle the different databases (ETL 1–9).The first part we will cover is the preprocessing of our data. As I said, the database is not very friendly to Data scientists (although of course massive projects are on a whole different caregory). Let’s open the dataset:In this part I am opening a single character from the database (using ETL -4 database only at the moment). The code I am using is taken from here with different tweaks concerning the execution . As you can see the letter needs a serious preprocessing like cropping, filtering out the noise and stronger greyscale contrast in order to recoginze the character (which is 小 by the way, means “small” ).This function(create_data) creates the dataset itself in order to handle it in numpy array for convenience. There are 6113 pictures (grey scale) with resolution of 76x72 pixels. Let’s set a function that cleans our dataset.I implemented a simple gaussian blur and then thresholding (otsu’s histogram method) and a “TOZERO” binarization in order to preserve the stroke pressure grey scale. Did this in order to get a better a results hopefully later on, When we create the Japanese letters.Let’s check some random samples from the dataset.Great, let’s move on to our model after this preprocessing phase.First, I will explain in a nutshell the concept of the VAE in order to shed some light for those of you who are not familiar with this architecture. For a more in depth and elaborate explanation you can try this page. It gives a very thorough explanation about the relationship with Probablistic graphical models and deep learning concepts.Autoencoders are in great use in many fields of data science. The autoencoders can be used to compression of feature vectors, anomaly detection etc. It is based on unsupervised approach. The main idea of the autoencoder is to encode the data (labeled x in the graph) to a smaller dimension vector and then try to decode it back to the original (reconstruct) x’. The main difference between the AE (Autoencoder) and the VAE is that in the VAE the middle layer is considered as a normal distribution (every node represents its own normal distribution). How do we achieve that? — good question.2 main things are different then the AE:So you asking, how will the network will synthesize it’s own Japanese letters then?What we’re going to do is , after we trained the network on our dataset and made sure we were satisfied with the reconstruction, we will sample variables from standard normal distribution. Later, insert it as the input to the Decoder only and watch the output of the network ,which is basically random since we don’t have a designated input besides random variables sampled from a normal distribution! nice, isn’t it?So our goal here is to train the network with the dataset we preprocessed beforehand in order to create new handwritten Japanese letters that are not part of the dataset but based on it.I seperated the preprocessing and the model to two seperate notebooks since the “bitstring” package and tensorflow weren’t compatible for some reason on my rig. Saving the data we preprocessed using:And went on to the next notebook:Let’s move on to build our encoder and our model:The batch size is set to 32, as a loss function we have a leaky relu (probably other loss functions will do in this simple model) with the negative end slope set to 0.3 (totally arbitrary) and not used in all layers. Our dataset is made of greyscale images of 72*72 . dropout helped to avoid isolated activated neurons and overfitting. Kernel size not too big of 4 and other standard hyperparameters. The number of latent units (the sampled layer) is 12 after several failed optimization attempts.z variable holds all the hidden units composed of mean and a standard deviation multiplied by a normal distribution sampled epsilon (This is the reparametrization trick mentined before)This is the decoder, it recieves a sampled z like we mentioned before (12 latent variables) and reconstructs the image.The most important part in this piece of code is the loss function definition, as we discussed before we have two parts, the image loss (MSE/L2 loss for this simple image) and the latent loss for the KL divergence. Adam optimizer, learning rate and all the other stuff are quite standard.This is the batch function for convenient input. And now let’s move on to train the network.You can see here the reconstruction ability in the beginning of training(left) and the end of the training session (right).Now, after we finished training let’s see if the decoder is able to produce a new letter from a random sampled z (normally distributed).Well we got a decent looking さ(“sa”) as you can see. I guess with more hyperparameters tuning we can get way better results.The notebooks: Preprocessing , Model.Thanks for reading so far, make sure to take a look at the reference link since I took some of my code from there.References:For any questions , let me know: tomer@nahshoh.netThank you!",09/11/2018,12,44,10,"(458, 432)",7,2,0.0,11,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,subjective,positive,joy/calmness
222,Instance Segmentation with Mask R-CNN and TensorFlow on Onepanel,Onepanel,Joinal Ahmed,7.0,7.0,692,"This project can be easily forked from Onepanel .Instance segmentation is the task of identifying object outlines at the pixel level. Compared to similar computer vision tasks, it’s one of the hardest possible vision tasks. Consider the following asks:Training DatasetWe’ve created this python scriptwhich is then connected to a NodeRED flow, which scrapes images from google image search and puts them to the desired location.I picked a total of 100 images and divided them into a training set and a validation set. Finding images is easy. Annotating them is the hard part.Wait! Don’t we need, like, a million images to train a deep learning model? Sometimes you do, but often you don’t. I’m relying on two main points to reduce my training requirements significantly:First, transfer learning. Which simply means that, instead of training a model from scratch, I start with a weights file that’s been trained on the COCO dataset (we provide that in the github repo). Although the COCO dataset does not contain a balloon class, it contains a lot of other images (~120K), so the trained weights have already learned a lot of the features common in natural images, which really helps. And, second, given the simple use case here, I’m not demanding high accuracy from this model, so the tiny dataset should suffice.There are a lot of tools to annotate images. Onepanel has CVAT inbuilt which is a simple interactive video and image annotation tool for computer vision. Annotating the first few images can be very slow, but once you get familiar used to the user interface, you can annotate at around an object a minute.There isn’t a universally accepted format to store segmentation masks. Some datasets save them as PNG images, others store them as polygon points, and so on. To handle all these cases, our implementation provides a Dataset class that you inherit from and then override a few functions to read your data in whichever format it happens to be.Code Tip:An easy way to write code for a new dataset is to copy coco.py and modify it to your needs. Which is what I did. I saved the new file as balloons.pyBalloonDataset class looks like this:load_balloons reads the JSON file, extracts the annotations, and iteratively calls the internal add_class and add_image functions to build the dataset. load_mask generates bitmap masks for every object in the image by drawing the polygons.image_reference simply returns a string that identifies the image for debugging purposes. Here it simply returns the path of the image file.You might have noticed that my class doesn’t contain functions to load images or return bounding boxes. The default load_image function in the base Dataset class handles loading images. And, bounding boxes are generated dynamically from the masks.To verify that code is implemented correctly use this Jupyter notebook. It loads the dataset, visualizes masks and bounding boxes, and visualizes the anchors to verify that my anchor sizes are a good fit for my object sizes.Here is an example of what you should expect to see:The configurations for this project are similar to the base configuration used to train the COCO dataset, so I just needed to override 3 values. As I did with the Dataset class, I inherit from the base Config class and add my overrides:The base configuration uses input images of size 1024x1024 px for best accuracy. I kept it that way. My images are a bit smaller, but the model resizes them automatically.Mask R-CNN is a fairly large model. Especially that our implementation uses ResNet101 and FPN. So you need a modern GPU with 12GB of memory. It might work on less, but I haven’t tried. I used Onepanel’s K80 instances to train this model, and given the small dataset, training takes less than an hour.Start the training with this command, running from the balloon directory. Here, we’re specifying that training should start from the pre-trained COCO weights. The code will download the weights from our repository automatically:And to resume training if it stopped:Use this script to spin up multiple jobs using various set of hyperparameters to train your models parallel and choose the best one.Read more about the Onepanel SDK/CLI here .References:https://github.com/matterport/Mask_RCNN",05/09/2019,6,15,8,"(566, 305)",5,1,0.0,8,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,anger/irritation
223,Polls Apart,Stats with Cats,Charlie Kufs,267.0,8.0,1351,"Election season is here so you can be sure a plethora of polls will soon be adding to the mayhem. Polls educate us in two ways. They tell us what we, or at least the population being polled, think. And, in a more Orwellian sense, they tell us what we should think. Polls are used to guide how the nation is governed. For example, did you know that the unemployment rate is determined from a poll, called the Current Population Survey? Polls are important, so we need to be enlightened consumers of poll results lest we come to “love Big Brother.”The growth of polling has been exponential, following the evolution of the computer and statistical software. Before 1990, the Gallup Organization was pretty much the only organization conducting presidential approval polls. Now, there are several dozen. On average, there were only one or two presidential approval polls conducted per month. Within a decade, that number had increased to more than a dozen. These pollsters don’t just ask about Presidential approval, either. Polls are conducted on every issue of real importance and most of the issues of contrived importance. Many of these polls are repeated to look for changes in opinions over time, between locations, and for different demographics. And that’s just political polls. There has been an even faster increase in polling for marketing, product development, and other business applications.So to be an educated consumer of poll information, the first thing you have to recognize is which polls should be taken seriously. Forget internet polls. Forget polls conducted in the street by someone carrying a microphone. Forget polls conducted by politicians or special-interest groups. Forget polls not conducted by a trained pollster with a reputation to protect.For the polls that remain, consider these four factors:Here’s what to look for.The percent difference between the choices on a survey is often the only thing people look at, with good reason. It is often the only thing that gets reported. Reputable pollsters will always report their sample size, their methods, and even their poll questions, but that doesn’t mean all the news agencies, bloggers, and other people who cite the information will do the same. But the percent difference between the choices means nothing without also knowing the margin-of-error. Remember this. For any poll question involving two choices, such as Option A versus Option B, the largest margin of error will be near a 50%-50% split. Unfortunately, that’s where the difference is most interesting, so you really need to know something about the actual margin of error.You might have seen surveys report that the percent difference between the choices for a question has a margin-of-error of plus-or-minus some number. In fact, the margin-of-error describes a confidence interval. If survey respondents selected Option A 60% of the time with a margin-of-error of 4%, the actual percentage in the sampled population would be 60% ± 4%, meaning between 56% and 64%, with some level of confidence, usually 95%.For a simple random sample from a surveyed population, the margin-of-error is equal to the square root of a Distribution Factor times a Choice Factor divided by a Sample Size Factor times a Population Correction.So the entire equation for the margin-of-error is:Or in mathematical notation:This formula can be simplified by making a few assumptions.These assumptions reduce the equation for the margin-of-error to, roughly, 1/√n. What could be simpler? Here’s a chart to illustrate the relationship between the number of responses and the margin-of-error. The margin-of-error gets smaller with an increase in the number of respondents, but the decrease in the error becomes smaller as the number of responses increases. Most pollsters don’t use more than about 1,200 responses simply because the cost of obtaining more responses isn’t worth the small reduction in the margin-of-error. Don’t worry about the Current Population Survey, though. The Bureau of Labor Statistics polls about 110,000 people every month so their margin-of-error is less than half of a percentage point.Always look for the margin-of-error to be reported. If it’s not, look for the number of survey responses and use the chart or the equation to estimate the margin of error. Here’s a good point of reference. For 1,000 responses, the margin-of-error will be about ±3% for 95% confidence. So if a political poll indicates that your candidate is behind by two points, don’t panic; the election is still too close to call.Sampling error in a survey involves how respondents are selected. You almost never see this information reported about a survey for several reasons. First, it’s boring unless you’re really into the mechanics of surveys. Second, some pollsters consider it a trade secret that they don’t want their competition to know about especially if they’re using some innovative technique to minimize extraneous variation. Third, pollsters don’t want everyone to know exactly what they did because then it might become easy to find holes in the analysis.Ideally, potential respondents would be selected randomly from the entire population of respondents. But you never know who all the individuals are in a population, so have to use a frame to access the individuals who are appropriate for your survey. A frame might be a telephone book, voter registration rolls, or a top-secret list purchased from companies who create lists for pollsters. Even a frame can be problematical. For instance, to survey voter preferences, a list of registered voters would be better than a telephone book because not everyone is registered to vote. But even a list of registered voters would not indicate who will actually be voting on Election Day. Bad weather might keep some people at home while voter assistance drives might increase turnout of a certain demographic. Limiting polling places or voting-by-mail can have a HUGE impact on voter turnout.A famous example of a sampling error occurred in 1948 when pollsters conducted a telephone survey and predicted that Thomas E. Dewey would defeat Harry S. Truman. At the time, telephones were a luxury owned primarily by the wealthy, who supported Dewey. When voters, both rich and poor, went to the polls, it was Truman who was victorious. This may seem obvious in retrospect but there’s an analogous issue today. When cell phones were introduced, the numbers were not compiled into lists, so cell phone users, primarily younger individuals, were under sampled in surveys conducted over land lines.Another issue is that survey respondents need to be selected randomly from a frame by the pollster to ensure that bias is not introduced into the study. Open-invitation internet surveys fail to meet this requirement, so you can never be sure if the survey has been biased by freepers. Likewise, if someone with a microphone approaches you on the street it’s more likely to be a late night talk show host than a legitimate pollster.Measurement error in a survey usually involves either the content of a survey or the implementation of the survey. You might get to see the survey questions but you’ll never be able to assess the validity of the way a survey is conducted.Content involves what is asked and how the question is worded. For example, you might be asked “what is the most important issue facing our country” with the possible responses being flag burning, abortion, social security, Congressional term limits, or earmarks. Forget unemployment, the economy, wars, education, the environment, and everything else. You’re given limited choices so that other issues can be reported to be less important.Politicians are notorious for asking poll questions in ways that will support their agenda. They also use polls not to collect information but to dissemination information about themselves or disinformation about their opponents. This is called push polling.The last thing to think about for a poll is how the responses were collected. For autonomous surveys, look at how the questions are worded. For direct response surveys, even if you could get a copy of the script used to ask the questions, there’s no telling what was actually said, what body language was used, and so on. Professional pollsters may create the surveys but they are often implemented by minimally trained individuals.Originally published at http://statswithcats.net on May 8, 2011.",09/05/2011,0,4,10,"(563, 331)",4,3,0.0,7,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
224,Conv1D Layers in Time-Series,,Albert Um,,4.0,630,"For this blog, I will describe the parameters of the Conv1D layer and a simple WaveNet-like DCNN(Dilated Convoluted Neural Network) used for a time-series problem. Hackathon contestants predominantly solved time-series problems with ARIMA and GRU or LSTM neural networks. However, recent DCNN architectures have been showing up on winning hackathon notebooks. A convolutional layer is a piece of a neural network architecture often used for image classification. Still, CNN can also be applied as a sequence model with the right formatting and parameterization.The Conv1D layer has some interesting characteristics that I can use to help solve sequence problem. I came across an exciting architecture from a blog post created by Joseph Eddy, which explains a build-up of a simple WaveNet-like model for time series. Check the blog out here.The objective is to use past observations to predict the next time step. Separating the dataset into windows of input and output is necessary before fitting the model. The diagram below represents an input shape of 8 and an output shape of 1. The objective is to use the inputs(past observations) to predict the output(future).The inputs from the diagram above take in the 8 inputs to output the next time step. The model can get more involved by adding dense layers between the input and the output. However, it might be even more interesting to create dense layers where the inputs are more refined. For example, the first hidden layer’s output might take in the 2 time-separated inputs in a binary tree-like fashion. I can use the Conv1D layer as a time separated dense layer for this task with 6 hidden layers.The above architecture can become problematic as the input size increases because the number of hidden layers will increase one to one. If the input shape were 16 instead of 8, the number of hidden layers would increase by 8.To reduce the number of hidden layers, a WaveNet architecture used dilated convolutions, which practically skips some of the inputs in between the hidden layers. Instead of applying the filter(weights) in a sequential form, the information between the hidden layers skips at a “dilation rate.”A sample neural network might look like the one below. The for loop is only to collect the layers of the skipped inputs as the dilation rates increase.The filter parameter is the number of filters/weights applied to the n_inputs and n_features shape. For the above example, with an input shape of 8, 1(8 inputs, 1 feature), the output of Conv1D(filters = 16) will produce 16 different outcomes resulting in a shape of (8, 16).The kernel size is the size of the sequential window of the input. If the kernel size is set at 1, then each time interval will have its kernel and therefore, the output shape won’t change from the (8, 16)[16 filters as above example]. However, a kernel size of 2 means that the window of input is the 2 adjacent inputs like below.Without padding, the output of the Conv1D layer with kernel size of 2 will reduce to shape from (8, 16) to (7,16).With ‘causal’ padding, zeros are included only on the left side of the time input so that the output can match the original input shape(8).Setting ‘same’ under the padding parameter will fill in zeros from both left and right sides of the input shape. It’s unnecessary to define this when kernel size is at 1, but I included it in the code for the code example.The dilation rate is the skipping rate for the kernels. At dilation_rate = 1 and a kernel_size = 2, the inputs are paired sequentially. At dilation_rate = 2, the inputs from the first hidden layer(dilation_rate=1) will skip 1 as shown below. I don’t believe the greyed neurons appear during computation; however, I included them for this visualization.",12/02/2021,0,0,1,"(700, 397)",7,0,0.0,3,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,anger/irritation
225,"Interview with Ian Goodfellow, Research Scientist at OpenAI",WithTheBest,With The Best,630.0,3.0,692,"July 2016Ian Goodfellow is a top machine learning contributor and research scientist at OpenAI. Not only did he invent Generative Adversarial Networks (GANs), max-out networks, multi-prediction deep-boltzmann machines, and a fast inference algorithm for spike-and-slab sparse coding while doing his PhD - he also led the development of Pylearn2 (the machine learning library for ML researchers), and contributed greatly to Theano. Cutting his teeth at Google then becoming Senior Research Scientist on the Google Brain team, Ian has now found his way to OpenAI - the non-profit research institution funded in part by Elon Musk and Peter Thiel and is working hard on developing breakthrough Deep learning techniques. Oh, he’s also lead author of a recently launched three part series available online co-written with Yoshua Bengio and Aaron Courville. We catch up with him.WTB: Your book www.deeplearningbook.org covers Applied Math and Machine Learning Basics, explores Modern Practical Deep Networks, and Deep Learning Research. Who will benefit from this incredible knowledge source?In the short term, I expect that software engineers who want to get involved in deep learning will benefit the most from the textbook.In the long run, hopefully everyone will benefit, because more engineers will use deep learning and build smarter apps that help everyone, even people who don’t know that their app uses deep learning.WTB: Personally, what’s most exciting about machine learning today?I’m excited that machine learning now works well enough that we can focus on making it private and secure. There is a lot of work on resisting adversarial examples and on differential privacy.WTB: With the information overload — how can we ensure efficient organisation and collaboration?Within an institution, I think it’s important to keep teams small and focused, and to offer a nice low-bandwidth way for people to communicate between teams. At OpenAI we use Slack, and I think it works very well.Across the whole AI research community, it’s actually very difficult to stay caught up with everything that’s going on. A few years ago, I felt like I actually knew absolutely everything that was happening within the field of deep learning, but now I don’t think that is feasible anymore. Every day, 4–5 new papers come out on ArXiv. I don’t think I even know everything being done with GANs. It’s important to talk to other people a lot, and find out which papers your friends think are really important.WTB: Tell us about OpenAI Gym pleaseI don’t personally work on OpenAI Gym, but I can tell you about it anyway.Most advances in AI have been triggered by the availability of better datasets, not the invention of a new algorithm (source: edge).For reinforcement learning, we don’t need just a dataset, we need entire environments. Gym provides a unified framework for reinforcement learning environments, and also provides several specific environments, in order to provide the data necessary to spark the next advance in reinforcement learning.WTB: What’s the most exciting part of your job?The most exciting moment is when something suddenly works after weeks of it not working.WTB: What advice would you give to budding AI research scientists and developers?Focus on learning the fundamentals: good linear algebra, probability, and software engineering skills. The state of the art in machine learning changes from one year or even month to the next, but the fundamentals stay the same for decades.WTB: What did you enjoy about speaking at AI With The Best 1st Edition and are you excited about September’s talk?As a researcher, I’m excited about the potential of technology to transform society, but most of research-related institutions don’t actually make much use of technology. Most conferences are still held only in person and require everyone who attends to buy expensive plane tickets and release a lot of carbon into the atmosphere.I like that AI With the Best uses the internet to bring everyone together, so there are fewer barriers to attendance and a truly global audience.Thanks so much Ian for chatting with us!You can catch Ian’s talk “Practical Methodology for Deploying Machine Learning” from last year’s edition of AI With The Best.Find out more about Generative Adversarial Networks with Ian 11.20am Saturday 24th at AI With The Best online conference September 24–25th 2016",27/07/2016,0,14,8,"(700, 570)",1,0,0.0,5,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
226,Crazy GPT-3 Use Cases,Towards AI,Przemek Chojecki,5300.0,2.0,232,"GPT-3 has taken the world by storm. There are thousands of tweets about it with numerous mind-blowing use cases which you can see for yourself if you search Twitter for #gpt3 hashtag.In this text, I have taken a bunch of them to show general trends. In brief, GPT-3 allows humans to communicate with machines in Simple English. That means that by simply describing what you want to do you can get:GPT-3 definitely will influence how we communicate with our devices and lower the level of technical sophistication one needs to build new applications. As such it will be a tool in a process of democratization of AI.If you want to test similar models yourself, check out Contentyze.If you want to test similar models yourself, check out Contentyze. This is the platform I’m building with my team, with a goal to make creating texts much simpler. Our beta version is now open for tests.Finally if you prefer a video version of this text, have a look here:Also in my previous video I have analysed the actual paper from OpenAI on GPT3. So if you have a technical background and what to learn more about GPT-3 in the context of Transformers and past models like Megatron, Turing-NLG and GPT-2 have a look at this video:If you want to stay tuned for more news about AI, technology, and machine learning, subscribe to my newsletter here:",27/07/2020,0,2,0,"(700, 292)",1,2,0.0,17,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,joy/calmness
227,Time Series Forecasting with RNNs,Towards Data Science,Marek Galovič,252.0,6.0,1043,"In this article I want to give you an overview of a RNN model I built to forecast time series data. Main objectives of this work were to design a model that can not only predict the very next time step but rather generate a sequence of predictions and utilize multiple driving time series together with a set of static (scalar) features as its inputs.On a high level, this model utilizes pretty standard sequence-to-sequence recurrent neural network architecture. Its inputs are past values of the predicted time series concatenated with other driving time series values (optional) and timestamp embeddings (optional). If static features are available the model can utilize them to condition the prediction too.Encoder is used to encode time series inputs with their respective timestamp embeddings [x] to a fixed size vector representation [S]. It also produces latent vectors for individual time steps [h] which are used later in decoder attention. For this purpose, I utilized a multi-layer unidirectional recurrent neural network where all layers except the first one are residual.In some cases you may have input sequences that are too long and can cause the training to fail because of GPU memory issues or slow it down significantly. To deal with this issue, the model convolves the input sequence with a 1D convolution that has the same kernel size and stride before feeding it to the RNN encoder. This reduces the RNN input by a factor of n where n is the convolution kernel size.Context layer sits between the inputs encoder and a decoder layer. It concatenates encoder final state [S] with static features and static embeddings and produces a fixed size vector [C] which is then used as an initial state for the decoder.Decoder layer is implemented as an autoregressive recurrent neural network with attention. Input at each step is a concatenation of previous sequence value and a timestamp embedding for that step. Feeding timestamp embeddings to the decoder helps the model learn patterns in seasonal data.At the first step, encoder takes the context [C] as an initial cell value and a concatenation of initial sequence value [v] and first timestamp embedding [E] as a cell input. First layer then emits attention query [q] that is fed to attention module which outputs a state [s] that is then used as a cell state in the next step. Lower layers of the decoder don’t use attention. Outputs of the decoder [o] are the raw predicted values which are then fed to the next step together with a timestamp embedding for that step.Attention allows the decoder to selectively access encoder information during decoding. It does so by learning a weighting function that takes previous cell state [q] and a list of encoder outputs [h] as an input and outputs a scalar weight for each of the encoder outputs. It then takes a weighted sum of encoder outputs, concatenates it with the query and takes a nonlinear projection as a next cell state. Mathematically this can be formulated as follows:I won’t do a step by step tutorial on how to prepare data for a sequence to sequence learning problem in this article but I’ll try to give you an overview of the steps needed to get the model working.First you want to make sure that the span of your time series features doesn’t overlap with the span of your targets and that the latest features timestamp is right before first targets timestamp. Also, if you have any static features (aggregate statistics for example) they need to be aggregated up to the last features timestamp. I know this sounds obvious but sometimes it’s really easy to overlook and get excited by how great your model performs just to find out that you leaked future information into your training dataset.The data, both features and targets, need to be normalized into a suitable range for a neural network model. This usually means somewhere between -1 and 1. Normalization scheme I decided to go with was to first take a log to remove any potential skew and then compute a mean and standard deviation. Input to the network is then a z-score of the log.For the target value there are multiple normalization options. One could for example forecast a relative change from the latest input value (can be an issue in case it’s 0) or normalized absolute values using a similar approach I described above for the features.As an example for this article I used the model described above to predict closing price of Shopify stock for next five trading days given data from last sixty trading days. An input sequence convolution layer with kernel/stride = 5 was used to reduce the encoder RNN input size from 60 to 12 steps.One could argue that the stock price is unpredictable without taking other factors such as news into consideration (even then it’s very hard). That is why I decided to use another six tickers (Apple, Amazon, Google, Facebook, Microsoft and IBM) as inputs to the model so that it can learn possible correlations between them. The features used were daily Open, High, Low, Close Price (OHLC) and Volume. I augmented the time series features with “spread” (abs(high-low)) and a past 60 days mean of each feature as a static input.Chart 2. shows a mean absolute error per day. We can see that the further we go into the future the worse our predictions become. Intuitively this makes sense as the model has a better chance of making a good prediction for the very next trading day than five days from now.As with every machine learning model there are successes where the model makes a very good prediction and failures where the prediction is not so great. The following are some examples of such cases.Recurrent neural networks are a powerful tool for modeling sequential data. The model described in this article can be applied to many problems ranging from sales forecasting to energy consumption forecasting. It can condition its predictions on multivariate input series together with scalar inputs which makes it flexible enough to incorporate multiple data sources. Tensorflow implementation of the model can be found here.If you liked this article please recommend it to others. Also, if you have any suggestions feel free to leave a comment below.",02/11/2018,0,23,2,"(621, 314)",10,0,0.0,6,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,trust/acceptance
228,Polynomial Regression with Scikit learn: What You Should Know,Towards Data Science,Angela Shi,765.0,6.0,1065,"Polynomial regression is an algorithm that is well known. It is a special case of linear regression, by the fact that we create some polynomial features before creating a linear regression. Or it can be considered as a linear regression with a feature space mapping (aka a polynomial kernel). With this kernel trick, it is, sort of, possible to create a polynomial regression with a degree that is infinite!In this article, we will deal with the classic polynomial regression. With scikit learn, it is possible to create one in a pipeline combining these two steps (Polynomialfeatures and LinearRegression). I will show the code below. And let’s see an example, with some simple toy data, of only 10 points. Let’s also consider the degree to be 9. You can see the final result below.Do you see anything wrong?Well, in theory, this is wrong! For 10 points, a 9th-degree polynomial should fit them perfectly!Or maybe, I am sure that some of you are thinking: why are you saying that this is wrong? This may be the right model. You think that the model should fit perfectly, but no, you are confused with polynomial interpolation!First, you can try it for yourself using the following code to create the model.Before talking about the difference between polynomial regression and polynomial interpolation. Let’s first talk about an answer that I got from the scikit learn team: you should not be doing this, expansion to a 9th-degree polynomial is nonsense. And scikit learn is built for practical use cases, and it works with finite-precision representations, not theoretical representations.Yes, they are totally right! Just look at the numbers, how big they become: 1e24!But if they cannot handle big numbers, shouldn’t they throw an error or a warning? Without any message, one will just consider that the model is correct, whereas, well, it is actually not.OK OK, I know, some of you are not convinced that the result is wrong, or maybe it is impossible to handle big numbers, let's see with another package, numpy!For the same example, polyfit from numpy has no problem finding the model. You can see the plot and the code below.Now I know some of you are thinking: polyfit is a very different thing, it is an interpolation and not regression.Because when asking around, I got some answers like this (but they are not accurate, or wrong):polyfit is doing an altogether different thing. It is performing a univariate polynomial fit for some vector x to a vector y. Here we are performing a polynomial expansion of some feature space X in order to represent high-order interaction terms (equivalent to learning with a polynomial kernel) for a multivariate fit.OK, what is a polynomial interpolation?Well, for this kind of question, Wikipedia is a good source.In numerical analysis, polynomial interpolation is the interpolation of a given data set by the polynomial of lowest possible degree that passes through the points of the dataset.And we have this result that is proven: given n+1 distinct points x_0,x_0,… ,x_n and corresponding values y_0,y_1,… ,y_n, there exists a unique polynomial of degree at most n that interpolates the data (x_0,y_0),… ,(x_n,y_n).Going back to our example: there are 10 points, and we try to find a 9th-degree polynomial. So technically, we are doing polynomial interpolation. And polyfit found this unique polynomial! This is not the case for scikit learn’s polynomial regression pipeline!And this is precisely why some of you are thinking: polyfit is different from scikit learn’s polynomial regression pipeline!Now, wait!In polyfit, there is an argument, called degree. So you can modify the degree, let’s try with 5.Yes, with polyfit, it is possible to choose the degree of the polynomial and we are doing polynomial regression with it. And degree 9, chosen by the user, is the special case of polynomial interpolation.And it is reassuring because the linear regression tries to minimize the squared error. And we know that if there are 10 points, and we try to find a polynomial of degree 9, then the error can be 0 (can’t be lower!) because of the theorem of polynomial interpolation.For those who are still doubting, there is the official document for polyfit: Least squares polynomial fit. Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). Returns a vector of coefficients p that minimizes the squared error in the order deg, deg-1, … 0.OK, time to go back to our scikit learn’s polynomial regression pipeline. So now, why the difference? Are there really two different polynomial regression (or fit), using both Least Squares, but using them differently?I found this answer, but I am not getting it yet.Both models uses Least Squares, but the equation on which these Least Squares are used is completely different. polyfit applies it on the vandemonde matrix while the linear regression does not.While digging around, another important transformation of features should be mentioned: feature scaling.In several books on machine learning, when performing polynomial regressions, the features are scaled. Maybe from the beginning, some of you were saying that it should be done.And yes, scikit learn’s polynomial regression pipeline with the feature scaling, seems to be equivalent to polyfit! according to the plot (I didn’t really check, but visually they are the same).You can use the code below:Now, we didn’t answer our previous questions, and we have more questions: does feature scaling have an effect on linear regression?Well, the answer is No.To discuss this, there can be another article written and for our discussion about the effect of polynomial regression, we can just do another transformation.That’s right, you just divide the predictors by 1000. Now, you know that the effect on the linear regression model is only proportional, but in practice, the difference is huge.So that is why we can conclude that the initial numbers are too big for scikit learn.In the end, we can say that scikit learn’s polynomial regression pipeline (with or without scaling), should be equivalent to numpy’s polyfit, but the difference in terms of big number handling can create different results.And personally, I think that scikit learn should throw an error or at least a warning in this case.I really would like to know your opinions!If you like to know more about how polynomial regression is related to other supervised learning algorithms, you can read this article:towardsdatascience.comYou will see that the polynomial regression is a special kind of feature space mapping.",25/06/2020,6,23,12,"(439, 273)",6,0,0.0,2,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,expectation/interest
229,Train your own object detector with Faster-RCNN & PyTorch,,Johannes Schmidt,,18.0,3662,"After working with CNNs for the purpose of 2D/3D image segmentation and writing a beginner’s guide about it, I decided to try another important field in Computer Vision (CV) — object detection. There are several popular architectures like RetinaNet, YOLO, SDD and even powerful libraries like detectron2 that make object detection incredibly easy. In this tutorial, however, I want to share with you my approach on how to create a custom dataset and use it to train an object detector with PyTorch and the Faster-RCNN architecture. I will show you how images that were downloaded from the internet can be used to generate annotations (bounding boxes) with the help of the multi-dimensional image viewer napari. The provided code is specifically written for Faster-RCNN models, but parts might work with other model architectures (e.g. YOLO) because general principles apply to all common object detection models that are based on anchor/default boxes. Due to transfer learning, you will see that training an object detector sometimes requires very few images! You can find the code and a jupyter notebook on my github repo.For this tutorial, I am going to train a human head detector. I can imagine that this is a common task for phone camera applications: detecting human faces or heads within an image. If you want to train your own object detector, e.g. for racoon detection, car detection or whatever comes into your mind, you’re at the right place. So please go ahead. It might be useful for you.For training and experiment management, I will use PyTorch Lightning and neptune. If you’re not familiar with these packages, do not worry, you’ll be able to implement your own training logic and choose your own experiment tracker. Here’s the table of content:In order to train an object detector with a deep neural network like Faster-RCNN we require a dataset. For this, I downloaded 20 images (selfies) from the internet. You can do this manually or use web scraping techniques. All images are .jpg or .png rgb or rgba files.Here is the full dataset:Let’s assume you have downloaded your images into /heads/input. Before adding bounding boxes to our input images, we should first rename our files so that they all follow the same pattern. An example on how to rename them is shown below. This simply renames all files within a directory to something like 000.jpg, 001.png etc. You can find the function get_filenames_of_path() in the utils.py script.There are plenty of web tools that can be used to create bounding boxes for a custom dataset. These tools usually store the information in a or several specific files, e.g. .json or .xml files. But you could also save your annotations as python dicts if you don’t want to learn another file format.Edit: I’m using `.json` in my repo.Pytorch’s Faster-RCNN implementation requires the annotations (the target in network training) to be a dict with a boxes and a labels key anyway. The boxes and labels should be torch.tensors where boxes are supposed to be in xyx2y2 format (or xyxy format as stated in their docs) and labels are integer encoded, starting at 1 (as the background is assigned 0). The easiest form to save a dict as a file is using the pickle module. Fortunately, the torch package integrates some functionality of pickle, e.g. it allows us to save a file like a dict with torch.save() and load it with torch.load(). This means that we can store the annotations that we create in a pickled file. If this annotation file happens to have the same name as the image, mapping the image to its annotation file becomes really easy and creating a dataset for neural network training as well. If you already have a labeled dataset at hand, you can skip this section.As I recently discovered napari, a multi-dimensional image viewer for python, I decided to use it to generate the labels/annotations for my dataset. Please do not expect a full fledged, perfectly working code for creating bounding boxes. This is just me making myself familiar with napari and using for my needs. If you prefer an out-of-the-box solution, I recommend taking a look at myvision.ai.Let’s take a look at how to generate annotation files for our heads dataset with napari. For this, I heavily made use of napari’s shapes layer and created a specific Annotator class that makes annotating much easier. You can run the code within a jupyter notebook or an IPython kernel. No need to run the magic command %gui qt, as this is automatically called before starting the qt application.This will open the napari qt-application that shows one image at a time. You can navigate through your list of images by pressing ’n’ to get the next or ‘b’ to get the previous image (custom key-bindings).Now, if you would like to add a label with bounding boxes for the current shown image, just enter the following into your IPython console or jupyter notebook session.You just need to specify the label you want and the color. Now you can start using napari’s functionality to draw bounding boxes.Note: Don’t worry if you accidentally click ’n’ or ‘b’ on your keyboard. The created bounding boxes are saved automatically. It also doesn’t matter if you delete the image layer, as the image is read from disk every time you display the image (e.g. by clicking ’n’ or ‘b’). However, if you delete the shape layer for a label, this information is lost for this image.We can create as many classes as we want. For each new class, a new shape layer is created, which means we can hide specific labels if the image is cluttered with bounding boxes. We basically can do whatever we want with the bounding boxes, e.g. changing it’s color or width etc.If you want to export the annotations for this image, you can write the following:You could also specify a name for the annotation file. When no name is given, the image’s name is taken and the .pt extension appended. I recommend this approach as this makes resuming a labeling session with the Annotator possible. Here’s an example, where annotation_ids is a list of pathlib.Path objects, similar to image_files.If the annotation files are in the right format and have the same name as the image itself, these annotations will be used. You can for example start labeling a bigger dataset, export some of the annotations that you managed to create in a certain time and resume labeling at a later time.Let’s continue to create bounding boxes for every image we have. For this tutorial, we’ll stick to our heads bounding boxes and delete the eye layer that I showed above. Once you’re satisfied with the result, you can export all annotations in one go with:For this project we now have two directories, something like /heads/input and /heads/target. In /heads/input, we find all images that we downloaded and in the /heads/target directory the corresponding annotations with bounding boxes and labels that we just generated. 20 images and 20 annotation files (pickled python dicts) in total. Let’s quickly take a look at the annotation files.This gives us the following:Looks about right. The labels and boxes are both stored in numpy.ndarrays in xyxy format. Now we can build a proper dataset for network training.Here’s how to build your own dataset that you can use to feed the network with batches of data. The approach is similar to my previous tutorial: 2D/3D semantic segmentation with the UNet.Let’s take a look at the dataset class ObjectDetectionDataSet:To better understand the arguments, here’s some more information:Let’s use this class to build the dataset for our head detector.As you can see in this example I use the class ComposeDouble. This allows to stack different transformations. Clip() is used to identify the bounding boxes that are bigger than the actual image and clips them accordingly. To augment the dataset one can use the albumentation module, for which I wrote the AlbumentationWrapper class. In order to use any numpy based function on the data, one can use the FunctionWrapper class. This wrapper takes in a function and an arbitrary number of arguments to return a functools.partial. I use Double to highlight that the data comes in input-target pairs (image + annotation) as opposed to Single. Whether the input or target should be transformed, can be specified with the boolean arguments input and target. By default, only the input is used. For more information I encourage you to take a look at transformations.py. In this example, we linearly scale our image and bring it in the right dimensional order: [C, H, W].We can now take a look at a sample from the dataset with:We can see that the sample is a dict with the keys: ‘x’, ‘x_name’, ‘y’, ‘y_name’.These transformations are, however, not the only ones. The Faster R-CNN implementation by PyTorch adds some more, which I will talk about in the next section. But first, let us again visualize our dataset. This time, we can pass the dataset as an argument with the DatasetViewer class instead of passing a list of image paths.This will open a napari application, that we can navigate with the keyboard buttons ’n’ and ‘b’ again. There is, however, only one shape layer that contains the bounding boxes of every label. We can assign a color to different labels by passing a dict to our DatasetViewer or changing the color within the napari viewer instance. The label is shown on the top left corner of every bounding box. You probably can barely see it, as the text’s color is white by default. But you can change the size and color, either accessing the napari viewer instance directly with datasetviewer.viewer or by opening a small GUI application withThis functions takes in the shapes layer for which we would like to change the text properties. The GUI is shown on the bottom left of the viewer and was created with magicgui.The dataset is now ready for network training. In the next chapter we’ll talk about the Faster R-CNN implementation.In this tutorial I made use of PyTorch’s Faster R-CNN implementation. Taking a look at the provided functions in torchvision, we see that we can easily build a Faster R-CNN model with a pretrained backbone. I decided to go for a ResNet backbone (either with or without FPN). You can take a look at the functions I created in faster_RCNN.py in my github repo. To quickly assemble your model with a ResNet backbone, you can use the get_fasterRCNN_resnet() function and specify the details such as the backbone, anchor size, aspect ratios, min_size, max_size etc. I will talk about these parameters in detail in the following section.torchvision.models.detection.faster_rcnn.FasterRCNN has some requirements for its input. Here’s an important section from its docstring:As you have seen, our dataset outputs the data in a different format — a dict. For this reason, we need to write our own collate_fn when instantiating a dataloader. The dataset’s output should be transformed into a list of tensors and a list of dicts for the target. Here’s how this could be done:To get a batch from your dataset, you just need to callRemember that I said that there are some additional transformations happening in PyTorch’s Faster R-CNN implementation? As a matter of fact, it uses a transformer that can be found here: torchvision.models.detection.transform.GeneralizedRCNNTransformThe important arguments for this transformer are the following:To investigate the behavior of this transformer, we should take a look at how our data will look like after it is transformed with GeneralizedRCNNTransform:As our backbone was trained on ImageNet, we will use the same normalization values for mean and std. For training, I would like to have my 20 images to be comparable in size, so I choose 1024 for both, min_size and max_size. Again, I will use the DatasetViewer to visualize the dataset:Notice how there appears to be a padded border at the bottom. This is automatically added by the transformer to provide adequate image sizes to the model without distorting them too much. To better see the impact of the transformation, we can gather some statistics from our dataset with and without the transformer:stats and stats_transform are dicts with the following keys:Here’s an example:Alright, that’s basically all you need to know about the implementation. The next section covers the probably most important hyperparameter for training an object detector — anchor boxes.If you have a hard time understanding anchor boxes, you should probably read more about them first. I think this is mandatory to really understand current object detection approaches. To better understand the relationship between anchor boxes, the input image and the feature map(s) that are returned by the feature extractor (backbone), I think it is best to visualize it. This also helps in choosing good anchor sizes and aspect ratios for your problem. For this reason I created the class AnchorViewer. In this example, I will use a simple ResNet backbone (e.g. ResNet18) without FPN that outputs a (512, 32, 32) feature map when given an image of size (3, 1024, 1024).The AnchorViewer returns a napari application with three layers: the image, the shape and the points layer. The image layer displays the image that is taken from the dataset, the shape layer shows the first anchor boxes for a given feature map location and the points layer displays all available anchor positions mapped onto the image. You probably can imagine how cluttered the image would be if one would display anchor boxes of every position. Visualizing the anchor boxes and their positions within the image makes it easier to find adequate anchor boxes. In Faster-RCNN, these boxes are compared to the ground truth boxes. Boxes that have an IoU greater than a certain threshold are considered positive cases. In layman’s terms, that’s how the target for a given image is generated for network training.The image here is the first of the heads dataset, which is a (3, 1024, 1024) image. You can identify the feature_map_size for example by sending a dummy torch.tensor through the backbone model:These are expected to be tuples of tuples of integers. In essence, the tuple (128, 256, 512) contains the different sizes for the first feature map of the backbone’s output. In our example, the backbone only returns one feature map, which is why we should write it like this: ((128, 256, 512), ). The same applies to the aspect ratios.For training, we could use our own training loop logic. However, I think it is best to make use of higher level APIs such as Lightning, Fast.ai or Skorch. But why? This is well explained in this article. You can probably imagine that if you want to integrate functionalities and features, such as logging, metrics, early stopping, mixed precision training and many more to your training loop, you’ll end up doing exactly what others have done already. However, chances are that your code won’t be as good and stable as theirs (hello spagetthi code) and you’ll spent too much time on integrating and debugging these things rather than focusing on your deep learning project (hello me). And although learning a new API can take some time, it might help you a lot in the long run.Here, I will use Lightning, because it gives you a lot control for training without abstracting away too much. It’s a good fit for researchers. But there’s definitely room for improvement in my opinion. Although Lightning encourages you to integrate your model and the dataset into your lightning module, I will disregard this advice and write a LightnigModule like this:Some things you might already have noticed but I want to highlight anyway, because that’s what I like about Lightning:However, you could do all this manually and overwrite the existing behavior as stated on their website.In my implementation, the __init__ method only requires a few arguments:While the first two are self explanatory, the IoU deserves some attention. This argument is an important value for the evaluation of the model, for which I use the code of this github repo. This computes the metrics used by the pascal voc challenge. This aspect of object detection took me probably the longest to get a good grasp on, so I’d recommend reading a bit about object detection metrics. The threshold essentially determines when to count a prediction as a true postivite (TP), based on the IoU.Luckily, we do not need to worry about the loss function that was proposed in the Faster-RCNN paper. It is part of the Faster-RCNN module and the loss is automatically returned when the model is in train() mode. In eval() mode, the predictions, their labels and their scores are returned as dicts. Therefore, it is sufficient to write the loss calculation in the training loop like this:For training, I’d recommend following the guidelines in the literature and to stick to classic SGD. The learning rate and the parameters for the learning rate scheduler were arbitrarily chosen. It’s possible to move these parameters to the init method and then view them as important hyperparameters, but they’ll work for this example and might as well for others.In the Lightning module, you may have noticed that I use logging commands to keep track of my running losses and metrics. The logging software I will use is neptune. But your are not bound to use neptune, you could instead use a csv logger, tensorboard, MLflow or others without changing the code. neptune is just personal preference and only the second logger I’ve used so far. Right after my rather disappointing experience with tensorboard.Now that we have put everything together and spent some time on building the dataset, talked about the model implementation, the training logic and evaluation, we’re ready to write our training script. Here it is:For our 20 images, I made a manual split like this:Apart from that, there is not much interesting code here. I create the different datasets and assign them different stacks of transformations. I use a dict with parameters that I want to have logged by neptune, which you’ll see in the next section.This is how I personally initialize my neptune logger, there are other and probably better ways to do so. The project might need to be created beforehand.Next, we can initialize our Faster-RCNN model with:and init our lightning module with:Finally, we can create our trainer with callbacks:Now we can start training:You can watch the training progress on neptune. Once the training is finished, we can use the best model, based on the validation datset and according to the metric we used (mAP from pascal VOC) and predict the bounding boxes of our test dataset:This is how training looks like in neptune:Our neptune logger will create plots based on the values that are given to the logger in the lightning module:We can also monitor memory usage during training:It’s also possible to log some additional information of the experiment. For example, all packages and versions of the conda environment that was used.This will log html tables that can be accessed in the artifacts section:With lightning, checkpoints (e.g. based on a metric) are automatically saved to a directory that can be specified. After training is finished, I like to upload the model’s weights to neptune and link it to the experiment. Instead of uploading the checkpoint, I prefer to upload the model itself, for which I use:With lightning still undergoing many changes with every release, I like the model to be separate from the lightning model. This is just personal choice.We can now use our trained model to make some predictions on similar but unseen data. Let’s download some more images (e.g. selfies) from google, which I will store in /heads/test:To load our model from neptune or from disk, we can write a simple script:Again, I use a dictionary at the beginning of my script to allow some customization. As my dataset does not have a target, I use ComposeSingle, FunctionWrapperSingle, as well as ObjectDetectionDatasetSingle with collate_single. To visualize this dataset, I will use the DatasetViewerSingle class. Instead of downloading the model from neptune, I’ll just load in the checkpoint and extract the model. I use the parameters saved in the experiment to initialze the correct model and load the weights.For inference, I simply loop through the dataset and predict the bounding boxes of each image. The resulting dictionary with boxes, labels and scores for every image is saved in the specified path:In order to visualize the results, I can create a dataset the same way I created the training dataset:And here’s the result:The results already look pretty good! However, there might be redundant, overlapping bounding boxes that we need to get rid of. The first thing that comes into mind, is to set a threshold for the score. Every bounding box with a score that falls below a certain threshold is removed. We can play around with a score threshold by creating a small GUI within the napari viewer:A score of 0.727 allows us to only see the high scoring bounding boxes. This gives us pretty good results and a working object detector. A slightly better solution, however, is using non-maxium supression. A good description of NMS can be found in this article:NMS greedily selects the bounding box with the highest score and suppresses ones that have a high overlap with it. The overlap is measured by comparing Intersection-over-Union (IoU) threshold to a predefined threshold, usually ranging from 0.3 to 0.5.We can experiment with different IoU thresholds for NMS by creating another small GUI within the napari viewer (This will destroy the score slider though):An IoU threshold of 0.2 seems to be a good fit for these test images.Et voilà, we have trained an object detector, that works remarkably well for detecting human heads!This tutorial showed you that training an object detector can be as simple as annotating 20 images and running a Jupyter notebook. I hope this made it easier for you to start your own deep learning object detection project. If you have any questions, feel free to contact me on Github or LinkedIn.",23/02/2021,18,56,28,"(731, 491)",18,5,0.0,31,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
230,Train DeepLab v3 + with your own dataset,,MLBoy,,4.0,318,"You can train DeepLab v3 + with the original dataset.Use the official TensorFlow model.How to use DeepLab is basically written in the official repository.If you have any questions, please read this article.The images above are PASCAL VOC dataset, but you can train with your own dataset.1. Clone the official model repository2. Work in the research directory.3. Select TensorFlow1 when working with Colab.4. Install tf_slim.The required modules areNumpyPillow 1.0tf Slim (which is included in the “tensorflow/models/research/” checkout)MatplotlibTensorflowIf not, please install them.5. Add the tensorflow / models / research / directory to PYTHONPATH so that you can use the library.Prepare the following.1,Original images2, Class images・ Image requirementsTools such as labelme can be used to create segmentation data.The black and white converted image is saved in the SegmentationClassRaw directory.It is a black and white image with a small label value, so it is almost black.2. Create a text file of list of the image file names.image0image1image3Create a text file with a list of file names in a format without an extension.Create the following 3 text files.1, trainval.txt: All image file names2, train.txt: Training set (assign about 90% of trainval?)3. val.txt: Of trainval.txt, internal verification set (allocate about 10% of trainval?)There seem to be various theories about the optimal balance between training and verification allocation.3. Make the dataset in TFRecord format.Make dataset TFRcord format that can be read efficiently by training in TensorFlow.2. Download the pre-trained model.Use the weights of the pretrained model for transfer learning.Download your favorite checkpoint from the official repository link.There are MobileNetv2 base (22MB) and Xception base (439MB).2. In the case of Xception_65 backboneAfter training with 5000 images, 30000 epochs, MobileNetv2 backbone, and Colab GPU, it took about 1 hour.The result was very satisfying, probably because there was only one object (2 labels) that I wanted to segment.Test with a validation set.The original image and the color segmentation map image are saved in the log directory.🐣****Request for work:rockyshikoku@gmail.com",10/10/2020,11,0,1,"(475, 248)",4,4,0.0,4,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
231,GAN Loci,Towards Data Science,Kyle Steinfeld,51.0,14.0,2858,"This project applies generative adversarial networks (or GANs) to produce synthetic images that capture the predominant visual properties of urban places. Imaging cities in this way represents the first computational attempt to documenting the Genius Loci of a city: those forms, spaces, and qualities of light that exemplify a particular location and that set it apart from similar places.Presented here are methods for the collection of urban image data, for the necessary processing and formatting of this data, and for the training of two known computational statistical models (StyleGAN and Pix2Pix) that identify visual patterns distinct to a given site and that reproduce these patterns to generate new images. These methods have been applied to image nine distinct urban contexts across six cities in the US and Europe, the results of which will be presented in our next post.When a town pleases us because of its distinct character, it is usually because a majority of its buildings are related to the earth and the sky in the same way; they seem to express a common form of life, a common way of being on the earth. Thus they constitute a genius loci which allows for human identification.- Christian Norberg-Schulz, Genius Loci: Towards a Phenomenology of Architecture, p 63While for many architects and urban designers design intent is a necessarily tacit concept, most CAD tools are made to support only the explicit articulation of the intention of their author. In contrast, this project represents a small step toward a new approach to CAD tools based on machine learning techniques that are made to better support tacit or difficult-to-articulate design intent. To clearly demonstrate the needs of design intent such as this, we consider here a scenario that offers an especially difficult to explicitly articulate quality of the built environment: the phenomenon of “place”.In his seminal work that defines a phenomenological approach, “Genius Loci: Towards a Phenomenology of Architecture”, Christian Norberg-Schulz argues that the design of cities and buildings must center on the construction of “place”, which he defines as a “space with a unique character” (Norberg-Schulz 94). But how is this “unique character” defined, and how can it be captured using digital tools in manner that affords the advantages of a computational medium? In anticipation of a future tool that better supports the development of tacit design intent, we seek to leverage methods in machine learning to attempt to capture “place”, as described by Norberg-Schulz.It is the ambition of this project to use a GAN as a tool for tacit design, and to apply the capacity of this technology for capturing the implicit yet salient visual properties of a set of images. We speculate that this capacity will prove useful in uncovering and encoding a phenomenological understanding of place. Presented in overview here are the steps required to train a generative adversarial network (GAN) to produce images that capture the predominant visual properties of an urban context. The work proceeds in three stages: data preparation, model training, and latent space exploration.In the data preparation stage, we first collect, clean, and curate a large number of images related to a selection of urban contexts that are significantly different sorts of places, and compile these into distinct sets. Then, each set of these images is processed to serve as training data for one of two ML models.To this end, a Python library has been developed that supports the collecting, curating, and processing panoramic images using Google’s StreetView API. This section details this process, which includes: tasks related to the identification of a desired geographic location, the collection of a large and diverse set of images from this location, the curation of this set to define a relevant sub-set of valid images, and finally, the processing of these images such that they are appropriately formatted for training.The collection of data begins with the identification of a geographic location of interest. Since the images at which panoramas are taken are likely not coincident with the given geo-locations of interest, a number of failure scenarios must be accommodated. For example: not all locations of interest are related to a panorama, and not all panoramas depict the external urban environment (there are panoramas of interiors as well). For each of the nine urban contexts listed below, approximately 500 panoramas are sampled.Beyond the basic validation mentioned above, due to the structure of the data returned, even given a successful call to the API, a number of auxiliary processing steps are required. Most notable among these auxiliary processing steps is the collection of depth information related to each StreetView panorama. Although it is not made clear from the Google StreetView interface, many two-dimensional panoramas provided by this service also hold some rudimentary three-dimensional data that describes objects in the urban scene (typically building forms).In summary, the collection of data begins with the defining of a single geographic point of interest, and results in the compilation of several hundred samples that may be further processed for training.Each sample includes:With a set of images related to a given urban place collected and curated, the task of the data processing step is to prepare this set of images for their role in training a GAN model. In summary, this training data is best described as pairs of related square-cropped raster images: one RGB image that represents a crop of a larger panorama image scene, and another greyscale image that represents the “depthmap” of this scene, with the value of each pixel representing the minimum distance from the camera to any occluding objects.The production of the sceneographic images is largely straightforward, with just one process worthy of mention: The equalrectangular projection of the panoramic images must be transformed to arrive the cubic environment map that better approximates what we expect for the synthetic images we aim to produce. This is accomplished following previous work describing the relevant conversion (Bourke, 2006). It is noteworthy that the same panoramic image may be arbitrarily rotated along the z-axis (a transformation equivalent to a horizontal rotation of the cube) to produce slight variations of the same scene that may still be seamlessly tiled together. Expanding the breadth of training sets through slight transformations in this manner, a practice known as “data augmentation”, is a common practice in ML.In summary, the data preparation step for any given urban place begins with a curated collection of panoramic equalrectangular images and related information (including a description of occluding planes), and results in two sets of cubemap projection images: one set of RGB images that describe an urban scene, and one set of greyscale images that describe the effective depth of objects in that scene.In the model training stage, we use the collected image sets to train GAN models capable of generating new images related to each selected urban context. To this end, two distinct GAN architectures are employed: StyleGAN and Pix2Pix, the particular implementations of which are discussed below. Once trained, each of these models prove valuable in their own way, as each offers a distinct interface for the production of synthetic urban images.Pix2Pix (Isola et al., 2016) is an architecture for a particular kind of GAN: a conditional adversarial network that learns a mapping from a given input image to a desired output image. From the perspective of a user of a trained Pix2Pix model, we offer an input image that conforms to some mapping convention (such as a color-coded diagram of a facade, or an edge drawing of a cat) and receive in return an image that results from the transformation of this input into some desired output (such as a photographic representation of a facade, or of a cat).The particulars that guide the training of a Pix2Pix model strongly depend upon the specifics of the implementation employed. This project relies upon a “high-definition” version of this architecture implemented in Pytorch (Wang, 2019). Some modifications of this implementation were required: in particular, to correct problems with unwanted artifacts forming in cases of low-contrast source images (as seen in the figure below). Following suggestions offered by the community of Pix2Pix users, zero paddings were replaced with reflection paddings, and the learning rate was temporarily adjusted to 0.0008.Once trained, each model operates as implied by the nature of a conditional GAN and by the structure of the training data: given a greyscale depthmap image that describes a desired three-dimensional urban scene, a synthetic RGB sceneographic image is returned. Since these models are trained on subsets of data segregated by site, each model produces synthetic images specific to just one urban place: the Rotterdam model produces images that “feel” like Rotterdam, while the San Francisco model generates ones that appear more like San Francisco. This feature allows for direct comparisons to be drawn.In contrast with a traditional GAN architecture, StyleGAN (Karras et al., 2018) draws from “style transfer” techniques to offer an alternative design for the generator portion of the GAN that separates coarse image features (such as head pose when trained on human faces) from fine or textural features (such as hair and freckles). Here, in comparison to the Pix2Pix model, the user experience is quite different: rather than operating by mapping an input image to a desired output, users select a pair of images from within the latent space of a trained model, and hybridize them. Rather than a simple interpolation between points in latent space, however, these hybrids correspond to the coarse and fine features of the given pair.As above, the particulars that guide the training of a StyleGAN model strongly depend upon the specifics of the implementation. This project relies on the official TensorFlow implementation of StyleGAN , which was employed without modification to train a single model on a combination of RGB sceneographic data drawn from all nine urban places. Once trained, the model may be queried either by sampling locations in latent space, or by providing coarse-fine pairs of locations in latent space to more precisely control different aspects of the synthetic image.Building upon the former technique of taking samples in latent space, linear sequences of samples may be combined to produce animations such as the ones discussed below.In the image generation stage, we develop methods for interfacing with the trained models in useful ways. This task is non-trivial, since each GAN model, once trained, is capable of producing a vast and overwhelming volume of synthetic images, which is described in terms of a high-dimensional latent space. The StyleGAN model offers a unique form of guiding the generation of images as combinations of features drawn from other images selected from latent space. The Pix2Pix model offers quite a different interface, with new synthetic images generated as transformations of arbitrary given source images: in our case, these are depth-maps of urban spaces. We present here a brief overview of these methods, and leave a more complete unpacking and visual analysis of the resulting images to a future post.Here, greyscale depthmap images are produced by sampling a scene described in a 3d CAD model. These depthmaps of constructed scenes are then used as the source image by which the Pix2Pix models for each urban place produces a synthetic photographic scene. By providing precisely the same input to models trained on different urban places, direct comparisons between the salient features picked up by the transformation models may be made.For example, while each of the synthetic images below were produced by sampling the same depthmap, we can clearly see those imagistic properties that characterize each of the urban places sampled. A large massing that appears in the depthmap is interpreted by the Rotterdam model as a large brick housing block, as is typical in the Dutch city, while the Pickwick Park model renders this massing in a manner typical of the Northern Florida flora, suggesting the mass of a mossy Live Oak. A long and receding urban wall is broken up by the Alamo Square model into a series of small scale forms, an interpretation that expresses the massing of a line of Edwardian townhouses that dominate this San Francisco neighborhood; this same urban form is understood as something resembling a red-brick industrial warehouse building by the model trained on images from the Bushwick area of Brooklyn.The other two image generation methods developed here rely on the StyleGAN models.Like the method discussed above, the first of these two offers opportunities for comparisons to be drawn between the urban places sampled. Using the StyleGAN interface as it was intended by its authors, it is possible to separately assert control over the fine and coarse aspects of generated images. The interface that results may be seen as the design of urban scenes “by example”: the user need only offer examples of images that contain desired features of a place, without explicitly stating what these are, where they came from, or how to construct them. In the context of this study, as above, this allows for comparisons between urban places to be conducted. For example, the nearby figure demonstrates how coarse features and fine features may be combined to form new scenes that hybridize aspects of existing scenes.Finally, a method for generating animations by sampling linear sequences of the latent space of images implied by the StyleGAN model is developed. While not directly supportive of a controlled comparative study of urban places, these animations do offer insight into the structure of the latent space of the StyleGAN model, including which features and scenes are similarly parameterized, and which are far from one another.As described above, a GAN instrumentalizes the competition between two related neural networks. Since the effective result of this competition is the encoding of the tacit properties held in common by the given set of images, this project proposes that an interrogation of the synthetic images generated by the GAN will reveal certain properties useful in uncovering the nature of urban places. Relying on Norberg-Schulz’s characterization of a “place”, which is understood to mean a “space with a unique character”, and to include those forms, textures, colors, and qualities of light that exemplify a particular urban location, we can see that the immediate aim has been met, as the initial results documented above exhibit imagistic features unique to each of the sites studied. Much work remains, however, to realize the larger aim of this project to develop tools that support tacit intent in architectural design. Future work in this area includes the extension of the “design by example” paradigm from images of urban places, as demonstrated here, to more directly architectural representations, such as three-dimensional forms and spaces.Aish, Robert, Ruairi Glynn, and Bob Sheil. “Foreword.” In Fabricate 2011: Making Digital Architecture, DGO-Digital original., 10–11. UCL Press, 2017.Bechtel, William, and Adele Abrahamsen. Connectionism and the Mind: Parallel Processing, Dynamics, and Evolution in Networks. Wiley-Blackwell, 2002.Cheng, Chili, and June-Hao Hou. “Biomimetic Robotic Construction Process: An Approach for Adapting Mass Irregular-Shaped Natural Materials.” In Proceedings of the 34th ECAADe Conference. Oulu, Finland, 2016.Chung, Chia Chun, and Taysheng Jeng. “Information Extraction Methodology by Web Scraping for Smart Cities: Using Machine Learning to Train Air Quality Monitor for Smart Cities.” In CAADRIA 2018–23rd International Conference on Computer-Aided Architectural Design Research in Asia, edited by Suleiman Alhadidi, Tomohiro Fukuda, Weixin Huang, Patrick Janssen, and Kristof Crolla, 2:515–524. The Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), 2018.Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. “Generative Adversarial Nets.” In Advances in Neural Information Processing Systems 27, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, 2672–2680. Curran Associates, Inc., 2014. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. “Image-to-Image Translation with Conditional Adversarial Networks.” CoRR abs/1611.07004 (2016). http://arxiv.org/abs/1611.07004.Karras, Tero. StyleGAN. NVIDIA, 2019. https://github.com/NVlabs/stylegan.Karras, Tero, Samuli Laine, and Timo Aila. “A Style-Based Generator Architecture for Generative Adversarial Networks.” CoRR abs/1812.04948 (2018). http://arxiv.org/abs/1812.04948.Kilian, Axel. “Design Exploration and Steering of Design.” In Inside Smartgeometry, 122–29. John Wiley & Sons, Ltd, 2014. https://doi.org/10.1002/9781118653074.ch10.Ledig, Christian, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, et al. “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,” 105–14, 2017. https://doi.org/10.1109/CVPR.2017.19.Ng, Andrew Y., and Michael I. Jordan. “On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes.” In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, 841–848. NIPS’01. Cambridge, MA, USA: MIT Press, 2001. http://dl.acm.org/citation.cfm?id=2980539.2980648.Norberg-Schulz, Christian. Genius Loci: Towards a Phenomenology of Architecture. Academy Editions, 1980.Peng, Wenzhe, Fan Zhang, and Takehiko Nagakura. “Machines’ Perception of Space.” In Proceedings of the 37th Annual Conference of the Association for Computer Aided Design in Architecture (ACADIA). Cambridge, MA: Association for Computer Aided Design in Architecture, 2017.Sculley, D., Jasper Snoek, Alex Wiltschko, and Ali Rahimi. “Winner’s Curse? On Pace, Progress, and Empirical Rigor.” Vancouver, CA, 2018. https://openreview.net/forum?id=rJWF0Fywf.Steinfeld, Kyle. “Dreams May Come.” In Proceedings of the 37th Annual Conference of the Association for Computer Aided Design in Architecture (ACADIA). Cambridge, MA: Association for Computer Aided Design in Architecture, 2017.Wagener, Paul. Streetview Explorer, 2013. https://github.com/PaulWagener/Streetview-Explorer.Wang, Jason, and Luis Perez. “The Effectiveness of Data Augmentation in Image Classification Using Deep Learning.” Convolutional Neural Networks Vis. Recognit, 2017.Wang, Ting-Chun. Pix2PixHD: Synthesizing and Manipulating 2048x1024 Images with Conditional GANs. NVIDIA, 2017. https://github.com/NVIDIA/pix2pixHD.",06/07/2019,0,3,2,"(713, 630)",21,2,0.0,28,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,expectation/interest
232,Painless Text Classification Using RNN,Level Up Coding,Aaron Lee,84.0,7.0,1259,"This will be a minimal working example of Natural Language Processing (NLP) using deep learning with a Recurrent Neural Network (RNN) in Python. For this project, you should have a solid grasp of Python and a working knowledge of Neural Networks (NN) with Keras.The goal here to build a NLP deep learning model to analyze Twitter sentiment about Apple and Google products. The dataset I used comes from CrowdFlower via data.world. In this dataset, human raters were asked to rate the sentiment of over 9,000 Tweets as positive, negative, or neither. I am seeking to use this labled dataset to build a NLP model that can predict the sentiment of the Tweet based on its text content. (with an aim for Proof of Concept only). The dataset can be found at the link below.data.worldI found NLP fairly confusing until I did a couple projects. Usually, a Google search will land me on some tutorial or blog that I can vibe with. For RNN, I had a tough time finding one that clicked for me, so I wanted to share what I did here in the hopes of helping out another lost soul and avoid pulling up the ladder behind me.I also won’t go into the weeds here with math or theory. Just a simple overview and some code that should get a working example and a way forward for someone new to the subject/skill.We start (as we do in most example projects) by downloading the csv and placing it into the local project folder. We then read the data into a Pandas DataFrame with the code below. (Line 4)Our dataset contains the text of each tweet in a column called “tweet_text”, and it has a target column which I have renamed to “emotion” for convenience (Line 5). I dropped any NaN values (Line 6) leaving over 9000 rows. The first 5 look like this:I used the nltk (natural language toolkit) library to tokenize all of our texts and determine the number of words contained in all of our tweets (Lines 8–9). We end up with over 10,000 unique words, and will use this number later for the input to our NN.In this next block of code below, we import everything we need to build our deep learning neural network. We then set the ‘emotion’ column as our target, and we use one hot encoding (line 12 ) to transform our target since it can be one of four categories (positive, negative, neutral, or can’t tell). Since I have four categories, the shape of our target/output will also be 4. This is important for our neural network later. One of my early struggles with Neural Networks was getting the shapes to match between layers (especially the input and output layers). Once I had a better understanding of the layers, it really clicked for me.In the second half of the above code block, Keras does a lot of work for us:At this point in our project, we have our ‘y’ which is the target/output of our NN, and ‘X’ which contains the numerical input arrays for each tweet. Now we are ready to make our neural network.We started with 9000+ tweets, and now we do a standard train test split on them before building our neural network.Before moving on, I think it is prudent to generally understand the purpose of each of our layers. We are creating a Sequential model, and using the ‘add’ method to build the NN:Embedding Layer: This layer requires that the input data be integer encoded, so each word being fed into the embedding layer is represented by a unique integer. We did this task earlier with the tokenizer.The embedding layer is initialized with random weights and will be learning for all words in the training dataset.The embedding layer has arguments including:The output of this layer is a 2D vector with one embedding for each word in the input sequence of words.LSTM: Our RNN layer will immediately follow the input and must occur before any dense or pooling layers. This is where the magic happens for a recurrent neural network (RNN). Traditional RNNs have lost popularity in favor of more robust versions like LSTM (Long Short term Memory) and GRU (Gated Recurrent Unit) layers. However, what makes them special remains the same. An RNN has a memory and the ability to remember and/or forget.RNNs can temporarily store word weights based on their importance and relative location to other words in the text. They are well-suited to classifying, processing, and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. This also makes them great for NLP as it lets your model pick up on nuances of structured language like proximity and context using relative positions within the text, and also by remembering important words for later in the text.We follow our layer with a 1D pooling layer to simplify the output.Dense and Dropout Layers: We add some standard dense layers to our network. We also add dropout layers with the intention of improving generalization. The number of layers and nodes you use are adjustable if you want to tune your model further.Note that our final output layer must have the same shape as our target. For this project, we use a Dense layer of size 4 to match the number of categories of my output (emotion).Now it’s time to compile the model. Here we provide important parameters such as which loss function to use (‘categorical_crossentropy’, since this is a multiclass classification problem), and the optimizer to use.After compiling the model, we can quickly check the summary to see what the model looks like, and make sure the output shapes line up with what you expect.Now we can fit the compiled model. This step can take some time depending on the parameters you have chosen.More epochs improves the overall result, but can also greatly affect the training time. The model generate scoring outputs as it fits the model. You can look at the validation score it prints out to see if the model is still improving with each epoch to know where an acceptable cutoff might be.The batch size limits the number of samples to be shown to the network before a weight update can be performed. 32 is a common size for smaller datasets.Now we can test our model using the data we set aside with the split. We can use any of a number of scoring metrics to see how we did. Though not shown here, this is also where you would create a confusion matrix and do additional evaluation of your results. For simplicity, we will just look at the overall accuracy of the model.We end up with an accuracy over 80% which turns out to be a pretty good result for this multiclass problem. With further tuning, we can increase the accuracy (or other metrics as desired).That’s it! A working RNN model for NLP. Not a full project yet, but it should give you some working code.I recommend trying to build another NLP project from scratch as well. A dataset you might try is https://www.kaggle.com/uciml/news-aggregator-dataset. It is a collection of 400,000 news headlines categorized as [‘business’, ‘entertainment’, ‘health’, ‘science and tech’]. You can use a very similar technique to what was shown here to create a model that categorically classifies the articles by looking at only their title.I’d love to hear from you if you find this helpful, but also if you find any errors or need a little help.Best of luck!",07/05/2021,3,5,0,"(700, 369)",3,2,0.0,4,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,neutral,expectation/interest
233,Review: CRF-RNN — Conditional Random Fields as Recurrent Neural Networks (Semantic Segmentation),Towards Data Science,Sik-Ho Tsang,6100.0,7.0,307,"In this story, CRF-RNN, Conditional Random Fields as Recurrent Neural Networks, by University of Oxford, Stanford University, and Baidu, is reviewed. CRF is one of the most successful graphical models in computer vision. It is found that Fully Convolutional Network (FCN) outputs a very coarse segmentation results. Thus, many approaches use CRF as post-processing steps to refine the output semantic segmentation map obtained from the the network, such as DeepLabv1 & DeepLabv2, to have a more fine-grained segmentation results. However, the parameters of CRF are not trained together with FCN. In other words, the FCN is unaware of CRF during training. This might limit the network capability.In CRF-RNN, authors proposed to formulate CRF as RNN so that they can integrated with FCN and train the whole network in an end-to-end manner to obtain a better results. It is a 2015 ICCV paper with over 1300 citations. (Authors have also created a live demo for it:Here are my trials, it is quite funny:It is quite accurate, of course I also tried some that CRF-RNN can’t work.Though CRF-RNN is published in 2015, this paper has introduced an important concept/logic to me, i.e. converting/approximating a conventional/non-deep-learning approach into deep-learning-based approach and turn it into an end-to-end solution.[2015 ICCV] [CRF-RNN]Conditional Random Fields as Recurrent Neural NetworksImage Classification[LeNet] [AlexNet] [ZFNet] [VGGNet] [Highway] [SPPNet] [PReLU-Net] [STN] [DeepImage] [GoogLeNet / Inception-v1] [BN-Inception / Inception-v2] [Inception-v3] [Inception-v4] [Xception] [MobileNetV1] [ResNet] [Pre-Activation ResNet] [RiR] [RoR] [Stochastic Depth] [WRN] [FractalNet] [Trimps-Soushen] [PolyNet] [ResNeXt] [DenseNet] [PyramidNet] [DRN]Object Detection[OverFeat] [R-CNN] [Fast R-CNN] [Faster R-CNN] [DeepID-Net] [CRAFT] [R-FCN] [ION] [MultiPathNet] [NoC] [G-RMI] [TDM] [SSD] [DSSD] [YOLOv1] [YOLOv2 / YOLO9000] [YOLOv3] [FPN] [RetinaNet] [DCN]Semantic Segmentation[FCN] [DeconvNet] [DeepLabv1 & DeepLabv2] [SegNet] [ParseNet] [DilatedNet] [PSPNet] [DeepLabv3] [DRN]Biomedical Image Segmentation[CUMedVision1] [CUMedVision2 / DCAN] [U-Net] [CFS-FCN] [U-Net+ResNet] [MultiChannel]Instance Segmentation[DeepMask] [SharpMask] [MultiPathNet] [MNC] [InstanceFCN] [FCIS]Super Resolution[SRCNN] [FSRCNN] [VDSR] [ESPCN] [RED-Net] [DRCN] [DRRN] [LapSRN & MS-LapSRN] [SRDenseNet]",03/03/2019,1,72,68,"(573, 283)",18,19,0.0,97,en,image,spatial,cnn,convolutional,pooling,convolution,filter,detection,segmentation,overlap,objective,neutral,joy/calmness
234,Everything You Need to Know About Artificial Neural Networks,"Technology, Invention, App, and More",Josh,42000.0,9.0,1986,"The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we’re learning more about how to improve their systems. Everything is starting to align, and because of it we’re seeing strides we’ve never thought possible until now. We have programs that can tell stories about pictures. We have cars that are driving themselves. We even have programs that create art. If you want to read more about advancements in 2015, read this article. Here at Josh.ai, with AI technology becoming the core of just about everything we do, we think it’s important to understand some of the common terminology and to get a rough idea of how it all works.A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). If you’ve read anything about them before, you’ll have read that these ANNs are a very rough model of how the human brain is structured. Take note that there is a difference between artificial neural networks and neural networks. Though most people drop the artificial for the sake of brevity, the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work. Below is a diagram of actual neurons and synapses in the brain compared to artificial ones.Fear not if the diagram doesn’t come through very clearly. What’s important to understand here is that in our ANNs we have these units of calculation called neurons. These artificial neurons are connected by synapses which are really just weighted values. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it “travels.” The weighted result can sometimes be the output of your neural network, or as I’ll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning.Artificial neural networks are not a new concept. In fact, we didn’t even always call them neural networks and they certainly don’t look the same now as they did at their inception. Back during the 1960s we had what was called a perceptron. Perceptrons were made of McCulloch-Pitts neurons. We even had biased perceptrons, and ultimately people started creating multilayer perceptrons, which is synonymous with the general artificial neural network we hear about now.But wait, if we’ve had neural networks since the 1960s, why are they just now getting huge? It’s a long story, and I encourage you to listen to this podcast episode to listen to the “fathers” of modern ANNs talk about their perspective of the topic. To quickly summarize, there’s a hand full of factors that kept ANNs from becoming more popular. We didn’t have the computer processing power and we didn’t have the data to train them. Using them was frowned upon due to them having a seemingly arbitrary ability to perform well. Each one of these factors is changing. Our computers are getting faster and more powerful, and with the internet, we have all kinds of data being shared for use.You see, I mentioned above that the neurons and synapses perform calculations. The question on your mind should be: “How do they learn what calculations to perform?” Was I right? The answer is that we need to essentially ask them a large amount of questions, and provide them with answers. This is a field called supervised learning. With enough examples of question-answer pairs, the calculations and values stored at each neuron and synapse are slowly adjusted. Usually this is through a process called backpropagation.Imagine you’re walking down a sidewalk and you see a lamp post. You’ve never seen a lamp post before, so you walk right into it and say “ouch.” The next time you see a lamp post you scoot a few inches to the side and keep walking. This time your shoulder hits the lamp post and again you say “ouch.” The third time you see a lamp post, you move all the way over to ensure you don’t hit the lamp post. Except now something terrible has happened — now you’ve walked directly into the path of a mailbox, and you’ve never seen a mailbox before. You walk into it and the whole process happens again. Obviously, this is an oversimplification, but it is effectively what backpropogation does. An artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given. When it is wrong, an error is calculated and the values at each neuron and synapse are propagated backwards through the ANN for the next time. This process takes a LOT of examples. For real world applications, the number of examples can be in the millions.Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there’s another question that should be on your mind. How do we know how many neurons we need to use? And why did you bold the word layers earlier? Layers are just sets of neurons. We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.Layers themselves are just sets of neurons. In the early days of multilayer perceptrons, we originally thought that having just one input layer, one hidden layer, and one output layer was sufficient. It makes sense, right? Given some numbers, you just need one set of computations, and then you get an output. If your ANN wasn’t calculating the correct value, you just added more neurons to the single hidden layer. Eventually, we learned that in doing this we were really just creating a linear mapping from each input to the output. In other words, we learned that a certain input would always map to a certain output. We had no flexibility and really could only handle inputs we’d seen before. This was by no means what we wanted.Now introduce deep learning, which is when we have more than one hidden layer. This is one of the reasons we have better ANNs now, because we need hundreds of nodes with tens if not more layers. This leads to a massive amount of variables that we need to keep track of at a time. Advances in parallel programming also allow us to run even larger ANNs in batches. Our artificial neural networks are now getting so large that we can no longer run a single epoch, which is an iteration through the entire network, at once. We need to do everything in batches which are just subsets of the entire network, and once we complete an entire epoch, then we apply the backpropagation.Along with now using deep learning, it’s important to know that there are a multitude of different architectures of artificial neural networks. The typical ANN is setup in a way where each neuron is connected to every other neuron in the next layer. These are specifically called feed forward artificial neural networks (even though ANNs are generally all feed forward). We’ve learned that by connecting neurons to other neurons in certain patterns, we can get even better results in specific scenarios.Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn’t make decisions based on previous knowledge. A typical ANN had learned to make decisions based on context in training, but once it was making decisions for use, the decisions were made independent of each other.When would we want something like this? Well, think about playing a game of Blackjack. If you were given a 4 and a 5 to start, you know that 2 low cards are out of the deck. Information like this could help you determine whether or not you should hit. RNNs are very useful in natural language processing since prior words or characters are useful in understanding the context of another word. There are plenty of different implementations, but the intention is always the same. We want to retain information. We can achieve this through having bi-directional RNNs, or we can implement a recurrent hidden layer that gets modified with each feedforward. If you want to learn more about RNNs, check out either this tutorial where you implement an RNN in Python or this blog post where uses for an RNN are more thoroughly explained.An honorable mention goes to Memory Networks. The concept is that we need to retain more information than what an RNN or LSTM keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other.Sam walks into the kitchen.Sam picks up an apple.Sam walks into the bedroom.Sam drops the apple.Q: Where is the apple.A: BedroomSample taken from this paper.Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. However, the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized. This is done by noting a certain symmetry in how the neurons are connected, and so you can essentially “re-use” neurons to have identical copies without necessarily needing the same number of synapses. CNNs are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels. There’s redundant information contained when you look at each individual pixel compared to its surrounding pixels, and you can actually compress some of this information thanks to their symmetrical properties. Sounds like the perfect situation for a CNN if you ask me. Christopher Olah has a great blog post about understanding CNNs as well as other types of ANNs which you can find here. Another great resource for understanding CNNs is this blog post.The last ANN type that I’m going to talk about is the type called Reinforcement Learning. Reinforcement Learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward, which means that it in itself isn’t an artificial neural network architecture. However, you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before. A great example and explanation can be found in this video, where YouTube user SethBling creates a reinforcement learning system that builds an artificial neural network architecture that plays a Mario game entirely on its own. Another successful example of reinforcement learning can be seen in this video where the company DeepMind was able to teach a program to master various Atari games.Now you should have a basic understanding of what’s going on with the state of the art work in artificial intelligence. Neural networks are powering just about everything we do, including language translation, animal recognition, picture captioning, text summarization and just about anything else you can think of. You’re sure to hear more about them in the future so it’s good that you understand them now!This post was written by Aaron at Josh.ai. Previously, Aaron worked at Northrop Grumman before joining the Josh team where he works on natural language programming (NLP) and artificial intelligence (AI). Aaron is a skilled YoYo expert, loves video games and music, has been programming since middle school and recently turned 21.Josh.ai is an AI agent for your home. If you’re interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.Like Josh on Facebook — http://facebook.com/joshdotaiFollow Josh on Twitter — http://twitter.com/joshdotai",28/12/2015,0,26,6,"(484, 307)",9,0,0.0,32,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,positive,expectation/interest
235,"Fake News Detection with Machine Learning, using Python",Towards Data Science,Piero Paialunga,531.0,5.0,1009,"One of the most challenging area of Machine Learning is the one that regards the language and it is known as Natural Language Processing (NLP). It is true that all the area of Machine Learning can be complex and challenging at some level, but NLP is particularly difficult as it requires to explore human communication and, somehow, human consciousness. Moreover, while it is relatively easy to encode an image in terms of data (i.e. a bidimensional matrix), or a physics experiment (that is basically a .csv file), it is extremely harder to encode a text as a number or a vector.But what do we actually want to solve? What are the so difficult tasks that I’m talking about? Well, for example, in this blog I will discuss an example of text classification. In particular, we want to classify wether or not the news are fake.Moreover, we want to face this task using the State of Art methods proposed by BERT and a special encoder released by Google known as Universal Sentence Encoder. Plus, we will use a traditional Machine Learning tool that is becoming more and more popular for its easiness of use and its interesting features: PyCaret.The theory that is behind BERT or the Universal Sentence Encoder is deep and complex, and it would be necessary more than a single blog to explain it further. Moreover, I would still not be able to explain them as precisely as their creators, so I’m not even going to try it.On the other hand, the practical usage of these tools are really simple and will properly be explained during this post.Let’s go.1The Dataset:The dataset is open-source and can be found here. As it will be clearer, the real and fake news can be found in two different .csv files.2The Libraries:In order to perform this classification, you need the basic Data Scientist starter pack (sklearn, pandas, numpy ,… , ), plus some specific libraries like transformers and pycaret. Here is a list:Note: I had some problems with pycaret so I’ve decided to use it on Google Colab. Plus Google Colab is recommended because the dataset is large and it may affects your computer resources.3Data Exploration:The real news and the fake ones are reported into two csvs.A Target column is added and the data are merged & randomly mixed into a single dataframe known as data.Let’s see if the dataset is well balanced.The Target column is made of strings, and it is not computer-friendly. Let’s adjust it:So right now what we want to do is to take the title of an article and predict wether or not the news is fake.Sounds good. Let’s start dancing.4. Text ClassificationNote: This part is inspired by this great article.Ok, so now that we have the data we can start with the Machine Learning part. The idea behind the BERT fine-tuning is simple. We have an extraordinary good model (and this hides the complexity of the approach) that is trained to perform good in terms of classification. We use this extraordinary good model (named BERT) and we fine tune it to perform our specific task. Pretty simple, isn’t it?Now, follow me.1.Train-Validation split2.Validation-Test split3.Defining the model and the tokenizer of BERT.4.Plotting the histogram of the number of words and tokenizing the text:As almost all the texts have 15 words (approximatively), we truncate all the texts to 15 for computational reasons with few damage5.Converting lists to tensors:6.Data Loader structure definition:7.Freezing the parameters and defining the trainable BERT structure:8.Defining the hyperparameters (optimizer, weights of the classes and the epochs)9. Defining training and evaluation functions:10. Train and predictOk, I know that it is not super-funny, but you should follow all these steps exactly as I’ve written above. Each step is essential.As a final step, let’s check the performance here:Pretty interesting! 88% of accuracy, and high values of precision and recall as well.Here is the classification matrix:Here, the situation is much simpler. The Universal Sentence Encoder is an Encoder (a special way to transform a sentence in a vector) that has been trained on several classification tasks. This permits to change each instance of the dataset into a 512 dimensional vector. The embedding model is called with this line of code:And the dataset is encoded using this one:Let’s do the train-test split:At this point we want to apply traditional Machine Learning methods. In particular, PyCaret permits to adopt almost all the most famous and efficient classification algorithms and compare them. Nonetheless, we don’t want to use 512 x 40000+ numbers, so it is wise to perform a Principal Component Analysis (PCA) dimensional reduction.Let’s do the PCA with 3 components (it will be clear why):Now it goes interesting.If you look at the PCA dataset it gets like that:That means that data are almost linearly separable!Now let’s use PyCaret and its Machine Learning models:Now, the best model is saved as best_model and it is the Random Forest Classifier. Let’s use it to predict the test set.jovian.mlSimilar results (88% of accuracy) and similar confusion matrix too:5 ConclusionsWe are in the fantastic era of Deep Learning. One of the great thing about it is that while it is extremely difficult to train a state of art neural network, it is way easier and faster to use a pretrained neural network, fine tune it and obtain state of art results on your dataset.More complex and efficient methods could be surely applied to this dataset, for example using the entire text or extracting other features.If you liked the article and you want to know more about Machine Learning, or you just want to ask me something you can:A. Follow me on Linkedin, where I publish all my stories B. Subscribe to my newsletter. It will keep you updated about new stories and give you the chance to text me to receive all the corrections or doubts you may have.C. Become a referred member, so you won’t have any “maximum number of stories for the month” and you can read whatever I (and thousands of other Machine Learning and Data Science top writer) write about the newest technology available.Ciao! :)",05/05/2021,0,61,0,"(700, 1050)",1,0,0.0,10,en,word,sequence,attention,sentence,bert,language,representation,text,rnn,hidden,objective,neutral,joy/calmness
236,GAN — How to measure GAN performance?,,Jonathan Hui,,4.0,601,"In GANs, the objective function for the generator and the discriminator usually measures how well they are doing relative to the opponent. For example, we measure how well the generator is fooling the discriminator. It is not a good metric in measuring the image quality or its diversity. As part of the GAN series, we look into the Inception Score and Fréchet Inception Distance on how to compare results from different GAN models.IS uses two criteria in measuring the performance of GAN:Entropy can be viewed as randomness. If the value of a random variable x is highly predictable, it has low entropy. On the contrary, if it is highly unpredictable, the entropy is high. For example, in the figure below, we have two probability distributions p(x). p2 has a higher entropy than p1 because p2 has a more uniform distribution and therefore, less predictable about what x is.In GAN, we want the conditional probability P(y|x) to be highly predictable (low entropy). i.e. given an image, we should know the object type easily. So we use an Inception network to classify the generated images and predict P(y|x) — where y is the label and x is the generated data. This reflects the quality of the images. Next we need to measure the diversity of images.P(y) is the marginal probability computed as:If the generated images are diverse, the data distribution for y should be uniform (high entropy).The figure below visualizes this concept.To combine these two criteria, we compute their KL-divergence and use the equation below to compute IS.One shortcoming for IS is that it can misrepresent the performance if it only generates one image per class. p(y) will still be uniform even though the diversity is low.In FID, we use the Inception network to extract features from an intermediate layer. Then we model the data distribution for these features using a multivariate Gaussian distribution with mean µ and covariance Σ. The FID between the real images x and generated images g is computed as:where Tr sums up all the diagonal elements.Lower FID values mean better image quality and diversity.FID is sensitive to mode collapse. As shown below, the distance increases with simulated missing modes.FID is more robust to noise than IS. If the model only generates one image per class, the distance will be high. So FID is a better measurement for image diversity. FID has some rather high bias but low variance. By computing the FID between a training dataset and a testing dataset, we should expect the FID to be zero since both are real images. However, running the test with different batches of training sample shows none zero FID.Also, both FID and IS are based on the feature extraction (the presence or the absence of features). Will a generator have the same score if the spatial relationship is not maintained?If the generated images look similar to the real images on average, the precision is high. High recall implies the generator can generate any sample found in the training dataset. A F1 score is the harmonic average of precision and recall.In the Google Brain research paper “Are GANs created equal”, a toy experiment with a dataset of triangles is created to measure the precision and the recall of different GAN models.This toy dataset can measure the performance of different GAN model. We can use it to measure the merit of different cost functions. For example, will the new function good at producing high-quality triangle with a good coverage?Improved Techniques for Training GANsAre GANs Created Equal? A Large-Scale StudyGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",18/06/2018,0,2,17,"(700, 251)",10,1,0.0,7,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,surprise/amazement
237,Forecasting with Bayesian Dynamic Generalized Linear Models in Python,Towards Data Science,Ryan Clukey,17.0,10.0,1896,"Forecasting is critical for nearly all businesses when planning for revenue goals, inventory management, headcount, and other economic considerations essential for managing a successful business. Highly accurate forecasts are often difficult to achieve, but essential to enable businesses plan for shifts and changes in a dynamic market. Often forecast models can (and should) take advantage of additional inputs or variables that may help explain the variability in the target or dependent series. For example, predicting the number of units sold based on visits to the website, or forecasting monthly revenue based on monthly marketing spend. As is often the case, the available data is often limited or incomplete, which complicates the ability to use standard algorithms to generate accurate forecasts.In this article I’ll introduce the Bayesian approach to multivariate time series and provide a contrast to traditional frequentist methods, like ARIMA. Time series is a special case of regression where the independent variable is a regular interval time measure (i.e. weeks, months, years, etc.), along with potential exogeneous features which may be useful in explaining the residual variation of a sequential series. ARIMA (Auto Regressive, Integrated, Moving Average) has been one of the standard approaches to time series forecasting; it is a powerful algorithm and widely used across many industries. However, there are many complex considerations: the data should be stationary, there may be multiple trends, independent variables are often lagged, there is seasonality — sometimes multiple seasons in the same data, there must be a considerable amount of available data, etc. For these reasons, time series is a difficult statistical technique to master, and generating accurate forecasts is quite challenging.The Bayesian approach offers a probabilistic approach to time series to reduce uncertainty and incorporate “prior” information. These models are referred to as Dynamic Linear Models or Structural Time Series (state space models). They work by fitting the structural changes in a time series dynamically — in other words, evolving and updating the model parameters over time with the addition of new information. In contrast, ARIMA estimates parameters for the series, which remain fixed, then uses Maximum Likelihood estimation for determining the time series predictions. Bayesian methods use MCMC (Monte Carlo Markov Chains) to generate estimates from distributions. For this case study I’ll be using Pybats — a Bayesian Forecasting package for Python. For those who are interested, and in-depth article on the statistical mechanics of Bayesian methods for time series can be found here.In this case study we evaluate the effect of two independent time series (covariates) on our dependent variable: total number of monthly vehicle purchases for a particular auto manufacturer. Since this is count data, we are looking at a Poisson process, which assumes a different underlying distribution than that of Gaussian. Poisson models are based on counts, and therefore the lowest possible value is 0. The two covariates include: spend on marketing, aggregated at the monthly level and a measure of consumer sentiment for the given brand, normalized to a scale of 0–100.Our data consists of 48 observations between January 2017 and December 2020. There are no missing values. The data represent monthly counts of vehicle sales, average marketing spend (in tens of thousands) and average consumer sentiment — a proprietary metric. Since we are using Python, I’ll provide some code snippets through to demonstrate the analysis steps.Our dataset above shows the three variables we will be using for this analysis: Transactions is the dependent variable, with Marketing Spend and Consumer Sentiment as two Independent variables (or covariates) in the Dynamic Regression model. What we’re essentially saying here is: do Marketing Spend and Consumer Sentiment have a time-varying effect on the number of Transactions; can we model their effects and use the knowledge we have of these variables to generate reasonable predictions of Transactions beyond our current dataset?Let’s take a look at the 3 separate time series we have so far:DV: Number of Vehicles SoldSimply visual examination reveals a couple of important things happening in this series: 1) the data is not stationary, 2) there appears to be a possible seasonal effect, 3) the variance increases with time. For ARIMA, these would be problematic, and our series would be subject to multiple “adjustments” such as differencing and seasonal decomposition in order to achieve the requirement of stationarity.IV 1: Marketing SpendThe plot below shows the variability of Average Marketing Spend over the same period of time. This series is less clear. There might be seasonality, there might be shifts in trend; the series does not appear stationary. However, as mentioned, such considerations are irrelevant for Bayesian approaches to time series.IV 2: Consumer SentimentOur metric of Consumer Sentiment follows closely with that of Marketing Spend, which we might expect given that marketing efforts should have an effect on consumer sentiment.To build the model we will be build a function in Python to make things a little easier. There are a number of parameters to adjust in the model itself, including learning and decay rates, seasonality, how long the prior period should be (to learn the prior variance), etc. I’ll not go into those here, as descriptions and guidelines are provided by the Pybats tutorial.We’re going to use our Python function above to also run the equivalent model without the covariates — so a standard univariate time series. We’re doing this to determine if including the independent variables in the model is having the intended effect of reducing the overall residual error in the model. In other words, does including the variables improve our understanding of the data; is it worth including them in the model?The plot below shows the results of our analysis. There are a number of observations we have. First, the model had a difficult time learning the beginning structure of the series, as evidenced by the wide credible intervals at the beginning of the series. Eventually the model parameters “learned” and the one-step ahead predictions for the multivariate model begin to more closely and follow the original series. This what we expected. However, note that the Univariate series did quite a poor job at capturing the movement and values of the original series. While it appears to learn the trend, its predictions are substantially and consistently off from the original dependent values, especially later in the series. This tells us something about the efficacy of the additional covariates in the model — we’ll come to that next.One-Step-Ahead PredictionsThe plot above shows the output from the Bayesian forecast. It shows one-step-ahead predictions with a 95% Credible Interval. This differs from ARIMA, which produces in-sample predictions.An important difference between in-sample ARIMA predictions and those made with Pybats: In ARIMA, the in-sample predictions are practically quite useless. They reflect how well the estimated parameters fit the data, and to this end you can very easily over-fit the data simply by over parameterizing your model; it tells us nothing about how well the parameters perform on unseen data (not to mention the parameters are fixed). with Pybats, it’s not really an “in-sample” prediction. It’s a one-step-ahead prediction starting from the beginning of the series and updating each parameter as you move through the series. Therefore, each step-ahead is actually a true “out-of-sample prediction”, and thus there error from the prediction reflects true out-of-sample estimates based on the posterior probabilities. For ARIMA to do this, you would need to specify a hold-out sample at the end of your series, then implement a for-loop that iterates of each data point in the hold-out sample, update the model, move to the next point, update the model, etc.Bayesian: Comparing Univariate to MultivariateDid our multivariate Bayesian model perform better than the univariate model? We can observe this difference more readily by comparing the cumulative absolute error in the multivariate and univariate predictions. The plot below compares the cumulative error between the two models (univariate and multivariate). It shows that the covariates begin to explain more of the variability in the time series model as time progresses — which is what we would expect as the model learns and adjusts the parameters based on previous values.Additionally we can examine the MAPE (mean absolute percentage error) for each model, and determine which model is more accurate in the one-step-ahead forecasts. In this case we examined the predictions for the previous 12 months and compared the MAPE. For the multivariate model we achieved a MAPE of ~20%, while the univariate model achieved a MAPE of 54%. 20% leaves a lot of room for improvement, but it’s certainly much better than 54! The MAD (mean absolute deviation) for the multivariate model is ~3,300, which means that our vehicle transaction estimates are estimated to be off by about 3,300 units each month.Let’s take a look at how ARIMA does with our data. First, we’ll need to create a train/test split in the data; here we’ll use the last 12 months of the series as a holdout sample.We’re using pmdarima, which is a convenient package that has built a wrapper around the Statsmodels implementation of SARIMAX.Our output from the SARIMX (because it’s seasonal, and there are two exogenous covariates) is shown below. There’s lots of complexity here. First, the model is differenced, we observe a moving average component in both the main series, and in the seasonal component, the seasonal effect is also differenced. Clearly the data is not stationary. Neither of our covariates were found to be statistically significant in the model; according to ARIMA they are having no effect. This is a different outcome than what the Bayesian model observed.How well did the ARIMA model perform in the hold-out sample of 12 months? Not too bad. we observed a MAPE of 22% The Bayesian model had a MAPE of 20% for the same 12-month time period.The next step is to predict the future using the model we have created. Because we have introduced additional covariates, we will also need to predict (or somehow know) the future values of those covariates as well. In reality the practitioner will need to use their domain expertise to generate reliable estimates of the predictors out into the future. In some cases, the best approach may be to just use another model to forecast future values for these variables. However, cautionary note: generating forecasts from forecasts is potentially dangerous territory! For the purposes of this example, we estimated the 12-month forecast for each covariate using a univariate BTS model (Pybats using just a single variable), and used those projections as inputs when predicting the target series — against our better judgement.The plot below shows how the model predictions the next 12 months. I’ve also provided the comparable ARIMA prediction using the same data. Note that ARIMA predictions are notably higher than those of the Bayesian model. We’ll need to track these as data comes in each month. I wouldn’t be surprised if the ARIMA model were constantly off, and the Bayesian model was more accurate, given the historical pattern of transactions.This article provided a brief introduction to using Pybats for multivariate Bayesian forecasting. There tool is quite powerful, and worth looking into for those needing to produce accurate forecasts leveraging the power of dynamic linear regression models. The Bayesian approach more than makes up for the short-comings of ARIMA, especially where there is insufficient data, multiple variables and needing to understand the relative importance of variables in the model — in a more transparent way than ARIMA can provide.",18/03/2021,5,6,0,"(684, 494)",10,0,0.0,3,en,regression,polynomial,descent,optimal,minimum,action,following,scikit,kernel,distribution,objective,neutral,surprise/amazement
238,Understanding few-shot learning in machine learning,Quick Code,Dr. Michael J. Garbade,2900.0,4.0,798,"Machine learning has experienced tremendous growth in recent years. Some of the factors fuelling this wonderful growth include increase in the sophistication of algorithms and learning models, the growing computing capability of machines, and availability of big data.AndreyBu, who has more than five years of machine learning experience and currently teaches people his skills, says that “data is the life-blood of training machine learning models that ensure their success.” “A learning model fed with sufficient, quality data is likely to yield results that are more accurate,” he adds.However, sometimes accruing enough data to increase the accuracy of the models is unrealistic and difficult to achieve. For example, in enormous business situations, labeling samples becomes costly and difficult to manage.In such limited-data and challenging scenarios, few-shot learning algorithms have been employed successfully to discover patterns in data and make beneficial predictions.What is few-shot learning?As the name implies, few-shot learning refers to the practice of feeding a learning model with a very small amount of training data, contrary to the normal practice of using a large amount of data.This technique is mostly utilized in the field of computer vision, where employing an object categorization model still gives appropriate results even without having several training samples.For example, if we have a problem of categorizing bird species from photos, some rare species of birds may lack enough pictures to be used in the training images.Consequently, if we have a classifier for bird images, with the insufficient amount of the dataset, we’ll treat it as a few-shot or low-shot machine learning problem.If we have only one image of a bird, this would be a one-shot machine learning problem. In extreme cases, where we do not have every class label in the training, and we end up with 0 training samples in some categories, it would be a zero-shot machine learning problem.You will benefit from learning Tensorflow since it is one of the machine learning libraries that demands more effort while learning the TensorFlow Python framework.Motivations for few-shot machine learningLow-shot learning deep learning is based on the concept that reliable algorithms can be created to make predictions from minimalist datasets.Here are some situations that are driving their increased adoption:Low-shot learning approachesGenerally, two main approaches are usually used to solve few-shot or one-shot machine learning problems.Here are the two main approaches:a) Data-level approachThis approach is based on the concept that whenever there is insufficient data to fit the parameters of the algorithm and avoid underfitting or overfitting the data, then more data should be added.A common technique used to realize this is to tap into an extensive collection of external data sources. For example, if the intention is to create a classifier for the species of birds without sufficient labeled elements for each category, it could be necessary to look into other external data sources that have images of birds. In this case, even unlabeled images can be useful, especially if included in a semi-supervised manner.In addition to utilizing external data sources, another technique for data-based low-shot learning is to produce new data. For example, data augmentation technique can be employed to add random noise to the images of birds.Alternatively, new image samples can be produced using the generative adversarial networks (GANs) technology. For example, with this technology, new images of birds can be produced from different perspectives if there are enough examples available in the training set.b) Parameter-level approachBecause of the inadequate availability of data, few-shot learning samples can have high-dimensional spaces that are too extensive. To overcome overfitting issues, the parameter space can be limited.To solve such machine learning problems, regularization techniques or loss functions are often employed — which can be applied to low-shot problems.In this case, the algorithm is compelled to generalize the limited number of training samples. Another technique is to enhance the accuracy of the algorithm through directing it to the extensive parameter space.If any standard optimization algorithm is used, such as the stochastic gradient descent (SDG) it may not give the desired results in a high dimensional space because of the insufficient number of training data.As such, the algorithm is taught to go for the best route in the parameter space to give optimal prediction results. This technique is typically referred to as meta-learning.For example, a teacher algorithm can be trained using a big quantity of data on how to encapsulate the parameter space. Thereafter, if the real classifier (student) is trained, the teacher algorithm directs the student on the extensive parameter to realize the best training results.Wrapping upFew-shot learning in machine learning is proving to be the go-to solution whenever a very small amount of training data is available. The technique is useful in overcoming data scarcity challenges and reducing costs.What’s your experience with low-shot learning in machine learning?Please let us know in the comment section below.",25/08/2018,0,7,0,"(543, 340)",1,1,0.0,4,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,joy/calmness
239,Appearance of The Principate [Pt. III],,Daniel Voshart,,8.0,525,"Using the neural-net tool Artbreeder, Photoshop and historical references, I have created photoreal depictions of Roman Emperors. Scroll down to see each emperor.ON CREATIVE COMMONS & COPYRIGHT: Faces can be shared non-watermarked at 200 pixels max height OR 512 pixels with the digital mosaic watermark with Attribution-NonCommercial-ShareAlike. Please link back to this page. Continuation of this project depends on prints, licensing and commissions.These are my collected notes and references. Many written sources are known to be unreliable but remain here for posterity and debate.**CONCISE UPDATE (July 31st) replacing a July 27th CLARIFICATION: ‘TheApricity’, a tertiary source, has been removed entirely. I knew it to be unreliable prior to starting this project but kept here for posterity and debate. It is now clear to me they have distorted primary and secondary sources to push a pernicious white supremacist agenda. I am instead quoting A primary Greek text by John Malalas has also been removed. Only three Emperors (in Part II) are impacted by these changes.96–98 (Died aged 67 — Natural causes)98–117 (Died aged 63 — Natural causes)Adopted son and heir of Nerva. His reign marked the geographical peak of the empire117–138 (Died aged 62 — Natural causes)Adopted son and heir of Trajan138–161 (Died aged 74 — Natural causes)Adopted son and heir of Hadrian161–169 (Died aged 39 — Natural causes)Adopted son and heir of Antoninus Pius and son-in-law of Marcus Aurelius; Co-emperor with Marcus Aurelius until his death161–190 (Died aged 58 — Natural causes)Adopted son, son-in-law and heir of Antoninus Pius; Co-emperor with Lucius Verus until 169. Last of the “Five Good Emperors”177–192 (Died aged 31 — Assassinated in palace, strangled in his bath)Father: Marcus Aurelius; joint emperor from 177193 (Died aged 66 — Murdered by Praetorian Guard [so that they could auction off the empire to the highest bidder])The son of freedman Helvius Successus.193 (Died aged 56 or 60 — Executed on orders of the Senate)After the murder of Pertinax, the Praetorian Guard auctioned the office of Emperor. Didius’ winning bid was 25,000 sestertii per man and he was duly declared Emperor. Marcus Didius Julianus (Julian I) was murdered after a reign of only 9 weeks.193–211 (Died aged 65 — Natural causes)198–217 (Died aged 29 — Murdered by a soldier as part of a conspiracy involving Macrinus)Son of Septimius Severus; co-emperor with Severus from 198; with Severus and Geta from 209 until February 211; co-emperor with Geta until December 211211 (Died aged 22 — Murdered on the orders of Caracalla)Father: Septimius Severus.Co-emperor with Severus and Caracalla from 209 until February 211; co-emperor with Caracalla until December 211217–218 (Died aged 53 — Both executed in favour of Elagabalus)218 (Died age 9 — Both executed in favour of Elagabalus)Father: Macrinus218–222 (Died aged 18 — Murdered by Praetorian Guard)Grandnephew of Septimius Severus, first cousin once removed and alleged illegitimate son of Caracalla; proclaimed emperor by Syrian legions.222–235 (Died aged 27 — Murdered by the army)Grandnephew of Septimius Severus, cousin and adoptive heir of Elagabalus←PREVIOUS [Pt II] 68–96: Year of the Four Emperors and Flavian dynastyNEXT [Pt IV] 235–285: Gordian dynasty and Crisis of the Third CenturyABOUT THE AUTHORThis is a quarantine project by Daniel Voshart. 🖼️ Prints available here.",24/07/2020,0,113,26,"(700, 354)",27,17,0.0,118,en,image,latent,space,distribution,gan,stylegan,generated,artificial,adversarial,attack,objective,neutral,expectation/interest
240,Zero and Few Shot Learning,Towards Data Science,Eram Munawwar,56.0,6.0,895,"The field of NLP is getting more and more exciting each day. Until a few years ago, we were not able to fully leverage the vast sources of data available online. With the amazing success of unsupervised learning methods and transfer learning, the NLP community has built models which serve as a knowledge base for multiple NLP tasks.However, we’re still dependent on annotated data for fine-tuning on a downstream task. More often than not, getting labeled data is not handy and is a relatively expensive and time taking exercise. What can we do if we don’t have any labeled data or have very less of it? The answer to this problem is zero-shot and few shot learning.There is no single definition of zero and few shot methods. Rather, one can say that its definition is task dependent.Zero shot classification means that we train a model on some classes and predict for a new class, which the model has never seen before. Obviously, the class name needs to exist in the list of classes, but there are no training samples for this class.In the more broader sense, zero shot setting refers to using a model to do something it wasn’t trained to do.Let’s assume we train a language model on a large text corpus (or use a pre-trained one like GPT-2). Our task is to predict whether a given article is about sports, entertainment or technology. Normally, we would formulate this as a fine tuning task with many labeled examples, and add a linear layer for classification on top of the language model. But with zero shot, we use the language model directly (without any explicit fine-tuning). We can give the article along with explanatory label names and get a prediction.Does it still sound confusing? Fret not! and dive into the intuition behind these concepts to make things clearer. Also, the example in the end will ensure a deeper understanding of its usability.We as humans store a huge amount of information that we learn from every resource, be it books, news, courses, or just experience.If we are asked to do the following task:“Translate from english to french”: How are you? -> ?From the task’s description, it is quite clear to us what is to be done here. We have used our knowledge base to infer what translation means.Another task can be as follows:“I loved the movie!” -> happy-or-sadReading the self explanatory task explanation (happy-or-sad), we understand that it is a classification task. Our knowledge base also helps us understand the sentence and infer that it is happy!This is exactly how zero shot classification works. We have a pre trained model (eg. a language model) which serves as the knowledge base since it has been trained on a huge amount of text from many websites. For any type of task, we give relevant class descriptors and let the model infer what the task actually is.Needless to say, the more labeled data we provide, the better the results would be. And many times, zero shot doesn’t work very well. If we have a few samples of labeled data but not enough for fine tuning, few shot is the way to go. As used in GPT-3, “Language Models are Few Shot Learners”, the authors prove that very large language models can perform competitively on downstream tasks with much lesser labeled data as compared to what smaller models would require.Few Shot is simply an extension of zero shot, but with a few examples to further train the model.Both FlairNLP and Huggingface have zero shot classification pipelines for english (since they use bert as the model). Even though flairNLP uses bert-base-uncased for english as its base model, it works surprisingly well with simple indonesian text. In the below example, I’ll walk you through the steps of zero and few shot learning using the TARS model in flairNLP on indonesian text.The zero-shot classification pipeline implemented by huggingface has some excellent articles and demos. Check out this excellent blog and this live demo on zero shot classification by HuggingFace.Since I am using Bahasa Indonesia, I will not use the huggingface zero-shot pipeline, but use the GPT-2 model (trained on Bahasa Indonesia by cahya and available on huggingface) and implement zero-shot classification.Use the zero shot classifier (TARS) model to classify whether the sentence is about sports (olahraga) or politics (politik)OutputThe english model works surprisingly well on Bahasa Indonesia! Although it predicts the second example incorrectly, the sentence “The cabinet ministers are playing a nasty game” has been categorized as sports and not politics. Probably because the word playing confusing the model! Let us make it learn with 4 examples (2 from each class) and re-evaluate on the same example using few shot classification.Train the model on the new dataOutputOutputZero shot and few shot learning methods are reducing the reliance on annotated data. The GPT-2 and GPT-3 models have shown remarkable results to prove this. However, for low resource languages like Bahasa Indonesia, it is still an area of active research. However, thanks to valuable resources like cahya, we have a GPT-2 model trained specifically for Indonesian. However, since the data for indonesian is not humongous, few shot is preferred over zero-shot learning. Nonetheless, it is absolutely remarkable what we can achieve with only a few examples, and how libraries like flairNLP and huggingface have made life so much easier to implement cutting edge models!Cheers!Eram",05/01/2021,7,0,3,"(700, 394)",1,0,0.0,9,en,image,regression,các,post,gpu,sample,level,trained,convolution,python,objective,neutral,joy/calmness
241,Classification on a large and noisy dataset with R,Towards Data Science,Gabriel Pierobon,324.0,12.0,1903,"Some days ago I wrote an article describing a comprehensive supervised learning workflow in R with multiple modelling using packages caret and caretEnsemble. Back then I mentioned that the I was using was kind of an easy one, in the sense that it was fully numeric, perfectly filled (not a single missing value), no categorical features, no class imbalance (of course since it was a regression problem), and it was a fairly small dataset with just 8 predictors.It felt almost like cheating but it really helped both for my first article written but also to go through the full workflow without much trouble and finally to do some multiple modelling using caretEnsemble.So now I’ve decided to take this from easy difficulty to normal difficulty.In that sense, I came across this dataset in the UCI Machine Learning Repository which I intend to use.What is the new difficulty we come across with this dataset? Let me summarize it:This is the description of the dataset and task included by the owner of the repository:“The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that are utilized in various functions in a truck, such as braking and gear changes. The datasets’ positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.The attribute names of the data have been anonymized for proprietary reasons. It consists of both single numerical counters and histograms consisting of bins with different conditions. (…) In total there are 171 attributes, of which 7 are histogram variables. Missing values are denoted by “na”.— Challenge metricCost-metric of miss-classification:In this case Cost_1 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop, while Cost_2 refer to the cost of missing a faulty truck, which may cause a breakdown.Total_cost = Cost_1 * No_Instances + Cost_2 * No_InstancesWe’ll give it a try!1.1) What are we trying to predict?We need to predict the type of system failure. It can be either a failing component of the APS or a failing component not related to the APS. This is very important since our prediction errors can result in unnecessary spending by the company. Specifically we want to avoid type 2 errors (cost of missing a faulty truck, which may cause a breakdown).2.2) What type of problem is it? Supervised or Unsupervised Learning? Classification or Regression? Binary or Multi-class? Uni-variate or Multi-variate?It is a binary classification problem with multiple features.2.3) What type of data do we have?We have two csv files, one for training and one for testing. Both present 20 unimportant text lines before the actual data begins and we we will have to skip those lines when importing the data frame. There is one row for column names and missing values are denoted by “na”, so we will make sure we include that when we read the csv.2.4) Import the datasetWe check the dimensions of both data sets:2.5) Activate packages to be used during the project2.1) View Data (str or dplyr’s glimpse). First look. Anything strange?Specifying na.strings=”na"" when we imported the sets allows R to recognize each feature as numeric. If we didn’t do that, the presence of na in each column would automatically result in them being categorized as character type.Using the code below, we can see that other than our response variable class, all the other features are numeric.2.2) Is it a “tidy” dataset? Need to “gather” or “spread” it? Is it presented in a way we can work with?It’s tidy. Each row is an observation, each column is a feature.2.3) rownames and colnames ok? Should we change them?I don’t see a reason to change them.2.4) Check data types. Are they OK? If not, convertWe already assessed that data types are ok. Here you want to also check that the response variable is of the factor type, with the expected two levels.2.5) What is our response/target variable? Class imbalance? Study itOur target variable is class, with two levels: neg and pos. Positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS.How many neg and pos do we have in each set?Let’s check proportions:Ok, looks like have a problem here. Class is totally imbalanced.We could come up with a useless model that classified every observation as neg and get 97.7% accuracy, so let’s be careful with our accuracy score interpretation.Later we’ll see how we can deal with class imbalance.2.6) Rest of the features. Summary statistics. Understand your dataWe are working with 171 features, so calling summary() on the entire dataset is not going to help a lot in terms of visual interpretation. Instead, we are going to create a data frame out of the summary() function. This will enable us to calculate some new statistics, specifically related to missing values, which as you will see, is another big issue of this data.With this summary data frame we will also calculate the mean quartiles for all the data. This will also allow us to understand more about the distribution of the features and the average number of missing values.We can see above that the average number of missing values per feature is 5,000 out of 60,000 samples. This means we have 8.3% o missing values in average in each column. That is a lot!Let’s check the same for our test set:Again, we have 8.4% missing values. We’ll have to deal with them and there’s a specific section for that afterwards.2.7) Categorical data/Factors: create count tables to understand different categories. Check all of them.We are not working with any other categorical feature other than our response variable.2.8) Unnecessary columns? Columns we can quickly understand we don’t need. Drop themSo far, we haven’t detected any column that we want to remove at this time. We will leave this for later.2.9) Check for missing values. How many? Where? Delete them? Impute them?Our features present more than 8% missing values in average.Because it’s a lot of information we don’t want to lose, our approach is going to be to impute them. But first, we don’t want to use a technique to impute each of our two sets separately (train and test). It has to be a one time imputation using full information. In order to do this, we are going to combine both sets, work on them and then separate again.We end up with a single set containing 76,000 samples (16,000 from test set and 60,000 from train set). The number of columns is 172 (171 features + 1 “set” column)Missing values imputationWe are going to impute missing values using package mice. Here’s a nice explanation of how mice worksThe following formula allows us to impute the full dataset using mean imputation:Now we store the imputed values:We then check that we still maintain the same dimensions:And now let’s check that we don’t have missing values:Wait. If you run that code and look at each row, you will notice there are some features that still have missing values. Specifically there are 9 rows.Let’s take a look at what is causing them:I filtered the $loggedEvents attribute of the imputed data frame. Here we see that some features were tagged as constant or collinear. One of them is our set column (the one we used to combine the two sets into one), so we don’t worry about that one. The rest of them are mainly collinear variables and one constant variable. mice automatically skips those columns and lets us know of the issue. We will want to drop those features so we have a full dataset without missing values.To do that, we first store the column names being careful not to store the set column name (we need it)We then use the stored vector to remove those columns from the data frame and store it as our final imputed data frame:Notice the number of columns reduced from 172 to 164.Now we don’t have missing values any more! (check it!)Finally, it’s time to separate our full imputed dataset into train and test sets again, and we need to separate it into the exact same samples that it was splitted before. In order to do that we just filter the data frame using our set column.Great!, we have our training and test set splitted and with no missing values!2.10) Check for outliers and other inconsistent data points. Box-plots. Cooks’ distance. DBSCAN?For this dataset we are going to use cook’s distance:Let’s plot the results:In this plot, what seems to be a dark thick black line is actually all our data points. In the right-top corner we see also what seems to be 1 outlier, or a bunch of them grouped.Let’s check how many are there:This is a total of 22 points that are considered outliers according to cook’s distance test.Since this is a very low number of observations compared to our total 60,000 I decided not to remove them. This just seems to be variability in the measurement rather than experimental error.2.11) Check for multicollinearity in numeric dataLast step ouf our EDA phase is to check for multicollinearity. We have already removed some collumns which mice detected as collinear. Here we will just do some analysis of how many bivariate relations have high correlation and if it’s a significant high proportion:This means that out of 26,244 possible variable correlations:I won’t go any further than this and consider multicollinearity not to be a big issue at this point.We won’t also spend time engineering features in this already heavy feature loaded dataset. There is no additional information we could use.We have now both our training and testing data sets ready for modelling. We have imputed missing values, removed collinear features and verified that outliers and multicollinearity is not a big deal that we should be concerned about.Due to the size of the data, we will train a Logistic Regression model and a Naive Bayes model. Both are very fast compared to more advanced algorithms such as random forests, SVMs or gradient boosting models.I’m not looking here to win the contest but to have an acceptable scoring just to demonstrate.I’ll use caretEnsemble’s caretList()to train both at the same time and with the same resampling .Notice I’m specifying “up”-sampling within trainControl(). This will take care of class imbalance. More about upsampling.Our results:Notice that our accuracy scores are lower than our usless predict-all-neg model that had 97,7%. The reason we don’t score higher is because we upsampled the data, thus generated new data points to fix the class imbalance. We did it as a parameter set within caret’s trainControl so I’m not showing any details of that, but by doing it, we have improved our ability to predict the positive class (in this case “neg”) in detriment of just looking to maximize accuracy.Let’s quickly check the confusion matrices:According to the cost matrix for each type of error, our scoring is:Logistic Regression model: 52 x 500 + 834 x 10 = 34,340Naive Bayes Model: 42 x 500 + 498 x 10 = 25,980This is fine-ish performance for this quick of a modelling. I bet that with some more work we can get very close to the best 3 contestants:",01/10/2018,23,7,41,"(541, 196)",13,2,0.0,7,en,dropout,graph,event,could,interview,text,batch,help,meteor,thing,objective,positive,joy/calmness
