{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling + Sentiment (tone + emotion) Analysis + Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import transformers\n",
    "import pandas as pd\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './scrapped_data'\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'scrapped_train.csv')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'scrapped_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(TRAIN_PATH, index_col='idx')\n",
    "data = pd.read_csv(TEST_PATH, index_col='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>italic_text_count</th>\n",
       "      <th>mean_image_width</th>\n",
       "      <th>mean_image_height</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_lists</th>\n",
       "      <th>n_vids</th>\n",
       "      <th>n_links</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>SGD: MNIST — Putting it all together</td>\n",
       "      <td>unpackAI</td>\n",
       "      <td>Becky Zhu</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>There are 7 steps to train/get a model in deep...</td>\n",
       "      <td>06/06/2021</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4212</th>\n",
       "      <td>Silhouette Coefficient</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Ashutosh Bhardwaj</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>After learning and applying several supervised...</td>\n",
       "      <td>26/05/2020</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title           publication  \\\n",
       "idx                                                                \n",
       "3884  SGD: MNIST — Putting it all together              unpackAI   \n",
       "4212                Silhouette Coefficient  Towards Data Science   \n",
       "\n",
       "                 author  followers  reading_time  n_words  \\\n",
       "idx                                                         \n",
       "3884          Becky Zhu        6.0           3.0    432.0   \n",
       "4212  Ashutosh Bhardwaj       44.0           3.0    411.0   \n",
       "\n",
       "                                              pure_text        date  \\\n",
       "idx                                                                   \n",
       "3884  There are 7 steps to train/get a model in deep...  06/06/2021   \n",
       "4212  After learning and applying several supervised...  26/05/2020   \n",
       "\n",
       "      n_code_chunks  bold_text_count  italic_text_count  mean_image_width  \\\n",
       "idx                                                                         \n",
       "3884              0              9.0                3.0             722.0   \n",
       "4212              7              5.0                0.0             467.0   \n",
       "\n",
       "      mean_image_height  n_images  n_lists  n_vids  n_links language  \n",
       "idx                                                                   \n",
       "3884              118.0       1.0      1.0     0.0      0.0       en  \n",
       "4212              316.0       4.0      0.0     0.0      0.0       en  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>italic_text_count</th>\n",
       "      <th>mean_image_width</th>\n",
       "      <th>mean_image_height</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_lists</th>\n",
       "      <th>n_vids</th>\n",
       "      <th>n_links</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, publication, author, followers, reading_time, n_words, pure_text, date, n_code_chunks, bold_text_count, italic_text_count, mean_image_width, mean_image_height, n_images, n_lists, n_vids, n_links, language]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.title.isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data.title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aleksey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aleksey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aleksey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aleksey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aleksey\\Documents\\[important]hackatons\\[a]ru_code\\championship\\claps_predictor\\lda.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m docs:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=46'>47</a>\u001b[0m     word_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=48'>49</a>\u001b[0m     filtered_sentence \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word_tokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (lemmatizer\u001b[39m.\u001b[39mlemmatize(w\u001b[39m.\u001b[39mlower()) \u001b[39min\u001b[39;00m stop_words) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (w\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m stop_words)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=49'>50</a>\u001b[0m     filtered_docs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(filtered_sentence))\n",
      "\u001b[1;32mc:\\Users\\Aleksey\\Documents\\[important]hackatons\\[a]ru_code\\championship\\claps_predictor\\lda.ipynb Cell 9'\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m docs:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=46'>47</a>\u001b[0m     word_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=48'>49</a>\u001b[0m     filtered_sentence \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word_tokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (lemmatizer\u001b[39m.\u001b[39;49mlemmatize(w\u001b[39m.\u001b[39;49mlower()) \u001b[39min\u001b[39;00m stop_words) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (w\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m stop_words)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000008?line=49'>50</a>\u001b[0m     filtered_docs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(filtered_sentence))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=33'>34</a>\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=34'>35</a>\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=35'>36</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=42'>43</a>\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=43'>44</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=44'>45</a>\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/stem/wordnet.py?line=45'>46</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2039\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2035'>2036</a>\u001b[0m forms \u001b[39m=\u001b[39m apply_rules([form])\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2037'>2038</a>\u001b[0m \u001b[39m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2038'>2039</a>\u001b[0m results \u001b[39m=\u001b[39m filter_forms([form] \u001b[39m+\u001b[39;49m forms)\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2039'>2040</a>\u001b[0m \u001b[39mif\u001b[39;00m results:\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2040'>2041</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2024\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy.<locals>.filter_forms\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2021'>2022</a>\u001b[0m \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m forms:\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2022'>2023</a>\u001b[0m     \u001b[39mif\u001b[39;00m form \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lemma_pos_offset_map:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2023'>2024</a>\u001b[0m         \u001b[39mif\u001b[39;00m pos \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39m_lemma_pos_offset_map[form]:\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2024'>2025</a>\u001b[0m             \u001b[39mif\u001b[39;00m form \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m seen:\n\u001b[0;32m   <a href='file:///c%3A/Users/Aleksey/AppData/Local/Programs/Python/Python39/lib/site-packages/nltk/corpus/reader/wordnet.py?line=2025'>2026</a>\u001b[0m                 result\u001b[39m.\u001b[39mappend(form)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import wordnet \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = ['one', 'two', 'three',\n",
    "                      'like', 'also', 'use',\n",
    "                      'value', 'dataset', 'target',\n",
    "                      'data', 'learning', 'model',\n",
    "                      'variable', 'using', 'training',\n",
    "                      'time', 'feature', 'linear',\n",
    "                      'code', 'function', 'object',\n",
    "                      'variables', 'bounding', 'used',\n",
    "                      'assistants', 'layer', 'input',\n",
    "                      'ha', 'make', 'get', 'box', \n",
    "                      'class', 'system', 'ga', 'di',\n",
    "                      'people', 'need', 'see', 'wa',\n",
    "                      'state', 'weight', 'scale',\n",
    "                      'company', 'network', 'gradient',\n",
    "                      'vector', 'file', 'size', 'train',\n",
    "                      'map', 'output', 'let', 'example',\n",
    "                      'first', 'number', 'different',\n",
    "                      'article', 'algorithm', 'de',\n",
    "                      'error', 'generator', 'loss',\n",
    "                      'building', 'build', 'problem',\n",
    "                      'want', 'take', 'let', 'step', \n",
    "                      'che', 'un', 'parameter', 'process',\n",
    "                      'mean', 'ai', 'really', 'point',\n",
    "                      'would', 'method', 'new',\n",
    "                      'update', 'result', 'prediction',\n",
    "                      'series', 'way', 'many', 'learn', \n",
    "                      'set', 'find', 'work', 'approach',\n",
    "                       'pdf', 'shot' 'cluster', 'result',\n",
    "                      'neural', 'based', 'good', 'much', \n",
    "                      'part', 'case', 'based', 'iteration',\n",
    "                       'machine', 'change', 'deep', 'question',\n",
    "                       'day', 'know', 'document', 'thought',\n",
    "                       'think', 'rate', 'shot', 'agent',\n",
    "                        'la', 'well', 'next', 'yang', 'even',\n",
    "                        'style', 'task', 'entry', 'project',]\n",
    "stop_words.extend(new_stopwords)\n",
    "filtered_docs = []\n",
    "for sent in docs:\n",
    "    word_tokens = tokenizer.tokenize(sent)\n",
    "    \n",
    "    filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not (lemmatizer.lemmatize(w.lower()) in stop_words) and not (w.lower() in stop_words)]\n",
    "    filtered_docs.append(' '.join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "topic_model = BERTopic(language=\"multilingual\")\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "TEST\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics, probs = topic_model.fit_transform(filtered_docs) #train\n",
    "topics, probs = topic_model.transform(filtered_docs) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = {\n",
    "    'key_word_1' : [topic_model.get_topic(topic)[0][0] for topic in topics],\n",
    "    'key_word_2' : [topic_model.get_topic(topic)[1][0] for topic in topics],\n",
    "    'key_word_3' : [topic_model.get_topic(topic)[2][0] for topic in topics],\n",
    "    'key_word_4' : [topic_model.get_topic(topic)[3][0] for topic in topics],\n",
    "    'key_word_5' : [topic_model.get_topic(topic)[4][0] for topic in topics],\n",
    "    'key_word_6' : [topic_model.get_topic(topic)[5][0] for topic in topics],\n",
    "    'key_word_7' : [topic_model.get_topic(topic)[6][0] for topic in topics],\n",
    "    'key_word_8' : [topic_model.get_topic(topic)[7][0] for topic in topics],\n",
    "    'key_word_9' : [topic_model.get_topic(topic)[8][0] for topic in topics],\n",
    "    'key_word_10' : [topic_model.get_topic(topic)[9][0] for topic in topics],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object chunker at 0x0000019DF34CB680>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunker(links_2022, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['key_word_1'] = key_words['key_word_1']\n",
    "data['key_word_2'] = key_words['key_word_2']\n",
    "data['key_word_3'] = key_words['key_word_3']\n",
    "data['key_word_4'] = key_words['key_word_4']\n",
    "data['key_word_5'] = key_words['key_word_5']\n",
    "data['key_word_6'] = key_words['key_word_6']\n",
    "data['key_word_7'] = key_words['key_word_7']\n",
    "data['key_word_8'] = key_words['key_word_8']\n",
    "data['key_word_9'] = key_words['key_word_9']\n",
    "data['key_word_10'] = key_words['key_word_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>...</th>\n",
       "      <th>key_word_1</th>\n",
       "      <th>key_word_2</th>\n",
       "      <th>key_word_3</th>\n",
       "      <th>key_word_4</th>\n",
       "      <th>key_word_5</th>\n",
       "      <th>key_word_6</th>\n",
       "      <th>key_word_7</th>\n",
       "      <th>key_word_8</th>\n",
       "      <th>key_word_9</th>\n",
       "      <th>key_word_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>Step-by-Step R-CNN Implementation From Scratch...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Rohit Thakur</td>\n",
       "      <td>206.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>Classification and object detection are the ma...</td>\n",
       "      <td>18/10/2019</td>\n",
       "      <td>11</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>scratch</td>\n",
       "      <td>detail</td>\n",
       "      <td>anaconda</td>\n",
       "      <td>python</td>\n",
       "      <td>digit</td>\n",
       "      <td>install</td>\n",
       "      <td>idf</td>\n",
       "      <td>windows</td>\n",
       "      <td>implement</td>\n",
       "      <td>tf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>How Transformers Work</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Giuliano Giacaglia</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>If you liked this post and want to learn how m...</td>\n",
       "      <td>11/03/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>transformers</td>\n",
       "      <td>transformer</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>bert</td>\n",
       "      <td>hugging</td>\n",
       "      <td>label</td>\n",
       "      <td>visually</td>\n",
       "      <td>roberta</td>\n",
       "      <td>xlm</td>\n",
       "      <td>extinction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>Neural Style Transfer using VGG model</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Darshan Adakane</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>Introduction:Before we begin, let’s go to this...</td>\n",
       "      <td>16/01/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>transfer</td>\n",
       "      <td>vgg16</td>\n",
       "      <td>vgg</td>\n",
       "      <td>mobilenet</td>\n",
       "      <td>cifar</td>\n",
       "      <td>keras</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>applications</td>\n",
       "      <td>hands</td>\n",
       "      <td>moving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>Amazon’s Artificial Artificial Intelligence</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Sachin Palewar</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>Today, we build complex software applications ...</td>\n",
       "      <td>21/11/2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>artificial</td>\n",
       "      <td>advantages</td>\n",
       "      <td>disadvantages</td>\n",
       "      <td>news</td>\n",
       "      <td>cyborg</td>\n",
       "      <td>specification</td>\n",
       "      <td>artificielle</td>\n",
       "      <td>assurance</td>\n",
       "      <td>revolutionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>This week in the #SDGs- February 17, 2017</td>\n",
       "      <td>SDG Counting</td>\n",
       "      <td>SDGCounting</td>\n",
       "      <td>679.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>1 . IISD provided context to news that the rep...</td>\n",
       "      <td>17/02/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>forecasting</td>\n",
       "      <td>forecast</td>\n",
       "      <td>rnns</td>\n",
       "      <td>weatherman</td>\n",
       "      <td>weatherbot</td>\n",
       "      <td>sales</td>\n",
       "      <td>judging</td>\n",
       "      <td>poncho</td>\n",
       "      <td>announcing</td>\n",
       "      <td>attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>Redes Neurais, Perceptron Multicamadas e o Alg...</td>\n",
       "      <td>Ensina.AI</td>\n",
       "      <td>Tiago M. Leite</td>\n",
       "      <td>113.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>Você já se perguntou como funcionam os sistema...</td>\n",
       "      <td>10/05/2018</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>backpropagation</td>\n",
       "      <td>propagation</td>\n",
       "      <td>back</td>\n",
       "      <td>complicated</td>\n",
       "      <td>backprop</td>\n",
       "      <td>backpropagations</td>\n",
       "      <td>deriving</td>\n",
       "      <td>beginning</td>\n",
       "      <td>gate</td>\n",
       "      <td>behaviors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>Spam Classifier in Python from scratch</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Tejan Karmali</td>\n",
       "      <td>105.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>We all face the problem of spams in our inboxe...</td>\n",
       "      <td>02/08/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>scratch</td>\n",
       "      <td>detail</td>\n",
       "      <td>anaconda</td>\n",
       "      <td>python</td>\n",
       "      <td>digit</td>\n",
       "      <td>install</td>\n",
       "      <td>idf</td>\n",
       "      <td>windows</td>\n",
       "      <td>implement</td>\n",
       "      <td>tf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>Preparing for Insight</td>\n",
       "      <td>Insight</td>\n",
       "      <td>Insight</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>John Joo is an Insight alumnus from the August...</td>\n",
       "      <td>16/04/2014</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>vae</td>\n",
       "      <td>pca</td>\n",
       "      <td>nlp</td>\n",
       "      <td>bert</td>\n",
       "      <td>pt</td>\n",
       "      <td>sequence</td>\n",
       "      <td>python</td>\n",
       "      <td>netflix</td>\n",
       "      <td>keras</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>GOOGLE AI CONTEST</td>\n",
       "      <td>Altsoph’s blog</td>\n",
       "      <td>Aleksey Tikhonov</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Вчера узнал, что сейчас идет Google AI Contest...</td>\n",
       "      <td>16/09/2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>vae</td>\n",
       "      <td>pca</td>\n",
       "      <td>nlp</td>\n",
       "      <td>bert</td>\n",
       "      <td>pt</td>\n",
       "      <td>sequence</td>\n",
       "      <td>python</td>\n",
       "      <td>netflix</td>\n",
       "      <td>keras</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>🏎 Smaller, faster, cheaper, lighter: Introduci...</td>\n",
       "      <td>HuggingFace</td>\n",
       "      <td>Victor Sanh</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2113.0</td>\n",
       "      <td>2019, October 3rd — Update: We are releasing o...</td>\n",
       "      <td>28/08/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>vae</td>\n",
       "      <td>pca</td>\n",
       "      <td>nlp</td>\n",
       "      <td>bert</td>\n",
       "      <td>pt</td>\n",
       "      <td>sequence</td>\n",
       "      <td>python</td>\n",
       "      <td>netflix</td>\n",
       "      <td>keras</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title           publication  \\\n",
       "idx                                                                             \n",
       "3756  Step-by-Step R-CNN Implementation From Scratch...  Towards Data Science   \n",
       "3757                              How Transformers Work  Towards Data Science   \n",
       "3758              Neural Style Transfer using VGG model  Towards Data Science   \n",
       "3759        Amazon’s Artificial Artificial Intelligence                Medium   \n",
       "3760          This week in the #SDGs- February 17, 2017          SDG Counting   \n",
       "...                                                 ...                   ...   \n",
       "4251  Redes Neurais, Perceptron Multicamadas e o Alg...             Ensina.AI   \n",
       "4252             Spam Classifier in Python from scratch  Towards Data Science   \n",
       "4253                              Preparing for Insight               Insight   \n",
       "4254                                  GOOGLE AI CONTEST        Altsoph’s blog   \n",
       "4255  🏎 Smaller, faster, cheaper, lighter: Introduci...           HuggingFace   \n",
       "\n",
       "                  author  followers  reading_time  n_words  \\\n",
       "idx                                                          \n",
       "3756        Rohit Thakur      206.0           8.0    800.0   \n",
       "3757  Giuliano Giacaglia     1100.0          14.0   2730.0   \n",
       "3758     Darshan Adakane       27.0           7.0   1013.0   \n",
       "3759      Sachin Palewar      130.0           1.0    127.0   \n",
       "3760         SDGCounting      679.0           2.0    280.0   \n",
       "...                  ...        ...           ...      ...   \n",
       "4251      Tiago M. Leite      113.0           9.0   1421.0   \n",
       "4252       Tejan Karmali      105.0           6.0    901.0   \n",
       "4253             Insight     2200.0           5.0   1108.0   \n",
       "4254    Aleksey Tikhonov      275.0           1.0     88.0   \n",
       "4255         Victor Sanh     1000.0          10.0   2113.0   \n",
       "\n",
       "                                              pure_text        date  \\\n",
       "idx                                                                   \n",
       "3756  Classification and object detection are the ma...  18/10/2019   \n",
       "3757  If you liked this post and want to learn how m...  11/03/2019   \n",
       "3758  Introduction:Before we begin, let’s go to this...  16/01/2020   \n",
       "3759  Today, we build complex software applications ...  21/11/2005   \n",
       "3760  1 . IISD provided context to news that the rep...  17/02/2017   \n",
       "...                                                 ...         ...   \n",
       "4251  Você já se perguntou como funcionam os sistema...  10/05/2018   \n",
       "4252  We all face the problem of spams in our inboxe...  02/08/2017   \n",
       "4253  John Joo is an Insight alumnus from the August...  16/04/2014   \n",
       "4254  Вчера узнал, что сейчас идет Google AI Contest...  16/09/2010   \n",
       "4255  2019, October 3rd — Update: We are releasing o...  28/08/2019   \n",
       "\n",
       "      n_code_chunks  bold_text_count  ...       key_word_1   key_word_2  \\\n",
       "idx                                   ...                                 \n",
       "3756             11             17.0  ...          scratch       detail   \n",
       "3757              0             48.0  ...     transformers  transformer   \n",
       "3758              0             17.0  ...         transfer        vgg16   \n",
       "3759              0              0.0  ...     intelligence   artificial   \n",
       "3760              0             17.0  ...      forecasting     forecast   \n",
       "...             ...              ...  ...              ...          ...   \n",
       "4251              0             39.0  ...  backpropagation  propagation   \n",
       "4252              0              5.0  ...          scratch       detail   \n",
       "4253              0             11.0  ...              vae          pca   \n",
       "4254              0              0.0  ...              vae          pca   \n",
       "4255              0             64.0  ...              vae          pca   \n",
       "\n",
       "      key_word_3     key_word_4  key_word_5        key_word_6     key_word_7  \\\n",
       "idx                                                                            \n",
       "3756    anaconda         python       digit           install            idf   \n",
       "3757       xlnet           bert     hugging             label       visually   \n",
       "3758         vgg      mobilenet       cifar             keras      beautiful   \n",
       "3759  advantages  disadvantages        news            cyborg  specification   \n",
       "3760        rnns     weatherman  weatherbot             sales        judging   \n",
       "...          ...            ...         ...               ...            ...   \n",
       "4251        back    complicated    backprop  backpropagations       deriving   \n",
       "4252    anaconda         python       digit           install            idf   \n",
       "4253         nlp           bert          pt          sequence         python   \n",
       "4254         nlp           bert          pt          sequence         python   \n",
       "4255         nlp           bert          pt          sequence         python   \n",
       "\n",
       "        key_word_8  key_word_9    key_word_10  \n",
       "idx                                            \n",
       "3756       windows   implement             tf  \n",
       "3757       roberta         xlm     extinction  \n",
       "3758  applications       hands         moving  \n",
       "3759  artificielle   assurance  revolutionary  \n",
       "3760        poncho  announcing    attribution  \n",
       "...            ...         ...            ...  \n",
       "4251     beginning        gate      behaviors  \n",
       "4252       windows   implement             tf  \n",
       "4253       netflix       keras           word  \n",
       "4254       netflix       keras           word  \n",
       "4255       netflix       keras           word  \n",
       "\n",
       "[500 rows x 28 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from danlp.models import load_bert_tone_model\n",
    "classifier = load_bert_tone_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [13:02<00:00,  1.56s/text]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tones = []\n",
    "with tqdm(data.pure_text.values, unit=\"text\") as tepoch:\n",
    "    for text in tepoch:\n",
    "        if text == '':\n",
    "            tones.append({'analytic': 'objective', 'polarity': 'neutral'})\n",
    "            continue\n",
    "        tone = classifier.predict(text[:min(len(text)-1, 500)])\n",
    "        tones.append(tone)\n",
    "print(len(tones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['analytic'] = [tone['analytic'] for tone in tones]\n",
    "data['polarity'] = [tone['polarity'] for tone in tones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_bert_emotion_model\n",
    "classifier_emotion = load_bert_emotion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_to_eng = {'Glæde/Sindsro':\"joy/calmness\",\n",
    "                \"Forventning/Interrese\": \"expectation/interest\",\n",
    "                \"Tillid/Accept\": 'trust/acceptance', \n",
    "                \"Overasket/Målløs\": 'surprise/amazement',\n",
    "                \"Vrede/Irritation\": 'anger/irritation',\n",
    "                \"Foragt/Modvilje\": 'contempt/reluctance',\n",
    "                \"Sorg/Skuffelse\": 'sadness/disappointment',\n",
    "                \"Frygt/Bekymring\": 'fear/anxiety',\n",
    "                \"no emotion\": 'no emotion',\n",
    "                \"No emotion\": 'no emotion' \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [12:29<00:00,  1.50s/text]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emotions = []\n",
    "with tqdm(data.pure_text.values, unit=\"text\") as tepoch:\n",
    "    for text in tepoch:\n",
    "        if text == '':\n",
    "            emotions.append('no emotion')\n",
    "            continue\n",
    "        emotion = classifier_emotion.predict(text[:min(len(text)-1, 500)])\n",
    "        emotions.append(emotion)\n",
    "print(len(emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [de_to_eng[emotion] for emotion in emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emotion'] = emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>...</th>\n",
       "      <th>key_word_4</th>\n",
       "      <th>key_word_5</th>\n",
       "      <th>key_word_6</th>\n",
       "      <th>key_word_7</th>\n",
       "      <th>key_word_8</th>\n",
       "      <th>key_word_9</th>\n",
       "      <th>key_word_10</th>\n",
       "      <th>analytic</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>Step-by-Step R-CNN Implementation From Scratch...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Rohit Thakur</td>\n",
       "      <td>206.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>Classification and object detection are the ma...</td>\n",
       "      <td>18/10/2019</td>\n",
       "      <td>11</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>python</td>\n",
       "      <td>digit</td>\n",
       "      <td>install</td>\n",
       "      <td>idf</td>\n",
       "      <td>windows</td>\n",
       "      <td>implement</td>\n",
       "      <td>tf</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>joy/calmness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>How Transformers Work</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Giuliano Giacaglia</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>If you liked this post and want to learn how m...</td>\n",
       "      <td>11/03/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>bert</td>\n",
       "      <td>hugging</td>\n",
       "      <td>label</td>\n",
       "      <td>visually</td>\n",
       "      <td>roberta</td>\n",
       "      <td>xlm</td>\n",
       "      <td>extinction</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>expectation/interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>Neural Style Transfer using VGG model</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Darshan Adakane</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>Introduction:Before we begin, let’s go to this...</td>\n",
       "      <td>16/01/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mobilenet</td>\n",
       "      <td>cifar</td>\n",
       "      <td>keras</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>applications</td>\n",
       "      <td>hands</td>\n",
       "      <td>moving</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>trust/acceptance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>Amazon’s Artificial Artificial Intelligence</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Sachin Palewar</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>Today, we build complex software applications ...</td>\n",
       "      <td>21/11/2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>disadvantages</td>\n",
       "      <td>news</td>\n",
       "      <td>cyborg</td>\n",
       "      <td>specification</td>\n",
       "      <td>artificielle</td>\n",
       "      <td>assurance</td>\n",
       "      <td>revolutionary</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>joy/calmness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>This week in the #SDGs- February 17, 2017</td>\n",
       "      <td>SDG Counting</td>\n",
       "      <td>SDGCounting</td>\n",
       "      <td>679.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>1 . IISD provided context to news that the rep...</td>\n",
       "      <td>17/02/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>weatherman</td>\n",
       "      <td>weatherbot</td>\n",
       "      <td>sales</td>\n",
       "      <td>judging</td>\n",
       "      <td>poncho</td>\n",
       "      <td>announcing</td>\n",
       "      <td>attribution</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>surprise/amazement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>Redes Neurais, Perceptron Multicamadas e o Alg...</td>\n",
       "      <td>Ensina.AI</td>\n",
       "      <td>Tiago M. Leite</td>\n",
       "      <td>113.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>Você já se perguntou como funcionam os sistema...</td>\n",
       "      <td>10/05/2018</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>complicated</td>\n",
       "      <td>backprop</td>\n",
       "      <td>backpropagations</td>\n",
       "      <td>deriving</td>\n",
       "      <td>beginning</td>\n",
       "      <td>gate</td>\n",
       "      <td>behaviors</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>no emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>Spam Classifier in Python from scratch</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Tejan Karmali</td>\n",
       "      <td>105.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>We all face the problem of spams in our inboxe...</td>\n",
       "      <td>02/08/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>python</td>\n",
       "      <td>digit</td>\n",
       "      <td>install</td>\n",
       "      <td>idf</td>\n",
       "      <td>windows</td>\n",
       "      <td>implement</td>\n",
       "      <td>tf</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>joy/calmness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>Preparing for Insight</td>\n",
       "      <td>Insight</td>\n",
       "      <td>Insight</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>John Joo is an Insight alumnus from the August...</td>\n",
       "      <td>16/04/2014</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>bert</td>\n",
       "      <td>pt</td>\n",
       "      <td>sequence</td>\n",
       "      <td>python</td>\n",
       "      <td>netflix</td>\n",
       "      <td>keras</td>\n",
       "      <td>word</td>\n",
       "      <td>subjective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>expectation/interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>GOOGLE AI CONTEST</td>\n",
       "      <td>Altsoph’s blog</td>\n",
       "      <td>Aleksey Tikhonov</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Вчера узнал, что сейчас идет Google AI Contest...</td>\n",
       "      <td>16/09/2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>bert</td>\n",
       "      <td>pt</td>\n",
       "      <td>sequence</td>\n",
       "      <td>python</td>\n",
       "      <td>netflix</td>\n",
       "      <td>keras</td>\n",
       "      <td>word</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>no emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>🏎 Smaller, faster, cheaper, lighter: Introduci...</td>\n",
       "      <td>HuggingFace</td>\n",
       "      <td>Victor Sanh</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2113.0</td>\n",
       "      <td>2019, October 3rd — Update: We are releasing o...</td>\n",
       "      <td>28/08/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>bert</td>\n",
       "      <td>pt</td>\n",
       "      <td>sequence</td>\n",
       "      <td>python</td>\n",
       "      <td>netflix</td>\n",
       "      <td>keras</td>\n",
       "      <td>word</td>\n",
       "      <td>objective</td>\n",
       "      <td>neutral</td>\n",
       "      <td>expectation/interest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title           publication  \\\n",
       "idx                                                                             \n",
       "3756  Step-by-Step R-CNN Implementation From Scratch...  Towards Data Science   \n",
       "3757                              How Transformers Work  Towards Data Science   \n",
       "3758              Neural Style Transfer using VGG model  Towards Data Science   \n",
       "3759        Amazon’s Artificial Artificial Intelligence                Medium   \n",
       "3760          This week in the #SDGs- February 17, 2017          SDG Counting   \n",
       "...                                                 ...                   ...   \n",
       "4251  Redes Neurais, Perceptron Multicamadas e o Alg...             Ensina.AI   \n",
       "4252             Spam Classifier in Python from scratch  Towards Data Science   \n",
       "4253                              Preparing for Insight               Insight   \n",
       "4254                                  GOOGLE AI CONTEST        Altsoph’s blog   \n",
       "4255  🏎 Smaller, faster, cheaper, lighter: Introduci...           HuggingFace   \n",
       "\n",
       "                  author  followers  reading_time  n_words  \\\n",
       "idx                                                          \n",
       "3756        Rohit Thakur      206.0           8.0    800.0   \n",
       "3757  Giuliano Giacaglia     1100.0          14.0   2730.0   \n",
       "3758     Darshan Adakane       27.0           7.0   1013.0   \n",
       "3759      Sachin Palewar      130.0           1.0    127.0   \n",
       "3760         SDGCounting      679.0           2.0    280.0   \n",
       "...                  ...        ...           ...      ...   \n",
       "4251      Tiago M. Leite      113.0           9.0   1421.0   \n",
       "4252       Tejan Karmali      105.0           6.0    901.0   \n",
       "4253             Insight     2200.0           5.0   1108.0   \n",
       "4254    Aleksey Tikhonov      275.0           1.0     88.0   \n",
       "4255         Victor Sanh     1000.0          10.0   2113.0   \n",
       "\n",
       "                                              pure_text        date  \\\n",
       "idx                                                                   \n",
       "3756  Classification and object detection are the ma...  18/10/2019   \n",
       "3757  If you liked this post and want to learn how m...  11/03/2019   \n",
       "3758  Introduction:Before we begin, let’s go to this...  16/01/2020   \n",
       "3759  Today, we build complex software applications ...  21/11/2005   \n",
       "3760  1 . IISD provided context to news that the rep...  17/02/2017   \n",
       "...                                                 ...         ...   \n",
       "4251  Você já se perguntou como funcionam os sistema...  10/05/2018   \n",
       "4252  We all face the problem of spams in our inboxe...  02/08/2017   \n",
       "4253  John Joo is an Insight alumnus from the August...  16/04/2014   \n",
       "4254  Вчера узнал, что сейчас идет Google AI Contest...  16/09/2010   \n",
       "4255  2019, October 3rd — Update: We are releasing o...  28/08/2019   \n",
       "\n",
       "      n_code_chunks  bold_text_count  ...     key_word_4  key_word_5  \\\n",
       "idx                                   ...                              \n",
       "3756             11             17.0  ...         python       digit   \n",
       "3757              0             48.0  ...           bert     hugging   \n",
       "3758              0             17.0  ...      mobilenet       cifar   \n",
       "3759              0              0.0  ...  disadvantages        news   \n",
       "3760              0             17.0  ...     weatherman  weatherbot   \n",
       "...             ...              ...  ...            ...         ...   \n",
       "4251              0             39.0  ...    complicated    backprop   \n",
       "4252              0              5.0  ...         python       digit   \n",
       "4253              0             11.0  ...           bert          pt   \n",
       "4254              0              0.0  ...           bert          pt   \n",
       "4255              0             64.0  ...           bert          pt   \n",
       "\n",
       "            key_word_6     key_word_7    key_word_8  key_word_9  \\\n",
       "idx                                                               \n",
       "3756           install            idf       windows   implement   \n",
       "3757             label       visually       roberta         xlm   \n",
       "3758             keras      beautiful  applications       hands   \n",
       "3759            cyborg  specification  artificielle   assurance   \n",
       "3760             sales        judging        poncho  announcing   \n",
       "...                ...            ...           ...         ...   \n",
       "4251  backpropagations       deriving     beginning        gate   \n",
       "4252           install            idf       windows   implement   \n",
       "4253          sequence         python       netflix       keras   \n",
       "4254          sequence         python       netflix       keras   \n",
       "4255          sequence         python       netflix       keras   \n",
       "\n",
       "        key_word_10    analytic polarity               emotion  \n",
       "idx                                                             \n",
       "3756             tf   objective  neutral          joy/calmness  \n",
       "3757     extinction   objective  neutral  expectation/interest  \n",
       "3758         moving   objective  neutral      trust/acceptance  \n",
       "3759  revolutionary   objective  neutral          joy/calmness  \n",
       "3760    attribution   objective  neutral    surprise/amazement  \n",
       "...             ...         ...      ...                   ...  \n",
       "4251      behaviors   objective  neutral            no emotion  \n",
       "4252             tf   objective  neutral          joy/calmness  \n",
       "4253           word  subjective  neutral  expectation/interest  \n",
       "4254           word   objective  neutral            no emotion  \n",
       "4255           word   objective  neutral  expectation/interest  \n",
       "\n",
       "[500 rows x 31 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(os.path.join(DATA_PATH, 'scrapped_train_wemb.csv'))\n",
    "data.to_csv(os.path.join(DATA_PATH, 'scrapped_test_wemb.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenized_filtered_docs = [tokenizer.tokenize(filtered_doc) for filtered_doc in filtered_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_filtered_docs)]\n",
    "model = Doc2Vec(documents, vector_size=300, window=50, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xDEAD\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "torch.cuda.random.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [''.join(random.choice(string.ascii_lowercase) for i in range(7)) for _ in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names), len(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 756.79doc/s]\n"
     ]
    }
   ],
   "source": [
    "embed_cols = {}\n",
    "for i in range(len(names)):\n",
    "    embed_cols[names[i]] = []\n",
    "with tqdm(tokenized_filtered_docs, unit=\"doc\") as tepoch:\n",
    "    for doc in tepoch:\n",
    "        embeds = model.infer_vector(doc)\n",
    "        for i in range(len(names)):\n",
    "            embed_cols[names[i]] += [embeds[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n",
      "C:\\Users\\Aleksey\\AppData\\Local\\Temp\\ipykernel_68516\\3936720218.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[key] = embed_cols[key]\n"
     ]
    }
   ],
   "source": [
    "for key in embed_cols.keys():\n",
    "    data[key] = embed_cols[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('./scrapped_data/scrapped_train_emb.csv')\n",
    "data.to_csv('./scrapped_data/scrapped_test_emb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'publication',\n",
       " 'author',\n",
       " 'followers',\n",
       " 'reading_time',\n",
       " 'n_words',\n",
       " 'pure_text',\n",
       " 'n_code_chunks',\n",
       " 'bold_text_count',\n",
       " 'italic_text_count',\n",
       " 'mean_image_size',\n",
       " 'n_images',\n",
       " 'n_lists',\n",
       " 'n_vids',\n",
       " 'n_links',\n",
       " 'language',\n",
       " 'key_word_1',\n",
       " 'key_word_2',\n",
       " 'key_word_3',\n",
       " 'key_word_4',\n",
       " 'key_word_5',\n",
       " 'key_word_6',\n",
       " 'key_word_7',\n",
       " 'key_word_8',\n",
       " 'key_word_9',\n",
       " 'key_word_10',\n",
       " 'analytic',\n",
       " 'polarity',\n",
       " 'emotion',\n",
       " 'lpqlhqy',\n",
       " 'elzthpe',\n",
       " 'kxkcjvr',\n",
       " 'wnuuqhp',\n",
       " 'mwgungb',\n",
       " 'gqgywwh',\n",
       " 'ecsoakw',\n",
       " 'dunylaz',\n",
       " 'sxcuhbt',\n",
       " 'xblynot',\n",
       " 'qdfbtmq',\n",
       " 'kwgnson',\n",
       " 'cnjymxs',\n",
       " 'soyuffp',\n",
       " 'mvbgbgl',\n",
       " 'whosezu',\n",
       " 'mpmtpal',\n",
       " 'vnmjfgr',\n",
       " 'racmuni',\n",
       " 'zlrdzhd',\n",
       " 'fyuzgrn',\n",
       " 'qdnrsss',\n",
       " 'enjohmw',\n",
       " 'wvbmmwn',\n",
       " 'tikowll',\n",
       " 'rnydxru',\n",
       " 'btbravh',\n",
       " 'rsswsws',\n",
       " 'qxblzfh',\n",
       " 'uvmdzce',\n",
       " 'wkvgvqk',\n",
       " 'gvgrpwt',\n",
       " 'hmuoukh',\n",
       " 'rjbmiob',\n",
       " 'rofiqbc',\n",
       " 'abbesxh',\n",
       " 'hegknua',\n",
       " 'mrrgihh',\n",
       " 'vuxtwwn',\n",
       " 'gextnye',\n",
       " 'uyybjsx',\n",
       " 'hwbcrsr',\n",
       " 'kjixhvh',\n",
       " 'prwnpli',\n",
       " 'utcplbi',\n",
       " 'bhsagcj',\n",
       " 'ghyaobc',\n",
       " 'zjadhaw',\n",
       " 'stovckf',\n",
       " 'ymritrd',\n",
       " 'hvmhcbj',\n",
       " 'gjczmer',\n",
       " 'hsulfdc',\n",
       " 'lsnmmen',\n",
       " 'jqplnrg',\n",
       " 'kcibpcs',\n",
       " 'zvqmjsw',\n",
       " 'tbonnlw',\n",
       " 'mxgslif',\n",
       " 'spuczbl',\n",
       " 'cxqmzcs',\n",
       " 'qutybvv',\n",
       " 'ekdpcqm',\n",
       " 'cvhjgaf',\n",
       " 'hbxwsef',\n",
       " 'ixebdjh',\n",
       " 'nwcxgkz',\n",
       " 'ohflwii',\n",
       " 'fhwrzwv',\n",
       " 'tympeer',\n",
       " 'ijlgxbq',\n",
       " 'svycdwa',\n",
       " 'tgmmvdv',\n",
       " 'xwtmisa',\n",
       " 'wgkwmpf',\n",
       " 'gbfgvvd',\n",
       " 'uyakhyz',\n",
       " 'gbihviz',\n",
       " 'ifzhlog',\n",
       " 'bzgklxm',\n",
       " 'ddmtaio',\n",
       " 'lhdtbnl',\n",
       " 'bofzllz',\n",
       " 'sowmorg',\n",
       " 'ehygihx',\n",
       " 'qhdknfb',\n",
       " 'nsiseag',\n",
       " 'avydxwq',\n",
       " 'qyaihtf',\n",
       " 'vwiojeb',\n",
       " 'xastgqb',\n",
       " 'miurwek',\n",
       " 'fkrbuss',\n",
       " 'gyzplce',\n",
       " 'zamgkzu',\n",
       " 'vqgythl',\n",
       " 'ycixzft',\n",
       " 'khpnkef',\n",
       " 'trcmdcw',\n",
       " 'rrkrtwf',\n",
       " 'gbxucke',\n",
       " 'zzlnttz',\n",
       " 'yvhzmlh',\n",
       " 'fpowfyx',\n",
       " 'qnbeihy',\n",
       " 'wrpwftj',\n",
       " 'ngdseip',\n",
       " 'rxyicdq',\n",
       " 'daswkpm',\n",
       " 'klohkad',\n",
       " 'aavrcbk',\n",
       " 'mjnzpoi',\n",
       " 'duvucjx',\n",
       " 'vzgyngq',\n",
       " 'keucicw',\n",
       " 'cadbzts',\n",
       " 'rhbsbso',\n",
       " 'mwuebos',\n",
       " 'xfoobam',\n",
       " 'yftjrza',\n",
       " 'jwsicaq',\n",
       " 'kafqter',\n",
       " 'foqmyvu',\n",
       " 'rscvfxw',\n",
       " 'nkusytp',\n",
       " 'mdgjbnz',\n",
       " 'umjwcfb',\n",
       " 'ioyldfk',\n",
       " 'lydjthg',\n",
       " 'auhxqvn',\n",
       " 'qnwuwui',\n",
       " 'ngaqrpt',\n",
       " 'nndpdke',\n",
       " 'hwptnsw',\n",
       " 'weqevig',\n",
       " 'escbfin',\n",
       " 'dxrldmm',\n",
       " 'zhnhdya',\n",
       " 'jvvgnak',\n",
       " 'ffuvqee',\n",
       " 'pxbpdwh',\n",
       " 'fwyfrzc',\n",
       " 'lhuelkb',\n",
       " 'azjweua',\n",
       " 'hnxozre',\n",
       " 'xuxmbuv',\n",
       " 'azofqrp',\n",
       " 'zqwsjwk',\n",
       " 'zqihlit',\n",
       " 'rnhaygj',\n",
       " 'stovmmq',\n",
       " 'iyxmcda',\n",
       " 'zyvrmty',\n",
       " 'tqyycgo',\n",
       " 'fpyjwjj',\n",
       " 'ymozzoc',\n",
       " 'bhmfuxv',\n",
       " 'dqcoeqp',\n",
       " 'gqimxzh',\n",
       " 'dmgbxom',\n",
       " 'aziruci',\n",
       " 'ccaleuc',\n",
       " 'dmrwurx',\n",
       " 'suixdda',\n",
       " 'zhbgxcb',\n",
       " 'wbkfrwa',\n",
       " 'bburclo',\n",
       " 'kqdhnkf',\n",
       " 'dudahec',\n",
       " 'axpnymm',\n",
       " 'urbocoa',\n",
       " 'oxxyvha',\n",
       " 'vwmcefj',\n",
       " 'hoakjym',\n",
       " 'dpzjylk',\n",
       " 'omwrnci',\n",
       " 'gksoqcb',\n",
       " 'sycpfzw',\n",
       " 'ataffxd',\n",
       " 'jatpxwv',\n",
       " 'rtvfdwn',\n",
       " 'ivssvbg',\n",
       " 'upkgtpg',\n",
       " 'vnllwxj',\n",
       " 'hmzmpcy',\n",
       " 'jmmwpjb',\n",
       " 'nrzkyya',\n",
       " 'vcuxvpo',\n",
       " 'rdwtxwl',\n",
       " 'jxpqbte',\n",
       " 'yjdptgy',\n",
       " 'pmgeilv',\n",
       " 'fqbmzpf',\n",
       " 'hlcicni',\n",
       " 'cmbadqx',\n",
       " 'kuuituw',\n",
       " 'vcavjjq',\n",
       " 'amgxbyq',\n",
       " 'nxjgqyl',\n",
       " 'vbdknik',\n",
       " 'fhsqwua',\n",
       " 'kmadfcz',\n",
       " 'aozkifh',\n",
       " 'mavucso',\n",
       " 'jmxthix',\n",
       " 'rcpxotm',\n",
       " 'rscnfqo',\n",
       " 'atbersw',\n",
       " 'ypczzmr',\n",
       " 'gholhsg',\n",
       " 'vqkaaik',\n",
       " 'dplvvwr',\n",
       " 'ztyfqoi',\n",
       " 'ltavpty',\n",
       " 'feshknv',\n",
       " 'adsvtvx',\n",
       " 'khwcicf',\n",
       " 'oqdebsv',\n",
       " 'shycsnq',\n",
       " 'heevscf',\n",
       " 'hboudty',\n",
       " 'fetztdb',\n",
       " 'hxspuzc',\n",
       " 'qpdtzkg',\n",
       " 'qklnwjy',\n",
       " 'mudfanj',\n",
       " 'bokwqwm',\n",
       " 'qqhirxg',\n",
       " 'wuyxcgm',\n",
       " 'ciestmz',\n",
       " 'qqtoklj',\n",
       " 'rvtwspw',\n",
       " 'aodlwed',\n",
       " 'bgkrwkl',\n",
       " 'uphrtfx',\n",
       " 'ckdwxvb',\n",
       " 'nawivbw',\n",
       " 'wpssfct',\n",
       " 'wkhxswj',\n",
       " 'rylqmes',\n",
       " 'kmlpvkz',\n",
       " 'zszwhbo',\n",
       " 'tzgxcvn',\n",
       " 'lzxkcor',\n",
       " 'uucsksx',\n",
       " 'wtxcetm',\n",
       " 'ccjakqk',\n",
       " 'ytftemj',\n",
       " 'hhswbzw',\n",
       " 'lsbxswi',\n",
       " 'sszembb',\n",
       " 'qjfukix',\n",
       " 'pqnwlln',\n",
       " 'soowhsg',\n",
       " 'foyrves',\n",
       " 'uzfgvzr',\n",
       " 'ueedudg',\n",
       " 'ordjzeo',\n",
       " 'wddosll',\n",
       " 'wvldntj',\n",
       " 'alueyxf',\n",
       " 'xknufum',\n",
       " 'yslqhcv',\n",
       " 'ojyhjau',\n",
       " 'osyehem',\n",
       " 'iegrigh',\n",
       " 'wxbvpwi',\n",
       " 'lkbxbik',\n",
       " 'iiyvawc',\n",
       " 'rtukvhe',\n",
       " 'lxgnraj',\n",
       " 'ausbulx',\n",
       " 'metrkef',\n",
       " 'qzrwajt',\n",
       " 'gjvaldl',\n",
       " 'rnbyikx',\n",
       " 'ygbrxxz',\n",
       " 'dvouwup',\n",
       " 'ajssotv',\n",
       " 'pghgqlu',\n",
       " 'mfipnhu',\n",
       " 'gtykppw',\n",
       " 'ymhkcib',\n",
       " 'hwxegeu',\n",
       " 'ungzijt',\n",
       " 'ayzlhui',\n",
       " 'tcgpvjk',\n",
       " 'zdythqj',\n",
       " 'zclpxed',\n",
       " 'zkexvks',\n",
       " 'teqfuoz',\n",
       " 'ghztkwo',\n",
       " 'aoqlbbp',\n",
       " 'pspwgzm',\n",
       " 'alwcsyr',\n",
       " 'hlkvlye',\n",
       " 'kucqrja',\n",
       " 'cmkluyt',\n",
       " 'zwfnazw',\n",
       " 'apdshvu']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>...</th>\n",
       "      <th>teqfuoz</th>\n",
       "      <th>ghztkwo</th>\n",
       "      <th>aoqlbbp</th>\n",
       "      <th>pspwgzm</th>\n",
       "      <th>alwcsyr</th>\n",
       "      <th>hlkvlye</th>\n",
       "      <th>kucqrja</th>\n",
       "      <th>cmkluyt</th>\n",
       "      <th>zwfnazw</th>\n",
       "      <th>apdshvu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Understanding Variational Autoencoders (VAEs)</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Joseph Rocca</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4892</td>\n",
       "      <td>This post was co-written with In the last few ...</td>\n",
       "      <td>24/09/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.275204</td>\n",
       "      <td>0.452670</td>\n",
       "      <td>-0.188352</td>\n",
       "      <td>1.122959</td>\n",
       "      <td>0.193645</td>\n",
       "      <td>0.896118</td>\n",
       "      <td>-0.565581</td>\n",
       "      <td>-0.196053</td>\n",
       "      <td>1.476925</td>\n",
       "      <td>-2.825912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Using Mixed-Effects Models For Linear Regression</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Guido Vivaldi</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>889</td>\n",
       "      <td>Mixed-effects regression models are a powerful...</td>\n",
       "      <td>18/05/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120053</td>\n",
       "      <td>0.158719</td>\n",
       "      <td>-0.268230</td>\n",
       "      <td>0.034773</td>\n",
       "      <td>-0.062082</td>\n",
       "      <td>0.031392</td>\n",
       "      <td>0.274946</td>\n",
       "      <td>-0.075915</td>\n",
       "      <td>0.200595</td>\n",
       "      <td>-0.302712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The New Moats</td>\n",
       "      <td>Greylock Perspectives</td>\n",
       "      <td>Jerry Chen</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2217</td>\n",
       "      <td>To build a sustainable and profitable business...</td>\n",
       "      <td>24/04/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230017</td>\n",
       "      <td>0.516112</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>-0.695624</td>\n",
       "      <td>-0.159152</td>\n",
       "      <td>1.828951</td>\n",
       "      <td>0.997532</td>\n",
       "      <td>-0.007328</td>\n",
       "      <td>1.790782</td>\n",
       "      <td>1.030197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Region of Interest Pooling</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Sambasivarao. K</td>\n",
       "      <td>239.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>523</td>\n",
       "      <td>The major hurdle for going from image classifi...</td>\n",
       "      <td>22/04/2019</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084163</td>\n",
       "      <td>0.110545</td>\n",
       "      <td>0.310296</td>\n",
       "      <td>0.080134</td>\n",
       "      <td>0.366252</td>\n",
       "      <td>0.455576</td>\n",
       "      <td>0.044713</td>\n",
       "      <td>-0.162835</td>\n",
       "      <td>0.297777</td>\n",
       "      <td>0.027572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Modern Gaussian Process Regression</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Ryan Sander</td>\n",
       "      <td>173.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1433</td>\n",
       "      <td>Ever wonder how you can create non-parametric ...</td>\n",
       "      <td>24/03/2021</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>0.109475</td>\n",
       "      <td>-0.181747</td>\n",
       "      <td>-0.261613</td>\n",
       "      <td>-0.409631</td>\n",
       "      <td>-0.283935</td>\n",
       "      <td>0.674631</td>\n",
       "      <td>-0.130203</td>\n",
       "      <td>0.504441</td>\n",
       "      <td>-0.481930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Forecasting with Bayesian Dynamic Generalized ...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Ryan Clukey</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1896</td>\n",
       "      <td>Forecasting is critical for nearly all busines...</td>\n",
       "      <td>18/03/2021</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186812</td>\n",
       "      <td>0.328196</td>\n",
       "      <td>-0.900748</td>\n",
       "      <td>0.099534</td>\n",
       "      <td>-0.454308</td>\n",
       "      <td>-0.088426</td>\n",
       "      <td>0.872544</td>\n",
       "      <td>-0.117408</td>\n",
       "      <td>0.135162</td>\n",
       "      <td>-0.527803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Understanding few-shot learning in machine lea...</td>\n",
       "      <td>Quick Code</td>\n",
       "      <td>Dr. Michael J. Garbade</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>798</td>\n",
       "      <td>Machine learning has experienced tremendous gr...</td>\n",
       "      <td>25/08/2018</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242746</td>\n",
       "      <td>0.110655</td>\n",
       "      <td>-0.041816</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>-0.083854</td>\n",
       "      <td>0.153117</td>\n",
       "      <td>0.053768</td>\n",
       "      <td>0.026139</td>\n",
       "      <td>0.283676</td>\n",
       "      <td>-0.203898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Appearance of The Principate [Pt. III]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Daniel Voshart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>525</td>\n",
       "      <td>Using the neural-net tool Artbreeder, Photosho...</td>\n",
       "      <td>24/07/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494427</td>\n",
       "      <td>-0.190719</td>\n",
       "      <td>0.040051</td>\n",
       "      <td>-0.219543</td>\n",
       "      <td>0.243626</td>\n",
       "      <td>-0.022882</td>\n",
       "      <td>0.073482</td>\n",
       "      <td>-0.018085</td>\n",
       "      <td>0.332167</td>\n",
       "      <td>0.345467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Zero and Few Shot Learning</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Eram Munawwar</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>895</td>\n",
       "      <td>The field of NLP is getting more and more exci...</td>\n",
       "      <td>05/01/2021</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177237</td>\n",
       "      <td>0.186244</td>\n",
       "      <td>0.236653</td>\n",
       "      <td>0.106422</td>\n",
       "      <td>0.455115</td>\n",
       "      <td>0.427260</td>\n",
       "      <td>-0.185777</td>\n",
       "      <td>0.163331</td>\n",
       "      <td>0.147951</td>\n",
       "      <td>0.226723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Classification on a large and noisy dataset wi...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Gabriel Pierobon</td>\n",
       "      <td>324.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1903</td>\n",
       "      <td>Some days ago I wrote an article describing a ...</td>\n",
       "      <td>01/10/2018</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566872</td>\n",
       "      <td>0.372082</td>\n",
       "      <td>-0.058999</td>\n",
       "      <td>0.259310</td>\n",
       "      <td>0.637857</td>\n",
       "      <td>0.073309</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>-0.108666</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>-0.091150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title            publication  \\\n",
       "idx                                                                             \n",
       "1        Understanding Variational Autoencoders (VAEs)   Towards Data Science   \n",
       "2     Using Mixed-Effects Models For Linear Regression   Towards Data Science   \n",
       "3                                        The New Moats  Greylock Perspectives   \n",
       "4                           Region of Interest Pooling   Towards Data Science   \n",
       "5                   Modern Gaussian Process Regression   Towards Data Science   \n",
       "..                                                 ...                    ...   \n",
       "237  Forecasting with Bayesian Dynamic Generalized ...   Towards Data Science   \n",
       "238  Understanding few-shot learning in machine lea...             Quick Code   \n",
       "239             Appearance of The Principate [Pt. III]                    NaN   \n",
       "240                         Zero and Few Shot Learning   Towards Data Science   \n",
       "241  Classification on a large and noisy dataset wi...   Towards Data Science   \n",
       "\n",
       "                     author  followers  reading_time  n_words  \\\n",
       "idx                                                             \n",
       "1              Joseph Rocca     3700.0          23.0     4892   \n",
       "2             Guido Vivaldi      256.0           6.0      889   \n",
       "3                Jerry Chen     4700.0          11.0     2217   \n",
       "4           Sambasivarao. K      239.0           4.0      523   \n",
       "5               Ryan Sander      173.0          10.0     1433   \n",
       "..                      ...        ...           ...      ...   \n",
       "237             Ryan Clukey       17.0          10.0     1896   \n",
       "238  Dr. Michael J. Garbade     2900.0           4.0      798   \n",
       "239          Daniel Voshart        NaN           8.0      525   \n",
       "240           Eram Munawwar       56.0           6.0      895   \n",
       "241        Gabriel Pierobon      324.0          12.0     1903   \n",
       "\n",
       "                                             pure_text        date  \\\n",
       "idx                                                                  \n",
       "1    This post was co-written with In the last few ...  24/09/2019   \n",
       "2    Mixed-effects regression models are a powerful...  18/05/2019   \n",
       "3    To build a sustainable and profitable business...  24/04/2017   \n",
       "4    The major hurdle for going from image classifi...  22/04/2019   \n",
       "5    Ever wonder how you can create non-parametric ...  24/03/2021   \n",
       "..                                                 ...         ...   \n",
       "237  Forecasting is critical for nearly all busines...  18/03/2021   \n",
       "238  Machine learning has experienced tremendous gr...  25/08/2018   \n",
       "239  Using the neural-net tool Artbreeder, Photosho...  24/07/2020   \n",
       "240  The field of NLP is getting more and more exci...  05/01/2021   \n",
       "241  Some days ago I wrote an article describing a ...  01/10/2018   \n",
       "\n",
       "     n_code_chunks  bold_text_count  ...   teqfuoz   ghztkwo   aoqlbbp  \\\n",
       "idx                                  ...                                 \n",
       "1                0               35  ... -2.275204  0.452670 -0.188352   \n",
       "2                9                6  ...  0.120053  0.158719 -0.268230   \n",
       "3                0                5  ... -0.230017  0.516112  0.031266   \n",
       "4                4                5  ...  0.084163  0.110545  0.310296   \n",
       "5                2               96  ...  0.199988  0.109475 -0.181747   \n",
       "..             ...              ...  ...       ...       ...       ...   \n",
       "237              5                6  ...  0.186812  0.328196 -0.900748   \n",
       "238              0                7  ... -0.242746  0.110655 -0.041816   \n",
       "239              0              113  ...  0.494427 -0.190719  0.040051   \n",
       "240              7                0  ... -0.177237  0.186244  0.236653   \n",
       "241             23                7  ...  0.566872  0.372082 -0.058999   \n",
       "\n",
       "      pspwgzm   alwcsyr   hlkvlye   kucqrja   cmkluyt   zwfnazw   apdshvu  \n",
       "idx                                                                        \n",
       "1    1.122959  0.193645  0.896118 -0.565581 -0.196053  1.476925 -2.825912  \n",
       "2    0.034773 -0.062082  0.031392  0.274946 -0.075915  0.200595 -0.302712  \n",
       "3   -0.695624 -0.159152  1.828951  0.997532 -0.007328  1.790782  1.030197  \n",
       "4    0.080134  0.366252  0.455576  0.044713 -0.162835  0.297777  0.027572  \n",
       "5   -0.261613 -0.409631 -0.283935  0.674631 -0.130203  0.504441 -0.481930  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "237  0.099534 -0.454308 -0.088426  0.872544 -0.117408  0.135162 -0.527803  \n",
       "238  0.030937 -0.083854  0.153117  0.053768  0.026139  0.283676 -0.203898  \n",
       "239 -0.219543  0.243626 -0.022882  0.073482 -0.018085  0.332167  0.345467  \n",
       "240  0.106422  0.455115  0.427260 -0.185777  0.163331  0.147951  0.226723  \n",
       "241  0.259310  0.637857  0.073309  0.235933 -0.108666  0.010706 -0.091150  \n",
       "\n",
       "[240 rows x 330 columns]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data #привести к интам followers, reading time, n words, code chunks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'KeyedVectors' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aleksey\\Documents\\[important]hackatons\\[a]ru_code\\championship\\claps_predictor\\lda.ipynb Cell 38'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Aleksey/Documents/%5Bimportant%5Dhackatons/%5Ba%5Dru_code/championship/claps_predictor/lda.ipynb#ch0000053?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mdv(\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'KeyedVectors' object is not callable"
     ]
    }
   ],
   "source": [
    "model.dv(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02490822, 0.00062468, 0.04075422, 0.00955177, 0.08878721],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(common_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7abe41bf88626c8179a12d706309ac1665b0164ec3bd4c11d7418ed9f0904ed3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
