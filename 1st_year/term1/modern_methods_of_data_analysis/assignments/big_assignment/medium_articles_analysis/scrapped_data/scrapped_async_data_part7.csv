,,https://medium.com/u/e44e80063f4d?source=post_page-----c35fc29ccf6--------------------------------,Javier Frias,13,,,,,,0,,,,,,,0,,
,,https://medium.com/u/b82ad41a1ef1?source=post_page-----c35fc29ccf6--------------------------------,Tim Hinz,2,,,,,,0,,,,,,,0,,
,,https://medium.com/u/e44e80063f4d?source=post_page-----c35fc29ccf6--------------------------------,Javier Frias,13,,,,,,0,,,,,,,0,,
Building a Wordle Bot In Under 100 Lines of Python,Better Programming,https://betterprogramming.pub/building-a-wordle-bot-in-under-100-lines-of-python-9b980539defb,Matt Dodge,348,10,1891,"If you don’t know what Wordle is, get out from under your rock and check it out. It’s a simple game at surface level but does end up needing some strategy to master it. It didn’t take more than a couple of rounds of playing it before my curiosity piqued and I felt the need to build a bot to play the game and learn more about it. Let’s get going!Build a Python program that plays Wordle. It should make guesses of words and try and figure out the ultimate answer word in as few guesses as possible.The basics of the game are pretty straightforward. We need to be able to keep a list of valid words, make guesses, collect responses, and update our word list based on the data in those responses. This high-level Python code can be the baseline of our solving application.We really only have 3 methods to build here with this simple structure. The first is make_guess. This method should take a list of valid words and guess a word out of that list. To start, let’s just choose randomly. We’ll make this smarter later. For now we just want to get the basic gameplay in place.The collect_result method just needs to collect some input from the user about the guess. The user of our program will input the guess into the Wordle website and that will tell us what is right and wrong. The user needs to input those values into our program. We’ll use 3 different characters to denote the 3 possible results for each letter:Some straightforward Python user input collection along with some regex matching for data validation gets us on our way.Now we need to update our valid words based on our guess and the result, that’s what the update_valid_words method is for. We’ll start with a trivial implementation of this that isn’t very performant but should get the job done. We’ll iterate through all remaining valid words and see if they would have yielded the same result had we guessed our guess. In order to do this, we’ll need a new method called get_result that will return what the result for a guess would be if we knew the answer.There are a few issues with this, specifically around words with duplicate letters (e.g. ROBOT, SPEED, etc). But we’ll sort those out later. At this point we have enough to play the game! We can put all of these methods together and play. Let’s see how our game plays with a target word of SNAKE.It worked! In 5 guesses even, not too shabby considering the simple approach for a lot of those methods. Since we’re relying on a random choice here this will likely be different each time too, 5 seemed to be the median solve amount for SNAKE given this algorithm. Let’s see if we can do better.A good Wordle player is one who can guess a good word. Astute analysis, am I right? To start we just randomly picked a word from our remaining words, but that’s not very smart. What makes a good guess though?My initial theory was that a good guess would be one that had the most common letters in it. This is a fairly safe assumption, especially early on in the guessing. With many words left we want to learn as much about them all as we can, so guessing common letters is a natural instinct. Surely a first guess of “RATED” would likely teach us more about the word than a guess of “JAZZY” would. Let’s enhance our make_guess method to employ this strategy.To do this, the first step is to figure out which letters are the most frequent. We should only consider valid words for these character counts too, no sense counting words we know are not correct. Python’s built-in Counter object can help with this.After we get the counts of each character in the remaining words, we want to find the words that have the most of these characters. We also don’t want to count duplicates here either. Remember, our goal is to find out about as many letters as possible. So while E may be the most common letter in words, it doesn’t make sense to guess EERIE. We don’t want to waste those other letter slots.Not a terribly complicated method. We can probably make a few enhancements, like ignore letters that we already know are in the word, but this should do for now. Let’s play again against the word SNAKE and see if we do better.We still got the word but it still took us 5 tries. Also, since our guessing algorithm is deterministic now, this will be the result every single time we play, at least for a target word of SNAKE.One interesting observation here is that this algorithm thinks the best possible starting word is LATER (or any of its anagrams i.e., ALTER, ALERT). If you’re looking for a good starting word, these might be good ones to try.This algorithm will get some words in just a few tries, but 5 tries doesn’t cut it for me. I can do that on my own, I want my computer to be smarter than me. Let’s do better.Linear-time frequency counting isn’t much of an algorithm to write home about. We might even be able to roughly eyeball that ourselves as humans. If we think about what a good guess is, it is one that will tell us the most information based on the response. In other words, it is the one that will yield, on average, the smallest next list of valid words.We have a computer in front of us, we can simulate this! For each valid word remaining, what would happen if we guessed it? We can run each valid word as a guess against every other valid word as an answer and see what the resulting word list would be. This comes out to an O(N^2) algorithm against a relatively small problem space of just a few thousand words. Let’s go!This doesn’t look too bad, basically we’re just seeing how many words will be remaining after every possible guess and every possible answer. It will definitely be slower, but I think it will be smarter too. However, there’s a small gotcha here. We thought our algorithm would run in O(N²) time, which isn’t too bad. But our rather simple implementation of update_valid_words comes back to bite us here. That method actually has its own O(N) runtime, so really this guess method will now run in O(N^3). It’s still a small data set, but cubic runtime gets big quickly.Rather than trying to optimize either of those methods, let’s just make this strategy the one that runs once we’re down to less than 500 words in our valid word list. We can keep the frequency counting algorithm for longer word lists but once we’re closer to the end we’ll do the deep search.Time to try it out against the word SNAKE again!Nice! Only 3 guesses! We might be getting a bit lucky here though. If we know the S, A, and E are in the right places then SNAKE, SUAVE, and SPACE should all be roughly equal. But this algorithm should still be more performant in the general case. Can we test that theory though?At this point I got a bit curious, how would this bot do against other wordle words? I had spot-checked a few of the recent games and the bot was consistently beating my mere human brain, exactly what I wanted! Would this be the case for any word? Was there a theoretical maximum number of guesses a bot with this strategy would need to guess a word?I decided to run a simulator, much like we do for the exhaustive guess algorithm. For every word in the word list, run the bot and see how many guesses it would take to guess it. The code for this wasn’t really challenging, but it’s a bit beside the point so I won’t share it here.As expected, most of the words were solved in 2 or 3 guesses. The occasional 4 would slip in there too. But there was one word that took 8, yes 8, guesses to get. Without further ado, I present to you the hardest possible Wordle word:Catch? Really? Why is that hard? Pretty easy word with pretty common letters. What’s going on here? Let’s play with our bot and see what happens.Interesting! The bot quickly got to the _ATCH suffix, but then was stuck spinning its wheels trying to guess the first letter. Because it didn’t have that much information, it was really only able to guess one letter at a time and hope that it was right. If we could somehow learn about more than one letter here we could probably solve it faster.And there it is, a truly marvelous realization after I looked at this for a while:It is sometimes advantageous to guess a word that you know is wrong in order to make a better guess on your next word.The rules just state the guess has to be a real word, not a word that is valid given the current set of known information. In other words, if we have 6 words that end in _ATCH left in our list, don’t just guess them one by one. Instead, guess another word that isn’t in the remaining valid word list to try and get as much info as possible.What does this look like in code? It’s surprisingly easy. Instead of iterating through valid words looking for possible guesses, we’ll iterate through all words. This will have an obvious performance hit, so we’ll only run this when we’re down to 50 words or less.Here’s the relevant part of our make_guess_exhaustive method nowNow what happens if we run against CATCH?Look at that! After finding out that the A and T were correct, the bot guesses CRIMP with zero intention of winning but instead gathering information. As a result, it knows which letter is the first letter and is able to guess CATCH correctly on the 3rd try. Much better than the 8th guess like before. I call that a win!This was a fun little experiment and weekend project. I think it also demonstrates how an idea can start small and with some pretty simple pseudo-code and eventually turn into a rather complex and impactful algorithm. The code is still concise, but it ends up getting the job done.There were a few gotchas I discovered along the way while building this that I didn’t get into too much here, for the sake of keeping the article focused. The final code, shared below, includes these enhancements and realizations though.To check out the finished code for this project, head on over to the mattdodge/wordle_bot GitHub repo. Feel free to fork, report issues, and/or submit PRs there.The ultimate file ended up being over 100 lines of code, but that’s mainly because it includes the enhancements and other features I mentioned here.I hope you enjoyed this post and this journey of making a fun little bot for a fun little game. Of course, don’t use this bot to cheat, that’s not cool. But learning and tinkering with Python for real-world projects like this is always fun — happy Wordle-ing!This post is cross-posted from Matt Dodge’s personal blog. Read the original post here.",,,5,4,6,1225,816,1,2,0,3,81
,,https://medium.com/u/274d35118c90?source=post_page-----c35fc29ccf6--------------------------------,Matt Dodge,348,,,,,,0,,,,,,,0,,
Reverse Engineering Wordle,,https://medium.com/@timhinz16/reverse-engineering-wordle-4674c3ae700d,Tim Hinz,2,5,811,"It was another day at work when a co-worker of ours dropped a link to a game called Wordle. At this point I had never heard of the game, you could say I was living under a rock, but it became a regular thing at our work to start the day. Now me being the nosey developer I am, I decided to take a gander at the website source and I found a bunch of interesting things that I’d love to share with you.In this article, we will learn about reversing a website's javascript, and then using what we learned we will create a script that will fetch the daily answer for us(for when we get stuck obviously).Before we can write a script to fetch us the word, we need to understand how the game works internally. Thanks to the wonderful developer tools built into modern web browsers this is quite trivial.First, we will open the developer tools in google chrome. To do this you can hit F12 on your keyboard. Once open we will have some tabs, we want the `sources` tab. At this point you should have a page that looks like this:Now you’ll notice the code is a minified mess that is unreadable. The good news is chrome has a built-in tool to help with this as well! If you look right above the source code you will find a blue button that says “Pretty-print”. Click that button and watch the magic happen as the code becomes readable!Finally, let’s go ahead and select all the code and copy it to our favorite code editor.At this point, you should have a code editor open with the game's source in it. Before we can write our solver script we need to figure out how the game's source works. The first thing you’ll notice is that while the code is formatted to be readable, the function and variable names are meaningless. This is due to the code having gone through obfuscation from what looks like webpack. This doesn’t matter too much as we don’t really care what things are called, but it’s something to be aware of.To start let’s look over the script. There are a few approaches we can take, but the simplest, in this case, is just searching for helpful words. In our case let's search the code for the word “solution”.After flipping through the results you will come across a really interesting code snippet where they set e.solution to the value of the function Da(e.today).Let’s trace this down and figure out what it’s doing! Search the code for the function Da(). Once you find it you will be looking at some code like this:This function calls another function Ga(e) (which we will come back to in a second) as well as returns a value using the variable La. Let’s start with La to see if this is the right function. If you search for the variable La you will eventually find a variable that looks like this:This looks like a word list to me! This tells us we’re probably in the correct function. Let’s create a new script called index.js where we will dump our findings to create our script. Go ahead and grab the variable La and paste that in your script. While you’re at it, grab the Da function and paste it as well. You should have something like this:Now if you remember, the function Da also calls function Ga so we should probably figure out what that does, let’s find it!This function is super simple, it takes a parameter e, then passes that to the function Na() with the variable Ha. Let’s add that to our script life then find Ha. If we search for Ha you will find a variable set to date.Let’s grab the whole thing and add it to our script. We should now have this:Okay so the next piece is function Na(), let’s find it:Cool! This is the end of the road for our function chain! Just like before let’s take it and add it to our script resulting in this:We now have all the pieces we need to finish our solver! If you remember the game source calls function Da() to start, so we should too. Let’s create a variable and store the result of the function chain:Now Da() needs a parameter called e. Let’s go back to where we started and see what it is.They supply e.today as the parameter, which is a variable they set, so let’s grab it.So using this information we can change our variable to:All that’s left is to run it. You can run it using:Congrats! You have a working Wordle solver!Using our code above I’ve gone ahead and created a basic webpage that will update with the answer for ease of use to show just what you can do with this code.Thanks for reading!Links:celtech.github.iogithub.com",,,12,0,0,1225,444,3,0,0,2,51
,,https://medium.com/u/fcfc270499a7?source=post_page-----c35fc29ccf6--------------------------------,Conor Aspell,38,,,,,,0,,,,,,,0,,
,,https://medium.com/u/8e7d2b0e2483?source=post_page-----c35fc29ccf6--------------------------------,Anna Azzam,118,,,,,,0,,,,,,,0,,
,,https://medium.com/u/e353ddb0c125?source=post_page-----c35fc29ccf6--------------------------------,Sejal Dua,455,,,,,,0,,,,,,,0,,
Beating Wordle with Python,,https://conor-aspell.medium.com/beating-wordle-with-python-1e338a43dfd1,Conor Aspell,38,7,1108,"Wordle is a simple game that has experienced a meteoric rise in growth over the past month. You have 6 attempts to solve a 5 letter word. If you get a letter in the correct position it will light up green. If it is in the word but in the incorrect position it will light up brown, otherwise it will stay grey.It has been featured in the New York Times, the Guardian and my family’s WhatsApp Group.I try and play optimally, picking words with common letters and no duplicates etc. but I occasionally still lose while other people who play a less optimal approach, win.I decided to use Python to see if Wordle can be solved every time.First we will select a secret word. For example, lets say the word is “stare”.We are going to get our pool of 5 letters words from here.We are going to score each word using scrabble scoring and give a small penalty for duplicates. Similar to my previous fantasy football post.We are going to pick a word and store the result.We want to know if each letter was correct and in position, correct and out of position or incorrect.For each correct and in position letter, we want to filter out all words from our pool that do not have a letter in that position.For each correct but out of position letter we want to remove all words from the pool that do not contain that letter.For each incorrect letter, we want to remove all words that do not contain that letter.We then repeat the process, picking words at random from the pool until we get the correct one or fail.I’m going to add up the Scrabble score for each letter and add 2 more points if the word has any duplicate letters.For example, the word “later” would receive a score of 5 as all 5 letters are worth 1 point in Scrabble but a word like “pzazz” would receive 3+10+1+10+10 and an additional 2 point penalty for having duplicates for a score of 36. P is worth 3 points, A is worth 1 and Z is worth 10.To do this,We get lowest score in the dataframe and remove all words from the pool that are higher than that score. We then take one of the lowest scoring words at random.On our first attempt, there are 213 words with a score of 5 (the lowest possible score). We pick out “liers” at random.If the word is correct, we end the game and start again with a new word.We want to loop through the word and check if the letter is correct and the right position, correct and in the wrong position or incorrect.We are going to return 3 things.These will be decided by:“Liers” is not the correct guess (reminder the word is stare). Our method loops through each letter and returns the following:We know that “L” and “I” are not in the word so we can filter them out using a regex and then use “contains” to remove all words that match the regex.We start with 5757 words in our pool, once we remove all words containing “L” and “I”, it falls to 3161.We then loop through the valid letters dictionary. If the position is a question mark, we remove every word that does not contain that letter. We want to remove all words that have the letter “E” or “S”. This can easily be done with “contains” like before.Our potential words falls from 3161 to 1601 after this filter.We know that there is an R in position 3 so we remove all words that do not have an R in position 3. This reduces the possible words from 1601 to just 43.We know that there is an S and an E in the word but also that the E is not in position 2 and S is not in position 4. We remove all words with an E in position 2 and an S in position 4. This reduces the possible words down to just 13.After 1 guess, we have eliminated <99% of possible words.The remaining pool looks like this.On our second attempt, 4 words have the minimum score store, stare, snore and snare. The sample picks out “stare” and the game is won.I decided to run this algorithm for all 5757 words in our pool. It solved for 5237 words with the following distribution:It solved most words in 4 guesses but almost 10% of words were not solved. I would like to declare victory and say that Wordle is not a 100% solvable game but unfortunately, the solution can be improved.Looking through the logs I noticed that words like “water” were not being solved as it would guess “later” around attempt 3 and be left with “cater”, “dater”, “hater”, “mater”, “pater” and “water” and its only option would be to try each one and hope to get the answer.If it were to try a word like “champ” it could eliminate “cater”, “hater”, “mater” and “pater” with 1 guess, leaving just “dater” and “water” to try on the next attempt.So I decided to check when recommending a word, if there is more 3 or more letters picked and we have 4 or less guesses, we would use a different method to pick a word.I decided to rescore the pool of words and give points if it is one of the remaining letters in a remaining column.We pass in the chose letters, letter value dictionary, vacant positions and a column map.We derive the remaining letters by looking at the vacant positions and taking every unique letter from the column.We change the column map so that every remaining letter gets -20 instead of their original score. We map and add up their scores like we did previously and add a 40 point penalty for duplicate letters.We finally take the best scoring word and return that as our chosen word. For “water” this is “champ”.Using this method, our words unsolved falls from 500 to 150. Our distribution significantly changes as the majority of words are solved on the 6th attempt. I can now solve 97.5% of words on Wordle.However, 97.5% is not 100%. If Wordle were to choose a word like “Kooks” it would be almost impossible to get under any rational approach as it has only 3 unique letters and the 5th least frequent letter in English. You would have to play Wordle like my younger sister and get lucky by picking “looks” and then guessing “kooks” instead of “books” or “cooks”.You can find all the code on my Github here. Feel free to give this a clap and connect with me on LinkedIn.",,,3,2,0,527,418,4,4,0,4,39
What’s the Best First Word for Wordle?,Better Programming,https://betterprogramming.pub/whats-the-best-first-word-for-wordle-bb94db15ad2e,Anna Azzam,118,4,415,"Wordle is an online puzzle game that has been taking the world by storm. Every day, Wordle gives its players a new word to guess, and players get six guesses to find the answer. With the game's popularity, many people have been debating if there is a definitive “best first word” that will maximize their chances of winning the puzzle in the least number of steps. In this blog, I’ll show you a basic algorithm I wrote to try and answer this question!The first step is to build a function that replicates the game checker. In wordle, when you guess a word, it will tell you how close your word is to the answer using three colors — grey, yellow and green.As an example, suppose the answer is grave, and your guess is large , wordle will return something like this:Meaning:To codify this, I added a type Result type:I then wrote an evaluateGuess method which takes in a guess and an answer, and returns an array of Result for each letter in the guess. Here’s the function:The next step was to build an algorithm that plays the game, by suggesting the next guess based on the current game state, then calling evaluateGuess to determine the new game state.Before writing an algorithm, we need a set of 5 letter words to choose the next guess from. Luckily, I found the set of words Wordle uses by viewing their source code:Next, I wanted to write an algorithm that would take the current game state, and return a suggestion for a next guess from the list of words. The current game state can be represented as:An example game state could be:Once I’ve defined the game state types, I want to write a makeNextGuess method which:Here’s the method:Now that I have a method to provide the next guess, I want to write a game runner that:Here’s an example of the expected output for firstGuess: 'share'and answer: 'hello'Here’s the method:Now that we have an algorithm that auto-runs the game given a firstGuess and an answer, I can use this to write a findBestFirstWord method that:Once we have the average number of guesses for each word, find the word with the lowest average.Here’s the algorithm:From running this algorithm, the output best Wordle first word was…. crate!It’s worth noting that there are many ways this algorithm can be improved. Different articles provide different strategies for finding the best first word, such as calculating the most common letters and their frequencies.Thanks for reading!",,1,2,5,7,839,356,4,5,0,2,71
"A Deep Dive into Wordle, the New Pandemic Puzzle Craze",Towards Data Science,https://towardsdatascience.com/a-deep-dive-into-wordle-the-new-pandemic-puzzle-craze-9732d97bf723,Sejal Dua,455,12,2254,"Wordle has reignited group chats, enlivened everyone’s competitive spirits, and inundated Twitter with cryptic emoji grids since it achieved viral status in January of 2022. In a tweet posted by content creator Emily Coleman, it is compared to the object of the baking obsession that infiltrated the kitchens of many back at the onset of the COVID-19 pandemic: sourdough starter.Echoing a similar sentiment, Django Gold, a New York-based comedian, tweeted that doing the puzzle at midnight is a crucial aspect of his daily existential dread, which, somehow not surprisingly, resonated with north of 24,000 Twitter users.Now, before I take you on an incredibly nerdy journey, let’s answer the million dollar question for those who are not familiar: what the heck is a Wordle?Wordle is a simple and freely accessible word-guessing game created by a Brooklyn-based software engineer named Josh Wardle. He designed it for his partner who enjoys playing word games. The rules of the game are simple. Players have a total of 6 guesses to guess a five-letter word by entering characters into boxes. The game then uses color-coding to indicate to the player if any of their letter selections are in the correct spot (green), present in the word but in the wrong position (yellow), or absent from the word entirely (grey). Some examples of this feedback system are provided below:If you’re wondering why the premise of this game seems so familiar to you, you may be thinking of the game Mastermind, which shares the same objective as Wordle, except the game tokens are colored beads instead of letters. I would argue that Wordle is harder than Mastermind because there is the added constraint that the words must be valid English words, so there is a Boggle-like element that gets intertwined with the logic puzzle essence of the game. Wordle is pretty simple, yet strangely addictive. While it is ultimately impressive for anyone to create an international internet sensation, what is most fascinating about it is the “why” behind what made it this big of a sensation.www.nytimes.comIn the New York Times article linked above, you can read more about Wardle himself and the origin story of the game. While reading the article, one question was at the forefront of my mind: why did this game, a game that has existed in various forms for a few decades now, go arguably more viral than the COVID-19 infection (okay… bad comparison, sorry) in the span of a few short weeks?The answer: scarcity and shareability! Wardle cracked the code on how to make a puzzle game thrive in the attention economy. He makes people experience an addictive relationship with the game, but when it came to thinking of an addictive gamification model, simply depriving the user of the fuel to their addiction, he found, makes them remain engaged. This case study at the intersection of business, technology, and gaming was impressively demonstrated earlier by the New York Times’ Spelling Bee game. The scarcity model, I imagine, also has the added benefit of not capitalizing on users’ already unhealthy relationships with technology.Given that addictive tendencies don’t justify the exponential curve of users hopping onto the Wordle wagon, the next candidate then becomes the shareability of the game, which Wardle also comments on in the NYT article. As you can see in the example above, in just one line of text and a simple 5x6 grid of 30 total emojis, Wardle created a simple yet satisfying way to create a sense of connection among users who have made a daily habit out of the puzzle game.Anyways, enough geeking out about Wordle as a concept, now let’s figure out how to strategically hack the game!If you want to skip all the nerdy analysis I am about to dive into and just play with the web application I built, you can play with it here. (Full Link: https://share.streamlit.io/sejaldua/wordle-analysis/main/app.py)For us word game aficionados, we are all familiar with the Wheel of Fortune Bonus Round, during which the contestant who is in control of the wheel spins it one last time and gets to guess 3 consonants and one vowel… that is, in addition to the 6 most common letters in the English language: R, S, T, L, N, E. I figured we could start off by verifying if those are, in fact, the most common letters. Does the Wheel of Fortune’s data-driven research hold true when we limit our scope to only 5-letter English words?As it turns out, the age old “R, S, T, L, N, E” does not hold up when looking at 5-letter words. It is actualy more like “E, A, R, O, T, L”. When I first discovered this, my oversimplified takeaway was… hmm, maybe I should start with “ORATE” as my first guess tomorrow? After a bit more digging, I learned that such a strategy would not be very optimal because position matters in the game of Wordle.The heatmap above is intended to be read from top to bottom, paying attention to the darkest tiles in each row to get a sense of where letters are frequently distributed with respect to position within the 5-letter word. One interesting finding is that, despite “X” not being a letter in most 5-letter words that might be floating around in your mind, it is actually one of the most common last-position letters. As for a more actionable insight that might help us with game strategy, if we visually traverse this heatmap, the guess that keeps that would result in the most most correct (green) tiles would be something like ‘SAAEE’… but that is obviously not very useful because it is neither a word nor does it give us five unique pieces of information. It is time to level up this analysis by running some simulations!The ideal first guess is a heavily debated subject amonst Wordlers. To give it proper due diligence, I took a little survey of close friends and Reddit-ers and asked them to share some of their go-to first guesses. Some answers I received are as follows: ARISE, ROUTE, AROSE, CHEAT, STEAM, RENTS, ABODE, ADIEU, POISE, BIOME, RAISE, YEARS, NOISE.Observed strategies from those that offered commentary to go along with their answer(s):From the above heatmap, we should bear in mind that STEAK versus STAKE, ABODE versus ADOBE, ARISE versus RAISE are all unique strategies and must therefore be assessed accordingly. While each anagram pair, in theory, tests out the same set of letters, the position matters. Hopefully this article (and the web tool that I will discuss later) helps you think critically about the strategic efficacy of a given word versus its anagram.The data that has enabled the following deep dive was compiled by Zach Wissner-Gross, author for fivethirtyeight. He has accessed and alphabetically listed “Wordle’s library of 2,315 mystery words as well as all 12,972 words you are allowed to guess.” So let’s programmatically determine the best initial guess, shall we?For a basic analysis of which 5-letter words result in the most greens and yellows, on average, across all 2,315 Wordle solutions, let’s write some code to get green (correct), yellow (present), and grey (absent) scoring heuristics.The top five best first guess strategies, optimizing for average number of green tiles that you might obtain if you made that guess, are listed below.You may be thinking to yourself, I have never used these words in conversation before or I have no idea what these words mean! However, referencing the earlier heatmap, it certainly makes sense that almost all of these words follow a pattern of starting with an ‘S’, followed by an ‘A’ or ‘O’, and then ending with an ‘E’. This is a great way to maximize coverage amongst 5-letter words and to quickly gain valuable information. The challenge is that many of these words consist of duplicated letters, which, as we discussed before, do not deliver the maximum amount of information. As a result, I think I would want to try out the word “SOARE”, which means “a young hawk”, if you’re curious.When sorting by the weighted average of average correct tiles and average present tiles, we see that, interestingly, all five top initial guesses are comprised of the following set of letters: {‘S’, ‘E’, ‘A’, ‘R’} . Just by looking at this list, I can call to mind numerous 5-letter words which utilize these letters, but, again, we want to refrain from going with a word with duplicates, so let’s filter them out. Nerdy note: this can be done in just 3 lines of Python code!That’s more like it. These are all great candidates to try next time you solve your daily Wordle!Fair warning: if you’re not a nerd, skip past this section. I am going to get into some technically more abstract terminology and lay out some strategies out on the table. While reading, your task is to consider these optimization techniques and, without me showing you any data, think to yourself which one you might prefer.According to my findings (NOTE: I’ll document the experimental analysis in a Jupyter notebook for all you nerds), which took 16 hours to simulate, the best guesses for each strategy are as follows:Max-size Prioritization: RAISEMax-entropy Prioritization: SOAREMax-splits Prioritization: TRACEI alternate between RAISE, SOARE, and ADIEU — and have had some success* with these go-to first guesses (* = undefeated). But my real secret is actually a not-so data-driven approach that I like to call “the 1–2 punch”:Step 1: use a strategic, statistically successful first guess that is loaded with vowels and common consonants.Step 2: Even if you yielded some green and yellow tiles, ignore them. Think of a word with 5 entirely new letters . Ideally use up all of the remaining vowels and other common consonants.For an example: I might guess “RAISE” initially, followed by “MOUNT”. This usually helps me to exhaust all of the vowels and squeeze as most information as possible from these two guesses. This way, I can proceed onward by just calling to mind words that abide by the 10+ criteria of the logic puzzle that I have just acquired.That’s just my personal strategy! I’d love to hear yours. Please leave a comment at the end of this post if you’re willing to share!I took this analysis further to better understand what makes a word relatively easier or harder to guess by applying the top 20 unique best guesses, as computed by the “max-size”, “max-entropy”, “and max-splits” strategies, and and calculated how many guesses it took, on average, to arrive at the correct answer. We see that the top five easiest guesses take less than 3 guesses and are characterized by having duplicate letters, with most of the letters being generally frequent letters in the English language.The top hardest guesses are “USAGE”, “LAYER”, “WATCH”, “VAUNT”, and “HOMER”. Even when using an AI-based strategy, it takes the three aforementioned algorithms nearly 10 guesses before arriving at the correct answer. I speculate that the reason for this is because these words tend to have many “lookalike” words, or words where all letters are the same, but one letter is substituted. For instance, “WATCH” is quite similar to “WITCH”, “LATCH”, “SATCH”, “BATCH”, “CATCH”, etc. In the later stages of the game, an AI would not be able to prioritize which of these to guess next, since each of them essentially provide the same amount of information. However, if we augmented the data to rank each potential guess by some kind of popularity score based on frequency of use in the English language, we may see better late-stage efficiency for these tougher Wordle strings.Just for fun, I made a web application using my favorite Python package. I wanted to enable Wordle’s most addicted members to do more than just share their emoji-formatted results with their friends. What if you could analyze how effective your guesses were? With this tool, can do exactly that!Or better yet, what if you guess “RAISE” as your first guess and you obtain colored tiles on the game board, but you don’t know what to guess next? The analysis that I just walked you through should be able to deliver some informative insight to you to help you make the second decision. Allow me to show you…In “Wordle Assist” mode, you can also try your hand at a past Wordle from the archives, random Wordles, or even manual Wordles if you want to master your strategy on a test example!At the end of the day, I am not going to claim that this game can or should be cracked with code — that would simply take the fun out of it. But I did enjoy the experience of learning new 5-letter words, as well as learning how to come up with a smarter strategy in order to get the most informative value out of my sequence of guesses. I hope this project only adds to the fun of the game! I truly believe that there is no objective “optimal” strategy to this game — that is what keeps the users hooked and, for data people like us, it is what makes this such an interesting problem to investigate.It can even be entertaining to just congratulate yourself for all of the random guesses that struck gold for you. The other day, my mom guessed the Wordle solution “WINCE”, seemingly out of nowhere, with 28 viable guesses still on the table.“There is no AI that compares to being a baller”- my momWeb Application: https://share.streamlit.io/sejaldua/wordle-analysis/main/app.pyCode Repository:github.comIf you enjoyed this article, feel free to check out my other portfolio work and/or connect with me on LinkedIn!",,4,1,13,15,908,629,13,2,0,17,246
Forget Luck: Optimized Wordle Strategy,Better Programming,https://betterprogramming.pub/forget-luck-optimized-wordle-strategy-using-bigquery-c676771e316f,Javier Frias,13,13,2370,"Wordle is a surprisingly addictive word game by Josh Wardle that has taken the Internet by storm. It has sparked many copycats, several spoofs, and playing in other languages is definitely possible: spanish, german, portuguese, welsh… Klingon anybody?Fun fact: the acronym of this article’s title is a nice word: FLOWS. Too bad it can’t solve the game. Keep reading to know why.After playing it myself for a while (read: getting addicted) and having studied multiple articles on what’s the best strategy to play or what‘s the best word to use as the first move, I decided to investigate to see if I could come up with something of my own.I’ve seen people using general programming languages (JavaScript, Python, C++…) or spreadsheets (Google Sheets, Excel…) to elaborate on Wordle, so I thought I’d try something different: I’m going to use a database. I will go into full serverless mode and leverage Google BigQuery to do the heavy lifting, Google Cloud Shell to do all the command line stuff, and Google Data Studio for visualizations.Disclaimer: this strategy might ruin your Wordle experience. Please be warned.As a general rule of thumb, once a 🟨 or a 🟩 is revealed, it’s a good idea not to repeat that letter and instead try new letters to explore as much as possible. This is especially true for 🟩’s. Reusing 🟩’s results, as expected, in the same 🟩 in the same positions: nothing new has been learnt.The amount of new information (called entropy in information theory) is maximized using different letters in each turn.Two players solving the same Wordle:Therefore, it’s better to use words with as many different letters as possible, ideally all 5 of them, to maximize the new information obtained per turn.Following this idea, 4 words ranked from “best” (more entropy) to “worst” (least entropy):But there are multiple 5 letter words with unique letters: BEGIN, BREAK, FALSE, FETCH, OUTER, RAISE, RANGE, RIGHT, TABLE, UNTIL, USING, WHILE…Is it possible to know if one word is “better” than another? Let’s find out!Wordle uses two different groups of words:In order to find the solution and guess words I turned to the game’s TypeScript source code in GitHub:Extracting and saving them to a CSV file is easy:The output is:Wordle’s lexicon comprises 12,972 words that you can use to play the game: 2,315 solutions and 10,657 guesses.I will create a new Google BigQuery dataset called wordle to keep things tidy. Then I’ll load those words into a table (raw_words) to start exploring.Let me quickly check that all data was correctly uploaded to Google BigQuery:Since Wordle uses all caps and the extracted words are lowercase, I’ll quickly create a new table (normalized_words) with all words in uppercase to make things look pretty. I’ll keep the original word because storage is cheap in Google BigQuery:The next step is to create a lookup table (letters) with all the different letters present in all the words (both solution and guess).I’ll precalculate a number (letter_bitmask) for each letter. This will make my life easier down the road.This table will look something like this:As you can see in the image above, the original Wordle uses 26 letters so think of the letter bitmask as a sequence of 25 0's and exactly one 1 that moves around.If I consider those 0s and 1 as a binary number and I convert them to decimal I get the letter bitmask:At this point, I can start analyzing Wordle’s lexicon. I’ll calculate a few things first:Let me briefly explain how the word bitmask is calculated.I start separating the 5 letters that make up a word, then I remove any duplicates (GROUP BY letter does this for me) and finally I add the individual letter masks of the letters that are left. Some examples:Note that FACED and DECAF have the same word bitmask because they both use the same letters, albeit in different order. These are the only two words with A, C, D, E, and F in Wordle’s lexicon:Fun fact: Wordle lets you play with 14 words that can be formed combining and repeating the letters A, D, E, and R. How many do you know? (Full list at the end)Calculating how many unique letters a word is using is very easy: I just need to count how many 1’s there are in the word bitmask.Google BigQuery has the BIT_COUNT(expression) function that will do exactly what the name says: count how many 1’s are present in expresion .Let’s take some word bitmasks, their binary equivalents and count the 1's:Let’s see how many solution words there are taking into account the number of unique letters:This information may be useful when you only have one turn left and you can’t decide whether you’re going to go with DOING or GOING as your final word.For every 3 words, odds are that 2 will have 5 unique letters and 1 will have 4. Therefore, the most probable answer is DOING: the word with 5 unique letters.Maybe it’s just me, but the fewer unique letters a word has, the harder I find it to solve.Fun fact: there is only 1 solution that uses 2 different letters. Don’t worry, I won’t spoil the surprise.Now I’m going to borrow a trick from cryptanalysis and I’ll perform a quick letter frequency analysis to determine how letters are used in Wordle’s lexicon, but only on solution words:The most frequent letters in Wordle solutions are E, A, R, O, and T, so a good starting word should be one that has all those letters.The least frequent letters are Z, X, Q, and J. Most probably, you won’t come across words that use them.Fun fact: out of 2,315 solutions, only 27 words use the letter J (the least used), that’s 2 orders of magnitude smaller. So yeah, pretty rare.Finding all words that have the top 5 letters exactly once is very easy leveraging the word’s bitmask:Sadly, none of those words is a solution, so even if they’re optimized guesses, it won’t be possible solve Wordle on the first turn. ☝They’re good starting words and all three have the same probability of unveiling 🟨’s on your first turn.But… can we do better? 🤔Definitely!Getting 🟨’s is very good, but getting 🟩’s is even better! Do any of these words have more probability of unveiling 🟩’s?To answer this question, I need to extend letter frequency analysis to the position of each letter in solution words:Getting back to ORATE, OATER, and ROATE, let’s find out the frequency of each letter at each position:The O is not very popular as a first letter, so it seems ORATE is not a good pick. However, A is the top performing third letter and E as fifth letter does twice as good as the R in that position.In order to understand how “good” a word is at unveiling 🟩, I need to come up with some formula that takes into account all these frequencies and outputs a number that I can use to compare two words’ efficiency in an easy manner.Let me define some things.First, the Positional Letter Weight (PLW for short) is the number of times a letter is present in a certain position. I will denote it as:So, PLW(O, 1) = 41 because O appears 41 times as the first letter. Similarly, PLW(E, 5) = 424 because E appears 424 times as the fifth letter.Next, I’m going to define the Word Weight (WW for short) as the sum of each positional letter weight. WW can easily be defined in terms of PLW as:Theoretically, the higher WW is, the more probable a word will unveil one or more 🟩. Getting back to the candidate words:Looks like ROATE is the “heaviest” word and therefore it has more chances of unveiling 🟩’s.But… can we do better? 🤔I think so!ROATE was selected purely based on the fact that those letters were the most used on solution words in any position.Now that I have the positional letter frequency for all letters, let me calculate the weight for all 12,972 words and check if ROATE is indeed the best starting word:I would discard SAREE because it only has 4 unique letters (remember, I want to maximize entropy using 5 different letters) and I’d favor SLATE over SAINE purely because their weight is very similar and SLATE may grant me a satisfying instant win! 🎯In case you’re wondering, ROATE ranks #160 🤦🏻‍♂ in the list of weighted words with 5 unique letters. It doesn’t look that good if we forget about overall letter frequency and instead we consider positional letter frequency and word weight.Fun fact: the “worst” starting word you can use is “IMSHI” (australian military slang: go away; be off). Not only does it have the lowest weight (191), it’s a guess word so it’ll never be the solution to the daily puzzle. This is like playing 2–7 offsuit in Texas Hold’em… you really need to be the Doyle Brunson of Wordle to pull it off. ♥♠♦♣Great! We have our optimized first guess: SLATE. Hopefully Wordle reveals some 🟨’s and 🟩’s.But… can we do better? 🤔Totally!With the first turn already defined. Wordle unveiled some ⬛’s, 🟨’s, and 🟩’s. Now what?Optimizing the first word to use is great. But it’s unlikely (1 in 2,370) I’m going to solve Wordle in 1 move. I need a strategy. And the simpler it is, the better: I don’t want to remember complex decision trees, tables, percentages or an endless list of words.Using the first move to test the letters (S,L,A,T, and E) allowed me to cover 37.40%. This is a bit less than 39.69% that the top 5 letters (see rectangle ① in the image below) cover.What if I use two moves to test the top 10 letters (E, A, R, O, T, L, I, S, N, and C)? I would cover 66.57% (rectangle ② in the image below) and I’d still have 4 turns left. Neat.Following that logic, using three moves to test the top 15 letters (E, A, R, O, T, L, I, S, N, C, U, Y, D, H, and P) jumps the coverage to 84.20% (rectangle ③ in the image below) and I’d be left with 3 turns to complete the game. I like those odds. 🎲Extending this logic to test the top 20 letters (E, A, R, O, T, L, I, S, N, C, U, Y, D, H, P, M, B, G, F, and K) is not very interesting since there are diminishing returns when investing an extra move: the coverage only jumps to 95.84% (rectangle ④ in the image below) but I’m left with only 2 turns to complete the game. Extremely risky.I’m going to settle with 3 turns. This means I will be testing the top 15 letters. Let’s find out how many words with 5 unique letters contain those top 15 letters. Once again, putting BIT_COUNT() to good use:Google BigQuery easily churns the data and reveals that out of 12,972 words there are 2,760 that contain 5 unique letters from the top 15 letters: 527 solutions and 2,233 guesses. Good.The next step is to check all the permutations of 3 words (triplets) that use the top 15 letters exactly once. For example: (ADULT, HYPER, SONIC). Calculating and checking all the permutations can be tricky since potentially Google BigQuery has to explore:That’s a “large” number but definitely nothing that Google BigQuery can’t handle.Leveraging once again the BIT_COUNT() function, performing some bitwise operations and using some creative pruning, it takes less than 4 seconds to come up with the answer: 299,544 triplets.Any of these triplets is a good choice for the first three tries:But… can we do better? 🤔Of course!There are some things that can still be done to optimize even further.Firstly, I can discard all triplets that have guess words and keep only those with 3 solution words. This narrows down from 299,544 to only 2,730 triplets.Secondly, I can pick the triplets with the highest total weight since this will give me a greater chance of revealing 🟩’s as early as possible. Turns out there are only 6 triplets with a combined weight of 3,505 formed by the permutation of these 3 words: CURLY, POINT, and SHADE.The best triplet overall (COUDE, PRINT, SHALY) has a combined weight of 3,588 and the best triplet formed only by solution words (CURLY, POINT, SHADE) is 3,505. Their weights are so similar that it’s worth discarding triplets with guess words and going for the hole in one! ⛳Thirdly, to reveal as much information as soon as possible, those words can be played in descending weight order:There you have it!Play SHADE as your first move.Most probably, you will need more information: play POINT on your second move.Chances are that you still need more 🟨’s and 🟩’s. Use CURLY on your third move.After the first 3 moves, it should be just a matter of using the 🟩’s and rearranging the 🟨’s to solve the word in the fourth turn.If I don’t have enough information to solve in the fourth move, I will need to explore more letters. I have 3 specific words I like to use in turn 4 involving the letters G, M, and B (the next 3 most popular letters) and any 🟨 vowels from the first 3 moves to try to turn them into 🟩’s. Any of this words jumps coverage to 92.04%.Similarly, there are 5 words for the fifth move involving two of F, K, W, V, and any 🟨 vowels from past turns to cover up to 95.84%.I won’t go into much more detail because I’m sure I’ve ruined the game for you too much by now… 🤦🏻‍♂But… can we do better? 🤔I’m pretty sure!I don’t think this strategy is optimal as it’s most probably not solving in the fewest moves. Instead, it’s easy to remember and, most of the time, solves the game🤞.Think of it as something similar to a beginner’s method (for example layer by layer) for solving the Rubik’s cube: very few short algorithms to remember but very slow compared to the Friedrich method used in speedcubing.Granted, this strategy is just a routine for the first 3 moves but turns Wordle into a logic puzzle in turn 6, rather than luck all along.Happy Wordling!Strategy in motion:Fun fact solved: The 14 words that use A, D, E, and R are:",,2,10,63,0,564,458,26,4,0,21,60
Forget Luck: Optimized Wordle Strategy,Better Programming,https://betterprogramming.pub/forget-luck-optimized-wordle-strategy-using-bigquery-c676771e316f,Javier Frias,13,13,2370,"Wordle is a surprisingly addictive word game by Josh Wardle that has taken the Internet by storm. It has sparked many copycats, several spoofs, and playing in other languages is definitely possible: spanish, german, portuguese, welsh… Klingon anybody?Fun fact: the acronym of this article’s title is a nice word: FLOWS. Too bad it can’t solve the game. Keep reading to know why.After playing it myself for a while (read: getting addicted) and having studied multiple articles on what’s the best strategy to play or what‘s the best word to use as the first move, I decided to investigate to see if I could come up with something of my own.I’ve seen people using general programming languages (JavaScript, Python, C++…) or spreadsheets (Google Sheets, Excel…) to elaborate on Wordle, so I thought I’d try something different: I’m going to use a database. I will go into full serverless mode and leverage Google BigQuery to do the heavy lifting, Google Cloud Shell to do all the command line stuff, and Google Data Studio for visualizations.Disclaimer: this strategy might ruin your Wordle experience. Please be warned.As a general rule of thumb, once a 🟨 or a 🟩 is revealed, it’s a good idea not to repeat that letter and instead try new letters to explore as much as possible. This is especially true for 🟩’s. Reusing 🟩’s results, as expected, in the same 🟩 in the same positions: nothing new has been learnt.The amount of new information (called entropy in information theory) is maximized using different letters in each turn.Two players solving the same Wordle:Therefore, it’s better to use words with as many different letters as possible, ideally all 5 of them, to maximize the new information obtained per turn.Following this idea, 4 words ranked from “best” (more entropy) to “worst” (least entropy):But there are multiple 5 letter words with unique letters: BEGIN, BREAK, FALSE, FETCH, OUTER, RAISE, RANGE, RIGHT, TABLE, UNTIL, USING, WHILE…Is it possible to know if one word is “better” than another? Let’s find out!Wordle uses two different groups of words:In order to find the solution and guess words I turned to the game’s TypeScript source code in GitHub:Extracting and saving them to a CSV file is easy:The output is:Wordle’s lexicon comprises 12,972 words that you can use to play the game: 2,315 solutions and 10,657 guesses.I will create a new Google BigQuery dataset called wordle to keep things tidy. Then I’ll load those words into a table (raw_words) to start exploring.Let me quickly check that all data was correctly uploaded to Google BigQuery:Since Wordle uses all caps and the extracted words are lowercase, I’ll quickly create a new table (normalized_words) with all words in uppercase to make things look pretty. I’ll keep the original word because storage is cheap in Google BigQuery:The next step is to create a lookup table (letters) with all the different letters present in all the words (both solution and guess).I’ll precalculate a number (letter_bitmask) for each letter. This will make my life easier down the road.This table will look something like this:As you can see in the image above, the original Wordle uses 26 letters so think of the letter bitmask as a sequence of 25 0's and exactly one 1 that moves around.If I consider those 0s and 1 as a binary number and I convert them to decimal I get the letter bitmask:At this point, I can start analyzing Wordle’s lexicon. I’ll calculate a few things first:Let me briefly explain how the word bitmask is calculated.I start separating the 5 letters that make up a word, then I remove any duplicates (GROUP BY letter does this for me) and finally I add the individual letter masks of the letters that are left. Some examples:Note that FACED and DECAF have the same word bitmask because they both use the same letters, albeit in different order. These are the only two words with A, C, D, E, and F in Wordle’s lexicon:Fun fact: Wordle lets you play with 14 words that can be formed combining and repeating the letters A, D, E, and R. How many do you know? (Full list at the end)Calculating how many unique letters a word is using is very easy: I just need to count how many 1’s there are in the word bitmask.Google BigQuery has the BIT_COUNT(expression) function that will do exactly what the name says: count how many 1’s are present in expresion .Let’s take some word bitmasks, their binary equivalents and count the 1's:Let’s see how many solution words there are taking into account the number of unique letters:This information may be useful when you only have one turn left and you can’t decide whether you’re going to go with DOING or GOING as your final word.For every 3 words, odds are that 2 will have 5 unique letters and 1 will have 4. Therefore, the most probable answer is DOING: the word with 5 unique letters.Maybe it’s just me, but the fewer unique letters a word has, the harder I find it to solve.Fun fact: there is only 1 solution that uses 2 different letters. Don’t worry, I won’t spoil the surprise.Now I’m going to borrow a trick from cryptanalysis and I’ll perform a quick letter frequency analysis to determine how letters are used in Wordle’s lexicon, but only on solution words:The most frequent letters in Wordle solutions are E, A, R, O, and T, so a good starting word should be one that has all those letters.The least frequent letters are Z, X, Q, and J. Most probably, you won’t come across words that use them.Fun fact: out of 2,315 solutions, only 27 words use the letter J (the least used), that’s 2 orders of magnitude smaller. So yeah, pretty rare.Finding all words that have the top 5 letters exactly once is very easy leveraging the word’s bitmask:Sadly, none of those words is a solution, so even if they’re optimized guesses, it won’t be possible solve Wordle on the first turn. ☝They’re good starting words and all three have the same probability of unveiling 🟨’s on your first turn.But… can we do better? 🤔Definitely!Getting 🟨’s is very good, but getting 🟩’s is even better! Do any of these words have more probability of unveiling 🟩’s?To answer this question, I need to extend letter frequency analysis to the position of each letter in solution words:Getting back to ORATE, OATER, and ROATE, let’s find out the frequency of each letter at each position:The O is not very popular as a first letter, so it seems ORATE is not a good pick. However, A is the top performing third letter and E as fifth letter does twice as good as the R in that position.In order to understand how “good” a word is at unveiling 🟩, I need to come up with some formula that takes into account all these frequencies and outputs a number that I can use to compare two words’ efficiency in an easy manner.Let me define some things.First, the Positional Letter Weight (PLW for short) is the number of times a letter is present in a certain position. I will denote it as:So, PLW(O, 1) = 41 because O appears 41 times as the first letter. Similarly, PLW(E, 5) = 424 because E appears 424 times as the fifth letter.Next, I’m going to define the Word Weight (WW for short) as the sum of each positional letter weight. WW can easily be defined in terms of PLW as:Theoretically, the higher WW is, the more probable a word will unveil one or more 🟩. Getting back to the candidate words:Looks like ROATE is the “heaviest” word and therefore it has more chances of unveiling 🟩’s.But… can we do better? 🤔I think so!ROATE was selected purely based on the fact that those letters were the most used on solution words in any position.Now that I have the positional letter frequency for all letters, let me calculate the weight for all 12,972 words and check if ROATE is indeed the best starting word:I would discard SAREE because it only has 4 unique letters (remember, I want to maximize entropy using 5 different letters) and I’d favor SLATE over SAINE purely because their weight is very similar and SLATE may grant me a satisfying instant win! 🎯In case you’re wondering, ROATE ranks #160 🤦🏻‍♂ in the list of weighted words with 5 unique letters. It doesn’t look that good if we forget about overall letter frequency and instead we consider positional letter frequency and word weight.Fun fact: the “worst” starting word you can use is “IMSHI” (australian military slang: go away; be off). Not only does it have the lowest weight (191), it’s a guess word so it’ll never be the solution to the daily puzzle. This is like playing 2–7 offsuit in Texas Hold’em… you really need to be the Doyle Brunson of Wordle to pull it off. ♥♠♦♣Great! We have our optimized first guess: SLATE. Hopefully Wordle reveals some 🟨’s and 🟩’s.But… can we do better? 🤔Totally!With the first turn already defined. Wordle unveiled some ⬛’s, 🟨’s, and 🟩’s. Now what?Optimizing the first word to use is great. But it’s unlikely (1 in 2,370) I’m going to solve Wordle in 1 move. I need a strategy. And the simpler it is, the better: I don’t want to remember complex decision trees, tables, percentages or an endless list of words.Using the first move to test the letters (S,L,A,T, and E) allowed me to cover 37.40%. This is a bit less than 39.69% that the top 5 letters (see rectangle ① in the image below) cover.What if I use two moves to test the top 10 letters (E, A, R, O, T, L, I, S, N, and C)? I would cover 66.57% (rectangle ② in the image below) and I’d still have 4 turns left. Neat.Following that logic, using three moves to test the top 15 letters (E, A, R, O, T, L, I, S, N, C, U, Y, D, H, and P) jumps the coverage to 84.20% (rectangle ③ in the image below) and I’d be left with 3 turns to complete the game. I like those odds. 🎲Extending this logic to test the top 20 letters (E, A, R, O, T, L, I, S, N, C, U, Y, D, H, P, M, B, G, F, and K) is not very interesting since there are diminishing returns when investing an extra move: the coverage only jumps to 95.84% (rectangle ④ in the image below) but I’m left with only 2 turns to complete the game. Extremely risky.I’m going to settle with 3 turns. This means I will be testing the top 15 letters. Let’s find out how many words with 5 unique letters contain those top 15 letters. Once again, putting BIT_COUNT() to good use:Google BigQuery easily churns the data and reveals that out of 12,972 words there are 2,760 that contain 5 unique letters from the top 15 letters: 527 solutions and 2,233 guesses. Good.The next step is to check all the permutations of 3 words (triplets) that use the top 15 letters exactly once. For example: (ADULT, HYPER, SONIC). Calculating and checking all the permutations can be tricky since potentially Google BigQuery has to explore:That’s a “large” number but definitely nothing that Google BigQuery can’t handle.Leveraging once again the BIT_COUNT() function, performing some bitwise operations and using some creative pruning, it takes less than 4 seconds to come up with the answer: 299,544 triplets.Any of these triplets is a good choice for the first three tries:But… can we do better? 🤔Of course!There are some things that can still be done to optimize even further.Firstly, I can discard all triplets that have guess words and keep only those with 3 solution words. This narrows down from 299,544 to only 2,730 triplets.Secondly, I can pick the triplets with the highest total weight since this will give me a greater chance of revealing 🟩’s as early as possible. Turns out there are only 6 triplets with a combined weight of 3,505 formed by the permutation of these 3 words: CURLY, POINT, and SHADE.The best triplet overall (COUDE, PRINT, SHALY) has a combined weight of 3,588 and the best triplet formed only by solution words (CURLY, POINT, SHADE) is 3,505. Their weights are so similar that it’s worth discarding triplets with guess words and going for the hole in one! ⛳Thirdly, to reveal as much information as soon as possible, those words can be played in descending weight order:There you have it!Play SHADE as your first move.Most probably, you will need more information: play POINT on your second move.Chances are that you still need more 🟨’s and 🟩’s. Use CURLY on your third move.After the first 3 moves, it should be just a matter of using the 🟩’s and rearranging the 🟨’s to solve the word in the fourth turn.If I don’t have enough information to solve in the fourth move, I will need to explore more letters. I have 3 specific words I like to use in turn 4 involving the letters G, M, and B (the next 3 most popular letters) and any 🟨 vowels from the first 3 moves to try to turn them into 🟩’s. Any of this words jumps coverage to 92.04%.Similarly, there are 5 words for the fifth move involving two of F, K, W, V, and any 🟨 vowels from past turns to cover up to 95.84%.I won’t go into much more detail because I’m sure I’ve ruined the game for you too much by now… 🤦🏻‍♂But… can we do better? 🤔I’m pretty sure!I don’t think this strategy is optimal as it’s most probably not solving in the fewest moves. Instead, it’s easy to remember and, most of the time, solves the game🤞.Think of it as something similar to a beginner’s method (for example layer by layer) for solving the Rubik’s cube: very few short algorithms to remember but very slow compared to the Friedrich method used in speedcubing.Granted, this strategy is just a routine for the first 3 moves but turns Wordle into a logic puzzle in turn 6, rather than luck all along.Happy Wordling!Strategy in motion:Fun fact solved: The 14 words that use A, D, E, and R are:",,2,10,63,0,,,26,4,0,21,60
Design a Wordle Python Helper,Better Programming,https://betterprogramming.pub/python-wordle-helper-b81aa05680aa,John Clark Craig,965,6,898,"Wordle virally stormed onto the Internet almost overnight. It’s fun, doesn’t hook you into playing for hours (there’s only one game per day), and it makes you think. I like it!In a nutshell, you guess 5-letter words, and the program color-codes the results to let you know how close you are to finding the right word.A dark-gray background marks letters that don’t appear anywhere in the target word.Yellow means the letter is in the word, but in the wrong place, and green means the letter is exactly in the right spot.You get six tries, and if you’re careful, and have a whole bunch of 5-letter words in your head, you can almost always find the solution!I also like fun Python programming challenges, so of course, I decided to create a program to help find words that work. I created whelper.py to sort through a long list of 5-letter words to find all that can still legally work for the puzzle after you enter your word guesses and get the color-coded results for each guess.Here’s the complete code listing for whelper.py:To use this program, load it into your favorite IDE or editor (IDLE works great).Rather than bloating the code by processing lots of input lines, I decided to keep it simple by letting you edit the guesses and results directly in the code.Take a look at the first three code lines. Set the first variable to True to tell the program to use the bigger list of words instead of the smaller one, although in most cases the smaller list works well.The next two lines are where you enter your guesses and the results that the Wordle site provides for each of these guesses. Separate the word guesses with a space, extending the string as shown to include up to 5 guesses.Directly below each word edit the hits_misses string to add the results of each guess. Enter a period for gray-background letters, a y for yellow, and g for green. Run the program and a list of words that can still be tried is output to your screen. If the list of possible words is long, only the first 100 are output, but that should give you a lot of choices to try next.The various “paragraphs” of code above process the guesses and the scoring results to eliminate all words from the list that won’t work.For example, none of the dark-gray letters are in the target word, so eliminating all words that have any of these letters shortens the list in a hurry. Processing the yellow and green “hits” also thins the list efficiently.I used several rather unique, but powerful Python functions to help sift through the list of words.If zip(), enumerate(), or any other functions look mysterious to you, I suggest using Google to search for explanations. This is a great way to enhance your knowledge of Python, by exploring working code to decipher how it works.I did run into one coding challenge worth explaining here. When a word in the list was to be eliminated, the remove() function made it easy to scratch each word off the list.I ran into a problem though, because removing items from the list messed up the iteration through the rest of the shortened list. The word following each eliminated word was skipped! A simple solution to this complication was to iterate through the list from last to first, instead of the usual first to last. Look for the reversed() function in several places, causing the words list to be checked in the reverse order.Here’s a simplified example demonstrating the problem, and its solution. Notice that the second word two is removed only when the words list is searched in reverse order. The code that is commented out shows the problem that happens when the list is not reversed.With some data scraping and processing (using Python of course), I ended up with two lists of 5-letter words.The shorter list contains about 2500 fairly common words, and the bigger list contains many more words, many of which are not so well known. I suggest sticking to the shorter list and moving to the bigger list only if required.For example, the shorter list did not have the word “whack” in it (it does now), so I resorted to the bigger list to help solve the January 26, 2022 Wordle game.I started to include these word lists here in this article, but they really are fairly long. Instead, you can easily download them from these two web pages I built…word_list and word_list_bigger.Each of these lists should be copied into its own text file, the first named word_list.txt and the second one named word_list_bigger.txt. Place these files in the same folder as the program.Using the shorter list, the guesses and results edited into the code above sifted the list down to just the word “whack”, whereas the longer list of words provided two legal choices, “kyack” and “whack”. I have no idea what the first word means so choosing the second one as the more probable solution was a no-brainer. If no words at all show up after processing your guesses, try again with the bigger list.Playing Wordle is good for your brain. And in my humble opinion, programming in Python to help you along is just as good for you, and just as fun. It just provides a different sense of satisfaction and stimulation.Have fun!",,1,3,1,1,872,632,5,0,0,14,106
"Why You’re Likely To Solve Wordle (according to 100,000 Simulations)",Better Programming,https://betterprogramming.pub/why-youre-likely-to-solve-wordle-according-to-100-000-simulations-7725b462e533,Daniel Tompkins,63,5,492,"One day I started noticing colored squares showing up on my social media timelines. Like many people, I was curious what “wordle” was and quickly fell into the routine of completing the daily “wordle” every morning. They are even considerate enough to make a colorblind-friendly layout for folks like me.If you haven’t played it, here it is.Wordle reminds me of the game Mastermind where you try to guess the order of colors in the shortest amount of time. However, it gives you important information that Mastermind doesn’t: which letter guesses are wrong.After your first five-letter Wordle guess you get important information. The letters light up different colors to mean:If I know that there is no “e” in the word, surely that excludes a lot of possible words. “E” is the most common letter of the English language. And if you get other information such as the third letter must be “k” that must further narrow down the words.Tyler Glaiel recently wrote an interesting article about the mathematically optimal first word to solve Wordle quickly.However, I wanted to know what the probability of solving it was if you just randomly guessed. I mean, when I’m on my sixth try, the problem is often trying to think of a word that follows all the hints about letters and placement from the previous five tries.(results below if you want to skip the code)I figured the easiest way to test this was to build Wordle in Python, have my computer play 100,000 games, and keep track of the statistics. I tracked the number of guesses before solving, and how many words were left when the correct word was guessed. Here is the basic logic flow:The list of words includes words like “xylol” and “aalii” which makes me wonder if the creators of Wordle actually randomly select a word, or if they choose one that is more commonly known.Let’s say they really choose from a “mental list” of 5,000 words. Since adults know somewhere around 42,000 words, the number of 5-letter words the general population knows is likely much smaller than the full list of 12,972 words. If that is the case, the mean number of guesses to solve is 4.55 while the instances of the final guess being the only option rises to almost 60%.Like any good game designer, the makers of Wordle know to balance the probability of guessing the right word in a manner to get players in the “flow” between too easy and frustrating — where it’s fun… that and the brilliant idea of sharing cryptic colored cubes on social media and only releasing one a day.So, you’re (probably, almost) always going to solve it. And it’s always going to be fun.P.S., Does your Wordle histogram look like mine? Are your stats better than a computer randomly guessing?P.P.S., I wrote the code and this post very quickly. I welcome anyone to let me know of any errors or suggestions you find, as always.",,1,5,4,1,813,522,2,7,0,3,106
,,https://medium.com/u/73618337c6a4?source=post_page-----c35fc29ccf6--------------------------------,Daniel Tompkins,63,,,,,,0,,,,,,,0,,
Wordle UX: Sometimes a game just feels good,UX Collective,https://uxdesign.cc/wordle-ux-sometimes-a-game-just-feels-good-8749b26834ef,Joe Bernstein,198,6,1162,"Every once in a generation something beautifully simple comes along and revolutionizes our lives: the paperclip, Post-its, the iPod, Wordle. Wordle? Okay, maybe that one doesn’t quite fit with the others, but by now you must be familiar with the cryptic square emojis filling your feed on every social medium. The massively popular word game went viral over the last few weeks. It wouldn’t be a stretch to say that Wordle is the thing of the month for January 2022. It’s even been declared as the most recent phase of the pandemic era.Only on its 216th day, Wordle flew under the radar through most of late 2021, but by January 10 it had 2.7 million players and counting. So how do we explain the overnight success of something so… simple? The game mechanics of Wordle are not novel or unique. In fact, it’s hardly any different than Mastermind, Lingo, and combines hangman with the kind of logic puzzles we’ve all encountered at some point in our lives. And since the recent virality of Wordle, many clones have appeared to offer the same (or similar) game. There’s nothing inherently innovative about Wordle except for that intangible thing that sets so many companies and products apart from their competition: good UX. Let’s break down the UX principles that make Wordle so great.UX writing is an often-forgotten art, but there’s a true skill and value around clear, concise, direct instructions. Among Connie Malamed’s advice for writing UI instructions, instructions should balance brevity with getting the point across.In just 85 words, Wordle’s instructions appear to first-time visitors and tell them everything they need to know to begin playing.Wordle does have the advantage of its similar game mechanics to other word games. There’s little risk that new players will need any additional context before they begin playing. Many products aren’t so lucky.In an era of app downloads, bloatware, logins, social media trackers, and email subscriptions, it’s refreshing to simply visit a website and immediately begin playing a game. But a completely anonymous game wouldn’t be fun either. Wordle requires only the least amount of information necessary to remember its users. As long as users play Wordle on the same device and browser each day (not a big ask when so many of us wake up with our phone in our hands), and as long as they don’t clear their browser cookies, Wordle tallies completion streaks and statistics for returning users, providing them with incentive to come back and a sense of cumulative accomplishment each day.One thing that sets Wordle apart from many of its copycats is its interaction design. It only uses HTML, CSS, and JavaScript, but when you submit an answer, the reveal is perfectly paced to match your anticipation. Once you become a daily Wordle player, you develop an emotional attachment to the yellow (right letter but wrong spot) and green (right letter and right spot) squares, as well as a dread for the gray squares (completely incorrect). When you submit a guess, the letters perform a vertical flip and show their true colors, and when you’ve found the word of the day the letters do a little synchronized jump for joy. This is all so simple, yet so aesthetically pleasing. Wordle’s creator Josh Wardle is no stranger to web design best practices, having created other viral successes with Reddit’s Button and Place social experiments. But he spared no shortcuts when designing this simple game as a side project for his partner.By now you’re probably tired of those social media posts. I think you know which ones I’m talking about.If you’re like me, 60% of your social media feeds have been filled with this exact sequence of text and emojis (plus or minus a couple details). But for many of us, this is how we first found ourselves Googling the word “Wordle.” We scrolled past what seemed like a string of meaningless characters and squares. And then another. And another. And suddenly we had to know what “Wordle 202 3/6” meant. (For the uninitiated, that’s the name of the game, the serial number of the daily game — day 202 — and the number of attempts out of 6 it took to get the right answer.) The emoji squares that follow map to the output of the correct/incorrect/partially correct results of each guess. And if you know what you’re looking at, you understand that each one tells a story of someone’s path to the right answer.But that’s just the thing: you have to know what you’re looking at. The sequence of words, numbers, and emojis that you can export and share to social media act as a shibboleth to separate the in-crowd from the out-crowd. To the initiated, it’s a really easy* way to parse your friends’ performance in this daily challenge we’ve all gone through together. To the uninitiated, you need to figure out what everyone else is talking about.This is the biggest factor in explaining Wordle’s popularity. Case in point, the emoji export feature was added on December 16, just before the game took off. While most popular internet content is shareable, tweeting out something like “hey this is a pretty cool game… <link>” just isn’t as likely to generate as much engagement as that seemingly cryptic string of characters. Rather than answer all of your questions, it leaves you asking all the questions, and by then you’re committed to find out for yourself.*Note that the export format is easy for sighted people to parse, but is notably atrocious for those using screen readers. Some people have even created tools for converting the standard Wordle output to a more screen reader-friendly output.Unlike many of the other aesthetically pleasing, dopamine-boosting apps and games that surround us, Wordle gives us strict limits. We get one word per day, refreshing at midnight local time. Once you’ve consumed your six-or-fewer guesses for the day, that’s it. You’re done until the next midnight rolls around. No infinite scrolling. No incentive to play with new words or improve your score. No freemium model to speed up your wait for the next word. It’s… refreshing.Like Tiger King and sourdough bread, we measure these phases of the pandemic by the few, rare, ubiquitous and apolitical trends that come our way. Because we only get one Wordle per day, the millions of us participating in this fad share a common bond. We get extra competitive with one another when the word of the day is relatively easy, and empathize with one another when it’s tricky.When most products go viral, they usually seek to monetize their fame. And who can blame them? Game developers need to keep lights on, rent paid, and websites hosted. But in that monetization process, costs exclude users, ads slow down the experience, and pay-to-win models corrupt the competition. Josh Wardle has stated that he has no plans to monetize the game in the future either. This little piece of ephemeral viral culture is his gift to us all.",,7,0,0,6,858,590,5,0,0,21,811
Dockerizing a Programming Language,,https://bellmar.medium.com/dockerizing-a-programming-language-b39bf7574d19,Marianne Bellotti,4800,9,1638,"A compiler is just a software application after all.I have a confession: for the last eight years I have been avoiding using Docker. It’s not that I don’t understand the value of containers, it’s that the first time I picked up Docker I had a really bad experience. So bad it convinced me that this technology was too difficult for me to master. I would find out later that because of the nature of what I was being asked to do with Docker at the time (and because I was and still am primarily a Mac user) I had managed to ran into most of the really nasty edge cases that plagued the early years of Docker. But by the time I realized that, I had shifted over to legacy technology full time where most of our problems were trying to roll out version control or basic testing … forget about Docker.A bit later I switched over to management and it looked like I would never have to face my fears around Docker! Huzzah!Then came Fault.Fault is my programming language project. As its design evolved it became obvious that executing Fault specifications was going to require a SMT Solver. That introduced the question of how to correctly manage the non-Go related dependencies of a Go code base. I tried out a variety of different approaches with various levels of success, knowing that Docker was looming in the distance.To make a long story short: this time around I found it pretty easy to figure out how to get Docker to do what I want. That was a huge relief! Go binaries built, solver up and running. I was ready to go.The only problem is that, as a programming language, Fault is supposed to run on the command line. Docker still assumes that at the end of the day you’re building a web service that will speak to others over open ports. Command line applications don’t communicate that way. I wanted to maintain the look and feel of a CLI, while still getting the benefits of containerization.It’s been a long time since I wrote a true technical tutorial. Unlike others I’ve done, all this information is out there and easy to find. But I really like how simple it was to make a Docker based application look and feel like a tradition CLI running directly on the host machine. I hope a thorough breakdown might help and inspire others.The whole reason why Docker had to be introduced is that Fault has a critical dependency on Z3. If Z3 isn’t installed and configured on the machine you are writing Fault specs on, Fault can compile to them to SMTLib2 (the language of SMT Solvers) but not evaluate them. This may seem like a problem very specific to my project, but actually most programming languages have a dependency on something like LLVM or JVM or other backends. The traditional approach is either build an installer that installs all these dependencies (and cross your fingers about version conflicts) or fire off errors pushing the user into the google->stackoverflow->apt-get cycle.In my case Docker was an especially nice solution because people who like solving technical problems with SMT solvers are kind of like people who like to drag race hotrods. And by that I mean it’s a small community filled with people who expect to be able to switch out parts as they like and run the engine to extremes. Z3 is by far the most mature and stable of SMT solvers, but people will inevitably want to do custom builds of Fault with different solvers behind them.Docker multi-stage builds made that super simple. The first stage uses an official image for Go as a base and compiles the Fault compiler into a binary. The second stage uses Z3’s image as a base, copies the Fault binary over and sets up much of the remaining configuration. Which solver the Fault build uses is configured via an environment variable in the container. On spec evaluation, Fault looks up that variable and loads the template for running commands on that solver.So adding support for a new solver is as simple as writing a template for it and adding a Dockerfile.The trick is that each step in the multi-stage build lives in the repo as its own Dockerfile, then the installer creates a new Dockerfile that concatenates the Fault binary stage Dockerfile and the solver Dockerfile together and that is what docker builduses. That’s all the installer script is doing: assembling the Dockerfile and running docker build. I chose to do it with make because I find Makefiles to be soothing and because it meant each new solver is just a new rule in the same file. I run make fault-z3 for a build of Fault that runs on Z3, later on I might be able to run make fault-yices and get a Yices backed build instead.You can Dockerize command line tools by using ENTRYPOINT and CMD in the Dockerfile. ENTRYPOINT tells Docker what command to run when the container starts and CMD will pass the defaults of whatever arguments or flags are needed.The Fault binary wants three pieces of information from the user (for now anyway…):Here’s what that looks like in the Dockerfile. mode and input have default values while the required argument defaults to an empty string. The empty string will trigger the correct error and pass it back to the host.Now we can run the Fault compiler on the host machine like so:Docker starts up the container, executes the compiler and shuts down the container when the compiler returns a result. Great … except that’s a long string to type in and it looks real ugly. I don’t want people to be running docker run commands to use Fault. I want them to run fault -filepath=... like a real programming language.That’s where the third piece comes in: a simple bash script that we can throw into the host machine’s PATH. That looks something like this:I chose to do this in bash for convenience sake but it could be done in any scripting language really.All that’s left to be done is to add a line to our make based installer copying this script out of Fault’s repo and throwing it in /usr/local/bin with the name fault and no extension.Of course the place where Fault is going to look for the spec file is inside the container, and the spec file is actually on the host filesystem. To make this work Docker needs to mount all or part of the host filesystem. Fortunately this is a pretty straight forward change to the bash script:-v ${HOME}:/host mounts the home directory to a directory in the container called host I thought about having the installer configure a specific directory and mounting that instead, much the same way older versions of Go required all go files to be in the GOPATH, but most developers like to keep their spec files with the source code the spec defines. There are some obvious security concerns with mounting such a large part of the host’s file system, but since Fault only needs to read spec files I felt comfortable making the bind between host and container read only (:ro) and moving on.One of the things I wanted to be careful about was not writing a dependency on Docker into the compiler. You should be able to run Fault in Docker, but if you already have LLVM and Z3 installed on your host machine, you should also be able to build the compiler from source and run it outside of the container.Mounting the $HOME directory under /host means that the final phase of this step is a little bit of path hacking. The compiler needs to figure out if it’s running in a container or not, then modify the relative paths accordingly. I solved this by setting a specific environmental variable defining the host path. If set then Fault is running in a container and needs to adjust filepaths before trying to open them. If not set then Fault is running outside a container and it leaves the paths alone.Some arguments we don’t want to pass through to Fault running on a container because starting up a container does take a bit (Z3 is REALLY LARGE) and it’s an inefficient use of resources to start up Fault only to do things like get a help menu or check what version you’re running.Instead I modified the bash script to handle those cases without interacting with Fault at all. Help is a simple function that prints and nicely formatted help guide:Version was a little bit more interesting. It’s common practice to keep version info in git tags, but the final Docker container Fault runs in should just have the binary, not the source. As we’re building the container image we need to put the version information into a LABEL and to be nice to future devs, I want to follow the Open Containers standard for this type of metadataThe values of BUILD_DATE and BUID_VERSION are passed in via the --build-arg flag which means another slight adjustment to the Makefile installerNow in the bash script I create another function:Voila! When the user runs make fault-z3 the Fault binary is built, copied into a container with the solver preinstalled and a bash script giving the user a clean interface without suggesting that Docker is in play is added to the PATH.Fault isn’t quite ready for primetime though. I did this work as part of a push to get Fault ready to take on other contributors. There’s still a lot of work to do for that and the full Fault grammar is only partially implemented. But it gets closer to being something people can run on their own computers every day, and Docker was a big help with that.",,,8,1,4,1225,1225,1,1,0,5,222
A Senior Engineer at Google Reveals “The Best Programming Language To Learn in 2023”,Better Programming,https://betterprogramming.pub/the-best-programming-language-to-learn-in-2022-senior-engineer-at-google-explains-5abcbc5f6556,Minhaz,4200,5,813,"Sometimes this question manifests as “Should I learn Java, Python or JavaScript?”To be completely honest, I decided to write this article after reading a take on the same question by Eric Von Reinhard in the article “The Best Programming Language To Learn in 2022? — Google Lead Developer Explains”. In fact, I decided to write about this because I have a very different opinion about this question, so please don’t mind the inspired title.I recommend “no specific one”.It doesn’t matter if you start with C++ or Java or Python or Javascript or any other language for that matter. Learning a new language doesn’t take long and it shouldn’t be a #1 priority either.Programming languages are means to the end (there I said it, again!) and in practice we tend to keep switching to one or the other which gets our work done.I started my journey with C++ in high school, spent a lot of time doing Javascript in university. My job at Microsoft required me to do a lot of C#/.Net and Typescript and when I switched over to Google I started working on Android Camera App — so I started to work with Java. Soon I switched over to computational photography aspect of the camera and these days I write a lot more C++ (production code) and Python for prototyping.And throughout this journey the cost of learning a new language was never high.I have to admit over time we have built different “kinds” of programming languages in terms of how they are used. A few languages are used purely for markup like HTML or XML, while others are purely for scriptings like shell script or bat scripts, some are functional in nature others are not, and so on.There might be a certain learning cost involved when switching over from one “kind” to other. Having the fundamental understanding of these “kind” will make this barrier to learning a new language very thin.In my honest opinion, the most important skill to work on is —the ability to address any given problem statementEven if you have “never” worked on it before! And what ever programming language it takes to get done!In the end, it’d all feel the same, so go ahead and focus on learning new technologies, try to dig deeper into concepts that look like magic, in the beginning, try to explore many different genres until it all starts to look the same. Eventually, pick something that you really like and go deep!I have different recommendations for programmers in different stages of their careers.I would recommend picking up a kind of technology you want to work on — it could be web, it could be mobile (Android / iOS), it could be game development, it could other app development, it could be training ML models, building desktop apps and so on.But the crux is to pick up a technology and learn the relevant languages there. Learn by building random software if possible and spend a good time at it. If possible continue to question how different magic you see are actually working.Eventually, start digging around more fundamental concepts that are being abstracted by the libraries you are using like concurrency or multithreading, databases, graphics rendering, image processing, networking etc.You see, a program we write in any language is either directly compiled to assembly instructions or to intermediate byte-codes which eventually again gets compiled to assembly instructions for the hardware to execute. If you accept this fact, syntaxes across the languages will start to look very similar.A certain requirement, say “Increase every value of the array by 1” can be handled in different syntaxes:In Python:In JavaScript:In Go:All of these more or less compiles to something like:What's more interesting here in the series of examples is that a certain few of them lead to array copy while others don’t — and I’d recommend focussing on learning more about those types of issues and their impact on the program.I’d still recommend the same —take a step back and think of the kind of problem statements that excite you. Then think of what are new means to the end you need to learn.For example, while working on the Camera App I got opportunity to work on a “Night Mode” in the Camera App, which requires us to capture a burst of images and merge them in robust fashion to produce a well exposed and low noise image. This is computationally very expensive and we needed our algorithms to run on resource-constrained hardware within a certain latency budget.This was a very exciting problem statement for me and required me to re-learn modern C++, image processing concepts etc.But it was so much fun, that the effort spent on learning didn’t feel like a bother. I’d recommend finding such directions and start learning what ever it takes.TL;DR; Don’t bother to find one programming language that would be your bet for 2022.",,50,10,1,0,842,609,2,1,0,2,3200
A Senior Engineer at Google Reveals “The Best Programming Language To Learn in 2023”,Better Programming,https://betterprogramming.pub/the-best-programming-language-to-learn-in-2022-senior-engineer-at-google-explains-5abcbc5f6556,Minhaz,4200,5,813,"Sometimes this question manifests as “Should I learn Java, Python or JavaScript?”To be completely honest, I decided to write this article after reading a take on the same question by Eric Von Reinhard in the article “The Best Programming Language To Learn in 2022? — Google Lead Developer Explains”. In fact, I decided to write about this because I have a very different opinion about this question, so please don’t mind the inspired title.I recommend “no specific one”.It doesn’t matter if you start with C++ or Java or Python or Javascript or any other language for that matter. Learning a new language doesn’t take long and it shouldn’t be a #1 priority either.Programming languages are means to the end (there I said it, again!) and in practice we tend to keep switching to one or the other which gets our work done.I started my journey with C++ in high school, spent a lot of time doing Javascript in university. My job at Microsoft required me to do a lot of C#/.Net and Typescript and when I switched over to Google I started working on Android Camera App — so I started to work with Java. Soon I switched over to computational photography aspect of the camera and these days I write a lot more C++ (production code) and Python for prototyping.And throughout this journey the cost of learning a new language was never high.I have to admit over time we have built different “kinds” of programming languages in terms of how they are used. A few languages are used purely for markup like HTML or XML, while others are purely for scriptings like shell script or bat scripts, some are functional in nature others are not, and so on.There might be a certain learning cost involved when switching over from one “kind” to other. Having the fundamental understanding of these “kind” will make this barrier to learning a new language very thin.In my honest opinion, the most important skill to work on is —the ability to address any given problem statementEven if you have “never” worked on it before! And what ever programming language it takes to get done!In the end, it’d all feel the same, so go ahead and focus on learning new technologies, try to dig deeper into concepts that look like magic, in the beginning, try to explore many different genres until it all starts to look the same. Eventually, pick something that you really like and go deep!I have different recommendations for programmers in different stages of their careers.I would recommend picking up a kind of technology you want to work on — it could be web, it could be mobile (Android / iOS), it could be game development, it could other app development, it could be training ML models, building desktop apps and so on.But the crux is to pick up a technology and learn the relevant languages there. Learn by building random software if possible and spend a good time at it. If possible continue to question how different magic you see are actually working.Eventually, start digging around more fundamental concepts that are being abstracted by the libraries you are using like concurrency or multithreading, databases, graphics rendering, image processing, networking etc.You see, a program we write in any language is either directly compiled to assembly instructions or to intermediate byte-codes which eventually again gets compiled to assembly instructions for the hardware to execute. If you accept this fact, syntaxes across the languages will start to look very similar.A certain requirement, say “Increase every value of the array by 1” can be handled in different syntaxes:In Python:In JavaScript:In Go:All of these more or less compiles to something like:What's more interesting here in the series of examples is that a certain few of them lead to array copy while others don’t — and I’d recommend focussing on learning more about those types of issues and their impact on the program.I’d still recommend the same —take a step back and think of the kind of problem statements that excite you. Then think of what are new means to the end you need to learn.For example, while working on the Camera App I got opportunity to work on a “Night Mode” in the Camera App, which requires us to capture a burst of images and merge them in robust fashion to produce a well exposed and low noise image. This is computationally very expensive and we needed our algorithms to run on resource-constrained hardware within a certain latency budget.This was a very exciting problem statement for me and required me to re-learn modern C++, image processing concepts etc.But it was so much fun, that the effort spent on learning didn’t feel like a bother. I’d recommend finding such directions and start learning what ever it takes.TL;DR; Don’t bother to find one programming language that would be your bet for 2022.",,50,10,1,0,842,609,2,1,0,2,3200
,,https://medium.com/u/e1ac4edd2c86?source=post_page-----c35fc29ccf6--------------------------------,Jan Sunavec,264,,,,,,0,,,,,,,0,,
Http Server Performance: NodeJS vs. Go,Better Programming,https://betterprogramming.pub/http-server-performance-nodejs-vs-go-397751e8d275,Jan Sunavec,264,4,309,"We are developing something like an ad proxy or Google Ad Buffer. Service just forward ad HTTP requests to SSPs server. For this purpose, it’s necessary to create many HTTP requests with minimum hardware resources. Therefore we decided to do research and compare programming languages with virtual machines and compiled one.We are pretty familiar with NodeJS and JavaScript technology. Therefore we started testing HTTP connection with the V8 engine. Of course, we didn’t start from scratch, we used the fastify package. It’s actually based on the NodeJS HTTP package. So at the bottom of the software stack is a compiled low level HTTP server. But anyway there is a tiny layer running under V8. Question is how this layer slows execution.The script is pretty straightforward.I used the ApacheBench (ab) tool for testing. Let’s skip the full hardware specification. I can just tell that I used an I7–8550U CPU.Let’s try more concurrent connections.So far so good. 500 concurrent connection hit CPU limit and Node solution starts struggling but let’s get Go numbers.The script is a bit longer but still short.As you can see I decided to use fasthttp as the HTTP server. The server is not based on any HTTP lib. So it’s really pure HTTP protocol implementation. Let’s see the result for 100 concurrent requests.Well, numbers are really great agains the NodeJS solution. Especially requests served within a certain time. It’s almost flat. Let’s start the final test.As you can see, the Go solution serving time is still flat. It looks like there is still space to get more concurrent requests but let’s just compare basic numbers.Go is the only winner here especially with the higher number of concurrent requests.So tiny layer running under the V8 engine is not so tiny. With 100 concurrent requests, deliver Go over 18% more requests. 500 concurrent requests increased gain to over 34%.",,25,6,2,0,992,635,2,0,0,3,1600
,,https://medium.com/u/820b4ad3efe1?source=post_page-----c35fc29ccf6--------------------------------,Minhaz,4200,,,,,,0,,,,,,,0,,
,,https://medium.com/u/7a530c004bc?source=post_page-----c35fc29ccf6--------------------------------,Joe Bernstein,198,,,,,,0,,,,,,,0,,
What I Learned from Playing More than a Million Games of Wordle,Towards Data Science,https://towardsdatascience.com/what-i-learned-from-playing-more-than-a-million-games-of-wordle-7b69a40dbfdb,barrysmyth,1800,18,3857,"Ok, full disclosure: I didn’t actually play a million games of Wordle, but I wrote a Wordle simulator that did. Why? Because I wanted to learn more about the dynamics of the game and evaluate some of the strategies that people have been discussing online, especially those that discuss the “best words to use at the start of Wordle”. What follows is a description of the approach taken and the what I learned from it.Note: In this article I will be discussing some words to use as good Wordle guesses. I won’t disguise these words because they have been widely discussed elsewhere, online and in the media. However, I will disguise the novel ‘best’ words found by this analysis so that a reader can read the article without receiving unwelcome or unwanted hints. At the very end of the article I will reveal these new words for those who are interested. It’s not that there is anything revelatory them, but I want to be respectful to those who, while interested in the approach taken here, do not want any hints that might spoil their future Wordle fun.The simulation of Wordle discussed below approximates a strong Wordle player and for this analysis I simulated more than 1M games for over 400 different (carefully chosen) sets of seed/starter words. The full technical details and findings are discussed below, but a summary of the key results includes the following:Wordle is a simple but compelling online word guessing game that has recently taken the Internet by storm. Players have one chance to play Wordle each day and every day a new target word is chosen.The objective is for the player to deduce a 5-letter target word based on a series of up to 6 guesses. Each guess is assessed and colour-coded to indicate which letters, if any, are contained in the target word (yellow), which, are in the correct positions (green), and which letters are absent from the target word (grey). In this way, with each new guess, the player gets more hints about the target word. The player can choose to use or ignore some or all of these hints to produce their next guess; there is a (more challenging) version of the game in which the player must satisfy all of the current hints with each new guess.The example above shows a game in progress, after 3 rounds (3 guesses), and before the target word has been guessed. The first guess revealed ’n’ and ‘t’ to be in the target word, although not in the correct position, and the second guess indicated that ‘a’ was also part of the target, although not its first letter. These first two guesses also eliminated a number of letters from further consideration (‘s’, ‘o’, ‘r’, ‘d’, ‘i’, ‘e’, ‘u’). Armed with these hints the player made good progress on round 3, guessing ‘t’, ‘a’, ’n’, ‘g’, all in their correct positions, leaving just one letter to complete the game.Writing a Wordle simulator requires a suitable dataset of 5-letter English words. I built such a dataset by bringing together a few different dictionaries and datasets that were available online including a dataset of common English words and a dataset of common person names; these datasets are publicly and readily available. I eliminated person names from the dataset of English words and selected a set of 2,500 common 5-letter words, which I understand this a good approximation to the dataset that Wordle uses, although no doubt it is not perfect.As for the logic of the simulator: given a target word, the basic idea is to iterate over up to 6 guesses, comparing each guess to the target word and updating a growing number of constraints about the letters that are found to be in or out of the target word, and their correct and incorrect positions. These constraints can be used to eliminate incompatible words from consideration when picking the next guess. Each guess must be a word from the dataset and the game stops when the player guesses the correct word or runs out of guesses.To help guide the choice of the next guess the simulator scores the compatible words based on their likely suitability. For this I currently use the sum of the (fixed) frequencies of the word’s unique letters in the dataset. In this way, compatible words with more frequent letters are more likely to be chosen than words with more unusual letters. As it stands the simulator always selects a next guess that is compatible with all of the constraints learned so far. This approach is likely to simulate play in a manner that is more consistent with a strong Wordle player and for future work it may make sense to weaken the simulator’s playing style to better approximate regular players. One way to do this might be to add some randomness into how new words are selected or perhaps by relaxing some of the constraints so that not every word chosen is always guaranteed to satisfy all of the current constraints. I’ll leave this for future work.One of the unusual features of Wordle is that a player’s first word must be chosen in the absence of any information about the target word, which has led to a lot of speculation about the best first-words to use. Are some words likely to be better than others? Is it worth trying to find, and stick with, a good opening word(s)?Words with more unique letters should do better than words with repeat letters, on the grounds that more unique letters mean there are more opportunities for a letter match with the target word? For example, a word like ‘mamma’, with its two unique letters, has at least one letter in common with 48% of the words in the dataset used. Compare this to a word like ‘admit’. Its 5 unique letters (including the ‘m’ and ‘a’ of ‘mamma’) mean it has a single letter match with approximately 82% of the potential target words. To put this another way, if you start your next Wordle with ‘admit’, then you will generate a single letter match with the target word approximately 82% of the time and, on average ‘admit’ will generate 1.29 correct letter matches.A variation on this strategy is to use words with lots of vowels since most, but not all (‘synth’, ‘lynch’ etc.) 5-letter words contain vowels. A good candidate is ‘audio’ and using it as your first word will generate at least one matching letter about 91% of the time — we will refer to this overlap as a word’s coverage so we will say that ‘audio’ covers 91% of target words — and on average you can expect to find approximately 1.33 correct letter matches with the target wordThe linguists have had much to say about Wordle too. One recent article highlights how various linguistic constraints can conspire to dictate plausible and likely letter sequences: ‘tr’ is fine at the start of a word but not at the end, while ‘ng’ can be used at the end but not at the start. That’s not so actionable when it comes to choosing your starter words but observations like this may help during later guesses.Then came the mathematicians, who pointed out that it can be useful to think about letter probabilities, because overlaps are more likely with words that have many commonly occurring (high probability) letters, such as ‘e’, ‘s’, or ‘t’. As mentioned above, we can score each word by calculating the sum of the frequencies/probabilities for its unique letters, and then pick high-scoring words, such as ‘roast’, as our initial guess, while steering clear of low scoring words, such as ‘edged’, even though ‘edged’ contains the ever-popular letter ‘e’. The coverage of ‘roast’ is just over 92% and, on average, it will deliver 1.75 letter matches. Compare that with the coverage of ‘edged’ at just over 60% and only 0.75 letter matches on average.This idea of letter probabilities brings us to an important branch of mathematics called information theory, which provides a formal treatment of the rules governing the encoding and transmission of information and messages. A key concept is that of entropy, which can be also used to estimate the amount of information contained in a message — independent of its meaning — and high entropy words such as ‘arose’ are also good candidates (95% coverage with about 1.95 letter overlaps on average).So far we have been talking about the initial Wordle guess – the single word used to get us started – but what about subsequent guesses? Must we be left to our own devices for all later rounds? Or are there fixed second and third words that can help in most cases? It turns out that an important problem in theoretical computer science, the set cover problem, can help us here. From a Wordle perspective, the following is a version of this problem:What is the minimum set of words which guarantees at least some overlap with every possible target word? This minimum set of words is called the minimum set cover.For reasons that are beyond the scope of this article, the (minimum) set cover problem is very difficult to solve. It is a so-called NP-Complete problem, which means that although it is easy to verify that a specific solution works, once found, the actual finding of the solution is very time-consuming; at least we have not found any shortcuts to date and a shortcut may not exist.Even though finding the (minimum) set cover is difficult we do have efficient algorithms to identify close approximations. For example, in the case of Wordle we can start with the word that has the highest coverage; that is, it has at least a single letter overlap with the highest number of other words. This is the first word in our set cover. Next, we eliminate all those words that are covered by our current set cover, recalculate the coverage scores for the remaining words, and identify the remaining word with the highest coverage, and then add that to our set cover. We repeat this process until there were no more words left to cover. This is called a greedy algorithm because, during each iteration, we greedily commit to the next highest scoring word. It is not guaranteed to produce an optimal set cover, because such a set cover may require a word that is not among the highest coverage words, but it should get us close.My intuition was that if we tried this then we might need 5 or 6 words to cover the 2,500 words in our dataset. Such a result might not be all that useful, given that you only have 6 guesses. However, I was surprised when I found a set cover of just two words. That is, using just two specific words as the opening two guesses guarantees a match with at least one letter from the target word, no matter which target word has been chosen. Of course, using two guesses in return for a single letter might not sound all that attractive, but on average these two guesses will net you 2.88 of the letters in the target word and 68% of the time it will get you at least 3 target letters. Moreover, 60% the time at least one of these letters will be in its correct position. That’s not bad for just two guesses!We can do better however. In the above I used the coverage score of a word as the basis for its selection for the set cover. We could try other ways to select words, such as those mentioned earlier: select the word with the highest letter probability score or the highest entropy score. It turns out that both of these lead to 3-word set covers— that is, 3 words are needed to fully cover the entire set — but with these 3 words we get a mean overlap of 3.78 letters, and 65% of the time we will get 4 out of 5 matching letters. Moreover, two-thirds of the time one of these letters will be in its correct position and one-third of the time two of the letters will be in their correct positions. Of course, as a result of these guesses you will have also learned a great amount about the letters that are not in the target word too.So at this stage we can identify 3 different set covers using the above procedure by selecting words using coverage, letter probability, and entropy. However, this greedy algorithm is not guaranteed to find the best set covers it is worth broadening our search. We can do this by beginning our searches with different (good) words. In this work, I focus on the top 100 scoring words using coverage and entropy to generate 200 possible set covers, some of which contain 2 words and some 3 words; in fact they generate 197 unique set covers. I also include the first words, and the first two words from each of these as additional seed words; these subsets are not valid set covers but they typically cover a majority of the words and so should provide a good start too. The end result is a list of 471 unique sets of seed words, each containing 1 - 3 words.Not surprisingly, these words include the words that others have commonly proposed as good single words or pairs of words to use with Wordle, such as ‘arise’, ‘audio’, ’notes’, ‘resin’, ‘stare’ as well as many popular two-word sets such as ‘arise’ followed by ‘count’ and ‘notes’ followed by ‘laird’.We are now ready to run our Wordle simulation experiment. Each simulation run will use a different one of the 471 sets of seed words as its initial guesses and each seed will be used to play Wordle for the 2,500 possible target words. Accordingly, we are effectively simulating the use of a given seed set for a complete run of the Wordle game (that’s the equivalent to almost 7 years of Wordle, based on a word per day) for a total of 1,177,500 (471 x 2,500) individual games and, as it turns out, 4,714,791 rounds of play; for the purpose of this analysis we exclude target words that were solved by a seed word, leaving 1,176,614 games and 4,713,381 rounds.It might be easier to think of this as 471 individual players playing Wordle for almost 7 years, each player using their own favourite set of starter words. If the average Wordle game takes 15 minutes to play then this amounts to about 30 years of actual play-time; fortunately my laptop can do this in about 6 hours.So what did I find? Let’s begin with some summary results, starting with the fraction of games where the target word is located on a particular round of play (1 to 6). The graph below shows that, on average (across all target words and seed guesses) 47% of games finish with the correct target word on round 4. Just over 4% of games require all 6 rounds and a similar number of games fail to be solved within 6 rounds; remember, in this simulation our simulator plays like a strong player using high quality initial guesses so regular play is likely to be associated with longer games and a greater proportion of unsuccessful games.As a game unfolds how much is being learned about the target word with each new guess? And how does this help to constrain the set of possible target words? Below we look at the number of correct and incorrect letters, and the cumulative number of correct and incorrect letter positions, after each new guess/round. For example, the first graph (top-left below) focuses on letters that are in the target word and their positions, and while the first guess typically reveals fewer than 2 target letters (mostly in incorrect positions) by the time the third guess has been made, a strong player will have located about 4 target letters, on average, with 2 or 3 in the correct positions. Likewise, by round 3, ~8 incorrect letters will be known and the player will have access to about 16 constraints with which to guide their search for the target word. These constraints will greatly limit the set of compatible words that remain after round 3, hence why more about 70% of games conclude successfully after round 3 (for a strong player using strong seeds).Are some target words more difficult to solve? Yes. When averaged across all of the seed sets of starter words, ‘hitch’ and ‘hatch’ are the least frequently solved targets. The simulator fails to solve these 90% of the time and when they can be solved these words can only be located in 3 or fewer rounds about 5% of the time. On the other hand, words such as ‘arose’, ‘alien’, ‘aisle’, and ‘raise’ can be solved 100% of the time and in 3 or fewer rounds for more than 66% of games; in this sense these are Wordle’s easiest targets.Let’s get back to the main question about which words are good as opening guesses. Are some of these high quality starter words better than others or are all the seeds similar in terms of their likelihood to lead for short-game success? Is it better to try 1, 2, or 3-word seeds? Which are the best 1, 2, or 3-words seeds?Below we can see the distribution of average game lengths for 1 (blue), 2 (orange), and 3-word (green) seeds, when each seed is tested against the full 2,500 set of target words; on average these seed sets successfully complete games more than 95% of the time. Again, the simulator is a strong player using good seeds, hence why the vast majority of seeds lead to average game lengths of just 3 or 4 rounds, as shown. But even within this collection of high quality seeds, it is clear that some lead to shorter games than others. In fact, the best 1, 2, and 3-word seeds can solve a typical target word in 4 rounds or fewer over 80% of the time.We can see too how 1 and 2-word seeds tend to produce shorter games than 3-word seeds. This suggests that players are better off trying to find their own third word rather than relying on a one-size-fits-all third word, which makes sense given that the first couple of words are likely to reveal quite a bit about the target word (~3 correct letters, one in its correct position about 6 incorrect letters). Blindly using a fixed third-word is ignoring a lot of information.However, the difference between 1 and 2-word seed sets is more modest even though some information is being ignored when a two-word seed set is used. On average the mean game length for the 1-word seeds (3.80) is slightly shorter than for 2-word seeds (3.86); shown as the colour-coded solid vertical lines in the graph). However, if we compare the single best 1-word seed to the single best 2-word seed — where ‘best’ means shortest mean game length — as indicated by the corresponding dashed vertical lines above, then the difference is much more marginal (3.66 vs 3.68 for the best 1-word vs 2-word seeds, respectively). Instead of average game lengths it might be worth looking at the fraction of games that can be solved in say 3 or fewer rounds — that is the fraction of very short ganes — and doing so we find 46% of the games associated with the best 1-word guess require 3 or fewer rounds compared to 48% of the games for the best 2-word query.For another perspective, it is useful to compare this to the worst performing seeds (those with longest average games) among these carefully curated sets. The worst 1, 2, and 3-word seeds achieve lower completion rates (90–92% instead of >95%), and they require more guesses to do so (4.12–4.45 guesses on average).So, what are the best 1, 2, and 3-word sets to use at the start of the game? If you don’t wish to know the actual words then it is still safe to read on, but be careful because at the end of this article they will be revealed. For now I’ll discuss these best sets in the abstract while making some final observations:Should you go with ‘****’ and a bespoke second word or use ‘#####’ followed by ‘$$$$$’? Note that nothing you learn from using ‘#####’ as your first guess with conflict with what you will learn from using ‘$$$$$’ as your second guess, because, by design, they share no letters in common, but you will be ignoring what you have learned from ‘#####’. However, ‘$$$$$’ has been chosen because it is also an inherently good word to use and in practice it works as well as a more bespoke second guess.We can see this in the graph above, which shows the various Wordle features discussed previously (correct/incorrect letters/positions etc.) by round of play. However, now we also include the results for ‘*****’ (the dotted lines) and ‘#####’ and ‘$$$$$’ (the dashed lines). As expected these seed sets perform better than the average (high quality) seed set, at least in terms of correct/incorrect letters, positions and reduced the number of compatible words etc.These results also help to clarify that there is relatively little difference between using ‘*****’ versus using ‘#####’ and ‘$$$$$’. If anything, the pairing of ‘#####’ and ‘$$$$$’ appears to do a slightly better job, on average, when it comes to whittling down the average number of compatible words in rounds 2, 3, and 4, which may explain why we find a greater fraction of games with 3 or fewer rounds for ‘#####’ and ‘$$$$$’ (48%) than we do for ‘*****’ (46%).Should any of this impact on how you play your next Wordle? That’s completely up to you. Certainly it makes sense to have a strong initial word to start, and if that’s all the assistance you want then ‘*****’ is a good bet, at least based on what we have found in this analysis.If you wish to go further and are willing to commit your first two guesses then ‘#####’ followed by ‘$$$$$’ will likely serve you very well, at least on average, and get you to the interesting middle-game a little faster, and if you do well with ‘#####’ then you can always chose a bespoke second guess to try and achieve a fabled two-round win.As an opening strategy using either ‘*****’ or ‘#####’ with ‘$$$$$’, is about as straightforward as it gets: no long lists of words or complicated decision trees or lookup tables, just a well chosen single word or word-pair to get you off to a good start. And don’t worry, there will still be plenty of work for you to do for your subsequent guesses.Speaking of which, if you want some help refining your guessing strategy then check out my new post which looks this very topic.Now’ where’s today’s Wordle….Having got to here, if you are sure you want to know the words that were found as the best to use then the decoding is below:Keep scrolling if you are sure …Almost there …",,11,0,0,97,1091,660,6,3,0,14,419
,,https://medium.com/u/667cfc1401ea?source=post_page-----c35fc29ccf6--------------------------------,John Clark Craig,965,,,,,,0,,,,,,,0,,
,,https://medium.com/u/a995c3b2ae8?source=post_page-----c35fc29ccf6--------------------------------,barrysmyth,1800,,,,,,0,,,,,,,0,,
,,https://medium.com/u/d196ba9afc2d?source=post_page-----c35fc29ccf6--------------------------------,Marianne Bellotti,4800,,,,,,0,,,,,,,0,,
4 Python Packages to Beautify and Format Your Codebases,Better Programming,https://betterprogramming.pub/simple-hacks-to-automate-python-code-beautification-5ad934cf5a29,Tapas Das,139,4,535,"Have you ever come across a poorly written piece of Python code?I’m talking about a tangled mess where you had to spend hours just trying to understand what piece of code goes where.Writing code is one part of a developer’s role. Writing beautiful and neat Python code, on the other hand, is a different ball game altogether. This could well make or break your image as a proficient programmer in the analytics or data science space (or even in software development).So how do we write this so-called beautiful Python code?PEP 8, sometimes spelled PEP8 or PEP-8, is a document written in 2001 by Guido van Rossum, Barry Warsaw, and Nick Coghlan, that provides guidelines and best practices on how to write Python code. The primary focus of PEP 8 is to improve the readability and consistency of Python code.As Guido van Rossum said, “Code is read much more often than it is written.” You may spend a few minutes, or a whole day, writing a piece of code to process user authentication. Once you’ve written it, you’re never going to write it again. But you’ll definitely have to read it again. That piece of code might remain part of a project you’re working on. Every time you go back to that file, you’ll have to remember what that code does and why you wrote it, so readability matters.If you’re interested, PEP 8 can be found here.When writing Python code, as developers, you need to make sure that the code:However, it can be overwhelming to check all of these criteria manually, every time you write a piece of code. Wouldn’t it be nice if you can automatically check and format your code before committing to a version control system?Let’s get started.Let’s start by writing a messed-up Python code that will violate all PEP 8 standards.We will start with checking the style and quality of Python code, by using the flake8 Python package.To install flake8, type:Below is the output of running flake8 on our Python code.As we can see, flake8 has pointed out below error types.Now that we know what the errors are, let’s start by formatting the code with a black Python package.To install black, type:Below is the output of running black on our Python code.And voila! The code is automatically formatted like below!As the next step, let's try to sort the imported libraries alphabetically and separate them into sections and types, for more organized code.To install isort, type:Below is the output of running isort on our Python code.Cool! The imports are much more organized now.Sometimes, we might forget to write docstrings for classes and functions like below:So as a final step, instead of manually looking at all our functions and classes for missing docstrings, we can run interrogate instead to check for missing docstrings.To install interrogate, type:Below is the output of running interrogate on our Python code.Voila! From the terminal output, we know which files, classes, and functions don’t have docstrings.Since we know the locations of missing docstrings, adding them is easy.Congratulations! You have just learned how to automatically check, edit and beautify your Python code before committing it. I hope this blog will make it effortless for you to review and format your code.",,1,4,3,0,773,545,10,2,0,7,217
,,https://medium.com/u/2c2834517eb2?source=post_page-----c35fc29ccf6--------------------------------,Berke Soysal,254,,,,,,0,,,,,,,0,,
,,https://medium.com/u/fc0fbc7da6cc?source=post_page-----c35fc29ccf6--------------------------------,Itır Ege Değer,162,,,,,,0,,,,,,,0,,
Simplifying Single-Responsibility Principle,,https://medium.com/@zackbunch/simplifying-single-responsibility-principle-9b57051477b3,Zack Bunch,124,3,488,"The Single Responsibility Principle (SRP) is the first letter in the acronym S.O.L.I.D. Each of the principles in S.O.L.I.D set forth standards that lead to developing software that is maintainable and extendable as the project grows. If you are working on small projects or still in school these principles may not seem the most beneficial, but as you move to larger projects you will appreciate code that follows them.Single-Responsibility Principle states:A class should have one and only one reason to change, meaning that a class should have only one responsibility.In software engineering the key to creating maintainable code is adhering to “low coupling and high cohesion”. If a class has more than one responsibility, it becomes highly coupled which will indefinitely lead to future headaches for new and seasoned developers on the code base. So how do we know if we are adhering to the Single-responsibility principle as we write code? I find a quote from “The Pragmatic Programmer” helpful to keep in mind when writing and maintaining code:When you come across a problem, assess how localized the fix is. Do you change just one module, or are the changes scattered throughout the entire system? When you make a change, does it fix everything, or do other problems mysteriously arise?If our classes focus on one responsibility our changes should be mostly localized to that single class. This can be better explained with code, so let's take a look at a short example in Python.Let’s pretend were building a city building game inspired by my man Randy. For sake of simplicity and time lets imagine a city building game where we have vehicles that move around on the map. Our vehicle needs to store information such as the year, model, color, position, current speed and any customizations the player has made while in game.If you run the code above you will notice everything works just as expected. This code may look fine, but it actually breaks the Single-Responsibility principle. A vehicle in the game shouldnt have to be responsible for saving itself. If you think of a game, many things need to be saved. So if were saving vehicles, people, rank etc we will need many classes to implement their own save function. This actually breaks another rule in programming. D.R.Y aka DON’T REPEAT YOURSELF. The save functionality for the game should be separated out to its own class that is only responsible for saving game data.To fix our Vehicle class, we need to remove the responsibility of saving to another class called DataService. This new class will act as a utility class meant to only save objects to a file so it will use an @staticmethod decorator.Running the program will result in the following:If you enjoyed learning about the first letter of S.O.L.I.D then go ahead and give this article a clap and follow to stay up to date with more articles like this one! Thanks for reading!",,2,1,1,1,500,233,1,0,0,1,60
Checking for Nulls in Java? Minimize Using “If Else”,Better Programming,https://betterprogramming.pub/checking-for-nulls-in-java-minimize-using-if-else-edae27016474,Itır Ege Değer,162,7,1094,"In this article, I will try to give some examples of the different types of null checks or NPE (NullPointerException) avoidance techniques that is used.There are many good articles about this topic but I will try to focus on some specific examples from my own experiences.Note that, I am still in the learning process — probably never-ending :)- so, if you see any mistakes, feel free to let me know about it.A container object which may or may not contain a non-null value. If a value is present, isPresent() will return true and get() will return the value.¹One of the most common place Optional is used in the project that I am currently working on is while retrieving data from the database.Let's say that you have 3 things:and you want to retrieve the student with the related id.Returns an Optional describing the given value, if non-null, otherwise returns an empty Optional.¹The returned value from this method will never be null. If it is null, the returned value will be Optional.empty(). This way, if the result of this method is used somewhere else, there will be no chance of getting a NPE.If a value is present, returns the value, otherwise throws NoSuchElementException.¹In the case of null, if you want to throw an exception you could just use orElseThrow().If a value is present, returns the value, otherwise returns other.¹In the case of null, if you don’t want to throw an exception but you want to return a sample student instance, orElse()could be used.If a value is present, returns the value, otherwise throws NoSuchElementException.¹In this case, if you are using IntelliJ, it immediately gives a warning:Basically it is saying “first check if your student is null or not and then proceed”.I mostly use get()in unit tests when I am sure that there is data returned from the method I called. But I would not use it without isPresent() in the actual code, even if I am sure that there will be no null.Another example; the below code piece tries to get the student with the related id and returns a default name (“Hayley”) if there is no such student. The value passed torElse()is also returned when there is student but no name to it.In case of lists, maps etc, isEmpty() checks if the collection/map is null or have size of 0. Similarly for String it checks if the Stringis null or have length of 0.In order to use CollectionUtilsand MapUtils, you need to add the following dependency to build.gradle file:and for StringUtils you will need:Returns true if the provided reference is non-null otherwise returns false .²Lets say that you have a stream of data, and you will perform some chain operations on this stream but before that you want to filter out nulls if there are any.The result: 1234Checks that the specified object reference is not null and throws a customized NullPointerExceptionif it is. This method is designed primarily for doing parameter validation in methods and constructors with multiple parameters.²Returns the first argument if it is non-null and otherwise returns the non-null second argument.³Returns the first argument if it is non-null and otherwise returns the non-null value of supplier.get().³The most common place I use these three are mainly constructors.Lets go over them with the example above.requireNonNull on id : we are saying that “this field is required, so if it is null; throw a NPE with “id is required” message”.requireNonNullElse on name : we are saying that “this field is required, so if it is null; don’t throw an exception but set a default value for it.” In our case default value is “hayley”.requireNonNullElseGet on classes: we are saying that “this field is required, so if it is null; don’t throw exception but set a default value for it.”.The difference from requireNonNullElse is that, this method expects Supplier as a second parameter.Thus, we can use method reference with requireNonNullElseGet. It is especially useful when dealing with Lists, Maps, Sets if you want to initialize them as empty lists,maps, sets etc.Let's see in action:Result:Please note that, these validations only work if the related constructor is called.Another example; lets say that you are calling a method (y) from your current method (x) and with the result returned from (y), you will do other operations in x. If you don’t check that the result returned from y is null or not, there is a possibility that you can get NPE.The result:If you are not familiar with Lombok, I highly suggest you to check it out. I personally love Lombok and it makes a developer’s life much easier :)Lets say that you have Student.javawith fields such as id, name and classes. You can use put @Builder.Default before the related field and give it a default value.When an instance of this Student class is created, it will have “classes” as an empty list, not null.The result:If you simply state fields in Student.java like this:The result:It is especially useful for me when dealing with lists, maps etc. Because for the current projects I am working on, lists or maps are more likely to be null than the other fields. Also, if more operations are being performed on those lists/maps its much likely to get a NPE when they are null.Let’s say that you want to get the names of the classes that a student is enrolled in and you did not use Builder.Default on “classes” list.Throws NPE.@NotNull : “The annotated element must not be null. Accepts any type.” ⁴@NotEmpty : “The annotated element must not be null or empty. Supported types are CharSequence, Collection, Map, Array.” ⁵@NotBlank: “The annotated element must not be null and must contain at least one non-whitespace character. Accepts CharSequence” ⁶Lets say that you have a controller and a saveStudent() method in it. When you want id, name, and classes fields not be null or empty, you could just put these annotations as below in Student class:And if you are using Spring Boot you can combine these annotations with @Validated annotation for the request body of API as below.For example, you have request as such:As you see, you will get “400” error and in the console of your application you will see:If preferred, you can catch this MethodArgumentNotValidException and return a custom error.The dependency needed:That was it for my current knowledge. I will try to update this list as I learn more ways to do null checks. There are times that I still need to use a good old “if else” block for null checking but I try to apply these methods whenever I can.Hopefully, you enjoyed this article.References:",,14,18,1,19,1132,442,6,3,0,10,669
,,https://medium.com/u/ac5ca15cbe98?source=post_page-----c35fc29ccf6--------------------------------,Zack Bunch,124,,,,,,0,,,,,,,0,,
SOLID Principles With (almost) Real-Life Examples in Java,Better Programming,https://betterprogramming.pub/solid-principles-with-almost-real-life-examples-in-java-b292a4e2c18b,Berke Soysal,254,3,467,"SOLID principles are some of the oldest rules in the software world. They enable us to write maintainable, readable, reusable code. In this text, I am trying to accomplish a somewhat real-life example, obeying the SOLID principles.Each class should have only one sole purpose, and not be filled with excessive functionality. Consider the following example:This class is implemented for hashing passwords, as the name implies. It should not be its responsibility to save them to the database. Each class should have a single responsibility to fulfill.There should not be “god classes” that have a broad variety of functionality that has too much to accomplish. Instead, we should write our classes as modular as possible. Implement the saving operation in another class.Classes should be open for extension, closed for modification.In other words, you should not have to rewrite an existing class for implementing new features.Let’s continue to our password-hasher example. Suppose we want our class to be able to hash with a variety of algorithm options.If we implemented this way, we would break the O in SOLID so bad. Every time a new algorithm is implemented, we need to modify the existing class, and it looks ugly.Thanks to OOP, we have abstraction. We should make our initial class an interface/abstract class and implement the algorithms in the concrete classes.In this way, we can add new algorithms without touching the existing codebase.A sub-class should be able to fulfill each feature of its parent class and could be treated as its parent class.To demonstrate our example, let’s create the Model (Data) Classes to use our hashing algorihms.And we implemented the same for other encodings…To fulfill Liskov’s Rule, each other extension of Hashed should use a valid implementation of hashing function and return a hash.For example, if we extend the Hashed class with a class called “NoHash” that uses an implementation that returns exactly the same password without any encoding will break the rule, since a subclass of Hashed is expected to have a hashed value of the password.Interfaces should not force classes to implement what they can’t do. Large interfaces should be divided into small ones.Consider we add decoding feature to the interface.This would break this law since one of our algorithms, the SHA256 is not decryptable practically, (it’s a one-way function). Instead, we can add another interface to the applicable classes to implement their decoding algorithm.Components should depend on abstractions, not on concretions.We have a password service like the following:We violated the principle since we tightly coupled the Base64Hasher and PasswordService.Let’s decouple them and let the client inject the hasher service needed with the constructor.Much better. We can easily change the hashing algorithm. Our service does not care about the algorithm, it's up to the client to choose it. We don’t depend on the concrete implementation, but the abstraction.https://www.baeldung.com/solid-principleshttps://www.digitalocean.com/community/conceptual_articles/s-o-l-i-d-the-first-five-principles-of-object-oriented-design",,19,0,0,0,600,567,1,0,0,3,2100
,,https://medium.com/u/75d735bef60b?source=post_page-----c35fc29ccf6--------------------------------,Alex Moss,89,,,,,,0,,,,,,,0,,
,,https://medium.com/u/c2859b77a925?source=post_page-----c35fc29ccf6--------------------------------,Alexey Soshin,805,,,,,,0,,,,,,,0,,
Why Spring is Faster Than Vert.x?,Better Programming,https://betterprogramming.pub/why-spring-is-faster-than-vert-x-bc09b436021d,Alexey Soshin,805,6,1193,"The question “Why Spring is faster than Vert.x?” in its different variations is being asked on StackOverflow once a month on average. After all, Spring is still the most popular JVM framework by far, so lots of companies use it. But Spring Framework isn’t known for its performance. Vert.x, on the other hand, is considered as one of the top-performing JVM frameworks. So it’s expected that Vert.x would outperform Spring in any benchmark. But that’s not the case.In this article, I’d like to address different reasons for those counterintuitive results and make a few suggestions on how to improve your benchmarking approach.First, what do we mean when we talk about a framework or language being “fast”? In terms of web services, we don’t talk about the speed of getting the response, also known as the request latency. What we usually mean is another metric, called throughput. Latency is about how much time it takes to return a response to a single request. Throughput is about how many requests can a server process in a given timeframe. Usually: within a second.Next, let’s understand where developers get the notion that Vert.x should be faster than Spring. There is a very popular benchmark for web frameworks powered by TechEmpowered, that attempts to measure the throughput of different languages, runtimes, and frameworks using a few scenarios. Usually, Vert.x framework performs very well in those benchmarks.For example in the 20th round Vert.x is 10th, with 572K requests per second, while Spring is 219th with 102K requests per second. This is very impressive indeed.But trying to reproduce those impressive results sometimes proves challenging, and hence the question from the title.Let’s try to understand what are the main flaws with the benchmarking strategy.While talking about Spring, I mean specifically Spring Framework, and not Spring WebFlux / Project Reactor, which operates differently. I’ll also assume that the Spring application is running within a Tomcat container.The ingenuity of Vert.x framework was recognising early on that the bottleneck of most real-world applications is waiting for I/O. What that means is that it doesn’t matter how well your application is written, how smart the JIT optimisations are, and how bleeding-edge the JVM GC is. Most of the time your application will be waiting for a response from the database, or from a service that someone wrote in Python or PHP maybe 10 years ago.The way Vert.x addresses that problem is that any I/O work is put in a queue. Since putting a new task in a queue is not a particularly heavy operation, Vert.x is able to process hundreds of thousands of those per second.This is a very simplistic explanation, of course. There are multiple queues, and context switches, and reactive drivers, and a bunch of other interesting stuff that I won’t cover there. What I do want you to remember though is that Vert.x is optimised for I/O.Now, let’s look at how Vert.x performance is usually tested:Let’s compare the example above with the code from the Vert.x benchmark, that still performs quite well, throughput of 4M requests per second, but not fantastic compared to some other languages and frameworks:github.comCan you spot the difference? In the benchmark most developers execute, there is almost zero I/O. There is some, yes, because getting a request and writing a response is still an I/O, but not much compared to something like interacting with a database or a filesystem.So, the advantage that a reactive framework such as Vert.x provides you is minimised by that test.If you want to see real benefits from a reactive framework such as Vert.x, write a benchmark application that does some I/O work, such as writing to a database or reading from a remote service.The way Spring Framework handles concurrency is by allocating a thread pool that is dedicated to serving the incoming requests. This is also called the “thread per request” model. Once you run out of threads, the throughput of your Spring application starts to degrade.Here we use a tool called Apache HTTP Benchmark to bombard our service with requests. The-c flag specifies to run 100 concurrent requests at the same time.You run this test on two services, one written in Spring, and another in Vert.x, and don’t see any difference in performance. Why is that?Unlike Vert.x, Spring Framework doesn’t control the number of threads it uses directly. Instead, the number of threads is controlled by the container, in our case — Tomcat. The maximum number of threads Tomcat sets by default is 200. This means that until you have at least 200 concurrent requests, you shouldn’t see much difference between Spring and Vert.x application. You simply not stressing your application enough.If you want to stress your Spring application, set the number of concurrent requests higher than the maximum size of your thread pool.Let’s go back to how Vert.x works. I’ve already mentioned that Vert.x optimizes its performance by putting all incoming requests in a queue. Once a response arrives, it is also put on the same queue. There is a very limited number of threads, called EventLoop threads, that are busy processing that queue. The more requests you have, the busier EventLoop threads become, and the more CPU they consume.What happens now when you run a benchmark on your machine? For example:What will happen next is the following. The benchmark tool will attempt to create as many requests as it can, utilising all of the CPU resources of your machine. Vert.x service will try to serve all those requests, also attempting to utilize all of the resources.To maximize the performance of Vert.x application during the benchmark, make sure to run it on a separate machine that doesn’t share CPU with the machines that run the benchmark.This brings us to the next point.I’ve been a huge fan of Vert.x for the past 5 years at least. But let’s look at the throughput of Spring application in the benchmarks we’ve mentioned earlier.Looking at those numbers, and taking into account that we usually run our services in clusters of 3 instances at least, you should be asking yourself: does my application need to handle 2K updates per second?If the answer is yes, you may need to run benchmarks from multiple machines to stress even the Spring application to the point of breaking.As software engineers, we love comparing the performance of our favorite language or framework with others.And it’s important to use objective metrics while doing so. Measuring service throughput using a benchmark is a good start, but this should be done correctly.Evaluate if the test that you are running is CPU bound or I/O bound or has another bottleneck.Also, make sure that you run your benchmarks on separate machines from those that run your application code. Otherwise, you may not be impressed by the results.Finally, I’ve seen companies hitting throughput bottlenecks of their language or framework, and even helped solve some of them. But there are many successful companies around there that may not need all that throughput, and you may be working for one of those. Getting a good benchmark is hard, and takes a lot of time to get right. Think well if that’s the most critical problem you should be solving.",,3,4,0,6,1225,816,1,1,0,7,191
,,https://medium.com/u/f387b9dead96?source=post_page-----c35fc29ccf6--------------------------------,Tapas Das,139,,,,,,0,,,,,,,0,,
Docker Desktop Alternatives for M1 Mac,,https://alex-moss.medium.com/docker-desktop-alternatives-for-m1-mac-918a2dcda10,Alex Moss,89,12,1730,"In this blog post I’m going to talk through my recent experiences as I attempted to ditch Docker Desktop — the licensing changes that come into effect at the end of January being the primary motivator.Without going into any detail about it, let’s just say I’m not a fan of taking something that you’ve made freely available previously and deciding that you now want to charge for it!In the end I tried t̶h̶r̶e̶e̶ five options for Mac — landing on one as my preference as it covered both the need to run on the newer Apple Silicon and allow mounting of volumes on the host OS, which is something I do fairly frequently (mostly to shorten the feedback loop when testing changes that run on an image intended to run in CI).Disclaimer: Most of the steps detailed below were found through following other fantastic blog posts I found out there 👏. These are of course noted wherever I’ve used them, with a few tweaks of my own I’ve made on top of these excellent guides. Hopefully having these options together in one blog post is somewhat helpful in choosing between them too!Updated 17/12/2022: Several months back I made the switch from Rancher Desktop to colima and haven’t looked back. It feels more streamlined and performant with some additional useful configuration options, as well as just less noisy. I’m now updating this post to reflect that colima is now my recommendation — although Rancher Desktop remains a perfectly viable choice (particularly if you prefer a bit of GUI action to configure things!).The remainder of the article is as it was — charting the various options I tried, but with an elaboration on colima and its benefits below, as the most recent option I’ve switched to.Sidebar: I also had to solve this problem for my Windows 10 + WSL (Ubuntu) setup. If you’d like to know more about that, I’ve covered it in a similar blog post on my own website — you can read it here.The installation is incredibly straight-forward, just brew install colima . You need the docker CLI installed too (brew install docker) if you don’t have it already also.Following installation, you then issue colima start when you want to start the daemon, and after that completes, you should find that docker commands work as normal. The first time you do this is a little slower due to downloading the image and configuring it, but following that only takes a few seconds on my machine. You can of course use colima stop to shut it down to save resources on your machine if desired between docker sessions.I’ve included some further tips n tricks for running it with better compatibility with third party tooling, as well as exploring some of its other options in my own blog site. This link should take you directly to the relevant bit: https://alexos.dev/2022/01/02/docker-desktop-alternatives-for-m1-mac/#recommended-option---colimaAlternatively, see their source on Github for additional configuration options as needed: https://github.com/abiosoft/colima.I started here. This is a simple near “drop-in” replacement for Docker Desktop, but does not work on M1 Macs. I used this on my older Macbook for a little while before replacing it with Rancher Desktop. It’s fully docker compliant, if there is such a thing.The instructions that follow are heavily based on this excellent blog post, which has some additional advice, especially if you want to get more out of the local Kubernetes cluster:I have the minikube start command set up in a start-docker.sh script I can run when needed, and the minikube docker-env in my shell startup (.zshrc, in my case).As you can see, pretty straight-forward standard brew installation stuff — plus a couple of commands to run before you try and do docker things (I personally never had Docker running all the time on startup anyway, as it was such a battery drain). As it’s still just the same docker CLI, the credentials helper to connect to a private registry also works fine out-the-box.However, volume mounts from the host did not … but thankfully the blog post I linked above has captured the solution for this. You can minikube mount to spin up a process to mount your local directory into the minikube VM:In my opinion, the advantages of this option — and why I kept it as the setup on my older Macbook — are:Downsides:However, I also needed an option that worked with Apple Silicon. My first attempt was with Podman …After realising that hyperkit didn’t work on M1, this was the next option I tried. I’d heard good things. It mostly worked fine but, as mentioned earlier, for me the crucial issue was the lack of ability to mount volumes from the host OS. I use this option a lot.That said, if that’s not important to you or they fix it subsequently, I’ve included the steps below. these were cobbled together from the Podman installation guide itself plus this great blog post — although I didn’t need most of the complexity involved here (I’m guessing it has been fixed since).From their install guide — things are nice and simple:Your docker equivalents should then work as intended:Other similar docker commands I tend to use also seem present:However, as mentioned earlier this crucially does not work:I looked around this topic a bit and there are some suggested workarounds, such as this one to mount the directory onto the podman VM first. But these look quite hasslesome (caveat: I didn’t try very hard 😉)I therefore backed away at this point as I had another option to try first … Enter lima + nerdctl …This option ticked all the boxes for me and I ran with it for a little while without issue, although with more setup needed than the minikube option. I’m comfortable with that though. I like this because it: a) distances me from Docker Inc. changes to licensing in the future (and a little bit on principle, not gonna lie!), and b) puts me closer to the OCI runtime of our Production Kubernetes clusters (they’re GKE, which just run containerd by default now).I followed the great guide in this blog post, which basically boils down to:Your docker equivalents then look like this (which can of course be aliased):Other similar docker commands also seem fine, just like podman:Crucially, this worked too without any special config or setup needed:That said, there were a couple of other steps I needed to go through to deal with my other requirements.Because it’s not Docker, the existing credentials helper I had setup to connect to e.g. Google Container Registry did not automatically work. Instead, these credentials need to be readily available on the intermediary lima VM, rather than the host. To solve this, I opted to jump onto the VM and install gcloud, login as I normally would, then ensure those credentials were available to the root user.To do this, we start with limactl shell default which should get you a shell prompt on your default lima VM. We then download and unpack the GCloud SDK:We then ensure the required binaries are in the path, and login:Unfortunately, we’re not quite there yet — but nearly! Back on the host machine, spinning up my Ubuntu docker image was met with an error message I’ve seen a few times before on Apple Silicon: standard_init_linux.go:228: exec user process caused: exec format error. We need to do a bit of work to give QEMU (the hypervisor behind the scenes) the option to execute non-native images.Thankfully the nerdctl docs point you in the right direction on this one, via this super-useful emulator. We therefore do the following:… et voila! Our ubuntu image built on amd64 in a private container registry with a local host volume mount works without issue 🎉:A small note: if you need that local directory to be writable by the container, you need to edit the file ~/.lima/default/lima.yaml on your host. There’s a mounts: section where you can choose to make your home directory and everything in it writable (dodgy if you run untrusted containers!), or add a block to a separate mount path, similar to the one already there for /tmp (the option I took!).All that’s left is to add the limactl start default to your startup script and alias lima nerdctl to something — you can even alias this to docker if you wish (although I personally prefer to be more explicit — I chose to alias it to nerd 🤘).In my opinion, the advantages of this option are:The downsides:Updated 14/01/2022: So we’re done right? Lima + nerdctl does the trick? As mentioned back at the top — whilst that was my preferred option for a while, I’ve recently discovered Rancher Desktop, and it’s latest version (0.7+) introduced Apple Silicon support.This used to be the option I used on both my MacOS machines for quite a while, before switching to colima as described near the top of this article. Rancher remains a perfectly fine choice, with good compatibility.Why did I use this option over the rest?The installation process is extremely simple with only a few choices to make (it does most of it behind the scenes — and makes use of lima to do it on Mac). You will be prompted to elevate your access a few times, which is understandable given what you’re setting up here.I opted to use the recommended stable Kubernetes release and the dockerd/moby engine — but I really like that it offered me the choice of containerd (nerdctl) in case that becomes handy in the future (and I understand from the docs that they can coexist too).I did hit a couple of small issues that were local to my device, and probably a result of my various experiments on this topic! Here’s a couple of quick bits of troubleshooting advice if you need it:I hope you found this article useful — I’ve presented a range of options to show my thought process, and am recommending colima (https://github.com/abiosoft/colima) as the pick, with Rancher Desktop (https://rancherdesktop.io/) as a viable alternative.Hopefully you found this run through the steps useful for your particular setup. As always with these things — and indeed in my own experience following the existing advice out there — it may not work flawlessly on your kit.If you find any issues, do let me know via the comments — I’d be interested to keep this post up to date with any additional advice over time too!",,4,14,14,7,954,583,8,6,0,24,334
The Life of a Bytecode Language,Better Programming,https://betterprogramming.pub/the-life-of-a-bytecode-language-fca666928e7b,Andy,146,3,58,"What would happen if programming languages were kingdoms? Will there be a war between them? Who will win?If you want to know, here’s a look at the comics I drew：Since then, the programming world became peaceful again. But will the peace be eternal? Which could be the next balance-breaking language (or kingdom)? Let’s wait and see.Long live Byte-code!",,5,0,0,0,1050,1009,26,0,0,0,479
Writing Robust and Error-Free Python Code Using Pydantic,Better Programming,https://betterprogramming.pub/writing-robust-and-error-free-python-code-using-pydantic-151a135a9ff0,Haseeb Kamal,283,6,1150,"Python is a dynamically typed language which means that type checking is performed at run-time (when executed). If there is an error in the code it will be thrown at execution. Languages such as Java, C# and C are statically typed meaning type checking is performed at compile-time. In this case, the error will be thrown before the program is run.In a statically typed language, the type of constructs cannot be changed. The compiler needs to know the types beforehand. A variable declared as an int in C for example cannot be changed to a string later.We can do this in Python however:This enhanced flexibility means that dynamically typed languages are slower at execution than statically typed ones. A lot of checking has to be done at run-time to figure out the type of variables and other constructs so that the program can be executed. This creates overhead.Now that Python is the go-to language for machine learning there is an increasing use-case for developing APIs and web applications that serve machine learning models. It is a lot simpler to have a single language for creating models and wrapping them up in a user-facing application than a variety of languages.However, for these full-stack applications, the chances of type errors increase when type checking is performed at run-time rather than compile time. This is where Python type hinting helps. It allows us to declare types of programming constructs directly in our code.First, we look at basics of type hints.To define a type hint for a function argument we can write a : (colon) followed by the type after the variable name. For non-singular types such as Lists, Dicts, Sets we need to import the typing package.Let’s look at some code:We define a function get_stuff() that appends the provided item to the item list fridge. Afterward, all items in the fridge are capitalized.The code works as expected returning the list of fruits:Since we define fridge to be a list of strings VS Code (with PyLance and Python extensions installed) provides instant code completion. If you type fridge. notice how the suggestions pop up:Similarly, as we have defined fridge to be a list of strings, we can write x. to see all operations possible on every item in the fridge which is a string:As you can see, type hinting saves a ton of time as there is no need to go back and forth looking up methods and attributes from online documentation.Data validation and settings management using python type annotations. pydantic enforces type hints at runtime, and provides user friendly errors when data is invalid. Source: PydanticAlthough Python supports type hinting, this is not enforced. So passing an object of an incorrect type is still possible and would cause an error if an unsupported operation is attempted. For example, attempting str operations on an int type. Pydantic is a Python library that enforces this, meaning it circumvents such errors.Let's see an example to consolidate this point.Let’s say we get some bad input to our function and the fridge contains an int along with the strings.The rest of the code remains unchanged and we call get_stuff() with the modified fridge:What happens?We get the following runtime error:Even though we declared x to be of type str the get_stuff() function happily accepts a List with one int element and toUpper() attempts to call capitalize() on the int object.At this point it may seem like the benefits of type hinting are limited to auto-completion only.We can refactor the code to use Pydantic. We define a data model that inherits from a Pydantic BaseModel. This is the main way to create data models in Pydantic. Since this is a blueprint for how our data should be represented, we define it as a class.Go ahead and install Pydantic with:Then define a Frdige class that inherits from BaseModel like so:We give the Fridge class an attribute called items which will be a list of strings.We create an instance of a Fridge and pass it as an argument when we call the get_stuff() function.The refactored code looks as follows:If we now attempt to run it again you will notice the code is error free!The int gets casted to a string object and appended to the list giving the following return object:You will also notice that we pass a Python set instead of a list when we create an instance of a Fridge object. Here again, Pydantic takes care of casting the set to a list!You might be wondering what should be done if we do wish to have a list of mixed types such as a list that contains either strings or integers. For that we can use the Union type annotation which acts like a logical OR. For example the Fridge would be defined as follows:Passing the following list to Fridge would now work:Please note that Pydantic gives precedence to the first type listed in the Union. So if we had instead written:Then the int in the passed list would be casted to a string even though int appears in the type annotation. This would give (which is not what we want):Ok, we have covered a lot of ground! But there’s one more thing to look at.Pydantic really shines when it comes to modelling more complex data types. For that we need to look at recursive models.It is also possible to define recursive models in Pydantic for more complex data models. A recursive model is a model that contains another model as a type definition in one of its attributes. So instead of List[str] we could have List[Cars] where Cars would be a Pydantic model defined in our code.Onto another example!Let’s assume we also want to store the number of each fruit in the fridge. To do this, we create a Fruit data model:In the Fridge data model we can define the list to be a list of Fruits instead of a list of ints:The full code is as follows:We call get_most_fruits() with a Fridge object containing a list of Fruit objects. Pretty straightforward.We wish to return the fruit with the highest number. Before doing operations on the list of fruit we use the jsonable_encoder() method to convert the list into a JSON compatible type. If we hadn't done this, then an element in the list would be of type Fruit which cannot be operated on.After the encoding stage, we get a list of dict objects with key, value pairs corresponding to the nameand numfields defined in the Fruit class.We can now sort this list and return the fruit with the highest number.In this post, we had a recap of dynamically and statically typed languages. We looked at type hinting in Python and the use of Pydantic to enforce the type hints.To conclude, type hinting helps:Hope you learned something useful in this post.Next time we will look at FastAPI, a popular Python web framework that fully supports Pydantic.Originally published at https://haseebkamal.com",,2,12,1,6,942,393,4,1,0,2,132
,,https://medium.com/u/b2da04cd555b?source=post_page-----c35fc29ccf6--------------------------------,Haseeb Kamal,283,,,,,,0,,,,,,,0,,
,,https://medium.com/u/888dfa574016?source=post_page-----c35fc29ccf6--------------------------------,Hahnbee Lee,112,,,,,,0,,,,,,,0,,
,,https://medium.com/u/2c8aac9051d3?source=post_page-----c35fc29ccf6--------------------------------,Logan Kilpatrick,1000,,,,,,0,,,,,,,0,,
Too Lazy to Write Documentation? Let the AI Write It for You,Better Programming,https://betterprogramming.pub/too-lazy-to-write-documentation-let-the-ai-write-it-for-you-8574f7cd11b2,Hahnbee Lee,112,3,427,"I’ve never met a developer that enjoys writing documentation. At the very least they understand the value of it and will begrudgingly write it, but will never enjoy the process of writing it.Some people go by the philosophy that good code should document itself, but if this were true then why is that one person who is familiar with the entire codebase so valuable to a team? There is a lot of knowledge, reasoning, and context that cannot simply be deduced from raw code. Good documentation that’s well-maintained only adds value and context to a codebase.This particularly holds for untyped programming languages. The one language that sticks out to me is Python. Although it’s considered extremely “readable”, many times I find myself trying to deduce the type of a variable or parameter by looking for other places in the code where it’s used.Here’s an example of a grammar object:Here is the same object with the documentation that it came with:Without the documentation, it’s nearly impossible to deduce the type/structure of the rules property.AI Doc Writer for Javascript, Typescript, Python, and PHP is a VS Code extension that generates documentation for you using AI. The way it works is that you select the code you want to document and you press the ‘Generate docs’ button or hit the keyboard shortcut Cmd/Ctrl + .Here it is in action:Now, how well can AI understand your code?Let’s look at some examples.Here’s the code from the above demo and the documentation that the AI generated:The AI gave a concise summary of each function and short descriptions of the parameters.Let’s see how it fares with some Javascript code:This output was particularly interesting to me because it was able to conclude that 15 was divisible by both 5 and 3 instead of stating that it prints “FizzBuzz” when i is divisible by 15. Then, this got me thinking how else could I test it — does it know simple geometry equations?It does! It didn’t say “Multiply PI with r to the power of 2"" AKA it didn’t explain what the code does verbatim, but rather it was able to intelligently deduce that these variables were calculating geometry equations.Overall, AI Doc Writer can save developers a lot of time while increasing the quality and readability of their codebases. It gives developers the power to intelligently explain and provide context to functions and code snippets in under a second.Maybe it’s time to skip out on the dread of writing documentation and let AI do it for you.Take a test run yourself! Head here to get access.",,14,0,0,0,1012,650,2,0,0,4,
,,https://medium.com/u/17016fd04e22?source=post_page-----c35fc29ccf6--------------------------------,Alison Yuhan Yao,250,,,,,,0,,,,,,,0,,
,,https://medium.com/u/fba05660b60f?source=post_page-----c35fc29ccf6--------------------------------,Moez Ali,7000,,,,,,0,,,,,,,0,,
,,https://medium.com/u/8ef9cb95132c?source=post_page-----c35fc29ccf6--------------------------------,Thomas Dimnet,104,,,,,,0,,,,,,,0,,
"IDEs, IDEs on the wall, which is the best of them all?",,https://medium.com/u/5fa30a3e12cb?source=post_page-----c35fc29ccf6--------------------------------,Andy,146,,0,,,2,0,0,0,1181,1772,1,0,0,2,91
A Tale of Two Engineers Discovering the Crystal Programming Language,Better Programming,https://betterprogramming.pub/a-tale-of-two-engineers-discovering-the-crystal-programming-language-104b1fdbe525,Thomas Dimnet,104,8,1747,"Ever heard of The Pragmatic Programmer? It’s a famous book written by David Thomas and Andrew Hunt. Both authors give advice on how to improve your skills as a developer and write better programs.To me, it’s one of the best programming books out there, along with Clean Code: A Handbook of Agile Software Craftsmanship and Design Patterns: Elements of Reusable Object-Oriented Software.One piece of advice, in particular, caught my attention while reading it: “Learn continuously”. According to the authors, you should learn one new programming language per year.For example, if you are a front-end developer and have no knowledge of back-end development, you should learn Go or Python. Or, if you already know a backend programming language, you can turn to Rust or Elm. Learning it doesn’t mean you’ll use it on a daily basis, just that you’ll learn new approaches to common problems and new concepts, for example, functional versus object-oriented programming languages.This is how I learned programming languages such as Swift, Python, Rust, and recently Crystal. A few weeks ago, a colleague, Matthieu Hermitte, came to me with a problem. He wanted to display markers on a Google Maps-like application in Datadog. Each marker requires latitude and longitude to be displayed on the map and this data was collected based on the user’s IP address.This is the first piece of a series of blog posts, in which you’ll read about Crystal. The second will be about how to create a custom Datadog Widget that displays an OpenStreetMap with custom markers. You can find the source code of the project on GitHub.No prior knowledge of Crystal is required. We will cover all the basic and more advanced concepts. However, you should have a basic knowledge of at least one programming language. For example, I will not go over what a function or a variable is but rather how these parts are implemented in Crystal.When I showed this project to my developer friends, they all had the first same question: why Crystal? Why did you choose Crystal instead of another, more popular programming language? Does Crystal offer something that other existing programming languages do not? Is it more flexible, faster, easier to use, etc. than Go, JavaScript, or Python?“Fast like C, slick like Ruby”: it is what I read when I discovered Crystal. Crystal is indeed all about performance. It gives you the performance of the C/C++ with the syntax of Ruby. So you don’t have to choose between performance and readability and you could create applications that do both.There are many other advantages to using Crystal and I suggest you look at its official documentation. I particularly like the two example projects the documentation lets you build: a small HTTP Server and a CLI. It’s a perfect place to start, using a real-world use case/scenario rather than a boring and useless “Hello, World”. Yes, I’ll always print a “Hello, World” at the beginning when trying to build something but it’s just the first step.The syntax of Crystal is really great. As the project’s home page says: if you know and like Ruby syntax, you’ll enjoy coding in Crystal. Static typing and object-oriented concepts are also present. I haven’t tried them yet but I will definitely give them a try next time.In 2017, Crystal hit an all time popularity and was at number 32 in the TIOBE index. I looked at it before writing this article and I could not find Crystal. It seems it is not as popular as it was in 2017/2018. This decline in popularity comes with other potential problems.I also had trouble finding sample projects or code bases. The same goes for answers to common problems on StackOverflow. For me, this is a big drawback. If you compare this to any other programming language, even Rust, you’ll find more answers to your problems. For example, I had a hard time finding how to create functions in Crystal, and even today, I am still not sure where I can find that information.There is the same problem with the dependencies, or Shards in Crystal. There are some very interesting shards, I even found some Datadog shards. But they are not really maintained and there are too few of them. I know it’s kind of a win/win or lose/lose situation: the more popular you are, the more Shards people will build.Anyway, even though Crystal is not maybe as popular as it was a few years ago, it does not mean that we should not use it. As I told you at the beginning of the article, it is always interesting and exciting to learn a new language. It’s like learning a new musical instrument but with all the knowledge you gained from your previous attempts. Now that you know more about the context of the project, it is time to dive into the project itself!When you started reading this post, I mentioned the project Matthieu and I decided to build a simple Proxy Server in Crystal. This proxy would communicate with IfConfig.co and pass it the IP address. IfConfig would then return a response with the latitude and longitude and we will send it back to the client.Here is how it works in just an image:Once we get the response from ifconfig, we pass it to the web browser. It’s that simple!Before looking at the application code, a few words about ifconfig.co. It’s a web service that allows you to find your IP address and information about it. You are able to get a lot of information about, for instance, country, region, city, latitude and longitude, user agent, etc. Please note that the source code and documentation are publicly available on GitHub.If you want to run the project, you can find its source code on this GitHub repository. Clone it and then go to the examples/geomap folder. There are three folders but we will use only what is inside the crystal-api folder. We will also use the docker-compose-api.yml file. If you are wondering what is inside the two other folders, those will be covered in the second blog post.If you want to launch the project on your machine, run the following commands:Let’s start with how to install Crystal on your machine! Depending on your operating system, you can install Crystal in different ways. You can check the installation steps in the documentation.However, I think this is one of the main problems: the Crystal compiler does not run natively on Windows. It was already the case in 2018 and it still is today. You can use the Windows Subsystem for Linux, but I think you need Windows 10 or 11.Or you can use one of the developer’s best friends: Docker. There are different versions of the Docker images, including an alpine one. However, I find the documentation on DockerHub to be a bit light, especially when compared to Python or NodeJS images. It might be interesting to have more information and guidance about the different types of images.Let’s now break down the code in our main.cr file and examine it element by element.Here is how to import, or require, a file or a library in Crystal. http/server and json are built-in libraries and there is no need to install them. However, if you need to install a dependency, you will need to update the shard.yml file.Once this is done, run the shards install command. Not only will it install the dependencies, and, you know the drill, the dependencies of the dependencies, but it will also generate a `shard.lock`. The shards.info website contains a list of repositories, including and recent and popular ones. If you want to know more about all the shards commands, here is the reference.You can create local or global variables without having to specify their type. Local variables start with lowercase letters. Their type is inferred from their use, not just their initialiser. In the code snippet above, we are looking for environment variables. If they are defined, we use it, otherwise we set default values. These environment variables are defined in the environment properties of the docker-compose-api.yml file.Creating a function, or a method, is the same as in Ruby: you must use the def keyword at the beginning, and end at the end of the function. Note also that the keyword return exists but it is not required and you can omit it if you wish. You can learn more about the method here in the official documentation.In the code example above, we access properties and methods within objects by using the dot notation.The whole project consists of creating a proxy server that will communicate with ifconfig.co. To create a web server in Crystal, you first need to import the http/server dependency, then init it, bind a port, and finally launch it with the server.listen.Being able to write control flow, or if, else if, and else, in programs is one of the first things we usually learn. In Crystal, there is no need for parentheses when testing conditions. The switch operator is called case where until is just syntax for a while condition. The documentation contains a lot of information about Control expressions, you can find it here.If you want to catch errors in your code with try...catch blocks, we will have to use the begin...rescue syntax. It’s the same as in Ruby. The project is using it to parse JSON data. If you want to learn more about exception handling, you can look at the documentation here.Finally, before we get into more advanced concepts, if you want to know how to log information, as print in Python or console.log in JavaScript, you use the keyword puts. ""puts ""My variable is #{my_variable}""” lets you add a logging statement. However, I could not figure out how to create proper logging in Crystal.There are still things that were not covered in this article. For example, Crystal is an object-oriented programming language and we haven’t covered that in this blog post. The same goes for the frameworks and libraries. Matthieu and I are currently working on other projects in Crystal and we’ll continue to add to this series as we go.In the meantime, if you are already hungry for more, here is a tutorial that will explain how to create a Simple Static File Server in Crystal. The Getting Started tutorial also offers two small interesting projects in Crystal.If you have any questions, don’t hesitate to ask them, I’ll be more than happy to discuss them with you! Until next time, thanks for reading and goodbye!",,,0,2,3,1070,428,3,1,0,18,93
6 Julia Frameworks to Create Desktop GUI’s and Web Apps,Towards Data Science,https://towardsdatascience.com/6-julia-frameworks-to-create-desktop-guis-and-web-apps-9ae1a941f115,Logan Kilpatrick,1000,5,747,"Julia is used for a lot of deeply technical applications like Machine Learning and Data Science. But as a general-purpose programming language, Julia can also be used for things like building websites and interactive visualizations. In this article, we will go over 5 Julia packages that can be used to create desktop GUI’s or web applications.Edit: My Co-author and I are thrilled to share that pre-orders our new book, Julia Crash Course, are now live:logankilpatrick.gumroad.comGenie.jl is a pure Julia web framework based on Django. From the Genie website:Genie Framework includes all you need to quickly build production ready web applications with Julia Lang. Develop Julia backends, create beautiful web UIs, integrate with databases and set up high-performance web services and APIs.Like Django, Genie is not just a stand alone-package, it is an entire ecosystem! Let’s look at a basic hello world example with Genie:As you can see, Genie follows a similar design pattern to Django and comes with features like a web server, templating engine, cookies, encryption, authentication, a routing engine, backend views written in Julia, and much more!If you want to build modern web applications and are familiar with Django, Genie is the right place to start! You can find out more here: https://genieframework.comFor a comprehensive video tutorial on Genie, check out:Gtk.jl is a Julia package built on top of the very popular GTK windowing toolkit. You can find the getting started manual here: https://juliagraphics.github.io/Gtk.jl/latest/manual/gettingStarted/Let’s look at a simple example:First, we set the name of the window and the dimensions. Then, we create a button object with a specific text label and push it into the app. Last, we display the app by calling showall.Gtk.jl has been used to build some very cool applications. I highly suggest checking out this video:Makie is one of the most loved visualization packages in the Julia ecosystem. There is an incredible depth to what you can build.Makie allows you to build interactive visualizations that run on the GPU and can also be run in your browser. The Makie docs also recently went through a large update so the content there is up to date and very helpful: https://makie.juliaplots.org/stable/I would also suggest checking out https://lazarusa.github.io/BeautifulMakie/ which is a gallery of lots of really nice animations built using Makie.Blink.jl is the Julia wrapper around Electron. It can serve HTML content in a local window, and allows for communication between Julia and the web page. In this way, therefore, Blink can be used as a GUI toolkit for building HTML-based applications for the desktop.I could not think of a better way to re-state this, so the above is a quote from the Blink docs. What makes Blink different from other packages is that you can build HTML-based GUI’s using it.We start off by creating the Electron window, then appending in some text to the body tag of the window, and finally, loading a URL in the window. If you run this code locally, you will see that the window changes dynamically as we execute these commands.While I don’t think Blink is widely used, I have had a lot of fun playing around with it on various projects.Dash is a Julia interface to the Dash ecosystem for creating analytic web applications in Julia without requiring JavaScript. This means you can build impressive dashboards like https://covid-county-dash.herokuapp.com and deploy them with ease!You might also want to check out the Dash site https://dash.plotly.com/julia for details on getting started. There is also comprehensive documentation available on Dash.jl: https://github.com/plotly/Dash.jlRight now, the best way to create a desktop app that can be shared and run on computers without Julia installed is to use PackageCompiler. PackageCompiler allows you to take an entire Julia project and compile it to an exe file. This process bundles all of the dependencies together into a single file to make it distributable.I will note that there are currently limitations concerning what you need to do to create an exe. There might be some code re-writing that is required to make it compatible. For a step by step walkthrough on using PackageCompiler, check out:And you can read more about Package Compiler in the docs: https://julialang.github.io/PackageCompiler.jl/stable/apps.htmlThe 5 packages highlighted above are just a small sample of the packages available today in the Julia ecosystem. Below, I will add a semi-exhaustive list of all the visualization packages I can find in case the first 5 did not suit your use-case:Am I missing one? Feel free to comment on this article and I will append the package to this list!",,1,3,0,1,840,632,4,1,0,18,451
PyCaret 2.3.6 is Here! Learn What’s New?,Towards Data Science,https://towardsdatascience.com/pycaret-2-3-6-is-here-learn-whats-new-1479c8bab8ad,Moez Ali,7000,6,623,"PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the experiment cycle exponentially and makes you more productive.By far PyCaret 2.3.6 is the biggest release in terms of the new features and functionalities. This article demonstrates the use of new functionalities added in the recent release of PyCaret 2.3.6.📚 Official Docs: The bible of PyCaret. Everything is here.🌐 Official Web: Check out our official website😺 GitHub Check out our Git⭐ Tutorials New to PyCaret? Check out our official notebooks!📋 Example Notebooks created by the community.📙 Blog Tutorials and articles by contributors.❓ FAQs Check out frequently asked questions.📺 Video Tutorials Our video tutorial from various events.📢 Discussions Have questions? Engage with community and contributors.🛠️ Changelog Changes and version history.🙌 User Group Join our Meetup user group.Installation is easy and will only take a few minutes. PyCaret’s default installation from pip only installs hard dependencies as listed in the requirements.txt file.To install the full version:This function will generate the interactive dashboard for a trained model. The dashboard is implemented using the ExplainerDashboard.Video Demo:This function will generate automated EDA using the AutoViz integration.Video Demo:This function will transpile trained machine learning models into native inference scripts in different programming languages (Python, C, Java, Go, JavaScript, Visual Basic, C#, PowerShell, R, PHP, Dart, Haskell, Ruby, F#). This functionality is very useful if you want to deploy models into environments where you can’t install your normal Python stack to support model inference.Video Demo:There are many approaches to conceptualizing fairness. This new function follows the approach known as group fairness, which asks: Which groups of individuals are at risk for experiencing harm. This function provides fairness-related metrics between different groups (also called subpopulations).Video Demo:This function will create a POST API for the ML pipeline for inference using FastAPI framework. It only creates the API and doesn’t run it automatically.Video Demo:This function will create a Dockerfileand requirementsfile for your API end-point.Video Demo:This function creates a basic Gradio web app for inference. It will later be expanded for other app types such as Streamlit.Video Demo:A new parameter called drift_report is added to the predict_model function that generates the drift report using Evidently AI framework. At the moment this functionality is in experimental mode and will only work on test data. Later on, it will be expanded for production use.Video Demo:plot_model function is PyCaret is now more configurable. For example, previously if you wanted to see percentages in Confusion Matrix instead of absolute numbers, it wasn’t possible, or if you want to change the color map of visuals, it wasn’t possible. Now it is possible with the new parameter plot_kwargs in the plot_model function. See example:This is not a new function but it was completely revamped in 2.3.6. This function is to optimize the probability threshold for binary classification problems. Previously you had to pass cost function as true_positive , false_positive , true_negative , false_negative in this function and now it automatically picks up all the metrics including the custom ones from your active experiment run.The biggest and hardest of all is the completely new documentation. This is a single source of truth for everything related to PyCaret, from official tutorials to release notes and from API ref to community contributions. Take a video tour:Finally, if you want to take the tour of all new functionalities added in 2.3.6, watch this 10 minutes video:To learn about all the other changes, bug fixes, and minor updates in PyCaret 2.3.6, check out the detailed release notes.Thank you for reading.I write about PyCaret and its use-cases in the real world, If you would like to be notified automatically, you can follow me on Medium, LinkedIn, and Twitter.",,8,12,50,0,1062,493,13,0,0,24,609
Bechdel Test: Comparing Female Representation Metrics in Movies,Towards Data Science,https://towardsdatascience.com/bechdel-test-comparing-female-representation-metrics-in-movies-6cbade15010f,Alison Yuhan Yao,250,14,2610,"· Data Collection ∘ Using an API to fetch data ∘ Data Wrangling ∘ Dataset Update· Data Analysis & Visualization ∘ Understanding the Bechdel data ∘ Comparing on-screen & off-screen metrics· Conclusions· References & Related ReadingsAs a big fan of movies and TV shows, I was intrigued when I first learned about the Bechdel Test in class. It measures the representation of women in fiction by checking 3 criteria:This test was invented by Alison Bechdel in 1985, but it is still relevant in the present day. I expected it to be a simple test that all movies should pass, but is that the reality?Before knowing what the Bechdel Test is, I would think about metrics such as if the director is a woman or the percentage of female cast members when talking about female representation in movies. These are off-screen representation metrics, while the Bechdel Test gives us a guideline for on-screen female representation, something that is rather difficult to quantify.Therefore, to further investigate this topic and bridge the gap between on-screen and off-screen metrics, I obtained the Bechdel Test data on 9,300+ movies from this amazing website called Bechdel Test Movie List to answer the following questions:Check Kaggle for the dataset. Code in this post can be found in this GitHub repo.All credits of the Bechdel Test data go to Bechdel Test Movie List, which provides a handy API for anyone to retrieve the raw data. The data comes with a CC BY-NC 3.0 license. We’re grateful to bechdeltest.com for the permission to use the data in this post.The API documentation gives us 4 methods to call the data:We can see right away that simply using one method will not give us all the information in the database. A dataframe of all information should probably contain all movies with 9 columns, so what are the missing 3 features from method #4 getAllMovies? Well, let’s check what getAllMovies really get us.The movies are added chronologically, so the more recent ones are at the bottom. And we can see that Cruella does not have an imdbid, which is a problem we need to fix later.The useful information method #4 getAllMovies gives us are movie title, IMDb id, unique website id (id), Bechdel Test score (rating), and year of release. The Bechdel test score, or rating, is calculated by checking the 3 criteria. Since each criterion is built upon the previous one, ie. a movie cannot fulfill criterion #2 if criterion #1 is not met, a score of 0 means a movie does not have 2 female characters. 1 means a movie has 2 female characters but they do not talk to each other. 2 means a movie has 2 women talking but they talk about men. 3 means completely passing the Bechdel Test. So, congrats to Cruella, West Side Story, Every Time a Bell Rings, and Single All The Way!Now let’s see what method #1 getMovieByImdbId gives us.It does not have an index but has 4 additional columns: visible, date of the movie being added to the list, dubious, and submitter id. Visible is always 1 for every movie because only the visible movies are returned by the API call. What is interesting is the dubious column. It indicates “whether the submitter considered the rating dubious”. In other words, we cannot trust the ratings of dubious movies, as they are susceptible to modification.And this complicates things… dubious is now too important a column to ignore. We may discard the dubious movies, or we may treat dubious as another category. Either way, the df we have now needs a new column — dubious. And that took me an extra 7 hours.Now that we have the IMDb id of each 9,300+ movies, we can use it to get the full information on each movie, which means we need to call the API thousands of times. The website states that:Please keep in mind I’m running this site on a shared hosting plan, so if you send lots of queries in a short time, you might get me in trouble. Please be nice and definitely don’t use this data on anything with a lot of traffic.I don’t want to cause any trouble, so I called the API every few seconds (hence it took me 7 hours). I did experience some timeout errors and unstable internet (that’s my bad), so maybe a few seconds was still too frequent. But thankfully, the website did not crash. For this reason, I suggest checking out this Kaggle database if you want to use the dataset and avoid calling the API again and causing more unnecessary traffic.But the code to get the extra 4 columns is here:However, we are not done yet. If we check the Bechdel_detailed.csv file, we will see 3 new columns and some NANs.A lot of dubious are NAN because the website returns null in their API, but there are 9373–9369=4 movies that seems strange. Let’s take a look at them.No surprise here because index 9369 is Cruella, which does not have an imdbid. We expected it to cause problem and now it’s time to fix it. We can go to IMDb and manually get the imdbid. Now, just a bit more data cleaning and we are done.The current Bechdel_detailed.csv file should look like this. It contains 9,373 movies from year 1874 to 2021.The website is updated quickly and the analysis here is based on the data from Dec. 23, 2021. Please check Kaggle for the latest dataset (I intend to maintain it quarterly).As always, for any dataset, we start from exploratory data analysis (EDA) and visualize the data to get a sense of what we are dealing with.Continuing where we left off, let’s first import more libraries and check the basic information of bechdel_detailed_df again.There are some dubious = NaN in the dataset, but not too many, so we can go ahead and drop them.Now, we have 9,074 movies in total. We also need to check duplicates and drop the 9 duplicated movies.Okay, we can start to visualize. Since I have been learning R in the past few months (my native language is Python), I have started to love the ggplot2 style, so I chose to use a mixture of matplotlib and plotnine for visualization.First, I am curious about the score distribution and percentages in the dataset.More than half of the movies pass the Bechdel Test, which is quite disappointing considering the Bechdel Test does not seem too difficult to pass. However, fivethirtyeight says that the ~56% passing rate is already higher than expected [1]. They also point out a “feminist-leaning” problem, which means that people subconsciously pick the movies that are more likely to pass the Bechdel Test, because they know in advance that they are going to submit a score to the Bechdel website [1]. Also, it is not difficult to see that most of the movies in the database are popular Hollywood movies, which puts a geolocation restriction to our analysis as well.It is important to acknowledge the biases in datasets and EDA helps us do that.Let’s continue by dealing with dubious movies. Please recall that dubious movie scores are susceptible to changes and we have dropped the rows with dubious = nan, so now we are interested in the movies marked dubious = 1.Percentage of dubious movie scores: 8.92%~9% is not too bad, but I don’t think we should drop the dubious movies right now. Instead, I intend to treat it as a new category at the same level as bt_score = 0, 1, 2 and 3. Let’s create another column called “category” and mark the 5 possibilities:Alright, let’s go on to analyze the trend over the years. We can calculate the mean score of each year and visualize in a scatter plot.I added a smooth curve so that it is easier to see the trend. Movies in the early years are performing extremely poorly, but the mean score is improving over time. Recent years have seen an all-time high.Is it because the proportion of movies passing the test is increasing? Let’s find out.I chose to use an animated pie chart for visualization because it shows the time flow nicely (fitting ~150 years in a bar chart looks terrible). Plus, it’s good to practice something new. And this time, I color-coded the 5 categories.We can see that the early years are all orange, meaning that 0 movies pass the test. But over the years, more green color is popping up, meaning that more movies pass the test. However, the green proportion is unstable, because the interval of 1 year is too small. So, let’s use an interval of 10 years instead. And this time, we can finally fit everything in a bar chart.Dubious movies are in the middle so that the human eyes can better compare the green and the orange proportions. We can see that as the green proportion is getting bigger, the orange proportion is getting lighter. That is, many movies still fail the test, but more are getting 1’s and 2’s instead of 0’s, which shows progress. Yay!Now, I want to take some time to emphasize that the terms “more” or “fewer” here are all in terms of proportion, or percentage, or ratio. They do not refer to the pure number, or volume, or quantity of the movies. Comparing numbers is meaningless. Why? Because of the population effect, or the size effect.For example, there might be more movies (in terms of number) passing the Bechdel test in the year 2122 than 2022 simply because the year 2122 has 10 times more movies released than 2022. The proportion may drop, even if the number rises, so numbers alone do not tell us much useful information. Another example I heard recently is that a friend of mine is doing NLP and he found that the negative comments in sentiment analysis tend to be shorter, but the reason could be there are more short comments (in terms of number) on the internet in general, so his conclusion might not be meaningful. This pitfall has the term “population” in it because it is commonly associated with population. China has more births than Japan simply because China has a larger population. This is not interesting. What is interesting is the birth rate, not the birth number. Similarly, we talk about GDP per capita, not GDP as a whole. It is surprising how often we misinterpret the population effect as something meaningful.Okay, let’s get back to the analysis. Now that we have a pretty good understanding of the Bechdel data and the general trend over time, let’s compare it with off-screen metrics.By off-screen metrics of female representation in movies, I mean the female ratios in cast and crew members. To get the ratios, we can use this popular Kaggle dataset. The credits.csv file marks the gender information.To join the Bechdel data with the gender data, we need the links.csv file.There are some empty values in cast and crew. Let’s calculate the percentage of empty values and see what we should do.Empty cast percentage: 0.41% Empty crew percentage: 0.11%This is a very small percentage, so we can go ahead and drop them. Also, we need to drop duplicates.However, there is another problem of unknown genders in the Kaggle dataset. The original data source of the Kaggle dataset did not keep a detailed record on the gender information. In fact, there are a lot of unknowns.Percentage of unknowns in Cast: 36.29% Percentage of unknowns in Crew: 59.01% Percentage of unknowns in Directing: 39.44% Percentage of unknowns in Writing: 38.91%Since there are way too many unknowns, we can fill in the blanks by predicting gender from the first name. The gender-guesser package is a good choice [2]. This package treats gender as binary (could be a limitation) and tells us if a first name is male/female, or mostly male/female, or unknown/androgynous. For example, my name Chinese name Yuhan can belong to any gender and the package would tell you my gender is unknown, but you can tell from my English name Alison that I’m female.Percentage of unknowns in Cast: 4.78% Percentage of unknowns in Crew: 4.99% Percentage of unknowns in Directing: 4.21% Percentage of unknowns in Writing: 4.99%The percentages of unknown have dropped significantly, which is great! Now, it’s time to decide which metrics we want. The female ratios that I think are of importance are:So, we can add a new column by running:There are some NaN in the writing_female_ratio column because 0/0 is NaN. If the total number of writers is 0 (the denominator), then the records of these movies are probably incomplete and not useful to us. Let’s drop the null. Also, this time, we cannot consider the dubious anymore when comparing metrics, because dubious scores are not reliable.Now we can check the mean Bechdel score for each metric.By eyeballing the means, it seems that a higher female ratio is correlated with a higher bt_score. To visualize the means and their uncertainty intervals, we can use error bars to compare the metrics with the Bechdel score. For cast female ratio vs Bechdel score, we have:The error bars do not overlap, which indicates that the means of the 4 groups where bt_score = 0, 1, 2 and 3 are statistically different. And the positive correlation is very obvious.By changing the column name from cast_female_ratio to others, we can plot all 4 graphs.Groups failing the Bechdel test are not always different from each other, but they all have a lower female ratio compared to movies passing the test. The positive correlation between on-screen and off-screen metrics is quite salient. That is, the higher percentage of female members in the cast and crew, the more likely the movie is to pass the Bechdel Test, and vice versa. More female on set can indeed translate into a better female representation on screen.And that’s the end of this fun analysis.In this article, we have talked about:We have answered these questions:Yes! The mean Bechdel score and the percentage of passing movie are rising. For the movies failing the Bechdel Test, more are closer to passing the test now.2. How does the Bechdel Test compare with other benchmarks of off-screen representation?There is a positive correlation between the Bechdel score and the female ratios in cast, crew, directing and writing. More females in the workplace can translate into a more feminist output.The quantitative work here focuses more on data acquisition, data analysis, and visualization because this project is originally designed to explore the human-centeredness in Data Science. What’s important is learning to ask the right questions, identify biases and limitations, and be aware of why each decision was made on the dataset. I don’t think it makes much sense to, for example, predict the Bechdel score based on gender ratios and involve Machine Learning models in this project. Nor do I want to overcomplicate things by introducing statistical concepts like Tukey’s HSD for pairwise comparison if the visualization already says it all. But you are most welcome to do so if it suits your need.My wonderful teammates JB, Min Jie and Fatima went through the comments on the Bechdel website and did the qualitative analysis that made the project whole. Please check here if you are interested!Code in this post can be found in this GitHub repo. Check Kaggle for dataset.Here are some articles that I found extremely helpful and inspirational when working on this project. They explore relationship between the Bechdel scores and other interesting aspects such as rating, budget, etc. Enjoy the fun read![1] https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/[2] https://towardsdatascience.com/the-bechdel-test-analyzing-gender-disparity-in-hollywood-263cd4bcd9dSpecial thanks to TDS editor Thank you for reading! I hope this has been helpful to you. Please leave a comment if you have any feedback :)",,4,14,15,19,1200,434,18,7,0,29,285
PyCaret 2.3.6 is Here! Learn What’s New?,Towards Data Science,https://towardsdatascience.com/pycaret-2-3-6-is-here-learn-whats-new-1479c8bab8ad,Moez Ali,7000,6,623,"PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the experiment cycle exponentially and makes you more productive.By far PyCaret 2.3.6 is the biggest release in terms of the new features and functionalities. This article demonstrates the use of new functionalities added in the recent release of PyCaret 2.3.6.📚 Official Docs: The bible of PyCaret. Everything is here.🌐 Official Web: Check out our official website😺 GitHub Check out our Git⭐ Tutorials New to PyCaret? Check out our official notebooks!📋 Example Notebooks created by the community.📙 Blog Tutorials and articles by contributors.❓ FAQs Check out frequently asked questions.📺 Video Tutorials Our video tutorial from various events.📢 Discussions Have questions? Engage with community and contributors.🛠️ Changelog Changes and version history.🙌 User Group Join our Meetup user group.Installation is easy and will only take a few minutes. PyCaret’s default installation from pip only installs hard dependencies as listed in the requirements.txt file.To install the full version:This function will generate the interactive dashboard for a trained model. The dashboard is implemented using the ExplainerDashboard.Video Demo:This function will generate automated EDA using the AutoViz integration.Video Demo:This function will transpile trained machine learning models into native inference scripts in different programming languages (Python, C, Java, Go, JavaScript, Visual Basic, C#, PowerShell, R, PHP, Dart, Haskell, Ruby, F#). This functionality is very useful if you want to deploy models into environments where you can’t install your normal Python stack to support model inference.Video Demo:There are many approaches to conceptualizing fairness. This new function follows the approach known as group fairness, which asks: Which groups of individuals are at risk for experiencing harm. This function provides fairness-related metrics between different groups (also called subpopulations).Video Demo:This function will create a POST API for the ML pipeline for inference using FastAPI framework. It only creates the API and doesn’t run it automatically.Video Demo:This function will create a Dockerfileand requirementsfile for your API end-point.Video Demo:This function creates a basic Gradio web app for inference. It will later be expanded for other app types such as Streamlit.Video Demo:A new parameter called drift_report is added to the predict_model function that generates the drift report using Evidently AI framework. At the moment this functionality is in experimental mode and will only work on test data. Later on, it will be expanded for production use.Video Demo:plot_model function is PyCaret is now more configurable. For example, previously if you wanted to see percentages in Confusion Matrix instead of absolute numbers, it wasn’t possible, or if you want to change the color map of visuals, it wasn’t possible. Now it is possible with the new parameter plot_kwargs in the plot_model function. See example:This is not a new function but it was completely revamped in 2.3.6. This function is to optimize the probability threshold for binary classification problems. Previously you had to pass cost function as true_positive , false_positive , true_negative , false_negative in this function and now it automatically picks up all the metrics including the custom ones from your active experiment run.The biggest and hardest of all is the completely new documentation. This is a single source of truth for everything related to PyCaret, from official tutorials to release notes and from API ref to community contributions. Take a video tour:Finally, if you want to take the tour of all new functionalities added in 2.3.6, watch this 10 minutes video:To learn about all the other changes, bug fixes, and minor updates in PyCaret 2.3.6, check out the detailed release notes.Thank you for reading.I write about PyCaret and its use-cases in the real world, If you would like to be notified automatically, you can follow me on Medium, LinkedIn, and Twitter.",,8,12,50,0,,,13,0,0,24,609
My Top 10 Pandas Functions for Preparing Data,Better Programming,https://betterprogramming.pub/my-top-10-pandas-functions-for-preparing-data-3ec7a1451a84,Holly Dalligan,638,4,760,"Sadly, useful analysis of datasets is rarely as simple as downloading data into your chosen platform, creating some plots or metrics, and using the results to aid and inform decision-making.Data is messy, it comes from multiple places and can be full of noise or have chunks missing altogether. Sometimes we need to create new columns, set flags, or remove specific rows before we can move on to the next step.The following are 10 operations from the Pandas library, at least one crops up in every single script I write, if not more. While many of these operations can be done in excel, why not save yourself the effort and let Pandas do the hard work for you?You may need to join data from a lookup table, or combine multiple sources into one place based on something common, like name or timestamp. Similar to the vlookup in excel but can be done quickly on large datasets, joining multiple columns with one command.For more information on getting started on merging, refer to my previous article.Vertical stacks work best when column names match. For example, you may have a number of excel templates filled in at different times, by concatenating them, you have them all in one place ready to interrogate.Depending on your data source, a blank record may show as null or NaN (not a number) and in reality, may mean a missing record, or it may mean ‘nothing’ or zero. It is important to check and clarify so as not to incorrectly skew your results.This is especially relevant when analysing multiple datasets at once, definitions may differ across them. Unfortunately, if you do some numerical operation, like summing, and the column contains at least one NaN, pandas will not play ball.The example above fill with 0, but you can fill it with whatever your heart desires.Rather than replace missing values or NaN, you may wish to just remove them completely.The syntax how=’any’ drops the rows where at least one of the columns has missing/ NaN data. You can also edit the inputs to drop rows that:- have all columns missing/ NaN- have at least x number columns missing/ NaN- have specific columns missing/ NanAdding axis = 1 will apply the same methodology but to columns instead of rows. A great article with more information can be found here.Spaces before or at the end of values can be extremely annoying, especially if you don’t realize they are until it’s too late. The code below strips both, it is quick to run and I try and remember to run it as standard if I intend to use string searches or something that relies on that being formatted consistently.Signal dropouts, mistakes, misplaced files can all result in missing data. These rows may still be present with empty values, or they may be missing altogether. An example of time series data would be: if the system logs every minute but pauses for 5, those 5 minutes may be present as timestamps or completely gone and show as a time jump.If you know what your output set should look like, you can create this as a dummy set and then join your actual data onto it; comparing the two to understand what’s present and what’s not.This doesn’t just apply to time series, for example, you may collect data from hundreds of different assets, it is difficult to easily see if any are missing by eye. Joining onto your known list of assets will easily highlight any that are not included.This one speaks for itself. Combine this with a sort to smartly drop certain duplicates (e.g. the earliest or latest record).Use bitwise operators (& and | instead of AND and OR) and construct whatever criteria you need.You may only be interested in analyzing a specific asset type or, following on from #8, you may have set criteria to identify ‘possible erroneous data’. It is now easy to isolate these records for further investigation or removal.You can filter the existing DataFrame or insert the filtered data into a new one (as shown above).Averages, sums, max/ mins are often the information we use in later analysis. It is common for this step to be done manually in excel which works fine. But it especially makes sense to do it in pandas instead if it is a repeatable process such as a monthly report, or if it is a large dataset that is either slow or impractical to load into excel.This has been a selection of the common operations I use, I hope you’ve found it useful. Thanks for reading.medium.com",,4,10,8,5,1225,918,1,0,0,5,448
,,https://medium.com/u/cde0f6304466?source=post_page-----c35fc29ccf6--------------------------------,Gleb Vazhenin,29,,,,,,0,,,,,,,0,,
,,https://medium.com/u/51ad6744291b?source=post_page-----c35fc29ccf6--------------------------------,Nils Reimers,311,,,,,,0,,,,,,,0,,
Evaluation of classification models on unbalanced production data,Bumble Tech,https://medium.com/bumble-tech/evaluation-of-classification-models-on-unbalanced-production-data-ae5735229410,Gleb Vazhenin,29,11,2071,"When it comes to benchmarking machine learning models, the reliability of the analysis plays the most important role. To draw the right conclusions, not only do the right metrics need to be selected but also the test dataset distribution needs to be class-balanced as well as represent the real production load.Here at Bumble Inc — the parent company of Bumble and Badoo, two of the world’s highest-grossing dating apps with millions of users worldwide — in terms of safety moderation, we realised that an analysis of the photo distribution in production was not going to give us the desired insights we wanted because the data is very unbalanced.In the light of this, we added an artificial bias to our testing dataset and it helped us to reach conclusions that were far from obvious. We conducted an interesting comparative analysis between our internal photo moderation models and those of third-party providers. In this post, we will cover the methodology we used.Our mission is to create a world where all relationships are healthy and equitable. Therefore, user safety is our highest priority. We are constantly striving to improve our controls and processes around user safety. Along with photo moderation, we also use information from text data to help improve user safety. We have implemented a rude-message detector in one of our apps. Find out more about that here.A massive number of profile photos are uploaded to the platforms every single day, and we are keen to make the content as safe as possible. Currently, profile photos uploaded to the app undergo a two-step moderation process. The first one is automatic moderation based on various computer vision models, and the second is manual moderation. The photos that fail to pass automatic moderation and therefore are potentially unsafe, are sent for manual moderation.Given how sensitive the task is, human moderation is very thorough and time-consuming, that is why we wanted to automate as much of the moderation as possible whilst keeping it high quality. We are focused on improving our photo moderation algorithms, and that is the reason why we are not only improving the models we built in-house — but we are also trying to leverage open-source frameworks and various third-party models.Class balance is a common problem for both training and validation of the model. A production distribution of samples is rarely uniform. A lot of common tasks like fraud detection, spam filtering, and anomaly detection have a class balance lower than 1:100. And the problem of class balance becomes even more severe when you have limited model capacity or staging environment limitations. For example, having 10,000 available requests for a dataset with a 1:100 class distribution makes quality assessment almost impossible, since 100 cases of a minor class might be very specific and will not generalise to the production distribution. To prepare a solid model quality comparison with these limitations, metrics should be chosen carefully, and the dataset collected smartly.Here follows a brief description of the most popular metrics which are used for comparing classification models and for quality assessment. We will return to the example of a model that outputs the probability of a photo being unsafe. A probability above a certain threshold indicates the photo might be unsafe.The confusion matrix represents all possible classification cases. Leveraging it for the photo moderation task:The main advantage of using a confusion matrix is that we can measure the specific algorithm outcome and easily transfer it to the production load. For example, if we’re going to automate all the safety moderation, the FN part of the matrix will show us how many unsafe photos will be passed to the platform and FP will represent the number of unnecessary moderations. Moreover, with these metrics, we can easily estimate moderation costs. For instance, if the model performs well on negative cases (TN+FN with a low FN), and we have decided not to apply manual moderation for them, we can calculate how many moderators we will need to handle all the positive (TP+FP) cases.Model threshold dependency could be considered as a main disadvantage of the confusion matrix in terms of model benchmarking. We would have N confusion matrices for the N thresholds we are testing, which increases the complexity of the analysis. To provide comprehensive and more digestible insights, the confusion matrix elements are usually combined to generate aggregated measures. The most common metrics based on a combination of confusion matrix outputs are — Accuracy, Precision, Recall and F-scoreAccuracy, precision, recall, and F-score are the combinations of the confusion matrix output, which represents the specific bias of the model.All these metrics being generated using a confusion matrix, they inherit its dependency from a specific threshold. They yield useful business insights, for example, the rate of unsafe photos we’re going to pass to the platform if we start using a model without manual moderation (1-recall); or the rate of unnecessary moderation there is going to be (1-precision).However, the class imbalance could lead to wrong decisions. For example, for the production load of 97% safe photos and 3% of unsafe photos, we’re going to have 0.97 Accuracy (which is high!) for a dummy model that always outputs 0 (safe). And the threshold dependency will not allow us to compare models’ performances until we tried all possible thresholds. The latter problem is solved by calculating the area under the curve that represents precision-recall dependency for a range of thresholds.The problem of a threshold dependency is solved by the AUC (area under the curve) approach. For a range of thresholds, specific metrics are calculated:The AUC approach deals with the problem of threshold specificity; that is one of its greatest and most useful advantages. On the other hand, it cannot give us the desired business insights we need. The probability of a specific unsafe photo being correctly classified as unsafe could be an interpretation of the ROC AUC number, but still not be enough to understand how the model will perform in production. Because of this, the lack of business insights could be considered as the main disadvantage of AUC-ROC or AUC-PR metrics.Let’s go back to the photo moderation example. Imagine we have a binary classification Model A in production, and we want to switch it to binary classification Model B, which is not deployed on a production-load infrastructure and therefore has limited request capacity. The main question is — how will Model B perform in production?Imagine we have 97% of safe photos in production so, in terms of automation, we want to avoid sending photos with “Negative” (0, TN+FP) model output to moderators since it will save us on moderation time. On the other hand, we are keen to reduce the number of unsafe photos passed to the platform as far as possible, so the number of FN should be considered as well. In short, we want to increase automation (TN+FP), with as few FN cases as possible (safety).The diagram above shows the possible production distribution of model outputs over manual moderation results. And we can see that there are only 300 unsafe cases here, which makes the generalisation ability of data questionable. Ten thousand photos from production could be taken and passed to Model B — but this approach could lead us to a wrong conclusion because positive cases could be very specific and might not represent the real model behaviour.Instead, we can try to artificially bias the distribution of the dataset we are going to use, to make it balanced. There are two ways of doing that, both of which will reduce the probability of the dataset being non-general, and therefore boost the reliability of the comparison analysis. This is especially so when it comes to positive cases about which we want to be confident of the model’s performance.The first option is “photo-based”. We can create a balanced dataset in terms of safe and unsafe photos, saving the production distribution of confusion matrix cases. In other words, we can pick 5,000 safe and 5,000 unsafe photos randomly. The distribution should look like this:Selecting this option will allow us to compare models on the more general dataset since we have an equal quantity of unsafe and safe photos. To get accurate business insights into safety and automation, we would then need to transfer the error from the confusion matrix to the production distribution.The second approach is “model-based”. We can collect output-specific photos (in terms of Model A), making the confusion matrix cases balanced.If we collect a dataset with 25% of all cases (TN/FP/TP/FN), we can compare the performance of Model A and Model B in production, as well as find out if Model B could help us with misclassified samples in further analysis. This approach is very similar to what happens under the ‘boosting’ technique. Boosting is an ensembling algorithm that converts multiple weak learners into a strong one. Therefore, understanding the weak and strong sides of both algorithms could help us create a much better model overall. Having this kind of split could be beneficial not only for comparing models but for gathering more insights around using the new model in ensemble with the old one.Let’s look closer at how we can compare models using the “model-based” biasing approach.After running a Model B over the “model-based” biased dataset, we get the following confusion matrix:Looking at the confusion matrix above one could wrongly conclude that Model B is better than model A as there are more TN and TP cases. But we can see that this is not the case when we take a closer look at the error transferring:Using the transfer chart above, we can analyse how Model B performs on specific Model A cases. Since we have added an artificial bias to the dataset to make it balanced, we need to ‘un-reweight’ samples according to the way the dataset was collected to read the production performance.TN was 90% of the production load. And 77.47% of the load (2152/2500*100=86.08% of TN and 86.08*0.9=77.47% of all production samples) stayed TN, while 12.53% (348/2500*100=13.92% of TN and 13.92*0.9=12.53% of all production samples) turned to FP.FN was 0.1% of the production load. And 0.08% of it stayed FN, while 0.02% turned to TP.Doing similar calculations for the rest of the cases we see:Even though the confusion matrix was showing us better results for Model B, now we can see that production stats seems to be worse, both for:To see more clearly what happened, let’s look at the possible and simplified classification example:The orange line here is Model A which separates negative (red) and positive (blue) samples. The plot above represents the production distribution of samples. When we balance our dataset in terms of confusion matrix cases, we are increasing the importance of FN, FP, and TP, and reducing the weight of TN:Now, let’s try to draw possible Model B behaviour. It might differ from the one that is presented, but this illustration can help us understand the underlying pattern in a simple example. Concerning what we’ve seen in the confusion matrix, Model B should work better with emphasised FN and FP. But still, it might have larger errors on non-emphasised samples (which were TP and TN according to Model A).And that’s the case here — as we can see. More FN and FP appeared on previous TP and TN respectively. Finally, un-weighting samples back to production distribution:Consequently, we can see that, on production, Model B is going to perform worse than Model A, and so the decision is to stay with Model A for now. However, further investigation on a model that has better identification of some complicated cases (with respect to model A) can lead us to a generally more solid solution that uses strong sides of both Model A and Model B. So the next step for this case would be to try to somehow combine these models in order to get better overall model performance.Keeping it up to a decent level has its challenges, especially when it comes to Machine Learning. We remain keen to improve our photo safety moderation systems and are constantly trying the latest solutions and models.To come to the right conclusions with respect to model performance comparison, not only should insightful metrics be selected, but also a dataset should be carefully prepared, and the results transferred to production load.The growing popularity of AI is bringing lots of new solutions available to the market. Having the ability to accurately compare similar ones is the key to success.",,3,0,46,2,1225,612,14,3,0,6,538
,,https://medium.com/u/68bf0657a0cd?source=post_page-----c35fc29ccf6--------------------------------,Roman Orac,7600,,,,,,0,,,,,,,0,,
,,https://medium.com/u/75b5f5a46f52?source=post_page-----c35fc29ccf6--------------------------------,Julia Kho,2800,,,,,,0,,,,,,,0,,
,,https://medium.com/u/254e653181d2?source=post_page-----c35fc29ccf6--------------------------------,Piero Paialunga,1300,,,,,,0,,,,,,,0,,
,,https://medium.com/u/19d0fc7fc960?source=post_page-----c35fc29ccf6--------------------------------,Holly Dalligan,638,,,,,,0,,,,,,,0,,
,,https://medium.com/u/d66134dd9f0d?source=post_page-----c35fc29ccf6--------------------------------,Rohith Teja,629,,,,,,0,,,,,,,0,,
"Exploring Microsoft PowerPoint AI, using Python",Towards Data Science,https://towardsdatascience.com/exploring-powerpoint-ai-using-python-75f94d55f8f4,Piero Paialunga,1300,5,845,"A couple of days ago I was working on a PowerPoint presentation for my PhD research and this happened:It was not the exact same image, it was actually way more explicit, with the x label,y label, title and all of that, but it is not really important right now.The very interesting thing is the Alt Text. The AI system of PowerPoint is not only able to detect that we actually have a 2d plot (or Chart) but it recognizes that we are talking about a boxplot!Of course, I don’t exactly know how they do this, but as I work with Machine Learning and Data Science all the days of my life I can try to take a guess. As the readers may know, the technology that it is very widely used to classify images is known as Convolutional Neural Networks (CNNs).They may have used CNNs as a multi-class classifier. Here is an example of a Butterly image classifier (more than 70 species/classes). A way more complicated thing that they may have done is image captioning. Nonetheless, CNNs are surely used in their deep learning algorithm, at the very minimum as basic bricks of something that is much larger and complex.In this very small example I will show how it is possible to build a Machine Learning model that helps you distinguish boxplots and other kinds of plots, for example lineplots.Let’s do this.These are the libraries that I used for this notebook:In a few words, I used keras, matplotlib, and a curious library known as RandomWords that generate random english words. I used it to make up the x and y axes.The very fun part of this notebook is actually the data generation. I tried to build the lineplots and boxplots in the most general way as possible, making up the x and y labels, creating different lines and boxplots, again, in the most general way as possible.With this setup that you can virtually create an infinite numbers and kinds of plots. I created two classes of data and performed a binary classification, but you can slightly modify the code and create multiple classes.Let’s dive in:The code that I used to create the line plot is the following:It has different degrees of randomness:Here is an example:The code that I used to create the box plot is the following:Different degrees of random here as well:Actually, the codes that I used to build the training set and test set are slightly differences from the one above, that I was using to show you the results. Here is what you will need:Here you create the plots:Here you create k of them and store them. CREATE A TRAINING SET AND TEST SET FOLDER FIRST OR IT WON’T WORK!Here you read them and label themAfter you define this function, you will have your dataset by doing this:Here are some examples of the training set:The exact same process has to be done for the test set and the strings has to be converted to something more readable to a ML model (sklearn will do this for you with the so called LabelEncoder feature):The Machine Learning model that we are going to use is basically the application of different Convolutional layers and some Max Pooling operations, it will then end up with a softmax that will tell you the probability of the image to belong to the first class.The model that I used was the same of this article I published and you can find more details about how the structure actually works.Here is how you train and test your model:And as we can see, the final result is perfect. Even if it may sounds exciting, I have to say that the experiment is pretty easy (we are all able to distinguish a plot with box and a plot with lines) and the model is more than sufficiently powerful (a little bit of overkill here).As a final prove that the model is correctly distinguish boxplots and lineplots, here are some examples:And here are the plots:I have been using PowerPoint since I was just a kid and did my first presentations in high school. I can really see the improvement that this product had during all these year and I find their AI recognition models are really impressive.This notebook was just a very simple experiment about how Deep Learning algorithms can help you detecting what is inside a picture and using some (in this case very basic) image classification techniques.If you liked the article and you want to know more about Machine Learning, or you just want to ask me something you can:A. Follow me on Linkedin, where I publish all my stories B. Subscribe to my newsletter. It will keep you updated about new stories and give you the chance to text me to receive all the corrections or doubts you may have.C. Become a referred member, so you won’t have any “maximum number of stories for the month” and you can read whatever I (and thousands of other Machine Learning and Data Science top writer) write about the newest technology available.Ciao :)",,,0,50,0,1031,903,3,2,0,8,71
The Data Scientist’s Guide to Creating an NFT Collection in Python,Better Programming,https://betterprogramming.pub/the-data-scientists-guide-to-creating-an-nft-collection-in-python-de1b98163875,Rohith Teja,629,7,1233,"Non Fungible Token or NFT has been a buzzword lately and finally, I gave in to jump onto this NFT bandwagon to see if an average person like me with zero-to-none artistic skills can sell something on the marketplace.NFT marketplace is growing rapidly and there are some collections like the Bored Ape Yacht Club which crossed $1 billion in total sales, which is pretty amazing.For some context, NFTs are digital art pieces that can be bought or sold on a blockchain. I was a bit familiar with blockchain already, which is basically a decentralized ledger of all the transactions happening on the system. I started to wonder how they can fit data in the form of images and videos into this blockchain as it would be a challenge. The solution to this problem was found by using something called an InterPlanetary File System. It is a decentralized storage mechanism where the files are stored in a peer-to-peer file-sharing network that works similar to that of a BitTorrent.Okay, this is cool!I thought to myself, just for the fun of it, can I make an NFT collection and sell something. So, I did an experiment where I generated a collection just using the expertise and knowledge I have, which is not exactly in producing art. As I am not an artist by any means, this was tricky!In this article, I will outline all the details of me being involved in making an NFT art collection purely by using Python and some level of understanding of the internet.The first thing to start any project is to have a good idea of what you want to do. I am currently working as a data scientist in the domain of climate change, so I had some experience with climate data and doing some analysis on it.From my research, I noticed that the climate domain was not much explored in the marketplace so I decided it is a good idea to come up with something interesting.Global warming is one of the biggest problems that mankind has to face. Anthropogenic emissions over several decades contributed to this story of global warming and I thought, can I art capture the evolution of that story over a span of years.Luckily, the climate-related datasets are freely available to download and use. That’s a good start!Copernicus is an Earth monitoring program that is managed by the European Union and its Agencies. They provide an API called Climate Data Store to download any data relating to any climate variable. I chose temperature which is one of the most important variables of climate and in climate analysis. Also, Global Warming can be explained to a layman using temperature easily.Now, retrieving the dataset from the API is as simple as running a few lines of code.Firstly, I had to install the python package by using pip install cdsapi and registering on this website to get the credentials. After setting up the environment, it is as simple as running the following code to get data relating to any climate variable you want.Although the process is very simple, the climate files are usually very large in size. I like to work with NetCDF format (widely used in Climate Sciences) where the data is in the form of gridded data i.e, the geographic coordinates (Latitude and Longitude).netcdf4 or Xarray are some useful python libraries to handle such gridded datasets.I selected the temperature data (air temperature at 2 meters above the surface) from 1979 to 2021 (a total of 43 years of data). In fact, this data was so huge that it took me a couple of weeks to process it entirely.Using this global dataset, I designed two cases namely:In order to compute average country temperatures, we have to mask the shapefile of the country onto the gridded data and clip the portion that falls inside the polygon.For a Data Scientist, the art generation using data can simply be a visualization. And that’s what I did. I tried to capture the daily temperature patterns over a span of 43 years to see if something informative and interesting pops up.And it did!The image above shows the global average daily temperature values in different colors (purple tones for low temperatures in winter and orange tones for warm temperatures in the summer). The x-axis represents the months from Jan to Dec and the y-axis represents the years from 1979 to 2021. Simply, the top left of the image is Jan 1979 and the top right is Dec 1979. And, the bottom left is Jan 2021 and the bottom right is Dec 2021.The image shows a difference between warmer temperatures from colder temperatures and the recent decades (from 2020–2021) have been considerably warmer than before (especially the summer). This gives an indication of rising global temperatures over the years.In order to transform this informative visualization into art, I used different types of color palettes. This resulted in some cool patterns which you can see here and the first batch is up for sale already!Atmos Portals, short for atmospheric portals, is an artsy representation of the climate indicator: Temperature. The visualization looks a bit like a portal to somewhere, hence this name was chosen :)Disclaimer: Everything written in this post is my opinion and it may or may not reflect the actual.Here is a list of tools that I used to realize this project:There are already so many tutorials online on how to create your own NFT project, so I did not go much into the details of setting up the wallet and minting the NFT.If you want to make your own NFT, then one idea is to look into generative models such as GANs but models require GPUs and high processing power.The idea of NFT, digital ownership, and smart contracts are interesting. I like how we are leaning towards a decentralized ecosystem and using tools to create such a thing (like cryptocurrency, NFT, etc). But at the same time, there are some downsides to it.To mint an NFT, we add the image metadata onto the blockchain and it costs energy (in terms of electricity and processing power) which might have a significant carbon footprint as the crypto art grows more in demand. Ethereum, a cryptocurrency widely used to sell and trade NFTs has a power consumption equivalent to that of Kazakhstan.There are some ideas floating around on how to make the minting process a bit greener. I would like to see the crypto marketplace turn more eco-friendly in the coming years.As a tech enthusiast, I wanted to explore the world of NFTs and see what was all the hype about.As a Data Scientist, I created an informative NFT collection that is backed by climate data. Well, climate change is real and the global temperatures have something to say!All in all, this was a pleasant experience in creating something unique and informative (which actually looks cool) just by using the Python programming language and its libraries. There are a lot of possibilities for Data Scientists to create digital art with various tools at their disposal and I just exhibited one such tool. Therefore you don’t have to be an artist to create an NFT, you just have to think out of the box!Here’s a link to my collection at OpenSea, in case you are interested to see how it works.If you have reached this part of the post, thank you for reading and also your attention.",,2,2,6,0,1225,871,5,2,0,21,135
OpenAI GPT-3 Text Embeddings - Really a new state-of-the-art in dense text embeddings?,,https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9,Nils Reimers,308,12,1911,"This week, OpenAI announced an embeddings endpoint (paper) for GPT-3 that allows users to derive dense text embeddings for a given input text at allegedly state-of-the-art performance on several relevant tasks. In this post, I will be reviewing how good these new GPT-3 embeddings really are. Are they really a new state of the art?Dense text embeddings are useful for many tasks, including clustering, topic modeling, deduplication, paraphrase mining and semantic search. As part of my research, I’ve worked on dense text embeddings since 2019 and released my research as part of the sentence-transformers framework, which provides open & free state-of-the-art text embedding models for many use-cases.OpenAI provides endpoints for three different use-cases:I wanted to investigate how well these GPT-3 based embeddings would work so I benchmarked the text similarity on 14 datasets and text search embeddings on 6 datasets from various domains: Twitter, StackExchange, Reddit, emails, news, scientific publications and many more.While I was excited about OpenAI’s new release, the results were not what I expected:Via a REST API endpoint, you can access four types of models from OpenAI:Davinci is claimed to be the most capable model (and most expensive), while Ada is the least capable but cheapest model.12288 dimensions for Davinci is extremely high-dimensional. For comparison, all-MiniLM-L12-v1 produces embeddings of 384 dimensions, Universal Sentence Encoder of 512 dimensions, and all-mpnet-base-v2 of 768 dimensions.Dimensions are not for free: Assume you want to build a semantic search engine over the English Wikipedia, which has about 21 million passages you need to encode. Using float16 (and no further compression techniques) and 384 dimensions, the resulting embeddings have a size of about 16GB, which can fit easily on a decently sized server (like an n2-highmem-4 for about $150/month on Google Cloud). Using 12288 dimensions, you need at least 516 GB of memory to store the embeddings, increasing your compute cost to $3,000/month for an n2-highmem-80 instance.Further, any downstream task like clustering or search in 12288 dimensions is a lot slower than in lower dimensional vector spaces. Hence, I would only find Ada (1024 dim) and maybe Babbage (2048 dim) to be practical for most scenarios. Curie and Davinci produce vectors just too high dimensional to work for any larger scale task. Dimensionality reduction techniques like PCA cannot solve this, as they significantly impact downstream performance.OpenAI has made it easy to compute embeddings by a REST-API:I used the endpoint in December 2021, when it was still in beta.Computing embeddings for the open-source framework sentence-transformers is similarly easy and runs on your local machine or server:First I tested the OpenAI embeddings model for their ability to encode sentences in a semantic vector space. For this, I created a benchmark that consists of 14 complex tasks:To excel on this benchmark, text embedding models must be able to understand text from various domains and create vector spaces that require different properties, e.g. properties for clustering vs. properties for retrieval.For comparison, I include the following models:As the results show, the sentence similarity models from OpenAI perform a lot worse than models such as the Universal Sentence Encoder, which was published in March 2018, and also much worse than the state-of-the-art models from sentence-transformers & Sentence-T5. In fact, the largest model (davinci) with 175B parameters is around 10 points weaker than the all-MiniLM-L6-v2 with just 22M parameters — a model that you can easily run in your browser.In the paper, OpenAI evaluated the model on SentEval, a benchmark to test sentence embedding models for text classification.First, this comparison leaves out many relevant models from 2020 and 2021, which are substantially better than the models they compare against. Second, SentEval tests sentence embeddings for a rather small use case.SentEval tests sentence embeddings for their ability to do text classification by adding a softmax classification head on top and fine-tuning only this head on the available training data. This only makes sense if you want to run many different classifiers on the same text. By pre-computing and sharing the text embeddings across classifiers, you can save a lot of compute time. However, if you only run a single text classifier, it makes much more sense to fully fine-tune your network. For instance, for the Microsoft Research Paraphrase Corpus (MRPC) dataset, a tiny model like MiniLMv2 with just 30M parameters (~60MB in size) achieves an accuracy of 88.7. Using the largest embedding model from OpenAI, cpt-text XL, with 175B parameters (~350 GB in size), you achieve an accuracy of just 78.1.Furthermore, from SentEval, we cannot conclude how well a model will perform for the advertised downstream applications like clustering, semantic search, or paraphrase mining. Even text encoders with random parameters perform well on SentEval while being unusable for vector space tasks like clustering & search.From the paper it appears that the text similarity model was trained using a nearly identical approach to DeCLUTR using consecutive texts in documents as positive pairs. While it is interesting to see how these approaches scale to billion parameter models, the produced models are significantly weaker than models which exploit more structure from the data. For example, Sentence-T5 and all-mpnet-base-v2 used question-answer pairs, conversation pairs, and title-body pairs crawled from the web, which yields significantly better models.If we compare the OpenAI models only to models trained on unstructured data, they perform a bit better than the strongest unsupervised model (princeton-nlp/unsup-simcse-bert-large-uncased), which achieves an average of 60.83 on the above benchmark.The next area of focus is on text search, where OpenAI provides dedicated models. Unfortunately the paper does not clarify how these models were trained and on which datasets.In the paper we find numbers for 11 out of 18 datasets on BEIR, a benchmark that tests models for zero-shot information retrieval which my research group developed last year. Why 7 datasets from the benchmark were left out is not clear.The results on retrieval look much better than the results for text similarity indicating a strong model. In December I tested the model exposed via the API on the FiQA dataset, but sadly got different results than what was reported in the paper:It might be that the paper used a different model or that the model behind the API had been different when I tested it in December. Or the authors did some different pre-processing. (Update 2022–02–09: The difference in performance is due to different truncation. GPT-3 just supports inputs up to 2048 word pieces. Sadly the API doesn’t offer a truncation service and trying to encode text longer than 2048 word pieces results in an error. It is up to you to figure out how much text you can encode. I used a simple truncation strategy where I only encoded the first two thousand characters. The author later provided a script that uses a GPT-2 tokenizer and iteratively remove words from the end until they are below 2040 word pieces. With this advanced truncation strategy, results are supposed to be re-producable)I tested the models available via the API a bit further on several (query, document) retrieval datasets:The average results are depicted below. I was just able to test the ada & babbage, as my access was restricted to run further experiments.The OpenAI Models perform comparably to open dense models. However, the biggest difference is in terms of costs.When building a search application, two factors are highly relevant: Operation costs, i.e. how much does it cost to setup & run the index, and latency, i.e. how quickly does the search return the results.I assume we want to do semantic search on the English Wikipedia with about 1 million queries per month. As a free comparison system, I use SpladeV2, a sparse embedding model that performs well for semantic search. According to the OpenAI paper, SpladeV2 and the OpenAI GPT-3 embedding models perform in the following way on BEIR:As we see, the largest OpenAI model with 175 billion parameters is just 0.1 points better than SpladeV2 which has just 66 million parameters. How the results will change when evaluated on all 18 BEIR datasets remains open.The English Wikipedia had in 2020 around 6 million articles with about 2 billion tokens. When broken down into paragraphs of 100 tokens each, this yields 21M paragraphs.Using the OpenAI Davinci model, it would cost us over $1 million to encode all English Wikipedia articles. In contrast, SpladeV2 is based on a distilbert-base model, which can encode about 300 paragraphs per second on a T4-GPU. Using a preemptive T4 GPU on Google Cloud, we have costs of $0.13 per hour (as of 27.01.2022). Hence, encoding Wikipedia with SpladeV2 might cost as little as $2.50.Besides encoding, we have operating costs: Search queries must be encoded and queried against your index.When we assume 1 million queries per month, each with on average 10 tokens, we get the following monthly costs:The 175B Davinci model would cost us about $6,000 on a monthly basis. Estimating the costs of SpladeV2 is much harder, as you can run it on your own server. Here it depends how much compute you use to encode queries. But in general, SpladeV2 can be run on a CPU server, making it rather cheap. In the above table, I used an n1-standard-2 instance, which costs about $50 / month and can encode around 100 queries / second. With further model quantization, it can encode up to 500 queries / second. When your number of queries doubles, the costs for the OpenAI will double, while your n1-standard-2 instance can handle it with ease.Finally, we also need an index server that stores the embeddings for our 21 million Wikipedia paragraphs. As mentioned above, the Davinici model yields 12288 dimensional vectors, hence we need at least 516 GB of memory to store the embeddings. This adds to your operation costs $3,000/month for an n2-highmem-80 instance.In contrast, Spladev2 has about 250 non-zero elements, so storing the sparse embeddings requires about 21GB of memory. Here, you could use an n2-highmen-8 for about $300/month.As you want to quickly search through these vector spaces, you would need further memory to build a respective index. I left-out the memory requirement for this index, as it is non-trivial to compute and depends on many trade-offs like recall, latency, index build time and many more.OpenAI also provides an endpoint for code-search. I did not run any tests on it, but the mentioned issues (slow, too many dimensions, extremely expensive) remain the same. But luckily there is a free alternative to use: st-codesearch-distilroberta-baseWould be interesting how this model performs on suitable benchmarks.The text similarity models are weaker than e.g. Universal Sentence Encoder from 2018 and much weaker than text embedding models from 2021. They are even weaker than the all-MiniLM-L6-v1 model, which is so small & efficient that it can run in your browser.The text-search models perform much stronger, achieving good results. But they are just on-par with open models like SPLADEv2 or multi-qa-mpnet-base-dot-v1.The biggest downside for the OpenAI embeddings endpoint is the high costs (about 8,000–600,000 times more expensive than open models on your infrastructure), the high dimensionality of up to 12288 dimensions (making downstream applications slow), and the extreme latency when computing embeddings. This hinders the actual usage of the embeddings for any search applications.I ran the experiments in late December 2021, when the embedding endpoint was in beta and not yet publicly announced. At that time, using the endpoint could be used without charge. I cannot tell if the endpoint / deployed models has changed with the official release. Maybe the models got significantly better since December. Running the tests now would costs $1,000,000+.",,8,2,46,0,767,421,8,6,0,43,1300
An Easy Guide to Advanced SQL Window Functions,Towards Data Science,https://towardsdatascience.com/a-guide-to-advanced-sql-window-functions-f63f2642cbf9,Julia Kho,2800,15,2332,"This article is a guide on advanced window functions for data analysis in SQL. This is definitely a must know for data scientists and analysts. I will first introduce what window functions are, why you should use them, and the 3 types of window functions. Next, I will go through real-life examples to show how each of these functions are used.Window functions were first introduced to standard SQL in 2003. Per the PostgresSQL documentation:“A window function performs a calculation across a set of table rows that are somehow related to the current row…Behind the scenes, the window function is able to access more than just the current row of the query result.”Window functions are similar to the aggregation done in the GROUP BY clause. However, rows are not grouped into a single row, each row retains their separate identity. That is, a window function may return a single value for each row. Here’s a good visualization of what I mean by that.Notice how the GROUP BY aggregation on the left hand side of the picture groups the three rows into one single row. The window function on the right hand side of the picture is able to output each row with an aggregation value. This may save you from having to do a join after the GROUP BY.Here’s a quick example to give you a taste of what a window function does.Let’s say we have some salary data and we want to find to create a column that gives us the average salary for each job title.On the left is what a GROUP BY aggregation would return and on the right is what a window function would return. As you can see, the group by consolidates our data into just three rows. With a window function, we retain the original 11 rows and have a new column called AVG_SALARY. We could then choose to compare each individual’s salary to the average salary if desired.One major advantage of window functions is that it allows you to work with both aggregate and non-aggregate values all at once because the rows are not collapsed together.Window functions are also simple to use and read. That is, they can reduce the complexity of your queries, which makes it easier to maintain down the road.In addition, they can help with performance issues. For example, you can use a window function instead of having to do a self-join or cross-join.I promise, window functions are truly amazing and great to know.Before we start, it is important to note that in terms of the order of operations in SQL, window functions come in sixth on the list.This is important because based off of this logical order, window functions are allowed in SELECT and ORDER BY, but they are not allowed in FROM, WHERE, GROUP BY, or HAVING clauses.Note: If you really need to have it inside aWHERE clause or GROUP BY clause, you may get around this limitation by using a subquery or a WITH query.Here’s what the generic syntax looks like for a window function in the SELECT clause.There’s a lot of words here, so let’s look at some definitions:Don’t worry about memorizing the definitions and syntax or even fully understanding what it means exactly right now. Everything will make a lot more sense once you look at the examples in the article and get an intuitive understanding of how to go about writing a window function.To help you get a better idea of how the syntax really works, below is an example of what a window function would look like in practice.This is the query that would have generated the output we saw earlier regarding salary by job title.Here, AVG() is the name of the window function, SALARY is the expression and JOB_TITLE is our partition list. We did not use an ORDER BY as it is not needed and we do not want to use ROWS because we do not want to further limit our partition.Again, no need for memorizing syntax for now. At this stage, the one concept I want you to understand is that the window function computes a value for each row in the “window” or “partition”. A window can be one of more rows and it is specified by the clause PARTITION BY. In our example, we partitioned by job title. As you can see in the snippet above, I’ve highlighted each job title a different color. Each color represents a different “window” or a different “partition”. The window function computes one average salary value for each partition.Now that you know the syntax, let’s take look at the different kinds of window functions that can be substituted in place of the red font below.There are three main types of window functions available to use: aggregate, ranking, and value functions. In the image below, you can see some of the names of the functions that fall within each group.Here’s a quick overview of what each type of window function is useful for.Aggregate functions: we can use these functions to calculate various aggregations such as average, total # of rows, maximum or minimum values, or total sum within each window or partition.Ranking functions: these functions are useful for ranking rows within its partition.Value functions: these functions allow you to compare values from previous or following rows within the partition or the first or last value within the partition.Now let’s start doing some fun exercises to help you really grasp how window functions work. We’ll go through various exercises on aggregate, ranking, and value functions.For the example problems below, I am using data from the Northwind database located on this website. See Northwind_small.sqlite. The file can also be found in my github repo.Per the download source, “The Northwind sample database was provided with Microsoft Access as a tutorial schema for managing small business customers, orders, inventory, purchasing, suppliers, shipping, and employees. Northwind is an excellent tutorial schema for a small-business ERP, with customers, orders, inventory, purchasing, suppliers, shipping, employees, and single-entry accounting.”The full schema of the database is displayed on the website linked above. For the examples in this article, I will only be using the [Order] and [OrderDetail] tables.From the list of aggregate window functions listed on the left, we can see that AVG() will be the window function that we want to use. Our expression will be the Unit Price column because we want to calculate the average of Unit Price.Next, we need to figure out how we want to partition. That is, how should the rows be grouped together in order to create our partitions? The exercise statement tells us to find the avg price for each CustomerId. That tells us that we want to group rows that have the same CustomerId, so that is will be part of our partition list. For the purpose of this exercise, we have no use of an ORDER BY. Below is what our query would look like.As you can see in the image above, an average unit price is computed for each of our partitions of CustomerId.Here’s some exercises to try for yourself:You can choose to partition by more than 1 column. Earlier, we calculated the average unit price for each CustomerId group. This time, we’ll add in EmployeeId.Notice how the partition changes from earlier. The calculations are computed for every unique group of CustomerId and EmployeeId as visually shown by the different colors in the table.We can complete this exercise in three different ways.We’ll use the first three ranking functions on the list on the left: ROW_NUMBER, RANK, and DENSE_RANK.Each one has a slightly different way of ranking the data.We can use this function to show the row number of a given row within its partition. Note that for the ranking functions, we do not have to specify an expression within the parentheses like we did previously for the aggregate functions.In addition, since are doing a ranking, order is important here. We have to make sure that the Unit Price column is ordered correctly so that the ranking is applied correctly. To do so, we can add ORDER BY UnitPrice DESCas part of the windows function, right after PARTITION BY.As you can see in the output above, our UnitPrice column is in descending order and the unit’s rank is shown for each customer id in the last column. There are 12 rows for customer ALFK, so the rank goes from 1 to 12.You may be wondering, what happens if I use the ORDER BY at the end of the SQL statement and not inside the windows function, will I get the same results?Take a minute a think about it and come back. Does it matter if I have the ORDER BY inside the windows function versus outside?Let’s try it out! Remove the ORDER BY from the windows function and let’s add it to the end.Hm, it looks like we aren’t getting the same results as earlier. Unit price is ordered correctly in descending order, but the unit’s rank doesn’t look right. Why not?Remember back to the SQL order of operations. Window functions are processed sixth whereas the ORDER BY is processed tenth.So the row numbers were created BEFORE the UnitPrice was ordered. That’s why we don’t get the same results! Makes sense.Now, let’s try RANK() in place of ROW_NUMBER().What’ the difference now? With row number, there were no repeated numbers. But with RANK(), if you have multiple values with the exact same value, the rank function will give them the same rank.Notice in rows 2 and 3, the unit price is 45.60, so both rows are given the same rank of 2. Rows 7 and 8 also have the same unit price and are given the same rank of 7.Also note that the ranking skips a number. For example, in row 3, the rank skips to 4 since there are two rows of rank 2. If there were three rows with rank 2, then it would skip to rank 5 and so on.’Well, what if you don’t want it to skip numbers? Well, we can use DENSE_RANK() instead.Again, replace our window function to be DENSE_RANK() and keep all else the same.It follows the same behavior as RANK() in that if the values are the same, the same rank will be given to those rows. See rows 2 and 3. Notice that in row 4, the rank does not skip a number now. It is rank 3 instead of 4.Your homework now is to learn how PERCENT_RANK() and NTILE() work and to try those functions for yourself.To me, value functions are probably the top reason why window functions are so amazing. These functions are great for extracting values from other rows that might be useful for a report.We can use the LAG or LEAD functions to help us create a column that is able to pull values from other rows. LAG can return values from the previous rows whereas LEAD returns values from following rows. Comparing previous or following rows can be super useful when working with time series data and calculating differences across time.We use LAG on the Quantity column to return the value from the previous row. Just like before, we need to make sure our data is sorted in order inside our windows function. We’ll sort by the OrderDate here.As you can see in the image above, we get a column with the previous OrderDate’s Quantity. This is really useful because we could compare the current order date to the previous order date and calculate the differences across the two time periods. In the first row, there is no previous order date, so it is NaN, or null.This is going to look pretty similar to our earlier example. However, this time, since we want the following row, we will use LEAD().As you can see, the new column LEAD contains the values from the the next row down.To get the first quantity, we can use the FIRST_VALUE function, which will give us the first value within a partition.As you can see in the image, the first order for product ID 1 was on 8/20/2012 with a quantity of 45, so we get a value of 45 for all rows related to product 1.Here’s an exercise for you to try yourself.Let’s take a quick break from the exercises to learn a new concept that hasn’t been discussed yet.You may remember from the definitions at the beginning that we can specify ROWS with a frame_clause to further limit our window size. I’m saving this towards the end here because people tend to get a bit confused about this. I’m going to quickly go over the syntax and how it’s used, then let’s look at an example to really understand what’s going on.Here’s what the generic syntax looks likeROWS BETWEEN <starting_row> AND <ending_row>In the <starting_row> and <ending row>, we have the following options at our disposal:Here’s some examples of how it could be written:One worthy note is that anytime that you add an ORDER BY clause, SQL sets the default window as ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.In order to calculate a cumulative moving average, we will take advantage of the frame_clause.In the output above, you can see that an average is recalculated at every row. In row 1, there is only 1 number, so the average is 45.60. In row 2, the CumAvg is the average of 45.60 and 18. In row 3, the CumAvg is the average of 45.60, 18, and 12. And so on…Here’s some exercises to try for yourself:If you’re up for a challenge, try this bonus one.Challenge Exercise: Create a new column that provides the last Quantity ordered for each ProductId. (Hint: use the LAST_VALUE() window function and think about the partitioning)Great work getting through this article! Window functions are really powerful and a lot of fun, so I hope you enjoyed going through these exercises. Thank you for reading! Drop a comment below if you have feedback or questions.And if you are looking for a cheatsheet, check out this link here.",,18,10,37,9,775,544,32,6,0,17,952
Try Mito Python Package: The Data Scientist’s Spreadsheet,Better Programming,https://betterprogramming.pub/try-mito-python-package-the-data-scientists-spreadsheet-69f1f4160810,Roman Orac,7600,4,478,"Mito is a spreadsheet interface for Python. You can import Mito into your Jupyter Notebook and a spreadsheet will appear.Just as the saying goes, one video is worth thousands of words — I know… it doesn’t go like that :)Each edit you make in the spreadsheet will generate the equivalent Python in the code cell below. As an example, If you make a pivot table in Mito, the Pandas pivot code will automatically appear, with documentation, in the following code cell.Auto-generated code is great for repeating your analysis on another machine and for Data Science newbies to get up and running with pandas way of working with data.Mito is most commonly used in one of two ways:I assume you have a recent Jupyter Lab version installed on your machine. If that’s not the case, see official Jupyter Lab documentation to get started.To install Mito, run these commands:Then open Jupyter Lab and import the Mitosheet:You can see the full installation instructions in the “docs” page of the Mito website.In Mito, you can import Excel files or CSVs. All you need to do is click the import button and you can browse your local files.Handling a large dataset in a spreadsheet can be incredibly slow — updating formulas, creating pivot tables, or generating charts can take minutes or even hours to load. And if you have a dataset larger than 1 million rows, Excel won’t even accept it.Many Mito users take their spreadsheets and import them into Mito to process the data quickly. Mito can handle any data size that fits into a Pandas DataFrame (that’s millions and millions of rows), and when you import your dataset, Mito automatically turns it into a DataFrame.With Mito, you can perform spreadsheet and Data Science operations such as:In the visualizations below, you can see what it is like filter a dataset, make a pivot table, and make a graph, and have the equivalent code generated in the code cell below.After you complete your analysis in Mito, there are a few ways you can share your result. Some users want to take the generated code and either copy it to another script or apply something more advanced like an ML model later in the notebook.Many users want to bring the output back to a spreadsheet. Within Mito, you can click “Export button” and get the current state of Mito back as an Excel file or CSV.Here is the Mito website, where you can learn more about the tool and get the install instructions.Mito is a powerful tool for those wishing to transition from spreadsheet environments like Excel or Google Spreadsheets to Python.I use Mito for the initial Exploratory Data Analysis — to get the feel of the data. Typing the same set of commands over and over gets tedious.Will you add Mito to your Data Science toolbox? Let me know in the comments below.",,2,2,0,0,1276,640,8,2,0,11,119
Scaling Kubernetes to Over 4k Nodes and 200k Pods,The PayPal Technology Blog,https://medium.com/paypal-tech/scaling-kubernetes-to-over-4k-nodes-and-200k-pods-29988fad6ed,Abdul Qadeer,389,8,1315,"At PayPal, we recently started testing the waters with Kubernetes. A majority of our workloads run on Apache Mesos, and as part of this migration, we needed to understand several performance aspects of clusters running Kubernetes with a PayPal-specific control plane. Chief amongst these aspects is understanding the scalability of the platform as well as identifying opportunities for improvement through tuning the cluster.Unlike Apache Mesos, which can scale up to 10,000 nodes out of the box, scaling Kubernetes is challenging. Kubernetes’ scalability is not just limited to the number of nodes and pods, but several aspects like the number of resources created, the number of containers per pod, the total number of services, and the pod deployment throughput. This post describes some challenges we faced when scaling and how we solved them.We have clusters of varying sizes in production spanning thousands of nodes. Our setup consisted of three master nodes and an external three-node etcd cluster all running on Google Cloud Platform (GCP). The control plane was placed behind a load balancer and all data nodes belonged to the same zone as the control plane.For the purpose of performance testing, we used an open-source workload generator, k-bench, modified for our use cases. The resource objects we used were simple pods and deployments. We deployed them both in batches and sequentially with varying batch sizes and inter-deployment times.We started with a low number of pods and a low number of nodes. Through stress testing, we found opportunities for improvement and continued to scale up the cluster as we observed improved performance. Each worker node had four CPU cores and could hold a maximum of 40 pods. We scaled to about 4,100 nodes. The application used for benchmarking was a stateless service running on 100 millicores of guaranteed Quality of Service (QoS).We started with 2,000 pods on 1,000 nodes, followed by 16,000 pods, then 32,000 pods. After this, we jumped to 150,000 pods on 4100 nodes, followed by 200,000 pods. We had to increase the number of cores on each node to accommodate more pods.The API server proved to be a bottleneck when several connections to the API server returned 504 gateway timeouts, in addition to local client level throttling (exponential backoff). These increased exponentially during ramp-ups:The size of the queue which governs rate-limiting at the API Server was updated via max-mutating-requests-inflight and max-requests-inflight. These two flags at the API server govern how the Priority and Fairness feature introduced as Beta in 1.20 divides the total queue size amongst its queue classes. For example, requests for leader election get priority over pod requests. Within each priority, there is fairness with configurable queues. There is room for further fine-tuning in the future via PriorityLevelConfiguration & FlowSchema API objects.Controller Manager is responsible for providing controllers for native resources like Replica Set, Namespaces, etc., with a high number of deployments (which are managed by replica sets). The rate at which the controller manager could sync its state with the API server was limited. Multiple knobs were used to tune this behavior:When tested independently as an independent component, the scheduler can support a high throughput rate of up to 1,000 pods per second. However, upon deploying the scheduler to a live cluster, we noticed a real-world throughput reduction. A slow etcd instance caused increased binding latencies for the scheduler, leading to an increased pending queue size in the order of thousands of pods. The idea was to keep this number under 100 during test runs, as a higher count affects pod startup latencies. We also ended up tuning leader election parameters to be resilient against spurious restarts on short-lived network partitions or network congestion.etcd is the single most critical part of a Kubernetes cluster. This is evident from the plethora of issues etcd can cause all across the cluster manifested in different ways. It needed careful investigation to surface the root causes and scale etcd to handle our intended scale.When ramping up, a lot of Raft proposals started failing:Through investigation and analysis, we determined that GCP throttled PD-SSD disk throughput to about 100MiB per second (as shown below) on our disk size of 100G. GCP doesn’t provide a way to increase the throughput limit — it only increases with the size of the disk. Even though etcd node only takes < 10G of space, we first tried with 1TB PD-SSD. However, even the larger disk became a bottleneck when all 4k nodes joined the Kubernetes control plane at once. We decided to use local SSD, which has very high throughput at the cost of a slightly higher chance of data loss in case of failures as it is not persistent.After moving to local SSD, we didn’t see the expected performance out of the fastest SSD. Some benchmarks were done directly on the disk with FIO and the numbers there were expected. However, etcd benchmarks showed a different story for concurrent all member writes:The local SSD performed worse! After deeper investigation, this was attributed to ext4 file system’s write barrier cache commits. Since etcd uses write-ahead logging and calls fsync every time it commits to the raft log, it’s okay to disable the write barrier. Additionally, we have DB backup jobs at the file system level and application level for DR. After this change, the numbers with local SSD improved comparable to PD-SSD:The effect of this improvement was seen in etcd’s Write-Ahead Logging (WAL) sync duration and backend commit latencies, which decreased by more than 90% around the 15:55 time mark as seen below:The default MVCC DB size in etcd is 2 GB. This was increased to a maximum of 8 GB upon setting off out of DB space alarms. With the utilization of about ~60% of this DB, we were able to scale up to 200k stateless pods.With all the above optimizations the cluster was much more stable at our intended scale, however, we were far behind in SLIs for API latencies.The etcd server would still restart occasionally and just a single restart can spoil the benchmark results, especially the P99 numbers. A closer look revealed that there is a liveness probe bug in etcd YAML for v1.20. A workaround was applied to fix this by increasing the failure threshold count.After exhausting all avenues to scale etcd vertically, mainly resource-wise (CPU, memory, disk) we found that the etcd’s performance was affected with range queries. Etcd doesn’t perform well when there are many range queries and writes to the Raft log suffer, thereby slowing down the cluster’s latencies. The following are the number of range queries per Kubernetes resource that affect performance in one of the test runs:The etcd backend latencies were majorly impacted due to these time-consuming queries. After sharding the etcd server on the events resource, we saw an improvement in cluster stability when there is high contention of pods. In the future, there is room to further shard the etcd cluster on the pods resource. It is easy to configure the API server to contact the relevant etcd for interacting with a sharded resource.After optimizing and tuning various components of Kubernetes, we observed a huge improvement in latencies. The following charts demonstrate the performance gain achieved over time to meet SLOs. The workload here is 150k pods total with 250 replicas per deployment on 10 concurrent workers. As long as the P99 latencies for pod startup are within five seconds, per Kubernetes SLOs, we are good.The following chart demonstrates the API call latencies well within SLOs at 200k pods in the cluster.We also achieved P99 pod startup latencies of around five seconds for 200k pods at a much higher pod deployment rate than what K8s tests for 5k nodes claim at 3000 pods/min.Kubernetes is a complex system and a deep understanding of the control plane is necessary to know how to scale each component. We have learned a lot through this exercise and will continue to optimize our clusters.",,16,4,5,10,1161,526,7,1,0,13,1500
,,https://medium.com/u/9d6e9d66f369?source=post_page-----c35fc29ccf6--------------------------------,Gav Grayston,167,,,,,,0,,,,,,,0,,
,,https://medium.com/u/dbd909ab7ead?source=post_page-----c35fc29ccf6--------------------------------,Mengying Li,62,,,,,,0,,,,,,,0,,
Scaling cross-team contributions to a native mobile app,,https://engineering.salesforce.com/scaling-cross-team-contributions-to-a-native-mobile-app-79c4ec9669e5,,,,0,,,,4,0,0,150,150,1,1,0,5,
,,https://medium.com/u/89878df718d2?source=post_page-----c35fc29ccf6--------------------------------,Abdul Qadeer,389,,,,,,0,,,,,,,0,,
,,https://medium.com/u/b1b178533834?source=post_page-----c35fc29ccf6--------------------------------,Dor Indivo,55,,,,,,0,,,,,,,0,,
Scaling Kubernetes to Over 4k Nodes and 200k Pods,The PayPal Technology Blog,https://medium.com/paypal-tech/scaling-kubernetes-to-over-4k-nodes-and-200k-pods-29988fad6ed,Abdul Qadeer,389,8,1315,"At PayPal, we recently started testing the waters with Kubernetes. A majority of our workloads run on Apache Mesos, and as part of this migration, we needed to understand several performance aspects of clusters running Kubernetes with a PayPal-specific control plane. Chief amongst these aspects is understanding the scalability of the platform as well as identifying opportunities for improvement through tuning the cluster.Unlike Apache Mesos, which can scale up to 10,000 nodes out of the box, scaling Kubernetes is challenging. Kubernetes’ scalability is not just limited to the number of nodes and pods, but several aspects like the number of resources created, the number of containers per pod, the total number of services, and the pod deployment throughput. This post describes some challenges we faced when scaling and how we solved them.We have clusters of varying sizes in production spanning thousands of nodes. Our setup consisted of three master nodes and an external three-node etcd cluster all running on Google Cloud Platform (GCP). The control plane was placed behind a load balancer and all data nodes belonged to the same zone as the control plane.For the purpose of performance testing, we used an open-source workload generator, k-bench, modified for our use cases. The resource objects we used were simple pods and deployments. We deployed them both in batches and sequentially with varying batch sizes and inter-deployment times.We started with a low number of pods and a low number of nodes. Through stress testing, we found opportunities for improvement and continued to scale up the cluster as we observed improved performance. Each worker node had four CPU cores and could hold a maximum of 40 pods. We scaled to about 4,100 nodes. The application used for benchmarking was a stateless service running on 100 millicores of guaranteed Quality of Service (QoS).We started with 2,000 pods on 1,000 nodes, followed by 16,000 pods, then 32,000 pods. After this, we jumped to 150,000 pods on 4100 nodes, followed by 200,000 pods. We had to increase the number of cores on each node to accommodate more pods.The API server proved to be a bottleneck when several connections to the API server returned 504 gateway timeouts, in addition to local client level throttling (exponential backoff). These increased exponentially during ramp-ups:The size of the queue which governs rate-limiting at the API Server was updated via max-mutating-requests-inflight and max-requests-inflight. These two flags at the API server govern how the Priority and Fairness feature introduced as Beta in 1.20 divides the total queue size amongst its queue classes. For example, requests for leader election get priority over pod requests. Within each priority, there is fairness with configurable queues. There is room for further fine-tuning in the future via PriorityLevelConfiguration & FlowSchema API objects.Controller Manager is responsible for providing controllers for native resources like Replica Set, Namespaces, etc., with a high number of deployments (which are managed by replica sets). The rate at which the controller manager could sync its state with the API server was limited. Multiple knobs were used to tune this behavior:When tested independently as an independent component, the scheduler can support a high throughput rate of up to 1,000 pods per second. However, upon deploying the scheduler to a live cluster, we noticed a real-world throughput reduction. A slow etcd instance caused increased binding latencies for the scheduler, leading to an increased pending queue size in the order of thousands of pods. The idea was to keep this number under 100 during test runs, as a higher count affects pod startup latencies. We also ended up tuning leader election parameters to be resilient against spurious restarts on short-lived network partitions or network congestion.etcd is the single most critical part of a Kubernetes cluster. This is evident from the plethora of issues etcd can cause all across the cluster manifested in different ways. It needed careful investigation to surface the root causes and scale etcd to handle our intended scale.When ramping up, a lot of Raft proposals started failing:Through investigation and analysis, we determined that GCP throttled PD-SSD disk throughput to about 100MiB per second (as shown below) on our disk size of 100G. GCP doesn’t provide a way to increase the throughput limit — it only increases with the size of the disk. Even though etcd node only takes < 10G of space, we first tried with 1TB PD-SSD. However, even the larger disk became a bottleneck when all 4k nodes joined the Kubernetes control plane at once. We decided to use local SSD, which has very high throughput at the cost of a slightly higher chance of data loss in case of failures as it is not persistent.After moving to local SSD, we didn’t see the expected performance out of the fastest SSD. Some benchmarks were done directly on the disk with FIO and the numbers there were expected. However, etcd benchmarks showed a different story for concurrent all member writes:The local SSD performed worse! After deeper investigation, this was attributed to ext4 file system’s write barrier cache commits. Since etcd uses write-ahead logging and calls fsync every time it commits to the raft log, it’s okay to disable the write barrier. Additionally, we have DB backup jobs at the file system level and application level for DR. After this change, the numbers with local SSD improved comparable to PD-SSD:The effect of this improvement was seen in etcd’s Write-Ahead Logging (WAL) sync duration and backend commit latencies, which decreased by more than 90% around the 15:55 time mark as seen below:The default MVCC DB size in etcd is 2 GB. This was increased to a maximum of 8 GB upon setting off out of DB space alarms. With the utilization of about ~60% of this DB, we were able to scale up to 200k stateless pods.With all the above optimizations the cluster was much more stable at our intended scale, however, we were far behind in SLIs for API latencies.The etcd server would still restart occasionally and just a single restart can spoil the benchmark results, especially the P99 numbers. A closer look revealed that there is a liveness probe bug in etcd YAML for v1.20. A workaround was applied to fix this by increasing the failure threshold count.After exhausting all avenues to scale etcd vertically, mainly resource-wise (CPU, memory, disk) we found that the etcd’s performance was affected with range queries. Etcd doesn’t perform well when there are many range queries and writes to the Raft log suffer, thereby slowing down the cluster’s latencies. The following are the number of range queries per Kubernetes resource that affect performance in one of the test runs:The etcd backend latencies were majorly impacted due to these time-consuming queries. After sharding the etcd server on the events resource, we saw an improvement in cluster stability when there is high contention of pods. In the future, there is room to further shard the etcd cluster on the pods resource. It is easy to configure the API server to contact the relevant etcd for interacting with a sharded resource.After optimizing and tuning various components of Kubernetes, we observed a huge improvement in latencies. The following charts demonstrate the performance gain achieved over time to meet SLOs. The workload here is 150k pods total with 250 replicas per deployment on 10 concurrent workers. As long as the P99 latencies for pod startup are within five seconds, per Kubernetes SLOs, we are good.The following chart demonstrates the API call latencies well within SLOs at 200k pods in the cluster.We also achieved P99 pod startup latencies of around five seconds for 200k pods at a much higher pod deployment rate than what K8s tests for 5k nodes claim at 3000 pods/min.Kubernetes is a complex system and a deep understanding of the control plane is necessary to know how to scale each component. We have learned a lot through this exercise and will continue to optimize our clusters.",,16,4,5,10,1161,526,7,1,0,13,1500
Terraform Refactoring Nightmare,,https://tomharrisonjr.medium.com/terraform-refactoring-nightmare-232790fcdb12,Tom Harrison,1800,6,1210,"We have gone through several iterations of our Terraform code now, and it’s getting better. We have moved several different implementations of modules from various sources into a central GitHub repository of modules, releases using semantic version tags, all up to TF 1.0.10 or even 1.1 and handling the several different formats, names, and implementations that existed in our infra. It took a lot of work to get everything aligned.The language and tools are also getting a lot better, and pretty quickly. I am looking forward to what’s next.But recently we had to upgrade a bunch of databases running old versions of PostgreSQL. While I would have loved to do these upgrades using Terraform, the logic required ruled out terraform. We needed to control the timing, we needed to drop several views before and reapply after, several new extensions, parameter groups and, so on.Nearly all of this could be done in terraform, but as we tested, we found there were multiple unpredictable states. For example, if the code ran during the maintenance window it would fail, or if a database needed maintenance during upgrade it would fail. So we built scripts that called AWS APIs and knew how to handle the exception cases.In the end, we had the same databases running on higher versions, and with instance names that now finally met our naming standards. These resources were already in terraform, and just needed to get updates.Going back to our terraform code to synchronize with the new state of the world has proven to be a nightmare.To be clear, I did not expect Terraform to do the major version upgrade of our PostgreSQL databases. This is a complex set of dependent and occasionally flaky (non-deterministic) operations. But in the end, I have a newer version of the same database already managed by Terraform, and this seems like state drift to me.For example, we had an AWS Aurora database running Postgres 9.6.22 called company. The cluster was called company-production-2018-0121431932022 and the writer instance was called company-production-2018-0121431932022-1. Our other databases followed the pattern name-environment-<cluster|instance-N> so as part of the upgrade, we renamed company database as company-production-cluster and company-production-instance-1 and company-production-instance-2. The instance also has a newer engine version, and this version has features not available in 9.6. To be sure, several changes.I would like to think I could simply replace the engine version 9.6.22 with the later version now present, specify the new name of the cluster and instances. Pretty much everything else is the same. For a moment, I thought maybe terraform would detect drift, and I could just run:But that didn’t work.I also briefly hoped that the new moved configuration block in TF 1.1 would help. But my resource identifiers are the same. I am not moving resource definitions from one place to another. I am just updating the existing resources definitions with new ones.Terraform now sees my state as an old database instance it must destroy and a new database instance to create, and the same with all of the other resources define in the module.The module address is the same, so I can’t simply use terraform state mv. I can’t use terraform import directly because terraform sees the old resource in the state and says it’s already managed. And there’s no way I have found to tell terraform that the old instance it knows about is really the new instance it wants to create.This means I have to remove the old reference from state with terraform state rm <address> then import the new one using terraform import <address> id. There are several other resources that depend on the aws_db_instance so each needs to be rmd and re-imported.But it’s not that easy. Our module also configures aspects of the underlying postgres databases: names, passwords, roles, extensions and so on.The aws provider implements an aws.db.instance resource that defines the server infrastructure. The result of the aws_db_instanceexports attributes like host address, master username and password, and port. These values are necessary inputs for the postgresql provider so it can connect to the instance and create the database resources.In short, my PostgreSQL database depends on the AWS db instance. Unfortunately, depends_on allows only relationships between resources, and cannot be used in my provider block.I no longer have the aws.db.instance in the statefile, as noted above. There’s no way to specify the dependency of username, password created by theaws.db.instance to the postgresql provider. The postgresql provider fails with an obtuse error message when the postgresql provider can’t log in to the database. The workaround is to hard-code the host/user/port/password credentials in the module until the specific aws.db.instance and postgresql provider have both been applied to the terraform state file.As it happens, many of the changes to one part of the AWS code leave the database code unable to connect. So in this iteration, I added four variables to the postgresql part of the module. I wrote code in the module for the four attributes that got values like this:Now in my root module, to bootstrap the resource creation, I called my module like this:(I could probably use tfvars to pass in values as well.)With the temp settings in place, the database module can successfully connect, and import its resources, and the AWS module can import its resources, and now the state file knows about these resources again. At this point, I can remove the temporary variables from the root module.Now, I just need to state rm then import all the “new” resources into the state. Yikes!All of this is work I have done locally on my laptop. In order to get to the point that I can check in the updates to the calling module, I need to apply changes to the terraform state. Our CI system is sophisticated, but there’s no easy way for me to provide tf.var files that contain the temporary host/user/port/password set. So I apply locally.And now, our shared state file exists, but the code in production is behind. I need to get my PR approved, run the CI system to run the plan, then apply and then check in the code. To make this work, turnaround on PRs needs to be very quick!(An important change we made: because we tag the module directory when we are done with features we can be confident that old modules that haven’t had new databases will continue to work.)But in any case, having a single developer (me!) applying changes directly to production and then urgently requesting approval for a PR is just not how proper development flows should work. I wonder if there are patterns of using development state files?As we have understood the dependency between the two parts of our database system: the server, and the underlying database, it’s clear that we should have two separate modules: the code to manage the AWS Db Cluster and AWS Db Instances, then another to manage the PostgreSQL modifications. We should create outputs in the AWS Db Module that can be referenced by the PostgreSQL module.And I’ll do that. And the new move functionality will be awesome.But this still doesn’t address the fact that I had to jump through many different hoops just to get the new state of reality aligned with the terraform state file.Oh well. Still better than managing infra by hand!",,5,3,0,2,1225,918,1,0,0,3,67
,,https://medium.com/u/1fba4038dcff?source=post_page-----c35fc29ccf6--------------------------------,Tom Harrison,1800,,,,,,0,,,,,,,0,,
Splitting Compilation + Execution in v8go,,https://medium.com/@genlesperance/splitting-compilation-execution-in-v8go-a913761a9b1f,Genevieve L'Esperance,10,4,842,"V8 is Google’s open source high-performance JavaScript and WebAssembly engine, written in C++. The V8 engine orchestrates isolates where an isolate is equivalent to a sandbox for running JS code. Cloudflare has a great explanation so I won’t rehash it here.v8go is a library written in Go and C++ that allows users to execute JavaScript from Go using V8 isolates. Using Cgo bindings means we can run JavaScript in Go at native performance.Why would someone want to render JS from a Go server you might be wondering. Go is a pretty phenomenal language for web servers — it scales well and its concurrency features play a big role in that. There are other approaches to this — for example, Deno used to be written in Go and then switched to Rust to maximize performance. At the end of the day, if you choose to use Go for this, you’ll probably end up using v8go and you want the library to support you making the best performance decisions.Like in any software, it’s important to evolve and find different ways to amortize and reduce the cost of repeated actions. One of the major changes I contributed to the v8go library was with this goal in mind — splitting the compilation of code from its execution. This allows users to cache (in memory or elsewhere) the compiled bits and use that whenever it needs to be executed instead of having to recompile every time it needs to be run. Here’s how and why I made those changes.Let’s start with the initial API supported by v8go for compiling and running JavaScript from Go. You would first initialize a V8 Isolate, a V8 Context, and then use the context’s RunScript function to execute the provided code in that V8 Isolate. It would look like:If you cared about the return value of the code, you could handle the returned V8 Value:Under the hood, what RunScript is doing is taking the provided JavaScript and compiling it into a V8 Script that is bound to the V8 Context, and then running it and returning to you the Go version of the resulting JS value. The internals of v8go’s C++ code looks like:Essentially, because of the way v8go’s RunScript is written, you pass your code in and you get your result back — but each time you run the same code, it has to be recompiled. There are some internal optimizations to V8 whereby it keeps a cache of the compiled data such that it could reuse it for the same isolate, but across isolates or across processes, it would be recompiling.For large, complex bits of code, it’s obvious then that having to recompile each time is a pain we should avoid. Even if it doesn’t take a particularly long time for an individual piece, if you have many pieces that need to be executed every time (like polyfills), you are unable to amortize in this fashion. Folks try to work around this by reusing the same V8 Isolate for as much as they can, and distinguishing different requests by their V8 Contexts. But this hits a separate issue in v8go that still currently stands which is a “memory leak”.To investigate v8go performance, let’s start with a simple example. We can take a URL polyfill and measure how long it takes to compile and run:On average, this takes ~3.0ms.If we do this concurrently like we would if we were handling different requests in a Go server and each gets its own V8 Isolate and Context, then that average bumps to ~6.0ms.Now that we have a baseline of performance for the existing RunScript implementation, let’s experiment with the idea of splitting compilation and execution, and measure the performance benefits.V8 has a few APIs for supporting this flow. The important one is that V8 supports compiling an unbound script in a V8 Isolate (unbound means that the script is not bound to a V8 context.) The V8 ScriptCompiler has the ability to create a “code cache” from an unbound script. From this code cache, the cached data (the compiled code) can be extracted as bytes. In order to use those bytes, they can be passed back in as compile options to any new V8 Isolate.In v8go, the API looks like:Under the hood, it’s pretty straightforward. The c++ changes are here.With this change, we can do some performance tests.So the situation where it’s run in order:This averages ~2.2ms.In the concurrent case, we see a run time of ~3.0ms.In v8go, we kept the original ctx.RunScript API to enable one-off or occasional executions of a script. In these cases it’s fine to compile and run when you need to.However, when your system is particularly sensitive to performance, every millisecond counts. This example only shows a decrease from 6ms to 3ms, but there are plenty of cases where larger script inputs or repeated usage of many common libraries could make this improvement quite significant. Hopefully you found this useful and you can find similar ways to amortize those costs in your own codebases because… Speed. Is. Everything.",,,0,5,3,1225,1209,1,0,0,8,14
How We Migrated from Python Multithreading to Asyncio,,https://medium.com/@DorIndivo/how-we-migrated-from-python-multithreading-to-asyncio-128b0c8e4ec5,Dor Indivo,55,6,969,"At Lemonade, we seek to improve the insurance experience by incorporating Machine Learning into many aspects of the process, from onboarding users with our chat experience, Maya, to predicting catastrophes. In practice, we currently have dozens of ML models in production, all of which significantly rely on our in-house feature store for real-time inference and batch training.The feature store service is exactly what we aim to improve.In this context, a feature is a data point for a machine learning model. If you’re interested in understanding a bit more about machine learning platforms, this is a great resource to get you started on feature stores.Our feature store, like many others, was designed and developed to increase our Data Science velocity by addressing the below challenges:The feature store service was written in pure python (version 3.7) on the FastAPI web framework using a variety of additional auxiliary packages.A single HTTP call to the service can request numerous CPU or I/O bound features. In the past, both FastAPI and the internal multi-feature request concurrency implementation used threading for simplicity.Let’s take a closer look at how we used threading:In this example, we have a model that asks for three different features (feature1, feature2, feature3), hence our web framework (FastAPI) will handle this request behind the scenes using a thread-pool. We use an internal thread-pool to calculate the features concurrently by requesting data from upstream services.The feature store service has evolved into a key component of many customer flows, and over time we realized that the current architecture will be difficult to maintain while demand grows greatly.Why? There are several compelling reasons:In comparison to the thread-based concurrency paradigm, the (mostly) I/O bound feature store workload is a great choice for using an event loop.I’m not going to explain what asynchronous programming is, but if you’re familiar with the topic you’ve undoubtedly heard of asyncio, which is the python implementation.The main idea was to start with the infrastructure code such as making our HTTP client support asynchronous methods (such as async get, async post, and so on).When we finish with the HTTP client we can move to the feature resolver (where the majority of the complicated code is) and only then we should move to the top and make the FastAPI handling requests functions asynchronous.To make production stable throughout this long migration process, we divided it into three phases (in each phase we will migrate from synchronous to asynchronous and validate performance):This is a fantastic idea, but there is one snag:How do we handle asynchronous and synchronous models at the same time without degrading performance?This coroutine allows us to calculate some features asynchronously, while others can be calculated concurrently using the thread pool!As always, warnings can help you find bugs in your code, in our case you should pay attention to this specific warning:It means that you probably forgot to use the await keyword. Finding this error quickly when refactoring a large amount of complex code can be quite valuable.When migrating synchronous code to asynchronous code, we should keep performance in mind, especially when it comes to loops.We have a basic for-loop in this example that calls the method calculate for each item in the listTo run this code asynchronously, we can simply add the await keyword before calling to this coroutine (assume we already changed calculate to coroutine):The issue with this approach is that it will run in a synchronous manner. We’ll wait for the last feature calculation to finish before moving on to the next iteration.We can do this concurrently with the coroutine asyncio.gather:Using the threading library’s active_count is a great approach to keep track of the number of active threads (in our case, this number should be 1):This method was useful in identifying cases when we were still creating unnecessary threads, such as when handling HTTP requests.In theory, converting the FastAPI requests handling functions to async would result in a significant reduction in the number of threads because we are now only using one thread, the event loop.Here’s the catch, all of your dependencies (e.g. get_put_features_service) should be marked as async , otherwise FastAPI will continue to use the thread pool for concurrency!The code can be structured in an inheritance-based manner to accommodate both async and sync models.we’ve a basic implementation of Feature which has only one function called calculate that calculates the value of the specific feature.FeatureProviderService is responsible for getting the feature value from the right service.There are two features A and B, The first one can be calculated asynchronously by the AsyncFeatureProviderService, while B can be be calculated synchronously by the FeatureProviderService.This implementation gives us more flexibility when it comes to running features concurrently:We discovered that when we switched from a boto3-based cache client to an async aioboto3 cache client, the performance of the feature store service suffered significantly.The main reason for this was that instead of using a single persist object, we created several clients and tables objects.There are numerous ways to solve this problem, such as creating a global client, but before jumping into implementation, please read your client’s documentation first. In most cases, you’ll find the right answer for you.TL;DR: Making the client a class member solved this problem in our scenario.The async code was written as coroutines, making it slightly more difficult to test with standard testing tools, but we found some great libraries that made testing really simple.To mark tests as asynchronous we used the decorator pytest.mark.asynciofrom the pytest-asyncio library.We also needed to mock coroutines and other async objects, so we used the asyncio-test library to do so.In order to create the most precise and reliable benchmark possible, we chose to conduct the following in this process:We discovered astounding outcomes in terms of performance and resource utilization:With these excellent results, the feature store service can easily handle increased load and scale horizontally more effectively.",,5,5,2,0,1042,456,5,6,0,5,540
117. Open Source with Jim Jagielski,,https://medium.com/u/41ea9b1cdc2b?source=post_page-----c35fc29ccf6--------------------------------,@SalesforceEng,2200,,78,"ListenSubscribeJim Jagielski is the newest member of Salesforce’s Open Source Program Office, but he’s no newbie to open source. In this episode, he talks with Alyssa Arvin, Senior Program Manager for Open Source about his early explorations into open source software during his time as an actual rocket scientist at NASA, what he’s learned from open source over the years, and how you can get started in open source, from both a company and an individual contributor perspective.",,,0,0,3,1000,833,1,0,0,4,3
Design for Services; Not Microservices,Better Programming,https://betterprogramming.pub/design-for-services-not-microservices-e339883946d7,Gav Grayston,167,8,1268,"If you are starting a new project and thinking of a microservice architecture, or you have an existing monolith that you want to break into different services, then read on for a strategy that enables you to adopt the architecture as and when it’s needed.Building microservices is in vogue and has been for a few years now.On the one hand, it is getting people to refocus on cohesion and loose coupling. On the other hand, it brings a lot of complexity, especially if you want a robust enterprise solution.Over a wide range of projects over a few decades, I’ve had to work on the distributed computing problem.It’s something I like, and Microservices is a more modern spin on some computing patterns around years ago but with better tools these days :-)If you start a new project with microservices, but without a clear need to run your services in separate instances, you will probably spend more effort on the application ‘infrastructure’ than the results merit.But there is a better place to start.What do I mean by a ‘Service’ in this context?Well, it’s a cohesive set of code or functionality (i.e. its responsibilities are focused on just one domain) with loose coupling with the rest of your codebase.You’ll see that this is precisely like a microservice.However, the difference here is that we are not specifying that this service runs within a dedicated execution environment. Instead, the service runs within the instance of your application.In such a situation, you don’t have the same issues with observability across execution boundaries or the latency (and retries) when making a remote call.In this sense, the service is a component within the application.If you have defined your services correctly, a couple of design patterns will enable you to transition from the component-based service model to the microservice model without a sizeable re-coding effort.These are the Factory Method and the Proxy design patterns.Let’s use a straightforward service as an example: a mail service.MailService is a technical service encapsulating the domain of sending emails.This example is straightforward, but the concepts equally apply to any service type, including those encapsulating business domains.Our service has a single send method.Suppose you have an existing monolith application and deploy the MailService as a microservice. In that case, it may look like the following, with the application logic making a call over HTTPS to the new service.Alternatively, if we deployed the Mail Service as a component that runs inside your main application code, it may look like the following:Note: Depending on the programming language you use, you can still develop your service independently and then use a dependency manager (such as NPM, Composer, etc.) to pull the required version of your service during the build.To begin with, the Mail Service may be a simple class.However, we need to define an abstract class or interface to adopt the Service Factory.Which could look like this:The Service Factory is a new class that your code now calls to get an instance to your service.This sequence might look like a lot of code to call a function on a class. And it is.Shortly I’ll show you how this can help you with your microservice strategy. But first, let’s look at how this helps with your testing.Using an interface and an indirect way to instantiate the class, you can improve unit testing by providing a mock mail service.The mock service will simulate the mail service, enabling testing that your code handles both successful and unsuccessful (and don’t forget, slow) calls to the send method.Note: Depending on the programming language and unit testing framework you use, creating a mock from the interface can be trivial.At this stage, you have all your email sending logic in one service and a mechanism to test it.We will introduce the Proxy pattern, enabling you to evolve this service into a microservice.The Proxy is another class that implements the service’s interface.However, instead of this new class sending emails, the code calls the mail service, which runs in a separate container with an HTTPS API interface. In other words, this new class is a proxy for the actual service class.There has been no change to the main application code in the diagram above, except the Service Factory configuration to instantiate the Proxy instead of the Service class.Note: The dependency manager’s package to your code can be updated to deliver the proxy code instead of the actual service code. All of this keeps changes minimal with your application.Your proxy class can become more intelligent than just calling a remote service.For example, it could handle network call timeouts, perform logging and tracing, and handle authentication with the remote service.Direct calls to a remote mail service may not be the most practical, and we can do better in decoupling the service.An asynchronous message queue replaces the synchronous HTTPS call in the diagram below.Important: To change the architecture to support asynchronous communication with the service requires no coding changes to the main application code.The Proxy encapsulates all the logic to communicate with the remote service.A developer working on the main application logic calls the Proxy as a local class.If you want to replace it with a commercial service instead, you can. A new version of the service proxy can call the remote mail product without re-coding your main application logic.Alternatively, you may have developed many microservices and decided to adopt a mesh, such as the one provided by DAPR.The solution outlined in this article, where services run as in-process components, is an excellent way to deliver a modular monolith.Monoliths aren’t sinister. They are a good pattern for many solutions.Monoliths fail when they are a big ball of mud, where there is no consideration for coherence, information hiding and encapsulation, and loose coupling.The approach outlined in this article will help you create a modular monolith that provides the development and architectural flexibility to turn some services into microservices if and when they need it.If you have an existing monolith that is in desperate need of refactoring, you can use these techniques to help get it under control.For example, if you are calling functions to send emails from several places within your code, why don’t you start by bringing all your email sending logic into a MailService ;-)Hopefully, you’ve followed this simple example and now have a strategy for starting with a modular monolith that can expand into microservices.Or, if you already have a monolith, you have a strategy to modularise and evolve it.In most programming languages, you can create the Factory Method and Proxy design patterns without requiring any specific framework.If you want to take it further, look at the Dependency Injection pattern.Frameworks widely use this pattern to achieve a “behind the scenes” service factory. If you are using a framework, search for Dependency Injection in its documentation, and see if it enables you to achieve the same results as this article but with less code.Just remember that you want to keep it as configurable as possible, external to your code, just like the Service Factory had a configuration file to find out what class to instantiate.A CI/CD pipeline can deliver different configurations to different environments. Plus, the configuration technique provides an easy feature toggle mechanism for rapid deployment (and backing out) of new features.However, some frameworks rely on techniques like code annotations, which hardcode the configuration within your source code, which is not a great idea.Another instance where a framework may cause problem is if your existing monolith doesn’t support it. Ideally, you want to create the Factory or Dependency Injection that is framework neutral, so it will work with your existing monolith and with your new services.",,,3,37,14,815,384,11,1,0,9,259
"Why Do We Use Kubernetes, Anyway?",Better Programming,https://betterprogramming.pub/why-do-we-use-kubernetes-anyway-644544082f43,Mengying Li,62,14,2872,"As a less than two years old software engineer myself, I found there are mainly two types of companies that I and my friends end up going in:Speaking from my own experience, I’ve been in both types of companies but I still cannot figure out why do people use Kubernetes. To be more specific, what problems are container orchestration tools like Kubernetes solving? As an effort to answer this question for myself — and other Junior software engineers or beginner Kubernetes users out there — I’m writing this blog in an attempt to dig deeper into these questions.First things first, let’s start with the definition.Short definition: Kubernetes is a container orchestration tool.Longer definition: Kubernetes, also known as K8s, is an open source system for automating deployment, scaling, and management of containerized applications.Yes, I hear the word container orchestration everywhere. But let’s face it, this definition is a giant blur for beginners like me. I’m 100% certain I never heard about the word “container” or the word “orchestration” in any of my computer science classes during college. At most, I heard about distributed systems. I know, technology evolves so fast.If you remember the big and small companies I mentioned above, I felt like there’s no clear path to understand how companies went on the road of container orchestration in both cases. I feel like I was in a new universe where I saw the big bang (startup with monolith) and the advanced current world (giant corporation modern infrastructure), but I couldn’t connect the dots.How did the monolith deployed on one server evolve and become the modern infrastructure that has hundreds (maybe thousands) of microservices deployed on Kubernetes? At what point did they feel the need to migrate to Kubernetes? What problem sets are Kubernetes solving? This is an extremely hard question to answer for junior software engineers. This is because if you are working in a big corporation like in case 1, the problems that Kubernetes is solving are no longer there (or invisible).If you are working in startups like in case 2, then you might not reach the stage of being exposed to the problem set Kubernetes is targeted to solve. To give a more specific example, if your startup is not using containers yet, then how do you expect to face the problems of needing to orchestrate the containers?Believe me, it’s not easy for me to figure these things out as well despite the fact that I have always been working as an infrastructure engineer and going to Kubecon for three years in a row.Allow me to demonstrate the journey to Kubernetes using a made-up story of how a company is built up from the very beginning. As you could have told by my current level of experience, I couldn’t possibly witness a company’s journey from the start. So the following demonstration is a story built upon some of my personal observations from internships and full-time jobs with a combination of what I’ve researched on the internet.I really enjoyed using this mind exercise to understand where the company I’m working for currently stands and how they reached this particular stage. I hope you enjoy reading this article as much as I enjoyed writing it. Please take my words with a grain of salt. I would be happy if you agree with me and if you gained useful information from this blog. If there’s anything you disagree with or have questions about, please comment below the article because I would love to learn more as I’m still at the beginning of my career.Kubernetes is the most popular container orchestration tool lately and the tool I’m most familiar with. So I’m going to use Kubernetes as an example but also try to generalize this discussion that I would like to call my journey to container orchestration.Here, I’m breaking down the development stage of a company into three stages:I would like to claim that by no means I’m suggesting every company has to follow these three steps or have to follow them in order. These stages can definitely happen in parallel — overlapping in two or three stages. In reality, you are unlikely to make all teams sync in these processes. Some teams will be pioneers to try out newer infrastructures (like migrating to Kubernetes) first, and some other teams that maintain more crucial apps with heavier traffic will likely be more conservative and use the more stable and older infrastructures.Overall, I feel like these three stages are a great way for me to understand the company development cycle and the importance of why we use Kubernetes.Startups usually start with a monolith. Think about something similar to the university web projects that are built by a team of students. One giant codebase to handle everything (yes you can have all kinds of controllers, but still, at the end of the day, a giant monolith). Every engineer works on this monolith, opening PRs to implement features.Once the team grows to roughly 50 engineers or more (or reaches a certain scale), engineers run into different problems working on the same monolith. Some problems would constantly need to do complicated rebase because the main branch changes a lot every day and other people could be working on the same file causing merge conflicts. The risk of stepping onto each other’s toes results from the unclear separation of responsibilities and ownerships of features.Long testing time caused by a long queue to access shared staging environment to test your feature. Long build/deployment time and potentially degraded database performance resulting from a large and complicated application with heavy traffic. Developer velocity starts to slow down and the team faces various challenges.Then, gradually the teams start to build smaller services (microservices) to handle some specific isolated functions. To give a more concrete example, assume that you are building YouTube and you would like to add a comment section. Then you can build a new comment service to handle everything related to it and establish API endpoints for the monolith or other services to retrieve related data.This new service can have its own separate database, code repo, and even deploy and test pipelines. These newer smaller services composed of a small set of strongly related functions can be easily developed, maintained, tested, and deployed by one team. This could allow a looser coupling between the teams resulting in a faster release cycle that pushes out new features faster.As time passes, your company starts to have more microservices, implementing either various business capabilities (following Decompose by business capability pattern) or different subdomains (following Decompose by subdomain context pattern). It starts to be more than a monolith. Maybe a monolith with several extra microservices.For the backend side, you might allocate some machines (EC2 for instance) to run the extra microservices and some copies to run the main app (monolith). You might have a small ops team (maybe 1–5 people), and if a new team reaches out to you requesting a machine to run their new service, you write some scripts to provision the server and then deploy it.You start to find out there’s not enough ops people to deploy hotfixes and other stuff every time different service teams have a new request. At the same time, you might find a few teams using Ruby and deploying on EC2 share a lot of similarities. Then you start to write some small tools, like gem tools to help people to generate a standard template to manage their infrastructure, as well as it comes in with some standardized script to build and deploy. You find this mode working for a while.Later on, if the situation arises, you might start to consider breaking down the monolith completely into multiple microservices to better divide the responsibility between teams and achieve better developer velocity. This is a more complex topic on its own whether you should or should not break down the monolith and how should this happen (check out strangler fig pattern for more information). I won’t go into details here but the natural trend is going from one giant monolith to gradually more microservices as the company expands in size.If your startup is very successful, your customers will continue to grow and your engineering team will also expand over time. It goes without saying, the complexity of the infrastructure for supporting these microservices and main applications also grows. One big problem engineers are facing is the inconsistent environment from the developers' desktop to the servers that run the productions application. With different deployment environments, your production server can have completely unexpected behavior from local testing and even staging environment as shown in the picture:Of course, nobody wants to break the production code. How does modern infrastructure deal with this challenge? This is where containers come to the rescue!According to Red Hat documentation:“Containers are technologies that allow you to package and isolate applications with their entire runtime environment — all of the files necessary to run. This makes it easy to move the contained application between environments (dev, test, production, etc.) while retaining full functionality.”The ultimate goal is to build a container that has consistent behavior in different environments. Docker, for instance, is a popular tool for supporting companies to do containerization.It is a well-known fact that containerization comes with its many many benefits:Since there’s so much to look forward to after the containerization, your older services will start to gradually migrate to new containers. At the same time, your company might be promoting the new microservices to be directly built containerized to save the future migration effort.Now that the majority of services are on containers and running relatively stable in all environments, are all infrastructure problems resolved? Too young, too naive. New challenges always present themselves as the company grows. Allow me to demonstrate. Imagine you are working in a company where the traffic peaks during some hours or some days drastically. Then imagine hosting a shopping website where the customers drastically increase on Black Friday sales. The traffic could easily be 3x compared to regular times.In order to properly handle all the traffic, you will need to scale up. Say that you are running 1 EC2 regularly, now you need to bring up 3EC2 instances to handle the extra load. And of course, you will need a load balancer to route the traffic to the different machines. It might look something like this:But this is NOT an easy task to deal with. So many things can go wrong. Tasks associated with containers that needed to be handled manually, including but not limited to:Imagine the amount of effort to write these scripts and make sure they run properly. Even under the best-case scenario where you handle these tasks associated with containers properly, with the magnitude of complexity dealing with infrastructure, things can still go north.Yes, you can pre-provision 3 EC2 instances to deal with the anticipated growth of traffic, but what if the actual traffic is lower or higher than expected? Do you add extra logic to your script to handle these cases?Scaling efficiently is a complex subject to discuss, and it goes way beyond finding the correct number of EC2 to spin up. A simple example can be a situation where you need to scale application A three times but not application B. This can be a real-life situation where the loads on each application spike differently and you will need to handle the scaling separately. How would you do it efficiently? Allow me to demonstrate with some drawings.Here, I’m taking the example of m6g.xlarge EC2 instance, based on the documentation it has 4 vCPU and 16GiB. Here we are using a simple assumption, say that to maintain good performance, we would like the CPU usage of the EC2 instance to run below 75% and the memory usage to run below 80%. So in this case, we would like to keep the applications in one EC2 instance consuming less than 3 vCPU and 12.8 GiB.Let’s consider this as an available resource size in our graph to simplify the problem. I also put some numbers regarding how much CPU and memory resources Application A and Application B need to consume shown in the graph to help better illustrate the scaling problem.Now let’s go back, how can we scale application A three times but not scaling application B. There’s definitely more than one solution to this problem. Here I would like to discuss three possible solutions.The most straightforward solution would be spinning up two extra same EC2 instances and running one application A on each EC2 instance.This will definitely work, but the issue is very obvious. While together with App A and App B in the first EC2 instance, they consume a total of three vCPU and 12.8 GiB, which equals 75% CPU and 80% memory that m6g.xlarge size EC2 has to offer. They are fully utilizing the available resources. But in the later 2 EC2 instances, we can calculate CPU and memory usage as follows:Obviously, more than half of the resources on the later 2 EC2 instances are wasted. So it’s definitely not cost-efficient.Let’s take a look at another solution. What if we scale down the two later EC2 instances to m6g.large to avoid wasting too many resources?Similarly, we can calculate CPU and memory usage of the later two EC2 instances:These numbers definitely look much better. However, there’s some hidden dangers. 95% memory usage is very high. With any unexpected spike, your application can easily run into the out-of-memory danger, causing undesired behaviors on the user side.Instead of using a smaller instance of EC2, we can keep using m6g.xlarge but try to put two Application A’s in the second EC2 instance. We probably need to reconfigure the load balancer to route more traffic from Application A to the second EC2 instance in this solution. Let’s not go into details and assume the load balancer is doing a proper job.Similarly, we can calculate CPU and memory usage of the second EC2 instance:The number is the same in solution 2 for this example. 95% memory usage is definitely entering the out-of-memory risk danger zone. The application could be killed due to being out of memory sometimes. So in a real-life scenario, this is definitely not an ideal case and might not even be an acceptable solution if we want our applications to run stably.From the three scaling solutions above, you can easily see the challenge of scaling things individually and having unstandardized infrastructure. These behaviors can result in not being able to run the services cost effectively. These are precisely the problems container orchestration is trying to solve. If you don’t choose to use existing container orchestration tools like Kubernetes, you might end up needing to reinvent the wheels and build your container orchestration tool. Because these problems, with or without using container orchestration tool, will be the problem you need to solve once you reach a certain scale.Let’s take a closer look at the definition of container orchestration and its use cases. This is the definition given by vmware:“Container orchestration is the automation of much of the operational effort required to run containerized workloads and services. This includes a wide range of things software teams need to manage a container’s lifecycle, including provisioning, deployment, scaling (up and down), networking, load balancing and more.”Container orchestration is used to automate and manage tasks such as:Kubernetes is a very popular open source container orchestration tool. I personally like to imagine container orchestration tools as an extra layer of abstraction on top of servers (like EC2) to allocate the resources more efficiently when dealing with complex infrastructure. Kubernetes helps you build a pool of nodes (servers) to use and then allocate different applications on different nodes based on the application’s CPU and memory request and free available space on each node.The scheduling, scaling, and load balancing are all done automatically. Kubernetes is a great tool to help developers focus on the application without worrying about the bare-bone metal server and be limited by the resources that come along with the server.Combining all these tools together brings the ultimate developer experience that enables great velocity. With one button click, a default microservice from the default template is brought up with build and deployment pipelines with a Kubernetes namespace for your service and deployment. You can code features in your application codebase and all the codes are shipped to the testing environment for you to verify and test inside the cluster using the docker image you just built out. Once everything is ready to be shipped, the release is as easy as changing the docker image in the deployment pods. And voila! Your code is successfully used by customers.It’s obvious that container orchestration is the unavoidable future for running stable applications on a large scale. Kubernetes is currently the most popular container orchestrator originally created by Google. Kubernetes is used by a number of well-known companies like Google, Spotify, Intuit, and so on, and the number is growing! I hope my blog — which is a really long version of the question of why Kubernetes gives you a clear vision of how a company grows in different stages and how they come into needing container orchestration in the end — helps you.Thank you for reading.",,1,3,2,8,,,9,5,0,10,168
,,https://medium.com/u/c5c942a4ef87?source=post_page-----c35fc29ccf6--------------------------------,Taavi Laanemaa,3,,,,,,0,,,,,,,0,,
,,https://medium.com/u/a78bd0833de8?source=post_page-----c35fc29ccf6--------------------------------,Bharath,382,,,,,,0,,,,,,,0,,
,,https://medium.com/u/de241747fd6c?source=post_page-----c35fc29ccf6--------------------------------,Genevieve L'Esperance,10,,,,,,0,,,,,,,0,,
Our Slack App Got Compromised — Here’s How You Can Secure Redis Instances,Better Programming,https://betterprogramming.pub/our-slack-app-got-compromised-heres-how-you-can-secure-redis-instances-bf6be1503dce,Denny Sam,706,5,834,"On 10th January 2022, we were resuming our work on a Monday.One of our products, Dixiapp, is used for our daily standups. Every day, it asks a set of questions along the lines of what did you do yesterday, what are you planning to do today, etc. to which we answer and our backend records the responses and submits a report to a chosen channel.Dixiapp (adorably called Dixi) is supposed to trigger a question at exactly 9 AM, but that day it didn’t.What happened? I am the one responsible for anything happening to this app so I got into debugging it.Our health check monitoring was working fine which means the Gunicorn server was not down. Neither the Nginx.The way our app works is, whenever we send a message on the Dixiapp channel, Slack sends an event request to our servers and we process it. This triggers a response from our servers which is displayed on Dixi’s channel. But there is no response that day. Interesting. So has Slack stopped sending messages? I started debugging and found that it’s not a Slack issue.Finally, I had to SSH into our production server. I checked Gunicorn and Nginx systemd processes. They were running as expected. After checking the logs and finding nothing, I ran the most versatile command which shows us everything about the system, i.e. htophtop shows us all the processes that are running in the system and how much CPU, memory and network bandwidth are they consuming.And when I ran it, I was shocked to see the result!All our CPUs were maxed out! They were operating at 100% capacity. No wonder other processes were not running properly. I took a look at the processes and saw a suspicious one consuming all the CPUs.See that? Hmm, who is this kdevtmpfsi?After some exploring, I understood what’s happening. Our Redis port which was exposed to the internet was the culprit. Or rather, it was the victim. Someone tried to exploit a known Redis issue to gain access to our system.kdevtmpfsi is a crypto miner. Hackers/script kiddies try to exploit vulnerable ports on a server and install this program to run their mining operations. This is carried out through a malware called Kinsing.First things first, I gotta get rid of this malware and get the system up and running. After going through a bunch of articles and StackOverflow questions (duh, I am a true developer), I figured to remove it. I am attaching some helpful resources regarding malware and how to tackle it at the end of this article.I will explain.According to my research, it is based on exploiting the CONFIG command of Redis. Here’s a breakdown of the attack without giving the exact details so that script kiddies won’t exploit the loopholeThis consumed 100% of our CPUs resulting in non-availability of the service, while the attacker is enjoying their free CPU💡 Lesson learned: Be careful when you expose your ports to the internet.This incident led me to think deeper about security. I started understanding the architecture and security issues of Redis.Redis is designed to be accessed by trusted clients inside trusted environments. This means you need to run your Redis instances in a self-contained environment which is not directly connected to the internet. Only the computers running the application should directly access it.Following are some security measures you need to take while having a Redis instance.The main configuration file of Redis is something we should look at carefully and understand what each option means. The file is named redis.conf by default and is located in your Redis installation directory.You can bind Redis to a single interface so that it will only receive requests from that interface. This is done by adding a line in your redis.conf file like this:For versions since 3.2.0, Redis has added a special setting called protected mode. Redis configuration doesn’t have password protection by default. If you run a Redis instance with the default configuration, then it enters a special mode called protected mode.When Redis runs in this mode, it only replies to queries from loopback interfaces, that is localhost (127.0.0.1). If other clients try to connect, they are responded with an error.As I mentioned above, Redis by default doesn’t have password authentication by default. You can enable it by adding the following line to your redis.conf fileNote that this is just a tiny layer of security added since the password is stored in plaintext and anyone with access to this file can read it.One of the best security features that Redis provides is to disable commands which you don’t require. You can also rename them to some unguessable name. Here’s how you rename a command:To completely disable a command you can rename it to the empty string as belowWhen we start building an application, we tend to take security for granted. We tend to expose unnecessary ports and forget to close them. Every assumption that we make while building an application should be noted down and revisited once we have built the app.Resources:",,1,5,3,6,856,463,2,2,0,9,71
,,https://medium.com/u/9d0f2c37880b?source=post_page-----c35fc29ccf6--------------------------------,Denny Sam,706,,,,,,0,,,,,,,0,,
,,https://medium.com/u/5b74ba5e5726?source=post_page-----c35fc29ccf6--------------------------------,Jawad Margieh,165,,,,,,0,,,,,,,0,,
Next.js vs. Remix: Analyzing Key Aspects and Differences,Better Programming,https://betterprogramming.pub/next-js-vs-remix-analyzing-key-aspects-and-differences-8674beaba695,Jose Granja,1800,6,1145,"The React web-based ecosystem is looking better by the day. At the end of 2021, Next.js launched its great 12 release. It was packed with lots of features and optimizations. However, a little after, we saw how Remix unveiled its latest version. The Remix team changed its strategy and decided to be free and open-source.So when starting a project in 2022 which one should we pick? It is a hard choice to make and has a big impact on your web application. Each framework is opinionated and will shape how you think and build your components.In this article, we will analyze what is the key aspect of each and by the end, you will be able to decide if those fit you. Some aspects might be deal-breakers or decisive factors. It all boils down to your use case and personal preferences.In this paradox of choice just be reassured by knowing that both are great frameworks and you can’t go wrong with either of them.The Next.js framework is built on top of NodeJs. It is highly bound to the React ecosystem making it nearly an extension of it. It tries to embrace early on its latest features. The Vercel team works closely with the React one in order to make that happen. They mostly provide abstractions built on top of React components like: next/dynamic, next/head, next/link, …Their latest release already includes support for:You can enable both features in the configuration file:The approach from the Remix is noticeably different. They are more decoupled from the React Ecosystem and keener on waiting for stable React features. It is built on the Web Fetch Api (instead of Node) which enables the app to run anywhere. It focuses on leveraging as much as possible to the server. This has two benefits: faster bandwidths when loading data and the capacity to provide a progressive enhancement experience.Another drastic difference is that Remix tries to rely as much as possible on the Browser Native Web Standard Features. What does that mean? That their pages are 100% functional even with JavaScript disabled. When JavaScript is available the whole experience is enhanced.Their built-in form feature is a clear example of that. They rely on the basic Form browser feature. The form actions are always posted and executed on the server. That is the place where the validation/redirect/database logic will happen.When JavaScript is available, a xhr request will happen, otherwise, a traditional HTTP Post will take palace.The only thing we need to do is export an action function in the root page and use their built-in forms API abstractions.In Next.js you can granularly configure how your data is loaded from your web application. You may choose for data to be loaded exclusively in the server, client, both and at runtime or build-time.A summary of the Next.js loading APIS:Why this variety of methods? That puts the developer at the driving wheel. The Next.js framework has mostly SEO-focused goals and claims that loading all in the server might hurt the app's performance.The framework includes the popular feature Static Site Generation. It has some further improvements. If you need those SSG pages to update periodically, you can use the stale-while-revalidate strategy in the getStaticProps method. That will create a web with Incremental Static Regeneration. You need a CDN that supports this feature.Example of ISR in action:The Remix vision is quite different. From their perspective, data should be always loaded on the server-side first. Because it is based on the fetch API we can execute the code on the edge by using services like Cloud Flare Workers.How does that work? On each root page, we can define a named function loader that will be executed in the server and data is then available through a react hook API in the server and client. The Next.js equivalent to this behavior is getInitialProps/getServerSideProps. On the Next.js implementation, you might face a lot of prop drilling though.Let’s see a Remix working example:That means that the page layout can be completely processed in the server and sent to the client in parallel. Once all is downloaded the user won’t experience any loading spinners. The only caveat is the time penalty of the data fetching on that same server.When it comes to routing both frameworks have a few similarities. They rely on a file-system-based routing system. It is becoming the standard when it comes to SSR frameworks. It makes the route page predictable and allows some optimizations.They do support nested routing and dynamic route segments. In Next.js routes must be placed on the pages directory.In Remix, those need to be placed in the app/routes directory:Their engines are different. The Remix framework is built on top of the React Router which has 45k starts in GitHub. Remix lets you create nested layouts. It achieves that by using the Outlet feature from React Router v6. You can create composed pages in a clean way and save time with caching and data-loading.You can opt out of that nesting layout URLs behavior. How? Just by using the . notation instead of creating folders. It will swap each . with a / in the URL. Unnested layouts are the way Next.js is rendering its routes.In my opinion, Remix has the upper hand in this area and Next.js has quite to catch up.The Next.js ships with a ton of features. They recently launched the Next.js Live. It enables real-time collaboration which might dramatically increase the productivity of the team.They also launched the middleware feature. It enables developers to write code over configuration. It brings JavaScript to the edge, closer to the user and thus making pages snappier. It combines the benefits of caching with the power of dynamic execution. The middleware logic can be scoped by page decoupling functionality like authorization from your components.The Next.js framework has support for turning any React page into an AMP page. That is useful if you want to create a news web page.We have seen how Next.js and Remix compare. They both have amazing features, deliver a performant experience, and have a great community. They both provide great developer experience with their lightning-fast Rust and Go compilers.But… Before making any decision be aware that Next.js comes with a certain level of vendor locking. To run it at its max peek with all its features you need to use the Vercel platform. Unlocking its full potential in another provider might be too time-consuming.There are some scenarios where choosing Next.js is a no-brainer: when you have static sites or need to use AMP. For when you need a responsive site without JavaScript you should use Remix.Other than that, it comes to a personal preference. Currently, I’m leaning towards Remix as I love its simplicity and its routing. However, in the future, we will be able to use features like React Server Components.Thanks for reading. If you liked what you read, check out these related articles as well:betterprogramming.pubbetterprogramming.pub",,1,2,16,1,1118,917,2,4,0,2,289
,,https://medium.com/u/8ae6a5b70ece?source=post_page-----c35fc29ccf6--------------------------------,Jose Granja,1800,,,,,,0,,,,,,,0,,
Next.js vs. Remix: Analyzing Key Aspects and Differences,Better Programming,https://betterprogramming.pub/next-js-vs-remix-analyzing-key-aspects-and-differences-8674beaba695,Jose Granja,1800,6,1145,"The React web-based ecosystem is looking better by the day. At the end of 2021, Next.js launched its great 12 release. It was packed with lots of features and optimizations. However, a little after, we saw how Remix unveiled its latest version. The Remix team changed its strategy and decided to be free and open-source.So when starting a project in 2022 which one should we pick? It is a hard choice to make and has a big impact on your web application. Each framework is opinionated and will shape how you think and build your components.In this article, we will analyze what is the key aspect of each and by the end, you will be able to decide if those fit you. Some aspects might be deal-breakers or decisive factors. It all boils down to your use case and personal preferences.In this paradox of choice just be reassured by knowing that both are great frameworks and you can’t go wrong with either of them.The Next.js framework is built on top of NodeJs. It is highly bound to the React ecosystem making it nearly an extension of it. It tries to embrace early on its latest features. The Vercel team works closely with the React one in order to make that happen. They mostly provide abstractions built on top of React components like: next/dynamic, next/head, next/link, …Their latest release already includes support for:You can enable both features in the configuration file:The approach from the Remix is noticeably different. They are more decoupled from the React Ecosystem and keener on waiting for stable React features. It is built on the Web Fetch Api (instead of Node) which enables the app to run anywhere. It focuses on leveraging as much as possible to the server. This has two benefits: faster bandwidths when loading data and the capacity to provide a progressive enhancement experience.Another drastic difference is that Remix tries to rely as much as possible on the Browser Native Web Standard Features. What does that mean? That their pages are 100% functional even with JavaScript disabled. When JavaScript is available the whole experience is enhanced.Their built-in form feature is a clear example of that. They rely on the basic Form browser feature. The form actions are always posted and executed on the server. That is the place where the validation/redirect/database logic will happen.When JavaScript is available, a xhr request will happen, otherwise, a traditional HTTP Post will take palace.The only thing we need to do is export an action function in the root page and use their built-in forms API abstractions.In Next.js you can granularly configure how your data is loaded from your web application. You may choose for data to be loaded exclusively in the server, client, both and at runtime or build-time.A summary of the Next.js loading APIS:Why this variety of methods? That puts the developer at the driving wheel. The Next.js framework has mostly SEO-focused goals and claims that loading all in the server might hurt the app's performance.The framework includes the popular feature Static Site Generation. It has some further improvements. If you need those SSG pages to update periodically, you can use the stale-while-revalidate strategy in the getStaticProps method. That will create a web with Incremental Static Regeneration. You need a CDN that supports this feature.Example of ISR in action:The Remix vision is quite different. From their perspective, data should be always loaded on the server-side first. Because it is based on the fetch API we can execute the code on the edge by using services like Cloud Flare Workers.How does that work? On each root page, we can define a named function loader that will be executed in the server and data is then available through a react hook API in the server and client. The Next.js equivalent to this behavior is getInitialProps/getServerSideProps. On the Next.js implementation, you might face a lot of prop drilling though.Let’s see a Remix working example:That means that the page layout can be completely processed in the server and sent to the client in parallel. Once all is downloaded the user won’t experience any loading spinners. The only caveat is the time penalty of the data fetching on that same server.When it comes to routing both frameworks have a few similarities. They rely on a file-system-based routing system. It is becoming the standard when it comes to SSR frameworks. It makes the route page predictable and allows some optimizations.They do support nested routing and dynamic route segments. In Next.js routes must be placed on the pages directory.In Remix, those need to be placed in the app/routes directory:Their engines are different. The Remix framework is built on top of the React Router which has 45k starts in GitHub. Remix lets you create nested layouts. It achieves that by using the Outlet feature from React Router v6. You can create composed pages in a clean way and save time with caching and data-loading.You can opt out of that nesting layout URLs behavior. How? Just by using the . notation instead of creating folders. It will swap each . with a / in the URL. Unnested layouts are the way Next.js is rendering its routes.In my opinion, Remix has the upper hand in this area and Next.js has quite to catch up.The Next.js ships with a ton of features. They recently launched the Next.js Live. It enables real-time collaboration which might dramatically increase the productivity of the team.They also launched the middleware feature. It enables developers to write code over configuration. It brings JavaScript to the edge, closer to the user and thus making pages snappier. It combines the benefits of caching with the power of dynamic execution. The middleware logic can be scoped by page decoupling functionality like authorization from your components.The Next.js framework has support for turning any React page into an AMP page. That is useful if you want to create a news web page.We have seen how Next.js and Remix compare. They both have amazing features, deliver a performant experience, and have a great community. They both provide great developer experience with their lightning-fast Rust and Go compilers.But… Before making any decision be aware that Next.js comes with a certain level of vendor locking. To run it at its max peek with all its features you need to use the Vercel platform. Unlocking its full potential in another provider might be too time-consuming.There are some scenarios where choosing Next.js is a no-brainer: when you have static sites or need to use AMP. For when you need a responsive site without JavaScript you should use Remix.Other than that, it comes to a personal preference. Currently, I’m leaning towards Remix as I love its simplicity and its routing. However, in the future, we will be able to use features like React Server Components.Thanks for reading. If you liked what you read, check out these related articles as well:betterprogramming.pubbetterprogramming.pub",,1,2,16,1,1118,917,2,4,0,2,289
,,https://medium.com/u/d8171ba3761?source=post_page-----c35fc29ccf6--------------------------------,Ryan Perry,171,,,,,,0,,,,,,,0,,
Continuous Profiling for Go Applications,Better Programming,https://betterprogramming.pub/continuous-profiling-go-applications-8cdbdfdfc5ab,Ryan Perry,171,5,535,"In this example, I show a basic use case of how to use Pyroscope — an open-source profiling library — to speed up a Golang application using continuous profiling.We simulate a “rideshare” company that has three endpoints found in main.go:I also simulate running 3 distinct servers in 3 different regions (via docker-compose.yml)One of the most useful capabilities of Pyroscope is the ability to tag your data in a way that is meaningful to you. In this case, we have two natural divisions, and so we “tag” our data to represent those:Tagging something static, like the region, can be done in the initialization code in the main() function:Tagging something more dynamically, as we do for the vehicle tag can be done inside our utility function FindNearestVehicle() using pyroscope.TagWrapper() :The functions of this are:The first step when analyzing a profile output from your application is to take note of the largest node which is where your application is spending the most resources. In this case, it happens to be the OrderCar() function.The benefit of using the Pyroscope package is that now that we can investigate further as to why the OrderCar() function is problematic. Tagging both region and vehicle allows us to test two good hypotheses:To analyze this we can select one or more tags from the “Select Tag” dropdown:Knowing there is an issue with the OrderCar() function we automatically select that tag. Then, after inspecting multiple region tags, it becomes clear by looking at the timeline that there is an issue with the us-west-1 region, where it alternates between high-CPU times and low-CPU times.We can also see that the mutexLock() function is consuming almost 70% of CPU resources during this period.Using Pyroscope’s “comparison view” we can select two different time ranges from the timeline to compare the resulting flame graphs. The pink section on the left timeline results in the left flame graph, and the blue section on the right represents the right flame graph.When we select a period of low-CPU utilization and a period of high-CPU utilization we can see that there is different behavior in the mutexLock() function where it takes 33% of CPU during low-CPU times and 71% of CPU during high-CPU times.While the difference, in this case, is stark enough to see in the comparison view, sometimes the difference between the two flame graphs is better visualized with them overlayed over each other. Without changing any parameters, we can simply select the difference view tab and see the difference represented in a color-coded difference flame graph.Whether you’re a developer working on a side project or a devops engineer wondering “how can I speed up my go application”, Pyroscope has a number of use cases that will make it easy to see how you can profile your application. Some examples are:If you’d like to try out this example and see what ways you can adapt this to your Go application here is a link to this example on github. Continuous profiling has become an increasingly popular tool for the monitoring and debugging of performance issues (arguably the fourth pillar of observability).If you have ideas for how Pyroscope can improve feel free to write an issue on the GitHub page! Thank you for reading.",,1,2,3,1,1115,647,6,6,0,4,136
Why we strayed from our middleware stack for a micro-services framework called Steve,Klarna Engineering,https://engineering.klarna.com/why-we-strayed-from-our-middleware-stack-for-a-micro-services-framework-called-steve-62fdebde197c,Taavi Laanemaa,3,10,1821,"I’ve worked with Node.js micro-services for a while now, and the same set of struggles seem to pop up wherever I go. When I joined the Consumer Card domain at Klarna, I saw many of the same things.A common issue with designing well-structured backend services with Node.js is that you often need to pass something from the very top layer to the very bottom. This often leads to ugly code where the middle layers need to know about some arbitrary data being passed through them, which they themselves don’t use.First, there’s the issue of passing a request context through the application logic. For example, you usually want your logs to contain some identifier connecting them to the request that was being served — this is often called a correlation ID. But how does the correlation ID get to where you log?Here’s an example of how we could pass a logger that knows about the request context to where it’s needed:As you can see, we’re forced to pass the logger into our getFavoriteNumber function, even though it doesn’t have anything to do with its logic. Not ideal.Here’s what the getFavoriteNumber function could look like:Now, while it is annoying to pass the logger around like this, it does give us the benefit of easy testing. We can easily construct a mock within the test and pass it to the function — no module mocking and mock resetting needed. Test-local mocks are also preferred as there’s no chance of them getting mixed up between each other, and they’re just easier to work with and understand.Here’s an example of a test with both local and global mocks:Another way to achieve this is by designing your codebase as a stack of Express middleware. This allows you to use the request object as a central message bus that carries data from one function to another.Here’s an example of a middleware-based design:And our function could be written as a middleware like this:This has the benefit of making it easy to pass the request context around; we just attach it to the request object that acts as our central message bus. It’s also easy to test since we can build mock request objects and pass them into our middleware function.However, since we’re mutating a single object over and over as it passes through the middleware, it’s tough to be sure about what exists on it at any given middleware. As a consequence of that, it’s also very difficult to type the object as its shape in any given middleware depends on the sequence of middleware before it. This makes it hard to know what our middleware depends on, so you need to keep the whole middleware stack in your head when working on a single middleware. You could say that the middleware are heavily coupled because any change in one could potentially affect the others, which creates a significant and unnecessary cognitive load when working on them.These experiences led me to the idea of using a design pattern that would be easier to reason and work with. I pitched the idea of building a micro-services framework to the team, and it was well-received. The solution I suggested was to utilize dependency injection to enable a good system design. This would become the backbone of the new framework and would heavily influence the final product.Once we were all in agreement, we set to work answering the critical question of what we look for in a “good design.” This is where my past experiences with Node.js came in.The main benefits of a good framework are that it simplifies the development process and helps control complexity in your application.You want the framework to address some common concerns for you, like handling HTTP traffic, and enable you to quickly write well-structured code that is easy to reason about and test. Ideally, you want your application to be made up of clearly separated pieces that are easy to reason about, and a good framework should give you that.For us, the main objectives we identified for our new framework were as follows:We named our Node.js micro-services framework Steve the Stitcher. The big idea about Steve is the container. You hand your components to Steve, and Steve puts them into a jar. Then, when you need a component, you just ask Steve to give it to you. Steve then reaches into the jar, finds your component, figures out how to initialize it with its dependencies, and hands it to you. You don’t have to worry about how the dependency graph is resolved, Steve does that for you; it stitches the components together.In other words, Steve makes heavy use of dependency injection and emphasizes readability and developer experience. Dependency injection essentially means that, instead of a component importing its dependencies, it effectively “asks” for them, and they will be injected at runtime.On a more practical level, Steve is a wrapper around a library called Inversify. Inversify is a pretty minimal dependency injection library that does one thing and does it well. Steve uses that to facilitate the injection and then adds support for more everyday concerns like handling HTTP requests and so on.To make it easier to build real systems with Steve, we’ve implemented three main types of components: Controller, Service, and Client.This is your transport layer. Anything related to traffic coming into the service should be handled here. Keeping your transport layer separate from the business logic allows you to change the transport with ease, for example, going from a synchronous HTTP API to an asynchronous Kafka-based one.Here’s what a controller looks like:As you can see, there are two main parts to it: validate and run.Incoming requests are first passed into the validate method to ensure they meet the controller’s requirements. This is where you’d implement your payload validation. Whatever is returned from the validate method is then passed into the run method for processing. Keeping those two concerns separate enables us to work with known objects in the run method. Notice how the controller class implements the HttpController<Payload> interface? This is what links together our methods and gives us helpful type safety. The validate method has to return an object of type Payload (or throw an error), and the run method has to take a param of the same type.This is where you handle your business logic. Services should not know anything about the transport layer, no requests or Kafka messages, just business logic. As mentioned above, this decoupling makes them easily portable, so you can move your service between different endpoints or even to a CLI if you want to. When reaching out to the outside world, the service should utilize a client.Here’s what a service looks like:As you can see, it’s a simple class that takes all of its dependencies as constructor parameters and exposes some helpful public methods. Notice how we can inject the class-based dependencies with no special notation, but we need to use the inject() decorator with ones that aren’t classes? That’s because classes work as both a type and a reference, allowing us to use them for injections. Plain values don’t have that, so we must manually provide the injection ID and the type. Don’t worry about the local dependencies here, it’s just the console object we’ve registered as a constant for example purposes. We’ll cover how that’s done in a second.The testing benefits are also evident here as we can easily initialize our service with mock dependencies. As long as they match the interface, the service wouldn’t know the difference.Here’s what a test for this service might look like:Notice how easy it is to build and use mocks for our dependencies.Clients act as a barrier between the outside world and your service. While controllers control incoming traffic, clients control outgoing traffic. Any external system, be it a database, some other API, or even logging, should go through a client. That allows you to easily replace them if, for example, you want to go to a newer version of an API or completely replace a REST API call with a DB call.A client would look the same as a service, there isn’t any difference in how they’re implemented, but the different name still helps in structuring your codebase.Finally, once we have all of our components, we’ll have to register them with Steve. That is done with the different registration methods, depending on the type of components.Here’s an example:This is where the logger we used earlier is registered. As you can see, it’s just a constant value, but we register it with an ID and a type to give us something to use when injecting and to provide some extra type safety.Also, notice how controllers need to provide a route and a method upon registration. That is how Steve knows which request to route to the controller. Services and clients don’t require any extra information since Steve doesn’t do anything special with them; they’re just injected when asked for.Combining these components gives us a system design like this:But what is made injectable from every request? And how does error handling on HTTP routes work? That’s what the connector does. You can think of the connector as a plug-in to Steve that tells it how to do those exact things.Remember the console constant we registered and used as a logger? The connector is where you’d regularly set up components like that. You need to provide a method on your connector that receives the HTTP request and returns a map of component IDs and values. This way, you can hook into Steve and register some dynamic components before the request reaches your controller.The connector is a mandatory parameter when creating an instance of Steve, but a default connector is included to make your life easier.As a whole, I think Steve has been received very well. When I first approached the idea of doing services differently, I imagined it would be limited to my team. But, as time went on, it became apparent that it would be a somewhat Klarna-wide project. There’s been interest from multiple teams and, while Steve is currently only used in production by a single service (that my team owns), I’d say that what we have so far has been wildly more successful than I originally anticipated.But, with that success also came a problem. Developing a Klarna-wide library as a single product team is unfeasible. It takes significant effort to develop and maintain a library like that. Effort a product team cannot invest when there are other product priorities. Our current approach to solving this problem is to open-source Steve. Once Steve is once-sourced and put on GitHub, its development will be carried on in a hobby project format by me and others interested in it.In the meantime, we have several new features in the works. We’re planning to add Kafka and GraphQL support. We’re also looking into building custom decorators for components and a singleton component initialization helper.",,1,0,10,1,1063,516,3,3,0,3,108
"CQRS Software Architecture Pattern: The Good, the Bad, and the Ugly",Better Programming,https://betterprogramming.pub/cqrs-software-architecture-pattern-the-good-the-bad-and-the-ugly-e9d6e7a34daf,Jawad Margieh,165,11,1828,"The Command and Query Responsibility Segregation (CQRS) it’s an architectural pattern where the main focus is to separate the way of reading and writing data. This pattern uses two separate models:In a Nutshell -The Command and Query Responsibility Segregation (CQRS) pattern separates read and update operations for a data store.Implementing CQRS in your application can maximize its performance, scalability, and security.The image below illustrates a basic implementation of the CQRS Pattern. (figure 1.)Later on I will address the sync between both storages.Commands represent the intention of changing the state of an entity. They execute operations like Insert, Update, Delete. Commands objects alter state and do not return data.Commands represent a business operation and are always in the imperative tense, because they are always telling the application server to do something.Queries are used to get data from the database. Queries objects only return data and do not make any changes.Queries will only contain the methods for getting data. They are used to read the data in the database to return the DTOs to the client, which will be displayed in the user interface.Event Sourcing ensures that all changes to application state are stored as a sequence of events.Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.Change Data Capture (CDC) is a solution that captures change events from a database transaction log (or equivalent mechanism) and forwards those events to downstream consumers.CDC ultimately allows application state to be externalized and synchronized with external stores of data.CDC can be implemented using Debezium as:CQRS can be considered to be used on scenarios where:When working with CQRS, it’s also possible — but it’s not mandatory — to have separate databases:When we deal with more complex applications like microservices or any kind of application that has a high demand for data consumption, the common approach can become unwieldy, because having much writing and reading in the same database can affect the performance of the application (read and write workloads have very different performance and scale requirements).If separate read and write databases are used, they must be kept in sync. Typically this is accomplished by having the write model publish an event whenever it updates the database.More information later about events and different approaches on keeping the read and write databases synchronized.By separate models we most commonly mean different object models, probably running in different logical processes, perhaps on separate hardware.It’s common to see CQRS system split into separate services communicating with Event Collaboration. This allows these services to easily take advantage of Event Sourcing.The CAP Theorem states that between consistency, availability, and partition tolerance (which can generally be considered “scalability”) we can only have two.A huge advantage of CQRS is that you will be able to relax the consistency requirements of the read model in order to gain high availability and scalability.So…Why we need the Transactional Outbox Pattern?Transactional Outbox pattern is used for reliable messaging and guaranteed delivery of events.Microservices often publish events after performing a database transaction.Writing to the database and publishing an event are two different transactions and they have to be atomic.A failure to publish an event can mean critical failure to the business process.This approach works well until an error occurs between saving the entity object and publishing the corresponding event. Sending an event might fail at this point for many reasons:Whatever the error is, the result is that the EntityCreated event can't be published to the message bus. (figure 2.)Other services won’t be notified that an entity has been created.The Entity service now has to take care of various things that don't relate to the actual business process.It needs to keep track of events that still need to be put on the message bus as soon as it’s back online.There’s a well-known pattern called “Transactional Outbox” that can help avoid these situations.This pattern provides an effective solution to publish events reliably. The idea of this approach is to have an “Outbox” table in the command service’s database.When receiving a request for creating entity (Command), not only an insert into the Entity table is done, but a record representing the event is also inserted into the Outbox table.The two database actions are done as part of the same transaction. (figure 3.)An asynchronous background process monitors the Outbox table for new entries and if there are any, it publishes the events to the Event Bus.Transactional Outbox pattern guarantees messaging reliability and at-least-once delivery.Outbox table records describe an event that happened in the command service.For example, a JSON structure representing the creation or update of an entity, data on the entity itself, as well as contextual information, such as a use case identifier.By explicitly emitting events via records in the outbox table, external consumers can be assured that events are structured correctly.This also ensures that event consumers will not break when, for instance, the internal domain model or Entities table are changed.Log-based Change Data Capture (CDC) is a great fit for capturing new entries in the outbox table and stream them to Apache Kafka. As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime. Debezium comes with CDC connectors for several databases such as MySQL, Postgres and SQL Server.The below diagram (figure 4.) describes the overall architecture which is centered about two microservices, the admin-service and user-service. since Debezium is used, both microservices are written should be written in Java.Debezium tails the transaction log (“write-ahead log”, WAL) of the Admin service’s database in order to capture any new events in the outbox table and propagates them to Apache Kafka. refer to Outbox Event Router configuration for more about event routing.id — Unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure. Generated when creating a new event.entity_type — The type of the aggregate root to which a given event is related; the idea being, leaning on the same concept of domain-driven design.entity_id — The id of the aggregate root that is affected by a given event; this could for instance be the id of a Catalog.topic — the topic of event, e.g. ""Catalog Created"" or ""Catalog deleted"". Allows consumers to trigger suitable event handlers.payload— a JSON structure with the actual event contents, e.g. containing a catalog information.When registering the Debezium connector — we should make sure to discard Deletion of Events from Kafka Topics. By setting the tombstones.on.delete to false, no deletion markers (""tombstones"") will be emitted by the connector when an event record gets deleted from the outbox table.Outbox message processing can be done in a separate process depends on the use-cases. However, instead of creating a new process, you also might use the same process but with in a separate thread, depending on the requirements. (figure 5.)The idea is to use a scheduler that will periodically run and process messages from the Outbox table. the task execution schedule can be configured to run every few seconds. The scheduler’s configuration really depends on how many messages you will have and need to be processed in your system.As we’re using Postgres for persistency of the Outbox table, we will use Postgres’ CTE (common table expressions) to fetch 100 messages ordered by their time creation as well we identify and fetch messages which are being processed for more than 2 minutes. notice that we also set the status of the outbox message to PROCESSING in the same transaction.Putting it all together —As I wrote earlier, if there was an error between processing the message and publishing it to setting it as processed in the outbox table, the task would pick up that message after 2 minutes from last time it was picked in the next task iterations. that way we guarantee at-least-once-delivery.status —picked_at — The time in which the message was picked up by the job for processing. this value is also used to identify messages that were picked for processing but their status wasn’t set to PROCESSED. Messages that were picked for more than 2 minutes their status is set to PROCESSING will be picked again by the job.The rest are self-explaining.Why is it so hard to add a simple entry to some list and return the new list as a response? Well it all depends on the context.Let’s start with the more common context, where the data is on a single machine and the users are in a reasonable size, for example, hundreds or thousands of users. In our case CQRS was complex, We have prioritized availability where users favored to receive consistency.Take, for example, Facebook or Twitter, which must serve millions of users at the same time and must optimize for scalability and availability. It doesn’t matter if you get 10 instead of 11 likes or comments for a couple of minutes as long as you can still use their platform and continue to be engaged. or for google to display the most updated search result instantly.In this case, CQRS is not difficult since the effort of synchronizing several storages is less that the overhead of building single model that accomplishes everything. The increased complexity of synchronizing the two models is countered by reducing the scalability challenges.CQRS adds complexity to the system, making it more difficult to change it. This only makes sense if you have some extreme performance issues that are better dealt with two data models rather than one. This does not apply to the majority of applications.It’s possible that CQRS is a good fit for you. Have you tried a simpler approaches first? Do you have any numbers to back up your claim that a typical relational database won’t suffice? Did you create a proof of concept (POC) to demonstrate the feasibility of the solution?Last but not least, when it comes to eventual consistency it is the product owners to decide whether it fits the business requirements or not.IF you’re using a NoSQL, have you considered migrating to a RDBS? or have you considered using materialized views to pre-compute your queries? or even maybe data remodeling?CQRS and Event Sourcing aren’t a mystical combination. It’s important to grasp the many implications of the two patterns before starting on your journey. Otherwise, it is quite easy to make a total mess both on a technical and functional levelCQRS and/or Event Sourcing and/or CDC can be an excellent solution to many problems assuming you have a clear understanding of the constraints and downsides.Do not rush with making decisions out of necessity that will go against your original plans for your business. Before implementing the CQRS pattern into your system make sure to do a POC, and other testing. creating a proof of concept to test your solution will ensure you arrive at the best version of it and will save you time and money in the process. Don’t be afraid to get your hands dirty.Thanks for reading!",,10,0,14,9,1122,693,6,9,0,14,832
The Clean Architecture — Beginner’s Guide,Better Programming,https://betterprogramming.pub/the-clean-architecture-beginners-guide-e4b7058c1165,Bharath,382,6,1081,"The Clean Architecture is the system architecture guideline proposed by Robert C. Martin (Uncle Bob) derived from many architectural guidelines like Hexagonal Architecture, Onion Architecture, etc... over the years.This is one of the guidelines adhered to by software engineers to build scalable, testable, and maintainable software.“The goal of software architecture is to minimize the human resources required to build and maintain the required system.” ― Robert C. Martin, Clean ArchitectureHere’s the clean architecture illustration created by Robert Martin:We can see there are four layers in the diagram. Blue layer, Green layer, Red layer, and Yellow layer.Each circle represents different areas of the software. The outermost layer is the lowest level of the software and as we move in deeper, the level will be higher. In general, as we move in deeper, the layer is less prone to change.The Dependency Rule states that the source code dependencies can only point inwards.This means nothing in an inner circle can know anything at all about something in an outer circle. i.e. the inner circle shouldn’t depend on anything in the outer circle. The Black arrows represented in the diagram show the dependency rule.This is the important rule that makes this architecture work. Also, this is hard to understand. So I’m gonna break this rule at first to let you understand what problems it brings and then explain and let’s see how to keep up with this rule. So please bear with me.First of all, this circular representation might be confusing for many. So let's try to represent it vertically.The colors represented here are the same as the colors represented in the clean architecture diagram.Remember, the arrow should be read as “depend on”. i.e. Frameworks and Drivers should depend on Interface Adapters, which depend on Application Business Rules which depend on Enterprise Business Rules.Nothing in the bottom layer should depend on the top layer.Software areas that reside inside this layer areThis layer holdsRules which are not Core-business-rules but essential for this particular application come under this. This layer holds Use Cases. As the name suggests, it should provide every use case of the application. i.e. it holds each and every functionality provided by the application.Also, this is the layer that determines which Controller / Gateway to be called for the particular use case. Sometimes we need controllers from different modules.This is where different modules are coordinated. For instance, we want to apply a discount for the user who purchased for x amount within a month.Here we need to get the amount the user has spent on this month from the purchase module and then with the result we need to apply the discount for the user in the checkout module. Here applyDiscountUseCase calls the purchase module’s controller for the data and then applies the discount in the checkout module.This is the layer that holds core-business rules or domain-specific business rules. Also, this layer is the least prone to change.Change in any outer layer doesn’t affect this layer. Since Business Rules won’t change often, the change in this layer is very rare. This layer holds Entities.An entity can either be a core data structure necessary for the business rules or an object with methods that hold business logic in it.For example: calculating Interest module in the banking application is the core business logic that should be inside this layer.Let’s look at a simple example to understand this well.The example demonstrates a simple application that has only one network request.How can we architect an app that translates the sentence given by the user using a translation API? let’s try to architect.Each layer does a specific thing. Looks good right? Let’s check the dependency flow for this above architecture to know if anything is wrong.Remember Dependency Rule? “The Dependency Rule states that the source code dependencies can only point inwards”.UI → Presenter (✅ Not Violating)Presenter → Translate Usecase (✅ Not Violating)Translate Usecase → Translate Controller (❌ Violating)Translate Controller → Web (❌ Violating)But it seems correct, right?UI requests data from Presenter which requests data from Use Case which should request data from Controller which should request data from Web.After all, how can we expect the web to throw some data to the Controller without the Controller being dependent on it? Also, how can we expect the Use Case to get the proper data from the Controller without depending on it?But the Dependency Rule strictly says dependencies can only point inwards. It adds up by saying this is the rule that makes the architecture work.In order to pass this rule, we need to invert the arrow to the opposite direction. Is that possible? Here comes Polymorphism. When we include some Polymorphism here, something magic happens.Simply by having an Interface between these 2 layers, we could invert the dependency. This is known as The Dependency Inversion Principle.Let’s implement the Dependency Inversion Principle in the cases where the Dependency Rule is violated.Thus the flow becomes:Let’s check the dependency flow now to know if anything violates it.Now we can see that no inner layer depends on any outer layer. Rather, the outer layer depends on the inner layer.So why should the outer layer depend on the inner layer but not the other way around?Imagine you’re in a hotel. We want the hotel to serve us what we want, but not what they offer right?. The same thing is happening here, we want the DB to give the data the application needs but not the data it has.Application orders what data it wants and it doesn’t care how DB or API prepares the data. This way, the application doesn’t depend on DB or API. If we need/want to change the DB or API Schema in the future, we can simply change it. As far as it gives what the application asks for, the application doesn’t even know the change in DB or API.Also, the single-way dependency rule saves the application from the deadlock state. i.e. imagine in a 2 layer architecture, the first layer depends on the second layer, and the second layer depends on the first layer. In such a case, If we need to change anything in the first layer, it breaks the second layer. If we need to change anything in the second layer, it breaks the first layer. This can be rejected by following the deadlock state.This is the clean architecture described by Uncle Bob.We are yet to see how to move the data across the boundaries and how to handle errors. We’ll do so in future articles.Thanks for reading.",,18,0,10,12,838,803,9,3,0,3,2400
"Don’t Use CRUD Styled APIs, Consider Intent-Based Rest APIs",Better Programming,https://betterprogramming.pub/intent-based-rest-apis-or-an-alternative-to-crud-based-rest-apis-1815599db60a,Dominic C,83,5,864,"Looking back on this article, I believe the example used is quite poor in the context of an Intent-based API. Due to this, I wrote a sequel article to this one to not only expand on the topic but also better explain some of the sections I explained poorly here. Feel free to read the article linked here next, it should fill in a lot of gaps and re-explain some concepts better and more correctly. Out of respect to Better Programming, I’d like to mention the sequel article is on a different publication.REST APIs have become an industry-standard in recent years, seemingly taking over RPC and SOAP APIs in terms of popularity.Coined by Roy Fielding in his dissertation in 2000, the term REST or RESTful stands for Representational State Transfer, and is a design paradigm for building APIs that has several core constraints:Oftentimes, REST APIs are built to mimic a CRUD-like interface, and this stems from the fact that the HTTP standard methods are structured very similarly to the CRUD interface, and REST APIs deal in resources that can be manipulated by said interface.Extrapolating from this, the CRUD life-cycle of a REST resource is thus: You create a resource, maybe you want to update it occasionally. Down the track, you read the resource, and maybe you decide to delete it.This article isn’t a CRUD slander article, I promise. It has its place in the world (in most systems, truthfully), and personally, I believe it’s really handy in a majority of cases.When a domain or problem set starts to spiral outwards in terms of complexity, however, I believe CRUD starts to show some of its weak points.Anyway, let’s create an example we can use for the rest (heh) of this article, it’s not a very complex design, but it should serve the purpose of demonstration.Here we have a simple model for some kind of product shipping domain.The entity here is the order and it encapsulates an active orders state, and has several value objects within it to help explain that state.It’s true, and that’s fine for a lot of cases, especially regarding smaller domains or models. Many resources contain sub-resources or fields within them that are tied to certain pieces of domain logic.In the example above, if you wanted to make changes to the orders CurrentLocation, there may be three reasons why:Let’s say you were given an Order that was in progress, and its CurrentLocation was “Arizona”. Its destination was “Mexico”.In a normal CRUD like operation, to make a change to the Order’s CurrentLocation to deliver the Order, you might do something like this:This is coarse-grained as it takes the entire object as the payload, which forces the user to have knowledge of how the business logic works in order to create a correct call (i.e. if CurrentLocation == Destination, then the status should be Delivered).This is a request tailored to the CRUD interface, as opposed to being tailored for the problem at hand.By understanding that a user will interact with your API to perform specific actions or with a specific intent in mind, it is easy to tailor the models of the request to these intents to alleviate the burden of knowledge from the end-user and shift it into the system’s responsibility.This strays away from some of the conventional aspects of REST and into almost an RPC-style area, as the API allows actions on verb-based resources or intents/actions as opposed to traditional nouns.In the example above we saw some cons in the CRUD styled request, so let’s compare it to a possible intent-based variant:Pretty simple, eh? Well, we’re dealing with a very simplified domain. Regardless, this requires the user to have much less knowledge of the domain and makes it impossible for the user to create requests that would leave the data in a bad state.By POSTing to this resource, we’re acknowledging that the user wants to set the given Orders state to delivered. So, let’s set it’s CurrentLocation to the Destination, and while we’re at it, let’s change its Status to delivered.Through this, we fulfill the user's intent and perform the required business logic within our system, without requiring the user to understand how the fields should wire up together.As mentioned, this is a very simplified example, which is why it may seem a bit plain or simple, however, GitHub actually does something similar with its merges, and a few other places in its API. It uses the resource merges and accepts a POST request with a required few fields, however, it ends up returning a “201 created” for the commit resource. The merges resource doesn’t map to a physical entity, but instead an intent. Neat!This may come as a shock to you, but even though CRUD APIs work great in a lot of use-cases, it doesn’t work so well in all of them. Intent APIs exist as a way to reify the user intent as an API model, pulling away domain logic from the request or chain of requests, and into the handler for that intent resource, nice and safe away from the poor client.ThoughtWorks has a brilliant article on this topic which goes into much greater detail. Definitely check it out!",,38,1,9,16,969,883,4,2,0,6,1000
,,https://medium.com/u/faf2e58f3d28?source=post_page-----c35fc29ccf6--------------------------------,Jennifer Fu,1500,,,,,,0,,,,,,,0,,
,,https://medium.com/u/cb3118e5eb15?source=post_page-----c35fc29ccf6--------------------------------,Akshay Kumar,87,,,,,,0,,,,,,,0,,
,,https://medium.com/u/89e0540c5c95?source=post_page-----c35fc29ccf6--------------------------------,Inna Sinicka,120,,,,,,0,,,,,,,0,,
,,https://medium.com/u/d7b3eaa11fe7?source=post_page-----c35fc29ccf6--------------------------------,Dominic C,83,,,,,,0,,,,,,,0,,
I Built a Blog with RemixJS so You Don’t Have To (You’re Welcome),StackAnatomy,https://medium.com/stackanatomy/i-built-a-blog-with-remixjs-so-you-dont-have-to-you-re-welcome-143c51aee8f9,Fernando Doglio,6800,8,1429,"RemixJS is in every React developer’s mouth right now, as they seem to be the new kid in the block. But how good are they? Instead of reviewing other people’s experiences, I went ahead and followed their quick start guide to build my own mini static blog and then added some dynamic behavior. Was it good? Does it live up to the hype?Let’s find out.Getting started with Remix is relatively easy, although their one-liner didn’t exactly work as planned. I guess that’s because I’m on a Windows box and NPM had some trouble installing some dependencies. I’m assuming this is something they’ll work on in the future.So according to them, all you have to do to create and then start your project is:And then answer the questions. My problems:Mind you, this is not a big deal, but just so you know, if this happens, the best alternative for you is to then execute:That will boot up the dev server and it’ll watch on any changes you perform to the files while working on your code.Everything else, to be honest. Once I managed to get the dev server up and running (meaning, once I understood the script I had to run), it just worked. The generator worked properly and created every file it needed and even though I had a bunch of errors during the dependency installation step, the needed ones were there, so it all worked.The watch feature also was great because both the server and the UI were updated before I could finish ALT+TABing into the browser.So far, let’s give it an 8 out of 10.Any basic blog will have 2 types of routes:Additionally, as part of the tutorial, you’ll also create an “Admin” section, which will allow you to create articles directly from the UI. This is interesting because it covers an aspect that the first 2 don’t: client-server communication.Routes are the bread and butter of everything you’ll do with Remix. Not because this is a framework for designing APIs, but rather because there is no distinction between back-end and front-end code while you’re writing it. The communication between both parts is done by the framework as long as you follow some standards:For example, if you want to create a page that will list all your blog posts, you’d write somethign liek this:There are several things to note here:This makes it possible for you to provide the required data on the server-side the first time this page is rendered, and then to provide the dynamic content, requested directly by the UI using the fetch API. That's right, the framework is doing all the internal wiring for you. This is very powerful.As already mentioned, actions are functions that you defined (and export) when you have to do some data transformation. Routes that respond to POST, PUT, PATCH or DELETE will have their action function called before the loader and whatever the first one returns, that'll be the result of the request.Let’s see what an action function looks like following the blog example. In this case, this action takes care of receiving the “crate new blog post” request and saving the data to disk:Now, here are several interesting bits:Now, there is one interesting bit I didn’t mention so far: this action is part of the /posts/new route. And the default function on the route is the following:What I’m trying to show you here is that both, the above function and the action are part of the same file. Now, notice how the form element doesn't have an action attribute. That means that when you hit the ""Create Post"" button, it'll post its content onto itself. Thanks to the fact that you defined the action, our function will be called once the method goes from GET to POST (because of the form) and the redirect will prevent it from displaying the same form again.The wiring of actions and routes is so transparent that you don’t really have to worry about writing front-end or back-end code. You just have to worry about your business logic.This part worked great, and I while my old web dev brain had some trouble at first, it wasn’t the framework’s fault. So I would rate this part with a 10 out of 10.Debugging a web application in production may be challenging and time-consuming. OpenReplay is an Open-source alternative to FullStory, LogRocket and Hotjar. It allows you to monitor and replay everything your users do and shows how your app behaves for every issue. It’s like having your browser’s inspector open while looking over your user’s shoulder. OpenReplay is the only open-source alternative currently available.Happy debugging, for modern frontend teams — Start monitoring your web app for free.Now, this is where things started to get interesting. Once I had the basic routes working and some static content generation ready, the tutorial took me out for a spin with the “Admin” section, because it very quickly created a dynamic interface that would partially update depending on what I was doing.Here is a quick example of what I mean:The left-hand side always stays the same, just listing the blog posts on the system (the ones returned by the first loader I showed you before). And the right-hand side goes from showing a link, to the form from before and then back to the link.Now, this is not something unseen, however, the fact that it was so easy to implement is that mind-boggling part of this whole thing. Let’s take a look.The key to the above behavior are “nested routes”, which essentially allow you to define sub-routes. Something like /posts and /posts/new or /posts/edit/1234. The last 2 are nested within the first one.The interesting aspect from Remix though, is that nested routes map directly to nested UI components.So if you want to achieve this partial update on your UI, you’ll define a “wrapper” route inside your “routes” folder, like this:There are 2 main points to notice here:From now on, you can define a set of routes within the routes/admin folder (notice how the name of the folder and the above file match). And all those routes will be rendered within the Outlet component. I've already shown you the/routes/admin/new.tsx file, it was the one with the form.The cool thing to notice here, is that the form is only saving a file to disk. That’s it, the left-hand section of our page, which is controlled by the above code is automatically pulling the updated data without us having to worry about any of it. Did you catch that? Go back to the GIF and notice how the left-hand side is updated after I save my new article.If we’re talking about ratings, I would rate this feature with an 11 out of 10.I’m not big on CSS, however, the way Remix handles the styling of components is quite simple. If you see the above code snippet, you’ll notice the links function being exported. Every route can export its own links and these functions will return an array of stylesheets for the framework to merge and render. And you can just import the CSS file into a variable and then use that as part of the return of the links function:This is all you need to worry about, the nested routes will share the routes from their parents as well as their own. It’s a simple, yet powerful solution to understanding how to load what on each section and prevents you from having to worry about modifying the actual root template depending on each section.I personally found this quite easy and simple, so it’s a 9 out of then, considering the class attributes on the HTML code doesn't work (you need to use className instead).Remix is a very easy-to-use framework as long as you’re the type of developer that is OK with the Convention over Configuration principle. This means that as long as you abide by their rules, a lot of things happen automatically and simplify your life (like nested routes and dynamically updated sections of the UI, or the UI-Backend connection happening without you writing a single line of code). And on top of that, the documentation, while not yet finished, is great and filled with useful examples.If you’re wondering whether or not you should try it, RemixJS needs a chance, so the answer is “yes, you should try it right now”.Finally, in case you’re wondering about the rest of the code, you can get the full source for my example here. Just clone it, and run npm install && npm run dev, it should work.Originally published at https://blog.openreplay.com on January 23, 2022.",,,2,2,5,935,548,3,6,0,5,91
,,https://medium.com/u/4ca6a60d91c0?source=post_page-----c35fc29ccf6--------------------------------,Amy Blankenship,504,,,,,,0,,,,,,,0,,
,,https://medium.com/u/d55832d5b3b1?source=post_page-----c35fc29ccf6--------------------------------,Fernando Doglio,6800,,,,,,0,,,,,,,0,,
,,https://medium.com/u/98c3d5a5a383?source=post_page-----c35fc29ccf6--------------------------------,Manfred Lange,128,,,,,,0,,,,,,,0,,
Creating a Dev Container for TypeScript and VS Code — Part 1,,https://manfredmlange.medium.com/creating-a-dev-container-for-typescript-and-vs-code-part-1-e653bb95c27f,Manfred Lange,128,16,3303,"In this article:This is part 1 of a two-part article. Part 2 can be found here.Containerization as a concept for deploying SaaS (Software as a Service) products has now been established for quite some time. AWS, Azure, Google, Portainer and others offer a range of options for deploying and managing container-based systems.In this article I will demonstrate how to create a dev container for TypeScript from scratch. You will need to have a basic understand of Docker. You need to understand the difference between a container image (short “image”) and a container instance (aka “container”) and how a container is different to a virtual machine.Also, you need to have some initial experience with VS Code and how extension add functionality to VS Code. Some familiarity with some basic bash commands won’t hurt but I’ll explain them along the way.The complete source code for this article is available at https://github.com/RimuTec/dev-container-typescript/tree/part-1. Be aware, though, that the git repository may contain newer versions of the code base.Windows Only: There are a couple of points of interest for readers on a Windows computer. Additional details are provided in italics like this paragraph. Non-Windows users can skip those paragraphs.Windows Only: If you are on Windows like me, be prepared that we will be using WSL2. We will be using the Linux files system instead of the Windows file system (NTFS). For reasons, please see my earlier article “Docker Desktop on WSL2: The Problem with Mixing File Systems”.Let’s get started!By minimizing pre-requisites, it is easier to get a dev environment set up and it is more likely that the dev environment is consistent between computers.For the dev container in this article, we need the following prerequisites:Windows Only: If you are on Windows, you need the following two elements as well and prior to installing Docker Desktop:Windows Only: optionally you may want to install the new Windows Terminal.And that are all prerequisites you need. No need to install nodejs, npm or TypeScript on your host.When you open a folder in VS Code the “Remote Development” extension will check for the existence of one of two things:Since we will need several files for our dev container, we will use the second option.Open a bash shell and switch to a directory which we are going to use as our starting point.Windows Only: Starting a bash shell is the same as starting an app such as “Ubuntu 20.04 LTS” from the start menu. If you do, it will give you a terminal with a bash shell for WSL2.I typically create a directory named “projects” in my home directory. Let’s take this approach here, too. With the bash terminal open, execute the following sequence of commands:The command “cd ~” switches to your home directory. In may case this is “/home/manfred”. The command “mkdir” creates a directory. The command “cd” switches to that directory.Before you continue, you can confirm the Path to your current Working Directory by using the command “pwd”. In my case the output is “/home/manfred/projects/rimutec/dev-container-typescript”.Windows Only: If the path starts with “/mnt/c” then you are in a directory of the Windows file system. Use “cd ~” and then “pwd” again to confirm you have a path that does not start with “/mnt/c”.In this directory, we then start VS code with “code .” (note the space between “code” and “.”). This should give us a view like the following:Don’t worry about the two files “License” and “README.md”. You won’t have them in your environment. Their existence or content are not relevant to the dev container or for this article.Next we’ll create a directory named “.devcontainer” at the root of our workspace folder. You should then have the following:The next step will involve creating a range of files in the “.devcontainer” directory. I’ll explain their purpose and some highlights along the way. The files we are going to use are:When we issue a command to tell Docker to create a container image (short “image”), then based on the directory in which we issue the command, Docker will consider all directories and files in that directory as being required to build the image. All of that will then be sent to the build context. The smaller the set of directories and files sent to the build context, the faster the container can be built.There are some files or directories that are not required by Docker in our scenario. One example is all directories named “node_modules”.Therefore we can let Docker know which directories and files to ignore when creating an image. By convention this file is named “.dockerignore”, using the same naming convention like “.gitignore” for git to express which directories and files git should ignore.Here is the content of the .dockerignore file:Obviously, the file itself it not needed. We’ll cover the file “.env” later in this article. “.git” is the directory containing the local clone of a git repository. It and the file “.gitignore” are only relevant git. We don’t need them for Docker. The folder “.vs” and “.vscode” are Visual Studio and/or VS Code related folders. Again, these are not required for Docker. We also exclude “docker-compose.yml”. In some dev environments you may have additional docker-compose files, e.g., “docker-compose.prod.yml”. The naming convention in your environment may vary. In any case we don’t need them for our dev container.The directory “node_modules” is likely to make the biggest difference. I have seen “node_modules” directories with size in excess of 100 Mbytes. No need to copy all of that around.It’s not the end of the world if you miss an entry in this file. However, it pays to review its content occasionally to adapt it to your specific setup.The next file is the only file that is VS Code specific. All other files in directory “.devcontainer” are VS Code agnostic. You can also use them in environments with a different IDE (Integrated Development Environment).Let me share the content of “devcontainer.json” first. I’ll explain the content right after that.Line 3 gives the dev container a name. This name will be displayed in the bottom left corner of VS Code once the dev container is running. When you run multiple dev containers, this helps with identifying very quickly, which one you are looking at when you switch between different VS Code windows. You can choose this name freely. If you want to make it user friendly, consider that VS Code will prefix it with “Dev Container: “, so no need to include either. The name “ACME OrderService” is just to give one example for a name that looks meaningful. The resulting label will be “Dev Container: ACME OrderService”.Line 8 is the name of the “service”. Here the term “service” is the Docker definition. It refers to a container that we define in the “docker-compose.yml” file later. The name you choose here needs to match the name of a “service” (in the Docker sense) in the docker-compose.yml file. Other than that, it can be what you want it to be. No need to be creative, though, as the name doesn’t matter beyond this and the docker-compose.yml file.Line 9 tells VS Code (the Remote Extension actually, but I’ll leave out the detail here) in which directory to find the workspace. The path provided here is the path as seen within the dev container. The dev container we are going to create will have the working directory at “/src”.Lines 10 to 18 instruct VS which extensions you’d like to use when the dev container runs. This is one of the huge advantages of dev containers. For each source code repository, you can choose to have a different set of extensions. The VS Code installation on your host (Windows, MacOS, Linux) can be minimal with just “Remote Development” installed. No need to have extensions for .NET in a TypeScript container. Or to have npm installed in a Python dev container that implements a micro service. As a result, extensions that don’t make sense for a given git repository, stay out of the way.Line 19 tells VS Code what to do when we close the folder or VS Code. Here we want it to shut down the container(s) that we ran up when we started the dev container. In a simple way, VS Code will run some “docker-compose … stop” command. This will just stop the container(s) but not remove them. Note how I refer to singular and plural. It is possible to have a dev container with dependencies such as a container with a database server pre-installed. It’s possible to write the docker-compose.yml file accordingly. However, I won’t cover this scenario in this article.Finally, in line 20 we tell VS Code which user to run as when it remotely connects with the dev container. We will be mounting directories from the host into the dev container. This will give allow code running inside of the dev container to effectively access directory and files on the host. While in general this is not a big deal, you don’t want to run as root. If a 3rd party package is compromised, which has happened in the past not just for npm packages, then you want to limit the potential blast radius as much as possible. Concepts such as Zero Trust or Principle of Least Privilege help increase software security. In this case we will use the user named “node” when inside the dev container. It has no root privileges. “sudo” is not installed either.Now things will become more interesting. The next two files are Docker related so are at the core of what we want to accomplish with the dev container.With just a Dockerfile (see below) we can easily create a container image. Using commands like “docker” we can then also create a container from that image and run the container. This works just fine. However, often in development we may have dependencies. For example, we may need a Postgres database, or we may need RabbitMQ. Whatever it might be, the best option is to run up additional containers with the dependencies pre-installed. I may cover that scenario in a future article, though.A docker-compose file allows you to specify one or more “services”. Here the term “service” is used as Docker defines it. A service is essentially a container.With that in mind, let’s share the content of the file first, then explain its details.One note to start with: In case you are unfamiliar with YAML in general please check out sites such as this one.Line 1 tells Docker which syntax version the file uses. For a full description of Docker Compose see Docker’s web site. In general, it is not critical to always use the latest one. You may want to check, though, occasionally and go to a later version. At the time of writing there was a version 3.9. However, version 3.7 is just as fine.Line 3 indicates where the “services” in the Docker sense are listed. Remember that Docker calls a container a “service” in this case. We have one such service. It is named “devcontainer” (Line 4).Lines 5 to 16 describe some properties of our dev container.Line 5 gives the image, once built, a tag (see Docker terminology).Line 6 specifies the build context for docker-compose. This is a path relative to the directory in which the docker-compose.yml file is located. By convention docker will look in that location for a file named “Dockerfile” and use that file to build the container image.Line 7 gives the container a human readable name. We’ll see later where this will show up.Line 8 gives the dev container a hostname it should use. Docker sets up a virtual network within which different container can communicate with each other. For example, the dev container may talk to the container with the database server. The human readable name can be used for diagnosing the virtual network. In our example the command “ping orderservice.local” will work if the virtual network is configured correctly and if the command ping is installed. Virtual networking in Docker is not covered in this article.In line 9 we define the working directory within the dev container once is has started.Lines 10 to 13 list mounted volumes. Here we use two of them. The first is critical as it mounts a host directory to the path “/src” in the dev container. In other words: the directories and files and directories become accessible at “/src”. But which host directory is mounted? Look at the “..” just before the colon (“:”). “..” refers to the parent directory. Again, this is relative to the directory in which the docker-compose.yml file is located. One level up, gives us the root of our workspace. Or later it will give us the root of the local clone of the git repository.The second mount — see line 13 — is included to illustrate how you can make other files or directories available within your dev container. The example is for one way of making AWS credentials available. If you don’t use AWS or if you use a different mechanism to provide credentials, then you can delete lines 12 and 13 without side effect.Line 14 instructs docker which command to start when the dev container is running. Here we tell it to sleep infinitely. Without this command, the dev container will start and then terminate immediately. A container can be described as a runtime environment for one process. Once the process terminates, the container terminates. This is also one of the differences to a virtual machine, which typically runs dozens or even hundreds of processes at the same time. As a point of reference: on my Windows host, there are 441 processes running as I am writing this article. I have no idea what all of them are needed for, though, but hope they do something valuable.Yes, I admit it: This subtitle looks funny. However, I didn’t come up with the default name of that file. It feels a little like “PIN Number” or “LCD Display”. Anyways.The Dockerfile is the center piece for creating the dev container. Here is its content:Line 1 specifies which image we want to use as our base image. While there is a way to create an image from scratch, in general, you may want to use a pre-built image to start with. The one I selected here, has Node.js version 14.17.4 pre-installed. Obviously, it also comes with an operating system, here some Linux distro. When you select a particular node version, this may also be limited by what you find in the target environment. For example, when you implement a Lambda for AWS, only select versions of nodejs are supported. By choosing the same version in line 1 of the Dockerfile, you can ensure that you will be using a version that will be supported by AWS. One less problem to worry about.In line 4 we install a specific version of npm. Whatever version of nodejs you choose as the base image, it comes with an npm installed as well. However, that may not be the same version that you want to use. To reduce differences between developers and between the points in time when this is executed, you want to be very specific about the version. As each version may use a slightly different algorithm to determine direct and indirect npm package dependencies and versions, this can potentially lead to a different set of npm packages being installed between different developers.In line 9 we tell docker to update the apt package information to the latest available. This does not install any packages. It just makes sure the information about packages is updated.Lines 10 to 12 installs a couple of tools that may come in handy at times. The package “lsb-release” allows us to find out which distro we run inside the dev container. At times this is helpful when searching the internet for instructions to install other things in the dev container.With all the relevant files in place, it’s time to start up the container. Within VS Code first click the green area in the bottom left corner. This opens a list of commands useful for remote development. Click “Reopen in Container”.If all goes well and there is no error, you should end up with something like the following:The first time you start up your dev container, it will take some time because docker needs to download (“pull”) the base image(s) that you have specified. Subsequent starts are faster.You can see in the bottom left corner as well as in the title bar, that we are now running the dev container. VS Code still runs on the host. However, everything else runs inside the dev container. One way of finding out is running the command “pwd” which will give you “/src”. You can also run “lsb_release -a” and it will print something like the following:This shows that the distro in this dev container is Debian version 9.13 which is codenamed “stretch”.If I run the same command in WSL2 on my host I get this:Both share the same Linux kernel, though, as evidenced by the output of the command “uname -a”. For the host it looks like this:The output for the same command executed in the dev container is:Here it shows Microsoft’s kernel in the most recent version as of 30 Jan 2022. The output on your computer, both on the host and in the dev container, is likely to be different.To demonstrate that we have the desired version running inside of the dev container, we can use the commandsandand look at the output. It should be as follows:It does not matter whether your host is Linux, MacOS or Windows. The output will be the same in all cases.In this article we created a basic dev container for nodejs that we can use on Linux, MacOS and Windows. We also have confirmed that we have the desired versions available in the dev container.We also learned that we could switch to different nodejs versions if we want to be in line with the production runtime version. The example we used was a Lambda for AWS. Other scenarios exist, though.From here, you can already explore further options with regards to setting up the environment for TypeScript. Alternatively, you can wait until part 2 is published sometime soon.Please don’t hesitate to comment or ask questions about this article. Also, if you’d like to be notified of my future articles (about one per week), consider following me on Medium and subscribing to Medium through my referral page. Thank you!Thank you for reading!Update 06 Feb 2022: Part 2 is now available.The following links offer additional details about the concepts in this article as well as suggestions for additional material for more advanced topics.In a corporate environment where IT installs software for your, including VS Code, or where IT manages a firewall, a proxy, etc. for you, it is possible that you encounter problems when starting your dev container. Symptoms include:In these cases, work with IT to sort out the proxy and firewall settings.Windows specific: On your computer uninstall all VS Code instances you may have. Then install just one instance and install it just for yourself. If required, ask IT to remove the global VS Code installation and then give you the permissions to install VS Code as a user. The difference is that if VS Code is installed on a per-user basis, files are installed under your directory in “c:\Users” instead of in “c:\Program Files”. You typically have full control for the former but limited access only for the latter.Generally, I believe developers should have root access on their development computer. They need to be able to maintain their own toolset including the computer. It’s like a chef with their set of knives. If you want to find out what I mean, get a job in a restaurant kitchen, then just touch one of the chef’s knives. Their response will be quite particular!",,,4,8,25,867,352,10,7,0,20,8
Reverse Engineering TypeScript Types,Better Programming,https://betterprogramming.pub/reverse-engineering-typescript-types-21196a97a0f6,Amy Blankenship,504,9,1963,"TypeScript is easy, they said. All it is is saying what type you intend your variable to be/function to take/etc., they said. But what “they” are ignoring is that we as developers spend more time reading code than writing it, and understanding what type your variable should be to be used in someone else’s code can be an all-day project, especially if the code you’re trying to work with is designed to be flexible.While there are plenty of references on the internet for writing flexible types, there are far fewer that tell you how to recognize a type that is written to be flexible and how to give that type exactly what it is looking for.I especially found typeof and keyof to be extremely confusing. I used to say “typeof means it has the type it has and keyof means it has the keys it has.” And this was mainly due to the fact I was viewing it through the lens of trying to understand what type to give a variable somewhere, not through the lens of trying to construct a flexible type.My epiphany came through trying to construct more and more flexible types myself, and seeing how typeof and keyof work together to make this possible.So I thought it would be helpful to walk through an open-source type I found really gnarly and deconstruct how it fits together and what I think the author was trying to accomplish at each step. Note that I’m just using this type as an example, and this process is not specific to code included in react-testing-library or really directly related to tests.The type we’re going to be looking at is the react-testing-library RenderResult. Feast your eyes on this baby:Let’s talk a moment about what this is and why we care (or why I cared — most people probably will never care about this particular type). This type is what’s returned when you call render() from react-testing-library. In other words, it’s a representation of your imaginary DOM, with some additional bells and whistles you can use in your tests.Most examples of using render show dumping the result straight into variables through destructuring, which allows Typescript to infer the types of the individual pieces, such as Queries.If you are a fresh-faced innocent, and you haven’t yet been bludgeoned out of trying to keep your tests DRY by using beforeEach and afterEach, you might just want to store a reference to something like findByTestId or queryByText.In order to do that, you have to declare the variable outside beforeEach so it’s accessible from your test. This means you have to know what type that variable should be (unless you’re a complete savage and use any, in which case why are you even reading this).So that’s the why. Now let’s break this apart and see what sense we can make of this.For lack of any better system, I will usually first try to read a type from left to right and top to bottom, though often that’s not the best way to understand it. So let’s start with this:This is creating our type, RenderResult, using a generic input type Q, which extends Queries and defaults to typeof queries. We’ll stop there, because already I’m like what???To find out what Queries means, we need to step out to dom-testing-library. I could write a whole article on how to find referenced types, but not today. So that type looks like this.Well, what does that mean? It means that Queries describes a hash, and each of its properties is a function that takes an HTMLElement as its first argument and then some other arguments, and it will return you an HTMLElement (or something — read the rest of the list of potential returns yourself).The default value here, then, is going to be a type describing such a hash. One way to think of typeof is imagine you go to a car dealership and see this lovely red car. “What type of car is that?”“It’s a Ferarri.”You have an instance of the car, and you work backward from that to figure out what type of car you have. Or the Typescript compiler does. Let’s see if we can follow along to understand what Typescript “sees” in this instance.In the case of RenderResult, you have an imported “variable”, queries, and the typeof operator inspects it to discover what its properties are. I put variable in quotes, because it’s slightly more complicated than that but this is already going to be a long post so I’m not going to go down that rabbit🐰 hole.Let’s take a look at queries to see what that tells us about our type.We can see here that the exports from the queries file does, in fact, contain the two keys we’re looking for, queryByText and findByTestId. Could we then just type our variables as either of these (or the less-specific Query from above)? Sadly, no, we’re not finished.If you look at these types, you can see that the first argument for both of these queries is an HTMLElement. Compare that to the argument being used for getByText in this example, copied from the react-testing-library API documentation (line 17, passed as an argument to click).That’s a regular expression, not an HTMLElement. What’s going on here? Long story short, there are lots of different ways to access queries, and one of them is to just import them from react-testing-library (which internally punts to dom-testing-library).When you import queries this way, they’re “pure” functions, so they need a reference to an element to query within. When you destructure them from the RenderResult, they’re actually methods of the rendered root element. They don’t need a reference to that element. Nosce te ipsum.This doesn’t feel like it moved us forward much, does it? A smarter person would have cut it off there and just used the imported version or just used screen. But then you wouldn’t have this post to read, so there’s that. Let’s look back atRenderResult type and see if it helps us. As a reminder, RenderResult looks like this:And we just figured out that the queries exports contained the keys we were looking for, just the type wasn’t an exact match for the function we want to store as a variable. Let’s see if we can find something that refers to the keys of queries. Oh, look! there it is in line 18. typeof queries is the default typing for the generic type Q, and here we’re saying something something keyof Q.Before we do that, let’s first talk about all the code between the “=” on line 4 and the curly bracket at the beginning of line 18. That is basically a normal Typescript type that defines all the other properties a RenderResult has. None of those properties is the elusive findByTestId or queryByText we’re on the hunt for, so we’re going to ignore that entire section.If you look to the right of the “&”, you’ll see the entire rest of the line within curly brackets. If you squint a little, you can see this is an intersection type. In plain English (sorry international readers), this is a new type that contains all the properties of the type on the left and all the properties of the type on the right. Which are…what…?Let’s take a mental side-trip and imagine that instead of a type, this is a Javascript function. And the purpose of the function is to set a property whose name will come in as an argument to a value that will also be specified as an argument:This works because, in JavaScript, we can specify object properties with strings, like so:Typescript can do roughly the same thing, using indexed access types. If we look again at our queries exportstypeof queries[“queryByText”] ought to be a function that takes parameters that look like the parameters QueryByText takes and returns some kind of HTMLElement as per the return type of QueryByText. It then follows if we can access that type for that specific key that we could write code that “loops” through all the exports of queries and references each of those keys.Which, it turns out, is exactly what’s going on with {[P in keyof Q]: BoundFunction<Q[P]>}.So what’s all that BoundFunction stuff? Let’s ignore BoundFunction itself for a moment (we’ll come back, I promise) and just look at <Q[P]>. Doesn’t that look an awful lot like our JS function that accessed a variable property of an object? I’m gonna answer for you and say yes, it does (sorry if you were going to answer no). And that is sort of what it’s doing. Our “object”, Q, is actually the generic type we looked at from the very first line of the type definition, which defaults to the type of the exports of queries. That’s a mouthful, but skipping any of the distinctions there would make this harder to understand than it already is.I think of [P in keyof Q] as kind of like a loop that creates a “variable” P for each property of Q. And then on the right-hand side of the colon, we’re saying that the type that BoundFunction wants to use for its generic type is the actual type of queries[‘whatever is in P’]. Which we figured out how to track down above.This means that the key of findByTestId in our intersection type ought to be BoundFunction<typeof queries[‘findByTestId’]>. I’ll leave it as an exercise for the reader to go look at BoundFunction to see exactly how it’s modifying the type.If you can’t even with this anymore, and just want to get back to work, I get it. We’re done with the “meaty” part of the post, and now we’re into the weeds of my speculation of why this is done this way. Enjoy the rest of your day.For everyone else, let’s think about what might prompt someone to do this to themselves (and us!)My first thought was if you wanted to create a self-maintaining type where you can add new queries to the library without updating the type manually, this would be a way to do it. However, the same file that has the definition for BoundFunction in it also has a big long type that explicitly repeats all the keys of queries, so that seems unlikely.In the end, it seems that this is all about allowing users of the library to extend it with queries of their own.You can see that BoundFunctions looks to see if it got all the keys of queries, and if so it manually overwrites each key with the new definition that strips out the container argument, before tacking any user-supplied queries on at the end. If it does not get a type that looks like queries, it just dynamically creates bound queries from the user-supplied ones. I think it would be interesting to know if this is a result of code that existed before extensibility was added, or if there’s some limitation that makes it better to use the manual method vs. the dynamic method. The world may never know.With all that said, I did eventually recognize that often Repeating Yourself is the fastest way to do things in TypeScript, along with creating completely illegible code by skipping the step of creating an interim variable that I then have to research the type for. So my advice to you is to cut to the chase and just do what TS wants you to do, even if every principle you previously learned screams not to do it that way. Unless, of course, you just happen to want to understand the kind of meanderings we just went through above. Then knock yourself out.If you enjoyed this, consider readingamy-blankenship.medium.comIf you were thinking about subscribing to Medium and would like to directly support me with your subscription, you can do that hereamy-blankenship.medium.com",,1,2,0,6,1225,689,1,0,0,19,62
An In-Depth Guide for Create React App 5 (CRA 5),Better Programming,https://betterprogramming.pub/an-in-depth-guide-for-create-react-app-5-cra-5-b94b03c233f2,Jennifer Fu,1500,7,850,"Create React App (CRA) is a quick way to scaffold a React project. It can be easily generated by the command, npx create-react-app <project name>. With one command, we have the latest packages and the execution environment for a React project. It is convenient and effective.CRA 5 was released on Dec 14, 2021. It has the following new features and new packages:Let’s go through these details.create-react-app is a global command-line utility to create new React projects. The created projects have the latest version of react-scripts, which is currently 5.0.0. CRA 5 dropped support for Node 10 and 12, and it requires node 14 or higher. If the node version does not meet the requirement, create-react-app will fail.After setting the node version to 17 (14 or higher), the installation works.react-scripts includes scripts and configuration. Updating the Create React App project is achieved by upgrading react-scripts to a specific version. The official documentation recommends running the following command to upgrade to the latest version:By running this command, we upgrade react-scripts from version 4.0.3 to version 5.0.0 smoothly.Here are the differences of package.json files:We have upgraded the version of react-scripts. Here are the differences between CRA 4 package.json and CRA 5 package.json.The differences look minor. We can manually update the versions of testing-library and web-vitals to match the versions in CRA 5.If you use TypeScript, you can create a new project by running the following command:Here are the differences between JavaScript CRA package.json. and TypeScript CRA package.json.From CRA 4 to CRA 5, TypeScript has been updated from version 4.1 to version 4.5.For the Hot Module Replacement (HMR) runtime, fast refresh has been improved with the bailout behavior described as follows:In CRA 4, npx create-react-app my-app will use yarn to install dependencies if there is yarn installed. Otherwise, a flag can be set to use npm:This behavior has been changed in CRA 5. If the env variable, npm_config_user_agent, is set to 'yarn', it will use yarn as package manager:Otherwise, it depends on how the command is invoked:In CRA 5, here is react-scripts in the installed package-lock.json:Watch the above code. What do you find?All versions use the caret dependencies, i.e. these packages will use the highest minor versions.For comparison, many packages in CRA 4's react-scripts pin to the exact versions.CRA 5 unpins babel-loader, which has been causing issues when using CRA with Storybook. In addition, CRA 5 unpins all dependencies for better compatibility with other tools.What else do you find?Tailwind is a CSS framework, packed with classes like flex, text-5xl, font-bold, text-green-500, etc. These classes can be composed to build any design, directly in the markup.Tailwind scans class names in HTML files, JavaScript components, and some other templates. It generates the corresponding styles, and then writes them to a static CSS file. Tailwind is fast, flexible, and reliable — with zero-runtime.CRA 5 added Tailwind support.Normally, it takes 5 steps to set up and use Tailwind. With the pre-configured CRA 5, it only needs 3 steps:Step 1: Configure the template paths.Create the configuration file, tailwind.config.js, at the root directory:Step 2: Add the Tailwind directives to the CSS file.Here is the src/index.css:Step 3: Use Tailwind in React components.Here is an example of src/App.js:text-5xl sets font-size: 3rem and line-height: 1.font-bold sets font-weight: 700.text-green-500 sets color: rgb(34 197 94).Execute the code, npm start, and we see that Tailwind styles are applied to the text:Webpack is a module bundler. Its main purpose is to bundle JavaScript files to be used in a browser, yet it is also capable of transforming, bundling, or packaging just about any resource or asset. The bundled modules can be CMJ, AMD, UMD, ESM, etc.Webpack 5 was release on October 10, 2020, with the following major features:CRA 5 is packaged with Webpack 5.Jest is a JavaScript Testing Framework that focus on creating, running, and structuring tests. Jest is one of the most popular test runners, which works with projects that using Babel, TypeScript, Node, React, Angular, Vue, etc.Jest 27 was release on May 25, 2021, with the following major features:CRA 5 is packaged with Jest 27.ESLint is a tool to identify and report on patterns found in JavaScript / TypeScript code. It does traditional linting to look for problematic patterns, and it also does style checking to enforce conventions. ESLint 8 was release on October 9, 2021, with the following major features:CRA 5 is packaged with ESLint 8.PostCSS is a tool to transform styles with JS plugins. These plugins can lint a CSS file, support variables and mixins, transpile future CSS syntax, inline images, etc. For example, autoprefixer is a popular plugin that applies css prefixes based on current browser popularity and property support.PostCSS 8 was released on September 15, 2020, with the following major features:CRA 5 is packaged with PostCSS 8.CRA 5 has arrived, with new features and new packages. The newly created project will use CRA 5. If you have an existing CRA 4 project, follow the above instructions to upgrade.If you have an existing CRA 3 project, follow this article to upgrade to CRA 4 first.Thanks for reading. I hope this was helpful. If you are interested, check out my other Medium articles.",,,4,4,0,923,624,5,7,0,20,341
Writing Unit Tests for Your Node.js API,Better Programming,https://betterprogramming.pub/writing-unit-tests-for-your-nodejs-api-13257bd0e46b,Akshay Kumar,87,7,1130,"Unit tests form an integral part of a developer’s development lifecycle. Although writing unit tests sometimes feels like a redundant process, the actual use of these tests comes into the picture while making changes to the piece of code later.If a piece of code has well-written unit tests in place, then if we have to make any changes to it, we can do so without doing complete end-to-end testing of the entire feature from scratch. In this way, unit tests help in boosting a developer’s productivity.But the problem with understanding the process of unit tests is, that most of the content related to it, either is too simple or is too complex. Either people try to explain how to write unit tests for a simple addition function, or we see them written in very complicated projects.So in this article, I will teach you how to write unit tests for an actual but simple Node.js API, performing CRUD operations on MongoDB.We’ll be using chai mocha and sinon for writing our unit tests. The source code for the entire project can be found hereWe’ll start by setting up our node js project. So the first step is to create an empty directorythen move into the repoand perform an npm initInstall the following dependenciesNow we have to reproduce the following project structure in our directory.Now for the sake of simplicity, we’ll be creating 2 APIs, one to add a user to our MongoDB database and the other to fetch a user through a unique profileId.So the user model user.model.jswill look something like thisNext, we create the service functions to add users and fetch a list of all users from the DB in the file user.service.jsThis is the piece of code for which we’ll be writing unit tests.Then, we create express routes to allow execution of our services via HTTP requests, in index.js file.Last we have to write logic to establish a connection to our MongoDB database.Here I’ve assumed that you have a local MongoDB setup, if you don’t have that you can also use a MongoDB atlas instance by replacing the URI.For writing unit tests we’ll be using mocha and chai . Now we won’t be discussing every feature provided by these packages, because that would be out of the scope of this article. Here we’ll only discuss features required to write some basic unit tests.First, let us start with the tests for saveUser service function. Since the job of the function is to add a user to the database such that each user has a unique profileId, there are 2 broad-level scenarios that it should fulfill.So let us first write a skeleton structure for our test cases using describe functions as followsdescribe blocks give structure to our tests and it blocks encapsulate a single unit test.To execute the above empty tests, add the above logic to index.test.jsfile in your test directory, and inside the package.json file add the following script.Now when we run npm run test the above script is executed, which in turn executes mocha, which by default looks for all files with the extension .test.js and executes them.Hence on running npm run test we’ll see the following output.Since there is no logic inside our tests currently, hence both the test cases pass successfully.Now it's time to add actual testing logic inside our test cases, so for the first test case let us try to import our save user function and try to save a user to the DB using it.The expect keyword is used to create assertions, meaning boolean expressions which are bound to be true unless there is a bug in the program.But when we run the above test now, we see the following errorThe problem here is that when we execute the saveUser service function, we haven't actually made any connection with the DB.So mongoose schema functions like countDocuments and save won’t give us any result, because they are not being able to connect with the DB in any way.So let us try to establish that connection in our index.test.ts file as followsOnce again here the assumption is that you have a local MongoDB up and running.Now if we run the test script we see the following output.Now all seems fine, and we feel like we’ve successfully written our test, but wait, try running the same tests again.The problem exists because there is already a user in our DB now, which has the profileId that we have provided, and since we have a check-in place that validates the existence of the provided profileId, that check fails, hence our entire test fails.Along with this, there are other problems that we face, when we run tests on the actual database.To overcome the above problems we have to create what is known as Stubs.By definitions, a stub in software development is a piece of code used to stand in for some other programming functionalityTo put it simply, a stub can be visualized as an override to a method of some existing functionality, so that we can mock calls to that functionality, such that it always returns our desired output.So in the above case, we’ll be using stubs to mock function calls to user.model functions such as save and countDocuments so that they always return our desired output without requiring an actual connection to the DB.To create stubs we’ll be using sinon.js which provides us a framework for creating stubs, mocks, etc.We can use the sinon library to mock responses for our user.model as followsThe complete unit test after adding sinon stubs and removing logic for DB connection will look something like thisNow as many times we run the above test, we will see the following response, irrespective of the DB connection.Now all that is left is writing test for the remaining scenario of saveUser service function, and writing tests for getUser service function.We can test the scenario when the profiled already exists in the DB as followsNote that the sinon.restore command is made to run after each test, to clear all stubs created during the test, and make them independent of one another.Now finally we write tests scenarios for the getUser serviceThe above scenarios can be implemented in a similar fashion using stubs.So the entire test file will look something like thisAnd when we run this we see the following outputIn this way, we’ve successfully written unit tests for both of our service functions.In this article, I’ve tried to provide a simple yet functional demonstration of how we can write unit tests for a simple Node.js API. But we’ve only scratched the surface of the functionalities that the libraries used in the demo provide.Therefore, I hope I’ve helped you get started with unit testing as a concept in this article so that you find future explorations in this domain comparatively easier.",,,7,0,0,,,7,3,0,6,129
10+ React VS Code Extensions for Increasing Productivity in 2022,Better Programming,https://betterprogramming.pub/vc-code-extensions-for-javascript-and-react-developers-in-2022-f0828b9ea00,Inna Sinicka,120,4,571,"We all want to work better, quicker, increase our productivity, deliver awesome products on time. Unfortunately, sometimes we spend too much time on keeping our code clean and clear, formatting and reorganizing.It’s much easier to work effectively when you have predefined rules and you don’t waste your time overthinking how to organize your project, how to structure components, where to put styles or tests, how to test your functions quickly, and much more. I'm not alone in a project and I have to define these rules, that’s why I'm always looking for better approaches and great tools, which could help me automate things as much as possible.I use different VS Code extensions, but I would like to tell you about those, which help me and my team write cleaner code much quicker without copy-pasting the old one. It’s important to mention, that my list of useful extensions became a bit shorter in 2022 because we don’t more need additional tools for debugging and colorizing brackets.You need them both! If you want to have nicely formatted and checked code in your project you must use the formatter and the linter.I usually have config files (.prettierrc and .eslintrc.json) where I define how my code should look like and how to check it.In the eslint config it is important to define which plugins will be used (order is important) — because you don’t want to have conflicts between eslint and prettier.Extensions show me all incorrect places in my code and I can easily format it on save (editor.formatOnSave must be true in vs code settings.json).I was looking for an extension, that could generate ReactJs components from predefined templates, but also I wanted a way to change the structure of the template and create very specific ones.Which is why I like this one the most. It isn’t just bound to react and I can create whatever I want.For example I have a template for creating views, which creates a component, that imports our List component and specific custom hooks for data fetching and state management.Since our team is using this approach, we don’t need to think about the component’s structure, we just choose type, name and we get ready to use component with default imports, test file, styles, story (for Storybook), and more.It is especially useful in libraries when it's nice to have all components structured in one way.This one is a great extension. It helps with creating new components, functions, and much more. On top of all, it is quick and makes it easy to write useEffect, useCallback or any other hook.In the picture below you can see how it looks in code and how prettier/eslint plugins are reacting.Sometimes we all need to refactor all code and it is nice to have a tool, that can help with it. With this extension, it is very easy to select code lines that should be extracted to new component and that’s all.I usually prefer creating new components with predefined templates, but sometimes component is so simple and small, that you just don’t need that approach.I love this one and literally can’t imagine my life without it.I usually have more than 2 projects opened and peacock is a timesaver for me.It is much easier to identify the correct project by Color — and not just by project name.That’s all about my favorite extensions for increasing productivity and better coding. Hope, that these extensions will help you too.",,,0,4,7,,,4,1,0,13,266
Build a Serverless Async CSV Parser,Serverless Cloud,https://medium.com/serverlesscloud/build-a-serverless-async-csv-parser-9ef8e5273c41,Ben,4,6,1276,"In this tutorial, we will be building a CSV parser, powered by Serverless Cloud. This tutorial will just be the API side of the application, with more instructions to come.Recently, the Serverless Cloud team introduced Serverless Storage, a simple cloud storage service for all of your application’s file storage needs. Even more recently, storage listeners were introduced, allowing your application to react to certain storage events outside of an API call, enabling heavy batch processing in the background without holding up the frontend client. This tutorial will utilize Serverless Storage listeners to asynchronously parse CSV files uploaded via an API.On your local machine, create a new directory called csv-parser, then open this new directory in any code editor you prefer. This directory will be the folder for your application files. Using your terminal, initialize a Serverless Cloud project by running the following in your terminal:You may be prompted to login if you haven’t already, then the CLI will ask you to name your new application and select a template. Enter “csv-parser” as the name, and choose the “JavaScript API” template.Within just a few seconds, your new application will be generated in the directory, and deployed live to your personal development instance. The CLI will now enter development mode, streaming live logs and errors to your terminal as you iterate through the project. Development mode does not prevent commands, so you can continue to interact with the CLI as you are working on the project.Our CSV Parser service will consist of three endpoints, one to upload and save a CSV, and two others to retrieve any processed data from the uploaded CSVs.Starting with the “upload” endpoint, we will need a .post method that receives the files. The api interface from the SDK simplifies this process, automatically loading the file into memory for you.Just copy the following code snippet into your index.js file, replacing all boilerplate code that was generated with the starter template.Let’s walk through the code here: api.post will create a POST endpoint available at the /csv route. The route also takes a “delimiter” query parameter, in cases where the submitted CSV files use something other than a comma, but defaults to a comma if not provided.With api.post, any posted files will be available in req.files, including a buffer and the original name of the file uploaded. Keep in mind that to get these file names, we need to use multipart form requests. If you post the file as the body, it will not contain any other data.To avoid flooding our Storage root directory with CSVs, we are going to make a “csvs” directory, to keep things organized. Finally, with the file’s data in hand as a buffer, and a name settled upon, we can write this file to Serverless Storage using storage.write. You can save any custom metadata with storage.write as well, so here we will store the provided delimiter for later parsing.With API calls, we have limited processing time (up to 29 seconds). We also want to send a response back to the caller as soon as possible to provide a better user experience. With nearly infinite CSV sizes possible, parsing and storing the data during the API call could be slow, and potentially cause the API call to timeout. To remedy this, we will add an event listener that fires when new files are written to the “csvs” directory, and only when those files end in “.csv”.First though, we need a parsing library! My personal favorite for CSV work in Node.js is papaparse, so we’ll use it for this tutorial. Feel free to use any library you are comfortable with to do the equivalent processing.Without quitting the CLI, simply type install papaparse. This will add the package to your application, and automatically sync its content with your personal instance. We will also need to import papaparse into our index.js file.To make all of our processed data queryable, we will be storing all the rows in Serverless Data. We will need to update our SDK import statement to include the data interface.Copy this code snippet into your application (including the updated import statements), and your application will now be listening to any CSV write events.For this tutorial, I’ve used a simple employee spreadsheet that expects the CSV headers to be “ID”, “First Name”, “Last Name”, and “Email”. With just a few lines of code, you could adapt this to fit your use case, or even add the ability to dynamically parse the headers.Now, let’s walk through this code. First, take a look at the first argument of storage.on. We want to react to storage writes to the csvs directory, and only process .csv files. We achieve this using the write action with the corresponding glob: write:csvs/*.csv. Now, we need the path of the file that just got saved, along with any metadata for possible custom delimiters. For this, we call storage.stat with the given path, which returns information about the file. In this case though, we only need the metadata.Now, we need to load the CSV into memory for processing. For this we will use storage.readBuffer, which returns the entire file as a buffer. Papaparse, though, takes entire CSV’s as strings in its simplest form, so we convert the loaded buffer into a string using .toString(‘utf-8’), with a utf-8 encoding just to be safe. Papaparse also takes some configuration options as the second argument, perfect for our saved delimiter!Finally, we check if the CSV had any data at all. If it does, we iterate through the array of rows, converting each value into a variable with camel casing. With all the data of the row loaded and ready, we can save it to Serverless Data using data.set. Serverless Data is a powerful key/value store that lets you create collections of data, so our key will use “employee” as the collection name, and the employee’s ID to uniquely identify them within the collection. We use a colon “:” to separate collection names from their key.Now any saved CSV’s (in the expected format) will be saved!If you’ve made it this far, your application is now able to take in CSV files, and parse them with the storage.on listener. But, don’t we want to do something with all that data? Of course we do!Let’s make two new GET endpoints using the api called “/employees” and “/employee/:id”. We want to be able to get everything that has been saved, but also allow the API to just return a single employee via a query.Copy and paste this into your project, and you will now be able to retrieve any stored employees.To walk through the “/employees” endpoint, all we need to do is use data.get with employee:* to return any database entry in the “employee” collection, or thought of another way, any key that begins with “employee:”. Since every row we write in the storage listener does this, it should be every row from any uploaded CSVs.To get a single employee, we do something very similar, except with a path parameter and by sending in the full key to data.get. This one also has a chance to not have any entries, so we need to check if nothing came back, and return a 404 status code to notate that this employee ID does not exist (yet).And there it is! An asynchronous CSV parser all built using Serverless Cloud. While there are a lot of improvements that can be made to the implementation here, this will do its job well. There is much more you can do with the power of Serverless Cloud that we didn’t touch on here. Check out the docs if you’re curious for more!Originally published at https://www.serverless.com.",,,0,1,5,1117,421,3,0,0,8,120
,,https://medium.com/u/7e19fe9822d0?source=post_page-----c35fc29ccf6--------------------------------,Ben,4,,,,,,0,,,,,,,0,,
,,https://medium.com/u/ac650b30cf52?source=post_page-----c35fc29ccf6--------------------------------,Juro Uhlar,27,,,,,,0,,,,,,,0,,
,,https://medium.com/u/2d65d03cee96?source=post_page-----c35fc29ccf6--------------------------------,Ethan Keiser,118,,,,,,0,,,,,,,0,,
Simplifying Lazy Loading in Next.js,Better Programming,https://medium.com/better-programming/lazy-loading-in-next-js-simplified-435681afb18a,Kithma Marindagoda,29,4,595,"Lazy loading is a concept that we can use to reduce the initial load time of a particular page in web applications.In a normal scenario when the user initially loads the page all the content of the page will be loaded, however, sometimes users do not care what’s at the bottom of the page and do not bother to scroll. So loading all content will be in vain.By using lazy loading we render content on demand of the user, so when the user scrolls down we load the content gradually, not during the initial rendering of the page.In this article, I’m going to explain how we can lazy load components in Next.js. But keep in mind that this is not for components that render on the server-side.Think of a scenario where you have multiple components loaded into a single component, such as a landing page with a lot of sections and also with a lot of API calls.Suppose when users visit the above page they only see the Child1 and Child2 content in the initial viewport.To see the contents of the Child3 component and below users need to scroll down. However, some users may not even need to see the content on the bottom part of the page.But according to our code snippet once the user visits the page, all of the 5 child components are rendered and if there’s any data fetching logic in those components there would be some API calls too.As I mentioned earlier most of the time it’s unnecessary because some users may not even want to see the rest of the content.This is an instance where lazy loading can be used to reduce the load time of a component.Since this will be used more than one time I’ll create a custom hook.This custom hook gets a ref and observes it. We use isIntersecting state and update the state using entry.isIntersecting. From the custom hook, we return the isIntersecting state. If isIntersecting value is true that means the ref element is visible to the user.2. In the next step we have to create refs for each of the Child components and observe the visibility using our custom hook (useOnScreen).Now the Child3 component will render only when Child3RefValue is true. But there is another problem. Think of a scenario when the user scrolls down and then scrolls back again. In situations like these, the child3RefValue will update like false → true → false → true.So when the value is updating to true twice, the whole Child3 component will render twice. We don’t need that to happen hence we need to prevent subsequent render.3. To prevent the component from rendering more than once we have to keep a state value in the Parent component:Now when the child3RefValue changes, the useEffect will run and if only the isChild3Ref is false the state will be updated. According to the isChild3Ref value, the Child3 component will be rendered. Therefore the component will render only once even though the user scrolls up and down several times.You can go one more step further using Next.js dynamic import. So the Child3 component will be imported only when isChild3Ref state value is true.Lazy loading is a great way to prevent needless content rendering on a page. By using it, you could minimize the initial load time of the page as the child components will be rendered on user demand only. Do not forget to try it out when you come across alike situations.If you are more interested in Next.js you could refer to my article regarding pre-rendering in Next.js.Happy coding!",,4,5,0,5,640,320,1,1,0,4,180
,,https://medium.com/u/3ba2fddcf7aa?source=post_page-----c35fc29ccf6--------------------------------,Colum Ferry,318,,,,,,0,,,,,,,0,,
Driving TypeScript Adoption With Automated Reports,Better Programming,https://betterprogramming.pub/driving-typescript-adoption-with-automated-reports-dee725298302,Juro Uhlar,27,5,818,"First, TypeScript is good. But that’s not what the article is about. It’s about what to do if the codebase at your job doesn’t have enough of it.“Do you use TypeScript in your React codebase?” I asked my future boss in a job interview.“Yes.” was the answer.“Great!”But alas, it was not great. I should have asked, “How [much] are you using TypeScript?”. TypeScript adoption is not a binary state, but a spectrum.Like many older React projects, the codebase was being rewritten to TypeScript in a “write new stuff in TypeScript, fix old stuff when you have time” kind of way. But without a set goal and a measure of progress, the incentive is always on delivering new features rather than solving technical debt.Several months after the switch, most of the code was still untyped, and progress was slow. We knew rewriting to TypeScript was valuable in the long run. But to justify spending development time on it, we wanted to track and visualize our progress.That's it. Progress bars are magic. They are stupidly simple, but they work. RPG players will grind for hours to get their Elder Paladin to Level 75 or whatever. Goal clarity drives motivation. People do things to see numbers go up. Developers will fix TypeScript errors for the same reason. You can buy them pizza for each big milestone.Don’t get me wrong, developers are and should be intrinsically motivated to improve code quality. But then this happens:“Why is that task taking so long?” — PM“Well, there was a lot of TypeScript to add and fix.” — Dev“Uh, okay, sounds like an excuse, but whatever, can we focus on delivering customer value here, please?” — PM (in his head , probably)Maybe next time you leave the code just as messy as you found it to get ahead on your next task. Your work fixing technical debt is invisible and only gets in the way. Or the conversation can go like this:“Why is that task taking so long?” — PM“Well, there was a lot of TypeScript to add and fix, so I fixed it. See the progress bar? We went from 54% to 60% TypeScript coverage this sprint. The closer we are to a 100, the more our long-term velocity as a team increases.” — Dev“I see, good work.” — PMYour work is not invisible anymore. It moved the team towards an established goal by a specific amount. “What gets measured, gets done” applies well here.Measuring TypeScript adaption depends on your flavor of not actually adopting TypeScript.Maybe you have .js(x) and .ts(x) files living side by side. This is the cleanest alternative. You can count them up with a little bash magic:In fact, both GitHub and GitLab display language statistics of your repositories automatically.Maybe all your files are .ts but you typed everything as any. Best to use a tool that analyses your any usage and spits out a report:Maybe your situation is messy. Your files are .ts and contain some types, but most of them have @ts-nocheck flags right on top. Then you need to count the offending files:You can do a similar thing for @ts-ignore. Whatever the problem is, you need to adjust your measurement accordingly.Percentages are fine, but you really want a nice fat colorful progress bar. Red is bad, green is good. Crucially, it helps to see the progress in time, to see the green slowly defeating the red, inching its way toward victory. A time chart can help too.Our report — smashed together during a one-day hackathon — looked like this:We used a bash script to count the files and generate an HTML file with a table and a Chartist.js chart. But you can use anything.A progress bar is no good if no one looks at it. It’s important to put the numbers in front of people so they don’t have to dig for them. Remove as much friction as possible. In our case, that meant a weekly scheduled GitLab pipeline that:Then once a week, we get this:It would be better to put the report directly in the message, but this is fine for now. A Slack notification or an email works too. Pick whatever communication tool your team uses the most.The last step is to share your progress. Our sprint reviews have a dedicated slide for updates on technical debt, so we talk about our TypeScript coverage there. The progress bar can be effective, even if only used internally within your team. But it’s much better to commit publicly to a goal and give stakeholders regular updates.Since building the report, our TypeScript adoption speed has notably increased. (Other factors like new team members also helped.)This time it was TypeScript coverage, but you can apply the same idea to different types of long-term technical debt:Make invisible work not just visible but easy to see regularly — and it will get done faster.Thank you for reading! Do you know a better way?",,2,0,4,3,1225,641,3,3,0,8,159
A Summary of React by 10+ Code Snippets,,https://medium.com/u/dac47b3482f4?source=post_page-----c35fc29ccf6--------------------------------,bytefish,3800,,28,"Create React App is a CLI for creating React projects.We can write HTML in JavaScript by JSX.Just use {} to wrap JavaScript expressions.It is a simple, stateless, functional…",,,3,2,0,1225,689,1,0,0,2,6
Component-First State Management for Angular Standalone Components,,https://colum-ferry.medium.com/component-first-state-management-for-angular-standalone-components-5c0b67f8c5c1,Colum Ferry,318,7,989,"In 2021, Angular announced an RFC (Request For Comments) for Standalone Components. Optional NgModules have been a frequent ask from the framework's community since their introduction in Angular 2-rc.5. Standalone Components (and Directives and Pipes) are Angular's answer to this request. It paves the way for our Angular apps to be built purely with Components.However, over the years we have built architectural patterns for Angular taking into account that NgModules exist and are the driving force of current Angular apps. With NgModules becoming optional, we need to think about new patterns that can help us to build the same resilient and scalable apps, but using a simpler mental model of our apps.This is where Component-First comes into play. It is a collection of patterns for designing Angular apps, once we have Standalone Components, that emphasises that Components, as the main source of user interaction, are the source of truth for our apps.We should be able to link all the components in our app together and know exactly how our app works. There’ll be no magic happening off in some obscure module somewhere.To achieve this, components need to manage their own routing and state.In this article, we’ll explore an approach to State Management that allows components to control their state and be their own source of truth.If you’re interested in seeing how Routing changes with Standalone Components, read the article I wrote on the matter belowComponent-First Architecture with Angular and Standalone ComponentsIn the current state of Angular, the framework does not ship with a built-in solution to state management. It does provide the building blocks, but it does not take an opinionated stance on how to manage the state in your app. The Angular community has stepped in to fill that gap in the ecosystem with the creation of packages such as- NgRx- NgXS- … Others that I have not listed.However, the ones I have listed, arguably the most popular in the ecosystem, rely on NgModules to instantiate the State Management Solution.If we want to move to a truly NgModule-less developer experience, we need to transition away from any solution that relies on NgModule, otherwise, we will always be coupling our components to NgModules. This coupling will continue to be more and more difficult to remove over time. It also complicates the modelling of our system. Our state will be created and handled in a separate location from our components. This increased obscurity in how our state gets managed makes it more difficult for us to evaluate our components and how they function.NgRx has already taken steps in the direction that I feel is perfect for a Standalone Components world. They created a package called Component Store which allows Components to manage their own state. It works and it is a great solution! If you’ve used it before and you’re comfortable with RxJS, use it! Other packages also attempt to solve the problem, but they also rely on RxJS. [Elf](https://ngneat.github.io/elf/) and [Akita](https://github.com/datorama/akita)However, I have created a package, @component-first/redux, that implements the Redux pattern in a local component store that does not use RxJS that we can also use to achieve the same effect.In the rest of this article, I’ll illustrate how we can use this package to manage the state within our apps for Standalone Component.Let’s take the following component as an example. It will be a basic ToDo List component that manages its own list of todos and allow actions such as add and delete.Our barebones component, without a store, should look similar to this:It’s a pretty straightforward component that is internally managing it’s own state. Creating a Store for it may be overkill, but it’ll be a good example to showcase the component store.First, we need to create the store. We create a file beside our component called todo-list.component.store.ts and it should look like this:It’s as simple as that, and now our state management is self-contained in a class and file that lives right beside our component. Now, let's modify our component to use our new store:It’s pretty straightforward to use our new Store and it follows an API we are all somewhat familiar with providing you have used NgRx in the past. We did have to introduce a new pipe, latest, that will always fetch the latest value from the store on a Change Detection Cycle.The Store also supports Effects. This can be useful in a wide variety of situations, however, let's modify our TodoListComponentStore to have an effect that will fetch our Todo list from an API.Now that we have added our effect, we can take advantage of it in our component by dispatching an action:Now that we don’t have NgModules, how can we share a store between components?Note: I wouldn’t recommend it, but it does have it’s uses, such as a global notification system.In Component-First, because all our components are children or siblings of each other, we can take advantage of Angular’s Injection Tree and simply inject a parent’s Store into our child component.Let’s say we had a component, TodoComponent, that was a child to TodoListComponent, then we could do the following:I’d advise caution with this approach as it forces a coupling between TodoListComponent and TodoComponent where TodoComponent must always be a child of TodoListComponent. In some scenarios, this makes logical sense, but it's something to be aware of!The @component-first/redux package is available on npm and you can use it to experiement with. Just note that the LatestPipe is currently not Standalone in the package (I do not want to ship the Standalone Shim provided by Angular), so you will have to add the LatestPipe to an NgModule's declarations. When Standalone Components arrive, I will make the pipe Standalone!I hope this article helps to get you excited about Standalone Components and helps you start to think about some approaches we can take to architecture when they do arrive!If you have any questions, feel free to ask below or reach out to me on Twitter: @FerryColum.",,1,6,0,3,1000,500,1,0,0,22,140
Too Many Dependencies in iOS? Use the Composition Root Pattern,Better Programming,https://betterprogramming.pub/ios-architecture-is-not-mvc-mvvm-m-5b34a04beb98,Ethan Keiser,118,4,571,"If you ask an iOS engineer, “which architecture will you use to design X application?”, often they respond with MVC, MVVM, MVP, etc.This post will explain why these popular acronyms do not fully answer the question and will showcase an alternative solution.MVC, MVVM, MVP are the UI layer designs or UI Architectures and not system architectures.They describe the flow of data and the separation of responsibilities within the UI layer. It does not answer any questions about navigation, networking, caching, business logic, etc.Adding these responsibilities to the UI layer creates monolithic applications with massive dependency graphs. This leads to rewrites, untestable code, and high cost for change.The solution is to break down the monolith into modular components and compose them together at the “Composition Root”. For example, you want to create a feed application of images similar to Instagram.A common UI architecture may look like this.A user selects a FeedImageCell and expects to navigate to a FeedDetailedViewController.The code above is a common strategy that requires view controllers to be responsible for navigation and creation of its children view controllers.Now imagine your requirements change and you now need to implement the ability to log, retrieve resources from remote or local repositories, and support older operating systems.You use URLSession and load the data at viewDidLoad as shown below:When the user selects an item you check to determine which controller to navigate to and log it.These minor changes required developers to modify the FeedViewController. This violates the Open/Close principle because the possible navigation routes are determined very early at compile time.Even if we inject the dependencies to delay behaviors of the FeedViewController via construction injection, we still need to inject the dependencies of any child view controller it creates.If the child view controller has children view controllers, we’d have to inject those responsibilities at the root view controller and cascade them down. This could continue indefinitely within large complex applications and create major problems with massive dependency graphs.You can already see the dependency web forming around the FeedViewController by coupling it with LoggingFramework and URLSession. Any dependency its children have would also be its dependency.The Composition Root pattern delays decision-making about how components interact and allows us to intercept and modify behaviors.It lives in the Main module, a concept that is foreign to most iOS engineers.Let’s take a look at how to compose components within the main module and break up the monolith.Now, the FeedViewController is no longer responsible for the creation and navigation of its children.The FeedComposer composes the FeedViewController and assembles it with all its dependencies. Using closures we can inject behavior without coupling UI module components with Networking or Logging modules.The FeedNavigation handles routing and the Logging module is notified via its delegate.Notice the flow of dependencies. Main is dependent on all other modules while no module knows about each other.By creating modular code and composing components together at the composition root, we create virtual boundaries between modules.Suppose we want our Feed application to run on the Apple Watch or Mac OS. The initial approach of coupling everything in the UI layers and calling it MVC/MVVM/M* would require a total rewrite.With our new approach, we’d only need to rewrite the FeedUI module and connect the components at the composition root within Main. All the business logic, networking, logging will be reused. This is the power of modular design.Thank you for reading. The full source code is available in the GitHub Repository",,7,1,4,0,1155,648,8,0,0,2,392
Too Many Dependencies in iOS? Use the Composition Root Pattern,Better Programming,https://betterprogramming.pub/ios-architecture-is-not-mvc-mvvm-m-5b34a04beb98,Ethan Keiser,118,4,571,"If you ask an iOS engineer, “which architecture will you use to design X application?”, often they respond with MVC, MVVM, MVP, etc.This post will explain why these popular acronyms do not fully answer the question and will showcase an alternative solution.MVC, MVVM, MVP are the UI layer designs or UI Architectures and not system architectures.They describe the flow of data and the separation of responsibilities within the UI layer. It does not answer any questions about navigation, networking, caching, business logic, etc.Adding these responsibilities to the UI layer creates monolithic applications with massive dependency graphs. This leads to rewrites, untestable code, and high cost for change.The solution is to break down the monolith into modular components and compose them together at the “Composition Root”. For example, you want to create a feed application of images similar to Instagram.A common UI architecture may look like this.A user selects a FeedImageCell and expects to navigate to a FeedDetailedViewController.The code above is a common strategy that requires view controllers to be responsible for navigation and creation of its children view controllers.Now imagine your requirements change and you now need to implement the ability to log, retrieve resources from remote or local repositories, and support older operating systems.You use URLSession and load the data at viewDidLoad as shown below:When the user selects an item you check to determine which controller to navigate to and log it.These minor changes required developers to modify the FeedViewController. This violates the Open/Close principle because the possible navigation routes are determined very early at compile time.Even if we inject the dependencies to delay behaviors of the FeedViewController via construction injection, we still need to inject the dependencies of any child view controller it creates.If the child view controller has children view controllers, we’d have to inject those responsibilities at the root view controller and cascade them down. This could continue indefinitely within large complex applications and create major problems with massive dependency graphs.You can already see the dependency web forming around the FeedViewController by coupling it with LoggingFramework and URLSession. Any dependency its children have would also be its dependency.The Composition Root pattern delays decision-making about how components interact and allows us to intercept and modify behaviors.It lives in the Main module, a concept that is foreign to most iOS engineers.Let’s take a look at how to compose components within the main module and break up the monolith.Now, the FeedViewController is no longer responsible for the creation and navigation of its children.The FeedComposer composes the FeedViewController and assembles it with all its dependencies. Using closures we can inject behavior without coupling UI module components with Networking or Logging modules.The FeedNavigation handles routing and the Logging module is notified via its delegate.Notice the flow of dependencies. Main is dependent on all other modules while no module knows about each other.By creating modular code and composing components together at the composition root, we create virtual boundaries between modules.Suppose we want our Feed application to run on the Apple Watch or Mac OS. The initial approach of coupling everything in the UI layers and calling it MVC/MVVM/M* would require a total rewrite.With our new approach, we’d only need to rewrite the FeedUI module and connect the components at the composition root within Main. All the business logic, networking, logging will be reused. This is the power of modular design.Thank you for reading. The full source code is available in the GitHub Repository",,7,1,4,0,1155,648,8,0,0,2,392
Animations Inside a ScrollView With SwiftUI,Better Programming,https://betterprogramming.pub/animations-inside-a-scrollview-with-swiftui-3b550c18a442,Sarah,1300,3,246,"In ScrollViews and Lists, embedded views are loaded before they show up on your screen. onAppear is triggered prior to the view’s appearance on screen. We need to wait for the animated view to be on screen before playing our animation.To delay our animation, we will create a view modifier to figure out the position of the view and trigger the animation play when the view reaches the bottom of the screen.Inside the body function, use GeometryReader to get the view’s global position. If the view is off-screen (lower on the scrollView/list), then the content will be at a position higher than the height of the screen.What we are going to do in our code:To test the view modifier we created, we can create a Text view with two animated elements; opacity and y position.Create a variable for both opacity and y position. Use the opacity() and offset() modifiers.Pass the modifiers their relative variables. Inside .onAppear(), use withAnimation() to animate the value changes of the opacity and y position properties.Lastly, add a modifier() and pass the AnimationModifier view modifier we created above to it.Here are other animation examples that use our AnimationModifier view modifier. In all examples below, I didn’t need to offset the midY position of any view.Lastly, we can add all of our examples inside our content view. Add a ScrollView or a List and embed inside it all of the animated views we created above.That’s all for this tutorial. Thank you for reading!",,1,0,0,0,704,1020,6,1,0,0,323
A Complete Guide to File Uploading in JavaScript,Better Programming,https://betterprogramming.pub/a-complete-guide-of-file-uploading-in-javascript-2c29c61336f5,bytefish,3800,8,1039,"File upload is a common function for a web project. I believe that everyone has encountered related requirements more or less during the development.In this article, I have summarized some scenarios and solutions, hoping to help you thoroughly grasp questions related to file uploading.First, let’s clarify the specific functions of file uploading.According to upload target, there are 3 kind tasks:According to the user actions, there are:From a performance perspective, we may need:And additionally, sometimes we may not upload files in the client browser, but through the server to upload to another server.We will discuss these in turn.Before the start coding, we still need to understand some background knowledge.First, when uploading files, we use Axios, the most popular HTTP Library. In actual development, we generally don’t use XMLHttpRequest directly, and using Axios conforms to the real development model.When we discuss uploading files in the front-end, if we want to fully understand the relevant principles, we must understand the relevant back-end code. Here we use the Koa to implement our server.Finally, I hope you will have a brief understanding of formdata, we use this data format to upload files.The need to upload a file is too common. For example, when you register for Medium, you need to upload an avatar.The file upload function requires cooperation between the client and the server. In our project, the user selects a file in the client and uploads it to the server; the server saves the file and returns the URL of it.Here is the project preview:The above Gif shows the complete process of file uploading:All the code of this project was held on GitHub:github.comYou can clone it to your computer:All the code related to single file uploading was put on 1-SingleFile folder.To run the server, you can go to the folder and run this command:Then you can open client.html on any browser.The specific operation I have shown in the gif above. You can try it for yourself first, and then read on.Um, How many steps does it take to put a giraffe into a refrigerator?Just three steps:The same is true for uploading files, we only need three steps:In HTML, we can use the input element. Just set the type of this element to file, then the element can be used to select the file.After the user selects a file, the metadata of the file will be stored in the files property of this input element.Finally, we use Axios’ post method to upload files. But before uploading the file, we also need to package this file into FormData format.Tips: FormData is a key-value type data format. Here is an example:Well, these are the knowledge points related to file uploading. The more complete code is as follows:This code is actually to implement the three steps we said before:It’s just that we added two additional functions:Then, when Axios uploads a file, it allows us to monitor the progress of the file uploading.We know that HTTP is built on top of TCP. If an HTTP packet is relatively large, it may be decomposed into multiple different TCP packets for transmissions in the network.If you need to write a progress bar to show the user the progress of the uploading, you can use this API.progressEvent.loaded means how many bytes have upload success and progressEvent.total means total bytes of the file.Ok, this is our client-side code.To start a server, we can use Koa. Here is a tiny server using Koa:This is the most basic Koa demo. Since this article focuses on file uploading, so I will not explain this in detail. If you don’t familiar with this, you can read the official documentation.Our client uses the format of FormData to upload files, then our server also needs to parse FormData. And Koa-multer is a middleware that helps us parse FormData data:About Koa-multer, you can read their official documentation:The key code is uoload.single('file'), this line of code can help us parse the data of FormData, and then put the corresponding information in the ctx.request.file.In fact, at this time, our server can already receive the files uploaded by the client, but it does not store them to the disk after receiving the files.If we want Koa-multer to save the file to disk for us, we can add the following configuration:The complete code is server.js, and you can read it directly in the code repository.The current flow chart looks like this:Anyway, you should try it yourself.With the above foundation, it is much simpler for us to write the code for uploading multiple files.First, we need to modify the input element and add the multiple attribute to it.This is to tell the browser that now this input element should allow the user to select multiple files at the same time.Then after the user selects multiple files, the data will be placed in fileElement.files. When we construct formdata, we need to traverse this list and put all files into formdata.Then the code of uploading the file doesn't need modification.Here is the complete code:The file is located on 2-MultipleFiles/client.html in the project.At the same time, we also need to adjust the code on the server-side.First, we need to add the corresponding route /upload-multiple-files, and then use the upload.fields([{ name: “file” }]) middleware to handle multiple files. After that, the FormData data in request will be placed in ctx.files.file.Demo:Now let’s look at the steps to upload a directory.Similarly to before, we need to set the attribute of the input element to this:Then when uploading the directory, the files object of input element will have the webkitRlativePath property, and we will also add them to the formdataIt should be noted here that when the file name contains \, koa-multer may make an error. To fix this, we need to replace \ with @ symbol.Then we also need to modify the corresponding server code:Demo:Well, we have analyzed the process of uploading a single file, multiple files, and directories in turn. It’s actually very simple, just 3 steps:All the code is on GitHub, you can try it yourself. If you have any questions, you can leave a comment.Due to the length of the article, the rest of the file uploading will be included in a later article. If you are interested in this content, you can follow me.Thanks for reading.",,3,11,0,0,1319,817,20,11,0,11,212
,,https://medium.com/u/41c4219534d4?source=post_page-----c35fc29ccf6--------------------------------,Kithma Marindagoda,29,,,,,,0,,,,,,,0,,
Networking With Combine and SwiftUI,Better Programming,https://betterprogramming.pub/networking-with-combine-and-swiftui-fdf8182f7360,Peter Friese,2200,8,1307,"Not keeping the UI up to date across the different parts of an app can result in an infuriatingly bad user experience, and I am sure we all have at least one or two apps in mind that are notorious for this kind of behaviour.Writing apps that keep the state in sync across the UI and the underlying data model has traditionally been a difficult task, and the development community has come up with plenty of approaches to address this challenge in more or less developer-friendly ways.Reactive programming is one such approach, and SwiftUI’s reactive state management makes this a lot easier by introducing the notion of a source of truth that can be shared across your app using SwiftUI’s property wrappers such as @EnvironmentObject, @ObservedObject, and @StateObject.This source of truth usually is your in-memory data model — but as we all know, no application exists in isolation. Most modern apps need to access the network (or other services) at some point, and this means introducing asynchronous behaviour to your app. There are plenty of ways to deal with asynchronous behaviour in our apps: delegate methods, callback handlers, Combine, and async/await, to name just a few.In this series, we will look at how to use Combine in the context of SwiftUI to… and deal with some advanced scenarios.Let’s kick things off by looking into how to use Combine to fetch data from a server and map the result to a Swift struct.Let’s assume we’re working on a sign up screen for an app, and one of the requirements is to check if the username the user chose is still available in our user database. This requires us to communicate with our authorization server. Here is a request that shows how we might try to find out if the username sjobs is still available:The server would then reply with a short JSON document stating if the username is still available:To perform this request in Swift, we can use URLSession. The traditional way to fetch data from the network using URLSession looks like this:And while this code works fine and nothing is inherently wrong with it, it does have a number of issues:Running the code samplesYou will find all the code samples in the accompanying GitHub repository, in the Networking folder. To be able to benefit the most, I've also provided a demo server (built with Vapor) in the server subfolder. To run it on your machine, do the following:When they introduced Combine, Apple added publishers for many of their own asynchronous APIs. This is great, as this makes it easier for us to use them in our own Combine pipelines.Now, let’s take a look at how the code looks like after refactoring it to make use of Combine.This is a lot easier to read already, and (except for the guard statement that makes sure we've got a valid URL) there is just one exit point.Let’s walk through the code step by step:This looks a lot better and easier to read than the initial version, and we could stop here, and integrate this in out application.But we can do better. Here are three changes that will make the code more linear and easier to reason about:We often find ourselves in a situation where we need to extract a specific attribute from a variable. In our example, we receive a tuple containing the data and the response of the URL request we sent. Here is the respective declaration in URLSession:Combine provides an overloaded version of the map operator that allows us to destructure the tuple using a key path, and access just the attribute we care for:Since mapping data is such a common task, Combine comes with dedicated operator to make this easier: decode(type:decoder:).This will return decode the data value from the upstream publisher and decode it into a UserNameAvailableMessage instance.And finally, we can use the map operator again to destructure the UserNameAvailableMessage and access its isAvailable attribute:With all these changes in place, we now have version of the pipeline that is easy to read, and has a linear flow:Let’s finish off by looking at how to integrate this new Combine pipeline in our hypothetical sign up form.Here is a condensed version a sign up form that contains just a username field, a Text label to display a message, and a sign up button. In a real application, we'd also have some UI elements to provide a password and a password confirmation.All UI elements are connected to a view model to separate concerns and keep the view clean and easy to read:Since @Published properties are Combine publishers, we can subscribe to them to receive updates whenever their value changes. This allows us to call the checkUserNameAvailable pipeline we created above.Let’s create a reusable publisher that we can use to drive the parts of our UI that need to display information that depends on whether the username is available or not. One way to do this is to create a lazy computed property. This makes sure the pipeline will only be set up once it is needed, and there will be only one instance of the pipeline.To call another pipeline and then use its result, we can make use of the flatMap operator. This will take all input events from an upstream publisher (i.e., the values emitted by the $username published property), and transform them into a new publisher (in our case, the publisher checkUserNameAvailable in ).In the next and final step, we will connect the result of the isUsernameAvailablePublisher to the UI. If you take a look at the view model, you will notice we've got two properties in the output section of the view model: one for any message related to the username, and another one that holds the overall validation state of the form (remember, in a real sign up form, we might need to validate the password fields as well).Combine publishers can be connected to more than one subscriber, so we can connect both isValid and usernameMessage to the isUsernameAvailablePublisher:Using this approach allows us to reuse the isUsernameAvailablePublisher and use it to drive both the overall isValid state of the form (which will enable / disable the Submit button, and the error message label which informs the user whether their chosen username is still available or not.When you run this code, you will notice a couple of issues:We are going to dive deeper into the reasons for these issues in the next episodes, but for now, let’s address this error message:The reason for this error message is that Combine will execute the network request on a background thread. When the request is fulfilled, we assign the result to one of the published properties on the view model. This, in turn, will prompt SwiftUI to update the UI — and this will happen on the foreground thread.To prevent this from happening, we need to instruct Combine to switch to the foreground thread once it has received the result of the network request, using the receive(on:) operator:We will look deeper into threading in one of the next episodes when we talk about Combine schedulers.In this post, I showed you how to access the network using Combine, and how this enables you to write straight-line code that should be easier to read and maintain than the respective callback-driven counterpart.We also looked at how to connect a Combine pipeline that makes network requests to SwiftUI by using a view model, and attaching the pipeline to an @Published property.Now, you might be wondering why isUsernameAvailablePublisher uses Never as its error type - after all, network errors very much are something that we need to deal with.We will look into error handling (and custom data mapping) in one of the next episodes. We will also look at ways to optimise our Combine-based networking layer, so stay tuned!Thanks for reading 🔥Originally published at https://peterfriese.dev.",,2,3,2,16,1225,816,1,4,0,2,232
,,https://medium.com/u/679d8738dbb4?source=post_page-----c35fc29ccf6--------------------------------,Debra Kerman,13,,,,,,0,,,,,,,0,,
,,https://medium.com/u/aff864e8163?source=post_page-----c35fc29ccf6--------------------------------,Emilio Peláez,228,,,,,,0,,,,,,,0,,
,,https://medium.com/u/ea0b1eb1f5d2?source=post_page-----c35fc29ccf6--------------------------------,Peter Friese,2200,,,,,,0,,,,,,,0,,
Networking With Combine and SwiftUI,Better Programming,https://betterprogramming.pub/networking-with-combine-and-swiftui-fdf8182f7360,Peter Friese,2200,8,1307,"Not keeping the UI up to date across the different parts of an app can result in an infuriatingly bad user experience, and I am sure we all have at least one or two apps in mind that are notorious for this kind of behaviour.Writing apps that keep the state in sync across the UI and the underlying data model has traditionally been a difficult task, and the development community has come up with plenty of approaches to address this challenge in more or less developer-friendly ways.Reactive programming is one such approach, and SwiftUI’s reactive state management makes this a lot easier by introducing the notion of a source of truth that can be shared across your app using SwiftUI’s property wrappers such as @EnvironmentObject, @ObservedObject, and @StateObject.This source of truth usually is your in-memory data model — but as we all know, no application exists in isolation. Most modern apps need to access the network (or other services) at some point, and this means introducing asynchronous behaviour to your app. There are plenty of ways to deal with asynchronous behaviour in our apps: delegate methods, callback handlers, Combine, and async/await, to name just a few.In this series, we will look at how to use Combine in the context of SwiftUI to… and deal with some advanced scenarios.Let’s kick things off by looking into how to use Combine to fetch data from a server and map the result to a Swift struct.Let’s assume we’re working on a sign up screen for an app, and one of the requirements is to check if the username the user chose is still available in our user database. This requires us to communicate with our authorization server. Here is a request that shows how we might try to find out if the username sjobs is still available:The server would then reply with a short JSON document stating if the username is still available:To perform this request in Swift, we can use URLSession. The traditional way to fetch data from the network using URLSession looks like this:And while this code works fine and nothing is inherently wrong with it, it does have a number of issues:Running the code samplesYou will find all the code samples in the accompanying GitHub repository, in the Networking folder. To be able to benefit the most, I've also provided a demo server (built with Vapor) in the server subfolder. To run it on your machine, do the following:When they introduced Combine, Apple added publishers for many of their own asynchronous APIs. This is great, as this makes it easier for us to use them in our own Combine pipelines.Now, let’s take a look at how the code looks like after refactoring it to make use of Combine.This is a lot easier to read already, and (except for the guard statement that makes sure we've got a valid URL) there is just one exit point.Let’s walk through the code step by step:This looks a lot better and easier to read than the initial version, and we could stop here, and integrate this in out application.But we can do better. Here are three changes that will make the code more linear and easier to reason about:We often find ourselves in a situation where we need to extract a specific attribute from a variable. In our example, we receive a tuple containing the data and the response of the URL request we sent. Here is the respective declaration in URLSession:Combine provides an overloaded version of the map operator that allows us to destructure the tuple using a key path, and access just the attribute we care for:Since mapping data is such a common task, Combine comes with dedicated operator to make this easier: decode(type:decoder:).This will return decode the data value from the upstream publisher and decode it into a UserNameAvailableMessage instance.And finally, we can use the map operator again to destructure the UserNameAvailableMessage and access its isAvailable attribute:With all these changes in place, we now have version of the pipeline that is easy to read, and has a linear flow:Let’s finish off by looking at how to integrate this new Combine pipeline in our hypothetical sign up form.Here is a condensed version a sign up form that contains just a username field, a Text label to display a message, and a sign up button. In a real application, we'd also have some UI elements to provide a password and a password confirmation.All UI elements are connected to a view model to separate concerns and keep the view clean and easy to read:Since @Published properties are Combine publishers, we can subscribe to them to receive updates whenever their value changes. This allows us to call the checkUserNameAvailable pipeline we created above.Let’s create a reusable publisher that we can use to drive the parts of our UI that need to display information that depends on whether the username is available or not. One way to do this is to create a lazy computed property. This makes sure the pipeline will only be set up once it is needed, and there will be only one instance of the pipeline.To call another pipeline and then use its result, we can make use of the flatMap operator. This will take all input events from an upstream publisher (i.e., the values emitted by the $username published property), and transform them into a new publisher (in our case, the publisher checkUserNameAvailable in ).In the next and final step, we will connect the result of the isUsernameAvailablePublisher to the UI. If you take a look at the view model, you will notice we've got two properties in the output section of the view model: one for any message related to the username, and another one that holds the overall validation state of the form (remember, in a real sign up form, we might need to validate the password fields as well).Combine publishers can be connected to more than one subscriber, so we can connect both isValid and usernameMessage to the isUsernameAvailablePublisher:Using this approach allows us to reuse the isUsernameAvailablePublisher and use it to drive both the overall isValid state of the form (which will enable / disable the Submit button, and the error message label which informs the user whether their chosen username is still available or not.When you run this code, you will notice a couple of issues:We are going to dive deeper into the reasons for these issues in the next episodes, but for now, let’s address this error message:The reason for this error message is that Combine will execute the network request on a background thread. When the request is fulfilled, we assign the result to one of the published properties on the view model. This, in turn, will prompt SwiftUI to update the UI — and this will happen on the foreground thread.To prevent this from happening, we need to instruct Combine to switch to the foreground thread once it has received the result of the network request, using the receive(on:) operator:We will look deeper into threading in one of the next episodes when we talk about Combine schedulers.In this post, I showed you how to access the network using Combine, and how this enables you to write straight-line code that should be easier to read and maintain than the respective callback-driven counterpart.We also looked at how to connect a Combine pipeline that makes network requests to SwiftUI by using a view model, and attaching the pipeline to an @Published property.Now, you might be wondering why isUsernameAvailablePublisher uses Never as its error type - after all, network errors very much are something that we need to deal with.We will look into error handling (and custom data mapping) in one of the next episodes. We will also look at ways to optimise our Combine-based networking layer, so stay tuned!Thanks for reading 🔥Originally published at https://peterfriese.dev.",,2,3,2,16,1225,816,1,4,0,2,232
,,https://medium.com/u/28751859258f?source=post_page-----c35fc29ccf6--------------------------------,Furkan Kaplan,79,,,,,,0,,,,,,,0,,
,,https://medium.com/u/267e16a7c893?source=post_page-----c35fc29ccf6--------------------------------,Sarah,1300,,,,,,0,,,,,,,0,,
RosaKit — librosa for iOS,,https://medium.com/u/96a43b4fc41a?source=post_page-----c35fc29ccf6--------------------------------,Dmytro Hrebeniuk 🇺🇦,23,,64,"For ability run mostly models for Sound Processing, Analysis on iOS there is missed step for sound preprocessing/post-processing: melspectrogram, stft, istft and others…Now I’m ported such important methods:Actually project hosted at: https://github.com/dhrebeniuk/RosaKitAvailable via CocoaPods and Swift Package Manger(*)stft(Short-Time Fourier Transform):(Article about Fourier Transform and Spectrogram: [link]])istft(Inverse Short-Time Fourier Transform)Melspectrogram:Main goal get same results as in python librosa for ability posing sound/audio solutions to iOS/MacOS.",,1,0,0,4,1225,557,1,1,0,4,8
Training LSTM Model With MLCompute on iOS or macOS,Better Programming,https://betterprogramming.pub/train-lstm-with-mlcompute-on-ios-macos-8e9326ce948f,Dmytro Hrebeniuk 🇺🇦,23,2,142,"During WWDC 2020 Apple presented a new MLCompute framework. It provides a flexible API for training and inferencing neural networks on different computing units.I recently had a task to train an LSTM model.LSTM is an advanced RNN(Recurrent Neural Network) that has 2 states between predictions, unlike vanilla RNN.This improves predictions results and protects us from vanishing gradients during training.Our task was to predict future tendentious using the current state. We can train LSTM for solve this task:MLCompute used MLCTensor for calculations. We need to setup an array of MLCTensor objects for setup LSTM:For working with LSTM, MLCompute provides 2 classes:Here’s the code for the MLCLSTMDescriptor class:And the code for MLCLSTMLayer is:Before training, we need wrap our data using MLCTensorData object and then execute the train loop as shown below:You can download a sample project from my GitHub Repository:github.comThat’s all. Thanks for reading.",21/09/2021,,0,5,0,1132,901,2,1,0,3,115
,,https://medium.com/u/bc346811239d?source=post_page-----c35fc29ccf6--------------------------------,Pablo Manuelli,229,,,,,,0,,,,,,,0,,
Building a Responder Chain Using the SwiftUI View Hierarchy,Better Programming,https://betterprogramming.pub/building-a-responder-chain-using-the-swiftui-view-hierarchy-2a08df23689c,Emilio Peláez,228,5,912,"Over the past 10 years I’ve spent a lot of time answering questions in StackOverflow, and a question I see very often is a flavor of:How do I trigger an event in class A from a function in class B?This may seem like a simple question for a seasoned developer, after all there are multiple ways we can go about this, we can use delegates, callbacks, notifications, etc., but it’s still a situation that we encounter often, and the farther apart that the two objects are, the more complex it is to communicate between them.Part of the challenge is to implement this in a way that is scalable and maintainable. You could create an app where every event is sent via NotificationCenter, but you’ll be knocking your head on your desk soon enough.A responder chain is a design pattern where responder objects form a “chain”. Events are generated in one of the “links” or “nodes” of the chain, and the node determines if it can handle the event or not. If it cannot handle the event, the event is sent to the next node in the chain. This process continues until a node can handle the event, or the end of the chain is reached.An object-delegate relationship is a trivial example of this pattern. The object creates an event that it can’t handle itself, so the event is sent to the next node in the chain, which is the delegate. A more complex example is the UIResponder chain.Explicit responder chains are not super common in iOS development, probably because most of the core building blocks of UIKit, like UIViewController and its subclasses, make it really easy to build without one.With SwiftUI, however, every object is linked to the root view via the View Hierarchy, which means that the overall structure we need to build a responder chain is already in place, all we need to add are the responders and the events.An interesting property of the SwiftUI View Hierarchy is that some of its values, like the Environment and EnvironmentObjects, are passed down the hierarchy until they are replaced. We can use this, especially the Environment, to register our responders.To make things a little bit more visual, this is the hierarchy that would be generated from this simple view. Notice how modifiers, likeforegroundColor, create a node that is a parent to the view they are modifying.Environment values can seem intimidating at first because they are often used for system actions, like dismissing a sheet, and adding our own requires extending a system type, EnvironmentValues. It’s perfectly safe to do so, though, and since they can be any kind of type, their Type can be a closure.With this value in place, any view can read our custom environment value, and also set a new value to it. Since the value we added is a closure, our view can also call this closure.Earlier we mentioned that a responder has the responsibility to determine if it can handle an event or not. Our views will register themselves as responders by registering their own eventClosure into the environment.Any child views that call eventClosure will be triggering our view’s handler, as long as an intermediate view hasn’t replaced it. If our view determines that it cannot handle the event, it can use the closure it read from the environment, which will come from a parent responder, to allow the event to continue through the chain.Since this will be a common pattern, we’ll create a ViewModifier that will encapsulate this funcionality.Our new handler closure has a return type of Any? because that’s how we’re going to determine if the event was handled or not. If the returned value is nil, the event will be considered handled, if the returned value is anything else, that value will be sent up the chain.Views can now register themselves as responders by calling our modifier and passing a handler closure.Triggering an event is a lot simpler, all we need to do is to read the eventClosure from the environment and call it with any value.A simple but complete example would look like this:An important detail is that for a responder to be able to receive an event, the source of the event has to be a direct descendant of the responder.In practice this means that our responders will usually be registered at the root node of each feature or screen.One of the more common ways of triggering events from a child view in SwiftUI is passing a closure as an argument, like you would with a Button. When the view that is responsible from calling that closure gets deeper in the view hierarchy, this closure has to be carried by multiple intermediate views.Our approach is much more maintainable because only the responder and trigger views need to know about the event. This means that our hierarchy can be modified without having to “carry” this closure through multiple levels.Thanks to this, our approach is also scalable, and creating an event is as easy as defining a new type. No need to add a new environment value for every event.Following this pattern I created the framework HierarchyResponder, designed to handle events and errors.This framework takes this concept and expands it by adding specific modifiers to receive, handle, or transform an Event or Error, as well as catching errors.Another feature is that each modifier has a generic version with a Type parameter that will automatically filter any values that don’t match the type you supplied.",,2,0,4,3,1225,516,4,1,0,3,122
Colorizing Xcode Logs to Improve Log Tracing,Better Programming,https://betterprogramming.pub/colorizing-xcode-logs-to-improve-log-tracing-e67fbc727fd6,Furkan Kaplan,79,3,475,"As discussed before in a lot of developer forums, Xcode doesn’t support console coloring since Xcode 8. Because Apple has removed plug-ins from Xcode — the reason is security that Apple cares a lot — and has introduced Xcode extensions. And Xcode extensions are not providing any solution to colorize the terminal. In a short, there is no built-in support to achieve this at these days.Having an Android developer experience in my career, I have used Android Studio a lot and still admire what IntelliJ is doing for developers. While I am tracing logs in my daily life as an iOS Developer, I do miss the few great features of IntelliJ. One of them is advanced logging, definitely.Fortunately, Apple supports Unicode characters in Swift. So you can distinguish your logs with several emojis related to the type of the log. I was following this way but solution isn’t the best for readability.So I thought to build a solution that reads the system logs, filtering the related apps, and displays them instantly.Logify is an easy-to-use and easy-to-implement CLI tool. The most important part is that Logify does not work if you are collecting logs with print(_:) method. You must be logging in with OSLog framework provided by Apple.There are two types of logging. If your app supports iOS 14 below and probably does, you should use os_logIf you are following bests practices in bundle identifier, your identifiers like that com.company.product or com.company.subdomain.product.In my example: com.github.furkankaplan.bispy. You can split it two-part as subsystem and category are shown above. Just replace it with your bundle identifier.In case your minimum SDK is iOS 14 and higher, there is a more modern way in favor of Apple’s API improvement on OSLog, Logger.You may wonder why encode extension was used in the two examples? Now, may want to see the request and response JSON outputs in your logs for bug tracking. For instance, your logs could have newline characters. System logs are not read line by line. Sometimes several log messages are combined and read within one line. So the parsing algorithm should know the answer to that question:Is “\n” character is taking place in the log message or is it splitting several log messages?This is why you must add the below extension for both solutions.There are 4 types of messages: .default, .info, .error and .fault. You can change the colors for each type of message by forking the repository.These are not built-in solution to change colors for types.Here is the Logify Github Repository colorizing Xcode logs.After setting up the application, open the terminal app. Run the below command:After the download is finished, run the following command:As the last step, move the file to /usr/local/binHere we go!You can also use log stream shell command to view instant logs but Logify was created to colorize Xcode logs and improve readability.Thanks for reading.",,3,4,2,5,1225,782,3,0,0,5,337
How to Use Alignments in SwiftUI — The Basics,Better Programming,https://betterprogramming.pub/introducing-alignments-in-swiftui-10d4f4ba252,Mark van Wijnen,184,5,520,"This is the third part in the ‘SwiftUI Layout System’ series in which we will learn everything you need to know about Alignments. Like Stacks, understanding how alignments work will give you another powerful tool in your toolkit for building great layouts in SwiftUI.(More to come …)We will use our mascot, Flippy the Dolphin, from the Stacks tutorial to demonstrate the basics for alignment.In earlier stories, we walked through the steps of the layout system. The alignment of the views will take place in the third step of this process, in a “Parent places child in parent’s coordinate space”.Center alignment is the default for stacks, so if we set it explicitly it won’t change anything visually as it already aligned the views in the center of the parent view.You won’t be surprised that if you change the value from .center to .bottom all the views will be neatly aligned to the bottom of the parent view.This was simple and looks pretty decent. It will become a different story when we change the font size of the text Flippy from .body to .caption.As you can see in the image above. All the views are aligned to the bottom, but the visual alignment is all messed up.You got the baseline of the smaller text Flippy, you have the bottom line of the image and you have the baseline of the text The Dolphin and none of them line up.Fortunately, SwiftUI has us covered and we can simply use the .lastTextBaseline alignment to fix this.But what about the image? See the image has no text in it but every alignment has a default value and the default value for the last text baseline is just the bottom edge of the view.Now you could say that there is a visual baseline on the image, 50% (0.5) from the top, right around his snout.We can handle that by telling SwiftUI how to compute a last text baseline for the image in terms of its other alignments using an alignment guide.As you can see all the views now neatly line up.Let’s go back to our combined stacks example from “How to use Stacks in SwiftUI”.The views are now centered vertically by default, but what if we wanted to align the hearts, which is a view within a VStack, with the title “Flippy the Dolphin”, which is a view in a different VStack?We need an alignment that marks the middle of the hearts. So to do this we need to define our own (custom) alignment.We extend VerticalAlignment with an enum conforming to AlignmentID. Which has one requirement, to tell SwiftUI how to compute the default value. We simply choose the .bottom of the dimension as the default for no specific reason. You could set it to anything you like the default to be.Last, we define a static instance of VerticalAlignment that takes the enum type as its argument.Now we can use it to align the stack, explicitly setting it to the center of the hearts and of the title.In the next part we will talk in detail about the steps that SwiftUI takes to align its views.",,,9,17,6,1225,447,12,1,0,8,193
SwiftUI Flow Coordinator pattern to coordinate navigation between views,Mac O’Clock,https://medium.com/macoclock/swiftui-flow-coordinator-pattern-to-coordinate-navigation-between-views-8fa6ac487585,Michał Ziobro,32,4,692,"This article covers navigation in SwiftUI pre-iOS 16. If you are interested with implementing flow coordinators for newer versions of iOS starting from iOS 16 I encourage you to look at my updated article here SwiftUI Flow Coordinator pattern with NavigationStack to coordinate navigation between views (iOS 16 +)In this article I would like to demonstrate you how you can use FlowCoordinators with SwiftUI to separate navigation logic from view logic. In UIKit this pattern was very popular and such Flow Coordinators enable to push or present new view controllers separating this task from view controllers and view models code. This allow to decouple view controllers view code from its navigation and to easily change flow in application. Similar thing can be done in SwiftUI to the some extent.Most of the navigation in SwiftUI can be done using @Binding that keeps activation state of navigation and SwiftUI special modifiers and views i.e.fullScreenCover, sheet, alert, confirmationDialog or NavigationLink.And to present modals we can use something like this2. View and ViewModelTypically we separate presentation logic from business logic (or preparing view data) by splitting code into View and ViewModel.ViewModel should prepare all necessary data to display in view (outputs), and handle all actions coming from view (inputs).View should just handle displaying of this data and layouting them on screen.Simple View can look like thisAnd ViewModal for this view preparing text to display and handling firstAction like this.3. Creating Flow CoordinatorIn SwiftUI all navigation primitives must be called in context of view to work correctly. So we can do some assumptions about flow coordinators: 1. Flow Coordinator is View 2. We have Flow Coordinator per screen3. Navigation events should be passed to Flow Coordinator from ViewModel4. We need some enum that will represent those navigation events3.1 Creating protocol representing flow coordinator state This protocol enable us to pass navigation events from view model to flow coordinator.Here ContentLink is enum that represents different navigation events/actions.This protocol should be implemented by our ViewModel. This way view model in response to user action can handle them and pass navigation events to flow coordinator via this FlowStateProtocol.So our complete ContentViewModel processing several user actions and implementing ContentFlowStateProtocol can look like this.3.2 Creating ContentLink enum for navigation events This enumaration defines different navigation events that can happen in our screen of application. This events can have some parameters passed along them. Moreover ContentLink enum should be Identifiable and Hashable.In enumeration we define several computed properties i.e id to to fullfill Identifiable protocol, navigationLink to map parametrized cases of events to its sibling cases, sheetLink to isolate and map cases that should be displayed using modal presentation.3.3 Implementing per screen FlowCoordinator viewThe most essential part of our Flow coordinator patter will be ContentFlowCoordinator view. It will handle all per screen navigation logic.Firstly I will demonstrate how such coordinator can look and then explain some things.Firstly its init (here implicit) will have to parameters both using generics. 1. state that is of type implementing ContentFlowStateProtocol. 2. content which will be screen view @ViewBuilderSecondly state need to be stored as @ObservedObject and it shouldn’t be @StateObject as ContentFlowStateProtocol is implemented by ContentViewModel and this view model will be already stored as @StateObject in screen ContentView.Thirdly we have helper bindings made as computed properties for NavigationLink i.e. activeLink and for sheet presentation i.e. sheetItem.All navigation logic is implemented inside ContentFlowCoordinator body computed property. There is added NavigationView, embedded navigationLinks property and attached sheet(item:…) modifier.Last but not least we have factory functions that build our destination/content views. They extract eventual navigation event parameters, construct view models with them and finally view using this view model.4. Using FlowCoordinator with ViewThe final step that remain to complete ContentView screen is to put it all together and implement this view. This is the same view you so at the beginning of this tutorial but with added our brand new ContentFlowCoordinator and extending view model generic type to require to adopt the ContentFlowStateProtocol.We also added more actions to this view.If you are interested in full code source where you can clone repository and test this Flow Coordinator pattern in action I encourage you to visit my github link: https://github.com/michzio/FlowCoordinator-in-SwiftUI",,3,2,37,0,,,2,1,0,2,194
WatchTabView Inside NavigationView in SwiftUI,Better Programming,https://story.tomasen.org/watchtabview-that-can-work-inside-navigationview-in-swiftui-f7b7f4b3fe40,Shen Sheng,139,2,394,"The official TabView in SwiftUI looks simple and elegant. But unfortunately, it still can’t work properly if you put it inside NavigationView, on iOS and WatchOS both.On iOS, if a TabView is embedded in NavigationView, it will reset the navigation state and pop the NavigationView back to root every now and then, especially when the app went into the background or resumed. I confirmed with Apple’s Developer technical support that I was witnessing a bug.And on watchOS, it goes worse, the swiping just simply will not work if the TabView is embedded in NavigationView.After a few tries, I found one way to avoid these bugs is to put NavigationView inside TabView. But it means I have to put one NavigationView for each Tab, not only does the code becomes messy and redundant, the tabbar are shown in every View — which might be undesired behavior in some cases, and multiple NavigationView also breaks the whole navigation flow control inside an App.Luckily, implementing a view that looks like TabView in iOS is simple. Just a HStack and a @State that control which View to show.And it looks exactly like an authentic TabView.While I was working on my side project: Wordbook, the TabView becomes more complicated because it involves handling Swipe Gestures, Paginator, Animation, and Focus switch.First, I used ZStack with a HStack of circle dots on the top layer to simulate the pagination indicator. The trick part here is I have to use.edgesIgnoringSafeArea(.bottom) to position the pagination indicator at the right place – close to the bottom edge.The second tricky part is to handle the DragGesture and calculate animation on finger release to make the swipe motion look smooth.Then it’s the focus state. Since we are using ZStack with offset to decide which tab to be shown, which means the views are still there and might holding the focus even if they are not showing on the screen.Especially if there are Lists, you will find the scrolling digital crown doesn’t work if we are not setting the focus point to the tab that just moved to the center stage.Thanks to the @FocusState which just made available since watchOS 8.0, we can simply control the focus after we added the onTabChanged event callback function. Just make sure you use the focusable(_:) modifier to make the views such as VStack, Text focusable, which by default are not.",,,0,0,0,636,423,2,0,0,2,55
Build a SwiftUI PreviewProvider for UIKit Views,Better Programming,https://betterprogramming.pub/swiftui-preview-provider-for-uikit-3dd089d77915,Emad Beyrami,76,4,604,"Have you ever tried the SwiftUI? Haven’t you found it so delightful and easy to work with?However, in UIKit views, if you are prototyping a design in your codebase, you have to recompile and build your project every time to see the results in the simulator. But in SwiftUI it’s so pleasant to view changes on-demand without rebuilding.What if we can do that something similar to it for our UIKit Components? Like previewing or hot reloading the UIKit changes immediately without the time-consuming build and compile process.Essentially, we’re looking to create a live preview system for our UIKit views using SwiftUI.There are many ways to make this process easier such as simple annotations like @IBDesignable, @IBInspectable and etc.But if you have experience using these sometimes it gets buggy and causes some compile errors for rational reasons and sometimes it’s not the best approach.So in this article, you will learn a new and way simpler way to see your changes instantly. if you used SwiftUI you already know it but we add a little trick to it so it will work with UIKit.PreviewProvider is a protocol type that produces view previews in Xcode. It contains an associated type called Previews which is of type View. View is a SwiftUI view that we’ll return for the PreviewProvider to display — we’ll see this later.After creating an object which is conforming to this protocol, we’ll pass the view we want to see in our preview. It’ll automatically pop up a split view like the picture below.If the preview doesn’t load, worry not: simply use the below keyboard shortcut to see the reset the live preview:Note: Make sure to click on the resume button that appears in the upright corner of the preview content if you won’t see that it means it is working and showing you the live content.So now that we know about the PreviewProvider and how it works in SwiftUI, how can we achieve this in UIKit?The answer is simple: We have to convert our UIKit codebase into something that SwiftUI understands and use the SwiftUI PreviewProvider to see the live simulation of the content. Any guesses how?It is way easier than you think. Don’t make it complicated. All we need is the popular UIViewControllerRepresentable protocol.UIViewControllerRepresentable acts as a bridge to help convert the UIViewController to SwiftUI view. Have you guessed what are we planning to do yet?Now that we know about UIViewControllerRepresentable and PreviewProvider in SwiftUI, Let’s get our hands dirty and use them in a practical way.Let’s create a simple UIKit controller (UIViewController) which contains some UI modifications:If we run this ViewController you will see something like this:Now let's create a SwiftUI Preview. Any changes we’ll make in the UIKit UI could be simply displayed in real-time and we won’t have to build and compile like earlier.We simply make an object from our controller which is conforming to UIViewControllerRepresentable to make it readable for SwiftUI.In the above code:Inside the MakeUIViewController(context: Context) function, you should simply instantiate the Controller you want to see in previews.Here I just instantiated my Main Storyboard.Now that we have the UIViewController and have already converted it to SwiftUI Controller we just have to add a few lines of code for our PreviewProvider.Add the following lines of code before the #endif:The full code should look like this:Now that we have the full codebase, you can see the live preview of the UIKit UIViewController on-demand. Try modifying the UI slightly and notice the instant results. Say goodbye to the time-consuming building process.That’s all for now. Thanks for reading. You can check the full source code below:github.comwww.paypal.comnowpayments.ioThanks for your support in advance!",,4,2,9,0,1211,1264,4,1,0,4,884
,,https://medium.com/u/c2d2d5a10465?source=post_page-----c35fc29ccf6--------------------------------,Emad Beyrami,76,,,,,,0,,,,,,,0,,
,,https://medium.com/u/5da485909a35?source=post_page-----c35fc29ccf6--------------------------------,Sanju Naik,58,,,,,,0,,,,,,,0,,
,,https://medium.com/u/ac87e5d30b2a?source=post_page-----c35fc29ccf6--------------------------------,Sam Proctor,141,,,,,,0,,,,,,,0,,
How To Use Hilt to Setup a Solid Architecture in Android,Better Programming,https://betterprogramming.pub/using-hilt-at-its-full-potential-our-success-story-be4445ef799d,Dhanesh Katre,27,6,948,"Want to build a complete end-to-end feature flow with proper state management while also surviving configuration changes and of course, shooing off all the bugs? Here’s how Hilt helped us to create the most suitable architecture for this exact use case…A few months ago, while we initially developed a brand new feature, we had used single activity + nav-graph, along with only one ViewModel to control the entire flow.The thought process behind this was to have a central place for data and data processing, but we failed to foresee the growing complexity of the end-to-end flow!As a result, this monolithic architecture caused data and logic processing to be concentrated into one big ViewModel, with over 2k+ lines of code in a single file!The codebase turned into an Augean stable real quick, resulting in tons of bugs, a hefty amount of production issues, and total havoc in surviving configuration changes! Thoughts of making this flow better were lingering in our minds for long time…Fast forward a few months, a requirement came to enhance that feature, to cover more use cases and all possible flows. The feature could only scale up vastly. We decided to seize the moment, refactoring our code to make it more maintainable with lesser bugs and also a true survivor in cases of all the configuration changes!We analyzed the latest trends in Android development, few design patterns in this scenario, and came up with our own mods to form a new suitable architecture with the help of the dependency injection framework, Hilt.The following diagram shows the general architecture we adhered to:The essential components of the entire architecture are BaseViewModel, BaseFragment, and the DataSource classes that control pretty much the entire architecture!The code for BaseViewModel is given below:BaseViewModel here controls almost all the general activities used throughout the flow, including fragment to fragment navigation (with default nav arguments, so less cluttered nav graph XML), showing/hiding the progress dialog (loader), showing relevant toasts, etc.ViewModels can inherit from this BaseViewModel to use all of the functionality directly!The code for the BaseFragment is:A base class for all the fragments in the feature, connected by a nav graph. It especially supports ViewBinding and our beloved BaseViewModel, while observing the common live data emitted from BaseViewModel.As ViewBinding is used, creation and destruction of views can be easily generalized in this fragment, so child fragment doesn’t have to take care of it! (It can also be used without ViewBinding, in which case the constructor needs a little bit of tweaking)Notice that this base fragment bears an abstract instance of BaseViewModel, which every fragment can decide upon the provision, on its own! (So, if some of the fragments still want to share the ViewModel, it is possible)The code for the DataSource:The heart of our feature, data holder class, or a data source (inspired from clean architecture) is a special Hilt provided class that holds the gathered data, state variables throughout the flow!The specialty of this class is @ActivityRetainedScope annotation! That means, all the members that have this dependency injected, will receive the same instance of it, as long as they live in the same activity instance.This was otherwise possible by the creation of a Singleton class, but Hilt further helps to reduce memory usage by cleaning unwanted states after the flow is terminated fully!This way, state corruption can be avoided in case the same flow is getting reused elsewhere. The hilt also helps to provide the same instance across the configuration changes, so the state is always stored in a stable manner.One use case of FragmentTwo here, inheriting from BaseFragment, and doing all the UI handling in setupUI() method, while taking care of ViewModel activities in separate dedicated method setupVM().Notice that the view creation and disposal is not handled by individual fragments, but is managed by BaseFragment itself! So much lesser code with concerns separated properly.Also, a point, since we allow fragments to manage ViewModel instance creation on their own, we can directly use by viewModels() extension for that purpose! If some fragment still wants to share the ViewModel with the other fragments, it is also possible with using by navGraphViewModels() or by activityViewModels() functions as per the requirement.Given below is the ViewModelTwo used in association with our FragmentTwo:ViewModelTwo assists our FragmentTwo in surviving the configuration changes as well as in processing and validating the events/data coming from it!When a value is entered in edit text, it is passed onto ViewModel for storage and validation, and a flow is exposed towards fragment which is used to further control states such as continue button’s enablement or edit text’s error visibility.On click of the button, the entered value gets stored in our data store, which is injected with the help of Hilt, and the same value can then be used by the next ViewModel appearing inflow, as the data source instance will be the same for any of the injected component.Finally, based on the entered value, the navigation is being taken care of from the view model. All of this code is fully testable from a unit testing perspective, resulting in higher code coverage and lesser bugs!You can check out the full codebase, an entire working application using this architecture I’ve built on: https://github.com/dkexception/architecture-with-hiltUsing Hilt effectively in unison with other Android components, helped us a lot in developing quality features in time.The secret to developing great applications is to follow some of the core principles and standard guidelines. This architecture solves many of the common problems related to Android applications and is much recommended for flow-based feature developments.What do you think about this architecture? Would you like to improve it furthermore? Anything that you’re concerned about? Please do let me know in the comments!Special thanks to ",,3,0,5,0,973,807,2,2,0,2,247
,,https://medium.com/u/d440e6d6899a?source=post_page-----c35fc29ccf6--------------------------------,Michał Ziobro,32,,,,,,0,,,,,,,0,,
,,https://medium.com/u/cad7f7bef2a?source=post_page-----c35fc29ccf6--------------------------------,Mark Lucking,7000,,,,,,0,,,,,,,0,,
,,https://medium.com/u/3a9c66959e30?source=post_page-----c35fc29ccf6--------------------------------,Baha Abisheva,69,,,,,,0,,,,,,,0,,
Handling Core Image Filter Processing With Concurrency in Swift,Better Programming,https://betterprogramming.pub/handling-core-image-filter-processing-with-concurrency-in-swift-a016396c3a07,Mark Lucking,7000,5,926,"Over the last month, I have published a few articles on concurrent coding. It is a subject that was a central pillar in the WWDC 2021 — and I am sure we’ll be returning too in WWDC 2022.I want, in this piece, to look into an area that is the perfect application for concurrent coding — at a framework that Apple slipped under the radar this past summer, Core Image.The update was to the image filters that can now be invoked with as few as two lines. You can find a great reference to them under this link. A link that takes you to some excellent documentation that shows how to configure each filter with an example of what it looks like. Although I contest, there is a missing link that you’ll need to build to understand what each filter really does. This code is the missing link.It produces a sequence of UnsafeMutableBufferPointers to the data that makes up an image. We can then do a numerical analysis to understand better what the filter has done to said image — sure; it is evident in some cases — but not so much in others. A sequence that you can see here on the last line, pixels.It is interesting, too, because we can use it as a real-world backdrop to examine the performance of different versions of the same code, including concurrent versions. To compare and contrast their legibility, correctness and speed. Talking speed, I will use CFAbsoluteTimeGetCurrent to measure the performance seen.Now, I start by defining a struct that contains all the data I need to analyse, a struct that looks like this. It is four dictionaries that will summarise the data found in an image.The image I am going to look at is the self portrait. In an initial analysis the figures I get back look like this. I found 120'434 colours. An analysis that look 11.2798 seconds with this code.I think a snippet of code is legible and correct, but it runs far too slowly and will fail code review 101 since it essentially does a background job on the main thread.To start to address the problems I change the code to use a lower level API, namely map; code that looks like this.It’s a correct snippet, but sadly, I feel it is less legible and hardly faster, taking 11.0946 seconds and still running on the main thread, too, failing 101 code review.Next up, I thought I would try and implement it using some concurrent code, which I did by moving my data structure into an actor with this code.Code that I subsequently call with this snippet.A snippet that is correct and legible — even if it is a tad longer. It will also pass the 101 code review because it uses the new async Task directive, which doesn’t run on the main thread. But shock horror, it takes even longer to do the job, some 12.8886 seconds. The additional locking of the resource is taking its toll, and we are gaining next to nothing with asynchronous code it seems.Now I did want to simply add this to the map function as I did before. But as of Swift 5.5 map doesn’t support calling asynchronous code within it. I got around that shortfall by using this extension I found in this excellent article on the subject.Changing my code to call it to this— But embarrassingly it is the slowest version yet; taking an astonishing 14.5760 seconds to complete. Evidently I needed a break.I slept on it and came back afresh in the morning to reconsider my code. I had moved one step forward and then two steps back.Eureka, it occurred to me I could do the job using Sets; a dictionary was the wrong data type. I redefined the structure with my actor and re-ran the test; the redefined code looks like this.Code that I called with this piece.It is correct; legible would pass the code review 101. And — is fast; running in 4.5465 seconds. Yes, that is three times faster than the dictionary version — But I wait; this wasn’t running in parallel. I re-read my notes and reconsidered my code. I needed to use a task group.I recoded it again and — before I give you the code — I’ll give you the result — I got a respectable 2.3590 seconds to process. A perfect result. I had split the image into two parts and run the analysis on the separate sections, merging them on completion. An execution that it had evidently used the 2 high-performance cores on my faithful iPhone 8.So, of course, you know what I did next — I looked up how many cores my iPhone 8 has, six. So I modified the code and tried it again.I got 1.9852 seconds with three cores — a staggering improvement on an initial 11.09 seconds, with the asynchronous version running almost six times faster now. Although beyond that, I suspected the merging of the sets started to slow things down. I also started to lose accuracy as the partitioning wasn’t as clean as it should have been.All of which brings me to the end of this article — although I haven’t finished looking at image filters using my new analysis code, so watch this space — I’ll be back.A final bonus — here is bitbucket of all the code I played within this article, including I confess research on those filters I mentioned, but didn’t discuss. It isn’t super neat I admit, more of a mind map of code really.",,,7,10,0,952,783,2,0,0,5,125
,,https://medium.com/u/ade3e4c0586c?source=post_page-----c35fc29ccf6--------------------------------,Shen Sheng,139,,,,,,0,,,,,,,0,,
Practice SwiftUI Skills: API Calls and Coding Practice on an iPad,Better Programming,https://betterprogramming.pub/practice-swiftui-skills-api-calls-and-programming-practice-6efbf6914db3,Sam Proctor,141,10,1661,"Like any new language, once we learn how to communicate something, we want to practice. This is true whether learning french or learning to code. The issue we have in the digital format is that finding an idea for an app is difficult. It becomes a lot of effort to program an application from scratch and to come up with the data needed. But to become proficient at that language you need to practice, a lot. You need the words to become second nature, and not to reference StackOverflow every two minutes.Another, albeit more painful point of view, is that there are literally millions of apps on the App Store. There is a very small chance that what you want to create is any different functionally from what has already been done. Your app is not unique. Sorry, but there it is. Depressing stuff for so early on in this article, but please stay with me. There is a positive side, and we can only go uphill from here.With that acknowledgment out of the way, there is another way to win over users: User Experience.Your app idea may not be completely unique, you may have a slight spin on an existing idea that opens up new possibilities but ultimately the fundamentals are still the same for the user.But where your app will differ (unless you want some serious copyright infringement) is with the way it looks, the way it feels, and the way the user interacts with it. This is all user experience and helps to make an app visually pleasing to look at, fluid to interact with, preemptive on what information to display to the user.As a programmer, practicing with different designs will help you appreciate what does and doesn’t work, how changing the spacing or colours can really define an app. These are all things you can learn and sadly you have to fail, or fall into traps before you will understand what works and understand the design patterns.‘So how can we practice quickly on different apps?’, I hear you ask. Well, through APIs.There are thousands of free APIs (Application Programming Interface) which exist on the internet which provide a wealth of information.This is great because it means you no longer need to scratch your head thinking of an idea for a practice app or spend time creating all of the data you need to display. This is already done for you and is easy to access.The concept is simple:Take any API and imagine you have been hired by a company to produce an app that shows this data.You can play with how the information will look, how much detail you want to provide, or how to query the data. You can play with branding, colours, fonts, spacing.The end goal is to produce an app that is pleasing to a user. The best part about this is it should take about 2–3 hours per app to create (at least once you start to recognise working design patterns) which makes it really easy to practice your skills and make sure some of the languages you are coding regularly is really engrained.I am using SwiftUI for these examples, but the principle can be applied to any language. Below we start we will look at the basic steps, followed by a working example.Following the practice steps above, we want to choose an API. I am going to start off with a simple insult app, mainly because it is simple. As a user, I press a button and I display a random insult on the screen. This is more an introduction to API functionality but gives us something to expand upon.Reference the documentation, which can be found on the API’s website. I am using an insult API:On the website, you will usually see an example of the JSON object that will be returned. This is what your app needs to translate.To do this, create a struct with all of the elements and expected data types declared, as above.You will see moving forward that other APIs have much more flexibility and can give links to images or other data types.Now our app knows what to expect, we can look at how to communicate with the API:This simple block of code will call the URL and retrieve the response, and then decode it into a data model object (as previously written).You can see that https://evilinsult.com/generate_insult.php?lang=en&type=json is the URL by which we call to retrieve our data.If you look at the API documentation online you will see that by modifying this URL we can apply additional filters to what we want to retrieve. There are not too many options for the insult API but it is useful to know this for future projects (we will see a demo of this below).Next, is where to place this code? We can place this as the function of a button, or we can make this code automatically run whenever our view is displayed. This is useful to know if we do not want our app to be empty when it loads.For the sake of this example, we will add the code to be called when we press a button.Referencing the image above, we define an insult String which will hold the value of the retrieved insult.When we press Insult Me! button, this will retrieve the JSON object, decode it into an InsultModel object, and then we update the insult string with that of the InsultModel object insult.If we wanted to make this appear when the app first launches, we simply place the code within Task into a .onAppear{} modifier.This is important to know this placement, as often the first view for an app is a list of objects. Putting an API call in the onAppear modifier means the first thing the app will do is fetch the objects so that it can be loaded into the list.An example of using this in .onAppear{} is below:To extend this functionality, you may want to display a symbol or image if the retrieved result is a certain condition i.e. if a weather app returns ‘rainy’ you may display a picture of rain, else if sunny, a picture of the sun. To do this, we can simply add a ‘switch’ statement.You may want to display an image from an API. To do this, use AsyncImage and pass a URL into this. You can try this by referencing the API below to display a random dog photo:dog.ceoMy final example is a simple brewery database:www.openbrewerydb.orgThis app will allow the user to find breweries by searching for a name. It will then retrieve a list of breweries for the user to see.Then as a final example, we will display a different icon depending on the country the brewery is in.Step 1 is to make the Brewery struct. We need to make this conform to Identifiable in order to put this info into a List.Step 2 is to create three properties at the top of our content view. One will store the full array of returned values. This will be a Brewery array. The second is the search word that will be typed by the user. Finally, below this, we declare the API URL (excluding the query itself).We can see that by combining the apiURL and the searchTerm we get the address to use:Step 3 is to create the list that will display the results. For now, all we will display is the name of each Brewery per row.The second half of this code is the TextField for the user to type their search query.The final step of functionality is to set up the search button. The only difference here from what we have done before is that for the URL String, instead of stating the String here to use, we simply add the apiURL and the searchTerm.We can also improve our code by placing this call in a do catch closure, and by adding the print(error) statement. This allows us to identify when there are issues with decoding the JSON object into our data.This can happen regularly with different APIs, and without information on the error, this will simply return a nil value. The error will say why it failed, such as expecting a different data type.Putting this all together we get a functional app that will call a search term on the brewery database, return the results in an array and display the results as a list.We can go one step further here to demonstrate to show an additional element if a criterion is met.For example here, I will show a smiley if the brewery is in the United States, and a scribble if not using SFSymbols:To do this, simply pass the country name in, and if the case matches a given scenario, return the string of the SFSymbol. Then, in the List, for each brewery we state the name, and then we can show the Image next to it. Here is the updated List code:The final result looks something like this:This is just a guide to getting a quick functional app with data. You can see from the above, that this is all done in less than 50 lines of code. This will take 10–15 minutes to get to this point once you are confident and have practiced, which means you can then concentrate your time on looking at colors, spacing, sizes, symbols, and Images.The apps I have created above are not yet pleasing, and the next step is to practice this to see what does and doesn’t work. Hopefully the above gives you the basic concepts to displaying elements based on an API receipt. Now find some APIs and get programming. Some of my favorite quick apps can be created using free APIs that do the following:Now that you have this. My next article will concentrate on how to manage different formatting, and create themes. Centralizing a lot of this information will help to create and modify views from one place. Happy coding.",,1,0,0,0,1225,846,8,2,0,3,67
Build a SwiftUI watchOS Weather App for an Existing iOS Application,Better Programming,https://betterprogramming.pub/adding-a-watch-app-to-your-existing-ios-application-xcode-13-d229dd9422c5,Baha Abisheva,69,4,407,"This demo will continue building upon the app created in the previous article.In this part we will:As before I am using Xcode13.2 for this demo.We will start where we left off in part 1. So open your project in Xcode.To add a WatchOS target to your project:3. Choose a product name and click “Finish”.4. In the next pop-up that asks if you want to activate the scheme for WatchApp complication feel free to choose “Cancel” as we won’t be using it in this tutorial.After these steps, you should end up with the following project structure and new files added.Delete the WeatherAppApp.swift and ContentView.swift files from the Watch App target, since we will be reusing the ones from the iOS app.To be able to share the files between both targets, select WeatherAppApp.swift, ContentView.swift, WeatherModel.swift, WeatherAPIClient.swift and from the File Inspector Tab add them to the WeatherWatchApp WatchKit Extension.If you run the newly created WeatherWatchApp scheme you will see that Xcode will automatically attach a phone simulator to your watch simulator.If you want to attach the watch to a specific simulator, you can do so from “Window -> Devices and Simulators”.Choose a simulator of your choice and tap on the + icon on the paired watches pane to add a watch simulator.If you don’t want to always launch a phone simulator when you run the watch app, select your watch app extension target, and in General settings choose “Supports Running Without iOS App Installation”.If you run the WeatherWatchApp scheme now, you will see it launch only on the watch simulator, without installing the app on the phone simulator.However, you will see that you will land on the default screen (middle image below) since location permissions have not been enabled for the app.To enable location permissions for the watch app, add the same location tracking permission as you did for the iOS app target to your watch app extension target as shown below and the next time you run your app you should see the app ask you to enable location services.To make sure the UI fits on the watch, update the ContentView.swift file to wrap the main VStack inside of a ScrollView as shown below:Now you have an app that runs both on the Apple watch and on the iPhone.In the next part I will go over establishing watch connectivity to send data back and forth between your iPhone and watch to update the UI and show notifications.Happy coding!",27/12/2021,,0,0,1,1225,756,11,2,0,5,220
Reducing Our Build Time By 50%,Gojek Product + Tech,https://medium.com/gojekengineering/reducing-our-build-time-by-50-835b54c99588,Sanju Naik,58,13,2371,"Gojek’s iOS consumer app is a multi-repo setup with 40+ git submodules. We are a team of around 100 iOS Engineers pushing hundreds of commits every day to ship features to the Gojek Super App. To give you a sneak peek into our CI stats, on a busy day we run around 300–350 CI Pipelines. 🔥We use the “God workspace” model for development, i.e., all the modules are added as subprojects to one single Xcode workspace. This large setup poses its own set of challenges and the build times are one of the topmost concerns. To keep up with the fast-paced dev environment, improving build time was the need of the hour for us.To give you an idea about the problem, our 95th percentile incremental build time was ~11 mins 📈, which is quite high. This was affecting developer productivity and also not a good developer experience.We used XCLogparser to analyze Xcode build logs, unearthed the hidden information there, corrected misconfigurations, and also changed the way we load Cocoapods. All of this helped us achieve a massive 50% reduction in build time 🎉 🎊 . Our incremental build time was reduced from 11 mins to 5 mins.The below graph shows the amazing results we achieved:It’s amazing how just tweaking the Xcode build settings helped us achieve such a massive reduction of 50%. Let’s dive into the details to understand more, well before that, a little bit of background about the problem.We have a Multi-repo setup with 40+ git submodules. The project repository structure looks something like this:Consumer app’s main repository has Xcode workspace which includes all of the module’s Xcode projects as subprojects.At Gojek we are a developer experience team focused on solving build and infrastructure-related problems to provide the best possible experience for our developers and improve their productivity.Here are the statistics around what was our incremental build time before.Before starting this project, our 95th percentile incremental build time was ~11 mins. The below trendline shows our incremental build time over a period of time.Since our incremental build time was considerably high, reducing it became our priority. Also, developers spend most of their time building the app incrementally hence any improvements we make to incremental build time have a direct impact on the developer productivity and the experience.Better developer experience would mean that developers can ship features faster and this, in turn, makes the business stay ahead of the competition.To analyze the problem better we needed to collect the data from developer machines, such as targets built part of a build, time-taken by each target, cache utilization, etc. We used XCLogparser for this.XCLogparser is an open-source CLI tool by Spotify which parses xcactivitylog file (A log file created by the xcodebuild system per build and stored in DerivedData, which has all the information related to the particular build) and gives a detailed build report in json or html format.We added XCLogparser in Xcode’s post-build script. Xcode triggers this action as soon as the build completes. So we were able to parse build data using XCLogParser and push it to our in-house dashboarding system.The below image shows Xcode’s post_build actions script.The post_actions.sh script internally uses XCLogParser, & at the end of the script makes sure the script executes in the background and doesn’t add any extra overhead on the build times.Here are some of the visualizations we built out of the data we collected.Dashboards were good, and gave us good insights about build time metrics, but they only gave us a bird’s-eye view of the problem. To understand finer details we needed something else.When we were thinking about the alternatives, one of the ideas was that we can just directly talk to the developer the moment the build happens. Talking at that immediate moment was crucial because later developers might not remember what exactly they did.The idea seemed good but how do we do this? We are a team of around 100 engineers, doing this manually would have been a nightmare, that’s when we built a slack workflow to automate this process.Since we were already measuring build times using XCLogparser, so whenever an incremental build took >3 mins, we pushed that data to our internal Slack channel using a Slack bot.Our Slack workflow looked something like this :And here is the screenshot of the message contents.That’s it 💥 🍾. The slack build time reports helped us to quickly talk to developers and also investigate build reports on our dashboards. This was the major breakthrough!Even though this was not a long-term solution, we had to do this to understand the ground reality. As a Developer Experience Engineer, it’s important to be a developer first and to empathize with a developer. This meant we march in tandem with the developers and know their pain in and out. And the results were surprising. The micro feedback established the connection between the build engineers and developers, we were able to capture developer sentiments and understand how exactly they work and their pain points.As we analyzed build logs and tried reproducing the build scenarios, we felt we need to know more to decipher the mysterious Xcode build logs. Compiler performance document by Apple has good information on how the compilation process works and some tools and tricks to debug build-related problems.From the above document we mainly used “-driver-show-incremental” and “-driver-time-compilation” flags. These flags need to be added to “other swift flags” build settings as shown in the below image:Once the above flags are added, we need to build the application and look for the “Compile swift sources” step in the build logs of a module. The logs would show the reason why that particular module is built.Build system output after adding the above flags would look like this:Along with this, we used a few tricks from the below gist, by Daniel Dunbar, Apple Build systems Engineer.From the above gist we enabled “defaults write com.apple.dt.XCBuild EnableDebugActivityLogs -bool YES” and “defaults write com.apple.dt.XCBuild EnableBuildDebugging -bool YES”.When the above flags are enabled, they create build.trace file at “DerivedData/<project>/Build/Intermediates.noindex/XCBuildData/buildDebugging-xxxx.yyy”. The trace file will have a list of rules created by the build system and tasks created to execute them. The contents of the trace file would look like this :This is how we interpret the above trace file: Build system created rule-R4 to build target WebServiceExample and in the next steps it checks whether this rule(R4) needs to be executed or not. Then in the next line, it says the “rule-needs-to-run” and the reason is “never-built”. This means the module was never built or artifacts don’t exist in the cache (DerivedData).In the next step, we can see that the build system creates a new task-T4 to run the rule R4. Likewise, we can search for the target we are debugging and for the reason, it's being built.Note: All the Above flags should be enabled only while debugging, If we leave those enabled it will slow down the build.We extensively used the above two debugging techniques and XCLogparser to debug build-related problems.The very first problem we identified was, every time we did a clean build, we were doing two clean builds instead of one. i.e Our first build after the clean build (first incremental build) also took an equal amount of time as the clean build.When we looked at the logs of the 2nd build, we noticed that most of the modules were getting recompiled even though they were compiled in the 1st build and there was no code change. We analyzed why modules were getting recompiled using our learnings and the results were surprising.When we looked at the build.trace file located at “DerivedData/<project>/Build/Intermediates.noindex/XCBuildData/buildDebugging-xxxx.yyy” to figure out why most of the modules were getting recompiled in the 2nd build. We noticed that one of the core modules was marked as changed in the 2nd build, which was causing a rebuild of all its downstream dependencies.To further analyze why the core module was marked as changed, we compared its xcodeproj build settings with other modules and noticed that one of the entries in “FRAMEWORK_SEARCH_PATHS” was set to ‘recursive’, because of this, instead of reusing artifacts from DerivedData it was getting rebuilt. Since it’s one of the core modules which other modules depend on, so all of its dependent modules were also getting re-built, resulting in a complete rebuilding of the app.The solution was to change the FRAMEWORK_SEARCH_PATHS to `non-recursive`. After this change build system was able to reuse artifacts from cache and dependencies weren’t getting compiled in the 2nd build. With this change, we saved ~15 minutes for our developers every time they did a clean build.It’s always recommended to set FRAMEWORK_SEARCH_PATHS to non-recursive to avoid such build-related problems.The second problem we identified was, almost our entire app was getting rebuilt when we switched between schemes.Developers can choose to build a particular module’s scheme or the entire Gojek app from the scheme selector menu as shown in the below imageEx: When developers built the main app Gojek-Integration and switched to building another scheme say Launchpad, all the dependencies of the Launchpad module were getting rebuilt instead of being utilized from the cache.We picked up a module (let’s refer to it as ModuleY ) for analysis. We wanted to see, does this module has the same compiler inputs when it's being built as part of both builds (Gojek & Launchpad schemes). As different compiler inputs can trigger a rebuild.When we looked at the compiler inputs, the Swift files compilation step wasn’t of much help because swift files had the same compiler inputs for both the builds. So when we were looking for a different file type, we found that every Xcode target has a C file called “ModuleName_vers.c”, so we used this file for comparison.The below image show’s the compilation step of WebService_vers.c, we can see the inputs passed to the compiler.When we compared compiler inputs of `ModuleName_vers.c` file, we noticed that 2 extra flags were passed when the Gojek scheme was built. Here is the screenshot of the comparison :The “-fprofile-instr-generate” and “-fcoverage-mapping” were the extra flags being passed. By looking at the flags we thought, this might be something related to code coverage and then when we verified the 2 schemes (Gojek-Integration and the Launchpad), we found out that the Launchpad scheme didn’t had the “gather coverage for” option enabled. As shown in the below image.So when we enabled the “gather coverage for” option for Launchpad, compiler inputs for both the builds were the same and dependencies weren’t getting re-built. With this change, switching between schemes just took 2 mins, compared to 13 mins earlier. i.e 80% improvement 🚀 💥.It’s Imperative to have the same build settings across Xcode projects when they are part of the same workspace.The third problem we identified was, our entire app was getting recompiled when developers switched between simulators. This problem was more concerning than the first 2 because It's quite common for developers to verify their changes on at least 2 simulators when they are working on UI-related features. And every time they switched simulators, they had to wait 15 mins for the app to launch😞.We tried switching between simulators and analyzed build logs to figure out why this is happening, every time we switched between simulators we noticed that our in-house modules were getting rebuilt with the reason - “because of external dependencies”.Using the dependency graph we tried looking at what are those external dependencies, when we traced the dependency tree we figured out that the external dependencies were 3rd party cocoapods libraries. From this, it was evident that for some reason cocoapods were causing the rebuilding of all of its downstream dependencies and that resulted in the almost complete rebuilding of the app.When we analyzed which task or step is getting executed in cocoapods, traversing through rules and tasks in the build.trace file, we noticed that cocoapods had a run script to copy generated header files and module map files to `BUILT_PRODUCTS_DIR` and this script was getting executed every time we switched between simulators.Since the run script was modifying `BUILT_PRODUCTS_DIR` i.e the output directory of the pod in DerivedData, hence the build system marked that pod as modified, which was causing rebuilding of all of its downstream dependencies.Cocoapods have a run script to copy the generated header files and the module map file to `BUILT_PRODUCTS_DIR`. More Information on module map and generated headers files can be found here https://clang.llvm.org/docs/Modules.html#module-map-file .The Below image shows the run script:Cocoapods adds this run script only when pods are added as static libraries. We have around 70+ cocoapods added as static libraries, adding them as dynamic frameworks wasn’t an option because that would affect our app launch time.Adding cocoapods as dynamic frameworks was not an option as that would affect our app launch time. We found out that Cocoapods provide a way to load pods as static frameworks as well, which is sort of the best of both worlds. The linkage is still static but the pods are added as frameworks so they don’t need a run script to copy module map and generated header files to built products directory.To add cocoapods as static libraries we need to add this line to Podfile - “use_frameworks! :linkage => :static” . This worked like a magic with this change switching between simulators just took 2 mins compared to the earlier 15 mins (i.e 85% reduction).As we solved the above 3 problems, we saw a massive reduction in 95th percentile incremental build time. Build time was reduced from 11 mins to 5 mins, i.e more than 50% reduction. Also, this change helped us save around 60 hours of developer time per week.The below graphs show the build time improvements we made:This was a huge win for us and was appreciated by everyone across the organization.We managed to reduce our build time by 50% from where we were, but we aren’t stopping here, our end goal is to have 95th percentile incremental builds under 1 minute. To get build times under 1 minute we have a few things in our pipeline, Spiking out on the Bazel build system is one among them.Thank you for reading this blog, do watch out, this space for our future blogs, we will keep sharing our learnings.Check out more stories from the vault here.Oh, and we’re hiring:",,4,0,1,17,1161,457,23,1,0,9,639
,,https://medium.com/u/e7afb7329437?source=post_page-----c35fc29ccf6--------------------------------,Frank,657,,,,,,0,,,,,,,0,,
,,https://medium.com/u/545fcf6f1868?source=post_page-----c35fc29ccf6--------------------------------,Ángeles VP,5,,,,,,0,,,,,,,0,,
Routes & Arguments In Jetpack Compose Navigation,,https://medium.com/@kevinskrei/routes-arguments-in-jetpack-compose-navigation-4b19db5cf76e,Kevin Skrei,1,3,403,"If you’re like me and recently started using Jetpack Compose for anything other than a simple view, you’ve probably run into the question “How can I navigate between composables?” I had trouble finding documentation that explained using composables, routes, arguments and HiltViewModel’s together. This post will cover how to define routes to navigate between composables and how to pass arguments between those views.How do you get from screen A to B? Let’s say you have a master-detail setup where your master screen shows a list of items and your detail screen shows details of the item you selected from the list.Define a master list where we’ll show a list of items that can be clickedWe can also define our other composable for showing details about that item.Those are the basic building blocks and we’ll expand upon those definitions later.To start defining routes between composables, add a NavHost to your root.There are a few things to unpack from this snippet:The route definitions and arguments are relatively simple to implement.At this point, we’re able to navigate to the detail page when the user clicks on a list item. The question now is, how do we access the route argument to know which item to show on the detail page. We’re using Hilt at Arccos Golf and Hilt makes it easy to integrate with compose navigation. We can do this by using the SavedStateHandle in the view model constructor.Read over the docs for SavedStateHandle to find out what types are supported and how to use non primitive types.One of the most glaring documentations omission was how to pass in multiple arguments to a view model? What if you also wanted to pass in an image url to the Detail composable?I’ve found that the argument definitions are very similar to a query string used on the web. Here is an example with a couple of extra parameters.Simply access these keys via the SavedStateHandle in the view model like the example above.Above is a very contrived and simple example. There is plenty more functionality I’m omitting for brevity. You probably want to use a little bit more abstraction when defining your navigation code instead of using magic strings everywhere. This example also glosses over setting up the various options you have when navigating and how best to setup the back stack. If you want to learn more about Compose navigation, check out the docs here.Thank you for reading!",,,5,1,15,,,0,1,0,4,5
,,https://medium.com/u/e4ae3ec302ba?source=post_page-----c35fc29ccf6--------------------------------,Alex Vanyo,128,,,,,,0,,,,,,,0,,
,,https://medium.com/u/317abb4f7762?source=post_page-----c35fc29ccf6--------------------------------,Dhanesh Katre,27,,,,,,0,,,,,,,0,,
,,https://medium.com/u/7e42b4b980b6?source=post_page-----c35fc29ccf6--------------------------------,Mario Pepe,28,,,,,,0,,,,,,,0,,
,,https://medium.com/u/317abb4f7762?source=post_page-----c35fc29ccf6--------------------------------,Dhanesh Katre,27,,,,,,0,,,,,,,0,,
Navigation in Jetpack Compose using ViewModel state,,https://medium.com/@ffvanderlaan/navigation-in-jetpack-compose-using-viewmodel-state-3b2517c24dde,Frank,657,3,426,"In this article, we will show an example of using a ViewModel to initiate navigation in Jetpack Compose. For this, we use navigation state, not events.In Google’s Jetpack examples, navigation is triggered from the composable View. As a result, control of the screen state is shared between the View and the ViewModel. Using the ViewModel to control navigation results in a single source of truth. For example, when the ViewModel starts an asynchronous action and wants to navigate after the action finishes, it can cleanly and easily do so.Joe Birch, among others, describes a singleton navigation manager to handle ViewModel initiated navigation. This implementation is simple, but has one vulnerability. Any ViewModel (or any class) can trigger navigation, not just the screen that is currently in view. This can cause ViewModels in the backstack to start navigation. For example, after an async action finishes. This could result in hard-to-find issues.When we let screens listen to their own ViewModel and navigation state, ViewModels in the backstack cannot initiate navigation because they don’t have a view. Only when the view attached to a ViewModel is resumed, then the ViewModel’s navigation logic can be triggered.When we abstract away the logic that is needed for this approach, then the implementation can be simple and short. But before we do so, let’s discuss state and events.“Events up, state down” is Google’s proposed way of communicating between ViewModel and View, also for navigation. They chose this unidirectional data flow because of data consistency, testability and maintainability.In practice, this means:For example, a navigation flow might look like this:Note that in the example project, most logic in this flow is abstracted away.Using state in the ViewModel for navigation might feel a bit strange at first, especially because of step 4 and 5 in the diagram: notifying the ViewModel of the navigation (4) and setting the state to Idle (5). Using state here does make the code unidirectional and unambiguous.The needed navigation logic can be found in the ViewModelNavigationCompose example project and is not elaborate. In your project, you will need to use these 3 files:If your composable supports navigation arguments, take a look at ContentPageRoute.With this approach, you will have a single source of truth. View state, including navigation, is controlled by the ViewModel. The navigation logic is abstracted away, keeping your ViewModels and Routes short and clean.This logic is a starting point. navigateForResult was not implemented yet; I usually try to keep it out of my projects as much as possible. If you have any comments or suggestions for improvements, let me know.",,2,0,0,7,919,463,2,5,0,18,119
,,https://medium.com/u/d39a01aa72f0?source=post_page-----c35fc29ccf6--------------------------------,Kevin Skrei,1,,,,,,0,,,,,,,0,,
How To Use Hilt to Setup a Solid Architecture in Android,Better Programming,https://betterprogramming.pub/using-hilt-at-its-full-potential-our-success-story-be4445ef799d,Dhanesh Katre,27,6,948,"Want to build a complete end-to-end feature flow with proper state management while also surviving configuration changes and of course, shooing off all the bugs? Here’s how Hilt helped us to create the most suitable architecture for this exact use case…A few months ago, while we initially developed a brand new feature, we had used single activity + nav-graph, along with only one ViewModel to control the entire flow.The thought process behind this was to have a central place for data and data processing, but we failed to foresee the growing complexity of the end-to-end flow!As a result, this monolithic architecture caused data and logic processing to be concentrated into one big ViewModel, with over 2k+ lines of code in a single file!The codebase turned into an Augean stable real quick, resulting in tons of bugs, a hefty amount of production issues, and total havoc in surviving configuration changes! Thoughts of making this flow better were lingering in our minds for long time…Fast forward a few months, a requirement came to enhance that feature, to cover more use cases and all possible flows. The feature could only scale up vastly. We decided to seize the moment, refactoring our code to make it more maintainable with lesser bugs and also a true survivor in cases of all the configuration changes!We analyzed the latest trends in Android development, few design patterns in this scenario, and came up with our own mods to form a new suitable architecture with the help of the dependency injection framework, Hilt.The following diagram shows the general architecture we adhered to:The essential components of the entire architecture are BaseViewModel, BaseFragment, and the DataSource classes that control pretty much the entire architecture!The code for BaseViewModel is given below:BaseViewModel here controls almost all the general activities used throughout the flow, including fragment to fragment navigation (with default nav arguments, so less cluttered nav graph XML), showing/hiding the progress dialog (loader), showing relevant toasts, etc.ViewModels can inherit from this BaseViewModel to use all of the functionality directly!The code for the BaseFragment is:A base class for all the fragments in the feature, connected by a nav graph. It especially supports ViewBinding and our beloved BaseViewModel, while observing the common live data emitted from BaseViewModel.As ViewBinding is used, creation and destruction of views can be easily generalized in this fragment, so child fragment doesn’t have to take care of it! (It can also be used without ViewBinding, in which case the constructor needs a little bit of tweaking)Notice that this base fragment bears an abstract instance of BaseViewModel, which every fragment can decide upon the provision, on its own! (So, if some of the fragments still want to share the ViewModel, it is possible)The code for the DataSource:The heart of our feature, data holder class, or a data source (inspired from clean architecture) is a special Hilt provided class that holds the gathered data, state variables throughout the flow!The specialty of this class is @ActivityRetainedScope annotation! That means, all the members that have this dependency injected, will receive the same instance of it, as long as they live in the same activity instance.This was otherwise possible by the creation of a Singleton class, but Hilt further helps to reduce memory usage by cleaning unwanted states after the flow is terminated fully!This way, state corruption can be avoided in case the same flow is getting reused elsewhere. The hilt also helps to provide the same instance across the configuration changes, so the state is always stored in a stable manner.One use case of FragmentTwo here, inheriting from BaseFragment, and doing all the UI handling in setupUI() method, while taking care of ViewModel activities in separate dedicated method setupVM().Notice that the view creation and disposal is not handled by individual fragments, but is managed by BaseFragment itself! So much lesser code with concerns separated properly.Also, a point, since we allow fragments to manage ViewModel instance creation on their own, we can directly use by viewModels() extension for that purpose! If some fragment still wants to share the ViewModel with the other fragments, it is also possible with using by navGraphViewModels() or by activityViewModels() functions as per the requirement.Given below is the ViewModelTwo used in association with our FragmentTwo:ViewModelTwo assists our FragmentTwo in surviving the configuration changes as well as in processing and validating the events/data coming from it!When a value is entered in edit text, it is passed onto ViewModel for storage and validation, and a flow is exposed towards fragment which is used to further control states such as continue button’s enablement or edit text’s error visibility.On click of the button, the entered value gets stored in our data store, which is injected with the help of Hilt, and the same value can then be used by the next ViewModel appearing inflow, as the data source instance will be the same for any of the injected component.Finally, based on the entered value, the navigation is being taken care of from the view model. All of this code is fully testable from a unit testing perspective, resulting in higher code coverage and lesser bugs!You can check out the full codebase, an entire working application using this architecture I’ve built on: https://github.com/dkexception/architecture-with-hiltUsing Hilt effectively in unison with other Android components, helped us a lot in developing quality features in time.The secret to developing great applications is to follow some of the core principles and standard guidelines. This architecture solves many of the common problems related to Android applications and is much recommended for flow-based feature developments.What do you think about this architecture? Would you like to improve it furthermore? Anything that you’re concerned about? Please do let me know in the comments!Special thanks to ",,3,0,5,0,973,807,2,2,0,2,247
Functional Error Handling in Flutter,Better Programming,https://medium.com/@mario.pepe/functional-error-handling-in-flutter-ac6fcf8ae22b,Mario Pepe,28,6,792,"While Functional Programming (FP) is a programming paradigm known to bring many advantages in specific software domains (compilers, programs that involve parallelism, concurrency, distributed computing, and more), I’ll argue that some elements of FP can be very beneficial in state-intense applications too.In this first blog post of mine, I’ll focus on error handling and I’ll show you how I use FP error handling to have a cleaner and safer way to handle the error flow in my software.For the sake of this article, I have created a simple application (here is the link for GitHub). This app fetches a list of posts from a remote API https://jsonplaceholder.typicode.com/posts (huge shoutout to @typicode for this great utility) and displays them in a list. Should any error happen, the application will properly reflect them in the UI.The architecture of the sample app is inspired by the principles of Clean Architecture (+ Repository Pattern) and it’s certainly well over-engineered for the simplicity of the goal, but since this article wants to present an advanced way of handling errors in software, I have reckoned it made more sense to have a well-architected sample app (plus I intend to reuse part of it for future articles).Some aspects of the sample app, such as the techniques for the state management, the use of sealed classes, the repository pattern, etc. are out of the scope of this article and I will not focus on them now, but I do welcome questions regarding any aspect of the software presented.The library I’ll use in this post is dartz, the most popular FP package on pub.dev with 579 Likes.It’s worth mentioning that there exists, in the flutter ecosystem, also another FP library, called fpdart with 143 Likes, at the moment of writing this article.For error handling in FP style, I find particularly useful the use of the custom type Either<Left, Right> and the .fold() method it encompasses.Either<L, R> is a special data type that can contain two different objects (not contemporary), in our sample app we will have Either<Failure, List<Posts>> so it will either contain failures detail (on the left side, where it’s common practice to store errors) or it will contain a list of posts, on the right side. For extreme clarity I repeat, naturally, it is not possible to have both a Left and Right object at the same time, either you find yourself with the data you were looking for or with an error state, not both.We will later use the fold function to perform different actions based on whether the value contained in Either variable is Left or Right, for instance, we will have:The files most relevant towards the topic of functional error handling are:We define our custom exceptions in lib/core/error_handling/exceptions.dart.We define 3 failure entities related to the 3 exceptions previously created in ‘lib/core/error_handling/failures.dart’. The Failures Entity are:Of course, this is a simplistic representation of the wide spectrum of all possible errors, but it is a reasonable proxy to present the core concept of this article.(I use freezed but this is definitely not needed for the sake of this exercise)We define an ErrorObject in ‘lib/core/error_handling/error_object.dart’. ErrorObject is a utility class to transform a failure into an Object that is representable for the UI (i.e. it contains a user-readable error title and an error message).We create our connection with the JSON placeholder API in ‘lib/features/post/data/datasources/json_placholder_v1.dart’ and based on different possible errors we throw the appropriate custom exceptions previously defined.In ‘lib/features/post/data/repositories_impl/posts_repository_impl.dart’ we convert exceptions (impure states) to failure objects (pure states) and we either return a list of posts on the Rights side of the Either object or, as we said, a Failure on the Left side.Finally we unpack the Either object inside the bloc at ‘lib/features/post/presentation/bloc/posts_list.dart’ and we emit different states based on what is needed.The UI result of all this is observable in ‘lib/features/post/presentation/posts_list_page.dart’, where, via the use of freezed sealed classes, every possible state resulting from the bloc is mapped into different UI representations.Sealed classes are extremely powerful and have many applications that I intend to detail more in future articles.While doing error handling we have 2 main broad objectives:I hope that in this article I managed to show how functional programming dramatically helps us in achieving these goals as it forces us to think about the possible application states and to proactively manage them.To make things even more explicit, think about this: when we want to access the posts list we have to call the fold() method and we have to provide proper action both for Left and Right cases, or the code will not compile, a pretty safe way to handle errors, I would say!Thanks a lot for reading until here, let me know if you have any questions, suggestions or you would do anything differently!github.com",,2,2,1,3,1225,845,2,3,0,6,562
Feature Flags inside our Android App,,https://medium.com/@avazpar/feature-flags-inside-our-android-app-7c9fa1f4c946,Ángeles VP,5,5,834,"Gradually, mobile application development teams are looking to provide more functionalities for their users, more reliable code and agility to deploy their applications, therefore, software development tools are evolving day by day.One of the best ways to provide greater automation and lower risks is to incorporate Feature Flags in our projects.Feature flag is a great software technique which allow software development teams to modify the behaviour of the application without changing the code. They only need to change a flag remotely and it updates this variable in the production application. This allows developers to take control of the release of their features and remove features that don’t work well once released to the Google Play Store (production app).There are many advantages to incorporating this software technique into our code, but from my point of view, the most important is peace of mind and reliability of the code. When uploading a new feature to production, in the event that it starts to fail, we can quickly deactivate this functionality and make the user go for the previous functionality that we know definitely works. It is a winwin; the development team can implement new functionalities and also have a contingency method that allows the user to continue using the functionality in case of error.Invisibility: Feature flags allow you to launch new features that are still under development. Since each new feature is behind a toggle, it will not become visible until you decide to activate it.Speed and flexibility: bugs are also dealt more efficiently: if someone finds a bug with a feature behind a toggle, you can immediately switch it to off. So, there is less pressure to try to get everything right first time.A/B Testing: Feature toggles are a good way to facilitate experimentation, test ideas and check what is best for your users.Risk reduction and security: You can revert your product or functionality immediately if you detect a bug or receive negative feedback. So, you could test your new features in production while collecting feedback from users.There are several ways to implement a Feature Flag, in my opinion, the ideal way would be to have these flags remotely and update them in our local device, to avoid making many calls to the API service.One way to develop it is making a call to our API service that would return a JSON object where all our feature flags values are stored, and when we start the application we could load and store them locally.Another way to implement it would be through the Firebase Remote Config tool and SharedPreferences to store our Feature Flags locally. For this article I chose the last option because Firebase Remote Config is a great tool. To do that, I used the following steps:1) Import Firebase Remote Config library in your project2) Create a CLEAN structure proyect, MVVM architecture. In my case I created a view, domain and data layer.- Data: Repository and DataSource class- Domain: UseCase class- View layer: Fragment and ViewModel classNow I am going to explain the different layers in more detail.I started with DataSource, I needed two interfaces and their implementations: RemoteFeatureFlagDataSource — Firebase Remote ConfigLocalFeatureFlagDataSource — SharedPreferencesThen, I created the Repository, in this part is where we can set the logic which determines how the code behaves. For that, you can code this logic here or you can create a new Policy class. Feel free to use whatever you prefer.After this, I created the UseCase. In this layer we want to be more concrete than before, because we want to create one UseCase for each feature that we want to enable/disable remotely. In this example I used DarkMode feature, so the class was named DarkModeFlagUseCase.At this point, we could create a class for each feature with the ID, name, default value or a constant. In this case, I chose a class for each one, but it’s up to you.Now, only the view layer is missing, but keep calm, it’s easy! We just need to call the UseCase from the ViewModel and use the flag value result to enable or disable some feature. In this example I wanted to show a screen in dark or light mode (depends if we receive a true or false value in this flag).Want to make your application cooler? If we want to go a step further and make our application reactive to changes (that occur remotely), proactively activate or deactivate our functionality in real time and also update the value in our local variable, we have to use some form of reactive programming.The feature flags usage can help companies deliver more frequently, minimise risk, increase productivity and add A/B test functionalities.I can assure you that when you start implementing feature flags in your project, they will give you so much peace of mind that from that moment on you will not be able to upload a new feature without its associated flag.I hope that it can serve as inspiration for coming up with a solution that adapts to your own needs.",,,5,182,11,1225,815,1,0,0,0,11
Jetnews for every screen,Android Developers,https://medium.com/androiddevelopers/jetnews-for-every-screen-4d8e7927752,Alex Vanyo,128,10,2121,"We recently updated Jetnews to enhance its behavior across all mobile devices, both big and small. We’d like to take you through our design and development process so that you can learn the philosophy and associated implementation steps for building an application optimized for all screens with Jetpack Compose, including how to build a list/detail layout. If you are new to Compose, take a look at the excellent guides on developer.android.com, as this post assumes a basic understanding of Compose as we improve an existing Compose app.Prior to our work to optimize for all screens, Jetnews found itself in a situation that is likely shared by many apps today. From the initial designs to implementation and testing, the primary focus was building a great experience for traditional portrait phone displays. This is understandable, but the growing diversity of devices challenges that focus. Across tablets, foldables, and Chrome OS there are now over 250 million active large screen devices running Android.Jetnews already had support for “traditional” mobile screens, so it was tempting to describe all of our changes as “adding large screen support.” While that is true, it misses the point of having adaptive UI. Let’s take a step back, and try to reframe the end result we want to create and maintain for our app.A user’s device is their unique, personal portal into the digital world. As an app developer, we should let users run the app in the orientation and configuration they prefer. Concretely, the user gives our app a window: a specific portion of the screen where we can display interactive UI. Most often this is the entire device screen, but it doesn’t have to be. If the user wants to use their phone in landscape or portrait, or split-screen multiple apps, they should be able to.There are many ways that an app’s window may change. To highlight just a few, split-screen support, foldable devices with an inner and outer display, and resizable windows on Chrome OS all impact your app’s window. It may be daunting to try to think about supporting each scenario individually, but there is a framing that simplifies the task significantly.The common thread between all of these scenarios is the screen size available to your app, which is the most relevant piece of information for displaying your app’s UI in the space the user is giving you. This is the primary reason why methods Display.getSize() and Display.getRealSize() were deprecated and replaced with WindowMetricsCalculator.computeCurrentWindowMetrics(). For example, if your app is running in split screen mode on a tablet, it shouldn’t try to display “tablet UI” unless it actually has enough space for it.This is the mindset we had while revamping Jetnews:Given the amount of screen space available to us, how can we best display content describing our app’s state to the user?To create a UI tailored to the size of the window, we want to perform logic based on the window metrics returned by WindowMetricsCalculator.computeCurrentWindowMetrics. In addition, we need to adjust the UI anytime this computed value changes. Because a configuration change can update the window size, we will need to compute the current window metrics again whenever a configuration change occurs. To do this, we build rememberWindowSize() (soon to be provided by a Compose material library itself!):Whenever the configuration changes, rememberWindowSize() will recompose and return the current Size for the new window metrics. Every piece of UI that reads that size will automatically recompose and update.This approach meshes perfectly with the declarative mental model of Compose. Instead of trying to use an onWindowSizeChanged callback or exposing the changes as an observable Flow, we reduce the size of the window into a simple piece of observable state. We can now use this state just like any other state in Compose, combining it with other states in our application to declaratively specify the UI displayed to the user.Convert the window size into observable stateNow that we have the raw window size, we want to turn it into a meaningful value to base layout decisions on. Since the raw window size can take on any arbitrary combination of width and height, we will want to use breakpoints to group allSize objects into a small set of buckets.To do this, we will use the Material window size classes specification, which groups window sizes into 3 distinct classes based on the width of the screen: Compact, Medium and Expanded. These three classes allow thinking about window sizes naturally in terms of common scenarios, which can be referenced in our designs.With all of this piping in place, deciding which elements to show is just a matter of checking the current window size class. For example, in Jetnews we want to replace the navigation drawer with a navigation rail when the window size is expanded.We accomplish this by disabling the drawer when the window size is expanded, and adding in a navigation rail:On the interests screen, we also want to change the style of the tab row depending on whether or not the screen was expanded, so we can again use isExpandedScreen to switch between which content to display:With Compose, it’s straightforward to swap out UI elements entirely based on the available space. With state hoisting, we can also preserve common state between these UI variants based on the screen size, which we will take a look at soon.Prior to our changes, the article list and article detail list were always displayed separately, at separate navigation routes. This works well on smaller screen sizes, but with wider screens we can take advantage of the extra space by using a list detail view to display the article list and a selected article at the same time.Because this impacts the design of the app at the screen level, this change will also have to impact the navigation structure of the app.The approach that seems the most straightforward at first would be to navigate in response to the screen size changes. If we were on an article detail screen, and then rotated the device to get more horizontal space, then perhaps we could navigate to the list screen to show both the list and the detail.This approach quickly shows some drawbacks, however. If we were to rotate our device back, the user would expect to be brought back to where they were before rotating the device at all. To accomplish that, we’d need to know that we were looking at the detail screen before rotating, and conditionally navigate back to the detail screen if this is the case, storing that state somewhere outside of our navigation graph.Going down this path fights the idea of unidirectional data flow, where side effects from state are avoided where possible. We will likely see some visual artifacts (destinations displayed for a single frame), since we won’t be able to decide to navigate until we compose the screen for the first time causing a recomposition loop.Avoid side effects (like calling navigate) upon size changesA better approach is taking a step back and adjusting our navigation graph to avoid needing to navigate as a side effect of a size change. Previously, we had our list and detail screens at two different navigation routes. If we combine these to have both be displayed at the same route, we can swap out the entire screens with a conditional statement just as we already have done for the navigation rail and the tab row. That structure looks like the following:Instead of a simple if statement like for the taskbar, we now want 3 cases: we have enough space to show the list and the detail, or if not, we want to show either the list or the detail independently. In code, that looks like the following:Traditionally, each navigation route is paired one-to-one with a specific screen, so each screen is always displayed at a different route, and each route displays a specific screen. However, here we’re breaking that normal pairing. Now, at the same home route, we can choose to display one out of three screens, instead of always displaying just one. To control that logic, we define HomeRoute to just switch which full-size “screen” we display, based on normal, conditional logic for whether or not the screen is expanded, and if the article is currently open. Each of these Screen composables are still responsible for displaying UI that fills up all of the available space, and this layering avoids each screen needing to know exactly where it sits in the navigation graph.To get our original navigation behavior when pressing the back button when just the detail screen is shown, we also include a BackHandler that will set isArticleOpen to false. This effectively provides a form of navigation without actually using the navigation library directly.By controlling the article and detail navigation ourselves within a single route, we keep the backstack stable no matter how we fold, flip, resize, or rotate our device, and we keep complete control over the state of the screen at all times.One consequence of combining two navigation routes into one was combining the ViewModel from each route into one as well. That is okay! The combined ViewModel would be bigger, but we also don’t have to put all logic into it. As explored in managing state in Compose, we can use a combination of different levels of state holders to avoid having a large object responsible for every single piece of state at each route.Any time the screen size changes, whether due to an orientation change, folding or unfolding a device, or resizing a window, an Activity can be recreated due to a configuration change. Even if you don’t change any UI for different screen sizes, ensuring that state is properly handled will avoid a bad experience where a user loses their work or their place in your app.Preserve the user’s state and current task while rotating, resizing, and foldingOur two primary tools to save state through activity recreation and process death with Compose are rememberSaveable and the Architecture Components ViewModel with the SavedState module. Many built-in Compose components already make use of rememberSaveable internally, like rememberLazyListState and rememberDrawerState.Once we are switching components depending on the screen size, we also want to preserve state within those different components, even if they are not currently visible.For example, we want to preserve the scroll state of the list and the articles, even if they might not always be visible at all times due to size changes. This preserves the user’s place, even as they fold or rotate their device.By hoisting the list and detail scrolling states to the level of the HomeRoute, the user’s scrolling state is preserved despite having multiple ways to display that state.Building upon this new navigation foundation and taking advantage of hoisting state, we can more easily add in additional behavior to make the experience even better for users.While the list and article screens are both visible, the user might be reading through an article, or they may be browsing the list for the next article to read. If they fold up their device, or otherwise cause the app to resize to a point where we only have space to show one or the other, we should remember which one the user was interacting with last.We can create a modifier to notify us whenever the user interacts with a composable, allowing us to update some state as the user performs any interaction with the app:Using this modifier, we can keep track of which pane the user interacted with last when both the list and details are showing, and update whether or not the article should be open if we only can show the list or the detail:With all of these changes, Jetnews is working better than ever on large screens, but also on small screens too. Ensuring the user’s state is preserved and intelligently displaying content based on the screen size available will lead to a better user experience when rotating between portrait and landscape on a traditional phone, while also setting up a solid foundation for differentiated experiences on foldables and large screen devices.The philosophy of Compose is built around transforming state into UI, so treating the available screen size as a state opens up many exciting doors and unique experiences. As you migrate components, screens or entire apps to Compose, or you build a new app from the ground up, we hope you keep these principles in mind to help make it easier to support every user’s device.You can download the sample and take a look at the improved Jetnews implementation on GitHub. You can find more design guidance for canonical layouts and large screens on the Material Design 3 website, and also see more information on responsive UIs for both Views and Compose on developers.android.com.",,3,0,6,2,967,484,6,0,0,18,480
Navigating with App Shortcuts,,https://medium.com/@umarauna/navigating-with-app-shortcuts-e0533cb6b010,Umar Saidu Auna,30,6,676,"Not long ago, iOS users were introduced to a new feature called 3D Touch. What this feature does is provide a limited list of actions that you can do without entering an app. Of course, the people from Google were ready to respond to that. Boom!!! the App Shortcuts feature was included in Android 7.1.1App Shortcuts are designed to do common actions in the application from the launcher screen. We can reach certain screens by just long-pressing the app icon and clicking the respective shortcut.These shortcuts are predefined. They come together with your .apk. Their number and actions stay the same until you publish a new version of your app.These shortcuts are constructed and included in the list with shortcuts on the fly. They can be created based on some usage statistics and can change over time. There is no need to publish a new version of your app, to update your dynamic shortcuts.These are used for specific, user-driven actions. For example, a user might want to pin a specific website to the launcher. This is beneficial because it allows the user to perform a custom action, like navigating to the website in one step, more quickly than using a default instance of a browser.App Shortcuts allow the user to access primary actions in your app straight from the launcher, taking the user deep into your application, by long-pressing on your app icon.Don’t overdo it, please…According to the official Android documentation, the current API supports up to five different shortcuts at any given time. BUT it is highly recommended to use a maximum of four of them.Another limitation is the launcher of the current device. Not all launchers support App Shortcuts yet. That is, before releasing this new feature we must test it on as many different launchers as possible.App Shortcuts works perfectly with PWA (Progressive Web App) and supports:Check this slide to know moreBefore we start there are a couple of attributes that I will like to go over, so we understand what we are doing.We will create three (3) static shortcuts in the app, so for each shortcut, we must provide:Codes Snippet goes here….Add the following code snippet in your AndroidManifest.xml file, if you don’t add it, your shortcuts will not appear, we have to add some <meta-data> to the launcher Activity. In practice, any Activity with the intent filter is set to action. The android: resource is where you specify where the shortcuts files are located in your resource folder.Navigate to shortcuts.xml where the shortcuts are defined. I had to set the tools:targetApi=”25"" because Android Studio just dey shout anyhow (meaning Android Studio keeps warning me). It’s a good practice regardless because this feature is only available on API level 25 and above. once you are done run your app on your emulator or device to see the shortcuts.Dynamic shortcuts are the second type of shortcuts, and to interact with them (create/destroy/update) you’ll need to use the ShortcutManager. You can get a hold of a ShortcutManager usingShortcutManager uses system service so this has to be in an activity and remember this only works on Android 7.1 so it's good to add the annotation @TargetApi(25) to this code to avoid compile errors and add version check before calling these methods.To create a new shortcut, we have to use ShortcutInfo. Here shorcut2 is the id given to the shortcut.setRank() the method is used to order the dynamic shortcuts in the shortcuts pane. it is used to position the shortcut in the list, 0(zero) being the lowest positioned one.Now that we have our shortcut ready, we need to set it as a Dynamic Shortcut.dynamicShortcutsmethod takes an Array. Since we have two shortcuts, I have created a listOf().manager.removeAllDynamicShortcuts() will remove all the dynamic shortcuts.Run your app and long-press the app icon in the launcher. You should see something like this:Code is available here on Githubgithub.comThank you for reading this article. Be sure to clap and recommend this article if you found it helpful and insightful. It means a lot to meChat me up on Twitter or Linkedinblog.iamsuleiman.comwww.journaldev.commedium.comguides.codepath.comhttps://developer.android.com/guide/topics/ui/shortcuts/creating-shortcutshttps://commondatastorage.googleapis.com/androiddevelopers/shareables/design/app-shortcuts-design-guidelines.pdf",,,0,16,0,855,736,12,5,0,10,10
,,https://medium.com/u/6bd8f468c0ac?source=post_page-----c35fc29ccf6--------------------------------,Umar Saidu Auna,30,,,,,,0,,,,,,,0,,
Animations in Jetpack Compose with examples,Canopas,https://blog.canopas.com/animations-in-jetpack-compose-with-examples-48307ba9dff1,Jimmy Sanghani,727,5,890,"Hi guys, today we are going to explore how we can implement animations in Jetpack compose. Full source code of this project is available on github, feel free to fork or directly use required animations in your applications.This post consists of 4 animations:Final implementation will look like this:Alright!!Let’s begin with first animation.We will first design static UI for the animation with tap functionality, which we will later animate.Let’s add following code to implement stepper UIWe have used a row that will have 3 divisions, first and third for detecting taps and middle one for showing current step number. We also have flipBack and flipNext functions which are called on tap but haven’t implemented yet.If you look at animation closely, stepper has two sides — front and back. We need two sides because we keep showing current side while switching to next one. Let’s make above UI reusable and implement stepper that will switch sides without animation.We have done following changes to previous code:If you run the code now, stepper will work but without any animation and that’s not cool at all. Let’s add some animations.To add animation, we will use state animation API of Jetpack compose.We added a rotation animation that will smoothly animate targetAngle value whenever its value changes. We updated stepper code to use rotation from animation instead of targetAngleWell, that’s it. Run the composable and you will see stepper animate!!This animation has two parts — Jumping heart image and bottom shadow. We will implement jumping heart first!Let’s start with basic UIA column with icon, pretty simple!Let’s add jumping animation, we will use Jetpack compose infinite animation API.Let’s discuss the changes we made:If you run the composable now, you will see heart jumping. Now let’s add the missing shadow!We added a new Box that’s responsible for showing shadow. You will see we have configured two graphicsLayer properties here which are dependent on the animation — scaleX and alpha . That’s because we want to animate those properties at the same time. We are not limited to two properties though, possibilities are infinite with jetpack compose!!Run the composable and you will see fully implemented heart animation.UI is pretty self explanatory except the dot variable at the top. Ignore that for now, it stores the animatable values that we will use later on.Otherwise, we just iterated an array with 3 elements and created 3 dots. Well, at least dot looking Box! We also have logic to add space between dots that is executed for each iteration except last one as we don’t need space after last dot.If you run the composable now, you will see three static dots.Now is the time to animate dots. We will use jetpack compose Animatable API.We should use this API if we want to animate the value dynamically, i.e in our case we want to delay animation start of the second and third dots. We can not use start delayMillis as that is considered for repeat animation as well. We just want to delay initial animation, and then continue loop without any delay.We create animation variable that will animate between 0 to 1. As previously discussed, we delay second and third dot ball animation, 100ms each. Also, we have used keyframes animation API here.keyframes allow us to define what target value should be at particular time. We set it to 1.0 at 200ms and set it back to 0 at 400ms as we want to jump dots only for 400ms. With keyframes, we can create very cool animations that will change its rate depending on time.Now let’s add remaining code to complete the animationHere we transform Animatable array to its values and create travelDistance variable for dot jump height. Later, we configure translationY of the dot view to change its position according to animation.We are done with the animation! Run the composition and see dots jumping in rhythm.Wave animation is very much like progress animation. The difference is in the UI and also the animation property. For waves, we animate alpha and scaleX scaleY properties.Let’s begin by adding static UIPretty basic UI stuffs. We iterated waves and added circle view for each wave. We also added a static mic icon in the center that will not be animated.If you notice, we have used same 50dp size for all waves. Well, that’s fine, we will animate it to different size with scale transformation.Now let’s animate the waves~Here, just like progress animation, we created animation that will animate between 0 to 1 infinitely. Based on that, we scaled waves and animated alpha between 1 to 0, that will make waves disappear at the animation end when they are scaled maximum.That’s it, if you run the composable now, waves will animate!That will be it for today. As we observed, Jetpack compose animation APIs are very straightforward and clear to use. Also, they allow us to write complex animations with very few lines of code.If we had to write the above animations using traditional android views, the implementation will not be as clear as above and it might take lots of lines of code and our brain too!Full source code of the animation is available on github.Thanks for your support!If you like what you read, be sure to 👏👏👏 it below — as a writer it means the world!Follow Canopas Software to get updates on interesting tech articles!Happy animating!",,,0,3,1,747,629,2,3,0,6,291
,,https://medium.com/u/fda3ab4e57cf?source=post_page-----c35fc29ccf6--------------------------------,Federico Torres,84,,,,,,0,,,,,,,0,,
Passing arguments to screens in Jetpack Compose,,https://fvilarino.medium.com/passing-arguments-to-screens-in-jetpack-compose-e01c2c51e73f,Francesc Vilarino Guell,206,4,669,"In anything but a trivial app we want to have the business logic of our screens separate from the UI (our Composable functions), and one way to do that is using the ViewModel classes available from the AndroidX libraries to host that business logic. In this short article we will see how we can have our ViewModels automatically receive the navigation arguments when we navigate from one screen to another.The first thing we need to do is add the dependencies for navigation, hilt and viewmodel, which we can do by adding these lines to our app’s build.gradle file:Next we want to define the routes that the user can navigate to in our app. In this example we will only have 2 screens, and navigation will be from Screen One to Screen Two.When defining the routes there has to be a root destination that will be the destination users land on when launching the app. Routes are defined as a string, and if there are any arguments that the route expects, those are defined as a path parameter in that route string, similar to the path segments of a URL. It is also possible to define arguments as query parameters, but this is better left for the scenario where arguments are optional. If you are interested in reading more about this you can check this link.For our example, our Screen Two will accept a single argument that we will define as of type String, so our route to the 2nd screen will have a path element for that argument. Arguments in the route are defined as a placeholder wrapped in curly braces, so for instance{userId}. We will call our routes one and two for our screens, and we will name our argument for the 2nd screen as arg for this example. With all that said, our routes will be:We have split the route to the 2nd screen into the root element and the argument element so that we can use those later separately, we will see how shortly.Next we want to build our navigation graph using the routes we defined earlier. For this we use the NavHost function provided by the navigation library. This function takes a few arguments, namely:In the lambda for the NavHost we will add each screen with their respective route. This registers each screen in our navigation graph, so that we can navigate to each of them using the unique route associated with each of them.It will probably become clearer if we see the code, so let’s do that:Let’s see how we build the navigation graph:The 2 most important things to note here are the following:As we saw above, the first screen navigates to the second and in doing so passes an argument. To retrieve this argument, all we need to do is define our ViewModel constructor as having an argument of type SavedStateHandle. This is a glorified map that has our navigation arguments and that we can also use to persist data from the ViewModel. When we navigate using the navigation library this SavedStateHandle is pre-populated with the navigation arguments, using as key the placeholder we used when defining the route, so all we need to do is retrieve them, which we can do by calling get on the SavedStateHandle object:It’s as simple as this:And that’s it. In this example we are passing a single String argument, but you can build on top of this and pass additional arguments and arguments of different types. However, it is worth pointing out that the navigation arguments should not be used to pass large objects (like a string representation of a Json object), you should instead pass an id that references the data you need at your destination and then you can retrieve the necessary data in your viewmodel using that id from your repository or any other data source you may have in your app.A complete app using the principles discussed in this post is available here. I hope you found this short article useful, happy coding!",,1,2,0,6,,,0,4,0,2,65
Blazing fast Compose tests with Robolectric,,https://medium.com/@sebaslogen/blazing-fast-compose-tests-with-robolectric-b059f5471495,Sebastian Lobato Genco,229,3,586,"While writing a new library for Compose that handles Compose behavior, I had to pick a framework for running automated tests.The library is about state restoration in Compose, so the tests involve navigation and interactions like device rotation, among other Android app behaviors that do not really depend on what is rendered on the screen. For these tests, I could write Espresso Instrumentation tests, but they are very slow, and a waste of time and resources when we don’t care about pixels.The goal is to have fast and reliable automated testsInstead, I decided to give Robolectric a chance. This framework offers an alternative implementation of Android APIs that brings reliable and fast unit tests to Android. Now I can focus on the behavior of the library instead of the implementation of Android, and run the tests inside the JVM on my computer in seconds, no device nor emulator needed.At least those are the promised results 😍 Let’s see if it’s true and how much we can improve our 🐌 5 minutes test runs.This article will cover the setup of the libraries, the tests, and how to write basic tests combining Compose and Robolectric test APIs.Some of the most valuable tests can be found in the user interaction flows and integration layers of our apps. In the case of flows, testing the sequence of steps and the correctness of the data, without testing the actual pixels on the screen, can give us 90% assurance that the flow works, especially when the new Compose UI framework promises to reduce the quirks and flakiness of the legacy Android View system.Using Robolectric and Compose we can achieve that level of confidence with tests that run faster on the JVM and tests that are more stable because they don’t depend on UI rendering speeds.Let’s start with the dependencies setup. There are only a couple of tricky points:Almost ready to write the first test, but before, let’s make the UI of our app testable by adding test tags. These will let us easily find UI nodes in tests:The basic setup for a test class is quite simple:The new thing here is the compose test rule that will help us control the tests.The step below is optional but I highly recommend it. It provides a nice text representation of the UI tree in the console output where we run the tests. To use it we need to call the printToLog method on a composeTestRule node.In our setup, tests are divided into two categories: with and without the activity controller, also known as the ActivityScenario.Without an ActivityScenario we can use the composeTestRule to manually set the content of our Composable screen during the test setup, then perform actions on the nodes of the Compose UI tree and assert the state of the UI:With an ActivityScenario we can let the Activity set the content and initialize ViewModels like in our real app. In this case, we have access to extra tools and the Activity where we can simulate a device orientation change:This is just the beginning of my ❤️ story with Robolectric tests. The official documentation has much more information and tips on testing. And there is even a codelab to learn how to test Navigation in Compose.And the results? The final test run went down from 5 minutes to 12 seconds 😍 plus a ridiculously simple integration with CI builds.🌈 Happy testing! 🌈PS: If you want to know more about the Compose library, check this article.Thanks to Jolanda Verhoef for peer reviewing this article.",,1,0,3,10,1225,719,1,1,0,16,37
,,https://medium.com/u/d5596a6fa49c?source=post_page-----c35fc29ccf6--------------------------------,Sebastian Lobato Genco,229,,,,,,0,,,,,,,0,,
,,https://medium.com/u/c09b24c8e7b2?source=post_page-----c35fc29ccf6--------------------------------,Ataul Munim,895,,,,,,0,,,,,,,0,,
Grouping Semantics in Jetpack Compose UI,Google Developer Experts,https://medium.com/google-developer-experts/grouping-semantics-in-jetpack-compose-ui-93fa47e615db,Ataul Munim,893,3,406,"The semantics modifiers let us change aspects of the semantics tree in Jetpack Compose UI — a representation of the UI that’s helpful for accessibility services and the testing framework.We can use these modifiers to group a number of widgets into one logical element, making it faster to navigate a list of similar elements using an accessibility service like TalkBack.For example, in YouTube Music, the current playlist is represented as a list of tracks, each with:Screenreaders like TalkBack will focus on each element with an intrinsic description (text) or action, which means that navigating from one track to the next track will cost users ~5 gestures.We can re-write the semantics information for the track so that it’s represented as a single logical element:Using the clearAndSetSemantics() modifier is necessary; it’s not enough to use the semantics() modifier in this case.Modifier.clearAndSetSemantics() will clear the semantics information for all descendant nodes and update the current node with the given properties.The documentation for this function says:this can be used to provide a more polished screen-reader experience: for example, clearing the semantics of a group of tiny buttons, and setting equivalent actions on the card containing them.That’s exactly what we were after! However, there are implications to clearing descendant nodes’ semantics:Modifier.semantics() lets us add semantic information to the current node. Our aim was to simplify the representation of each track to a single node, so this wouldn’t have helped here.In fact, we’d have made it worse by making the entire row focusable (in addition to what we already had).Using Modifier.semantics(mergeDescendants = true) is slightly more useful because it will reduce the number of focusable elements.Setting mergeDescendants to true will:Elements with a clickable modifier won’t be removed because they use mergeDescendants = true. In our case, this means the play/pause and the menu actions will still be focused.The title, artist and track length nodes aren’t clickable. Here, there’ll be an attempt to merge the semantic information from these nodes, which will result in duplicated information because we set the content description manually.Modifier.clearAndSetSemantics() gives us a lot of control over the semantic representation of a node and its descendants in Compose UI. While it’s more powerful than Modifier.semantics(), we also need to be more careful with it.As a rule of thumb, consider reserving its use for elements in a collection (lists or grids) and always test the behavior.If you liked this post, have any questions, comments or corrections, please reach out on Twitter.",,1,0,1,0,1010,482,1,3,0,2,93
,,https://medium.com/u/c35282dbcce8?source=post_page-----c35fc29ccf6--------------------------------,Jimmy Sanghani,727,,,,,,0,,,,,,,0,,
Preventing Smart Contract Attacks on Ethereum — Reentrancy attack,Better Programming,https://betterprogramming.pub/preventing-smart-contract-attacks-on-ethereum-a-code-analysis-bf95519b403a,Abhishek Chauhan,338,5,643,"One of the features of Contracts also typically handle ether, and as such often send ether to various external user addresses. These operations require the contracts to submit external calls. These external calls can be hijacked by attackers, who can force the contracts to execute further code (through a fallback function), including calls back into themselves.Attacks of this kind were used in the infamous DAO hack.This type of attack can occur when a contract sends ether to an unknown address. An attacker can carefully construct a contract at an external address that contains malicious code in the fallback function.Thus, when a contract sends ether to this address, it will invoke the malicious code. Typically the malicious code executes a function on the vulnerable contract, performing operations not expected by the developer.The term “reentrancy” comes from the fact that the external malicious contract calls a function on the vulnerable contract and the path of code execution “reenters” it.To clarify this, consider the simple vulnerable contract in EtherStore.sol, which acts as an Ethereum vault that allows depositors to withdraw only 1 ether per week:This contract has two public functions, depositFunds and withdrawFunds.The depositFunds function simply increments the sender’s balance.The withdrawFunds function allows the sender to specify the amount of wei to withdraw.This function is intended to succeed only if the requested amount to withdraw is less than 1 ether and a withdrawal has not occurred in the last week.The vulnerability is in line 17, where the contract sends the user their requested amount of ether.Consider an attacker who has created the contract in Attack.sol:How might the exploit occur?First, the attacker would create the malicious contract (let’s say at the address 0x0… 123) with the EtherStore’s contract address as the sole constructor parameter.This would initialize and point the public variable etherStore to the contract to be attacked.The attacker would then call the attackEtherStore function, with some amount of ether greater than or equal to 1 — let’s assume 1 ether for the time being.In this example, we will also assume a number of other users have deposited ether into this contract, such that its current balance is 10 ether. The following will then occur:The final result is that the attacker has withdrawn all but 1 ether from the EtherStore contract in a single transaction.There are a number of common techniques that help avoid potential reentrancy vulnerabilities in smart contracts.The first is to (whenever possible) use the built-in transfer function when sending ether to external contracts. The transfer function only sends 2300 gas with the external call, which is not enough for the destination address/contract to call another contract (i.e., reenter the sending contract).The second technique is to ensure that all logic that changes state variables happens before ether is sent out of the contract (or any external call). In the EtherStore example, lines 18 and 19 of EtherStore.sol should be put before line 17.It is good practice for any code that performs external calls to unknown addresses to be the last operation in a localized function or piece of code execution. This is known as the checks-effects-interactions pattern.A third technique is to introduce a mutex — that is, to add a state variable that locks the contract during code execution, preventing reentrant calls.Applying all of these techniques (using all three is unnecessary, but we do it for demonstrative purposes) to EtherStore.sol, gives the reentrancy-free contract:The DAO (Decentralized Autonomous Organization) attack was one of the major hacks that occurred in the early development of Ethereum.At the time, the contract held over $150 million. Reentrancy played a major role in the attack, which ultimately led to the hard fork that created Ethereum Classic (ETC). For a good analysis of the DAO exploit, check this blog.More information on Ethereum’s fork history, the DAO hack timeline, and the birth of ETC in a hard fork can be found in (ethereum_standards).",,2,0,1,0,1225,830,1,1,0,7,144
,,https://medium.com/u/5742b4fcf89e?source=post_page-----c35fc29ccf6--------------------------------,Elye,51000,,,,,,0,,,,,,,0,,
,,https://medium.com/u/d814f16b5155?source=post_page-----c35fc29ccf6--------------------------------,Francesc Vilarino Guell,206,,,,,,0,,,,,,,0,,
Kotlin Multiplatform Mobile and how to share ViewModel: An architecture proposal,ProAndroidDev,https://proandroiddev.com/kotlin-multiplatform-mobile-and-how-share-viewmodel-an-architecture-proposal-b6f86b61abf9,Federico Torres,84,6,864,"We will review a framework for building KMM apps with the objective of not only share business logic but also presentation logic.Kotlin shared code will contain the core logic while from the client side (iOS and Android) we will have very few code, just Compose UI and SwiftUI.TL;DRIf you want to go directly to the code, here is the demo code of my proposal which has been added to the official list of KMM samplesIf you are reading this article, I will assume you already have a notion of what KMM is, in which case you can move on to the next section. Otherwise let’s make a quick review.From its official web:Kotlin Multiplatform Mobile (KMM) is an SDK designed to simplify the development of cross-platform mobile applications. You can share common code between iOS and Android apps and write platform-specific code only where it’s necessary. For example, to implement a native UI or when working with platform-specific APIs.Kotlin shared code is compiled to JVM bytecode on Android and to native binaries on iOS, so on Android we just add a Gradle dependency and on iOS we just link a framework.Arch is a small and lightweight Kotlin multiplatform library that helps us to architecture mobile applications, it is based on Spotify’s Mobius library but instead of relying on RxJava, it relies on coroutines, SharedFlow and StateFlowWe will review the core concepts of the library, I will omit some topics like Events and error handling just for simplicity.This diagram shows how the components in this architecture interact between them, let’s review each one of them and give a brief explanation:Let’s dive into real code, we will write a very simple movies app with this proposed architecture using the movie db apiExcept for the last one, all these steps are implemented inside our shared kotlin module.For this app we just need to save two things in our state:we just need three actions:The first two actions will be dispatched by user events (like tapping or opening the app), the last one will be dispatched by the Processor when we get the response from the API.We have a single SideEffect: get the list of movies. We could set on which dispatcher or coroutine scope we want to run our SideEffect, by default it will use the viewModelScope on Android and a custom scope on iOSHere we decide how we want to modify the state based on the received action, we have to create a class implementing the Updater interface, which has only one method that returns a new state and (maybe) new SideEffects wrapped inside a Next object.Notice that fetchMovies() method is returning a NextResult object which contains a SideEffect, particularly the LoadMovies one. So the Processor will get notified of this new dispatched SideEffect and will execute the HTTP request operationNow we have to implement the Processor interface which has a unique suspend function that returns an action.TMDApi class is in charge of making the HTTP request for getting the movies and converting the json response into an array of movies, we don’t care how it is implemented but if you are curious you can always check the full demo code repository, under the hood it is using Ktor.After we get the list of movies, we just dispatch the action to save them to the state.Almost there, now we have to wire up all these classes that we have implemented via the ViewModel which must inherit from ArchViewModelYes! that is all the code our ViewModel needs.Note that we have set an initial state and an initial effect.Final step, just observe state changes! but how?ArchViewModel exposes a Flow which emits all state changes, so we just have to collect it.On Android collecting a Flow is straightforward but on iOS it is not that simple, that is why the Arch library implements the following FlowWrapperIf you have a better solution to this problem of collecting Kotlin Flows from Swift code, I’d love to read it.Let’s see the final code:And that’s all! remember that the full code is hereFrom an Android developer perspective, the development experience using KMM (and this approach) is pretty much the same as just developing pure Android. But for iOS developers the experience is not as good as for Android developers, on iOS we have to take care of stuff like concurrency, InmutabilityException and other incompatibility issues that may arise.Jetbrains is aware of these problems and is working on improving the experience for iOS developers, on August 31, 2021 they released the preview version of the new memory manager that should free us from the need of freezing objects.This architecture is a great fit for teams who maintains big native apps for iOS and Android, specially if the developers on your team have knowledge on both platforms.Usually iOS and Android versions of the same app have different architectures like MVC, MVP, MVVM, VIPER or Redux, keeping a same architecture on both platforms has a lot of advantages, requires less work, and helps to better distribute the tasks among your team, developers can even work on both platforms very easily.Overall I think there is no perfect solution, each team should consider which is best suited to their needs.",,3,0,12,41,763,414,4,7,0,9,311
Speed Up Mandelbrot Drawing on Android Jetpack Compose,Mobile App Development Publication,https://medium.com/mobile-app-development-publication/speed-up-mandelbrot-drawing-on-android-jetpack-compose-59e90b7f352b,Elye,51000,7,731,"If you have studied computer science parallel processing before, coding and experiment drawing Mandelbrot is an interesting assignment to explore how we can use parallel processing to speed up the drawing.With that, I explore on Android Jetpack Compose and see how the speed is. Below is some experiment result I got and how I go about speeding it up.It improves a 900-seconds drawing to 4 seconds, and then further improves to instant incremental update.Before we get into those experiments, below is an over-simplistic description from myself. It has a computation on every single pixel to be drawn, that has similar yet slightly different computation, that can be parallelized to speed up.It has a mathematical computation that I’m not going to code myself, so I refer to this Kotlin code here and make it work on Android.The drawing result as belowThe more iteration and zoom, the more computation it has to run to produce each Pixel.You can get the Android Jetpack Compose code that generates it here.Let’s get into the various approach I draw Mandelbrot on Android Jetpack Compose.The first natural thought is, we can use the Jetpack Compose Canvas.Using the Jetpack Compose’s Canvas, although straightforward, performance-wise is really slow. The result is in many minutes 🙀 !Given the slowness of Jetpack Compose Canvas, I revert to using SurfaceView, per previously I have explored the speed of SurfaceView is much faster.medium.comInstead of using SurfaceView and the Conventional XML View Layout, I wrap them using Jetpack Compose provided Android View.The result seems so much faster and as promising as expected.Some AnalysisThe ratio result of two different iterations looks much higher (compare to the Jetpack Compose Canvas approach). This meansAs we have seen in the SurfaceView result analysis, we know the Jetpack Compose Canvas direct drawing is slow.Perhaps we can explore the first draw on a manually created Bitmap and then send it onto the Jetpack Compose Canvas.Using this approach, the result looks great. It is as fast as the SurfaceView approach!Although the Bitmap approach seems promising, it is still relatively slow with a 9 seconds result on 1000 iterations and 1000 zoom factor result.Given that Mandelbrot pixel computation is relatively independent, perhaps we can use multiple processing to speed it up, as shared in the article below.medium.comIn this approach, we split up into several cores (settable), and each core is responsible for each section of color computation i.e.computationProducer. When each computation is done, then we send it for the point drawing i.e. drawComsumer.By altering the Fanout for Computing the Color, for Iteration 1000 and Zoom 1000, I can see some improvements from 1 core to 2, from about 9 seconds down to 5 seconds, and gradually stabilizing at a little below 4 seconds.A more than 50% improvement in performance. Not a bad result.Nonetheless, from a user point of view, waiting for 4 seconds before the UI show up is pretty long.Given the time to show drawing is still considered slow, I use a little technique to instead of showing all the drawings immediately, I incrementally update it.In Jetpack Compose, when we use a state variable, any update on it will perform a recomposition.Hence I make the bitmap a state variableI still use channel computation above, but upon updated of each section, I update the image bitmap accordinglyNote: I use LaunchEffect to make the computation separate from the Jetpack Compose function. Check out this article about them more.As I really want to have super quick feedback, hence I made a 100 fanout design. Below is the result.Although it still takes a few seconds to complete the drawing, at least it renders incrementally, making the viewer knows things are happening.To recap, below are a few performance improvement techniques.Hope this helps. You can get the Android Jetpack Compose code that generates it here.*UpdateAs per the Use Manual Bitmap First approach, there’s another tiny improvement we can gain.The example above is using bitmap and send into a Canvas to drawInstead of doing that, we can use Bitmap directly and just setPixel, skipping the need for canvas.This is possible for our experiment, as eventually, we’ll be sending the Bitmap to Compose’s Canvas.Such minor change, as some further improvement.You have added this code to the Android Jetpack Compose code sample too and changed the incremental approach using this method instead to gain the best performance.Thanks to Romain Guy for the extra bit of info that helps the improvement further.",,1,9,32,32,635,420,13,2,0,15,23
,,https://medium.com/u/2f9c93482971?source=post_page-----c35fc29ccf6--------------------------------,Abhishek Chauhan,338,,,,,,0,,,,,,,0,,
Without Permit: Multichain’s exploit explained,ZenGo,https://medium.com/zengo/without-permit-multichains-exploit-explained-8417e8c1639b,Tal Be'ery,729,6,873,"A few days ago Multichain’s users were hacked by several attackers groups, all abusing the same vulnerability in Multichain (previously AnySwap) smart contract. The attackers were able to steal tokens valued at millions of US Dollars.www.vice.comWhile we have been monitoring the (very interesting!) details of this hack closely, in this piece we would like to focus on the technical aspects of this vulnerability.To analyze this vulnerability we will primarily use tenderly’s debugger, a tool I was just recently made aware of and seems like a great addition to Ethereum’s researchers’ toolbox.Tenderly debugger allows its users to replay the execution of a smart contract transaction, including all of the parameters, internal function calls and the state of the blockchain at that time, stepping into each line of the open source smart contract’s code.Let’s jump right in and analyze one of the highest sum exploit transactions, responsible for the theft of 308 ETH ( ~$950K).Let’s provide this Transaction hash to the Tenderly Debugger and see what it can tell us about it.The debugger leads us to the function in Multichain code that the attacker probably abused. This is anySwapOutUnderlyingWithPermit() function in the anyswapRouterv4.sol contract.But in order to understand the abuse, we need to understand its normal functionality first.Multichain (previously AnySwap, hence the name) router mission, according to website:Multichain Router allows users to swap between any two chains freely. It reduces fees and makes it easier to move between chains.To do so, the router wraps the actual token with its “anyToken”. For example, the DAI token is wrapped as anyDAI, or conversely DAI is the underlying asset of anyDAI. The wrapped token is used for Multichain internal accounting and when user “transfers” DAI from Ethereum to BSC, actually anyDAI is added on Multichain anyDAI BSC contract and burned (subtracted)on anyDAI Ethereum contract.The aforementioned exploited function anySwapOutUnderlyingWithPermit, is swapping an underlying token using the ERC20 permit() function. The permit() function allows its user to supply a signed transaction of approving a contract to spend its funds without actually sending it to the blockchain, which can help in minimizing users’ gas cost . The signed transaction is expressed in (v,r,s) terms.After this introduction, we can now understand the anySwapOutUnderlyingWithPermit() functionality:The rest of the function deals with its wrapped version accounting and sending across chains.It’s worth noting that according to a query we created on Dune Analytics this vulnerable function was never in actual use, and its first use was by the exploit on January 18th. This means this function was actually a deadwood that just increased the contract’s attack surface.Now let’s take a look at the parameters the attacker passed to the vulnerable functionfrom is the address of the victim, the token is the attackers’ deployed contract and so is the to destination address.We can see that the attacker is not passing a valid signature as v,r,s are all zeros.The attacker is trying to get 308 ETH to its contract with an invalid signature. How can it work?2. IERC20(_underlying).permit(from, address(this), amount, deadline, v, r, s); Originally, the expected result was that the underlying token’s (“WETH”) ERC20 contract permit() is called to approve the router’s (this) ability to withdraw an amount from the user’s (from) address, as the user supplied a signed transaction for that denoted by (v,r,s). However, WETH contract does not have a permit() function! WETH contract does have a “fallback function” that is called when a function is called but not found. WETH’s fallback function is deposit() that does nothing material in this case, but allows its calling function’s execution to continue as it does not fail.3. TransferHelper.safeTransferFrom(_underlying, from, token, amount); Originally, we expected that if we got to this line it means the signature in the line above was verified and now we can use the approve granted by it to actually move the the amount from the user to the router. However, the signature was not verified, as seen above. In theory, it should not be a problem, as although the attacker’s input should not have passed the signature validation, it did not approve the router access to transfer the funds on the victim’s behalf. However, Multichain’s dapp requested from all of its users a practically infinite approval sum. This insecure methodology is quite common in dapps, to save user expenses on gas. We had warned in the past that such behavior (we named it baDAPProve) can be hazardous in case of a rogue or a vulnerable dapp, and now this potential threat had materialized. By abusing this excessive approval, the function transfers the WETH amount from the victim account to the attackers’ controlled contract.Now that the attackers got the victim’s funds, they just need to make sure the function will not fail, and this transfer will not be reverted and that’s what the rest of the code does.As often happens with both accidents and vulnerabilities, the vulnerability discussed here had multiple contributing factors, and probably even getting one of them right would have helped Multichain’s users in not getting exploited:Additionally Multichain did not apply any upgrade mechanism to this contract, and therefore the users’ only line of defense is to individually revoke their previous approvals.Our hope is that by sharing this research and learnings, we can all enjoy a more secure Web3 environment.",,1,0,7,31,1123,509,7,3,0,18,122
Preventing Smart Contract Attacks on Ethereum — Reentrancy attack,Better Programming,https://betterprogramming.pub/preventing-smart-contract-attacks-on-ethereum-a-code-analysis-bf95519b403a,Abhishek Chauhan,338,5,643,"One of the features of Contracts also typically handle ether, and as such often send ether to various external user addresses. These operations require the contracts to submit external calls. These external calls can be hijacked by attackers, who can force the contracts to execute further code (through a fallback function), including calls back into themselves.Attacks of this kind were used in the infamous DAO hack.This type of attack can occur when a contract sends ether to an unknown address. An attacker can carefully construct a contract at an external address that contains malicious code in the fallback function.Thus, when a contract sends ether to this address, it will invoke the malicious code. Typically the malicious code executes a function on the vulnerable contract, performing operations not expected by the developer.The term “reentrancy” comes from the fact that the external malicious contract calls a function on the vulnerable contract and the path of code execution “reenters” it.To clarify this, consider the simple vulnerable contract in EtherStore.sol, which acts as an Ethereum vault that allows depositors to withdraw only 1 ether per week:This contract has two public functions, depositFunds and withdrawFunds.The depositFunds function simply increments the sender’s balance.The withdrawFunds function allows the sender to specify the amount of wei to withdraw.This function is intended to succeed only if the requested amount to withdraw is less than 1 ether and a withdrawal has not occurred in the last week.The vulnerability is in line 17, where the contract sends the user their requested amount of ether.Consider an attacker who has created the contract in Attack.sol:How might the exploit occur?First, the attacker would create the malicious contract (let’s say at the address 0x0… 123) with the EtherStore’s contract address as the sole constructor parameter.This would initialize and point the public variable etherStore to the contract to be attacked.The attacker would then call the attackEtherStore function, with some amount of ether greater than or equal to 1 — let’s assume 1 ether for the time being.In this example, we will also assume a number of other users have deposited ether into this contract, such that its current balance is 10 ether. The following will then occur:The final result is that the attacker has withdrawn all but 1 ether from the EtherStore contract in a single transaction.There are a number of common techniques that help avoid potential reentrancy vulnerabilities in smart contracts.The first is to (whenever possible) use the built-in transfer function when sending ether to external contracts. The transfer function only sends 2300 gas with the external call, which is not enough for the destination address/contract to call another contract (i.e., reenter the sending contract).The second technique is to ensure that all logic that changes state variables happens before ether is sent out of the contract (or any external call). In the EtherStore example, lines 18 and 19 of EtherStore.sol should be put before line 17.It is good practice for any code that performs external calls to unknown addresses to be the last operation in a localized function or piece of code execution. This is known as the checks-effects-interactions pattern.A third technique is to introduce a mutex — that is, to add a state variable that locks the contract during code execution, preventing reentrant calls.Applying all of these techniques (using all three is unnecessary, but we do it for demonstrative purposes) to EtherStore.sol, gives the reentrancy-free contract:The DAO (Decentralized Autonomous Organization) attack was one of the major hacks that occurred in the early development of Ethereum.At the time, the contract held over $150 million. Reentrancy played a major role in the attack, which ultimately led to the hard fork that created Ethereum Classic (ETC). For a good analysis of the DAO exploit, check this blog.More information on Ethereum’s fork history, the DAO hack timeline, and the birth of ETC in a hard fork can be found in (ethereum_standards).",,2,0,1,0,1225,830,1,1,0,7,144
,,https://medium.com/u/e239df745beb?source=post_page-----c35fc29ccf6--------------------------------,Kalju Jake Nekvasil,11,,,,,,0,,,,,,,0,,
,,https://medium.com/u/aca00f3d57dc?source=post_page-----c35fc29ccf6--------------------------------,Tal Be'ery,729,,,,,,0,,,,,,,0,,
,,https://medium.com/u/4bb73ddafed4?source=post_page-----c35fc29ccf6--------------------------------,FreddyCoen,55,,,,,,0,,,,,,,0,,
,,https://medium.com/u/8e6bcf52156c?source=post_page-----c35fc29ccf6--------------------------------,Sicong Zhao,382,,,,,,0,,,,,,,0,,
,,https://medium.com/u/2355dcc4d3de?source=post_page-----c35fc29ccf6--------------------------------,Juan,214,,,,,,0,,,,,,,0,,
,,https://medium.com/u/589c510eb216?source=post_page-----c35fc29ccf6--------------------------------,Patrick Collins,5000,,,,,,0,,,,,,,0,,
Analyzing Pepsi NFT Smart Contract,Better Programming,https://betterprogramming.pub/nft-beginner-tutorial-pepsi-nft-smart-contract-explained-962721b7361a,Sicong Zhao,382,8,1328,"Pepsi dropped its genesis NFT collection on Dec. 2021 (source), till today the trading volume for this project is 2.3k ETH ($5.6 million). I took a close look into the Pepsi NFT contract, to learn how they implemented this project. In this tutorial, I will explain the smart contract line-by-line.If you are new to NFT and want to learn how to develop an NFT smart contract by yourself, this is the right article for you. If you are curious about how the technology behind NFT works, this article would also help.If you understand ERC-721 and have coding experience, you are good to skip this section.Like a lot of people, I am new to this industry. I tried to double-check the correctness of the content, but mistakes might occur. Please let me know if you spot any, I would truly appreciate your help and updating this tutorial.Many projects would open-source their contract, and people can get educated before making any (investment) decision. Transparency is also one of my favorite features in the Blockchain industry.Pepsi unveiled its NFT contract address on the official website. You can simply copy that address and search it in Etherscan, the go-to place to check information on the Ethereum blockchain. Then you will see the following page:By clicking the Contract button, you will see 14 files. However, we only need the first file, PepsiMicDrop.sol. This file contains the Pepsi NFT contract. The rest are ERC-721 contracts by Open Zeppelin, which we will not cover.For your convenience, I put the code on GitHub, and you can find it here. I recommend you to open the code side by side with this article.Before diving into the code, let’s first understand how smart contracts define NFTs. Under the hood, an NFT has two pieces of information: (1) Id (2) URI.Under the ERC-721 standard, each NFT is unique. How? The contract assigns a unique Id to each NFT, thus enabling the uniqueness. In an ERC-721 based contract, you will not find two different NFTs with the same Id.Then, where is the image (it can also be video, document, etc)? That is where URI comes into play. URI stands for Universal Resource Identifier. Think of it as an URL, through which you can get all metadata associated with the NFT. The metadata can be image URL, description, attributes, etc. The current most popular NFT marketplace OpenSea has its metadata standard. You can find a comprehensive list of metadata.Now, you might ask: Would URI and Id ensure the uniqueness of NFT?Ideally, ERC-721 based NFT will have its unique image or metadata, which is why they are non-fungible. However, you can assign the same URI to a different Id, thus creating two NFTs that look the same from the surface. However, that is not the purpose of ERC-721. There is another standard, ERC-1155, which supports semi-fungible NFT. You can learn more here.Besides, you might wonder: Why metadata is not stored in the contract? Because it is super expensive to store data on the blockchain, especially images or videos. However, you can still store metadata on-chain and pay the high gas fee. Some projects use SVG format images, which drastically reduce the data size, thus reducing gas fees. However, this is beyond the scope of this tutorial. Let me know if you want to learn more about SVG-based NFT.The contract might look complicated at first glance, but it is well organized. Fig.2 visualizes the structure of this contract. Let’s go through each component.Let’s look into the PepsiMicDrop contract. I will explain the contract by its functionality, each functionality is related to some functions and state variables.The constructor takes 2 inputs:The assignment happens in the ERC721 contract code, which the contract imports at line 51. That is why you cannot find anything related to this constructor body. It is the beauty of inheritance, and we do not need to redo it.Inside the constructor, we can see 2 state variables get new values, reserveMicDropsId, and micDropsId. Why they are needed? In this NFT drop, Pepsi kept the first 50 NFTs to themselves, so the NFTs that are publicly available start from id 51.Wait, isn’t NFT an image? Why the here they using numbers to represent NFTs? If you have these questions, I was with you. To understand that, we need to look into what is an NFT.Now let’s talk about the mint.Part 1 — Merkle ProofThere are two inputs of the mint function, proof, and leaf. They are used for Merkle Proof from lines 3 to 7. Essentially, it checks if a user is qualified to mint the NFT. There is a concept called whitelist. Sometimes you have to get into this whitelist, to be able to mint NFTs later. I will not explain this part, because someone did a great job. Please check this article, which gives a comprehensive explanation of Merkle Proof.Part 2 — PrerequisitesThe next part, lines 8 to 11, defines the 4 prerequisites of mint. Let’s go through each of them.(1) Before the NFT started to sell, the state variable saleStarted is set to false. So it will not pass the check-in line 8. When the sale begins, the contract owner can call the function startSale() to change the value of saleStarted.(2) This check makes sure the user address is not 0x0. 0x0 is the Ethereum genesis address, and no user will use it. I am also not sure why this check is necessary. Let me know if you understand.(3) In Pepsi Drop, each address can only mint one NFT. alreadyMinted is a state variable in mapping type, like a dictionary, that keeps track of all addresses that minted the NFT.(4) There is a limited supply, up to 1983 NFTs. This checks if all NFTs have been claimed. micDropsId is the unique token id we discussed above.Easy, right? And we are almost done, hanging there.Part 3 — The actual mint.This is where mint actually happens. The good news is we do not need to implement it because ERC-721 already did this.To mint, we call the function _safemint() with the user’s wallet address and the unique token id, and that’s it. Under the hood, there is a state variable (like a dictionary) that keeps track of the ownership of each token. By calling _safemint() function, we update this state variable, assign or change the ownership of the corresponding token.One note here, the state variable micDropId represents token id, and it increments by 1 right after each mint.Part 4 — Update Minted AddressAs mentioned in Part 2, each address can only mint one NFT.This line ensures whoever successfully minted an NFT gets recorded.Part 5 — ReturnAfter minting, the token id is returned to the frontend.The last part we need to discuss is the function that updates URI.As you already know, each NFT has its URI. And this is the function used to change the base URI.The base URI is the mutual part among each NFT’s URI. By default, the URI is baseURI/tokenId. The reason why we set baseURI is that it saves gas fees. Imagine if you set URI for each NFT that is extremely expensive.You can change how to combine baseURI and tokenId, by overriding the function tokenURI defined in the ERC-721 contract. For example, you can do something like baseURI/tokenId + ‘.json’ if that is how the URI is formatted.You might wonder, what are public and onlyOwner. These are function modifiers, which define the condition for a function to run. public means ‘inside and outside of the contract’, access to all. After the deployment, we interact with the contract through the website (more specifically, JavaScript), this is what public means. onlyOwner means only the contract owner can call this function. Of course, we only want the contract owner to have the capability to change the token URI.The uncovered parts are simply functions to get or set state variables, I am sure you do not need my help.So, that is it. Thanks for reading this article. I hope you find it helpful.",,8,7,34,0,1225,578,3,4,0,12,410
Generate random NFTs with Node.js + Sourcecode,,https://jcmacur.medium.com/generate-random-nfts-with-node-js-sourcecode-b93a2ab411fe,Juan,214,10,1437,"Randomly mix layers and create unique NFTs with both image and metadata using Node.js and JIMP.In this tutorial we will be generating random NFTs with its own metadata using different traits, using the OpenSea standards (This tutorial can be adapted also to Solana and other chains, keep in mind that you will need to build the metadata following the standards of the chain you’re using), this means that following this guide you will end up with a pack of NFTs ready to upload to the Ethereum or Polygon chain.Creating NFTs from scratch is not a standalone task, we would need an artist to draw the traits by layers. So we will need all the sets of eyes, mouths, noses, and every trait we want to include in our NFTs.Important note: Every layer/image should have the same dimensions; our script will focus on overlaying layers with transparent backgrounds, which is far easier than positioning them one by one. All traits PNG files will have the same dimensions as the background trait.See below an example of an NFT that what we will be building, with the right order of layers, examples of wrong layers, and a quick of 27 NFTs. For the sake of this tutorial I’ve drawn a few layers of a little guy with the iPad Pro and the Procreate app (I’m a developer, not a designer, but at least we can get a real world-like project):As displayed above, we will be having the following traits, in this precise order, which you can download right here from Github. Each trait is exported singularly :Now that we have the traits to use in our NFT generator, we can proceed to code it. This tutorial takes in mind that you already have Node.js and NPM installed, and you know the basics of creating an NPM project. You can follow along the source code on this Github repository where there’s the final code of this tutorial and the traits used to generate a good amount of NFTs.Let’s create a new folder, run npm init (you can enter blank on all fields), and copy the Traits folder (Which is on Github) inside. Now we will install the libraries we are going to use: fs (File system), Jimp (An image processing library for Node.js), dotenv (To safely save our pinata credentials), and Pinata SDK (To upload our images to IPFS).Now we will create our index.js file, the entry point to our script. We will write a while loop to go from 1 to our NFT supply number. Let’s do 100 on this tutorial, but you can put the amount you want. Just remember to add enough variation of traits to avoid having duplicates (Or add a middleware on this script to prevent them from creating NFTs with duplicated metadata). Inside index.js write the following:We will handle the trait generation in another file called generator.js, to keep the starting point from the logic separated. This generator will use a traits file to randomly get a trait variation with a common, uncommon, rare and legendary rarity.Let’s first create a traits.js file in the same folder with multiple methods to generate eyes, mouth, head, etc. We will use the Math.random function and return a String.Starting with the background trait, I chose Lava and Forest backgrounds to be Common, Mountain and Aqua to be Uncommon, Psycho and Snow to be Rare and Galaxy to be Legendary.With the name formatting, we are using, each trait filename is called CATEGORY_NAME. So for example, the Aqua background trait is called Background_Aqua. Always try to keep a formatting, not only for better management but also to reuse the naming to avoid dozens (or hundreds) of lines of code.I also set the constants for the ranges of rarity the following way:Now we can add the rest of the traits. Check the Github repository to get the full code: Link here.You will note that for the body and head we are using the same function: getBodyAndHead. This is because we want the body and head to match, alien with an alien, zombie with a zombie, and so on.Now that we have our traits.js file ready, let’s create our generator.js file, which we will use to actually generate the images and metadata. Since we will be using IPFS to save our images to the blockchain we will need Pinata credentials.Open Pinata website: https://www.pinata.cloud/ and create an account. The process is very straightforward and the Pinata team made a good job keeping everything as simple as possible. You will have to verify your account via email, and login again.Note: The free Pinata plan has a limit of 1GB uploaded. After that, you can pay the subscription and have unlimited uploads.After you’ve logged in, you will see a menu at the top right of the screen. Select the API Keys option.Now create a new API Key with Admin privileges, set a name and save the API Key and API Secret, we will need them on Node.js.Since we want to protect our credentials we will make use of the dotenv library. Let’s create a .env file with the following content:Going back to our generator.js file, let’s start adding the basics: importing libraries and setting up Pinata with our keys. We will also need a reference to our project folder (See _path variable below):Before going on, we can update our index.js file, which will call Generator every loop time. We update it now so we can test as we develop and see the results. Replace the index.js content for the following:We can start testing the script. Run:If everything went fine so far, you should see the following output which, at this point, is doing nothing else than logging.Now we can continue, but before let’s do a quick explanation about what is exactly the metadata, and what we need to build it the right way.On Ethereum, NFT smart contracts use an URL that synchronizes each token. So the token with ID: 1 will get its metadata from your baseUrl (usually your uploaded IPFS folder) + ID.This means that if your metadata folder is uploaded in ‘www.myhost.com/nfts/’, the NFT with ID 1 will be ‘www.myhost.com/nfts/1’.Each metadata file is a JSON formatted file without extension, named from 1 to our Max Supply. You can read the full specifications on the OpenSea documentation site, we will use the following structure:Now we can focus entirely on our generator.js file. Let’s explain the steps to follow:Replace our build method for the following code and run it. Since we have our loop to go thru 100 NFTs, this can take several minutes. You can stop it after it generated a few ones, just to make sure it is working. By this point we are only generating an image with the background:If you run it with node index.js and stop it, you should see the following:Now we can add the rest of the traits, starting with head and body. To do this, we will add also the clothing, because its layer should be between the head trait and the body trait. Let’s continue with adding below var _composedImage = backgroundJimp; the following code:If we run again our script, we will see 2 random bodies and their respective heads!We can proceed now adding all the rest of the traits, respecting the order, below our last _composedImage.blit(headJimp, 0, 0); :And it’s done! Let’s run node index.js again to see your new randomly created NFTs:We are not done yet. Once we have all NFTs created, we -or the designated developer- will have to link an IPFS folder to the smart contract. So we need to upload that folder and get that URL.How do we do it? We move all our files without extension to a separate folder (The images folder is not necessary, I usually create it just to see the results live, since IPFS can take some time to refresh, and it may take most of the space of your Pinata space). Once we have that folder with all the metadata files inside, let’s open https://app.pinata.cloud/pinmanager and go to Upload, Folder. Select our folder, put a name on it, and upload it:Once is uploaded, you will be able to see the IPFS hash:Your metadata folder URL would be https://ipfs.io/ipfs/Your CID/. This way, when an NFT is linked, for example, to the index 5, the url will be https://ipfs.io/ipfs/Your CID/5, and you can open the URL (replacing it with the CID of your uploaded folder) to test it yourself.That’s all for this tutorial, you can check the source code on Github: https://github.com/jcmacur/medium-tutorial-nft-generation. I will be uploading more tutorials regarding NFTs, Defi and Blockchain, so subscribe and stay tuned!",,2,12,23,2,1020,533,11,1,0,15,123
Top 10 Smart Contract Developer Tools You Need for 2022,Better Programming,https://betterprogramming.pub/top-10-smart-contract-developer-tools-you-need-for-2022-b763f5df689a,Patrick Collins,5000,24,5222,"Last year, I looked at the top smart contract developer frameworks, and this year, we are expanding that to look at more, way more.We will look at ten categories of tools and then look at the top ones in each category for smart contract/blockchain/solidity/vyper/rust/web3 developers (we have too many terms for “blockchain developer” now, don’t we).I’ve worked with every tool on this list in some capacity, so you can be assured I’m not copy-pasting some list. For those of you who don’t know me, I live and breathe smart contracts and work along some of the best in the industry as a developer advocate on the Chainlink project, CEO of my own blockchain infrastructure company, and lover of web3. These will give you exactly what some of the best developers in the world use. This list, however, is not a list of everything out there, so if I missed your tool, sorry! It’s just a list of what I think every developer needs to be aware of. So, let’s jump into it!Additionally, we made a video on this topic, if you want to check it out!The language is the most fundamental part of any smart contract developer, and is how to write smart contracts! Pick one of these languages and go.If you’re new here, you should start with solidity. If you’ve been making smart contracts, you probably know solidity. Solidity is the most dominant, most used smart contract development language, and it’s no wonder why. Solidity works on most smart contract platforms, like Ethereum, Avalanche, Moonbeam, Polygon, BSC, and more. This is due to most blockchains adopting the Ethereum Virtual Machine (EVM), which solidity was built for. Out of the top 10 Defi projects at the moment, nine of the ten use solidity as their primary programming language.If I had to guess, I’d imagine that 90% of the world’s smart contract value is done with solidity. This language is actively maintained and has massive support.You could also qualify yul/assembly as another language, but it’s used with solidity, so I’m not counting it as its own.Vyper is another EVM compatible language, and it’s used primarily by 1 out of the top 10 Defi projects (and some other projects use it as a secondary), namely Curve.fi. Vyper is a python based language that I’ve LOVED working with. It doesn’t have nearly as much use as its EVM sibling solidity; however, if you like python, this is a language that you might want to try over solidity.Rust is the new kid on the block, and you’re not going to be able to use it with our EVM blockchains like Ethereum, Polygon, Binance Smart Chain, and such. However, with Rust, you’ll be able to deploy to chains like Solana, Terra and build blockchains with Polkadot. Rust is a performant general-purpose language used even outside smart contracts; hence other chains are choosing to pick it up. The logic is, “oh, well, some developers already know rust, so we might as well let people use rust everywhere.”Rust is a solid choice if you want to tap into these up-and-coming blockchains. If I had to guess, I’d say Rust encompasses about 5% of the world’s value locked in smart contracts at the moment… but that’s a total guess.Must-Know: SolidityPython Lovers: VyperTerra, Solana, Polkadot, etc: RustThis is where we talk about tools to use in your smart contracts. You don’t have to pick just one here. You could use all of these if you like! I wasn’t sure how to categorize these, but I thought “essentials” were good because… well, they are essential.Chainlink is what’s known as a blockchain oracle, which means it’s a device that enables your smart contracts to become hybrid smart contracts, which means they include some off-chain component and connect with the real world. Smart contracts are great, but they have the massive issue of not being connected to the world in any way. Now to use some off-chain component (like data or external computation) to keep our smart contracts decentralized, we must also have our external data and computation decentralized, and that’s exactly what the Chainlink network and tooling are for.The vast majority of top smart contract platforms are hybrid smart contracts and use these oracles as critical components of their infrastructure. Just look at the top 10 projects from DeFipulse and see which ones are hybrid.We can see that at least 50% use oracles as essential pieces of infrastructure, and another 30% (totaling 80%) use oracles in some way. You can get asset pricing information, random numbers, event-driven execution, and so much more.Chainlink is an essential feature for any smart contract developer coming into this space, no matter the language or blockchain!Additionally, in 2020 and 2021, we saw countless hacks of flash loan attacks, oracle manipulation attacks, and the likes that accounted for nearly billions of dollars lost in the Defi space. What kills me the most is that more than half of these were preventable by using a proper oracle like Chainlink. Please, please, please, don’t let you or your friends use centralized or insecure oracles in 2022.Please use a secure oracle solution like Chainlink. We cannot allow 2022 to be another year where we go “whoops, use a centralized oracle and lost my users $100M”. So please get familiar with Chainlink to make amazing, robust applications and protect yourself against avoidable exploits.Openzeppelin has cemented itself as the “standard library for solidity.” Anytime any developer (myself included) is looking for a quick way to deploy an NFT/ERC721, ERC20, upgradable contracts, DAOs/governance, or really anything that many developers do in this space, nine times out of 10, you’re going to want to reach for an Openzeppelin package.Openzeppelin is a suite of smart contracts already written, so you don’t have to reinvent the wheel. I’ve saved countless hours using their already-audited extendable smart contracts in my projects. I cannot stress how much a staple Openzeppelin is. I think in about 80% of my tutorials, I use Openzeppelin in some capacity.Must-Know: Chainlink & OpenzeppelinYou can (and probably should) use both of these together. I cannot emphasize enough the knowing of these technologies.Last year, my review was exclusively for this category, so this year, I’ll make sure to give you the diff of where I think these frameworks stand today. Smart Contract frameworks are an essential part of any developer’s journey. They allow you to test and deploy your smart contracts effectively. Having at least one is a crucial step to success.To get you started, take a look at the chart I compiled showing the top Defi projects according to Defipulse and the framework they use.Additionally, here is how I look at the landscape of frameworks currently:Hardhat, Brownie, DappToolsRemix, Truffle, Apeworx, FoundryWaffle, sbt-ethereum, web3j, Embark, SaddleAnchor, TerraSDKEtherlime, Openzeppelin SDK, Cobra, ParasolI won’t be covering the outdated ones; see my review last year on them.Let’s Begin.Right now, the hardhat framework is easily the most dominant smart contract development framework. Hardhat is a javascript & solidity-based development framework that does a beautiful job of quickly getting your applications up to speed. You can check out the hardhat-starter-kit to see an example of what a hardhat project looks like.With Hardhat’s testing speed, typescript support, wide adoption, incredible developer experience-focused team, it’s no wonder why it’s risen so quickly in popularity. At around this time last year, I gave this framework the top spot, and it’s still there today. It uses ethersjs on the backend, its own local blockchain for testing, and the team is currently in the midst of building a new cutting edge development platform integrated into Hardhat that I’m BEYOND excited to try for 2022.If you know me, I’m not the biggest fan of javascript due to all its oddities, so often, I prefer to use Hardhat with typescript. Hardhat is easily my second most used framework.I highly recommend this framework if you like javascript or you want to use the most popular framework with the most support.I’m in love with the brownie framework, and if you’ve read any of my other material, you know this.Brownie is an open-sourced python-based framework built on top of web3.py used by protocols like Curve.fi, yearn.finance, and Badger. It supports both solidity and vyper, but the main draw to this framework is python-based. Brownie was created because so many python engineers despised working with javascript and wanted a pythonic framework. Additionally, most of the traditional fintech world uses python instead of javascript, so moving from fintech to Defi has been made easier with the creation of Brownie.Brownie is my go-to framework when creating a new project, and I’ve self-appointed myself as a brownie developer advocate. You can check out the brownie-starter-kit (also known as “mixes” in the brownie world) to begin your journey working with the framework. Be sure to check out the Brownie Mixes repo for a list of other starter kits.I highly recommend this framework if you like python, want to use a more straightforward framework or use my favorite one.Although Dapptools doesn’t have a fancy logo, it’s a compelling application built with Haskell. Don’t worry, though; you don’t need to know Haskell to use it!.Used primarily by the MakerDAO team (the group behind DAI), Dapptools was inspired by the philosophy of Unix, “Write programs that do one thing, and do it well.” Dapptools is a command-line-focused tool where instead of having to work with python, javascript, or some other high-level programming language to aid your development, you can use the command line/shell tools that you’re already familiar with, like bash or zsh. This effectively cuts down on one more technology that you must be familiar with and forces you to get better at your shell scripting! It comes with a suite of tools like dapp, seth, ethsign, and hevm, each a tool specifically designed for a portion of what you’ll need to do as a smart contract developer.I’ve started working with Dapptools a lot more recently, and I love the mentality behind it. I started a Dapptools starter kit to show other people how to work with the tool. If you work with Dapptools, you’ll likely be working with makefiles and shell scripts to “productionize” your code. Dapptools believes testing should be written in solidity, and fuzzing should be built-in.My experience working with it is a little clunky, but I feel a big part of that comes from my mediocre bash scripting skills. However, I loved how it forced me to get better at shell scripts, which I think everyone should do.I highly recommend this framework if you don’t want to learn another language like javascript or python, you prefer using as few tools as possible in your setups, you like MakerDAO, or @transmissions11 has converted you.Everyone and their mothers should know how to use Remix, but not necessarily for production. That’s this review of Remix in a nutshell.Remix isn’t a framework per se but more of an IDE. Remix is a tool that I think everyone should at least start with so that it’s a common ground that everyone can meet on when trying to share ideas. Not everyone will know python, javascript, or bash scripts to connect, so using a tool like Remix is perfect since it’s a more visual tool.When you deploy a contract, you get buttons to interact with it. It’s easy to choose solidity versions, compile, run solidity tests, and the like. And if you want to use javascript, it has javascript scripts available too! It comes fully extendable, so you can improve it if you want to add a feature, and you can even connect your local disk to remix to run code from your version-controlled repo.Remix is a tool that brings everyone together, and I’m so happy we have a team working on such a fantastic tool to help everyone. Remix isn’t something that I’d recommend advanced users to use, though, as you’ll get more fine-tuned tools in a local development environment instead of always having to rely on a web IDE. I don’t recommend it for advanced setups; however, use it if you like to work with it!I highly recommend everyone be familiar with Remix (it’s straightforward to get up to speed if you know solidity).Truffle is one of the original frameworks and was originally part of, spun out from, and then merged back into Consensys. It’s a javascript-based framework that comes packed with tools like Ganache (which even frameworks like brownie use), Drizzle, and the now late truffle teams. Truffle has the most historical impact of any framework, and you can see the impact they made on the industry, with many frameworks adopting practices from Truffle. We should feel lucky that this team came along and built such an excellent baseline for frameworks. If you want to try it out, take a look at the truffle starter kit.They’ve recently updated their documentation, they are in the middle of launching version 7 of Ganache, and I’ve spoken with their team, who have informed me they have big plans, which I’m excited for.Back in 2019, everyone had heard of or was using Truffle, but in the time since then, the landscape has changed. Hardhat has been in gogogo mode to solidify themselves as the defacto javascript framework. I often run into bizarre bugs when working with Truffle; their typescript support is subtle, the tests run much slower than Hardhat. It’s currently difficult to make a case for Truffle when it feels like Hardhat does everything just a bit better.At the start of 2021, I was nervous for Truffle, and I even mentioned it at the beginning of 2021, my reserves of the framework, and it looks like 2021 was the year Hardhat took over. However, I think the future of Truffle is still bright. They have a fantastic team, they’ve figured out where Truffle will live (with Consensys now), and I’m expecting big things out of them for 2022.Truffle is a framework that you should use if you want to support the one that started it all and prefer the syntax of Truffle. For the most part, I’d recommend Hardhat over Truffle; however, I think Ganache is a potent tool still and would recommend anyone looking to work with a local testnet take a look at Ganache. It has a beautiful UI component that you can use to “see” transactions easier; it’s simple to spin up and can be installed globally.ApeWorX is the python new kid on the block that branched out from the brownie community to make a more modular finance-focused framework than its brownie ancestor.Apeworx works with all the same setup and syntax as Brownie, with several fun improvements to the quality of life.I’ve only worked with Apeworx a little, it feels very brownie-like, and I like the addition of plugins. It’s unclear if this will be the sequel to Brownie (similar to how Hardhat took over Truffle mindshare) or if the two will work alongside each other. In any case, I’m very excited to see what Doggie and his team are going to create in 2022, and definitely, a framework to keep your eye on, and even try if you want something brownie-like that you can add plugins to.Foundry is dapptools written in Rust. That’s the most straightforward summary of the tool.I’ve worked with foundry a little less than I’ve worked with Dapptools, and it feels like this might be the sequel to Dapptools, but I’m not sure. It’s still command-line focused, expects a similar file structure, and nearly all the commands from Dapptools have a foundry analogy. The Paradigm team’s Georgios and friends built this amazing reimagination of the popular Dapptools, even paying homage to Dapptools in their release article(which you LOVE to see in this space.)Many of the commands and file structure setup work the same as Dapptools, and you can barely tell the difference. This framework is fast, powerful, and focuses on building your tests in solidity, with built-in fuzzing and other powerful tools.Rust is becoming more and more one of the most popular technologies to fuel the infrastructure of blockchain applications, and this feels like it’s in line with that trend. I’m excited to see which big players pick up both foundry and Dapptools.I’ll be a little quicker in this section, as these might be frameworks you’d want to use, but I’m less familiar with them, or I didn’t have a great time trying them out myself.Waffle is still a popular framework, except it focuses more on testing than a whole suite of tools. You can use waffle with Hardhat as they are both javascript-based, and many people use that combination setup.Saddle is Compound’s reimagining of Truffle. If you like Compound, give this a try!I hadn’t used java much in the last few years, which showed when I tried out sbt-ethereum. Sbt-ethereum markets itself as:“An interactive text-based platform with which “hobbyists” — people who are not programmers” — sbt-ethereum.ioThis is another framework I reviewed last year, and sadly my scala skills haven’t gotten much better. However, this is another one you might want to try out if you want to stay in the command line world.Another framework I reviewed last year; looking at their website, it seems like they’ve pivoted their main product from a framework to an enterprise block explorer. However, they still have their web3j package, which is a java based framework. You might want to try this out if you know java and see how it feels.Embark is one that I gave a pretty glowing review of last year. I was able to test and deploy smart contracts from their framework and work with some UI tools. However, it hasn’t received a git commit in over a year, and some things have changed in the smart contract ecosystem since then, so I’m not sure what this means for the future of this framework.As of right now, I’d say the Anchor framework is the biggest non-EVM framework out there for working with Solana. If you want to work with Solana, I highly recommend trying out Anchor, as it feels like it’s the best solution at the moment.Terra is another rust-based smart contract blockchain building SDKs/frameworks for developers. They now have Terra.js and a python SDK to start working with them. This is another framework I haven’t spent much time with, but one I plan on working with some more this year.If you’re deploying smart contracts in some capacity, you need a wallet to store funds or at least testnet funds. Many will use a combination of the below wallets for various purposes.Metamask is a tool that just about everyone in this space knows about or should know about. The concept is simple and great for testing things in the browser, which all front-end developers need to do. Just about everyone should have a Metamask or some Metamask-like browser wallet (like maybe Phantom for Solana).Metamask is a hot wallet that is easy to use. The problem with these is that once your private key is out, you’re screwed! Additionally, they are always connected to the internet, so if someone gets access to your computer, you might be out of luck too! Luckily there are some helpful other tools for us.Gnosis safe is known as a multi-sig wallet, meaning it takes X number of signatures to send a transaction. This way, if one wallet is compromised, it doesn’t matter since that attacker would need to compromise at least half of the keyholders.This is a massive tool for smart contract developers, especially those handling a lot of money and want to make sure their assets are safe. A lot of DAOs use Gnosis safe as well for storing their treasuries. Additionally, a gnosis safe has integrations with many other DAO tooling, like Snapshot, for casting votes.Ledger and Trezor are known as “cold wallets,” or wallets that are a bit more cumbersome to make transitions with. This cumbersomeness is intentional; they make it harder for you, especially attackers, to move funds.You can use any combination of cold wallets, hot wallets, and multi-sigs in your projects, and you probably should! Use hot storage for small funds that you need to access and move around a lot/quickly, cold storage for things you don’t want to touch for a long time, and multi-sigs for significant funds you wish to protect. You can even connect your cold storage wallet as one of the keys on a multi-sig!Block explorers are crucial tools for people who want to “see” transactions and what is going on in the world. You have all the tools at your disposal to build one of these if you have a layer one connection, but 99% of the time, you’re going to want to use someone else’s.Ah Etherscan, how are you free. Etherscan is one of those tools that I want everyone to know and understand how to use. I’d love to see them be open-sourced, but I understand that wouldn’t be an excellent incentive for them to stay as impressive as they are. Etherscan comes built-in with most services, and they do a fantastic job. Etherscan is easily the most dominant block explorer for the ETH community, and they have built support for projects like Polygon, ETH 2, and Binance Smart Chain.However, I am happy they have competition, so they have to stay on their toes! Etherchain is a lightweight block explorer that has also released a wonderful ETH 2 explorer that I use more than Etherscan at the moment!Another ETH Block explorer, Ethplorer, might be another explorer you might want to check out.Most other blockchains also have multiple block explorer clients; however, the EVM/ETH community quickly has the most, hence why I chose to focus on them here.Now to send transactions, you need a blockchain to send them to! Now, if you worry about cost, all of these have free options that I highly recommend trying out, but you can also always run your own layer one node yourself! A “layer one node” refers to your blockchain connection for those unfamiliar. For example, to send a transaction on the ETH chain, you need to send the transaction to an ETH node. Even Metamask and wallets have a connection to a layer one node running in the background!Alchemy is fantastic and is my go-to for an ETH connection and all EVM connections. It’s fast, has a beautiful UI for tracking requests, has a vast array of layer 1s, and it just works. They are incredibly responsive, have great support and developer advocate team, and I’m very excited for what they have in store for 2022.Infura is another application that was easily the most dominant back in 2020 and might still be. They have expanded into more products like IPFS, have a solid following, and many tools natively work with Infura. If you’re looking for a solid layer 1, this is the one for you.I didn’t even KNOW Moralis had a layer 1 product until a few months ago, and I’ve used it a few times, and it works great! It doesn’t have the same bells and whistles Infura or Alchemy have, but I think that’s because the Moralis value add is more in their front-end services (which are amazing, and we will get to soon). But Moralis, I think, is still a stellar choice for anyone looking to use one of these tools.If one of the above doesn’t have the blockchain I want, there is a chance QuickNode will. With a massive array of nodes you can connect to and a simple setup, QuickNode is another service that might be for you.For building full-stack dapps/front-ends, you’ll likely still use all the standard tools like javascript, HTML, CSS, and maybe a framework like react, angular, or svelte. In addition to those, several wonderful tools will make your front-end building life much better.Web3js and Ethersjs are the two main javascript frameworks that the world uses, and they both perform exceptionally well on the front end. You pretty much have to use one of these tools if you’re building a website, and they do an outstanding job. Recently, trends have been moving towards Ethersjs as the more popular one; however, they will both do the job.Moralis is a tool I think everyone should get to know. It has web3js support out of the box and a MASSIVE array of tools to improve your front-end life. It’s marketed as the “firebase of crypto,” and I think it does precisely that. Maybe you want a database for making your front end faster; perhaps you want to build an Etherscan or an Opensea; maybe you need a list of NFTs an address owns. There are many things you might want to do that Moralis has built-in support for.It has everything you need with both an open-sourced toolkit and a cloud-based serving framework.I highly recommend that everyone building a web3 website in 2022 know Moralis. They have many boilerplates to get your project started.Usedapp is another open-sourced project with great plugins for working with web3. I’ve used it in some projects and have enjoyed it. CheckoutDrizzle is from the family of Truffle, Metamask, Infura, and Consensys and does a great job. It creates some simple wrappers in your javascript to work with your contracts, and it has React support built-in! Check out Dapp University’s video on using Drizzle.The Graph is a foundational tool for doing what’s called indexing. When we want to look up something on the blockchain, it may take a number of queries to do so. For example, if we wanted to see all the addresses on chain that have 1,000 DAI tokens in them. This would be painstakingly long, going through each and every address on-chain and calling the balancefunction. Instead what we could do, is build what’s called a subgraph that indexes these queries that we want, and store them in a database much smaller than the entire blockchain. This makes getting specific data about the blockchain much easier!Note: Depending on the setup of ERC20’s it might be trivial to do something like what’s described above, but this is just an example query.Additionally, I think everyone should be aware of create-eth-app and scaffold-eth. They are not “tools” exactly, but starter kits for building web3 applications / dapps, so I think everyone should know about them!Every developer should know the essential roadmap of getting help on a problem they get stuck on.When asking questions in this space, you want your question to be:You want to think of asking questions in a forum or stack overflow as “living documentation.” The better you format your question, the easier it becomes to answer. There are no bad questions, only poorly formatted questions; make yours a good one!Here is your rule of thumb: “If I run into a problem, and an answer for it doesn’t show up in the first page of google results, I should post it on stack overflow.”Using this, you’ll get the answers you want and help the whole community while you do it! Now you won’t always get responses from these sites because sometimes your question might be too hard, no one has done it before, or the right eyes haven’t seen it. Sometimes, I’ll pop a link to my questions in discords to get more eyes on it. But a rule of thumb is that Stackoverflow or Stack Exchange ETH should be one of the first places I always look when I run into an issue.Now, which should you post to? Well, to be honest, it doesn’t matter. Stack Exchange is usually a little friendlier, but StackOverflow has a better search ranking, so it’s up to you. Both these forums are for specific technical questions. Don’t use these forums for more theoretical questions or massive questions. That’s more for a Discord or Twitter.In 2022, I hope we move from Discord to forums like StackOverflow as the primary means of support and questions. Discord questions don’t get indexed, and therefore can’t be ranked. Discord is great for quick chats and bouncing ideas but shouldn’t be used as a support channel.Forums are terrific since they index and rank like StackOverflow. A great example is the Openzeppelin forum, which ranks well, and amazing questions are asked.As we all know, audits are critical when releasing code onto the blockchain since anyone can view and potentially exploit it once it’s there! So having tools to help make sure we make fewer errors in our code are essential, and some tools have popped up in the space to make our smart contracts more secure.Sadly, I haven’t worked with all of the tools for this section. The few audits I’ve done have used the Trail of Bits suite and MythX (along with scrolling through every line of code manually!). You can check out other reviews of these tools from **Remember, these tools will not always catch every bug, be sure to do your own diligence!**Slither is becoming one of the go-to's for static analysis. Built by the Trail of Bits team (one of my favorite auditors in the space), slither is easy to use and can catch many simple mistakes engineers can make. I highly recommend every try out this open-sourced bug catcher!MythX is another Consensys tool that is cloud-based for testing bugs and is said to do a deeper dive. Many users like Aave and myself love how in-depth it goes. It does have a paywall for users, but it's worth it if you have the money.Mythril is an open-sourced subset of the MythX product's tools for analysis. It’s not one that I’ve dove into much, but it looks like it’ll give you some promising simple results.Manticore is another Trail of Bits tool, but instead of static analysis, it does symbolic execution, which figures out what will trigger your smart contract to work. If you work with Slither already, you might as well try this out too!Up next again from Trail of Bits (you see why they are one of my favorites) is Echidna. This is a fuzzer for EVM contracts that once again you might as well try if you’re working with the other Trail of Bits tools!Securify is an incredible open-sourced tool in that it looks for specific vulnerabilities. This is another one that you might as well try out!Our last section doesn’t have a picture since, for monitoring, I use a lot of my own scripts usually. However, I think that Openzeppelin’s Defender and Tenderly are tools you should 100% get familiar with. They each are tools that allow you to monitor your production code and ensure they stay safe!If you haven’t yet, I highly recommend trying both of these tools out, and let me know what you think!This is one of my largest articles to date, and it contains a massive list of tools. I know there are a lot, but the more you build in this space, the more you realize how important each of these tools is for quickly building applications.2022 will be an insane year, and I hope I have given you some ideas of what to try this year for building your amazing, unique, inspiring web3 applications.Let me know what tools you think I missed in the comments section! And have a FANTASTIC new year! Let’s make it a big one!If you’re looking to start your blockchain journey this year, do check out how to become a blockchain engineer and this list of 10 incredible resources to get started.",,11,1,17,13,906,549,19,2,0,165,885
Mint an NFT using Solidity,,https://knekvasil.medium.com/mint-an-nft-using-solidity-1ea416c86092,Kalju Jake Nekvasil,11,7,1279,"You can host an NFT on sites like OpenSea or Rarible fairly quickly. Just connect your wallet, upload an NFT and boom, it’s listed. They even force the NFT buyer to pay the NFT poster’s gas! Really convenient, actually. But we’re not going to do that. We’ll pay our own gas fees, thank you very much! Minting NFTs with our bare hands builds character and we’ll have a lot more of it after this tutorial.If you wanted to set up a local HTTP server, would you whip out the mechanical keyboard, load up good ole RFC 2616, open your favorite C compiler, snort some G FUEL and get crackin’? No you wouldn’t. No sane person would. You would install Apache, let it do most of the legwork, and not waste weeks of your life. Solidity will be our Apache.Solidity is a programming language written specifically to handle Ethereum smart contracts and comes with an ease of setup and a plethora of functionality. It also means we don’t have to spend I don’t know how much more time messing with any of the official reference implementations in C++, Golang, Java, or Python.I’m assuming no one reading this is filthy rich and can afford to burn actual Ethereum every time they run their Solidity app in development so we’ll be using the Rinkeby Testnet. Rinkeby is a carbon copy of the actual Ethereum network except with what is essentially Monopoly money. Once everything is up and running, we can then choose to switch to the Ethereum Mainnet.Before getting into Solidity, we need three things.Alchemy is a middleman that tells all the miners on the Ethereum network to add your transaction (your newly minted NFT) to the blockchain and recognize that it’s legitimate.MetaMask is the wallet you’re going to be creating your NFT from.Gas is Monopoly money Ethereum.Let’s start with the Alchemy deployment key. Create an Alchemy account then create a new app. The settings must lineup:Once created, keep the key on standby. We’ll use it soon.Step 2, MetaMask. Download MetaMask on your browser and create an account. MetaMask does not enable Testnet wallets by default. This must be changed in order to create a Rinkeby wallet. Look for the circle at the top-right corner of the MetaMask browser extension interface.Now you can create a new wallet on the Rinkeby Network. Once created, navigate to the wallet and grab the private key. Find the pseudo-snowman icon located right below the account circle icon at the top-right corner to access the wallet settings. Icon click > Account Details > Export Private Key > Enter Password. This key is highly sensitive. Do not share it with anyone, even your cat.We’re almost done… with the setup!!! You should now have two super secret keys on standby, now you just need gas. There are many ways to get Ethereum on the Rinkeby network, here’s one. Do what they say. If you don’t like what they say, find another site. We’ll talk again once you have Ethereum in your wallet.An NFT is just a glorified JSON file. It’s lame. But do you know what’s not lame? Base64 encoding. Let’s talk about that! Long story short, Base64 encoding converts binary data into an ASCII format. This allows you to send data (like SVG files) though channels that wouldn’t normally support such data (like JSON files). Interesting… Let’s look at an example.The image we are going to mint into an NFT can be seen below. It’s in the form of an SVG, or Scalable Vector Graphics file. SVG files essentially store information that describes an image in ASCII formatting. This is useful for similar reasons to those mentioned above. Let’s see this usefulness in action using Base64 encoding.You can find the source code of the SVG file in the gist. Copy it, visit this site, paste the code, and encode it.That output looks pretty gnarly. Here’s where the magic happens. Open a new window and paste the output with the formatting below.Our NFT image appears!Notice, we aren’t hosting the image anywhere. There are no 3rd party sites like Imgur, Discord, etc. that we rely on in order to access this image. All of the data needed to generate our image is in that line of text in the URL. Once converted to an NFT, the only way that this image would not be accessible is if the entire Ethereum blockchain shuts down. If that happens, we may have bigger problems.The NFT image is set, let’s create the actual NFT JSON file. We will need to abide by OpenSea’s strict standards on how NFT metadata needs to be structured so it doesn’t break when listed. Many NFTs will have more complex structures but we’re just interested in the essentials.Now we’re going to redo some of the same steps. Take the completed JSON, and re-encode it.Now we have then entire JSON encoded! You will know it worked when the JSON gets returned after searching in the URL:Let’s call this line above the Base64 encoded JSON file for future reference. Test the image again to make sure it’s correct!You’ve just technically created an NFT. The only difference between what we just created and an actual NFT is that the actual NFT is on the blockchain with a unique identifier. Let’s do that next.You’re ready. You have three things with you.Let’s get started.In a new directory, install npm, Hardhat, and OpenZeppelin. Hardhat is a local Solidity runtime environment. Without Hardhat, we can’t compile. We love Hardhat. OpenZeppelin will be crucial for creating NFT contracts.Hardhat will give you many project initialization options, just go with the basic one.Once the project has been initialized, run Hardhat.If the output looks like this, you’re in good shape:We’re finally going to use those secret keys. Update module.exports in hardhat.config.js to add the Rinkeby network and include both the Alchemy and Rinkeby wallet keys.This lets Hardhat know to use the Rinkeby network, where the source wallet is, and to hit up Alchemy for blockchain communication.To make the NFT we need to create an NFT contract. Create a new Solidity file in /contracts called NFT.solNot much is going on here. When initializing the contract, we let Solidity know we are specifically creating an NFT contract (ERC721 is the Ethereum NFT standard). One of the central features of any NFT is a unique id, initialized with a 256 bit integer. Then it sets the address of the NFT’s creator (your Rinkeby wallet) and mints it. The token URI holds the NFT’s JSON along with its unique id.Now create a deploy.js file in /scripts. The purpose of this file seems self-explanatory. We’re creating a new NFT contract and deploying it to the Rinkeby network using the Solidity NFT contract initializer we just created.Everything should be in order. Run deploy.js.A successful output looks like this:The last thing to do is to check OpenSea or Rarible to make sure that the NFT minted! Because we are using a Testnet and not the Mainnet, the NFTs can be found in the Testnet sites:Go to either site and search for that provided address from the terminal output in order to find your NFT. Know that it may take a couple minutes for your NFT to show up on either site. In my experience, Rarible is faster than OpenSea.Well, well, well… If it isn’t our NFT…If you’re confident enough with a final product and have the gas, you have the knowledge to switch from the Rinkeby to Ethereum Mainnet and make these NFTs legit! Just switch to the new network on Alchemy, and in hardhat.config.js, change the network, wallet address, and the run command.The project and code this article was based off of can be found here:github.com",,3,7,6,8,1172,621,11,3,0,16,77
EVM development Starter Kit,,https://freddycoen.medium.com/evm-starter-kit-1790bcc992ef,FreddyCoen,55,18,3968,"Note: This blog post is divided into two parts. Part 1 will cover the basics of the EVM and is a pre-requisite to Part 2 in which we will demystify the bytecode of a compiled smart contract. To understand the interactions between the stack, memory and storage and write more efficient code we will go step by step through the execution of two full transactions.You most likely have heard about Web3, but in its broadest sense what is it? In simple terms, Web3 adds a decentralised trusted state layer to the internet, opening the door for a multitude of empowering innovations. Moreover this state layer does not only store data but also stores programs that can be executed trustlessly and modify the state. The most popular Web3 infrastructure is powered by the Ethereum Virtual Machine (EVM). The concept of the EVM originated from the Ethereum yellow paper and is an implementation of its operation codes. This blog post will introduce the basics of the EVM, as well as provide a detailed practical example on how the compiled bytecode gets executed at the lower level. Any feedback is greatly appreciated :)First of all, why a virtual machine? When it comes to decentralised networks like Ethereum, it is important that you can run a program independently of an individual machine’s operating system or hardware architecture. Similarly to the way the JVM allows you to run JAVA programs on any machine, the EVM allows you to run EVM bytecode on any machine. (In other words, the resulting bytecode of these programs does not have to vary depending on the specific machine’s underlying cpu architecture like arm or x86, the VM abstracts away the hardware).The EVM is a virtual machine that implements the opcodes defined in the Ethereum yellow paper. It offers access to persistent storage and a stack based processor that is limited to 1024 256-bit sized words. As opposed to languages like C or C++ where value types are stored on the stack and reference types on the heap, the EVM does not store any values on the stack but loads them from memory when needed.Another important point to note, is that the EVM is completely isolated from the operating system having no access to the file system or other processes to guarantee that every machine on the network concludes on equivalent results for a computation.Last but not least the EVM is based on accounts as opposed to Bitcoin’s UTXO model, that stores a user’s balance through the sum of all transaction receipts. This simply means that data is mapped to an account key. Two types of accounts exist, externally owned accounts (EOA’s) and contract accounts. External accounts are controlled by public-private key pairs and can initiate state transitions by sending a transaction. Contract accounts on the other hand, are accounts that store a specific program.Before going through a practical example, lets run through the vocabulary of places where the EVM can access and store information.The Ethereum Virtual Machine has five areas where it can store data — stack, storage, memory, calldata and logs which are explained in the following paragraphs.Storage in the EVM is persistent, meaning it persists between transactions. Storage is represented in the form of a merkle patricia trie and various Ethereum clients use different database implementations for it. For example Ethereum’s Python and Go client implementation use leveldb to store their tries. You might have heard about this fancy term “Merkle Patricia Trie” as part of the Uniswap airdrop distribution, but in essence it allows two things. First, it allows us to efficiently verify the integrity of data and second, it enables us to verify the inclusion of a specific piece of data with only a small subset of data that makes up the tree. These are two attractive properties when it comes to sharing data across a decentralised network.Integrity of the global state is ensured by each new block header in the form of a state root. This state root represents the root node of a merkle patricia trie made up of 160 bit account keys mapping to the corresponding account state, that includes an account balance, a code hash, a nonce and a storage root. The storage root is again a merkle patricia trie root node and stores the contract’s state variables through a mapping of 256-bit words to 256-bit words. The code hash points to the given smart contract’s source code. Note that an EOA will have both of these fields empty as part of the account state. Keep in mind that the blockchain itself only stores the blocks (including a hash of the state root in the header), while the clients store the full tries content in a database.Temporary storage during runtime of a transaction, think of it as RAM.The data field of a transaction is read-only memory.Write-only output area to emit Logs.Stores value temporarily during runtime to be used by operations. Any operation takes words from the stack of pushed words onto the stack or both. It serves as the intermediary to read, write, and manipulate data from storage, memory, calldata or the logs.Solidity is the most popular high-level EVM language used and benefits from a rich ecosystem of tools and support. In this practical example, we will write a simple program in Solidity and investigate how the compiled byte code executes. Solidity is similar to other object oriented languages in that it is based on classes called contracts which when deployed create a single instance of that class by running the constructor function. The byte code of the resulting instance (post constructor execution) is stored as the program code of the given contract account (as part of the account state, mentioned in the Storage section above).To get started, you can either download the static binaries of the solidity compiler here, or use the JS version of the solidity compiler which can be conveniently used within your javascript project, or within a browser based IDE like Remix. Either way compilation will generate the resulting EVM bytecode of your programme along with its ABI (an interface which tells other applications how to interact with your contract).The EVM bytecode is a hexadecimal representation of a sequence of operation codes known as EVM opcodes (full list of EVM opcodes can be found here). Each opcode has a fixed amount of gas assigned and is a measure of resource consumption. In this practical example, we will use Remix to write a very simple contract and look at how it translates to a set of operation codes that get executed by the EVM.Below is the a very basic smart contract which allows us to get a sense of what the generated bytecode does at deployment as well as when when a function is being called thereafter. The contract Example has one public storage variable that gets assigned a value of 1 by the constructor function at deployment. Although simplistic, analysing the bytecode of this smart contract will illustrate the basic interactions between the stack, memory and storage allowing you to apply the same methodology on more complex contracts.Running the compiler will yield the following bytecode:Note: the deployment part of the byte code is highlighted in bold, more on the difference between runtime bytecode and deployment bytecode later“608060405234801561001057600080fd5b50600160008190555060b2806100276000396000f3fe6080604052348015600f57600080fd5b506004361060275760003560e01c80620511a014602c575b600080fd5b60326046565b604051603d91906059565b60405180910390f35b60005481565b6053816072565b82525050565b6000602082019050606c6000830184604c565b92915050565b600081905091905056fea2646970667358221220d6d4f6b7d89f1a77922be220ca2512cf050c8eb988d06ec97a32525e80f98ae464736f6c63430008070033”Which translates to the following opcodes (you can check for yourself here):“PUSH1 0x80 PUSH1 0x40 MSTORE CALLVALUE DUP1 ISZERO PUSH2 0x10 JUMPI PUSH1 0x0 DUP1 REVERT JUMPDEST POP PUSH1 0x1 PUSH1 0x0 DUP2 SWAP1 SSTORE POP PUSH1 0xB2 DUP1 PUSH2 0x27 PUSH1 0x0 CODECOPY PUSH1 0x0 RETURN INVALID PUSH1 0x80 PUSH1 0x40 MSTORE CALLVALUE DUP1 ISZERO PUSH1 0xF JUMPI PUSH1 0x0 DUP1 REVERT JUMPDEST POP PUSH1 0x4 CALLDATASIZE LT PUSH1 0x27 JUMPI PUSH1 0x0 CALLDATALOAD PUSH1 0xE0 SHR DUP1 PUSH3 0x511A0 EQ PUSH1 0x2C JUMPI JUMPDEST PUSH1 0x0 DUP1 REVERT JUMPDEST PUSH1 0x32 PUSH1 0x46 JUMP JUMPDEST PUSH1 0x40 MLOAD PUSH1 0x3D SWAP2 SWAP1 PUSH1 0x59 JUMP JUMPDEST PUSH1 0x40 MLOAD DUP1 SWAP2 SUB SWAP1 RETURN JUMPDEST PUSH1 0x0 SLOAD DUP2 JUMP JUMPDEST PUSH1 0x53 DUP2 PUSH1 0x72 JUMP JUMPDEST DUP3 MSTORE POP POP JUMP JUMPDEST PUSH1 0x0 PUSH1 0x20 DUP3 ADD SWAP1 POP PUSH1 0x6C PUSH1 0x0 DUP4 ADD DUP5 PUSH1 0x4C JUMP JUMPDEST SWAP3 SWAP2 POP POP JUMP JUMPDEST PUSH1 0x0 DUP2 SWAP1 POP SWAP2 SWAP1 POP JUMP INVALID LOG2 PUSH5 0x6970667358 0x22 SLT KECCAK256 0xD6 0xD4 0xF6 0xB7 0xD8 SWAP16 BYTE PUSH24 0x922BE220CA2512CF050C8EB988D06EC97A32525E80F98AE4 PUSH5 0x736F6C6343 STOP ADDMOD SMOD STOP CALLER “This can look intimidating but it is not really once you run through it opcode by opcode which we will do below. Once you see how things are being run, you will be able to think of ways to apply different opcodes that consume less gas to achieve the same thing. Learning solidity’s syntax and types in itself will take you a week at most. However if you want to deeply understand and improve your code, it is very helpful to understand what happens at the lower level.As mentioned before, the bytecode is a hex representation of EVM opcodes that are documented in the yellow paper. When deploying a new contract the bytecode as part of the calldata gets executed. The bytecode includes a section that is referred to as the init section which is not being stored as part of the contract’s account code state but returns the actual contracts runtime bytecode and runs the constructor function which in our case sets an initial value to our public storage variable exampleNumber. The init part of the byte code is highlighted in bold above. To create the contract as part of the blockchain state, a deployment transaction like any other has to be initiated except that the receiver address is set to be empty while the data field contains the compiled bytecode. Once the transaction is being executed an account is created for that contract and the bytecode is being executed.With that in mind, let’s run this bytecode that runs at deployment and demystify this series of opcodes by going through them sequentially. We will go through it by executing a series of opcodes and visualise the resulting stack. When no stack is shown it indicates an empty stack, i.e all words have been popped off.PUSH1 0x80 PUSH1 0x40 -> push 0x80 and 0x40 onto the stackMSTORE -> store value 0x80 at location 0x40 in memory and pop both words off the stack (this is a free memory pointer that comes with compiled solidity to reference the first unused word in memory and prevents overwriting memory that is used internally by solidity)CALLVALUE DUP1-> pushes the amount of ether that was specified as the value field within the deployment transaction on top of the stack and duplicates it with DUP1 opcodeISZERO -> checks if the top word of the stack is zero and pops the word off. If it was zero it pushes 0x1 else 0x0. For the sake of our example, let’s follow the execution path as if we had set the value field of our deployment transaction to 0. Hence ISZERO returns 0x1 on top of the stack.PUSH2 0x10 -> push 2 bytes 0x0010 onto the stackJUMPI -> Jump to instruction with program counter equal to top word of stack 0x0010 if second word of stack is 0x1 (if transaction value field was 0), else continue. Thus in our case we jump to program counter 0x0010 and continue execution since our deployment transaction did set the value field to zero. If we had set the value field to zero, we would not jump at this point but continue execution with PUSH1 0x0 DUP1 REVERT. That would push 0x0 onto the stack, duplicate that word and cause a revert with the top two words indicating the reason for the revert.Note: All that has been happening until now was to check if the constructor was payable and if the deploy transaction sent some ether along with it. Since our constructor was not payable in our exampleContract, sending ether with our deployment transaction should revert the deployment. Hence the conditional jump above.Program counter 0x0010: JUMPDEST POP-> valid jump destinations are indicated by the JUMPDEST opcode. Since we did not send ether with our deployment transaction the init execution jumps to this program counter and we continue execution while popping off one word with the POP opcode leaving us with an empty stack.Note: From this part onward the constructor is being executed as part of the init code.PUSH1 0x1 PUSH1 0x0 -> push 0x01 onto stack (the value we assign to exampleNumber in the constructor). Push 0x0 on top of the stack (we only have one storage variable so it will be located at storage slot 0x0)DUP2 SWAP1-> duplicate the second word of the stack and swap first word with second wordSSTORE -> store top word (0x1) value at storage slot second word (0x0), and pop both words off. The value 1 is now being stored as at slot 0 which is the storage pointer of our only variable exampleNumber.POP -> remove value from stack, leaving an empty stackNote: From this part onward the copying of the actual runtime bytecode starts and is being returned by our deployment transaction which stores it under the newly generated contract account code section of the global blockchain state.PUSH1 0xB2 DUP1 PUSH2 0x27 PUSH1 0x0-> push 0xB2 on stack, duplicate it push 0x27 and 0x0 on stackCODECOPY -> copies the runtime code to memory by by taking top word 0x0 as memory offset to write the code to, second word 0x27 as offset of the byte code to read from (this is 39 in decimal and if you look at the bytecode you can see that the runtime code start at the 40th byte), and third word 0xB2 as the length in bytes to copy (length runtime bytecode). All three words pop off stack.PUSH1 0x0 RETURN -> push 0x0 on top of stack and return. The RETURN opcode will return the runtime bytecode that was stored in memory at offset 0x0 with length 0xB2.RETURN -> end execution and return the runtime code as a result. First word of stack is the memory offset of the result and the second word is the ending offset in memory of the result.INALID -> indicates the end of the init code and start of runtime code.Our contract has now successfully been deployed to the blockchain and the runtime bytecode of our contract is part of the global state under our contract accounts address.We now move on to have a look at the runtime bytecode by calling a specific function of our deployed contract by initiating a new transaction. Our Example contract only has one function which is a getter for the storage variable exampleNumber (getters get automatically generated by solidity for public state variables). Keep in mind that purely reading state does not need a transaction and can be done by simply reading from your node’s database. However to keep this illustration simple we will read the value by triggering a transaction the same way we would call any function that mutates the state.Let’s go through the runtime bytecode’s only getter function exampleNumber() by. When calling a function on a smart contract the transaction data field will contain the first four bytes of the keccak256 hash of the function’s signature followed by the input arguments. In our case there are no input arguments and hence the data field of our transaction only contains those 4 bytes. In the case of exampleNumber() this yields 0x511A0 as input for the data field. Furthermore our transaction will have 0 as the value field since the getter is non payable. Let’s see how the exampleNumber() getter function e call executes by going step by step through the execution of the runtime bytecode.Note: Since there will be a lot of conditional jumping in this part of the bytecode, there is a table at the bottom of this post mapping each instruction of the runtime bytecode to its program counter so you can double check we jump to the right location.PUSH1 0x80 PUSH1 0x40 MSTORE -> setting up the free memory pointer like it was done at the start of the init code.CALLVALUE DUP1 ISZERO -> push the transaction value field input on the stack, duplicate it and check if its zero and pop it off, if it was true pushes 0x1 else 0x0. Let’s assume again our transaction did not send any ether as part of it’s value field, hence we push 0x1 onto the stack. You can probably already guess what is happening here, we are again checking if ether has been incorrectly sent to a non payable function.PUSH1 0xF JUMPI-> push 0xF on top of the stack and jump to instruction at program counter 0xF if the second word of stack is 0x1. If ether was sent as part of the value field with our transaction we would not jump to program counter 0xF and continue by reverting from here (PUSH1 0x0 DUP1 REVERT) because our only function exampleNumber() is not payable.Gas optimisation tip: You can save a little gas (CALLVALUE DUP1 ISZERO PUSH1 0xF JUMPI) by making all your functions payable to avoid the value field check as is also pointed out here by Mudit Gupta.From this part onwards the bytecode validates the data field of our transaction (calldata) is at least 4 bytes long (minimum requirement since it’s the size of a function selector as mentioned earlier).POP PUSH1 0x4 CALLDATASIZE -> remove top word of stack and push 0X4 onto it. Opcode CALLDATASIZE pushes the transaction data field length onto the stack.LT-> checks if calldata size (top word) is less than 4 (second word) bytes long , if it is 0x1 gets pushed on stack, otherwise 0x0. Top two words get popped off the stack. Since our transaction to call exampleNumber() had a calldata of exactly 4 bytes 0x0 gets pushed onto stack to indicate calldata size is larger or equal to four bytes.PUSH1 0x27 JUMPI-> push 0x27 on stack and jump to that program counter if the second word of stack is 0x1 (if calldata was less than 4 bytes) from where our execution path will revert and fail. Else (if calldata is more than 4 bytes) continue. In our case we continue since we our calldata input of our transaction is a valid 4 byte function selector.From here onwards the bytecode iterates through the function jump table by comparing our transaction’s calldata function selector to available functions in the contract and jump to the appropriate one to execute it’s runtime bytecode. Since a lot of jumping will occur we will not go through the bytecode byte by byte but instead follow the execution flow by continuing at the relevant program counter.PUSH1 0x0 CALLDATALOAD PUSH1 0xE0-> push 0x0 onto the stack and push the 32 byte long calldata input onto the stack, push 0xE0 onto the stack.SHR -> opcode to shift right. Here we shift the calldata input to the right 0xE0 (224) times. 32 bytes was the size of the calldata input of which the first 4 bytes were the function selector. Hence the second word of the stack has been reduced to be the function selector itself. Furthermore the SHR opcode pops the first word off stack leaving us with.DUP1 PUSH3 0x511A0 -> duplicate the function selector and push the following three bytes onto the stack 0x511A0 (remember this hexadecimal? this was the first 4 bytes of the keccak256 of the exampleNumber(), which we calculated earlier to input as our data field).EQ -> pushes 0x1 on stack if the second word is equal to top word, i.e if the function selector of the calldata is equal to 0x511A0. In the Example contract we only had one runtime function, a getter to retrieve exampleNumber value. Remember 0x511A0 is the first 4 bytes of the kekkak256 hash of “exampleNumber()”. Hence what the run time code is doing here is checking which function the transaction is calling. Once a match is found, the execution jumps to the relevant program counter where that function’s runtime code is located. In our case there is only one function, so the first comparison is a successful match and we push 0x1 onto the stack.Gas optimisation tip: In case we had more functions in our smart contract, we would keep on doing this comparison until we find the relevant function to execute. You can optimise your contract for runtime gas by naming your most used functions, so that they appear early in the search order. If there are more than four functions in your smart contract binary search is applied. The order of your functions appear in ascending order. Hence you can name your most used function so that it appears first (at mid).PUSH1 0x2C JUMPI -> push 0x2C (the location of the functions exampleNumber() run time code) onto the stack and jump to it since the top word of stack was 0x1 indicating jump should happen.Program counter 0x2C: JUMPDEST PUSH1 0x32 PUSH1 0x46 JUMP -> we jump to program counter 0x2C indicated as a valid jump destination by the JUMPDEST opcode, if the function selector in the calldata corresponded to the the function selector of our getter on exampleNumber() . we push 0x46 on stack and jump to program counter 0x46.Program counter 0x46: JUMPDEST PUSH1 0x0 SLOAD DUP2 JUMP -> we jump to program counter 0x46 validated by the JUMPDEST opcode. Storage slot 0 gets loaded on stack (which is the storage slot of our only state variable example number) and the second word from stack gets duplicated and we jump to that program counter 0x32Program counter 0x32: JUMPDEST PUSH1 0x40 MLOAD PUSH1 0x3D -> Push 0x40 on stack and load that location from memory which is 0x80. Remember 0x80 was the free memory pointer set up at the start. We push 0x3D on stackSWAP2 SWAP1PUSH1 0x59 JUMP -> swap 3rd and 1st element of stack leaving us with the stack on the left. We push 0x59 onto the stack and jump to program counter 0x59.Program counter 0x59: JUMPDEST PUSH1 0x0 PUSH1 0x20 DUP3 -> push 0x0 and 0x20 onto stack and duplicate 3rd word on top of stackADD SWAP1 POP PUSH1 0x6C PUSH1 0x0 DUP4 ADD DUP5 PUSH1 0x4C -> more stack manipulations leading to below stack (as an exercise, verify don’t trust;)) and we jump to top word of stack 0x4c.Program counter 0x4c: JUMPDEST PUSH1 0x53 DUP2 PUSH1 0x72 JUMP -> more stack manipulations… and jumping to 0x72Program counter 0x72: JUMPDEST PUSH1 0x0 DUP2 SWAP1 POP SWAP2 SWAP1 POP JUMP -> more stack manipulations and jumping to 0x53Program counter 0x53: JUMPDEST DUP3 MSTORE POP POP JUMP ->duplicate 3rd word from stack (the value of examplenNumber) and store it at location 0x80 (our free memory pointer) in memory and pop top two words off stack. Jump to pc 0x6CProgram counter 0x6C: JUMPDEST SWAP3 SWAP2 POP POP JUMP -> stack manipulation and jump to 0x3DProgram counter 0x3D: JUMPDEST PUSH1 0x40 MLOAD DUP1 SWAP2 SUB SWAP1 -> load free memory pointer 0x80 and some stack operations leading to:RETURN -> return the value at memory location 80 (the free memory pointer) of length 0x20 (32byte word). Last word of stack doesn’t get used.TL;DR: The runtime bytecode of the exampleNumber() getter function, loaded the only storage variable from slot zero, stored it in memory at our free memory pointer location and returned it with the RETURN opcode.Tada! That was it. We went from source code to deploying the resulting bytecode and calling a function on the stored runtime bytecode. Hope that helps and gives you an idea of what happens “behind the scenes”. This was a very simplistic example to demonstrate how to debug on the EVM level and hopefully helps in getting your hands dirty with more complex cases. A more thorough example of this byte by byte analysis can be found here. Have fun!",,2,1,58,23,,,27,0,0,9,172
,,https://betterprogramming.pub/how-to-create-an-ethereum-bot-55aac7534654,,,,0,,,,0,0,0,,,0,0,,0,
From Software Engineer to CTO,Better Programming,https://betterprogramming.pub/from-software-engineer-to-cto-c0da5f8d0bfd,Katia Gil Guzman,139,10,2072,"My life today is completely different than what it was a year ago. In April of 2021, I made what some would call a bold career move: I switched from being a Software Engineer at a large corporation to being the CTO of a startup I co-founded. I left behind a great job at one of the best companies in the world and its numerous benefits to try and build something from scratch and feel the excitement of creating something new.Fortunately, I wasn’t on my own: I joined a startup studio which helped me from the start. I didn’t even know what I was going to build yet and they provided a team of experts that had successfully launched several startups before. They identified a space where they thought there was a problem to solve, introduced me to a co-founder, and worked hands on with us on a daily basis.Thanks to them, last year was probably the year I’ve learned the most (I mean, if you don’t count the years where I was literally discovering life itself). While my previously acquired Software Engineering skills came in handy, I had to learn something they don’t teach at school and that you can’t just google: how to be a startup founder. And as the CTO of an early-stage startup, I had to unlearn some of my previous beliefs and train my brain to think differently.I’ll describe here what are, according to me, the main differences between Software Engineer and CTO, and how I adapted to this new role without having an extensive work experience behind me.As a Software Engineer, you spend most of your time coding or attending meetings to decide what to code. At my previous job, I was lucky enough to also be able to contribute my ideas about the overall architecture and be fairly independent in my technical choices.But as a CTO, that’s just one of the MANY activities on my plate. Oftentimes I don’t even have time to code during the day so I have to do it at night or on weekends.I’m guessing at a later-stage company, coding isn’t even part of the job description anymore. But when you’re at the very early stages of a startup and it’s just you and your co-founder, it’s up to you to build your product. And since we’re a SaaS company, this means coding.Except you also have to decide what to build, and this is the trickiest part. Before making that decision, you need to develop a strategy, find customers, iterate with them and then prioritize. And once you’ve finally decided and are actually in the building phase, you still need to spend time with customers, think about next steps and how to constantly improve, keep apprised of the latest technologies, start recruiting, and do a million other things! Of course, my co-founder, who is the CEO, takes care of a lot of things I don’t have to think about such as Sales & Marketing, as well as most of the admin stuff. But as a co-founder, I still need to be involved in every strategic decision, and that does take time. However, I wouldn’t have it any other way — to me, that’s what building a startup means: being ready to get involved in every aspect of the business and being aware of everything that’s going on.I try to keep a few large time blocks with no meetings to really focus on the technical stuff, but I admit there have been a few weeks where there were just too many urgencies. Between internal meetings, preparing demos for qualified leads, going onsite with customers, creating training or demo material and the occasional deck for VC intros, there have been times where the only moments I could make progress on the actual product were outside of regular work hours.As you can imagine, this led to a way different work/life balance than when I was a Software Engineer. I hate the hustle culture and I think promoting working hard all the time in the name of productivity is stupid. But the thing is — as a co-founder, your startup’s success relies heavily on you being productive…Inevitably, knowing that, I tend to de-prioritize personal stuff and spend way more time working than I should. For instance, I started painting regularly in 2020 and I loved it. But I haven’t picked up a brush since April, because it’s a quite time-consuming activity and I know I could be working on something else instead.When it’s your company, you’re never really “off the job”, at least I feel that way. But I think it’s important to have some balance for it to be sustainable in the long run. Once we grow, and I can rely on more people to get the job done, I’ll make sure to make more time for personal activities. I think spending X years at full speed without taking time to breathe is just plain unhealthy.That might be one of the biggest differences with being a software engineer: as an individual contributor, you don’t have the burden of knowing that things will stand still if you don’t push to move forward. You’re part of a whole and you can let yourself be guided by the rest of the company, even though you’re often asked to take some initiative and push yourself. Especially at large corporations, the stakes are just not the same, and neither is the mental load.As a software engineer, I’ve always been trained to find solutions. And I love finding solutions! That is one of my favorite things to do ever since I was a child.But there’s a catch: thinking of solutions on your own might be fun, but it’s actually really bad for business.When I first started interviewing potential users thanks to the startup studio’s network, I thought I had a solution to their problems after 5 minutes of questioning…But one skill required to be a CTO that I had a hard time acquiring is to stop trying to impose a solution and instead try to find out what the real problem is.Instead of asking questions, I had to learn to listen and observe. I have a tendency to want to act, go fast and build things right away. I know I can be fairly impatient. So that shift in mentality was actually one of the most difficult things for me in my transition from software engineer to CTO. But now that I have understood that, I realize the “builder” mindset is not that useful to launch a startup if it’s not coupled with empathy and good listening skills. What good is it to build something fast if it’s not really what users need?Another thing I had to retrain my brain for is to accept doing less. As a software engineer, I learned to produce work of the best quality. A PR couldn’t be accepted unless the code was unit tested and it had at least 2 reviewers. We contemplated what architecture would be the most optimized for a while before jumping in. I admit I was a little bit ashamed if I thought my code was “dirty” because I was afraid of being judged.That’s all over now. It’s paradoxical, because as a CTO you’re responsible for ensuring quality and setting up all kinds of processes for when you have a team with lots of engineers that need to work efficiently. But at the same time, you’re also responsible for going to market as fast as possible with a product that customers can use.So, it’s a trade-off; quality versus speed. You want to let go of processes that slow you down while keeping in mind the broader picture. You want to do quick & dirty while keeping technical debt to the minimum. You want to make choices that are fast to implement but are not incompatible with scale.Reid Hoffman said“If you’re not embarrassed by the first version of your product, you’ve launched too late”I like to call this concept “baby kangaroo”: when a baby kangaroo is born, it is just one inch long, and it crawls along the fur of its mom to go into the pouch, and develop there, out in the world. It’s already what it’s supposed to — a kangaroo. It just needs a few more months in a secure environment after it is born to grow to its full size. For a startup product I would say this means starting with the right tech stack, having a plan for how to scale and what processes to put in place, but putting the product out in the world before it is fully developed, while making sure it’s in an environment suitable for its growth (such as an early adopter program).I had only 3 years of work experience before making the switch to entrepreneur and had never managed a team before. As a CTO, I’ve had to do my first hiring and people management.We’re still a small team, but that’s already a big change from when I was an individual contributor. For starters, I had to learn to delegate. I was used to doing everything myself as an IC, but as I’ve mentioned above, as a CTO, there is just too much to do for me to not rely on other people.I had some trouble with this at first, not because I did not trust other people to do the job correctly, but mostly because I liked doing the tasks myself — I found it exciting and fun and I wanted to be the one that got to do it. I’ve learned to accept that I don’t have time for everything and that I can actually do the things that I should really focus on better if others can get the rest done. Plus, they can get it done better than me because that’s what we hired them for!I’ve also learned that spending time onboarding new hires and continuously making sure that people on the team are thriving is definitely worth it. It is just as important as spending time coding, and I expect it to be one of my main activities as we grow bigger.It’s interesting to reflect on what you did as a software engineer and what you think are the best practices the team should follow, to formalize it. New team members can then have a structured way of contributing to the product in a way that’s aligned with your expectations.To me, leadership and management are two very different things. You can be an individual contributor and still demonstrate leadership. Actually, at my previous job, leadership was encouraged and highly valued. So, I think this point might be where the Software Engineer and CTO roles diverge the least!But even though you might show leadership as an IC, by being proactive, confident in your choices, and convincing people to trust your judgement on a given project, you might have to take it up a notch as a CTO.I believe that as a CTO, and most of all, a startup founder, you need to inspire people, and for this, you need to be passionate about what you’re doing. It should not be a job but a mission for you and everyone who meets you should see that.And even if I’m passionate about what I’m doing and convinced that our mission is worthwhile, I think radiating confidence is the hardest part about being a CTO. I know to trust my gut when I believe something is right, I’m generally confident in my choices and ability to deliver, but I still doubt myself constantly. I guess that’s called imposter syndrome?If I’m honest, making the switch from Software Engineer to CTO was no easy feat, but I think that if building a startup is what you truly want, it’s not as hard as it may seem.I personally love having more to do than just coding. I’m comfortable with the fact that my personal and work life are kind of blended together because I’ve always wanted to be an entrepreneur. Learning to change my mindset was tough but I’m glad I did. Even though I had never managed people before, I enjoy it, and I think as long as you care, you can find a way to help people grow. Ultimately, I think the hardest part of all is actually taking the leap and being confident enough. I’m still relatively new at this, but I’m sure this just comes naturally with time.",,4,0,4,1,1225,818,6,0,0,12,296
,,https://medium.com/u/53f0fe2d14cb?source=post_page-----c35fc29ccf6--------------------------------,Angela Ching,395,,,,,,0,,,,,,,0,,
,,https://medium.com/u/25fc5577e294?source=post_page-----c35fc29ccf6--------------------------------,Rodrigo Herrera Itie,304,,,,,,0,,,,,,,0,,
,,https://medium.com/u/e98ee43541c0?source=post_page-----c35fc29ccf6--------------------------------,Michael Seemann,35,,,,,,0,,,,,,,0,,
Storing Data on Ethereum Blockchain With Node.js,Better Programming,https://betterprogramming.pub/storing-data-on-ethereum-blockchain-with-node-js-643cb2961ebc,Petros Demetrakopoulos,306,6,1058,"We are at the beginning of 2022 and everybody is talking about the “next big thing” on the internet: Web 3. But the majority of people who talk about NFTs, crypto assets, altcoins, and blockchain technologies in general, do not really know how they can use them in practice and they are unaware of their true capabilities.This article is a walkthrough that will help you understand some basic concepts of blockchains and smart contracts. It will also provide a hands-on guide to actually use the Ethereum blockchain to store structured data as you would do with a classic SQL database or a less classic No-SQL database.In this way, you can take advantage of blockchains and use them actively in you next web application.First of all, let’s remember what blockchains are:A blockchain in Layman’s terms is a kind of distributed database that offers more transparency and more security than other databases.This means that their main role is to store data.Ethereum blockchain is amazing because it was the first blockchain that offered the capability of running code over it with Smart contracts and Solidity, the language behind them.But Solidity smart contracts may be a real hell even for experienced developers as they have some major technical drawbacks (at least for now).The most important of them are the following:However, Ethereum Smart contracts are still extremely useful for a variety of applications ranging from complex auction scenarios and decentralized web apps to simple data storage.The gap between the complex, difficult to develop, test and maintain smart contracts, and the actual use of the Ethereum blockchain in JS-based web apps are covered by EthAir Balloons.EthAir Balloons is a strictly typed ORM (Object-Relational Mapper) library for the Ethereum blockchain. It allows developers to use Ethereum blockchain as persistent storage in an organized and model-oriented way without writing custom complex Smart contracts. We could say it is for Ethereum-based blockchains what Mongoose is for MongoDB.Of course, EthAir Balloons do not restrict developers to using only the Ethereum blockchain. The library can be used with any Ethereum-based blockchain (which can be either private or public).Actually, if you want to experiment with EthAirBalloons and data storage on the Ethereum blockchain, I would strongly suggest making any tests and experiments in a private instance of the Ethereum blockchain (you can run one locally using the ganache-cli tool). This is because, the public Ethereum blockchain would charge a massive amount of transaction fees (know as “gas”) for each write, update and delete operation.But enough with the theoretical part: Let’s see it in action.This part of the article assumes that you are familiar with the basic concepts of JavaScript and Node.js.First of all let’s create a new node project by typing the following commands on the terminal:Complete the wizard that will ask for further info about the projects (name, version, license, author, etc).Then install the EthairBalloons, Express library and body-parser by executing the following lines in the root directory of the project. We will use express in order to expose the CRUD operations for the data as REST API calls.After the installation of the libraries we need to create a file named index.js and a directory named contracts. This is where the auto-generated Solidity contracts will be stored.In the index.js file,we should add the following lines in order to import the various dependencies of the project and declare the model of the data that we want to store on the blockchain.In the first 8 lines, we import and declare the dependencies of the API.On line 10, we initiate an instance of ethairballoons (or what I like to call an ethAirBalloonsProvider) using only 2 arguments:1. the URL of the Ethereum blockchain provider that we want to use (in the example it is set to a local ganache-cli provider),2. the path where we want to save the automatically generated smart contracts of the models.After the provider initialization, we can create new data schemas using the createSchema() function and pass the schema details in JS object format (lines 12–29). Of course, developers are able (as advised) to keep the schema definitions in separate JSON files and then import them using the require() statement at the top of the file.Now that our data schema is set, it is time to deploy it in the blockchain, in this point I would like to remember that we do so in a local ganache-cli instance (which is an Ethereum blockchain simulator) and not in the actual Ethereum network.As transaction fees may be huge, it is strongly advised to only deploy EthAirBalloons models in private Ethereum blockchains or locally using ganache-cli.In order to achieve that, we declare a new endpoint on our API that is called GET /deploySo when this endpoint gets called, a new smart contract will be automatically generated and deployed based on the Schema we declared earlier, without the need to write a single line of Solidity code.This function (Car.deploy) returns a boolean indicating if the deployment is successful and an error object that will be undefined if the deployment is successful. After the deployment is completed we can call the other functions of the model.As we said earlier, EthairBalloons fully supports all CRUD (Create, Update, Delete) operations and even more.Apart from creating, updating and deleting an instance of the model we declared, EthairBalloons provide the ability to find an instance by Id and find all currently stored instances of the model.The endpoints exposing the CRUD operations are shown in the code below.Before running the server locally, using the well-known node . command, we should start the ganache-cli (assuming it is already installed, if not you can easily find out how to install it there).When running ganache-cli we must be sure that it is initialized started with enough funds to support all the transactions we want to do through our CRUD API and EthairBalloons. So we set parameter -l (gasLimit) to 800000000000000 which is enough gas for some create and update actions. The final command should look like this:Now you are ready to call the endpoints of the API to generate and deploy the Smart Contract to the Ethereum blockchain and then Create, Update, Delete and Find any instance of your model.I hope you find it interesting and that it will be useful in future projects! You can find the complete project on this GitHub repository. Do not hesitate to ask questions related to EthairBalloons.",,1,4,4,3,812,470,2,1,0,8,117
,,https://medium.com/u/3772d202e0d3?source=post_page-----c35fc29ccf6--------------------------------,James Hinton,287,,,,,,0,,,,,,,0,,
,,https://medium.com/u/ce6d4167ea77?source=post_page-----c35fc29ccf6--------------------------------,Petros Demetrakopoulos,306,,,,,,0,,,,,,,0,,
From Software Engineer to CTO,Better Programming,https://betterprogramming.pub/from-software-engineer-to-cto-c0da5f8d0bfd,Katia Gil Guzman,139,10,2072,"My life today is completely different than what it was a year ago. In April of 2021, I made what some would call a bold career move: I switched from being a Software Engineer at a large corporation to being the CTO of a startup I co-founded. I left behind a great job at one of the best companies in the world and its numerous benefits to try and build something from scratch and feel the excitement of creating something new.Fortunately, I wasn’t on my own: I joined a startup studio which helped me from the start. I didn’t even know what I was going to build yet and they provided a team of experts that had successfully launched several startups before. They identified a space where they thought there was a problem to solve, introduced me to a co-founder, and worked hands on with us on a daily basis.Thanks to them, last year was probably the year I’ve learned the most (I mean, if you don’t count the years where I was literally discovering life itself). While my previously acquired Software Engineering skills came in handy, I had to learn something they don’t teach at school and that you can’t just google: how to be a startup founder. And as the CTO of an early-stage startup, I had to unlearn some of my previous beliefs and train my brain to think differently.I’ll describe here what are, according to me, the main differences between Software Engineer and CTO, and how I adapted to this new role without having an extensive work experience behind me.As a Software Engineer, you spend most of your time coding or attending meetings to decide what to code. At my previous job, I was lucky enough to also be able to contribute my ideas about the overall architecture and be fairly independent in my technical choices.But as a CTO, that’s just one of the MANY activities on my plate. Oftentimes I don’t even have time to code during the day so I have to do it at night or on weekends.I’m guessing at a later-stage company, coding isn’t even part of the job description anymore. But when you’re at the very early stages of a startup and it’s just you and your co-founder, it’s up to you to build your product. And since we’re a SaaS company, this means coding.Except you also have to decide what to build, and this is the trickiest part. Before making that decision, you need to develop a strategy, find customers, iterate with them and then prioritize. And once you’ve finally decided and are actually in the building phase, you still need to spend time with customers, think about next steps and how to constantly improve, keep apprised of the latest technologies, start recruiting, and do a million other things! Of course, my co-founder, who is the CEO, takes care of a lot of things I don’t have to think about such as Sales & Marketing, as well as most of the admin stuff. But as a co-founder, I still need to be involved in every strategic decision, and that does take time. However, I wouldn’t have it any other way — to me, that’s what building a startup means: being ready to get involved in every aspect of the business and being aware of everything that’s going on.I try to keep a few large time blocks with no meetings to really focus on the technical stuff, but I admit there have been a few weeks where there were just too many urgencies. Between internal meetings, preparing demos for qualified leads, going onsite with customers, creating training or demo material and the occasional deck for VC intros, there have been times where the only moments I could make progress on the actual product were outside of regular work hours.As you can imagine, this led to a way different work/life balance than when I was a Software Engineer. I hate the hustle culture and I think promoting working hard all the time in the name of productivity is stupid. But the thing is — as a co-founder, your startup’s success relies heavily on you being productive…Inevitably, knowing that, I tend to de-prioritize personal stuff and spend way more time working than I should. For instance, I started painting regularly in 2020 and I loved it. But I haven’t picked up a brush since April, because it’s a quite time-consuming activity and I know I could be working on something else instead.When it’s your company, you’re never really “off the job”, at least I feel that way. But I think it’s important to have some balance for it to be sustainable in the long run. Once we grow, and I can rely on more people to get the job done, I’ll make sure to make more time for personal activities. I think spending X years at full speed without taking time to breathe is just plain unhealthy.That might be one of the biggest differences with being a software engineer: as an individual contributor, you don’t have the burden of knowing that things will stand still if you don’t push to move forward. You’re part of a whole and you can let yourself be guided by the rest of the company, even though you’re often asked to take some initiative and push yourself. Especially at large corporations, the stakes are just not the same, and neither is the mental load.As a software engineer, I’ve always been trained to find solutions. And I love finding solutions! That is one of my favorite things to do ever since I was a child.But there’s a catch: thinking of solutions on your own might be fun, but it’s actually really bad for business.When I first started interviewing potential users thanks to the startup studio’s network, I thought I had a solution to their problems after 5 minutes of questioning…But one skill required to be a CTO that I had a hard time acquiring is to stop trying to impose a solution and instead try to find out what the real problem is.Instead of asking questions, I had to learn to listen and observe. I have a tendency to want to act, go fast and build things right away. I know I can be fairly impatient. So that shift in mentality was actually one of the most difficult things for me in my transition from software engineer to CTO. But now that I have understood that, I realize the “builder” mindset is not that useful to launch a startup if it’s not coupled with empathy and good listening skills. What good is it to build something fast if it’s not really what users need?Another thing I had to retrain my brain for is to accept doing less. As a software engineer, I learned to produce work of the best quality. A PR couldn’t be accepted unless the code was unit tested and it had at least 2 reviewers. We contemplated what architecture would be the most optimized for a while before jumping in. I admit I was a little bit ashamed if I thought my code was “dirty” because I was afraid of being judged.That’s all over now. It’s paradoxical, because as a CTO you’re responsible for ensuring quality and setting up all kinds of processes for when you have a team with lots of engineers that need to work efficiently. But at the same time, you’re also responsible for going to market as fast as possible with a product that customers can use.So, it’s a trade-off; quality versus speed. You want to let go of processes that slow you down while keeping in mind the broader picture. You want to do quick & dirty while keeping technical debt to the minimum. You want to make choices that are fast to implement but are not incompatible with scale.Reid Hoffman said“If you’re not embarrassed by the first version of your product, you’ve launched too late”I like to call this concept “baby kangaroo”: when a baby kangaroo is born, it is just one inch long, and it crawls along the fur of its mom to go into the pouch, and develop there, out in the world. It’s already what it’s supposed to — a kangaroo. It just needs a few more months in a secure environment after it is born to grow to its full size. For a startup product I would say this means starting with the right tech stack, having a plan for how to scale and what processes to put in place, but putting the product out in the world before it is fully developed, while making sure it’s in an environment suitable for its growth (such as an early adopter program).I had only 3 years of work experience before making the switch to entrepreneur and had never managed a team before. As a CTO, I’ve had to do my first hiring and people management.We’re still a small team, but that’s already a big change from when I was an individual contributor. For starters, I had to learn to delegate. I was used to doing everything myself as an IC, but as I’ve mentioned above, as a CTO, there is just too much to do for me to not rely on other people.I had some trouble with this at first, not because I did not trust other people to do the job correctly, but mostly because I liked doing the tasks myself — I found it exciting and fun and I wanted to be the one that got to do it. I’ve learned to accept that I don’t have time for everything and that I can actually do the things that I should really focus on better if others can get the rest done. Plus, they can get it done better than me because that’s what we hired them for!I’ve also learned that spending time onboarding new hires and continuously making sure that people on the team are thriving is definitely worth it. It is just as important as spending time coding, and I expect it to be one of my main activities as we grow bigger.It’s interesting to reflect on what you did as a software engineer and what you think are the best practices the team should follow, to formalize it. New team members can then have a structured way of contributing to the product in a way that’s aligned with your expectations.To me, leadership and management are two very different things. You can be an individual contributor and still demonstrate leadership. Actually, at my previous job, leadership was encouraged and highly valued. So, I think this point might be where the Software Engineer and CTO roles diverge the least!But even though you might show leadership as an IC, by being proactive, confident in your choices, and convincing people to trust your judgement on a given project, you might have to take it up a notch as a CTO.I believe that as a CTO, and most of all, a startup founder, you need to inspire people, and for this, you need to be passionate about what you’re doing. It should not be a job but a mission for you and everyone who meets you should see that.And even if I’m passionate about what I’m doing and convinced that our mission is worthwhile, I think radiating confidence is the hardest part about being a CTO. I know to trust my gut when I believe something is right, I’m generally confident in my choices and ability to deliver, but I still doubt myself constantly. I guess that’s called imposter syndrome?If I’m honest, making the switch from Software Engineer to CTO was no easy feat, but I think that if building a startup is what you truly want, it’s not as hard as it may seem.I personally love having more to do than just coding. I’m comfortable with the fact that my personal and work life are kind of blended together because I’ve always wanted to be an entrepreneur. Learning to change my mindset was tough but I’m glad I did. Even though I had never managed people before, I enjoy it, and I think as long as you care, you can find a way to help people grow. Ultimately, I think the hardest part of all is actually taking the leap and being confident enough. I’m still relatively new at this, but I’m sure this just comes naturally with time.",,4,0,4,1,1225,818,6,0,0,12,296
,,https://medium.com/u/b1afabf2359d?source=post_page-----c35fc29ccf6--------------------------------,Patric,69,,,,,,0,,,,,,,0,,
Truffle * React * Monorepo With TypeScript: Build an Ethereum dApp,Better Programming,https://betterprogramming.pub/truffle-react-monorepo-with-typescript-4421039d4ba8,Michael Seemann,35,11,1945,"There are a lot of examples of how to set up a project for dApp development. But most of them stay with JavaScript.I’d like to use TypeScript for more productivity and better code quality. Also, I want to have everything in one repo and a valuable coverage report for my code. The GitHub repository can be found here— so you can use it as a starting point for your own project.Let’s clarify what we want to achieve:What should be available on your machine:Please refer to Truffle docs to know how to install them if this is not the case on your machine.To get started just create the project root folder: mkdir truffle-react-typescript and cd into it.The backend will live in a folder backend.So let’s create the folder and run truffle init in it:This will generate a bare minimum truffle project.Along with this, we can create a sample contract and a corresponding test for it:As you can see the migrations and test files are all generated as JavaScript files.Before we add the TypeScript feature let’s add some sample code to our contract, create the migration for our contract and check that all is working as expected.The contract just stores and reads a value:Add the file 2_deploy_contracts.js next to 1_initial_migration.js and paste the following content:After starting ganache with the defaults you should be able to run truffle migrate and truffle test without any error.First of all, we need to initialize the backend project as a node project.Simply run yarn init within the backend folder — you may use the provided defaults. After that, we can add the required npm packages to enable the typings feature. The packages are:All together in one command:Now we can use typechain to generate the type definitions of our contracts.This should be done automatically after the project is initialized because we don’t want the generated code to be part of the git project.This is possible with a postinstall script in our package.json:Now every time yarn install is running against our project the contracts are compiled (this will generate the contract JSON files in the build folder) and from that typechain will generate the type definitions in the types folder.The target must be specified so the generated type definitions match our truffle project. Later we will add types for web3.Let’s see if we can convert the unit test into TypeScript.Rename the file sample_contract.js into sample_contract.ts. Now paste the following content into it:If you check the code in your IDE you will see that there is no type information available.This can be fixed with a proper tsconfig.json in the project folder:Now the tests should work even if they are TypeScript files and you will get compilation errors if your tests no longer match the solidity contracts API.If we want coverage reports for our contract tests we can use solidity-coverage.So let’s add it to our backend project: yarn add — dev solidity-coverage and add a script to the package.json:Also, we need to tell truffle that there is a new plugin available.That needs to be done in the truffle-config.js file within the plugins property: plugins: [“solidity-coverage”].If the script is now executed truffle test is executed, the coverage is measured and the result is stored in the project. The report is generated in different formats. For example as HTML:Converting the migrations scripts into TypeScript is a little bit more complicated because truffle expects *.js files in the migration folder.To achieve this we need to generate the *.js files before running the migration. This is achievable by a special script in the package.json file:Also, a special tsconfig file is necessary to include the type definitions. The tsconfig.migrate.json file should be:As you can see this config inherits from the default tsconfig and includes the *.ts and type definition files.If you run the migration script the *.js and *.map files are written to the migrations folder.My IDE hides these files if a *.ts file with the same name is present and I have excluded the generated files from git. If this doesn’t suit you, you can change the migrations folder in truffle config. The property is migrations_directory.That’s it. The backend is now fully typed and we can care about the client.To generate the client stub we can use react-script with the TypeScript template:This was the fun part: all relevant files are already *.ts or *.tsx files. So let’s start building a sample app to demonstrate the usage of our backend contract.Because we will use web3 we need to add the web3 lib to the client project: yarn add web3. The App.tsx file will have the following content (a really simple example — for sure not suitable for a real app):As you may have noticed, there will be a bunch of TypeScript errors. So let’s fix them:You can see that we reference the compiled contract from the backend project. To make this working we need to setup a yarn workspace.Place a package.json in the root folder of the whole project — e.g. next to the folders backend and client. The content should be the following one:Delete the existing node_modules files folders and yarn.lock files in the backend and client folder and add “backend”: “1.0.0” as a dependency to the client.Once this is done, run yarn install from the root folder. You are now able to reference files from the backend project by referencing them with the backend prefix in imports.Unfortunately, the nohoist was necessary because of type clashes between the libs provided by the backend and by the client.It would be possible to exclude specific packages from hoisting. But I ended up with the exclusion of the complete client. Let me know if you have a better solution.The next missing import is the type SimpleContract from the backend project. What we need here are special type definitions for the web3 lib.So we must add @typechain/web3 as a dependency to our backend project and generate them in the same way we generated them for truffle.This should also be done in the postinstall phase of our yarn project:The type errors in the code are now nearly fixed.The compiler complains about the ethereum property on the window object.We can add the required typings in the react-app-env.d.ts file:Now the code should compile without any errors and we can start the development server. Because we have now a monorepo we can start the dev server from the root project:Your browser will open and will try to interact with our contract.Ensure MetaMask is connected to your local ganache instance and that your contract is deployed. If everything goes well you should see the stored value in the UI.There is only one little problem. The current version 5 of react-script uses webpack 5 under the hood. And webpack 5 no longer provides polyfills for node packages.But web3 needs them, so that our code works. If you run into this problem please have a look at this issue comment. I have described a workaround for how to fix this problem for now.For the unit tests there is already a script in the package.json if we want a coverage report for the client project we can provide a special option that tells react-scripts to generate the reports:The report is written to the coverage directory in the client folder.Now that the backend and client can be tested, build, deployed, and are running locally let’s ensure we have an automatic test and build process for our monorepo.To make it as simple as possible we will use GitHub actions for that because the code is already hosted on GitHub. I will only highlight the important aspects.What we want as a developer is a quick response if our lint, tests, and builds are running without any error.The monorepo makes it possible to run one yarn install command and cache the node_modules folder for future or following ci runs.Also, the backend and client steps can run in parallel.To avoid two yarn install executions for every part of our app we can run it once, ensure the node_modules folders are cached and the backend and client uses these cached files. This will look like this:Please ignore the finish job for now, as we will handle it later. The prepare-dependencies jobs look like this:After the steps checkout and node setup a cache job is used to restore the cached node_modules folders. This will restore the cache of every node_module folder — even from subfolders. If the cache could not be restored (e.g. there is no cache-hit) yarn install is executed.The two other jobs (backend and client) are executed after the prepare-dependencies job has run. So we can always be sure that the cache is available and we don’t need to run yarn install again.But we need to call the postinstall scripts again to compile the solidity code and generate our typings.The steps are the same for every job:As you can see we do not need to cd into the subfolders. This is handled by the yarn workspace.The execution of the solidity tests requires a running ganache instance and migrated contracts.To accomplish this we need to start a ganache instance before we execute the solidity tests.We can use the ganache-cli package (yarn workspace backend add — dev ganache-cli).After the installation, we create a script in the package.json:.. and a GitHub action job that will start the ganache instance and stop the job execution until the ganache instance is ready to accept calls at the TCP port:Sure enough, we need to stop ganache if we are done with all of our other jobs:That's it. Now we can execute yarn workspace backend migrate, test and coveralls.After the CI jobs are executed we can upload the coverage reports to coveralls. The only thing we need to take care of: we have two jobs running and want coveralls to combine the results to have an overall code coverage number.For the backend we define the following job:Coveralls needs read access to the project. We can use the auto-generated GITHUB_TOKEN for that (the default permission is read/write.This can be changed to read-only in action settings if needed). The flag-name is required to identify the jobs. And we need to tell coverall that we have parallel running jobs.The code for the client is:As you can see the flag-name for the client job is different from the backend job.And we tell coveralls the base-path for the client folder in our monorepo to align the path information. The backend uses absolute path information in the lcov.info file and did not need this additional information.The last step is to tell coveralls that all jobs have been executed. The job in our workflow must run if the backend and client job has finished:If our workflow is completed we can see the combined coverage results in the coveralls UI with the possibility to drill down to each line of code.As you could see it was a lot of effort to get complete typed code and a running workflow — more than I thought. Was it worthwhile?I would say: yes.Because we can now rely on typed code and two projects (backend and client) that benefit from the type information. Nevertheless, they are separated and every project has its own responsibilities. And we can easily add different client projects to our yarn workspace.Right now one pain point is the disabled hoisting of the client npm packages because of type clashes due to type information in packages that are required by the backend and the client.Keep in mind they use different testing frameworks. Also, the web3 types required explicit type castings. Maybe the eth-sdk project can overcome these restrictions.Let me know if you have any ideas to improve the approach shown above.You’ll find the current code in this GitHub Repo — just fork it.",,1,22,0,9,1163,520,4,3,0,12,90
"Ready, Set, Solana! Getting Your M1 Mac Ready for Web3 dev with Solana and Rust",Geek Culture,https://appnologyjames.medium.com/ready-set-solana-getting-your-m1-mac-ready-for-web3-dev-with-solana-and-rust-37695c47573a,James Hinton,287,4,401,"You’re ready to get going on blockchain. You’ve done your research (or maybe just went with something which looked interesting) and selected Solana to do your learning on.Super cool! Solana is an amazing blockchain with a lot to recommend it. It’s blazingly fast, boasting one of the highest Transactions Per Second (TPS) at the time of writing. It’s written in Rust which is an elegant, beautiful and memory safe language.You’ve made a great choice.At the time of writing, Solana and M1 chips don’t play nicely together. I kept finding weird and annoying errors any time I tried to test my code. Here’s how I set up my Mac.Here’s the error I got trying to run solana-test-validator and solana-test-validator --no-bpf-jitHere’s how to fix it.Rosetta 2 is the interpretation layer for interpreting x86 instructions into ARM instructions. The good news is that Apple has done an excellent job with Rosetta 2. You’ll barely even notice you’re using it.For the rest of this tutorial use your Rosetta TerminalNot everyone I came across online had this issue, but I certainly did. I had Homebrew installed on my M1 already, but I had to install it in a different location.Here’s the error I got:Here’s the solution:Awesome work! Now we’re ready for The Rust Programming Language.We start by getting RustLang ready to go on our machine under our Rosetta terminal.Big shout out to Nicholas Garfield for his webpage here. The next few steps came from that site.Save the file and let’s get into SolanaHere comes the exciting moment we’ve been working towards. Getting Solana working effectively on our M1 Macs.Phew. That part probably took you a solid 30 minutes of install time (or maybe it’s a couple of years later and it only took 5 mins. I’m jealous :P )This is the moment of truth. Did our steps work?If it’s working, you’ll get the following:Whew. What a relief!If you’re looking for some learning recommendations, I’d really recommend buildspace. Their courses are fun, interactive, useful and above all insightful.Specifically for Solana I’d recommend Build a Web3 app on Solana with React and Rust. It’s a brilliant course which walks you through the basics, and by the end you’ll have deployed your first thing on a blockchain (Solana). Plus it’s an easy introduction to Rust and React.If you found this content helpful, I’d love it if you could connect, clap and share the article. It means a lot ❤",,1,4,3,1,1054,470,2,5,0,7,35
Deploy Solidity Contracts to an NFT Marketplace With Price Feed,Better Programming,https://betterprogramming.pub/solidity-contracts-for-an-nft-marketplace-5a706bb94486,Patric,69,8,944,"We will go through the Contracts step by step, within every step we will first look at a diagram explaining the process on a high level, and afterward, we will look at the code.At the end of the article, we will deploy the contracts with the Remix IDE and Metamask to the Polygon Testnet and create + upload an NFT to the marketplace.You can see the complete code here.NFT (Non-fungible) more or less means that it’s unique and can’t be replaced with something else. For example, a bitcoin is fungible — trade one for another bitcoin, and you’ll have exactly the same thing. A one-of-a-kind trading card, however, is non-fungible. If you traded it for a different card, you’d have something completely different.Often used for art or items in the metaverse, but it can be anything digital or real. An NFT could be for example be a reference for a house in the real world and the ownership is publicly traceable.Let's start looking at the process to create and sell NFTs. To understand the Solidity code you need a basic understanding of solidity or you can check the solidity docs if you don't understand everything.First, an NFT must be created before it can be traded on a marketplace or somewhere else.In the diagram above you can see that the createToken() method calls three methods of the ERC721URIStorage contract that is one of the well tested and maintained contracts from the OpenZeppelin contracts repo here. The createToken() method will receive tokenURI as a parameter that will be the URI to IPFS for example:Within the _safeMint() method the NFT will be generated and the sender will be set as the owner. Afterward, the _setTokenURI() method will set the IPFS URI above as the token URI.Within the _setApprovalForAll() method the market contract will get the allowance to manage the NFT for the owner. This means the market contract will be allowed to sell the NFT and transfer the ownership after receiving payment to the buyer. At least it will return the newItemId starting at 1 and incremented for every new NFT.Next, we need to add the created NFT to the marketplace.After two requirement checks, we will create a new MarketItem and push it to an array that holds all NFTs of the marketplace, then the NFT is transferred to the marketplace, and afterward, we emit an event with the new MarketItem details.Next, we need to receive a list of unsold NFTs a buyer can look at and buy.We read the count of all NFTs the market has, iterate over the items, and check if the market contract is the current owner and if so add it to the items array and at the end return all unsold items.At least we want to sell the item and transfer the ownership to the buyer.The sellItemAndTransferOwnership() method has a modifier that will prevent the contract can calling itself, directly or indirectly. We require that the sender needs to pay the exact asking price. We transfer the NFT from the marketplace to the buyer and send the paid amount to the owner of the NFT.That was the main process.We have additional methods for getting the details of the market item by given id with getMarketItemById() and showing the NFTs by owner/creator with getItemsByOwner().An interesting helper method is getLatestPrice() that you can call on the market contract. It is inherited from the PriceConsumerV3 contract. Here we pass the address 0xd0D5e3DB44DE05E9F294BB0a3bEEaF030DE24Ada to the AggregatorV3Interface this address is the USD price of MATIC on the Polygon Testnet (Mumbai).If you deploy these contracts to another network and/or wanna use different price feeds you can check the chainlink docs for the address you need to add to AggregatorV3Interface.Here is a pseudo-code example of how to calculate the price for your NFTs and show the current USD value:When deploying these contracts you first need to deploy the market contract and afterward when deploying the NFT contract pass the market contract address to it.Here is a diagram with the (almost) complete process.Now we want to deploy our contracts and run some methods via the Remix IDE. You can get the complete code from here.Open up the Remix IDE.Now we wanna compile our contracts.If everything went fine you will get a green success icon on the compile tab.I am using Chrome and the Metamask Extension for deployment. You can download and install Metamask here. Afterward, open Metamask and follow the presented steps to set up Metamask.Please add the Polygon Mumbai-Testnet to Metamask.You can check the Polygon docs or follow the next two steps here.Open Metamask and:A new tab opened up in your Browser to Add a network.Now you need to get some test MATIC for deployment fees. You can get them here. Just paste your Account Address as Wallet Address and after submitting you will receive MATIC for testing, it doesn't have any real value.Now we deploy our contracts.Metamask will pop up and ask you to confirm the deployment, please click on confirm. After a few moments, you will see an entry under Deployed Contracts.Next, we deploy the NFT contract.Congratulations you deployed everything for interacting with the blockchain for running your NFT marketplace.Now we wanna run the methods we were going through theoretically before.If you put 1 into the tokenURI field on the bottom of the NFT contract panel you can check if everything worked you will receive the same IPFS URI back.Now we add the NFT to the Marketplace.The red on this button means that the method is marked as payable and you need to send (in this case some MATIC) to these methods.Thank you for reading. If you have questions, you are welcome to write a comment.",,6,6,5,1,1154,700,14,8,0,14,230
Design principles for web3,UX Collective,https://uxdesign.cc/designing-for-web-3-0-53ea939ac66,Angela Ching,395,14,1684,"So everyone and their cat seems to be talking about blockchain. More importantly, it’s on the radar of huge companies, and we know that where there’s money, there’s traction. However, as a designer what I really wanted to know was what this meant for user experience and design in web3. For the record, I’m mainly concentrating around the blockchain space — specifically, design around dApps (which stands for decentralised applications and are basically apps as we know it, but built off the Ethereum blockchain)What does it mean to design for web 3? What practices can be brought over from web 2.0? What are the new challenges and key considerations that designers have to be cognizant of? How does the evolving space of web 3 impact on the role of design? In other words, how does one prepare themselves for this new revolution if it was here to stay?This article is formed from an analysis of design case studies from DeepWork+ a lot of consuming various content (podcasts, videos, articles) of experts + my own opinions based off my journey breaking into the space and experience with dApps.No one asked for a history lesson but here it is because each internet evolution has fundamental differences in the way people used the web (and therefore how design for users was done). Learning the evolution of something and the different characteristics of each phase gives us an insight into the design challenges faced then. Take for example, the first automobile in 1886 and current-day self-driving Teslas. The design considerations between both would’ve been massively different because of contextual differences. The same thing applies here for the web.Web 1.0 was characterised by users being passive consumers of content. There were limited creators (developers), pages were ‘designed’ in a static environment.Web 2.0 (aka the internet largely, right now) is characterised by heavier user participation, interaction and personalisation. No longer were users just passively consuming, they were creating, sharing, and uploading in dynamic environment. Think apps, podcasting, social media, blogging, tweeting etc.The ability for us to so effortlessly navigate the internet this way was due to the success of a small number of software companies (think Twitter, Facebook, Instagram) with incredibly easy platforms. As more people hopped onto the internet and began using it for everything and anything, design started to take another meaning. Companies cared not just about aesthetics but also the whole ‘experience’ someone had when navigating their platform and services. Large tech firms went from focusing on user growth to monetizing users’ activities, the internet became more centralised dominated by a few large monopolies and we began to question the ethics of their data usage, security and privacy.Web 3.0 (aka what everyone’s hyping up) aims to solve a lot of these monopolistic problems, with the spotlight on decentralisation. A lot of apps built on blockchain (known as dApps) take with it the characteristics that blockchain embodies such as openness, security, fair distribution, community-driven and self-governing. Without getting too deep into it, essentially it means a landscape where overlords (investors/CEOs) won’t have concentrated power and it’ll be distributed amongst us, the plebians, the average joe who aren’t rolling in cash. You’re probably noticing that it’s a whole new way of doing things compared to web 2.0 — we’re moving from company-owned internet platforms to community-owned internet platforms.Design plays an incredibly important role in the uptake and adoption of web3. As most users will be onboarded into the world of blockchain through dApps, how they journey through this technical space can be heavily hindered or supported by design.Similar to normal web 2.0 apps, design considerations will change slightly depending on the dApp type (e.g. finance vs gaming vs arts and collectibles). However, due to the nature and current stage of blockchain adoption, there’s a few generalised design principles to consider that I talk about more in-depth below: educating through design, fostering trust and irreversibility.One thing that’s glaringly obvious about the state of web 3 is that it’ll require new mental models and ways of thinking that people don’t generally have (yet). As people learn about blockchain and its potential, it’s on the onus of designers to help people overcome the learning curve of blockchain. Just like it’s our job to communicate the value of SaaS products or services in an easily digestible manner, we’ll need to channel our product design knowledge from 2.0 and transfer it into web3 so that the masses can easily grasp fundamental principles around security, trust, and process. As Denelle Dixon, CEO of Stellar XRP says:“…One of the things that we need to do better in this industry, and I think we’re working in that direction is much like the early days of the web, we need to focus on consumer-oriented products that have a lot of information about the challenges, and also brings the person through from a literacy standpoint. So they understand you look at user experience, you look at UX design, all of these things are really, really important. And as we saw in the early days of the web, it happened, it came together. We became better at educating the audience about what’s available and what’s out there.”As blockchain becomes more mainstream, education will be needed less. But for now, companies are rightfully concentrating efforts in this space. A look at a few product designer role touts the designer’s mission in helping the average person understand blockchain.As designers, helping educate can be done by:The importance of this lies partially because of the newness of the tech, but also because trust and transparency are key characteristics of blockchain since it’s decentralised. Expectations should be managed by communicating capabilities clearly to users.Being clear about securityOften words like ‘decentralisation’ and ‘open-source’ can instill a sense of false security into users that blockchain is impenetrable. However, the level of security behind dApps lies in the strength of its blockchain code, and the 2016 DAO hack on Ethereum proved that hacks can in fact happen. Tell users about security measures taken e.g. if your smart contracts have been audited, as this helps instils trust in the protocol. Allow technical users to delve deeper into docs. that outline security measures and fallbacks, but make it easy for the average user to understand just how secure it is without relying on your word for it.Transparency of transactionsParticularly for De-Fi dApps that host many transactions, all relevant information regarding transactions should be available for users. For design this includes:A big part of blockchain is the immutability and self-governing aspect. From transactions to sign-ins, certain steps during user tasks need to be called out if it’s irreversible or outside the control of the dApp (since it’s decentralised). Benefits of this include 1) helping support education as the repeated emphasis on irreversibility helps people develop their mental mode in the blockchain space, and 2) generally being ethical, especially where livelihoods can be impacted or changed instantly through accidental taps and swipes (on that note, use interactions and gestures that are less prone to mistakes, e.g. swiping/sliding instead of tapping for a transaction to go through)There’s a lot left to be discussed around user experience problems for the blockchain itself that is more backend based -security and performance issues to name a few-, but which also affects user experience. Many innovative companies are trying to solve them right now, as poor user experience can easily be a deterrent for many entering the space. (One of my favourites right now are the guys over at Immutable)As designers, just like it’s incredibly useful in web 2.0 to understand the problems and limitations that our developers face when developing backend and frontend, it’s even more important in web 3.0 to understand the mechanics of blockchain. Doing so helps us empathise and foster communication with developers, and averting the need to rely on them to tell us what’s possible and design based on that. Communication is half the battle in design, and being able to speak the same language as the stakeholders around us means design doesn’t come as an afterthought.You might’ve noticed that there’s actually a lot of 2.0 design patterns in the following — which is good news. As people become more familiar with the tech, just like how people became more familiar with web 2.0 capabilities, design slowly matured from text hyperlink dominant pages to the sleek pages we have now. I assume web 3.0 will go through the same phase as it makes its way into mass adoption.As for where it’s all headed now? 2021s been a rollercoaster for web 3.0 and this article I feel like barely even scratches the surface of it all. Whether this is a phase or not, the possibilities of blockchain have us questioning legacy ways of doing things and to me, well, that’s always a good thing.✨Deep Work Studio✨Giving this first place and a shout out as Deep Work Studio is a literal goldmine of web3 related design case studies giving you full in-depth access to their facilitation sessions, UI links and research reports. I did a lot of analysis on their current case studies reviewing their themes to understand considerations for dApps.deepwork.studioBeltran — Web3 Design PrinciplesA literal bible and deeper look into the nitty grittys around web3 design (long but worth it read!)Sarah Mills — Blockchain Design PrinciplesAlso a really great read from IBM designers who’ve worked on blockchain projectsConsenSys Media — Design TabThe company behind MetaMask, this sections dedicated to designing on blockchain.2. Blockchain in generalMIT Introduction to Blockchain CourseGary Gensler is probably one of the best (if not, the best) lecturers I’ve come across. Would recommend watching all, but if not then at least finish lecture 11 on Blockchain Economics3Blue1Brown — But how does bitcoin actually workA great comprehensive 11-minute video that’s really easy to understand. One of the best breakdowns I’ve seen.Crypto Congress 2020A lot of different viewpoints from key players in the crypto world explaining it to congress that aren’t so well versed in the space.3. Plugging a dApp I’ve really liked so far (it’s a wallet fyi)rainbow.me👋 I’m still early in the learning curve, so any feedback/comments/resources are always appreciated!",,17,0,27,3,1142,718,16,11,0,34,1600
,,https://medium.com/u/eb0a3e13b95e?source=post_page-----c35fc29ccf6--------------------------------,Katia Gil Guzman,139,,,,,,0,,,,,,,0,,
,,https://medium.com/u/30da00d739b7?source=post_page-----c35fc29ccf6--------------------------------,Glamredhel,16,,,,,,0,,,,,,,0,,
Read This Before Hiring Another Senior Software Engineer,,https://lauratacho.medium.com/read-this-before-hiring-another-senior-software-engineer-ab436afc2447,Laura Tacho,63,5,1150,"This is Part 4 in a five part series on building diverse software engineering teams. Read Part 3, Recruiting More Underrepresented Candidates Won’t Magically Make Your Organisation Inclusive, here.I’ve met with dozens of engineering managers who lead teams made up of exclusively senior software engineers, and who are looking to hire only senior engineers. You might lead a team like this, or have been on one in the past.I noticed that these leaders disproportionately struggle with recruiting and retaining diverse talent. There are some other trends too — like needing more coaching on communication-based conflicts, generally speaking — but let’s focus on hiring for now.Conversely, leaders who make space on their teams for mid-level and junior talent consistently report more momentum when hiring, and better diversity metrics. This is not just true for tech, but across most industries. Of the dozen engineering leaders I interviewed for this blog post series, just two reported that they felt satisfied with the gender and racial diversity on their team. These were the only two leaders who had invested in hiring folks a bit earlier in their careers and creating opportunities for them to level up.Let’s be clear that seniority and a diverse team are not mutually exclusive, and there are many very senior technologists that come from underrepresented communities in tech. But, it is harder to find them, as the overall number of senior engineers is smaller than the overall population of software engineers on the hiring market. If you do find them, they might not want to work for you.Beyond addressable market, there are some “smells” when it comes to hiring senior engineers that have a negative impact on diversity and inclusion, and may work against you landing that amazing senior engineer from an underrepresented background. Senior is often times not defined, especially at earlier-stage companies, and it can often be used as a stand in to mean “hire more people who have the same background and experiences as the current engineers.” This opens the door for the hiring panel to define senior for themselves, and give preference to candidates who have similar backgrounds and experiences.The best, most sustainable path to a diverse team with senior talent is by hiring that talent earlier in their career and giving them an opportunity to grow with your company. This helps people from all backgrounds, not just underrepresented ones, while also leading to lower attrition overall.Do you really need a senior engineer with 10+ years of Javascript experience to fix every UI bug?A team consisting of a mix of great senior engineers and very solid mid-level engineers has some unique benefits. As mentioned above, these types of teams are generally indexing higher on diversity, which has quantifiable business advantages. Additionally, teams with more than one level have a build-in leadership pipeline, where early career employees are levelling up to become senior, but they’re taking all of the context of your company with them. They also execute more efficiently by allowing senior engineers to focus on the biggest problems that benefit most from their years of experience. Having mixed levelling is also a forcing factor to define what “senior” really means at your company, and lay out a path for people to get there.Listen, we all want to believe we show up to work to solve the hardest problems. That’s unfortunately probably not the case for most of us, so teams often do overestimate the complexity of the problems they need to solve. A team full of senior engineers is great and appropriate in some situations where the tech is complex and the business problems are still being defined. But for an established software product with a finite amount of technical decisions to be made, bugs and maintenance might make up more work than greenfield projects. Each type of project requires different skills, and some of those projects are very well-suited for a confident and eager early career engineer.Teams need to reflect the complexity of the problems they need to solve. In the same way a restaurant can’t function without a chef de cuisine, line cooks, and someone to pour the wine, the type of work an average engineering team needs to get done in a normal sprint doesn’t always require a team full of senior engineers.There are a lot of myths about building a high performing team, some that are so ingrained in the way engineering teams are built that we may not even recognize that we believe them, too.First, every employee, even a seasoned senior engineer, needs and deserves good management. Teams often look to hire exclusively senior engineers in order to reduce management overhead. You may get away with less project management, or needing to spend time breaking down tickets. But even senior employees need feedback, coaching, and opportunities for growth. To think otherwise is a disservice to your team.The same goes for onboarding. Onboarding takes time, and time is something that a lot of teams don’t have a lot of. That’s why they need to hire some more people in the first place. But don’t believe the myth that only a senior engineer can just hit the ground running on day 1, or that they can onboard themselves. It takes time to understand architecture and the dozens of business decisions and tradeoffs that are embedded in your code base. Everyone needs time to get up to speed. Leaving someone to self-onboard will most likely end up in that person feeling lost and unsupported until some pieces start coming into focus.Teaching and mentoring are tools for growth for both the teacher and the learner. It takes a higher level of mastery to teach someone a skill, or just explain it clearly, than to practice it. In teams that I’ve lead, I see fewer bugs and incidents coming from code that’s been developed via pair programming, and especially if that pair has mixed levelling.And while senior engineers undoubtedly have the experience to avoid mistakes and ship code faster, having mixed levels on your teams can actually help your seniors get more done, not less. You can protect the time of your senior folks by putting their brains to work on the problems that need it the most, while leaving more focused problems up to the rest of the team. Senior folks feel like their time is valued, they’ll spend less time context switching, and the mid-level engineers who are hungry to learn will have an abundance of stuff to work on.Up next in this series:Using the wrong reachout methods and messaging for the communities they want to see represented. How can you authentically let a candidate know that DEI is important to your team and company without it coming across as the only reason you’re approaching them?🎉 I’ve just launched a course called High-Performing Software Teams. If you want to know how to define, measure, and reinforce high expectations, this course is for you.",,,0,1,4,1225,867,2,0,0,8,120
,,https://medium.com/u/64bb7221dc1b?source=post_page-----c35fc29ccf6--------------------------------,Razvan Cirlugea,91,,,,,,0,,,,,,,0,,
Resolving the one Product Owner per product conundrum,Serious Scrum,https://medium.com/serious-scrum/resolving-the-one-product-owner-per-product-conundrum-a6b59b0e121a,Maarten Dalmijn,66000,15,3243,"Let’s start by asking some simple questions for a company using Scrum to build a single product with ten teams:The correct answer to all these questions would be: there isn’t a single person accountable.Let me provide the textbook Scrum Guide answers:To prevent possible confusion: accountable and responsible aren’t synonyms.To give a simple example: imagine I need to give an important presentation to the CEO and delegate the responsibility of making the presentation to someone on my team. The person on my team isn’t able to deliver and as a result, I show up empty-handed to my meeting with the CEO and look bad.Do you think the CEO cares why I didn’t have a presentation ready? He holds me accountable, even though someone else didn’t pull through on their responsibility.When someone is accountable, they are ultimately answerable for something. When someone is responsible, they carry responsibility for something happening. When you’re responsible for something happening, it doesn’t automatically mean you’re ultimately answerable (accountable) and vice versa.Now that we have refreshed our memory on the difference between accountability and responsibility, how does this apply to Scrum?In Scrum, a single Scrum Master is appointed as accountable at the team level. Once you go beyond the team level with multiple Scrum Masters working together on the same product, then all Scrum Masters together become accountable for the Scrum implementation at the organizational level.Many accountabilities in Scrum belong to groups of people, which makes sense considering Scrum centers around self-managing teams. Everything concerning delivery is a team effort without a single person being accountable.But even in the world of Scrum, there are exceptions to team accountability. The moment we slap on the word ‘Value’ and begin discussing value delivery, Scrum does a 180-degree turn in terms of accountability. Let’s show this by asking ourselves the following question: who is accountable for maximizing the value of the product?When we talk about maximizing the value of the product, then it mysteriously no longer becomes a team effort. Scrum requires a single set of vocal cords no matter how many teams work on the same product: the Product Owner.Scrum is all about self-managing, empowered teams, yet it always requires a single ‘Value Babysitter’ for the whole product. The same isn’t required for anything else. There is no ‘Scrum Owner’, ‘Architecture Owner’, or ‘Quality Owner’.What’s interesting is that people can simultaneously hold the belief that a single Product Owner for ten teams is an effective solution, but then if you ask if a single Scrum Master can handle ten teams, those same people will claim that you’re crazy.Is Scrum’s requirement to have a single Product Owner per product outdated? Is it at odds with the self-managing nature of Scrum Teams? Is it strange considering all other accountabilities in Scrum except the Product Owner can be shared by multiple people?The answer to all these questions, in my opinion, is yes. Let’s start by digging into the accountability of the Product Owner.Things get even more complicated when you reflect on the fact that Scrum is a framework that helps with the delivery of value. Delivery of value doesn’t exist in a vacuum. Nearly everything the Scrum Team does affects the delivery of value, e.g., their decisions on quality and architecture also affect the value of the product.When you keep these considerations in the back of your head, then it’s safe to say that the Product Owner should have far-reaching tentacles in everything the Scrum Team is doing. Product Ownership isn’t a passive role where you can sit back and let the Scrum Teams work their magic.It doesn’t matter if you work with 1, 10, 100, or 1000 teams. If all of these teams work on a single product, then all that responsibility rests on the shoulders of a single Product Owner.Does a picture of Atlas carrying the world on his shoulders come to mind yet?Why does Scrum go for the Atlas solution of requiring a single Product Owner per product to maximize the delivery of value?The single Product Owner per product is a solution to a particular problem: the desire to have a single interface and value decision-maker for all Scrum Teams working on the same product.The aim of appointing a single person as accountable is to prevent communication issues and long debates around what we should best be doing, resulting in noise and inaction in the Scrum Teams. It forces demanding stakeholders to all go through the same person. Plus, it allows global optimization of the product’s value versus limiting yourself to local optimization at the team level.However, as Scrum grows in popularity worldwide, the original vision for the Product Owner role is beginning to show its age. Companies rarely have a single Product Owner per product in practice.Why is the reality of the Product Owner role different than the theory?Now let me ask a different question: do you think one Product Owner per Product approach scales beyond a handful of teams?It absolutely does not.From personal experience, I have handled a maximum of 5 teams at once, and that’s when I told my manager: “If I need to do this for another six months, I quit”. I was constantly running, stressed, and never completely on top of things. It felt like everything was continuously burning and I always arrived too late.When I talk to other Product Owners, their limit is usually around 2–4 teams, though I have heard of anomalies who claim they can handle 10 teams. But you have to wonder if they really can keep all these balls successfully in the air.I’ve also never worked at a single company with 5 teams or more, where only one Product Owner was in play. I talk to many companies all over the world and none of them tout having a single Product Owner per product there either (unless they have a few teams). I only hear about these illustrious Product Owner superheroes on LinkedIn through comments.Sure, you can chalk this all off as anecdotal evidence, but let’s take a look at different scaling frameworks and how they scale the Product Owner role:All of them provide solutions for the one Product Owner per product conundrum (the SAFe solution sucks though).I believe the one Product Owner per product solution, depending on the context, can be a fantasy that doesn’t work. Why and when doesn’t it work? And what should we be doing instead?To answer these questions, let’s put the Product Owner role aside for now, and let’s think about how Scrum Teams should work together to maximize the delivery of value.Let’s start by turning to our trusty old friend the Scrum Guide. I will quote the relevant passages and break them down into simpler terms.The Product Owner is accountable for maximizing the value of the product resulting from the work of the Scrum Team. How this is done may vary widely across organizations, Scrum Teams, and individuals — Scrum Guide 2020We already covered this before. The Product Owner is accountable for the teams delivering as much value as possible in the product. The next part dives deeper into what this exactly entails:The Product Owner is also accountable for effective Product Backlog management, which includes:Developing and explicitly communicating the Product Goal;Creating and clearly communicating Product Backlog items;Ordering Product Backlog items; and,Ensuring that the Product Backlog is transparent, visible and understood. — Scrum Guide 2020What’s interesting here is that the Product Backlog is defined as follows:The Product Backlog is an emergent, ordered list of what is needed to improve the product. It is the single source of work undertaken by the Scrum Team. — Scrum Guide 2020What’s important to keep in mind, the Product Backlog is what the team will be working on. The Product Backlog doesn’t magically get conjured out of thin air. How do you arrive at an ordered Product Backlog?No answers here in the Scrum Guide.Building a Product Increment is a slow way of getting feedback. There are many steps you should perform before you start building. Scrum doesn’t prescribe these because it’s a framework for delivery, it doesn’t tell you what you do before you deliver something or afterward when you need to validate what you delivered.I want to stress, I’m not claiming you can’t do discovery or validation during a Sprint. I just don’t believe all discovery and validation can and has to happen in the same Sprint where you produce the Product Increment.Ideally, you should try to bring discovery, delivery, and validation as close as possible to each other, but it’s rare that all three are possible in the same Sprint. Especially considering the proof of the pudding often lies in lagging indicators that change over a longer time period than a single Sprint.And then here is where it gets interesting:The Product Owner may do the above work or may delegate the responsibility to others. Regardless, the Product Owner remains accountable. — Scrum Guide 2020Great news! Everything the Product Owner does may be delegated, but then the Scrum Guide starts making things murky again:The Product Owner is one person, not a committee. The Product Owner may represent the needs of many stakeholders in the Product Backlog. Those wanting to change the Product Backlog can do so by trying to convince the Product Owner. — Scrum Guide 2020Wait let me make sure got it right: The Product Backlog management can be delegated, but the Product Owner must be a single person AND when you want to change the Product Backlog, you have to convince a single Product Owner?Does running every change to the Product Backlog through a single Product Owner sound effective to you? It’s a bottleneck by design, which means it isn’t a robust or scalable solution.Could we do better than having a bottleneck by default? Should you really have to convince a single Product Owner whenever you want to change something to the Product Backlog?To be fair, the Scrum Guide also has the following passage that acknowledges the importance of self-managing teams that focus on the delivery of value:“The entire Scrum Team is accountable for creating a valuable, useful Increment every Sprint” — Scrum Guide 2020However, this is limited to the context of the Sprint and the creation of the Product Increment. It is important to be aware that the Product Increment represents the whole Product (including all Sprints before and the current Sprint).Though I can’t be sure, my hunch is there is a clear separation of concerns between the Product Owner ‘maximizing value’ accountability across all Sprints and the Scrum Team accountability to deliver a valuable increment during a single Sprint.I want to stress, since all responsibilities of the Product Owner may be delegated to the Scrum Team, then it’s possible that the Scrum Team is ultimately responsible for maximizing the value of the product. However, they still will never be accountable.If we agree the Product Owner is defined by its accountability and must be a single person, and that we can distribute the responsibilities as we see fit. The next question then becomes, how can you scale the Product Owner role?Let’s walk through three different solutions to the problem.The Scrum Guide defines a product as follows:“A product is a vehicle to deliver value. It has a clear boundary, known stakeholders, well-defined users, or customers. A product could be a service, a physical product, or something more abstract.” — Scrum Guide 2020This definition is so generic you could get away with splitting your Product into different areas and claiming each of them is a ‘Product’. Voila, a new product ! All you need is a clear boundary, and all the stakeholders, users and customers can remain the same.I don’t think splitting a product into artificial sub-products is a good way of solving the problem and I don’t believe it aligns with the intent behind Scrum Guide’s product definition, but I can’t be sure.Splitting a product into artificial sub-products to make scaling possible means you’ll still run in the global optimization of the product vs. local optimization of the sub-products problem.If people are only accountable for parts of the product, then nobody is responsible for the overall product, and as a result, maximizing value over the whole product becomes difficult. As a consequence, I don’t think this solution makes sense unless there is someone who is accountable for the overall Product. Who is accountable for all Product Owners across all products in this scenario?Even in the case where a single product really can be split up into multiple products that interact closely together, then you still run in the same global vs. local value optimization problem. Except it will be on the portfolio level instead of the single product level.In such a scenario, there will be no one accountable for the overall value of how the different products interact with each other, leading to the same issues the single Product Owner per product is supposed to help resolve. These issues just happen at a higher level: the portfolio (multi-product) level instead of the single product level.I don’t believe that being a Product Owner is something you can easily delegate to others. It’s a demanding role. I still struggle with aspects even though I have been performing the role for many years.The multiple Product Owners on the same Product is extremely common. Technically speaking, none of them are Product Owners as they lack the final accountability. However, they are all performing the responsibilities of the role. The single person who is accountable will ultimately be the real Product Owner, at least according to the Scrum Guide.The real Product Owner is usually the person with the most accountability in the Product Team for the product(Chief Product Officer, VP of Product) or sometimes even the CEO. It depends on who is considered ultimately accountable for how the whole product is performing in the organization.From Scrum Guide’s perspective, accountability determines the title of Product Owner. It doesn’t matter who performs the responsibility. However, in the real world, people give the Product Owner title to people who perform the responsibilities that belong to the role.All the Product Management expertise necessary to deliver value is part of the Scrum Team, without any Product Owner to call the shots.I think this scenario happening will be rare. It’s difficult to find a single person who is awesome at all facets of Product Ownership. Product Management is a broad field with a lot of depth. When a whole team takes on parts of the role, you can play to everyone’s strengths and cover more ground than if there is a single person trying to have expertise in all the facets of Product Management.The Scrum Guide defines the Product Owner role as an accountability meant for only a single person per Product. Whoever actually performs the responsibilities doesn’t matter from the perspective of Scrum, as long as they are part of the Scrum Team.However, the big problem is that in the real world everybody who does the responsibilities STILL labels themselves a Product Owner. And I believe that’s a reasonable stance. When you do all the work and cover the responsibilities, why can’t you claim you are a Product Owner?In theory, the Product Owner role is defined by your accountability, in practice, it often is defined by your responsibilities.Let’s also think about this logically: it’s way more important that you have one or more Product Owners that perform the responsibilities rather than a single person who is accountable.If you only have a Product Owner who is accountable but nobody who takes on the responsibilities of the Product Owner, then you will struggle to deliver value. If you have multiple Product Owners who are responsible, but nobody who is accountable, then you can still deliver value. You might make some suboptimal decisions due to being unable to reach an agreement together.Product Management expertise being part of the Scrum Teams is essential, no matter what solution you go for. A choice for Scrum means you are doing complex work and the aim should be to have feedback loops that are as short as possible. This is achieved by having self-managing, empowered Scrum Teams with all the necessary expertise to maximize the delivery of value.Whether you achieve that by having multiple Product Owners, Product Managers part of the Scrum Teams or by simply having teams who can cover all the essential Product Management expertise together doesn’t really matter. The result is the same: the Product Management expertise is part of the Scrum Teams and they have all skills necessary to make short feedback loops towards the delivery of value possible.I believe even the Scrum Guide is unclear about the difference between calling someone with the accountability the Product Owner or someone with the responsibility:Those wanting to change the Product Backlog can do so by trying to convince the Product Owner. — Scrum Guide 2020This sentence doesn’t make sense unless they are talking about those who perform the responsibility of the Product Owner. I can’t imagine they mean that every change in the Product Backlog has to run through a single person regardless of how many teams are working on that Product. Otherwise, this conflicts with the delegation of the Product Owner responsibilities statement also present in the Scrum Guide.When you have a big Product, there are often multiple Product Owners who do all the same work as a Product Owner for a small Product, but they are not allowed to call themselves a Product Owner according to Scrum because they lack the final accountability.Such a Product Owner could easily join a company that is smaller and then suddenly you would be allowed to call yourself a Product Owner, while the only thing that has changed is your accountability.In essence:The flip-flop that happens based on the number of Scrum Teams working on the same product is highly confusing. The distinction between accountability and responsibility is already difficult to understand, let alone that depending on the context, sometimes responsibility and accountability belong together in the Product Owner role, and sometimes they do not.I believe the Scrum Guide should be amended to make the distinction clearer. Here’s what I would propose to change in the Scrum Guide, let’s label it pragmatic Product Ownership for now:Allow a single product to have multiple Product Owners. However, allow only a single Product Owner to be designated as accountable. Make it clear this person is meant to resolve disagreements and make sure we go for a global optimum in the delivery of value with our product and not a local optimum.In the real world, large products already have multiple Product Owners working on them. We can try to deny this and wave our Scrum Guide in front of them, but I believe that won’t change. With this solution, you assign one of them as accountable and the problem is solved. Ideally, you let them self-organize to figure that out but your org chart may not permit that.Scaling the Product Owner role is a big problem for companies. The Product Owner's responsibilities can’t be easily pawned off to teams who lack the expertise or knowledge to carry the burden of the role.Without considering the situational nature of the Product Owner role, the rift between the theory of Scrum and the real-world application will grow. People don’t get the Product Owner role accountability/responsibility mess, as created by the unclear presentation in the Scrum Guide.We can either blame the people or try to make it clearer.Transparency, inspection, and adaptation? Or is that only something that applies to Scrum Teams and not Scrum itself?",,4,0,7,15,865,865,2,5,0,3,461
,,https://medium.com/u/96fbb7e3484b?source=post_page-----c35fc29ccf6--------------------------------,Laura Tacho,63,,,,,,0,,,,,,,0,,
,,https://medium.com/u/d4352eb68115?source=post_page-----c35fc29ccf6--------------------------------,Clive Thompson,22000,,,,,,0,,,,,,,0,,
The Interview — Decoding behavioral questions,,https://razvan-cirlugea.medium.com/the-interview-decoding-behavioral-questions-7c92660f5583,Razvan Cirlugea,91,10,2026,"I’ve heard people calling behavioral questions “formalities” or “the stupid stuff with HR”. Especially programmers, who went through tough technical interviews.However, I know programmers that have passed all their technical interviews but failed the behavioral interview, so did not get the job.But those interviews aren’t all about getting or not getting the job. They are essential for getting a bigger salary or more benefits. Do you think it’s easy to find a technical expert that is also communicative, humble, has a great mentality and a growth mindset? Not at all. So when a company finds someone like this, they’ll do their best to seal the deal.In this article, I’ll translate to you what those behavioral questions really mean and I’ll also give you my answers as an example. Here are the questions that we’ll discuss:Some of those questions might sound silly or trivial, but if you are in the middle of an interview and you’ve never thought about those subjects… your answer might sound sillier. 😛“I am Razvan, I am 25 years old, I am a bearded guy since I was 18 and I am a big fan of Borussia Dortmund”. Is this answer the best one for a software development interview?Don’t get me wrong, nothing from what I’ve said above is terribly wrong, but it is simply not that attractive for the interviewer. Generally, interviewers are looking for passionate, collaborative, open-minded (and so on) candidates. So, when you receive this question, it is your chance to showcase your best qualities. Here is the typical answer that I would give:“Hello, my name is Razvan, I am a Front-end Developer and Instructor. I am really passionate about web development and the learning process, whenever I learn something new I ask myself how I could explain it to someone else. I am a communicative person and I believe that you have things to learn from each person you interact with. I am a continuous learner and I constantly invest time in developing myself, both professionally and personally. My main passions are psychology, neuroscience, sports (especially football) and craft beer.”What are the main points of my answer?So, from the beginning, I’ve made the interviewers think “Hey, this guy has some good qualities”.“My life sucks, I want a well-paid job, so I can live better”. Nobody wants to hear this answer, even if it’s true for a lot of programming jobs.Imagine you’re dating and you ask your date: “Why did you go out with me?”. You do not want to hear a reply like “Oh, I’ve asked 50 other people, but no one replied, so I got nothing better than you…”. The same applies to companies, you want to make them feel important.Read the job announcement before you go to the interview. Search the company. Figure out some strong points that it has. A good generic answer would be the following:“I’ve heard a lot of great things about the company from my acquaintances. I checked the company’s website and I enjoyed its main activities. The technologies from the job description are modern and the projects seem challenging. I feel that there is an environment that would help me develop and contribute to the company’s growth.”Obviously, if you have more particular reasons, tell them. Just make sure that you do not look uninterested, you never know which company gives you the best offer.It is not included in the question, but they want to hear a successfully accomplished task. And they want to hear about a task that is somehow related to what you can encounter at the job you are interviewing for. So, if you are applying for a Software Development Engineer position, it should include some technical stuff.Even if you are applying for your first job, I’m sure you can come up with a task that highlights some important qualities:So basically, choose a situation where you used your problem-solving skills and collaborated with other people to successfully finish your task. Those people can be teammates, colleagues, friends, instructors, it doesn’t matter. Here is my answer:“One challenging task that I’ve had was deprecating Internet Explorer usage for all but some enterprise clients. It was challenging from a few points of view:Communication — The product owner didn’t know exactly what service we could use to except the enterprise clients from deprecation so I had to find out. I spoke with my manager and colleagues and found some possible external teams. I searched people from those teams and messaged them. When I would not receive any answers for a couple of days, I would talk to my manager and he would give some magic messages.Not enough knowledge — After finding out the solution, I realized that I had to write some scripts. I am a front-end developer, so writing scripts was definitely not my best skill. However, I had basic Node.js knowledge, so I did a few google searches, read some documentation and I was able in the end to make some fully functional scripts.New technologies — However, the technical challenges didn’t stop here. To finalize the process, I had to write some PHP code. I had previously written PHP code only once in my life, so it was definitely something difficult to tackle. However, I started searching in the project and I found some similar examples already written. By making analogies and googling a few things, I managed to write some functional PHP code.In the end, the selective Internet Explorer deprecation was completed on time.”The main point of my answer is that I know when and how to collaborate with my peers and that I can solve problems that seem really hard and definitely out of my comfort zone. Surely, I gave you an example from work, but you can think of something similar from your university or boot camp if it is the case.“He parked on my spot and did not leave his number. I lost my mind and started kicking his car.” If this example is not followed by some “I was kidding”, your interview might be over.Conflicts are unavoidable. No matter how much you hate them and try to run from them. How do you handle them? And especially, how do you handle them at work?What I consider to be the most important part here is to give an example where you’ve taken responsibility for your actions and apologized for your part of the responsibility to eventually get back on track with the people involved. Nobody wants a stubborn employee, who would put his ego above everything. Also, being too humble and not defending your points of view is not an option at all. Having a balanced approach is the key. Here is my answer to this question:“​​First of all, I am a non-conflicting person, I try to be as assertive as possible and to take into account the other person’s perspective.A conflicting situation I found myself in was the following: When I was teaching, it was my responsibility to close the classroom and leave the presence in the main office. One night I forgot to do these things, and someone in the office publicly criticized me. I was quite frustrated that my teaching efforts were not appreciated, so my response was sharp, minimizing what I did.The next day I went to the person and apologized for his behavior, taking responsibility for my actions and assuming that I had made a mistake. Later, I communicated my dissatisfaction regarding my teaching efforts and the situation got resolved.”The situation I’ve described above is pretty common: we have multiple sources of stress and sometimes our brain confuses them, so we start arguing for the wrong reasons. After a conflict, reflect on what really annoyed you. The source might be somewhere else. After calming down, go and talk to the other person, admit your part of the responsibility and seek reconciliation.“I love alcohol, wanna hang out those days, drink some whisky?”. Yep, another silly answer, I’m sure you got it. 😛Even though some people would enjoy having a drink with you, others won’t and it’s definitely the type of weakness they do not want to hear. Another ridiculous answer would be “I’m too perfectionist, I try to do everything perfectly”.So how can we be vulnerable without being ridiculous? Here is an example:“One weakness I have is that I tend to avoid tasks that massively take me out of my comfort zone. If I feel that I have very little expertise in a particular subject, I avoid getting involved.I’m working on improving this approach. One thing I’m starting to do is try to increase my knowledge of a particular topic when I know that a task is coming up on that topic. I also aim to be more courageous and regularly take on tasks that take me out of my comfort zone, because I am aware that it helps me grow professionally.”Do you see what I’ve done there? I’ve identified a thing that I can improve at myself and I am actively doing something in that direction. We all have areas that can be improved, there is no doubt about that. Realizing them is the first step towards progress.“Millionaire, owning my own business, on a beach, surrounded by beautiful girls.” Sounds nice, but it would not impress your interviewers.This question might sound silly, but all that it wants to say is “What is your personal and professional development plan?”. Companies want ambitious employees, that want to become better, both as a professional and as a person. And having a plan for that, as well as some goals, really helps. Here is what I would answer to this question:“First of all, I see myself as a much better professional, as I am in a continuous process of professional and personal development. I see myself as a mentor to my colleagues and reliable help, an important person in my team. I do not rule out involvement in management or leadership roles. I see myself sharing the knowledge I have gained, on YouTube and through online courses, so that I can help as many people as possible to learn web development.”The main takeaway of my answer is that I am continuously evolving, and I love to share the thing I have learned from this process with my colleagues, so the entire team would benefit from having me around. Wouldn’t you want such a guy in your team? 😄The worst possible answer here would be “I have no questions”. Also, do not rush to ask about the salary and do not ask silly questions.This is your chance to find out more about the company and the team. It is your chance to find out whether the work environment is right for you. So make the most out of it!Here are some possible questions that you can ask:Oh, and about the salary, you can elegantly ask: “Is this the meeting where we discuss salary and benefits, or will we discuss it in a future one? ”, after asking all the other questions. Generally, the interviewers will come up with this question. Also, do not forget to also discuss the benefits, after/before discussing the salary.I really hope that you have enjoyed this article and that it will help you in your future interviews! The main takeaway would be the following: prepare your behavioral questions before the interview. Reflect on encountered situations that would present the best version of yourself and write them down. Personally, I have a Google Docs file with the most common behavioral questions and my answers to them. Before an interview, I would go through them, to refresh my mind.If you want to learn more about interview preparation, especially about the technical part, check out the Master the Coding Interview: Data Structures + Algorithms course. And if you want to prepare for interviews at Google, Amazon, or Apple, make sure you check out the Master the Coding Interview: Big Tech (FAANG) Interviews course. 👀Did you encounter tough behavioral questions during your interviews? Send them to razvan.cirlugea@gmail.com. Maybe I’ll make the second part of this article. Also, if you’re interested in web development, make sure you follow me on LinkedIn and Twitter.Good luck with your next interviews! 💪🏻",,4,0,38,20,1225,817,1,4,0,7,914
,,https://medium.com/u/9ed13eb13fe1?source=post_page-----c35fc29ccf6--------------------------------,Andy Watt,727,,,,,,0,,,,,,,0,,
Make your Scrum team like the Justice League,Serious Scrum,https://medium.com/serious-scrum/make-your-scrum-team-like-the-justice-league-cce4a8ca070a,Sander Dur,3300,7,643,"Let’s face it, we could never make any of our products alone. We need an awesome team to do so. Even though some people like to think they can be a one-man army. Those people will stand strong for a little while, ultimately fading to the background.Every single team I have worked with to date had their individual superpowers, but none could make it with a single guy or girl standing out. In a previous article I discuss why, for instance, a Super Hero Scrum Master could be a burden. In this article, I want to take you along the Justice League type of teams I’ve had the pleasure of working with so far. Bare with me, as I’ll tell you why at the end.Clark Kent, better known as Superman, is not from this planet. Coming from the planet Krypton, he now resides on our puny planet. Living amongst us mere mortals, protecting us from things we shouldn’t know even exists. For instance, horrible decisions that have been made developing the code of the latest Increment of beautiful software (all neatly polished by the time of release, of course).Let’s look at other abilities our Superman has:Potentially damaging superpowersBruce Wayne is the protector of Gotham City. Not the hero we deserve, but the hero we need. Playboy billionaire extraordinaire has a large network of influential people, and he knows how to use them. Typically a Product Owner, there are things we can see in daylight, but it’s in the dark where he shines brightest, protecting the team of Gotham City from enemy threat.Abilities:Potentially damaging superpowersI’m 100% confident that you’ve met Aquaman before. As we ourselves, in contrantraty to Aquaman, cannot breath under water, we only see him rarely. Once you’ve met him, he dives under water, not to be seen until the end of the Sprint. You don’t know what exactly went on down there, but he got some things done.Abilities:Potentially damaging superpowersWonder Woman is a presence that balances out all the rest of the team. Every team needs a Wonder Woman. Challenging the status quo, she is defying the pop culture within the business.Abilities:Potentially damaging superpowersI’ve been watching the Flash Netflix series ever since the beginning (God knows why. I should’ve stopped long ago). Barry Allen is a young guy that acquired superspeed, allowing him to run faster than anyone else.Abilities:Potentially damaging superpowersVictor Stone, aka Cyborg, is a scientist that uses himself as his own test subject. No tests have been conducted that he has not created himself (and with a fallback plan readily available)! His tests don’t always make him the most popular guy around, but he knows it’s essential for the sake of knowledge to persevere.Abilities:Potentially damaging superpowersNow we’ve gone through some of the greatest comic heroes there are. But what’s the purpose, you ask? This group together is called the “Justice League” by DC. Even though it is a comic, they are a lot stronger than they are alone. They are complementary to each other, supporting each other’s weaknesses. Batman (or rather Bruce Wayne) is more politically talented than Superman, The Flash can support Wonder Woman with empathy, and so on. Each hero has its unique talents as well.I like to look at my teams as a “Justice League” team, too. What are the skills we need to be able to save the world (in other words, create a “Done” Increment)? What are our weaknesses and how can we mitigate those? And everyone on our crew is a superhero. This is what cross-functionality looks like. You can be T-shaped, pi-shaped, or shaped like a car tire, ultimately it’s most important that we as a team are able to deliver what is needed. My local football club wouldn't win the league with a single Messi either.What superheroes are in your teams? What has your best “saving the world” moment been like?",,,0,35,1,1180,590,1,12,0,2,65
Is the First Agile Manifesto Principle as Easy as It Sounds?,Better Programming,https://betterprogramming.pub/have-you-read-the-agile-manifesto-9785ce876078,Andy Watt,727,6,1212,"Hands up how many of us in here work in an Agile team? (All hands go up)Hands up who has read the Agile Manifesto (Some hands go up)Hands up who believes that the management and directors of their company have read the Agile Manifesto (No hands go up)The agile manifesto is not a long read — it is literally a page with a few bullet points on it. Have you read it? Have you understood it? Do you actually adhere to the principles in your day-to-day behaviours?I think that the processes and principles described in the Agile manifesto are a fantastic way to approach a software project. I also think (sadly) that Agile has failed. Not because it’s a bad idea — quite the opposite. I think that it has failed because, to be blunt, no one has actually bothered to read and understand what is in the Agile Manifesto.Agile done right can (not will) improve the outlook for a project, depending on the project, the client, the team, and a lot of other ‘stuff’. Good agile is a nudge in the right direction, rather than a panacea that will guarantee your project succeeds. Agile done wrong will (not can) sink your project into an abyss of mismanagement, blown budgets, missed deadlines, and irritated team members!There are twelve principles listed in the Agile manifesto. Over the next few weeks I’m going to go through them all, one at a time, and try to think about what is required for a team, an individual, or an organization to really live the principles.In this post, I’ll start with the first — namely:Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.As with every one of the agile principles, this is intuitively a good idea. “Keep your customers satisfied” has been foundational advice in all businesses for all of the time. Likewise, the benefit in timeous delivery of whatever product and service is being supplied is not hard to understand!Unfortunately, applying this basic and sound advice to a software project is much more involved and much more challenging than it may at first seem. In this post, I’ll talk about the underlying complexity in this, and suggest how to approach living this principle.One immediate challenge is that often a customer may not know what exactly they want at the start of a project — the customer's understanding of their own requirements will mature as the project progresses. Also often, customers may not be aware of the limitations and the possibilities of the available tech.Finally, delivering a software project may well take years, and from start to finish, the market may have fundamentally changed. What satisfies the customer at the start of the project, may not satisfy them at the (rather important) deliver date.In the above, there is also an assumption that the software company is talking regularly with their customers… which is not always the case. I know that I am not alone in my experience of managers simply detailing wish lists of specifications that have never been run by a customer.How can you satisfy a customer that you have never talked to?So while this sounds really basic and really easy, it is not. There are foundational shifts required in the way that an organization approaches its business required to really ‘live’ this principle.For a start, if they are not already, then it is essential that the business engage directly with their customers, seek guidance and feedback, log and process that feedback in some way, and then change the course of the project based on that feedback.This continuous and open dialogue with the customers and users of a system is vitally important, but is often ignored completely. This cannot be done hap hazardously. There has to be a regular scheduled feedback that follows a structure of some kind so that it can be checked and compared and monitored over the whole life of the project — starting before a single line of code has been written!The first few weeks of development are foundational and will set the tone for the rest of the project, and so this process has to be in place way in advance of the development process starting. The process has to continue throughout the project with constant and rapid shifts made based on the ever-changing requirements of the customer base, and the market into which the product is being sold.All is not lost, of course, if this process is being retrofitted to a brownfield project. The principles remain the same, but there is less flexibility to make changes to existing code, so the benefits are diminished (or maybe will take longer to realize).My strongest recommendation for progressing this principle in your team is to prioritize recruiting a UX resource. UX is often considered to be ‘just’ making wireframes, but this is a serious misunderstanding of the role of UX.‘UX’ stands for User eXperience, and so their role covers all aspects of how a user interacts with the system. Far far more than just wireframes and mockups. A good UX resource will fundamentally understand this agile principle and will bring your team in line almost by definition — the benefits of a good UX resource cannot be overstated.Even assuming that a UX resource has been hired, the customers are being consulted, and the feedback is gathered, incorporated, and built in to the product, there is a very important ‘second part’ to this agile principle. The second part states that customers are satisfied “through early and continuous delivery of valuable software”. This can be done manually, but these days meeting this requirement will mean building a CI/CD pipeline.Building a CI/CD pipeline is primarily a technical challenge. While it is a fairly well-understood process these days, that does not make it technically easy to do. There is a time cost associated with building and maintaining a CI/CD pipeline, and the is often an ongoing financial cost as well.The biggest challenge that I have experienced with CI/CD is convincing the management to support the efforts. Time spent on pipelines is time not spent on features. Despite the obvious importance of this, getting authorization to spend the time is not always easy.The traditional roughly annual release model means that ‘good ideas’ are stuck in a repo for up to a year before they get into the hands of a customer. Good ideas should be adding value almost as soon as they are ready.It’s very hard to make a specific recommendation on CI/CD beyond ‘you should do it’, because every project is very different, and there are a million ways to actually build it. I would definitely recommend investing in CI/CD, whatever the specific implementation ends up being — in my experience the investment will be paid back in spades in the form of satisfied customers.In my opinion, this is the first and most important item on the list of twelve in the agile manifesto, and is the one that is the least well adhered to in practice! There is significant low hanging fruit to be had here so… what are you waiting for!!The second principle in the agile manifesto is:Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage.Check back here shortly for it.",,,1,3,8,1729,1153,3,0,0,6,53
Why Open-Source Developers Are Burning Out,Better Programming,https://betterprogramming.pub/why-open-source-developers-are-burning-out-1a860854884c,Clive Thompson,22000,9,1828,"There’s a software library called “colors”, which does one simple thing: It lets programmers change the color of text in Node apps.It’s quite popular; around 19,000 software projects currently use it, and it has over 20 million weekly downloads from people trying it out.But recently, anyone using “colors” in their app got a shock when they updated that software library. Their screens all went haywire — producing the stream of nonsense text you see above.What happened? Well, Marak Squires, the software developer for “colors”, had intentionally sabotaged his own library of code. He added an infinite loop that generated gibberish characters, utterly b0rking the functionality of “colors”.And … why would he do that? It’s not fully clear; he may, as observers have noted, been having any number of mental-health issues.But one thing Squires had recently made clear is that he was feeling burned out and economically exploited, because so many profitable companies were using his code, and not giving anything back. As he wrote in November 2020 …Respectfully, I am no longer going to support Fortune 500s ( and other smaller sized companies ) with my free work.There isn’t much else to say.Take this as an opportunity to send me a six figure yearly contract or fork the project and have someone else work on it.Again, there may well be a lot of reasons why Squires decided to blow up his repo, as more on this episode comes to light. But after the “colors” brouhaha, many developers resonated with this complaint: They, too, were doing too much work, and not seeing enough benefit.So they’re walking away. Literally! They’re abandoning their projects. Studies suggest that perhaps 9.5 percent of all open-source projects are abandoned, and a quarter is probably close to being so.This is causing serious problems in the world of software, because code is highly interdependent. These days, nearly every application you use was built using many, many different open-source libraries. If one of those libraries isn’t being updated, it risks causing problems and bugs downstream. A surprising amount of mission-critical code was created — and is being maintained — by people like Squires.As the famous XDCD cartoon “Dependency” puts it …This is why the burnout of open-source developers is a big deal.But what’s causing it? Why are so many getting burned out?The best study of this is Working in Public, a 2020 book by Nadia Eghbal. She interviewed hundreds of open-source creators to sketch out the contours of their work — why they do it, what they love about it, and (most crucially) how the work becomes Sisyphean.I’d encourage everyone to read the whole book — it’s brilliant. But in summary, here are Eghbal’s main findings:One original key motivation behind the open-source movement — epitomized in the creation of Linux — is the idea that “many hands make light work”. If someone gets the stone-soup cooking, others will join in. If you create a rough working first version of a piece of code, other developers who use it will show up and offer improvements. And best of all, being open means your project will be more bug-free, since “given enough eyeballs, all bugs are shallow”.So, open source was supposed to be all about Amish barn-raisings.Twenty years into the open-source era, it hasn’t worked out that way. Eghbal found that only a minority of projects, perhaps 3%, resemble true barn-raisings — or “federations”, as she dubbed them — with lots and lots of contributors pitching in. (Some examples are Linux, Node, or Rust.)In contrast, the great majority of open source projects are run by tiny teams, and often only one lone person. Outside contributions are pretty minor, and limited to a tiny one-line syntatic bug fix. That’s not nothing! Those little contributions are great.But the upshot is, those lone coders wind up doing the lion’s share of the work. “Open source inexplicably skewed from a collaborative to a solo endeavor” as Eghbal writes.This becomes a real problem if an open-source library becomes really popular. Because then, for the core “maintainer” …Influencers on Instagram or YouTube or Tiktok have pretty miserable lives. They’re constantly having to manage their audience: Lashed to the mast of content productivity, they need to keep cranking out material to keep their fickle fans entertained, and they’re also expected to reply to the Niagaran stream of messages and requests from their base. Burnout in the creator/influencer economic is rampant.What Eghbal realized is that open-source developers suffer from eerily similar dynamics.When a project is run by one person (or maybe two, or a tiny team) and becomes popular and massively relied-upon, suddenly those lone maintainers are — in effect — playing to a “stadium” of fans, as Eghbal puts it. Their users (who are, of course, software developers themselves) flood them with requests for new features and complaints about bugs; they also send fan notes, which are nice but which, socially, require a response.The upshot is, the developers have to do a ton of social work and fan-base management. This isn’t what they signed up for; they may not be good at it. Hell, they initially started an open-source project because they love to write code, right? Now they’re spending long hours writing emails to users.Nolan Lawson, the maker of PouchDB, told Eghbal that it’s like managing a customer-line of “a few hundred people” …After a while, you’ve gone through ten or twenty people like this. There are still more than a hundred waiting in line. But by now you’re feeling exhausted; each person has either had a complaint, a question, or a request for enhancement.But there’s still code be slung! Indeed, once a project becomes popular, developers also feel the pressure to constantly update and improve it.When I wrote about Eghbal’s book for Wired, I spoke to Jacob Thornton, one of the cocreators of Bootstrap, the wildly popular framework for laying out websites. He told me that the success of Bootstrap left him feeling …… inundated. Countless people wrote him and Otto every week with bug reports, demands for new features, questions, praise. Thornton would finish his day job and then spend four or five hours every night frantically working on Bootstrap — managing queries, writing new code. “I couldn’t grab dinner with someone after work,” he says, because he felt like he’d be letting users down: I shouldn’t be out enjoying myself. I should be working on Bootstrap!“The feeling that I had was guilt,” he says.Now, this incessant workload might be easier if the barn-raising model were truly common. That way, the work would be spread around dozens of contributors.But barn-raisings aren’t common. Why not?The cognitive labor of software can be awfully hard to split between people. This is something many developers told me when I wrote Coders, my book on the culture of software engineering. Managing code requires a lot of mental castle-building, and it’s hard to figure out how to divvy that up between multiple brains. It’s why developers often take on more than they should. It’s hard to delegate.Large companies managing millions of lines of code need to figure this out, of course, so they do. But a big firm can pay managers to wrangle the overhead — the frequent communication that a big project entails, the incessant touching of base. A small unpaid project, in contrast, rarely has anyone willing or able to do that stuff. It feels easier for the maintainers to do all the work themselves, so they do … and, boom, burnout.Even if an open-source maintainer successfully does manage to integrate many small contributors, that just adds to that towering pile of daily email that Lawson of PouchDB described. Popularity can be a curse, even when you’ve got people pitching in. As one developer told Eghbal …it’s not fun anymore, you get literally nothing from maintaining a popular package.On top of this …Again, much like a popular influencer on YouTube or Instagram, the maintainer of a popular open-source package can wind up fielding a lot of abuse. Users write pissy notes complaining when something doesn’t work. They ask for weird features gnarlily specific to their edge-case personal needs, then get aggro if those features aren’t implemented immediately. It’s another log on the burn-pit of open-source burnout.And on top of all this …Sure, some creators of popular open-source projects find a way to make a living off it. Maybe they get well-paid gigs consulting on their library. Maybe they’re hired by a firm that created and uses the library, the way Facebook employs React developers.But as Eghbal found, these are rare, rare cases. The great majority of “stadium” developers aren’t making much, or even a penny, from their labor.To be sure, they didn’t necessarily expect, at the outset, to make any money. A lot of open-source libraries are created because a developer wants to solve their own problem. They’ll write the code anyway. But then they share it with the world because of a constellation of motivations, not limited to:i) The ecological generosity of developers (“I use lots of other people’s open-source code, so I should share mine back”);ii) the fun of watching your code be used worldwide;iii) the positive reputation and CV-burnishing that comes from being known as the creator of an awesome software library;iv) the sheer intrinsic joy of writing and shipping code.These are all wonderful things, and they’re part of why open-source software has blossomed.But if a developer suddenly finds themselves playing to a stadium — and swallowed up by the time-sucking demands thereof — then not getting paid can become a serious problem. There aren’t enough hours in the day. Plus, watching wildly profitable and highly-funded tech firms use their code without contributing anything back: That can just feel like a moral insult.As Eghbal quotes Ben Lesh …Open source is such a strange thing. The open source work I do is clearly the most impactful work I do, but no one wants to pay me to work on it.”It’s not hard to see why this can lead to bitterness and burnout.Nobody really knows what to do about open-source developer burnout.At least, in my research, I’ve never encountered a well-agreed-upon palliative. There’s a ton of disagreement, in fact. When I reported on this for Wired, some developers thought the solution was to create more well-funded foundations — like the Linux Foundation — that could pay open-source developers for their time. Others said for-profit companies should be pressured into giving back. Still other disagreed, arguing that money should be kept out of open-source development as much as possible, since it’s liable to poison the gift-economy dynamics that keep the grassroots flowering. And some told me they don’t see a problem in the first place: If a developer burns out and walks away from their library (or torches it), that’s their prerogative; and since open source is an economy of freely-given code, it’s caveat emptor the whole way down, for both creators and consumers.I’m not gonna pretend I have a solution here. It’s a pretty thorny problem.",,12,1,1,14,959,641,6,0,0,23,831
Improving Sprint Predictability in Agile Scrum,Better Programming,https://betterprogramming.pub/improving-sprint-predictability-in-agile-scrum-611e2d1081ad,Glamredhel,16,11,1724,"After the successful application of the framework mentioned in “Demystifying T-Shirt size project estimations”, we have used the approach with another team. After several sprints, the team’s sprint predictability has improved considerably.In the first four sprints the team results were on average (in Story-Points per sprint) 0 / 8 / 20 — fully delivered, partially delivered, committed to.YES. You read that right. Four sprints with nothing fully delivered and overall the tickets were less than half-complete.In the following four sprints, the team improved to achieve: 11 / 19 / 18 — fully delivered, partially delivered, committed to. Whereas in the last two sprints the committed and fully delivered amount of SPs matched exactly.The team not only delivered what they committed to. They delivered more.This article describes some considerations that we had over the course of several two-week-long sprints and the overall progress observed.If a picture says more than a thousand words — then here’s more than four thousand to convince you to spend the next few minutes reading:Example sprint burn-down charts before discussing changes with the team:Example sprint burn-down charts in the first sprints after we discussed the changes:On a bit of a personal note — writing this article feels pretty peculiar because some of the findings and experiences seem like they are taken from a “Captain Obvious” meme.Things so obvious, one might expect not to even have to explain them. But still, they do happen. People may know the principles and yet, still face difficulties, they simply might not see a better solution.It’s not the first, nor is it the last time. We’ve decided to share our insights because you may recognize some of the situations and problems that you are struggling with, maybe you can see some better approaches, in the very least — you will have arguments that you can use as to why not to do things in some ways.Let’s delve into the problems and the proposed solutions.We were interested in improving the reliability of the team’s commitments. After several sprints, during which the team was not delivering what is committed to, the cooperating business team was growing frustrated with the lack of predictability, tension was building up. Things needed to change.The team already had a pretty good understanding of what Scrum is about. The ceremonies were set in place and attended accordingly. The stories were being planned, refined, estimated. The team wanted to deliver the items and was dedicated to them. The cogs and gears were there and apparently crafted according to the plan.Yet, somehow, after all said and done — it all just didn’t work together as it should.The problem consisted of several components, all of them intertwined with each other:The handling of most of these topics is described by basic Scrum principles. However, based on the discussions with colleagues about their past and current experiences — it seems that these principles are not being applied in a successful way in a considerably large number of cases.One simple example of this is a movement of “no estimates” proponents (see an example video here). This article is not meant to vouch for or against estimates specifically — the aim is to show how to use them so that they can work.During the discussions with colleagues, the problems with applying Scrum was reflected by some interesting comments like:“Having worked … years with Agile and Scrum have taught me one thing — it never exactly works. It’s not an exact science.”Let’s focus on this one comment first. Because the thing is — Agile Scrum can be as ‘exact a science’ as we want it to be.So then, how can one make Scrum “an exact science”? What does Scrum actually have in common with science in the first place?Let’s compare how the core principles of Scrum and science overlap:In short, one can postulate that Agile Scrum is just a way of applying the scientific method in IT project management.How does one apply the scientific method?Sounds quite a lot like the Scrum cycle, doesn’t it?The main difference is what is being optimized.Let us get our hands dirty then. Let us formulate a few hypotheses. Some of them we will use as base — unproved and taken to be true (axioms) — still, just to be clear that we haven’t proven them, we will list them as hypotheses.I know, it may sound dull and boring. However, it’s a worthwhile exercise — first say what is expected to happen. How are things believed to work?Afterward we will see how much of this proves to be true…“The sprint outcomes can be improved by applying a consistent framework based on the scientific method.”Luckily enough, we already had tested this hypothesis with another team before. The results were described in the blog post “Demystifying Agile T-Shirt sizes”. This made the task of convincing the second team a much easier task.“Effort spent on items of high uncertainty, like spikes and bugs, should be time-boxed.”We have decided to cap the efforts on gaining necessary information to one or two Person-Days (PDs).Once the time-boxed effort is “spent”, the team decides if the gained knowledge is sufficient to estimate the complexity of the remaining task. If it is still not possible — another time-box effort may be taken (not at the cost of bursting the sprint delivery, however). No more than several (say three) attempts are considered reasonable.Obviously — there are two extreme sides encompassing all other situations:“Scrum is applicable for medium-uncertainty projects, whereas Kanban is better suited for large-uncertainty projects”In case the uncertainty of items is too big — when there are mainly spikes, bugs, or the quality of estimations doesn’t stabilize after several (say five) sprints — consider switching to Kanban. Maybe the project actually is not in the area of applicability of Scrum in the first place?“Keeping clear workflow transition rules leads to improvement of measurements.”Transitions between states like In progress ⇒ In review must have clear conditions.For instance — the developer has completed the work on his branch has written unit tests that provided X% of coverage, the main functionalities seem to be working, etc. The rules may be fairly simple and be subject to improvement themselves.The clue here is — do not falsify the information —this is because ANYTHING may be implied based on false information (see here).Things are Done when they are done. Testable — when they can be tested. Reviewable — when another developer can deliver only final polishing touches. In progress — when someone is actually working on them. To Do — when someone can actually start implementing, without having to ask a hail of questions.If the assignee must return the ticket to the previous assignee because their task cannot be performed — it is a sign that the transitions are not happening in a clear way. This may cause additional errors further along the way — in the sprint outcomes.If one wants consistent, reliable results, one has to base them on real and reliable information.“Unfinished items should be accounted for during the sprint, not averaged over multiple sprints”We have suggested using a simple approach, where the completion percentage grows linearly with each step of the workflow. So for example:This way — even if a sprint has failed, or some items spilled over — the team could still see progress.This breaks a vicious spiral, where team members see little point in doing much effort on their side, since the sprint will fail anyhow.Using this approach information should be available on a sprint-to-sprint basis. Otherwise, we would risk diluting the information over several sprints. Averaging over multiple sprints would mean adding unnecessary complexity to the calculations.“Calculations on a-posteriori values are more accurate.”Initial estimations will have high error bars — the team simply does not know yet how complicated the solution will be. A way to improve estimations, reporting and predictions is to re-estimate the issues after they have been done.It’s actually the scientific method in disguise: where initial estimation = prediction, and where implementation and re-estimation = measurement. The closer the estimation values can be brought to the re-estimation ones — the better the overall process gets. Naturally, one shouldn’t expect the values to be completely in-line, some discrepancies are to be expected. However if there is no sign of improvement — it may be a signal that the project is not in the area of applicability of Scrum.To obtain reliable results, apart from having reliable information, one needs to have a reliable method of working — a consistent framework.Again, please excuse for repeating the obvious and the basics, but they are essential.The following chart presents data for several sprints.We will list the hypotheses posted above. The axioms will be omitted, since they are taken as base of the applied method.“The sprint outcomes can be improved by applying a consistent framework based on the scientific method.”Is easily positively verified by the results shown in the above chart.“Keeping clear workflow transition rules leads to improvement of measurements.”At this point, it can be inferred that this hypothesis is very plausible.“Unfinished items should be accounted for during the sprint, instead of being averaged over multiple sprints”Is positively verified using a more detailed chart (below). The chart compares different methods of accounting delivery items:a) One obvious argument against the ‘green’ chart is the clear disparity between efforts delivered by the team during the first four sprints and lack of their visibility in the chart.b) The ‘blue’ allows for more detailed in-sprint progress tracking of deliverables. Obviously, according to Scrum principles all items committed to, should be delivered by the end of the sprint, hence the ‘yellow’ chart should suffice. However not doing in-sprint progress accounting may introduce a vicious spiral and the team not being able to ever achieve this state.“Calculations on a-posteriori values are more accurate.”Can be verified by observing the stability of results between the sprints where no re-estimations were done, versus the ones where it was introduced (Nikisicko, BrewDog, Nenea Lancu). This argument makes sense for us, since if the team size is not dramatically changing, nor is the team velocity — then the overall results should be stable over time. This again can be used as one of the arguments between the ‘yellow’ and ‘blue’ way of accounting items.We hope to have convinced you that Agile Scrum can be not only a working methodology, but also a reliable one, providing stable results and reliable predictability.",,1,1,15,13,1131,579,7,6,0,8,126
It is I!,,https://medium.com/u/f097b1290d5b?source=post_page-----c35fc29ccf6--------------------------------,Sander Dur,3300,,134,"👋 Sander Dur typing here, awesome of you to read my profile! I’m passionate about everything that is related to business agility, especially the people aspect. I’m an advocate of bringing back humanity to the workspace again.🏢 I work as a Lead Agile Consultant at Xebia. Besides loving my consulting job, I work as a trainer for the Xebia Academy.🎤 Next to these articles and my regular day job, I host a podcast called Mastering Agility, where all the biggest names in the industry drop by to talk about whatever they feel is important. Names include Mike Cohn, Maarten Dalmijn, Roman Pichler, and Jeff Patton.🌊 Editor and active writer at Serious Scrum, the largest publication on Medium about Scrum.✉️ Feel free to connect with me via LinkedIn! I’m always up for a good conversation.",,1,0,0,0,1225,633,1,0,0,7,80
,,https://medium.com/u/cdd4a7a16b9e?source=post_page-----c35fc29ccf6--------------------------------,Meriç Melike Yılmaz,233,,,,,,0,,,,,,,0,,
,,https://medium.com/u/3f117654a175?source=post_page-----c35fc29ccf6--------------------------------,Marisa Hoenig,477,,,,,,0,,,,,,,0,,
How to Transition to a Product Management Role,Better Programming,https://betterprogramming.pub/how-to-transition-to-a-product-management-role-8eebd71b566,Olaseni Adeniji,514,6,1291,"Have you ever wondered what it takes to switch from your current role to product management (pm)? Do you see yourself switching sometime down the line, or are you just curious about what it takes to make that switch? Well, I have good news — this is the right article for you.Switching from software engineering, cyber security and many other roles to product management is a question I have been asked numerous times. Unfortunately, I never had any concrete resource that broke down some strategies that can help you be successful as you take on this challenge. So! I interviewed a brilliant friend (Sean) who successfully switched from a software engineer at an investment bank to product management at Expedia. Today, I will be sharing how he did it and how you can too.He studied computer science in college and, like many people, followed the software engineering track. He came across product management in his junior year at a hackathon he participated in and decided that was the right space for him, but he had difficulties breaking in. If you have ever done any of the Associate Product Management (APM) interviews, you can relate to this feeling. After “failing” to land a PM job, it was a good thing his coding skills were up to par, so he took a software engineering job instead at an investment bank.Becoming a product manager was a goal he never took his eyes off, even while working full time as a software engineer. So, the game plan was to work as a software engineer for a while (maybe 2 years?) and then transition to product management, but the transition happened a lot faster. After working 15 months as an engineer, he found himself learning and reading more about product management vs. learning new engineering frameworks, languages, or even doing projects. So, he decided it was time to make the switch!I asked — “What helped you be successful in making the transition” and below are strategies he utilized to make this happen:1. Reflection: He had done numerous interviews previously and never landed a PM role; why was this ? He reflected on what worked and what did not. What are some areas of improvement? If you are on the spectrum of people that have never tried to break into PM, this is an opportunity to reflect “why product management?”, “what do I bring to the table?”, “what transferrable skills do I have.” When you have answers to this question, you are already one step closer.2. Learn: A common mistake I have seen people make is decide they want to do product management, scantily read about it, do a couple of mock interviews, and try to interview — now, if you are lucky, this might work but for most people, it will not. Take the time to learn about product management! There are so many resources (free and paid) out there. Some of the resources he used are:a) Courses & Certifications:i. Product School Certifications: Product Analytics and How to build digital products (both of these were free)ii. Coursera course: Fundamentals of digital product management. Offered through the University of Virginia business school. (This course was paid)b) Articles:i. Silicon Valley Product School (Especially those written by Teresa Torres)c) Podcasts:i. Product schoolii. Masters of product managementiii. How to succeed in product managementd) Books: Some great books to read are cracking the PM interview, decode & conquer, product management’s sacred seven and swipe to unlock.Completing these courses, certification, and reading multiple articles took about 2 months and now he was ready to interview prep.3. Resume: As he worked on completing his certifications and courses, he worked on having an application-ready resume. He shared the resume with trusted friends and mentors to ensure it was in the “most-ready” shape it could be. Like I always say, you cannot guarantee you will get a job, but to put you in the best position to do so, control the things you can and an example of this is having a great resume ready.4. Interview Prep: Sean mentioned that one mistake he realized earlier when he tried to break into product management, was doing too many mock interviews — almost like focusing on the quantity vs. the quality and this was something he had to correct this time. This time instead of racking up multiple mock interviews that didn’t translate into progress or improvement, he:a) Watched Exponent interviews on youtube and tried to analyze what made some interviews good and what made some great. Highly recommend.b) Analyzed some of the free product alliance videos on youtube to give him more insights into great product management interviews.c) Pramp.com — this was the tool he then used to prep for mock interviews. A peer-to-peer tool for interviewing prep. He did approximately 10 mock interviews there and some other ones with friends.5. Networking & reaching out: After doing his due diligence to learn, prep and practice, he started reaching out to other product managers at his current company to discuss PM nuances and to contacts in his network for referrals for available roles. He landed two interviews with different organizations/product groups at his current company through the referrals and one externally (where he is now) after reaching out to a hiring manager who posted an open role on his team on LinkedIn.Before jumping into the interviews, I wanted to highlight one area of this story because I think this is what separates decent candidates from great candidates.The third interview he secured at Expedia was by reaching out to the hiring manager who posted the role on LinkedIn. The message contained an intro, why he wanted to be considered, what he knew about the hiring manager’s team, and his resume attached. That resulted in a 15 minutes informal call with the hiring manager. Sean put together a short case study on an edge case of the solution this hiring manager’s team was building (I mean, that is just next level if you ask me) and this gave him the stellar first impression he wanted to make. I wondered why he put together that case study and the answer was, “Whenever you get an opportunity, put your best foot forward. You want to make sure you did everything remotely possible in your power to make things happen, and if it does not happen, you can live with the fact you did your best.”All 3 opportunities he had were averagely 5 interviews and the pattern was:a. 1 interview with a hiring manager: This interview is to know more about your background, check if there is an alignment with your background and what they are looking for, and behavioral and situational interviews to check if you are a good cultural fit.b. 4 Loop interviews: The other 4 interviews were a mix of technical with an engineering manager/software engineer, product-related interview (usually product strategy and/or product design), and then behavioral/situational interviews.The best part: He got offers for all 3 roles!! So my advice, follow it, or pick the pieces that work for you, or modify to your needs but overall, I hope these strategies help you as you embark on your journey. One thing Sean shared with me and I want you to leave with today is, product management has many nuances so learn the role. The more you know, the better for you and the more you can start to see glances of what it means to be a good product manager; this will translate in your interviews.Breaking/Switching to product management can feel daunting but many have done it before you and many will do it after you, so why not you? Good luck, and may the force be with you!Disclaimer: This is not the only way to make the transition.",,,1,12,4,1225,816,1,0,0,8,166
,,https://medium.com/u/b985a5a5dccd?source=post_page-----c35fc29ccf6--------------------------------,Olaseni Adeniji,514,,,,,,0,,,,,,,0,,
10 Tips To Write Effective Code Reviews,Better Programming,https://betterprogramming.pub/10-tips-to-write-effective-code-reviews-c25c25aa22c5,Meriç Melike Yılmaz,233,10,2118,"We have all seen those code reviews where the code keeps going back and forth and it takes several iterations until it finally gets merged.It is not only annoying for the author that the review won’t accept the code, but it is also annoying for the reviewer who will be thinking that the author just doesn’t “get it”.Some good news though; it is not only up to the author of the changes that the feedback is received smoothly and the process goes faster. There are also some good practices the reviewer can deploy ensuring a less dull and more clear review process.Effective code reviews are a win for everybody, because:Note that this article doesn’t explain how to conduct a code review or what to pay attention to while doing so. Instead, we list here some tips and rules of thumb to your approach and behavior regarding the review process. So if you already review code regularly in your work, this article is aimed to help you reconsider some of your practices and become a better reviewer.Let’s get to it.Let’s look at some potential review comments:bad function namethis shouldn't be hereI think this might breakThere are some things wrong with these comments, but the most prominent issue is that they don’t provide reasoning. From these comments, the author of the code can only assume what is wrong with their code, and we all know how assumptions are bad — specifically in this case;What would a much better comment be? Let’s check an example:Yes, you just wrote a bunch of more words, but you now clearly told the author what was wrong with their name and how they should go about it when they are thinking of function names.Do you know how great you will feel the next time the same author sends you a code review and you see that he/she now followed the correct rules for the function name? Give it a try!The author of the code can be the most senior person in your team knowing all the in’s-and-outs of the codebase or a new recruit that is submitting their first code review.This should not affect the quality or the thoroughness of your code review, nor should you give the senior employee any slack just because you trust their work.Code reviews aren’t only about ensuring code quality and structure, but also about looking out for corner cases or bugs that the author might have overlooked, or even just random typos.And admit it, it happens to the best of us. Catching a bug before it goes to production (and even to staging) is always more efficient than finding the bug, reporting it, finding the cause, fixing the issue, and sending it for another code review.As the reviewer of the code changes, it is not your job to fix the code or come up with a solution when you detect an issue.Yes, you can (and sometimes should) help out or discuss your ideas, but that does not mean that you should give a full solution to the problem. Consider a comment like the following:When you do this, the developer of the code will have learned very little from your suggestion; in some cases, they will even copy-paste your solution and skim through it without thinking about it in detail.And especially if you constantly do this, the developer will easily get lazy and don’t consider too much when writing complex code, thinking that the reviewer would suggest a fix if he/she finds something wrong with it. If you do this regularly, don’t be surprised if you are often chosen to be the reviewer of the code changes in your team.The correct behavior, in this case, would be to describe how the method should look like instead. You should state why it doesn’t do the job, and how you would approach the problem at hand, maybe write it down step by step. Even some pseudo-code is OK in some cases. As long as you make the developer actually think about your suggested approach and not just copy-paste your solution, you are goodIn any kind of team, but especially with engineering teams, it is very important to not block each other’s work.It is not an easy task for a developer to context-switch. And waiting for a code review on one of their issues, they would have to switch to another story to work on and go back and forth as they are getting the review and addressing its comments.This will anyway happen even when the review happens really fast — because the developer will want to move on to the next task. However, the longer the review drags out, the more stories and code reviews the developer will have to manage like this going back and forth between branches, re-read his/her code (again, context-switch), resolve merge conflicts, etc.Also, imagine that you would actually block the person’s work; meaning they cannot continue with their work until this feature is finalized / code changes are merged. In that case, they will literally not be able to work until you are done with the review. This whole thing is both inefficient and annoying for both parties (because, oh well, the developer will probably be messaging the reviewer every 30 minutes asking for his/her review once they are blocked 🙃).In order to make sure that you are not making way for such chaos, it is always a good practice to prioritize code reviews in your schedule. Of course, you might have more urgent tasks to work on or may not always be able to take the time to work on the review right away, but you are obligated to find the right balance between your own work and the reviews.If you have the responsibility to review the code for a lot of people, dedicate a time slot in your schedule each day for code reviews and only focus on them during that time (or maybe two slots to handle multiple iterations more quickly).If your team is big or structured enough, you will probably have some guidelines written in a knowledge-base explaining the guidelines to write code for a project/code-base.It can include general approaches to code organization, naming conventions, and many design choices. If you are actively writing code and reviewing others’ code you will probably be accustomed to these guidelines.But you should give it a try to go back to the guidelines and give it a quick read every couple of weeks or each month. You will be surprised to see how many of the points you have forgotten about or you haven’t been paying attention to in the code reviews.If your team doesn’t have such guidelines, it might probably be a good idea to write one while reviewing the code. In just a couple of code reviews, you will surely come up with a lengthy list of points that you find yourself paying attention to during the review.More importantly, such a list of guidelines would serve as the single source of truth among your team, ensuring that all the code authors and reviewers have a resource to look at when writing/updating/reading code.Code review isn’t always about bad news. Sometimes you will come across some code that will make you appreciate the author; a piece of logic, some very efficient query optimization, or a brilliant UX. Say it.You can comment on the related code simply with nice! or with a longer sentence mentioning what you liked about it. Either way, the person will feel appreciated, and that definitely makes a difference in teamwork. Also if the rest of the code isn’t as good, they won’t leave feeling too bad after the review.On top of all these, the author will now have validated that the piece they wrote is actually good, so they can try and create more content like that in the future or follow similar steps for other problems.Over the years I have stumbled upon a lot of review comments that read this is stupid, or just no. I might have written some of those too on some bad days.Let alone the above points of stating the reason etc, such comments are simply offensive and discouraging and do not create any kind of value whatsoever. Gitlab gives some nice tips on this if you would like to read further, and it sounds very in line with their company value of “positive intent”.Long story short, it pays to be kind in review comments, have respect for the other person’s time and work, and assume that they were trying their best while writing the code. There is absolutely no benefit to bragging, humiliating, or getting angry in code reviews.Let’s check out some real-life examples I copied over from previous code reviews. Note the words that are setting the tone in the sentences.Believe me, a little imho or “🤔” will go a long way.I cannot tell you how many times we shared this link about ordering css properties with each other in code reviews in my first frontending days (should have probably just configured stylelint 🙈). I believe it was even in my bookmarks at one point.If there is a resource that you know of that will help the author of the code on resolving an issue, link it in your comment. More often than not they will appreciate the comment and it might even become a resource for them to use regularly in their work. This resource can also be the specific section of your company guidelines, if you have a big one and information is difficult to detect.Maybe an even more important resource you can point to is within the codebase the author is working on. A similar logic or component that was already implemented by someone else in the past can be a great guide for the author, and they may not know of that particular section of the codebase if they haven’t worked with it before.If you detect a code that should be improved and you know a similar piece has been written before, don’t assume that the author would know to find it. Point it out in your comment so that the author can compare his/her code to the existing one, make improvements or follow the same conventions when necessary.While reading a code for review, it is essential that you can easily understand it. If there is a part of the code that is gibberish to you, it doesn’t mean that you are at fault for not reading the code carefully enough. On the contrary, it means that the code is just not clear enough.If you have trouble understanding what that chunk does, it will likely be difficult to follow for other developers in the future when they need to touch that part of the code. When you detect such code, you should not hesitate to speak out. And don’t just ask for the author to explain that code chunk to you, instead, ask for a rewrite.If it really cannot be written in a more clear way, at least ask for the author to add some comments in the code. Feel free to ask questions to understand the purpose of the code so you can suggest alternative approaches and discuss solutions, but you should not be just satisfied with the response and you should ensure that the code that ends up getting merged is easily understandable.There is usually more than one way to accomplish things. When reading a piece of code, you will likely imagine other ways of doing it, and will feel a tendency to suggest it in a comment because it is a better solution. Sometimes you need to ask yourself: is it really the better solution? Or is the author’s way also good but the suggestion is closer to the way you always do it?Unless there is a clear advantage for code complexity or readability, think twice before suggesting a rewrite. Re-read the code and ensure that it is in line with your team’s conventions and code styles, that it does the job well, and compare it with the alternative in your mind to see how it will affect the code quality. If the author’s way is OK but you still think that yours is better, write it as a suggestion rather than a must-have change and be open to discuss with the author the reasons and details of the change you are suggesting.Hopefully these tips will help you become a more effective code reviewer and in the long run save time for you, for the code’s author, and for the team in general. Let me know in the comments if you have any thoughts on the above, or any other suggestions that might come in handy while reviewing code.",,1,3,7,8,1225,816,1,2,0,4,285
,,https://medium.com/u/b40569a4ce51?source=post_page-----c35fc29ccf6--------------------------------,Maarten Dalmijn,66000,,,,,,0,,,,,,,0,,
,,https://medium.com/u/9833cc01f515?source=post_page-----c35fc29ccf6--------------------------------,Anupam Chugh,27000,,,,,,0,,,,,,,0,,
Is Workplace Stress Overwhelming You? Here Are 5 Strategies To Reduce It,Better Programming,https://betterprogramming.pub/is-workplace-stress-overwhelming-you-heres-5-strategies-to-reduce-it-4bc89e33f862,Vinita,1920,9,1485,"Work is a big part of our life. Done well, it can be a source of joy. But when managed poorly, it can often lead to stress, anxiety, and dissatisfaction.There’s no getting away with some amount of stress at work, and we shouldn’t try to either. After all, all stress is not bad. Oftentimes, it signals you are doing worthwhile work. That you care about adding value and creating an impact. Stress can also energize you, enabling you to put in the effort required to make something happen.But what if your stress is debilitating? What if it gets in the way of making meaningful contributions? What if instead of energizing you, workplace stress sucks into your energy and hurts your motivation?There can be many reasons for feeling stressed at work.Some stress may be completely outside your control, like the one resulting from some bad people or toxic culture in your organization.Your other source of stress is in a large part within your control as it relates to your own idiosyncrasies in how you manage and respond to various events at work.Whatever the reason, research shows that left unhandled, stress can lead to physical and mental health issues. It can impact your productivity and performance at work. It can even impact those around you.I am not going to talk about the stress that comes from things outside your control (bad boss, office politics, other negative cultural elements).Other than moving on to a better workplace that doesn’t cause you day-to-day stress, I don’t think you have another option. You can crib about these issues all you want, but if your stress results from a larger cultural issue within your organization, it won’t get better with time. You need to take a hard stance, get over your fear of not finding another job, and put your time and energy into actually making the switch.One of the mistakes many of us make is that we feel sorry for ourselves, or for others, thinking that life should be fair, or that someday it will be. It’s not, and it won’t. When we make this mistake, we tend to spend a lot of time wallowing and/or complaining about what’s wrong with life. “It’s not fair,” we complain, not realizing that, perhaps, it was never intended to be — Richard Carlson, “Don’t Sweat the Small Stuff: Simple Ways to Keep the Little Things from Taking Over Your Life”Now let’s talk about things that are within your control. Here are some of the strategies that have worked for me over the years to manage stress on the job.Oftentimes, workplace stress results from paying too much attention to little things in your life. Things that in the long run don’t really matter much. Things that you don’t really care about, but may feel like life-and-death decisions in the heat of the moment.Sometimes, it’s your ego that gets in the way. Other times, you lack the perspective to understand that whatever is the issue at hand doesn’t really deserve your time and attention.You have many opportunities to isolate yourself from such trivial nuances, but find yourself being more and more trapped by such situations.To save yourself the mental agony that comes with falling for inconsequential events in your life, ask yourself these questions:Knowing what’s causing you stress can enable you to rid yourself of the trivialities that don’t deserve your time and attention.Lack of control over how you get to spend your time at work is one of the hidden sources of anxiety and stress at work.A busy schedule with too many meetings and a packed calendar can make you feel important, but it does nothing to advance you in the direction of your goals. Rather, having a hectic day at work without achieving anything significant leaves you feeling inefficient and inadequate.A stressful morning at home may also get carried to work. Not having enough time to have a healthy breakfast, struggling to drop your kids at school while making it to your meeting on time can honestly be quite stressful. This lack of planning also impacts how you react to various events at work. If you started your day with a stressful morning, even a slight expectation mismatch or discomfort at work may put you off. You may get the feeling that everything around you is falling apart, further adding to your feelings of stress and anxiety.This often happens when you don’t consciously plan and prioritize your day. I am not saying that things can’t go wrong despite planning. However, having a plan in place leaves less to chance and gives you the mental space needed to choose your response.With better control over your schedule, you get to choose and do work that actually moves you forward, leaving less opportunity for stress to creep in.Feeling ineffective at the job is one of the major sources of burnout at work. If you don’t know what’s expected of you or how your work fits within the goals of your team, it may be hard to achieve anything significant.Lack of progress, purpose, and meaning often stems from missing clarity on goals and the opportunity to achieve those goals. It causes feelings of inadequacy, loss of sense of belonging, and cynicism and negativity towards the job, all of which may lead to stress.Instead of waiting for others to give you the clarity you need and expect them to make things better for you, actively seek clarity. Your feelings aren’t transparent to others and honestly, everyone is so busy with their own agenda at work that it’s extremely hard for them to notice what you need or what you don’t understand.To avoid stress, push for clarity. Ask questions to better understand your role, job expectations, and goals. Schedule time with your manager to get regular feedback.Shifting your focus to something that your mind perceives as a doable, completable task will create a real increase in positive energy, direction, and motivation — Jake Knapp, “Make Time: How to Focus on What Matters Every Day”Having clarity at work enables that positive shift leaving less room for negativity and stress.A lot of work in organizations requires collaborating across different teams and functions. Collaborating with others definitely creates great opportunities to learn and grow, but it may also come with a lot of stress.Differences of opinion may lead to disagreements. Conflicting priorities, schedule clashes, and expectation mismatch can cause you to spend more time arguing and less time putting your ideas into action.The negative emotions that such conflicts tend to evoke may cause you to bury your head in the sand. You may try to avoid dealing with them or delay them with the hope that they will disappear. However, avoidance and inaction only make things worse — small issues turn into major problems making it extremely difficult to seek alignment. And without alignment, nothing major ever gets done.The opposite of recognizing that we’re feeling something is denying our emotions. The opposite of being curious is disengaging. When we deny our stories and disengage from tough emotions, they don’t go away; instead, they own us, they define us. Our job is not to deny the story, but to defy the ending — to rise strong, recognize our story, and rumble with the truth until we get to a place where we think, Yes. This is what happened. This is my truth. And I will choose how this story ends — Brene Brown, “Rising Strong”So if you have a tendency to avoid conflicts, don’t. Face them head-on. Have the conversation. Try to understand another person’s point of view. Asking these questions can help you deal with your situation and avoid the stress that comes from ignoring them:What happens when things don’t go your way? Isn’t your instant reaction to assume the worst possible outcome? To blow things out of proportion?Not being mentally prepared to handle the unexpected can through you off course. Even a small deviation from the normal or a slight change in your circumstance can make you highly anxious and stressed thereby distorting your ability to think clearly.Negativity also causes you to repeat destructive patterns of behavior (procrastination, blame, distraction) which hurts your outcomes and leads to more negative thoughts thereby reinforcing your negative beliefs and convincing you that you were right in feeling a certain way. Sort of a self-fulfilling prophecy.To keep stress in check, whenever you find yourself overwhelmed with negative emotions, do this:We don’t know what’s going to happen — we just think we do. Often we make a big deal out of something. We blow up scenarios in our minds about all the terrible things that are going to happen. Most of the time we are wrong. If we keep our cool and stay open to possibilities, we can be reasonably certain that, eventually, all will be well — Richard Carlson, “Don’t Sweat the Small Stuff”Follow me on Twitter for more stories.This story was originally published at https://www.techtello.com.",,2,0,1,3,1200,630,1,6,0,14,238
A Computer Science Degree Isn’t the Only Way Out for Software Developers — 4 Related Jobs You Can Try,Better Programming,https://betterprogramming.pub/get-a-computer-science-degree-become-a-developer-right-437a0e3bcf90,Marisa Hoenig,477,5,1042,"While earning my computer science degree, I thought the only career path was software developer. After all, I was trained to be a software developer — how could I become anything else?No one mentioned the existence of other options. I was tied to one destiny.The more I talk to fellow grads, I realize I’m not the only one — many people think they’re stuck on the software development path because of their computer science background.However, that’s not the case.Sure, there are tons of roles in software development (backend, frontend, mobile, and more!), but there are other essential tech careers that anyone with (or without) a computer science degree can pursue.Don’t limit yourself to software development, especially if it doesn’t feel like the right career.While roles vary across companies and there are hundreds of tech careers, here are four common ones and why you’d be a good fit.This is the list I wish I had in college.“QA analysts are the first ones to encounter anything that might make the user experience worse.” — Jason Boog, What is a QA Analyst, and What Do They Really Do?Quality analysts are the testers. When developers finish coding a feature, the QA puts it through test cases. They are curious about how things work and how to break things to find bugs.Day after day, quality analysts work closely with developers. They check we have sufficient unit tests and integration tests, and the app works as expected on various devices. Sometimes they write their own automated tests, and sometimes they go through a manual testing process.Either way, they ensure the product is ready to be shipped off to production. They are usually the final check-in of a long line of contributions from the team.Do you have a natural curiosity about how things work? Do you like to take things apart and put them back together? Are you intrigued about understanding all the ways someone could use a product?If you answered yes to any of these, QA may be the job for you!“The Designer gets a chance to work in between the product building teams and product using people. Here in this in-between space, the designer gets immense opportunities to get into the life of different people to make a product that can solve the problem of their life.” — Aman Gupta, Why did I choose Design?Designers create mock-ups for the product by conducting user research and having an eye for design. These designs are the ones the developers code to fit the product’s vision.“Design” itself is a generic category, similar to software development. Sometimes, jobs will be referred to as UI/UX, which stands for user interface and user experience. The difference is simple: UI encompasses the look and structure of the application, while UX is how the user interacts with the app.Within design, there are various jobs, such as UX researcher, interaction designer, UX writer, usability analyst, graphic designer, and information architect.Are you artistic? Are you passionate about creating an experience for the users of your team’s software? Do you enjoy psychology and thinking about why people choose to use products in certain ways?If you answered yes to any of these, designer may be the job for you!“Project managers are change agents: they make project goals their own and use their skills and expertise to inspire a sense of shared purpose within the project team.” — Project Management Institute, Who are Project Managers?Project managers are the organization gurus — scheduling meetings, planning project logistics, organizing the team, and keeping the project on time and on budget. They keep everyone on track.Typically, project managers are great communicators and leaders. They also “take non-development work off [the] team’s plate to let them focus on the product” (Project Management Guide).In the software industry, project managers should have a baseline level of understanding of tech — enough to understand the developers and maintain realistic timelines. Having a computer science background is an uncommon superpower as a project manager.Are you organized? Do you enjoy taking notes and keeping others on track? Do people generally refer to you as a “people person”?If you answered yes to any of these, project manager may be the job for you!“Product management is about insights and judgment, both of which require a sharp mind. Hard work is also necessary, but for this job, it is not sufficient.” — Marty Cagan, Inspired: How To Create Products Customers LoveProduct managers are constantly thinking about the problem. They seek to understand the business requirements and nail down exactly what the product (app, website, etc.) should be. This often includes talking to users as well as stakeholders.One major responsibility is creating a product roadmap for the team to build toward. This includes the features to build and how those will work together to create one cohesive product. Other responsibilities include defining success metrics, maintaining a product backlog of upcoming development work, and making strategic decisions for the team.The role of a product manager can vary across companies and industries. Some take on responsibilities that would be typical of a project manager, and some share space with the designers. When looking at product manager jobs, make sure to understand the job responsibilities.Do you love problem-solving? Are you passionate about translating business requirements into reality? Do you enjoy leading teams and thinking strategically?If you answered yes to any of these, product manager may be the job for you!Choosing a path in the tech industry isn’t easy, and this isn’t an extensive list of tech careers. While I did choose the software development career path, I haven’t always enjoyed it. Honestly, I’ve thought about switching to different roles before.As a software developer, I work on a cross-functional team, so I’m never far from any of the roles defined here. Sometimes, I stretch my product management muscles or try my hand at design. You can do that, too — by choosing software development, you aren’t choosing it forever. You can dabble in other things.Don’t feel pigeonholed into a role because of your computer science degree.If you’re curious, try quality analyst.If you’re artistic, try designer.If you’re organized, try project manager.If you’re strategic, try product manager.If software development and these four roles don’t appeal to you, try exploring some of these:What other roles in tech should people pursue?",,,0,1,1,1225,816,1,1,0,12,126
,,https://medium.com/u/b892e7626234?source=post_page-----c35fc29ccf6--------------------------------,Vinita,1920,,,,,,0,,,,,,,0,,
