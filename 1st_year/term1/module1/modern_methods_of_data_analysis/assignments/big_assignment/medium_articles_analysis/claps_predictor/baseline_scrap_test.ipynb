{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium claps predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "#import torch \n",
    "#import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './'\n",
    "# TRAIN_PATH = os.path.join(DATA_PATH, 'articles_train.csv')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'articles_test.csv')\n",
    "SCRAPPING_OUT_PATH = os.path.join(DATA_PATH, 'scrapped_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #За детерминизм!\n",
    "# SEED = 0xDEAD\n",
    "# random.seed(SEED)#\n",
    "# np.random.seed(SEED)\n",
    "# torch.random.manual_seed(SEED)\n",
    "# torch.cuda.random.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TEST_PATH, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>Rohit Thakur</td>\n",
       "      <td>8</td>\n",
       "      <td>https://towardsdatascience.com/step-by-step-r-...</td>\n",
       "      <td>Step-by-Step R-CNN Implementation From Scratch...</td>\n",
       "      <td>Towards Data Science\\nOct 18, 2019\\nClassifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>Giuliano Giacaglia</td>\n",
       "      <td>14</td>\n",
       "      <td>https://towardsdatascience.com/transformers-14...</td>\n",
       "      <td>How Transformers Work. Transformers are a type...</td>\n",
       "      <td>Towards Data Science\\nMar 11, 2019\\nIf you lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>Darshan Adakane</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/neural-style-tr...</td>\n",
       "      <td>Neural Style Transfer using VGG model | by Dar...</td>\n",
       "      <td>Towards Data Science\\nJan 16, 2020\\nIntroducti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  reading_time  \\\n",
       "id                                       \n",
       "3756        Rohit Thakur             8   \n",
       "3757  Giuliano Giacaglia            14   \n",
       "3758     Darshan Adakane             7   \n",
       "\n",
       "                                                   link  \\\n",
       "id                                                        \n",
       "3756  https://towardsdatascience.com/step-by-step-r-...   \n",
       "3757  https://towardsdatascience.com/transformers-14...   \n",
       "3758  https://towardsdatascience.com/neural-style-tr...   \n",
       "\n",
       "                                                  title  \\\n",
       "id                                                        \n",
       "3756  Step-by-Step R-CNN Implementation From Scratch...   \n",
       "3757  How Transformers Work. Transformers are a type...   \n",
       "3758  Neural Style Transfer using VGG model | by Dar...   \n",
       "\n",
       "                                                   text  \n",
       "id                                                       \n",
       "3756  Towards Data Science\\nOct 18, 2019\\nClassifica...  \n",
       "3757  Towards Data Science\\nMar 11, 2019\\nIf you lik...  \n",
       "3758  Towards Data Science\\nJan 16, 2020\\nIntroducti...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['reading_time']\n",
    "cat_cols = ['author', 'link', 'title', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.431351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reading_time\n",
       "count    500.000000\n",
       "mean       7.880000\n",
       "std        4.431351\n",
       "min        1.000000\n",
       "25%        5.000000\n",
       "50%        7.000000\n",
       "75%       10.000000\n",
       "max       31.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=len(num_cols), figsize=(20,5))\n",
    "# fig.suptitle('Ящики с усами для числовых признаков', fontsize=16)\n",
    "# for i in range(len(num_cols)):\n",
    "#     ax[i].boxplot(train_data[num_cols[i]])\n",
    "#     ax[i].set_title(num_cols[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synced             7\n",
       "Susan Li           7\n",
       "Renu Khandelwal    5\n",
       "Manish Chablani    4\n",
       "Jonathan Hui       4\n",
       "                  ..\n",
       "Sumit Saha         1\n",
       "Gabe Flomo         1\n",
       "IPG Media Lab      1\n",
       "Michael Phi        1\n",
       "Victor Sanh        1\n",
       "Name: author, Length: 436, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! title clastering (maybe topic modeling LDA)\n",
    "# ! link parser (some sites are more popular than another)\n",
    "# ? text parser (only text without titles/dates...)\n",
    "# ! link scrapper (img per 1000 words, image sizes, count of words)\n",
    "# ! catboost, lgbm, ridge, random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def is_English(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def clean_string(a_string):\n",
    "    new_string = re.sub(r'‘|’|\"|—|“|”|,', '', a_string).strip()\n",
    "    new_string = re.sub(r'–', ' ', new_string)\n",
    "    #new_string = re.sub(r'é', 'e', new_string)\n",
    "    return new_string\n",
    "\n",
    "def filter_string(a_string):\n",
    "    if a_string == '.':\n",
    "        return False\n",
    "    elif a_string == '':\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ArticleScraper():\n",
    "    \"\"\"\n",
    "    Scrapes all data for an article.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.scraper_class = 'ArticleScraper' \n",
    "\n",
    "    \n",
    "    def get_title(self, soup):\n",
    "        try:\n",
    "            title = soup.find('h1').text\n",
    "            return title\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get title from article.\")\n",
    "\n",
    "    \n",
    "    def get_subtitle(self, soup):\n",
    "        try:\n",
    "            return soup.find('section').find('h2').text\n",
    "        except:\n",
    "            print(\"Couldn't get subtitle from article.\")\n",
    "\n",
    "\n",
    "    def get_publication(self, soup):\n",
    "        try:\n",
    "            for div in soup.find_all(\"div\"):\n",
    "                if div.text == 'Published in':\n",
    "                    try:\n",
    "                        return div.next_element.next_element.next_element.find('p').text\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            print(\"Couldn't get publication type from article.\")\n",
    "\n",
    "\n",
    "    def get_author(self, soup):\n",
    "        try:\n",
    "            return soup.find('h2', class_=r\"pw-author-name\").text\n",
    "        except:\n",
    "            print(\"Couldn't get title from article.\") \n",
    "\n",
    "\n",
    "    def get_reading_time(self, soup):\n",
    "        try:\n",
    "            return int(soup.find('div', class_=r\"pw-reading-time\").text.split()[0])\n",
    "        except:\n",
    "            print(\"Couldn't get reading time from article.\")\n",
    "\n",
    "\n",
    "    def get_date(self, soup):\n",
    "        try:\n",
    "            date_string = soup.find('p', class_=r\"pw-published-date\").text \n",
    "            return datetime.datetime.strptime(date_string, '%b %d, %Y').strftime('%d/%m/%Y')\n",
    "        except:\n",
    "            print(\"Couldn't get date from article.\") \n",
    "\n",
    "    def get_followers(self, soup):\n",
    "        # try:\n",
    "        #     link = soup.find('div', class_=r\"pw-author\").find('a').get('href')\n",
    "        #     if link[0] == '/':\n",
    "        #         driver.get('https://medium.com' + link)\n",
    "        #     else:\n",
    "        #         driver.get(link)\n",
    "        #     time.sleep(3)\n",
    "        #     new_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        # except:\n",
    "        #     print(\"Can't go to user's account\")\n",
    "        # try:\n",
    "        #     link = new_soup.find_all('a', {'role': 'tab'})[-1].get('href')\n",
    "        #     if link[0] == '/':\n",
    "        #         driver.get('https://medium.com' + link)\n",
    "        #     else:\n",
    "        #         driver.get(link)\n",
    "        #     time.sleep(3)\n",
    "        #     new_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        #     # for button in soup.find_all('button'):\n",
    "            #     if 'Followers' in button.text or 'followers' in button.text:\n",
    "            #         fol_string = button.text.split()[0]\n",
    "            # tens = {'K': 10e2, 'M': 10e5, 'B': 10e8, 'k': 10e2, 'm': 10e5, 'b': 10e8}\n",
    "            # if (fol_string[-1] != 'K' and fol_string[-1] != 'M' \n",
    "            #             and fol_string[-1] != 'k' and fol_string[-1] != 'm'\n",
    "            #             and fol_string[-1] != 'b' and fol_string[-1] != 'B'):\n",
    "            #             return int(fol_string)\n",
    "            # f = lambda x: int(float(x[:-1])*tens[x[-1]])\n",
    "            # return f(fol_string)\n",
    "        try:\n",
    "            for button in soup.find_all('button'):\n",
    "                if 'Followers' in button.text or 'followers' in button.text:\n",
    "                    fol_string = button.text.split()[0]\n",
    "            tens = {'K': 10e2, 'M': 10e5, 'B': 10e8, 'k': 10e2, 'm': 10e5, 'b': 10e8}\n",
    "            if (fol_string[-1] != 'K' and fol_string[-1] != 'M' \n",
    "                        and fol_string[-1] != 'k' and fol_string[-1] != 'm'\n",
    "                        and fol_string[-1] != 'b' and fol_string[-1] != 'B'):\n",
    "                        return int(fol_string)\n",
    "            f = lambda x: int(float(x[:-1])*tens[x[-1]])\n",
    "            return f(fol_string)\n",
    "        except:\n",
    "            print(\"Can't go to the followers info\")    \n",
    "\n",
    "    \n",
    "    def get_mean_size(self, soup):\n",
    "        try:\n",
    "            pics = soup.find('section').find_all('img')#, width=True, height=True)\n",
    "            sums = (0,0)\n",
    "            for pic in pics:\n",
    "                url = pic.get('src')\n",
    "                im = Image.open(requests.get(url, stream=True).raw)\n",
    "                sums = tuple(map(sum, zip(sums, im.size)))\n",
    "            mean = tuple(ti//len(pics) for ti in sums)\n",
    "            if len(pics) == 0:\n",
    "                return (0,0)\n",
    "            return mean\n",
    "        except:\n",
    "            print(\"Can't get image's sizes\")\n",
    "         \n",
    "\n",
    "    def count_figures(self, soup):\n",
    "        try:\n",
    "            return len(soup.find('section').find_all('img'))\n",
    "        except:\n",
    "            print(\"Can't get amount of pictures\")\n",
    "\n",
    "    def get_pure_text(self, soup):\n",
    "        try:\n",
    "            pure_text = ''\n",
    "            for unparsed in soup.find('section').find_all('p'):\n",
    "                pure_text += unparsed.text\n",
    "            return pure_text\n",
    "        except:\n",
    "            print(\"Can't get pure text\")\n",
    "    \n",
    "    \n",
    "    def count_words(self, soup):\n",
    "        try:\n",
    "           pure_text = self.get_pure_text(soup=soup)\n",
    "           return len(pure_text.split())\n",
    "        except:\n",
    "            print(\"Can't get words count from the article\")\n",
    "\n",
    "    \n",
    "    def count_lists(self, soup):\n",
    "        try:\n",
    "            return len(soup.find('section').find_all('ol')) + len(soup.find('section').find_all('ul'))\n",
    "        except:\n",
    "            print(\"Can't get lists count\")\n",
    "    \n",
    "    def bold_text_count(self, soup):\n",
    "        try:\n",
    "            return len(soup.find('section').find_all('strong'))\n",
    "        except:\n",
    "            print(\"Can't get bold text count\")\n",
    "    \n",
    "\n",
    "    def get_blockquotes(self, soup):\n",
    "        try:\n",
    "            notes = []\n",
    "            blockquotes = soup.find_all('blockquote')\n",
    "            for blockquote in blockquotes:\n",
    "                notes.append(blockquote.text)\n",
    "            return notes\n",
    "        except:\n",
    "            print(\"Couldn't get notes from article.\")\n",
    "\n",
    "\n",
    "    def italic_text_count(self, soup):\n",
    "        try:\n",
    "            return len(soup.find('section').find_all('em'))\n",
    "        except:\n",
    "            print(\"Can't get italic text count\")\n",
    "\n",
    "    def count_vids(self, soup):\n",
    "        try:\n",
    "            yt_vids = []\n",
    "            article_soup = soup.find('article')\n",
    "            for figure in article_soup.find_all('figure'):\n",
    "                yt_soup = figure.find('iframe', src=re.compile('.*youtube.*'))\n",
    "                if yt_soup == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    yt_vids.append(yt_soup)\n",
    "                    \n",
    "            return len(yt_vids)\n",
    "                    \n",
    "        except:\n",
    "            print(\"Couldn't get YouTube videos.\") \n",
    "    \n",
    "\n",
    "    def count_gists(self, soup):\n",
    "        try:\n",
    "            gists = []\n",
    "            article_soup = soup.find('article')\n",
    "            for fig in article_soup.find_all('figure'):\n",
    "                gist_soup = fig.find('iframe', title=re.compile('.*\\.py'))\n",
    "                if gist_soup == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    gists.append(gist_soup)\n",
    "            return len(gists)\n",
    "        except:\n",
    "            print(\"Couldn't get count of gists.\") \n",
    "\n",
    "\n",
    "    def count_links(self, soup):\n",
    "        try:\n",
    "            links = []\n",
    "            for a in soup.find('section').find_all('a'):\n",
    "                link = a.get('href')\n",
    "                if link != None:\n",
    "                    links.append(link)\n",
    "            return len(links)\n",
    "        except:\n",
    "            print(\"Couldn't get amount of links.\") \n",
    "\n",
    "    def count_code_chunks(self, soup):\n",
    "        try:\n",
    "            return len(soup.find_all('pre'))\n",
    "        except:\n",
    "            print(\"Couldn't get amount of code chunks from article.\")\n",
    "\n",
    "    def scrape(self, soup):\n",
    "        im_size = self.get_mean_size(soup)\n",
    "        if im_size == None:\n",
    "            im_size = (None, None)\n",
    "        article_data = {\n",
    "            \"title\": self.get_title(soup),\n",
    "            \"publication\": self.get_publication(soup),\n",
    "            #\"subtitle\": self.get_subtitle(soup),\n",
    "            \"author\": self.get_author(soup),\n",
    "            \"followers\": self.get_followers(soup),\n",
    "            \"reading_time\": self.get_reading_time(soup),\n",
    "            \"n_words\": self.count_words(soup),\n",
    "            \"pure_text\": self.get_pure_text(soup),\n",
    "            #\"blockquotes\": self.get_blockquotes(soup),\n",
    "            \"date\": self.get_date(soup),\n",
    "            \"n_code_chunks\": self.count_code_chunks(soup),\n",
    "            \"bold_text_count\": self.bold_text_count(soup),\n",
    "            \"italic_text_count\": self.italic_text_count(soup),\n",
    "            \"mean_image_width\": im_size[0],\n",
    "            \"mean_image_height\": im_size[1],\n",
    "            \"n_images\": self.count_figures(soup),\n",
    "            \"n_lists\": self.count_lists(soup),\n",
    "            #\"n_gists\": self.count_gists(soup),\n",
    "            \"n_vids\": self.count_vids(soup),\n",
    "            \"n_links\": self.count_links(soup)\n",
    "        }\n",
    "        '''\n",
    "        # !'bold_text_count',\n",
    "        # !'count_code_chunks',\n",
    "        # !'count_figures',\n",
    "        # !'count_gists',\n",
    "        # !'count_links',\n",
    "        # !'count_lists',\n",
    "        # !'count_vids',\n",
    "        # !'count_words',\n",
    "        # !'get_author',\n",
    "        # !'get_blockquotes',\n",
    "        # ! 'get_date',\n",
    "        # !'get_followers',\n",
    "        # !'get_mean_size',\n",
    "        # !'get_publication',\n",
    "        # !'get_pure_text',\n",
    "        # !'get_reading_time',\n",
    "        # !'get_subtitle',\n",
    "        # !'get_title',\n",
    "        # !'italic_text_count',\n",
    "        '''\n",
    "            \n",
    "        return(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import lxml\n",
    "from PIL import Image\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(link, options):\n",
    "    page_url = link\n",
    "\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    try:\n",
    "        driver.get(page_url)\n",
    "    except:\n",
    "        print('Time out')\n",
    "        return {\n",
    "            \"title\": None,\n",
    "            \"publication\": None,\n",
    "            #\"subtitle\": self.get_subtitle(soup),\n",
    "            \"author\": None,\n",
    "            \"followers\": None,\n",
    "            \"reading_time\": None,\n",
    "            \"n_words\": None,\n",
    "            \"pure_text\": None,\n",
    "            #\"blockquotes\": self.get_blockquotes(soup),\n",
    "            \"date\": None,\n",
    "            \"n_code_chunks\": None,\n",
    "            \"bold_text_count\": None,\n",
    "            \"italic_text_count\": None,\n",
    "            \"mean_image_width\": None,\n",
    "            \"mean_image_height\": None,\n",
    "            \"n_images\": None,\n",
    "            \"n_lists\": None,\n",
    "            #\"n_gists\": self.count_gists(soup),\n",
    "            \"n_vids\": None,\n",
    "            \"n_links\": None\n",
    "        }\n",
    "    time.sleep(0.1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    article_scrapper = ArticleScraper()\n",
    "    return article_scrapper.scrape(soup=soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Scrapped data become better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test.csv', index_col='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(TEST_PATH, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data) == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>italic_text_count</th>\n",
       "      <th>mean_image_width</th>\n",
       "      <th>mean_image_height</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_lists</th>\n",
       "      <th>n_vids</th>\n",
       "      <th>n_links</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>Object Recognition with OpenCV on Android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Akshika Wijesundara</td>\n",
       "      <td>465.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>763.0</td>\n",
       "      <td>This article is for a person who has some know...</td>\n",
       "      <td>20/12/2016</td>\n",
       "      <td>4</td>\n",
       "      <td>236.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>Understanding Attention Mechanism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shashank Yadav</td>\n",
       "      <td>85.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>Attention mechanism for sequence modelling was...</td>\n",
       "      <td>06/02/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>Sentiment analysis using RNNs(LSTM)</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Manish Chablani</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>Here we use the example of reviews to predict ...</td>\n",
       "      <td>21/06/2017</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title           publication  \\\n",
       "idx                                                                     \n",
       "4045  Object Recognition with OpenCV on Android                   NaN   \n",
       "4178          Understanding Attention Mechanism                   NaN   \n",
       "4052        Sentiment analysis using RNNs(LSTM)  Towards Data Science   \n",
       "\n",
       "                   author  followers  reading_time  n_words  \\\n",
       "idx                                                           \n",
       "4045  Akshika Wijesundara      465.0           6.0    763.0   \n",
       "4178       Shashank Yadav       85.0           5.0    800.0   \n",
       "4052      Manish Chablani     1700.0           4.0    445.0   \n",
       "\n",
       "                                              pure_text        date  \\\n",
       "idx                                                                   \n",
       "4045  This article is for a person who has some know...  20/12/2016   \n",
       "4178  Attention mechanism for sequence modelling was...  06/02/2019   \n",
       "4052  Here we use the example of reviews to predict ...  21/06/2017   \n",
       "\n",
       "      n_code_chunks  bold_text_count  italic_text_count  mean_image_width  \\\n",
       "idx                                                                         \n",
       "4045              4            236.0               49.0            1400.0   \n",
       "4178              0             28.0                8.0            1400.0   \n",
       "4052              2              0.0                0.0             489.0   \n",
       "\n",
       "      mean_image_height  n_images  n_lists  n_vids  n_links  \n",
       "idx                                                          \n",
       "4045              786.0       1.0      1.0     0.0      9.0  \n",
       "4178              625.0      10.0      1.0     0.0      1.0  \n",
       "4052              578.0       1.0      0.0     0.0      4.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0page [00:00, ?page/s]\n"
     ]
    }
   ],
   "source": [
    "is_null = data.isnull()\n",
    "row_with_null = is_null.all(axis=1)\n",
    "rows_with_null = data[row_with_null].index\n",
    "with tqdm(rows_with_null, unit=\"page\") as tepoch:\n",
    "    for idx in tepoch:\n",
    "        new_row = scrape_page(test_data.loc[idx].link, options)\n",
    "        data.loc[idx,\n",
    "                data.columns] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_column(column):\n",
    "    print('Before repeated scrapping: ', data[column].isna().sum())\n",
    "    indeces = data[data[column].isna() == True].index\n",
    "    with tqdm(indeces, unit=\"page\") as tepoch:\n",
    "        for idx in tepoch:\n",
    "            new_row = scrape_page(test_data.loc[idx].link, options)\n",
    "    #     print(new_row['pure_text'])\n",
    "            data.loc[idx,\n",
    "                column] = new_row[column]\n",
    "    print('After scrapping: ', data[column].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n",
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:09<00:56,  9.41s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:18<00:46,  9.24s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:28<00:37,  9.46s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:39<00:30, 10.20s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:50<00:20, 10.49s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n",
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:16<00:00, 10.96s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces_title = data[data['title'].isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indeces_title:\n",
    "    data.iat[idx-3756, data.columns.get_loc('title')]  = test_data.loc[idx].title.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/116 [00:00<?, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/116 [01:02<24:38, 13.32s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n",
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/116 [01:12<22:12, 12.11s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 9/116 [01:45<20:19, 11.40s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20/116 [04:03<18:33, 11.60s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 27/116 [05:24<16:58, 11.44s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 34/116 [06:45<16:36, 12.16s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 35/116 [07:17<24:12, 17.94s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 41/116 [08:59<18:33, 14.84s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 45/116 [09:58<16:29, 13.94s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't go to the followers info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 56/116 [12:31<14:28, 14.48s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 63/116 [14:03<12:02, 13.64s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 64/116 [14:12<10:32, 12.16s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't go to the followers info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 67/116 [14:51<10:15, 12.56s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 68/116 [15:01<09:30, 11.88s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n",
      "Can't go to the followers info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 76/116 [16:48<09:22, 14.05s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n",
      "Couldn't get title from article.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 79/116 [17:21<07:34, 12.29s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 112/116 [24:41<00:51, 12.75s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [25:54<00:00, 13.40s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('publication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.publication = data.publication.fillna('Medium').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['publication'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.24s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't go to the followers info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:14<00:44, 14.89s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't go to the followers info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:28<00:27, 13.96s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't get image's sizes\n",
      "Can't go to the followers info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:53<00:00, 13.37s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces_followers = data[data['followers'].isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indeces_followers:\n",
    "    data.loc[idx,\n",
    "                'followers'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.followers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.82s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('reading_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.75s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:47<00:00, 23.96s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('pure_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.83s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0page [00:00, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_code_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.35s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('bold_text_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.28s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('italic_text_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.61s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[data['n_images'] == 0].index, ['mean_image_width', 'mean_image_height']] = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean_image_width.isna().sum(), data.mean_image_height.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces_width = data[data['mean_image_width'].isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[indeces_width, 'mean_image_width'] = int(np.mean(data.mean_image_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces_height = data[data['mean_image_height'].isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[indeces_height, 'mean_image_height'] = int(np.mean(data.mean_image_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean_image_width.isna().sum(), data.mean_image_height.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0page [00:00, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.96s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_lists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.53s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_vids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repeated scrapping:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.87s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrapping:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_column('n_links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = []\n",
    "\n",
    "for text in data.pure_text:\n",
    "    try:\n",
    "        languages.append(detect(text[:50]))\n",
    "    except:\n",
    "        print('Sorry')\n",
    "        languages.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>followers</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>date</th>\n",
       "      <th>n_code_chunks</th>\n",
       "      <th>bold_text_count</th>\n",
       "      <th>italic_text_count</th>\n",
       "      <th>mean_image_width</th>\n",
       "      <th>mean_image_height</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_lists</th>\n",
       "      <th>n_vids</th>\n",
       "      <th>n_links</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>The best explanation of Convolutional Neural N...</td>\n",
       "      <td>TechnologyMadeEasy</td>\n",
       "      <td>Harsh Pokharna</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>CNNs have wide applications in image and video...</td>\n",
       "      <td>28/07/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Everything You Need to Know About Artificial N...</td>\n",
       "      <td>Technology, Invention, App, and More</td>\n",
       "      <td>Josh</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>The year 2015 was a monumental year in the fie...</td>\n",
       "      <td>28/12/2015</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "idx                                                       \n",
       "3780  The best explanation of Convolutional Neural N...   \n",
       "3986  Everything You Need to Know About Artificial N...   \n",
       "\n",
       "                               publication          author  followers  \\\n",
       "idx                                                                     \n",
       "3780                    TechnologyMadeEasy  Harsh Pokharna     2600.0   \n",
       "3986  Technology, Invention, App, and More            Josh    42000.0   \n",
       "\n",
       "      reading_time  n_words  \\\n",
       "idx                           \n",
       "3780           5.0    726.0   \n",
       "3986           9.0   1986.0   \n",
       "\n",
       "                                              pure_text        date  \\\n",
       "idx                                                                   \n",
       "3780  CNNs have wide applications in image and video...  28/07/2016   \n",
       "3986  The year 2015 was a monumental year in the fie...  28/12/2015   \n",
       "\n",
       "      n_code_chunks  bold_text_count  italic_text_count  mean_image_width  \\\n",
       "idx                                                                         \n",
       "3780              0             14.0               14.0             708.0   \n",
       "3986              0             26.0                6.0             578.0   \n",
       "\n",
       "      mean_image_height  n_images  n_lists  n_vids  n_links language  \n",
       "idx                                                                   \n",
       "3780              376.0      10.0      0.0     0.0      3.0       en  \n",
       "3986              375.0       9.0      0.0     0.0     32.0       en  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(SCRAPPING_OUT_PATH, 'scrapped_test.csv'))\n",
    "# data.to_csv(os.path.join(SCRAPPING_OUT_PATH, 'scrapped_train.csv'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "736c79110b90fcc0c5d76f382c6e50cefaa88c18e13916f93807986c023dc46d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
