idx,title,publication,author,followers,reading_time,n_words,pure_text,date,n_code_chunks,bold_text_count,italic_text_count,mean_image_width,mean_image_height,n_images,n_lists,n_vids,n_links,language
3756,Step-by-Step R-CNN Implementation From Scratch In Python,Towards Data Science,Rohit Thakur,206.0,8.0,800.0,"Classification and object detection are the main parts of computer vision. Classification is finding what is in an image and object detection and localisation is finding where is that object in that image. Detection is a more complex problem to solve as we need to find the coordinates of the object in an image.To Solve this problem R-CNN was introduced by Ross Girshick, Jeff Donahue, Trevor Darrell and Jitendra Malik in 2014. R-CNN stands for Regions with CNN. In R-CNN instead of running classification on huge number of regions we pass the image through selective search and select first 2000 region proposal from the result and run classification on that. In this way instead of classifying huge number of regions we need to just classify first 2000 regions. This makes this algorithm fast compared to previous techniques of object detection. There are 4 steps in R-CNN. They are as follows :-I am going to implement full R-CNN from scratch in Keras using Airplane data-set from http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html . To get the annotated data-set you can download it from link below. The code for implemented RCNN can also be found in the below mentioned repository.https://github.com/1297rohit/RCNN.gitOnce you have downloaded the dataset you can proceed with the steps written below.First step is to import all the libraries which will be needed to implement R-CNN. We need cv2 to perform selective search on the images. To use selective search we need to download opencv-contrib-python. To download that just run pip install opencv-contrib-python in the terminal and install it from pypi.After downloading opencv-contrib we need to initialise selective search. For that we have added the above step.Now we are initialising the function to calculate IOU (Intersection Over Union) of the ground truth box from the box computed by selective search. To understand more about calculating IOU you can refer to the link below.https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/The above code is pre-processing and creating the data-set to pass to the model. As in this case we can have 2 classes. These classes are that whether the proposed region can be a foreground (i.e. Airplane) or a background. So we will set the label of foreground (i.e. Airplane) as 1 and the label of background as 0. The following steps are being performed in the above code block.After running the above code snippet our training data will be ready. List train_images=[] will contain all the images and train_labels=[] will contain all the labels marking airplane images as 1 and non airplane images (i.e. background images) as 0.After completing the process of creating the dataset we will convert the array to numpy array so that we can traverse it easily and pass the datatset to the model in an efficient way.Now we will do transfer learning on the imagenet weight. We will import VGG16 model and also put the imagenet weight in the model. To learn more about transfer learning you can refer to the article on link below.https://medium.com/@1297rohit/transfer-learning-from-scratch-using-keras-339834b153b9In this part in the loop we are freezing the first 15 layers of the model. After that we are taking out the second last layer of the model and then adding a 2 unit softmax dense layer as we have just 2 classes to predict i.e. foreground or background. After that we are compiling the model using Adam optimizer with learning rate of 0.001. We are using categorical_crossentropy as loss since the output of the model is categorical. Finally the summary of the model will is printed using model_final.summary(). The image of summary is attached below.After creating the model now we need to split the dataset into train and test set. Before that we need to one-hot encode the label. For that we are using MyLabelBinarizer() and encoding the dataset. Then we are splitting the dataset using train_test_split from sklearn. We are keeping 10% of the dataset as test set and 90% as training set.Now we will use Keras ImageDataGenerator to pass the dataset to the model. We will do some augmentation on the dataset like horizontal flip, vertical flip and rotation to increase the dataset.Now we start the training of the model using fit_generator.Now once we have created the model. We need to do prediction on that model. For that we need to follow the steps mentioned below :As you can see above we created box on the proposed region in which the accuracy of the model was above 0.70. In this way we can do localisation on an image and perform object detection using R-CNN. This is how we implement an R-CNN architecture from scratch using keras.You can get the fully implemented R-CNN from the link provided below.https://github.com/1297rohit/RCNN.gitIf you would like to learn step by step about Face Detection and Face Recognition from scratch then you can head over to my article on that topic on the link : https://medium.com/@1297rohit/step-by-step-face-recognition-code-implementation-from-scratch-in-python-cc95fa041120Enjoy R-CNN !",18/10/2019,11,17.0,0.0,670.0,352.0,5.0,3.0,0.0,6.0,en
3757,How Transformers Work,Towards Data Science,Giuliano Giacaglia,1100.0,14.0,2730.0,"If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:www.holloway.comTransformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar — their program to defeat a top professional Starcraft player.Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..For models to perform sequence transduction, it is necessary to have some sort of memory. For example let’s say that we are translating the following sentence to another language (French):“The Transformers” are a Japanese [[hardcore punk]] band. The band was formed in 1968, during the height of Japanese music history”In this example, the word “the band” in the second sentence refers to the band “The Transformers” introduced in the first sentence. When you read about the band in the second sentence, you know that it is referencing to the “The Transformers” band. That may be important for translation. There are many examples, where words in some sentences refer to words in previous sentences.For translating sentences like that, a model needs to figure out these sort of dependencies and connections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been used to deal with this problem because of their properties. Let’s go over these two architectures and their drawbacks.Recurrent Neural Networks have loops in them, allowing information to persist.In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. A loop allows information to be passed from one step to the next.The loops can be thought in a different way. A Recurrent Neural Network can be thought of as multiple copies of the same network, A, each network passing a message to a successor. Consider what happens if we unroll the loop:This chain-like nature shows that recurrent neural networks are clearly related to sequences and lists. In that way, if we want to translate some text, we can set each input as the word in that text. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information.The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.Consider a language model that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence “the clouds in the sky”, we don’t need further context. It’s pretty obvious that the next word is going to be sky.In this case where the difference between the relevant information and the place that is needed is small, RNNs can learn to use past information and figure out what is the next word for this sentence.But there are cases where we need more context. For example, let’s say that you are trying to predict the last word of the text: “I grew up in France… I speak fluent …”. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text.RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.In theory, RNNs could learn this long-term dependencies. In practice, they don’t seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem.When arranging one’s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.RNNs don’t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important.Internally, a LSTM looks like the following:Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output. I won’t go into detail on the mechanics of each cell. If you want to understand how each cell works, I recommend Christopher’s blog post:colah.github.ioWith a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating.The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don’t do too well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. Not only that but there is no model of long and short range dependencies. To summarize, LSTMs and RNNs present 3 problems:To solve some of these problems, researchers created a technique for paying attention to specific words.When translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so.Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens.The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention.For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage.The step in green in charge of creating the hidden states from the input. Instead of passing only one hidden state to the decoders as we did before using attention, we pass all the hidden states generated by every “word” of the sentence to the decoding stage. Each hidden state is used in the decoding stage, to figure out where the network should pay attention to.For example, when translating the sentence “Je suis étudiant” to English, requires that the decoding step looks at different words when translating it.Or for example, when you translate the sentence “L’accord sur la zone économique européenne a été signé en août 1992.” from French to English, and how much attention it is paid to each input.But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text.Convolutional Neural Networks help solve these problems. With them we canSome of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are Convolutional Neural Networks.The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. Not only that, but the “distance” between the output word and any input for a CNN is in the order of log(N) — that is the size of the height of the tree generated from the output to the input (you can see it on the GIF above. That is much better than the distance of the output of a RNN and an input, which is on the order of N.The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why Transformers were created, they are a combination of both CNNs with attention.To solve the problem of parallelization, Transformers try to solve the problem by using Convolutional Neural Networks together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another.Let’s take a look at how Transformer works. Transformer is a model that uses attention to boost the speed. More specifically, it uses self-attention.Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders.Each encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network.The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.Note: This section comes from Jay Allamar blog postLet’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.Each word is embedded into a vector of size 512. We’ll represent those vectors with these simple boxes.The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512.In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented — using matrices.The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.Multiplying x1 by the WQ weight matrix produces q1, the “query” vector associated with that word. We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence.What are the “query”, “key”, and “value” vectors?They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.Transformers basically work like that. There are a few other details that make them work better. For example, instead of only paying attention to each other in one dimension, Transformers use the concept of Multihead attention.The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating “kicked” in the sentence “I kicked the ball”, you may ask “Who kicked”. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like “Did what?”, etc…Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation.I gave an overview of how Transformers work and why this is the technique used for sequence transduction. If you want to understand in depth how the model works and all its nuances, I recommend the following posts, articles and videos that I used as a base for summarizing the technique",11/03/2019,0,48.0,1.0,872.0,453.0,26.0,3.0,0.0,42.0,en
3758,Neural Style Transfer using VGG model,Towards Data Science,Darshan Adakane,27.0,7.0,1013.0,"Introduction:Before we begin, let’s go to this website to get some inspiration. On the website, we choose a photo from the local computer (let’s assume the image named Joey.jpg). Let’s call this content image. Then we choose another image, say style image named style1.jpg from the local computer. What this website does is produces a mixed image that preserves the contours of the content image and adds the texture and color pattern from the style image to the content image. Following is the result.Description:This is called Neural Style Transfer (NST) and is done by using Deep Learning, Convolution Neural Network (CNN) to be specific. I assume you are familiar with CNN. If not, I would highly recommend Andrew Ng’s Course on CNN.Let us understand the basics of NST with the help of the following flowchart. It shows the Style Transfer algorithm which has 13 convolutional layers (only a few are shown for simplicity). Two images are input to the neural network i.e. a content image and a style image. Our motive here is to generate a mixed image that has contours of the content image and texture, color pattern of the style image. We do this by optimizing several loss functions.The loss function for the content image minimizes the difference of the features activated for the content image corresponding to the mixed image (which initially is just a noise image that gradually improves) at one or more layers. This preserves the contour of the content image to the resultant mixed image.Whereas the loss function for the style image minimizes the difference between so-called Gram-matrices between style image and the mixed image. This is done at one or more layers. The usage of the Gram matrix is it identifies which features are activated simultaneously at a given layer. Then we mimic the same behavior to apply it to the mixed image.Using TensorFlow, we update the gradient of these combined loss functions of content and style image to a satisfactory level. Certain calculations of Gram matrices, storing intermediate values for efficiency, loss function for denoising of images, normalizing combined loss function so both image scale relative to each other.Coding :Now that we have understood the algorithm, let us begin coding. The original paper uses the VGG-19 model. But here we are going to use the VGG-16 model which is available publicly. Download the VGG-16 model from here (Please remember it is ~550MB file).In the root directory, create a new folder name it as vgg16 and paste the above file and vgg.py from the Github link. Also, we have modified the vgg16.py file by commenting out maybe_download function (since you have already downloaded the vgg16.tfmodel file)Let’s import the libraries first. Then import the vgg16 model.Let’s define a couple of helper functions for image manipulations.load_image load an image and returns a numpy array of floating points. The image is resized to a maximum of height or width.save_image saves an image to jpeg file with pixel values between 0 ad 255plot_image_big plots a larger image.plot_images plots content, style, and mixed image.Next, we will define loss functions that are used in Tensorflow for optimization.mean_squared_error operation will return a tensor that is mean squared error (MSE) difference between two input tensors.create_content_loss will calculate the MSE between the content and the mixed image. The loss is minimized so that the activation features of content are made similar to the mixed image so that it transfers the contours from content image to a mixed image.Next, our motive is to capture the style features of the mixed image. To do this, we will do something similar i.e measure which features activate for the style layers simultaneously and copy this pattern to the mixed image. An efficient way to do this is by calculating the Gram matrix. Gram matrix is essentially the dot product for the vectors of the feature activation layers. If the entry in the matrix has a smaller value it means the two features in the given layers do not activate simultaneously and vice versa.Let’s first define the calculating GramMatrix first as gram_matrix followed by create_style_loss which calculates the MSE of Gram matrix instead of the two raw tensors (as we did it in create_content_loss).To reduce the noise in the resultant mixed image, we use a denoising filtering algorithm called ‘Total Variation Denoising’ using the following code.The next section of code is the core. We will define the Style Transfer Algorithm that calculates the gradient descent on the loss functions. The algorithm uses normalization such that loss values are equal to one that helps in choosing the loss weights independent of the content and style layers.At first, we will load the content image which has contour feature we wish to be in the mixed image. Here I have named it as 1.jpgNow, we will load the style image of whose we want the color and texture to appear in the mixed image.Further, we will define the layer indices that we want to match the content image. Usually, it is a few layers just after the start. The 5th layer (index is 4) in the VGG-16 seems to work well. Similarly, the layer indices for the style image. Usually, it is towards the end of the total layer. Here we will define the 13th layer.The final part of the execution is applying style transfer to our defined content and style images. As defined, it automatically creates the loss functions for content and style layers and performs optimizations based on the number of iterations. Finally, displaying the mixed image.Results:P.S. This process may run slow on CPU (I am using CPU). Let’s see what results are given.Conclusion:Not bad! The results demonstrate the basic idea of combining two images. They are not at par like DeepArt who are pioneers in these techniques. Perhaps more iterations, smaller step sizes, higher resolution images, changing style and content layer indices or more computational power would increase the quality of the mixed image.Thanks for reading out the article. I hope it helps.You can find the github repo at this link.Reference :[1] Hvass-Labs, TensorFlow Tutorial #15 Style Transfer (2018), Source",16/01/2020,0,17.0,0.0,1208.0,378.0,26.0,0.0,0.0,7.0,en
3759,Amazon’s Artificial Artificial Intelligence,Medium,Sachin Palewar,130.0,1.0,127.0,"Today, we build complex software applications based on the things computers do well, such as storing and retrieving large amounts of information or rapidly performing calculations. However, humans still significantly outperform the most powerful computers at completing such simple tasks as identifying objects in photographs — something children can do even before they learn to speak.When we think of interfaces between human beings and computers, we usually assume that the human being is the one requesting that a task be completed, and the computer is completing the task and providing the results. What if this process were reversed and a computer program could ask a human being to perform a task and return the results? What if it could coordinate many human beings to perform a task?http://www.amazon.com/gp/browse.html/ref=aws_gen_nl13amt/104-0890544-5297546?node=15879911",21/11/2005,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,en
3760,"This week in the #SDGs- February 17, 2017",SDG Counting,SDGCounting,679.0,2.0,280.0,"1 . IISD provided context to news that the report of the 48th Statistical Commission (coming up March 7th-10th in New York) intends to include a draft resolution on the global indicator framework for the UN Economic and Social Council (ECOSOC) and the UN General Assembly to adopt. Last year, through the 47th Statistical Commission, the global indicator framework was agreed upon as a starting point and “taken note of by ECOSOC” in June 2016. A formal adoption would mean that methodology standards for indicator review and revision would be followed, as well as coming closer to the acceptance of all 230 indicators by all member states.sdg.iisd.org2. The Global Festival of Ideas for Sustainable Development is less than two weeks away, and a detailed agenda of the Festival has been released. Sponsored by the UN SDG Action Campaign, and other organizations and partnerships, the event is marketed as the first “playable” conference on the SDGs and highlights interactive solutions for understanding and achieving the SDGs. We’re interested because many of the policy sessions during the three-day conference must have a data element, and this is the first in a series of annual forums hosted by the UN SDG Action Campaign.globalfestivalofideas.org3. Released last month, after the UN World Data Forum, this paper takes a sharper look at alternatives to the household survey, and how civil registration and vital statistics can play a role in National Statistics Offices.devinit.org4. The World Government Summit was held this week in Dubai. While data and the Sustainable Development Goals were not front and center, the SDGs were part of the conversation. The UN Foundation created a twitter storify that highlights the UN’s involvement in the summit.storify.com",17/02/2017,0,17.0,0.0,0.0,0.0,0.0,0.0,0.0,15.0,en
3761,Views From A College Dropout’s Unconventional Life — Year 2,Medium,Tam Pham,2300.0,7.0,1350.0,"If you asked me where I would be right now a year ago, my prediction wouldn’t even come close.I had the opportunity to apprentice under radio show host and business coach, Margaret Jackson. On top of business skills, the biggest lesson she taught me was about legacy.What legacy do I want to leave behind in the world?Margaret told me to highlight my top 3 items on my bucket list. I wrote crazy goals likeMargaret looked at the rest of my (extensive) bucket list and started laughing to herself.“Tam, you know there are people in organizations devoting their lives to ONE of these goals. How the hell in the world are you going to accomplish everything?”Maintaining focus was the next crucial lesson. Mozart was known for music. Michael Jordan was known for basketball. How can I focus?I threw away my bucket list and wrote a single goal:Impact millions of lives with my message and through my actions.Vague, I know. But it was much better.I felt I had a story to share and wanted to spread this message to other young people. I did something I never thought I would try: motivational speaking.I always thought motivational speakers were full of crap who just wanted you to buy their book, but storytelling is such a powerful thing.I spoke at events like Independence High School, AIESEC, and Rotary International.I even had a College Rejection speech go semi viral.The best part was when people came up to you afterward and told you that your speech was legit.While this path sounded pretty nice, San Jose City tested my ego when they invited me to speak at their Youth Conference. I said yes for the fame, not for the benefit of helping others, and turned the opportunity down two days later.Summer came around and I went to work at Camp BizSmart, arguably the best youth entrepreneurship program in the world, held at Stanford University.For three months, I revisited my love for teaching. Working 1 on 1 with teens and helping them turn their ideas into reality.“You’re pretty cool for a grown up.”— One random studentI reflected and asked myself, how else can I help more young people and teach them something useful?That was when I launched my podcast, Outside Of The Classroom. I had the honor to interview amazing guests like Charlie Hoehn, Thomas Frank, and Ryan Porter.The podcast was on the front page of iTunes for a short while and gathers a few thousand listens a month. Nothing amazing, but one is greater than zero.How can I impact more people? What makes me happy? That was when the idea hit me: a book.I hated reading and writing throughout high school so a book was never even one of my dreams. Who knew?I spent the next 3 months writing my new book, How To Network, and it became an Amazon bestseller reaching 3,000+ people on launch week. You can get a free copy here.During this emotional high, I still felt like I wasn’t thinking big enough. Sure, I have a few thousand people reading my work, but how can I make an impact like Uber or Airbnb? Why am I thinking so small?This thought (and my friend Jj Tang) inspired me to enroll in Draper University, a 7-week entrepreneurship program. But it’s nicknamed to be a “human accelerator.”I wrote about my honest review of the program and lessons learned, but the most relevant takeaway was that there is no rush to start a startup.It’s difficult, complicated, and will eat up your life.And if I didn’t have a problem that I was so passionate about solving, why create a business?Also during this time, I worked on the Community Team for the Lean Startup Conference. Everything was a huge success.We even got to eat dinner at Eric Ries’ house!These two experiences helped me realize my love for people and community. When I asked Sam Parr, founder of The Hustle, what rocketship company should I join next…I didn’t know if he was serious, but I attended Hustle Con (their flagship conference) last year and loved everything they were doing. Their media company was already reaching hundreds of thousands of people. My role would involve writing, marketing, and building community… that sounds fucking perfect.I started my job the following week and I am currently sleeping on Sam’s couch in SF.We’re doing a trial period and if we both like each other, then there will definitely be a future. If it’s not a perfect fit, no sweat. On to the next venture!My current friendships were greatly weakened after dropping out of college. As expected.My life became so spontaneous, constantly experimenting with whatever sparked my curiosity. My friends didn’t know where I was most days and to be honest, I wouldn’t know either.The good (and bad) part of life exploration is that you become so open to anything. I never had any idea on what’s coming.Who knew Lean Startup Company would offer me a role? Who knew I would write a book? Who knew I would now be sleeping on the couch working for a new startup?With my constant travels, I spent time with friends who were closest around me. Whether that would be my awesome co-workers, housemates, and even my classmates.If this was the unconventional lifestyle that I wanted to pursue, I would have to make sacrifices and that is perfectly okay.If I do end up living in San Francisco, I know that this was my chance to start (kind of) fresh. (If you’re in SF, hit up your boy Tam Pham)They still don’t “get it.” No matter how much I try to explain what I’m doing, they think I’m wasting my time.And it’s difficult to explain what I’m doing because even I don’t know what I’m doing. I’m exploring careers, passions, and life. Creating stuff. Trying new stuff.Failing at new stuff.I’m actually doing stuff! But my parents think this will go to waste without college and while I disagree, I totally understand their viewpoint.I got some street cred become a “bestselling author” (which isn’t hard to do) and working for a few legit companies. That made my mom proud.To be continued.I’ve improved a ton in the past couple of months (after reading the Brain Fog Fix) but in the beginning of the year, I was super inconsistent.It wasn’t the “I’m too lazy to go to the gym” kind of excuse, but it was more “How could I not eat out (for the 4th time that week) with my group of friends?”And I said, “fuck it, why not?”Silly me.If I end up living in SF, I’ll start investing in a gym membership somewhere. Hopefully, it’s cheap. But it probably won’t be.Exactly a year ago, I dropped out of college and everything has been a whirlwind. I had no (or very little) idea of what I was doing, no solid plan, no nothing. Just a gut feeling I was going in the right direction.If I had to look back at everything I’ve been through, I’d have no regrets on my path. I truly believe that to find out what you really want to do in life, you have to find out what you don’t want to do in life.Like Jeff Goins mentions in his book, The Art of Work,“Clarity comes with action.”Could I have gotten the same exposure in college? Probably. But the best learning experience for me was by doing.This was the year of learning, personal growth, and exploration.Today, my full focus is kicking ass with The Hustle. If you haven’t heard of us, think of a younger and cooler version of the Wall Street Journal. Or think Vice meets Fast Company.I genuinely believe The Hustle will be a household name over the next five years, join in on the action.Thank you to my buddy Heath Padgett’s awesome post for inspiration. For 2016, let’s kill it. I’m ready for you.— — —If you dig this article, a recommendation shows me, the Medium staff, and the Medium community that this content is worth sharing.Get my #1 bestselling book, How To Network: Instantly Build Trust & Respect With Anyone You Meet, for free here.",04/01/2016,0,8.0,5.0,979.0,750.0,14.0,1.0,0.0,14.0,en
3762,Introducción a Machine Learning,ECLaboratorio,Virginia Peón,28.0,1.0,48.0,"Machine Learning, Big Data, Deep Learning, … ¿por qué cada día se oyen mas estos términos? y de hecho ¿qué significan? Acompáñame a descubrirlo de forma sencilla y con muchos ejemplos en el siguiente vídeo:… Y la presentación llena de enlaces que te pueden ayudar a profundizar más:",05/12/2016,0,0.0,3.0,1400.0,628.0,1.0,0.0,0.0,0.0,en
3763,When Exactly Did It Get Cool To Be A Geek?,The Awl,The Awl,27000.0,10.0,2321.0,"by Jane HuIn the final episode of “Freaks and Geeks,” the Freaks group leader Daniel Desario accepts an invitation to play Dungeons & Dragons with the notoriously geeky A/V club. Surprised by Daniel’s warm receptivity to the game, the Geeks wonders what this means for their future status. As Bill puts it: “Does him wanting to play with us again mean he’s turning into a geek or we’re turning into cool guys?” Sam answers, “I’m going to go for us becoming cool guys.” It’s a nice ambiguous note on which to end the show.Outside the universe of “Freaks and Geeks,” a similar drift has occurred. Geekiness has accrued cachet, and geeks are becoming the cool guys. In an interview about the comedy web series “Geek Therapy,” actress America Young observed: “We started talking about how geek is ‘in’ now. Everyone, all of a sudden, was proudly waving their geek flag. How cool is it that traditional high school status had been turned on its head?”An interesting thing about the name of Paul Feig’s show: When the words ‘freaks’ and ‘geeks’ first came into circulation, they were interchangeable; to be a freak was always to be a geek: an aberration, a spectacle, something to be gawked at. That said, who is and who isn’t a geek has been contested since the word’s first printed appearance, in 1876, and up through today, when the borders between ‘geek’ and ‘nerd’ are continually getting drawn, redrawn and then redrawn again. But if ‘geek’ always used to mean ‘freak,’ when did the two groups become (as suggested by Feig’s show title) separate, even opposed? And when did that magic moment occur when it became okay, even cool, to be a geek?***‘Geek’ makes its first printed appearance in F.K. Robinson’s Glossary of Yorkshire Words and Phrases with this definition: “Gawk, Geek, Gowk or Gowky, a fool; a person uncultivated; a dupe.” Parked there between “gawk” and “gowk,” geeks have always been, like freaks, positioned as a spectacle. Indeed, as everyone who loves Katherine Dunn’s Geek Love knows, for the first half of the twentieth century “geek” was used to describe carnival sideshow acts and circus freaks: specifically, the sort who bit the heads off live chickens. The performance of standing in front of a crowd, biting off a chicken’s head might be seen as the original — and quite explicit — embodiment of the notion of geek as other: no one asked those geeks if they wanted to maybe go out for coffee later, either. As Stephen Shiff notes in a 1995 The New Yorker essay(sub. required) — the essay’s title “Geek Shows” a clear play on ‘freak shows’ — geeks of the time were sometimes even more marginalized than freaks, who “had been reduced to their spectacular status by terrible accidents of birth, whereas the geek’s calamity was entirely voluntary.”Freaks and Geeks. Freaks are Geeks. Freaks = Geeks = Ultimate OutsidersThe beauty of Feig’s title for the show is how it puts the words on two opposite sides of the see-saw: opposed but on the same platform. As the show’s final episode hints at, freaks aren’t all that different from geeks, and both can be cool. But to pose freaks and geeks as cool is to move them from the outside to the very most inside — to the cultural and social center to which every normal person should aspire. As a phrase, ‘cool geeks’ is a contradiction in terms. But that’s not saying much. Since the meanings of ‘freaks’ and ‘geeks’ started to diverge in the 60s, the interpretations of ‘geek’ have grown hopelessly muddled.Throughout the 80s, geeks became to replace other derogatory terms such as ‘square’ and ‘egghead.’ Geeks became detached from freaks only to enter other semantic wars, such as the one fought between geeks and nerds — one that seems (perhaps in principle) irresolvable.An attempt to parse out the connotations of ‘geek’ and ‘nerd’ tosses one into a whirl of disputes (many of them rigorous and turning on subtle discrepancies). But for the most part, geeks have been associated with expertise, while nerds are afflicted with social ineptness. While some militantly police the borders between ‘geek’ and ‘nerd,’ others don’t observe the difference. Geeks and nerds tussle over who has claim to the always-expanding, ever-powerful tech culture (a group once seen as a hyper-specialized subculture, but who have come to increasingly define Culture, with a capital C, at large).Throughout the geek v. nerd debates, many have maintained that be you geek or nerd, either way you just can’t be cool. As Steve Lohr, a New York Times technology columnist, writes, “a geek suggests a person with special expertise, while nerd suggests social ineptness. And neither are cool.”“Special expertise” leads one to consider the myriad of geek subcultures: computer geeks, anime geeks, coffee geeks. Of course, being that he’s a journalist with specialized expertise in the topics regarding technology and innovation, the inference here is that Lohr himself might be something of a geek.Then there’s “Freaks and Geeks” creator Paul Feig, who despite being a millionaire with a series of Hollywood successes to his credit, would be the first to assert his own uncoolness, as he did during this interview with a writer from The Guardian:“Oh, I’m very nervous. I mean, you just don’t know, do you?” he frets, his dapper three-piece suit, lavender tie and matching pocket handkerchief unable to conceal his jitteriness. “I’m fighting against LA’s tyranny of the casual,” he says of his attire, with self-mocking defiance. But the reviews of Bridesmaids have been amazing, I say. He counters by reciting criticisms he claims were sprinkled through the praise (despite a thorough search later, I cannot find a single one of them).As if all this weren’t enough to give his publicist a heart attack, Feig launches into a theory about why he’s “a failure.” This leads to him recounting the days “when I couldn’t even get a TV pilot made!” with such eagerness that one might think he was still a jobless hack, as opposed to someone involved in the best American TV of the past decade, including cult teen drama “Freaks and Geeks” (which he wrote), “Arrested Development,” “The Office,” “30 Rock,” “Bored to Death” and “Mad Men” (all of which he did directing stints on). “Well, it’s nice that people keep giving me a shot,” he concedes.I, for one, think Feig’s claim to being “a pretty feminized geek” is utterly cool. But who am I to say? I’m just another geek.***But somewhere along the way, just like Feig, “Freaks and Geeks” itself became cool. These days, the short-lived production is a cult classic often invoked to signal one’s (arguably geeky) devotion to a once marginalized outsider, and underappreciated television series. The show’s a 9.2/10 rating on IMDB is just one marker of the wide admiration it now enjoys. The geeky show got voted Most Popular, and as it did, Feig’s show, as a cultural artifact, became one more example of how what’s geeky pivots on a hinge that swings between cool and un-cool so effortlessly that it’s often difficult to pinpoint which side our allegiances should lie if we are to remain at the popular kids’ table. Insiders are cool because they know something about the outside of the mainstream: this places them inside the inside, at the heart of cool. Geeks specialize and obscurity is often an indicator of their success. They know more. They know earlier. Hence they are cool.It’s hard to pinpoint who was the original cool geek, but chances are this eminent First One had something to do with the Internet and, by extension, the information boom: Bill Gates, Steve Jobs, Sean Parker (now forever represented in my mind as a particularly manic Justin Timberlake). Geeks are characterized by a drive and enthusiasm that, as set against the slacker culture of Feig’s Freaks, has returned as being incredibly powerful. And cool.A 2010 Wall Street Journal article titled “How Steve Jobs Made Geek Culture Cool” claimed:But these days, we’re all geeks. Everyone’s got an iPod, an iPhone or an iPad, and it seems the whole world eagerly awaits to see what Jobs will do next. […] Geek culture is a new counterculture — a world full of amazing products and films, music and games for people who are proud to be nerdy and square.Being always one step off and one remove from the crowd is to the geek’s benefit. Being a geek affirms one’s sense of individuality — one’s special status — through being different. If the rest of the world is catching up, then the true geek is only going to try a little harder to be the first to adopt.Geeks push envelopes. Slipping from the OED’s definitional radars, the current-day geek contains multitudes. The ‘cool geek’ can be intelligent and athletic. He can fix your hard drive, while holding a perfectly sociable conversation. These days geeks can even, goodness forbid!, be sexy. If power, which Kissinger assured us is the ultimate aphrodisiac, is sexy, then Silicon Valley geeks possess a lot of it. Having lots and lots of money probably doesn’t hurt either.Popular entertainment has capitalized on the connections between geekiness and desire through television shows like “Beauty and the Geek” and “Geek Love.” That first show, a reality show promoted as “The Ultimate Social Experiment,” spins upon the logic that the possibility of romance among beautiful females and geeky males is a social challenge. Watching these figures move on your television screen, we’re supposed to wrinkle our brows because, yeah, beautiful people just don’t look like they belong with buttoned-up brains. But just as “Freaks and Geeks” once did, these shows strive to reconcile two socially and visually disparate groups.Remember that first-ever, 1867 printed mention of ‘geek,’ which paired ‘geek’ and ‘gawk’? That definition shows the word literally emerging from a function of sight: “Gawk, Geek, Gowk or Gowky.” The geek evolves from the gawk to become the awkwardly gowky. The Cool may be the Beautiful (or at least the conventional idea of what that is), but they still see themselves as the physical antithesis of the Geeky, the to-be-gawked-at.***Equating seeing with knowing leads to mistaking seeing as knowing. In “Beauty and the Geek,” beautiful women struggle to reconsider their first impressions of gawky awkward men by recognizing the beauty inside the Geek. If we cut the link between seeing and knowing, what’s Beautiful and what’s Geeky no longer looks so clear-cut. What if we shuttle sight to the side? What if we try to see what’s on the inside? If on the Internet nobody knows what you are then they don’t know what you look like. When physical embodiment can be obscured, or misrepresented, or even cease to matter, we’re forced to reorient our communication tactics. (The trope of geek chic glasses conveniently emphasizes the sacrifice of visual clarity in the accumulation of knowledge.) If geeks are notoriously awkward in person, they are equally notorious for their fluency in the impersonal science of mediated, wired technology.As Internet culture (and its facilitation of specialization and fandom) grows, being a geek is increasingly associated with knowing. In a New Yorker essay, music critic Alex Ross highlights how blogs and Internet culture genre nourishes the classical music geek: “If, as people say, the Internet is a paradise for geeks, it would logically work to the benefit of one of the most opulently geeky art forms in history.”In The Laws of Cool, literary and technology scholar Alan Liu links cool with a capacity for knowledge: “Cool is a feeling for information.” This lexical association between “cool” and knowing, intuiting, or “feeling […] information” follows from communication theorist Marshall McLuhan’s division of technological media into the categories of “hot” and “cool.” According to McLuhan, interaction with “cool” mediums activated all of one’s senses, whereas “hot” mediums only triggered one sense — that of sight. Media is cool when it necessitates one’s fully active, stimulated, even erotic, engagement with a technology.As extensions of ourselves — as the things that help us see and help us be seen — technologies are as sexy as the geeks that work them. “Let me further point out how ubiquitously the word ‘sexy’ is used to describe late-model gadgets.” So writes Jonathan Franzen in a New York Times essay about his “infatuation” with a new BlackBerry. Franzen continues:… and how the extremely cool things that we can do now with these gadgets — like impelling them to action with voice commands, or doing that spreading-the-fingers iPhone thing that makes images get bigger — would have looked, to people a hundred years ago, like a magician’s incantations, a magician’s hand gestures; and how, when we want to describe an erotic relationship that’s working perfectly, we speak, indeed, of magic.Gadgetry, the Internet, infinite information. It’s hot because it requires our participation. It’s hot because it begs an act of extension — of engagement and agency — from us, for our fingers on the screen. It’s hot because it’s cool.Even the verb ‘to geek out’ (as opposed to the freaks who only ‘geeked’) implies an act or experience of excess. To geek out is to overdo it — to do it more and to do it better. More importantly for our ecology, to geek is to do it earlier. Being a geek is about caring, maybe even caring a little too much. If that’s what is currently perceived to be cool, I’m all for it.  Sponsored posts are purely editorial content that we are pleased to have presented by a participating sponsor, advertisers do not produce the content. This post is coming to you from MiO. Change your water. Change your day. What do you want to change?  Related: When Did The Remix Become A Requirement, Wikipedia And The Death Of The Expert and Free The Network  Jane Hu is our official correspondent for very recent history. Geeks and Nerds Venn diagram courtesy of xkcd.",22/02/2012,0,1.0,38.0,385.0,279.0,3.0,0.0,0.0,31.0,en
3764,K-Means vs. DBSCAN Clustering — For Beginners,Towards Data Science,Ekta Sharma,29.0,9.0,830.0,"Clustering is grouping of unlabeled data points in such a way that: The data points within the same group are similar to each other, and the data points in different groups are dissimilar to each other.The goal is to create clusters that have high intra-cluster similarity and low inter-cluster similarity.K-Means cluster is one of the most commonly used unsupervised machine learning clustering techniques. It is a centroid based clustering technique that needs you decide the number of clusters (centroids) and randomly places the cluster centroids to begin the clustering process. The goal is to divide N observations into K clusters repeatedly until no more groups can be formed.1. Decide the number of clusters. This number is called K and number of clusters is equal to the number of centroids. Based on the value of K, generate the coordinates for K random centroids.2. For every point, calculate the Euclidean distance between the point and each of the centroids.3. Assign the point to its nearest centroid. The points assigned to the same centroid form a cluster.4. Once clusters are formed, calculate new centroid for each cluster by taking the cluster mean. Cluster mean is the mean of the x and y coordinates of all the points belonging to the cluster.5. Repeat step 2, 3 and 4 until the centroids cannot move any further. In other words, repeat these steps until convergence.Label values represent the cluster number.The quality of clusters formed using K-Means largely depends on the selected value of K. A wrong choice of K can lead to poor clustering. So how to select K? Let’s take a look at the commonly used technique called “Elbow Method”. The goal is to select the K at which an elbow is formed.Steps:1. For different values of K, execute the following steps:2. For each cluster, calculate the sum of squared distance of every point to its centroid.3. Add the sum of squared distances of each cluster to get the total sum of squared distances for that value of K.4. Keep adding the total sum of squared distances for each K to a list.5. Plot the sum of squared distances (using the list created in the previous step) and their K values.6. Select the K at which a sharp change occurs (looks like an elbow of the curve).Looking at this plot, we can see the elbow at K=3 and hence that is our optimal number of clusters for this dataset.Sometimes we can end up with multiple values showing an elbow. In this case, to find the best K, an evaluation metric like Silhouette Coefficient can be used. The K that will return the highest positive value for the Silhouette Coefficient should be selected.DBSCAN is a density-based clustering algorithm that forms clusters of dense regions of data points ignoring the low-density areas (considering them as noise).DBSCAN uses the following two user defined parameters for clustering:Epsilon (eps): It is defined as the maximum distance between two points to be considered as neighboring points (belonging to the same cluster).Minimum Points (min_samples or minPts): This defines the minimum number of neighboring points that a given point needs to be considered a core data point. This includes the point itself. For example, if minimum number of points is set to 4, then a given point needs to have 3 or more neighboring data points to be considered a core data point.If minimum number of points meet the epsilon distance requirement then they are considered as a cluster.1. Decide the value of eps and minPts.2. For each point:3. For each core point, if it not already assigned to a cluster than create a new cluster. Recursively find all its neighboring points and assign them the same cluster as the core point.4. Continue these steps until all the unvisited points are covered.Label = -1 means it is a noise point (outlier).Label = 0 or more, indicates the cluster number.DBSCAN clustering algorithm is sensitive to the eps value we choose. So how can we know that we have selected the optimal eps value? Here is a commonly used technique called “Knee method”. The goal is to find the average of distances for every point to its K nearest neighbors and select the distance at which maximum curvature or a sharp change happens. The value of K is set to be equal to minPoints.Here is an example to show optimal eps value selection using Scikit Learn’s NearestNeighbors module.The optimal value should be the value at which we see maximum curvature which in this case seems to be near 0.5.Sometimes we can end up with multiple values showing a sharp change. In this case, to find the best K, an evaluation metric like Silhouette Coefficient can be used. The K that will return the highest positive value for the Silhouette Coefficient should be selected.When to use which of these two clustering techniques, depends on the problem. Even though K-Means is the most popular clustering technique, there are use cases where using DBSCAN results in better clusters.",27/05/2020,9,16.0,4.0,476.0,336.0,7.0,5.0,0.0,0.0,en
3765,Lou Reed: Entre “Transformer” e “Berlin” o mito.,Medium,Igor de Sousa,6.0,3.0,609.0,"as mentiras e conturbações feitas a vida do “monstro” e “mito” Lou Reed atráves de biográfias e material descártavel são absurdamentes grandes, mas de certa forma provacadas pelo mesmo, uma figura icônica e contráditoria.antes da fama Lou já era conturbado, mas a primeira aparição do Velvet em uma zine desmistifica algumas das alegações tardias do cantor e guitarrista. Lou Reed cita os beatles como criativos e absolutamente magníficos, assim como os Stones. ele cita Creedence como legais a primeira ouvida mas depois absolutamente tedioso e desgastante era 1972 e Lou estava no caminho de albúns como “Transformer” que absorvia toda a atmosfera Glam da época com uma pegada Rock and Roll, e ao mesmo tempo definia uma época, a ambiguidade sexual em sua capa e contra-capa, a volta de o Rock and Roll em “hagin’ round” no estilo anos 50, músicos prestisiosos trabalhavam com ele no processo, mas de nomes que Reed odiava, como Alice Cooper. Lou conheceu Bowie quando ele havia estado na Factory para contatar Andy Warhol durante os anos 60. Sempre cita o com palavras como: Esse menino tem tudo! absolutamente íncrivel!A entrada de Lou para o Glam Rock foi fácil sua bisexualidade e apelo sexual o confiriram um papel importante em Londres onde as coisas realmente estavam acontecendo. A decadência selvagem com linhas de baixo divertidas e um arranjo de guitarra variando de dó e fá, deu ao hit “Walk on the Wild Side” popularidade internacional, o Saxofone e o vocal em músicas como “Goodnight Ladies” dão um clima cabaré tudo isso adsomado com o apelo da mídia a David Bowie, amigo e discipúlo do cantor, mesmo que tendo passado por terras mods e hippies, em seus primeiros discos, terras que Reed desconhecia.eu aproveito esse artigo que é parte crítica que vinha dispondo a fazer e parte o choque com a nova bibliografia de Lou Reed para dar minha humilde visão sobre o nome.Estava presente quando o Velvet era quase inacessivel, você tinha que ir em sebos para achar discos como “Velvet Underground e Nico” Transformer e etc, e fui me habituando a sempre ler o que aparecia e ver o que estava disponível sobre o Warhol, em suas biblíografias encontrei muito sobre Lou e sua trupe de Junkys e uma “peça” que Warhol tinha montado em Nova York sua própria revolução cultural.Reed realmente como foi acusado em sua nova bibliografia, era violento ele própio deixa isso transparente em discos como “Berlin” Carolline says, parte dois, cita frases como“qual é a razão de bater-me, eu já não vou ama-lo mais”Citado como cinema para os ouvidos, tendo o produtor Bob Enzrin orquestrando perfeitamente cada movimento músical, era diferente de tudo que Reed tinha feito, usando nomes como Carolline e Jim como personagens de um filme grotesco e brutal, o disco tem seus altos e altos, Lou usou a história do disco como máscara para citar seus sentimentos mais introspectivos. O disco trata de amor, em sua visão contubarda e brutal, chegando ao suícidio citado em “The Bed”, pontos como “Oh!Jim” citam um Lou cansado de amigos que o depositavam no palco, pílulas, Lou cita “i don’t care… i’m just like a Alley cat” aspectos próprios de um Junky em desenvolvimento pessoal e artístico querendo atingir a profundidade de um mundo superficial que é o que vemos em seu trabalho relevante anterior “Transformer”, mas sem esquecer que o disco se trata de dois personagens ficcionais Carolline e Jim.cada um de nós temos um lado Berlin.Meláncolico, e inesperado pela crítica, que esperava um sgt. peppers ao estilo anos 70, mas saiu um “Tommy” sem ser um pé no saco, e sim realmente lindo, sem pretensões, e sim sentimentos.",13/11/2015,0,0.0,0.0,273.0,188.0,3.0,0.0,0.0,0.0,pt
3766,Machine Learning is Fun!,Medium,Adam Geitgey,55000.0,15.0,2861.0,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in 日本語, Português, Português (alternate), Türkçe, Français, 한국어 , العَرَبِيَّة‎‎, Español (México), Español (España), Polski, Italiano, 普通话, Русский, 한국어 , Tiếng Việt or فارسی.Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let’s change that!This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That’s what this is.The goal is be accessible to anyone — which means that there’s a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished.Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic.“Machine learning” is an umbrella term covering lots of these kinds of generic algorithms.You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. The difference is simple, but really important.Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses.To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for.So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:Using that training data, we want to create a program that can estimate how much any other house in your area is worth:This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.This kind of like having the answer key to a math test with all the arithmetic symbols erased:From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right.In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!”So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”.Unfortunately “Machine Figuring out an equation to solve a specific problem based on some example data” isn’t really a great name. So we ended up with “Machine Learning” instead.Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this:If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.Wouldn’t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number:One way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there’s an exact ratio of ingredients to stir in to make the final price.That would reduce your original function (with all those crazy if’s and else’s) down to something really simple like this:Notice the magic numbers in bold — .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!A dumb way to figure out the best weights would be something like this:Start with each weight set to 1.0:Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house.Now add up the squared amount you are off for each house you have in your data set. Let’s say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That’s how “wrong” your function currently is.Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function.If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that’s our goal — get this cost to be as low as possible by trying different weights.Repeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you’ve solved the problem!That’s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!But here’s a few more facts that will blow your mind:Pretty crazy, right?Ok, of course you can’t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you’d never run out of numbers to try.To avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here’s one way:First, write a simple equation that represents Step #2 above:Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):This equation represents how wrong our price estimating function is for the weights we currently have set.If we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we’d get a graph that might look something like this:In this graph, the lowest point in blue is where our cost is the lowest — thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we’ll have our answer!So we just need to adjust our weights so we are “walking down hill” on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we’ll eventually get there without having to try too many different weights.If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function’s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we’ll reach the bottom of the hill and have the best possible values for our weights. (If that didn’t make sense, don’t worry and keep reading).That’s a high level summary of one way to find the best weights for your function called batch gradient descent. Don’t be afraid to dig deeper if you are interested on learning the details.When you use a machine learning library to solve a real problem, all of this will be done for you. But it’s still useful to have a good idea of what is happening.The three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you’ve never seen before based where that house would appear on your line. It’s a really powerful idea and you can solve “real” problems with it.But while the approach I showed you might work in simple cases, it won’t work in all cases. One reason is because house prices aren’t always simple enough to follow a continuous line.But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.Also, I ignored the idea of overfitting. It’s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren’t in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it’s a skill that any developer can learn!Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have.For example, if you build a model that predicts home prices based on the type of potted plants in each house, it’s never going to work. There just isn’t any kind of relationship between the potted plants in each house and the home’s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.So remember, if a human expert couldn’t use the data to solve the problem manually, a computer probably won’t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn’t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it’s getting a little better every day.If you want to try out what you’ve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!If you want to go deeper, Andrew Ng’s free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.Also, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It’s a python framework that has “black box” versions of all the standard algorithms.If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter:Also, please check out the full-length course version of this article. It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course if you sign up with this link.You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.Now continue on to Machine Learning is Fun Part 2!",05/05/2014,4,37.0,73.0,1034.0,528.0,11.0,1.0,0.0,45.0,en
3767,Master the COCO Dataset for Semantic Image Segmentation — Part 1 of 2,Towards Data Science,Viraf,149.0,10.0,1342.0,"COCO (Common Objects in Context), being one of the most popular image datasets out there, with applications like object detection, segmentation, and captioning - it is quite surprising how few comprehensive but simple, end-to-end tutorials exist. When I first started out with this dataset, I was quite lost and intimidated. I had to plough my way through so many scattered, inadequate resources on the web, multiple vague tutorials, and some experimentation to finally see light at the end of this tunnel. When I was done, I knew I had to document this journey, from start to finish. And so I did. With the hope that someday, someone out there would find these of value and not have to go through all the trouble I faced.Here’s presenting you a two part series comprising of a start-to-finish tutorial to aid you in exploring, using, and mastering the COCO Image dataset for Image Segmentation. I’ll try to keep it as simple as possible, provide explanations for every step, and use only free, easy libraries. My job here is to get you acquainted and comfortable with this topic to a level where you can take center stage and manipulate it to your needs!In Part 1, we will first explore and manipulate the COCO dataset for Image Segmentation with a python library called pycoco. This library eases the handling of the COCO dataset, which otherwise would have been very difficult to code yourself.In Part 2, we will use the Tensorflow Keras library to ease training models on this dataset and add image augmentations as well.You can find the entire code for this tutorial in my GitHub repository. However, continue reading this post for a much more detailed explanation.“COCO is a large-scale object detection, segmentation, and captioning dataset.”Common Objects in Context (COCO) literally implies that the images in the dataset are everyday objects captured from everyday scenes. This adds some “context” to the objects captured in the scenes. Explore this dataset here.COCO provides multi-object labeling, segmentation mask annotations, image captioning, key-point detection and panoptic segmentation annotations with a total of 81 categories, making it a very versatile and multi-purpose dataset.In this walk-through, we shall be focusing on the Semantic Segmentation applications of the dataset.You’ll need to download the COCO dataset on to your device (quite obviously). You can download the 2017 dataset files using the links below. The files are quite large, so be patient as it may take some time. The files you need are:(a) 2017 train images(b) 2017 val images(c) 2017 Stuff Train/Val annotationsExtract the zipped files. From (c) only the instances files for train and val are needed i.e. instances_train2017.json and instances_val2017.json.Arrange these files as the file-structure given below. Some simple re-arrangement and re-naming of folders and files is required. I have done this just for ease of use and visualization, if you don’t wish to do it, all you need to do is change the code (mainly the file paths) accordingly.#Note that there is a way to access images with their URLs (from the annotations file), which would require you to download only (c). However, during a lengthy training process, it’s better if you do not depend on the internet and hence I recommend downloading (a) and (b) as well.Next, let’s install our major library, pycoco. I am particularly stating the procedure for this because you’ll face many errors in the installation and import process if you follow your standard procedure.In Conda, if you follow the procedure below, you should be able to install, import and use this library quite smoothly.Let’s import all the libraries we’ll be using for this tutorial. The installation for the other libraries is quite straightforward, so I won’t be mentioning the details here. Install all the libraries in your python environment.First, let’s initiate the PyCoco library. This library takes the COCO annotations (.json) file (the ones we downloaded in step 2) as an input. Either train or val instances annotations should work, but for this tutorial, I am using “instances_val.json” since it’s faster to load (reason: val dataset is smaller than train dataset).The output, i.e. the categories are printed as:The COCO dataset has 81 object categories (note that ‘id’:0 is background), as we printed out above (also listed here). However as you can observe, the label ids for these 81 classes range from 0 to 90 (some empty id numbers in between). Here is a convenient function which can fetch a class name for a given id number.This will give an output:Let’s say I want images containing only the classes “laptop”, “tv”, and “cell phone” and I don’t require any other object class. To get this subset of the dataset, follow the steps below:Now, the imgIDs variable contains all the images which contain all the filterClasses. The output of the print statement is:This implies, out of the entire validation dataset, there are 11 images which contain ALL the 3 classes which I wanted. And a sample image displayed is:To display the annotations we shall follow the code as below. Note that we use pycoco functionalities “loadAnns” to load the annotations concerning the object in COCO format and “showAnns” to sketch these out on the image. These functions largely ease the drawing of the annotation masks. You can have a look at the linked function definitions to see how they work internally.When we filter the dataset with classes, the pycoco tool returns images which consist of only ALL your required classes, not one or two or any other combinations. Thus, this piece of code will ensure that all possible combinations for the given filterClass lists are obtained in the resultant dataset.The output of the print statement is:See how above, we had received only 11 images, but now there are 503 images! We have avoided any repetition of images as well.For any Semantic Segmentation training task, you’ll require a folder full of the images (train and val) and the corresponding output ground-truth masks. This section will help create the corresponding image masks.Each pixel has a label according to the class it falls into. I am not using the official COCO ids, but instead allotting pixel values as per the order of the class name in the array ‘filterClasses’, i.e.:0: background1: laptop2: tv3: cell phoneThe output is a 2-channel semantic segmentation mask with dimensions equal to the original image, as displayed below:In general, your output mask will have N possible pixel values for N output classes. However, binary masking implies that the output mask will have only 2 pixel values, i.e., 1 (object: could be any of the N classes) and 0 (the background).The output is a 2-channel binary semantic segmentation mask with dimensions equal to the original image, as displayed below:Note: the format of how your desired masks can be different from the ones mentioned above. For example, you might want to keep the label id numbers the same as in the original COCO dataset (0–90). Or you might want an output format for an instance segmentation use case. Another example is, you might want your masks to be one-hot-encoded, i.e., number of channels = number of output object classes, and each channel having only 0s (background) and 1s (that object).For any case, modify the above code accordingly and you can get the mask as you desire. If I get some time in the future, I’ll try to add the codes for these additional types as well.Next, let’s move on to Part 2.towardsdatascience.comAgain, the code for this tutorial in my GitHub repository.And my friends, that’s it for the day! If you have come so far, I hope you have attained some kind of confidence with the COCO dataset. But don’t stop here — get out there, experiment the hell out of this, and rock the world of image segmentation with your new ideas!If you liked this article, this next one shows you how to easily multiply your image dataset with minimal effort. Do give it a read!towardsdatascience.comOr want to be rich overnight using ML in stocks? This article is (NOT) for you!towardsdatascience.comCan an ML model literally read the stock price charts?towardsdatascience.com",03/05/2020,5,16.0,19.0,586.0,374.0,7.0,0.0,0.0,22.0,en
3768,IT Support Ticket Classification and Deployment using Machine Learning and AWS Lambda,Towards Data Science,Pankaj Kishore,62.0,17.0,3202.0,"Project Description and initial assumptions:As a part of our final project for Cognitive computing, we decided to address a real life business challenge for which we chose IT Service Management. Of all the business cases, we were interested with four user cases that might befitting for our project.1. In Helpdesk, almost 30–40% of incident tickets are not routed to the right team and the tickets keep roaming around and around and by the time it reaches the right team, the issue might have widespread and reached the top management inviting a lot of trouble.2. Let’s say that users are having some trouble with printers. User calls help desk, he creates a ticket with IT Support, and they realize that they need to update a configuration in user’s system and then they resolve the ticket. What if 10 other users report the same issue. We can use this data and analyze the underlying issue by using unstructured data. Also, we can analyze the trend and minimize those incident tickets.3. Virtual bot that can respond and handle user tickets.Of the three use cases, we decided to go with the first use case for now owing to our time and resource limitation.Tools and technologies used:● ServiceNow Instance( an IT Service managementplatform)● AWS Lambda function● EC2 Instance● Python● Neural Network models● 50000+ sample tickets from open sourcesThe overall workflow of the process can be divided into various sub parts. A very high-level overview is shown in the figure below.Dataset:We pulled the dataset directly from servicenow and tried to classify them using ML techniques to make a labelled dataset. We just kept the description and categories from the ticket and went ahead with analytics on it to come up with top 5 categories for incidents.The labels were added once we have classified the top 5 incident categories using Topic ModellingUse Case Overview:Most of the critical issues are handled and being tracked in the form of tickets in a typical IT environment. IT infrastructure are group of components connected together in a network. So, when a component goes down, it takes many other components along with it. It may even take days to figure out what went wrong and find the root cause. So, it is essential to notify the component ownersImmediately so that proper measures can be taken to prevent this from happening.These issues are usually reported by users to helpline or Service Desk who are the first line of support. Unfortunately, it is difficult for anyone to be aware of all the components and hence tickets are being routed to the wrong team. For it to reach the original team it might even take days or weeks.Hence, we thought we could use NLP to solve this issue.IT Service Management, on an average generates more than 40 GB of data. That gives us enough data to train and implement our model.Our model when properly implemented could save billions of dollars to business units, as they lose many Service level agreements because of this.Cloud-computing based ITSM:This business case was almost impossible to handle before the arrival of cloud computing. Cloud computing made this integration between machine learning algorithms and IT Service Management possible at the first place.For our business case, we used a cloud-based IT Service Management application named Servicenow and we accessed the Machine learning model wrapped inside the AWS lambda function using API gateway.Integration between ServiceNow and AWS:Web services make it possible for applications to connect to other software applications over a network allowing an exchange of information between the provider (server) and client (consumer).A web service consumer (client) requests information from a web service provider (server). A web service provider processes the request and returns a status code and a response body. When the response body is returned, the web service consumer extracts information from the response body and acts on the extracted data.ServiceNow can consume web services from third party providers or from another ServiceNow instance.In our case, we used the endpoint url trigerred by the API Gateway using Rest web services and accessed it using javascript that runs on creation of ticket.On creation of a ticket, the javascript is triggered which sends the incident description to our model placed in AWS which performs the machine learning operations and returns back the predicted categories and probablity.· We pick the number of topics ahead of time even if we’re not sure what the topics are.· Each document is represented as a distribution over topics.· Each topic is represented as a distribution over words.We used NLTK’s Wordnet to find the meanings of words, synonyms, antonyms, and more. In addition, we use WordNetLemmatizer to get the root word.We then read our dataset line by line and prepare each line for LDA and store in a list.First, we are creating a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use.We then tried finding out 5 topics using LDA :-pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.From Topic Modelling we came to conclusion that the whole dataset can be divided into 5 categories: -· Network· User Maintenance· Database· Application Workbench· SecurityWe then labelled our dataset accordingly and prepared a dataset to perform supervised learning on it.An end-to-end text classification pipeline is composed of following components:1. Training text: It is the input text through which our supervised learning model is able to learn and predict the required class.2. Feature Vector: A feature vector is a vector that contains information describing the characteristics of the input data.3. Labels: These are the predefined categories/classes that our model will predict4. ML Algo: It is the algorithm through which our model is able to deal with text classification (In our case : CNN, RNN, HAN)5. Predictive Model: A model which is trained on the historical dataset which can perform label predictions.A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence.Using the knowledge from an external embedding can enhance the precision of your RNN because it integrates new information (lexical and semantic) about the words, an information that has been trained and distilled on a very large corpus of data.The pre-trained embedding we used is GloVe.RNN is a sequence of neural network blocks that are linked to each others like a chain. Each one is passing a message to a successor.a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:Long Short Term Memory networks — usually just called “LSTMs” — are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.The repeating module in a standard RNN contains a single layer.LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.The repeating module in an LSTM contains four interacting layers.Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”An LSTM has three of these gates, to protect and control the cell state.The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C̃ tC~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.It’s now time to update the old cell state, Ct−1Ct−1, into the new cell state CtCt. The previous steps already decided what to do, we just need to actually do it.We multiply the old state by ftft, forgetting the things we decided to forget earlier. Then we add it∗C̃ tit∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh (to push the values to be between −1−1 and 11) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.To use Keras on text data, we first have to preprocess it. For this, we can use Keras’ Tokenizer class. This object takes as argument num_words which is the maximum number of words kept after tokenization based on their word frequency.Once the tokenizer is fitted on the data, we can use it to convert text strings to sequences of numbers. These numbers represent the position of each word in the dictionary (think of it as mapping).· In this project, we tried to tackle the problem by using recurrent neural network and attention based LSTM encoder.· By using LSTM encoder, we intent to encode all the information of text in the last output of Recurrent Neural Network before running feed forward network for classification.· This is very similar to neural translation machine and sequence to sequence learning.· We used LSTM layer in Keras to address the issue of long term dependencies.Our model scoring and selection is based on the standard evaluation metrics Accuracy, Precision, and F1 score, which are defined as follows:where:· TP represents the number of true positive classifications. That is, the records with the actual label A that have been correctly classified, or „predicted”, as label A.· TN is the number of true negative classifications. That is, the records with an actual label not equal to A that have been correctly classified as not belonging to label A.· FP is the number of false positive classifications, i.e., records with an actual label other than A that have been incorrectly classified as belonging to category A.· FN is the number of false negatives, i.e., records with a label equal to A that have been incorrectly classified as not belonging to category A.Figure 3. “Confusion matrix” of predicted vs. actual categorizationsThe combination of predicted and actual classifications is known as „confusion matrix”, illustrated in Figure 3. Against the background of these definitions, the various evaluation metrics provide the following insights:· Accuracy: The proportion of the total number of model predictions that were correct.· Precision (also called positive predictive value): The proportion of correct predictions relative to all predictions for that specific class.· Recall (also called true positive rate, TPR, hit rate or sensitivity): The proportion of true positives relative to all the actual positives.· False positive rate (also called false alarm rate): The proportion of false positives relative to all the actual negatives (FPR).· F1: (The more robust) Harmonic mean of Precision and Recall.The general RNN model without LSTM provided us the accuracy of 66% whereas we were able to increase it to 69% using LSTM network layer in RNNThe low volume of data resulted in accuracy being almost stagnant after 70% and it didn’t matter whether we increased the epochs as was evident in the plot.Yet 69% was fair enough classification as we intend to train it online and continuously improve the accuracy as volume of data grows to higher degree.For the scope of this project, we planning to deploy the model on Amazon AWS and integrate it with Service Now so that the model do the online or real-time predictions. To perform this we will first export the model by dumping it into an pickle file. Also, we will write a function which will connect to the S3 bucket and fetch and read the pickle file from there and recreate the model.So the workflow looks like this:AWS Lambda is a compute service that let you run the code without any need of provisioning or managing the servers. It takes care of that part itself. The best part about AWS Lambda is that you pay only for the compute time you consume — there is no charge when your code is not running. With AWS Lambda, you can run code for virtually any type of application or backend service — all with zero administration. AWS Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging. All you need to do is supply your code in one of the languages that AWS Lambda supports.Though we were not able to configure the AWS lambda as none of us was familiar with it and have to go through the documentation and fell short of time. We are planning to extend the project for our other course and complete it. We will update the blog once we achieve our goal.Web services make it possible for applications to connect to other software applications over a network allowing an exchange of information between the provider (server) and client (consumer).A web service consumer (client) requests information from a web service provider (server). A web service provider processes the request and returns a status code and a response body. When the response body is returned, the web service consumer extracts information from the response body and acts on the extracted data.ServiceNow can consume web services from third party providers or from another ServiceNow instance.In our case, we used the endpoint url triggered by the API Gateway using Rest web services and accessed it using javascript that runs on creation of ticket.On creation of a ticket, the javascript is triggered which sends the incident description to our model placed in AWS which performs the machine learning operations and returns back the predicted categories and probability.This project is not yet completed as we were short of time. We are working on this project and we plan to take this project ahead so that we can deploy our code on AWS and integrate it with Service Now. We will update the blog as soon as we make any progress. :)Github link:-https://github.com/pankajkishore/Cognitive-ProjectResources:-https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5fhttps://medium.com/datadriveninvestor/automation-all-the-way-machine-learning-for-it-service-management-9de99882a33github.comBy: Pankaj Kishore, Anuja Srivastava, Jitender Phogat and Karthik Kandakumar",16/03/2019,0,31.0,0.0,880.0,386.0,39.0,2.0,0.0,17.0,en
3769,Text Classification with BERT in PyTorch,Towards Data Science,Ruben Winastwan,412.0,9.0,1460.0,"Back in 2018, Google developed a powerful Transformer-based machine learning model for NLP applications that outperforms previous language models in different benchmark datasets. And this model is called BERT.In this post, we’re going to use a pre-trained BERT model from Hugging Face for a text classification task. As you might already know, the main goal of the model in a text classification task is to categorize a text into one of the predefined labels or tags.Specifically, soon we’re going to use the pre-trained BERT model to classify whether the text of a news article can be categorized as sport, politics, business, entertainment, or tech category.But before we dive into the implementation, let’s talk about the concept behind BERT briefly.BERT is an acronym for Bidirectional Encoder Representations from Transformers. The name itself gives us several clues to what BERT is all about.BERT architecture consists of several Transformer encoders stacked together. Each Transformer encoder encapsulates two sub-layers: a self-attention layer and a feed-forward layer.There are two different BERT models:There are at least two reasons why BERT is a powerful language model:BERT model expects a sequence of tokens (words) as an input. In each sequence of tokens, there are two special tokens that BERT would expect as an input:To make it more clear, let’s say we have a text consisting of the following short sentence:As a first step, we need to transform this sentence into a sequence of tokens (words) and this process is called tokenization.Although we have tokenized our input sentence, we need to do one more step. We need to reformat that sequence of tokens by adding[CLS] and [SEP] tokens before using it as an input to our BERT model.Luckily, we only need one line of code to transform our input sentence into a sequence of tokens that BERT expects as we have seen above. We will use BertTokenizer to do this and you can see how we do this later on.It is also important to note that the maximum size of tokens that can be fed into BERT model is 512. If the tokens in a sequence are less than 512, we can use padding to fill the unused token slots with [PAD] token. If the tokens in a sequence are longer than 512, then we need to do a truncation.And that’s all that BERT expects as input.BERT model then will output an embedding vector of size 768 in each of the tokens. We can use these vectors as an input for different kinds of NLP applications, whether it is text classification, next sentence prediction, Named-Entity-Recognition (NER), or question-answering.For a text classification task, we focus our attention on the embedding vector output from the special [CLS] token. This means that we’re going to use the embedding vector of size 768 from [CLS] token as an input for our classifier, which then will output a vector of size the number of classes in our classification task.Below is the illustration of the input and output of the BERT model.Now we’re going to jump into our main topic to classify text with BERT. In this post, we’re going to use the BBC News Classification dataset. If you want to follow along, you can download the dataset on Kaggle.This dataset is already in CSV format and it has 2126 different texts, each labeled under one of 5 categories: entertainment, sport, tech, business, or politics.Let’s take a look at what the dataset looks like.As you can see, the dataframe only has two columns, which is category that will be our label, and text which will be our input data for BERT.As you might already know from the previous section, we need to transform our text into the format that BERT expects by adding [CLS] and [SEP] tokens. We can do this easily with BertTokenizer class from Hugging Face.First, we need to install Transformers library via pip:To make it easier for us to understand the output that we get from BertTokenizer, let’s use a short text as an example.Here is the explanation of BertTokenizer parameters above:The outputs that you see from bert_input variable above are necessary for our BERT model later on. But what do those outputs mean?As you can see, the BertTokenizer takes care of all of the necessary transformations of the input text such that it’s ready to be used as an input for our BERT model. It adds [CLS], [SEP], and [PAD] tokens automatically. Since we specified the maximum length to be 10, then there are only two [PAD] tokens at the end.2. The second row is token_type_ids , which is a binary mask that identifies in which sequence a token belongs. If we only have a single sequence, then all of the token type ids will be 0. For a text classification task, token_type_ids is an optional input for our BERT model.3. The third row is attention_mask , which is a binary mask that identifies whether a token is a real word or just padding. If the token contains [CLS], [SEP], or any real word, then the mask would be 1. Meanwhile, if the token is just padding or [PAD], then the mask would be 0.As you might notice, we use a pre-trained BertTokenizer from bert-base-cased model. This pre-trained tokenizer works well if the text in your dataset is in English.If you have datasets from different languages, you might want to use bert-base-multilingual-cased. Specifically, if your dataset is in German, Dutch, Chinese, Japanese, or Finnish, you might want to use a tokenizer pre-trained specifically in these languages. You can check the name of the corresponding pre-trained tokenizer here.To sum up, below is the illustration of what BertTokenizer does to our input sentence.Now that we know what kind of output that we will get from BertTokenizer , let’s build a Dataset class for our news dataset that will serve as a class to generate our news data.In the above implementation, we define a variable called labels , which is a dictionary that maps the category in the dataframe into the id representation of our label. Notice that we also call BertTokenizer in the __init__ function above to transform our input texts into the format that BERT expects.After defining dataset class, let’s split our dataframe into training, validation, and test set with the proportion of 80:10:10.So far, we have built a dataset class to generate our data. Now let’s build the actual model using a pre-trained BERT base model which has 12 layers of Transformer encoder.If your dataset is not in English, it would be best if you use bert-base-multilingual-cased model. If your data is in German, Dutch, Chinese, Japanese, or Finnish, you can use the model pre-trained specifically in these languages. You can check the name of the corresponding pre-trained model here.As you can see from the code above, BERT model outputs two variables:We then pass the pooled_output variable into a linear layer with ReLU activation function. At the end of the linear layer, we have a vector of size 5, each corresponds to a category of our labels (sport, business, politics, entertainment, and tech).Now it’s time for us to train the model. The training loop will be a standard PyTorch training loop.We train the model for 5 epochs and we use Adam as the optimizer, while the learning rate is set to 1e-6. We also need to use categorical cross entropy as our loss function since we’re dealing with multi-class classification.It is recommended that you use GPU to train the model since BERT base model contains 110 million parameters.After 5 epochs with the above configuration, you’ll get the following output as an example:Obviously you might not get similar loss and accuracy values as the screenshot above due to the randomness of training process. If you haven’t got a good result after 5 epochs, try to increase the epochs to, let’s say, 10 or adjust the learning rate.Now that we have trained the model, we can use the test data to evaluate the model’s performance on unseen data. Below is the function to evaluate the performance of the model on the test set.After running the code above, I got the accuracy of 0.994 from the test data. The accuracy that you’ll get will obviously slightly differ from mine due to the randomness during the training process.Now you know the step on how we can leverage a pre-trained BERT model from Hugging Face for a text classification task. I hope this post helps you to get started with BERT.One thing to remember is that we can use the embedding vectors from BERT to do not only a sentence or text classification task, but also the more advanced NLP applications such as question answering, next sentence prediction, or Named-Entity-Recognition (NER) tasks.You can find all of the code snippets demonstrated in this post in this notebook.",10/11/2021,1,24.0,20.0,1028.0,655.0,9.0,6.0,0.0,6.0,en
3770,Anatomy of an Elasticsearch Cluster: Part I,Insight,Ronak Nathani,478.0,9.0,1455.0,"Want to learn Elasticsearch and other big data tools from top data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training program where you can build cutting edge big data platforms and transition to a career in data engineering at top teams like Facebook, Uber, Slack and Squarespace.Learn more about the program and apply today.This post is part of a series covering the underlying architecture and prototyping examples with a popular distributed search engine, Elasticsearch. In this post, we’ll be discussing the underlying storage model and how CRUD (create, read, update and delete) operations work in Elasticsearch.Elasticsearch is a very popular distributed search engine used at many companies like GitHub, SalesforceIQ, Netflix, etc. for full text search and analytical applications. At Insight Data Engineering Fellows Program, Fellows have used Elasticsearch for its various different features like:Full-text searchAggregationsGeospatial APISince, Elasticsearch is so popular in the industry and among our Fellows, I decided to take a closer look. In this post, I wanted to share what I learned about its storage model and how the CRUD operations work.Now, when I think of how a distributed system works, I think of the picture below:The part above the surface is the API and the part below is the actual engine where all the magic happens. In this post, we will be focusing on the part below the surface. Mainly, we will be looking at:Before we dive deep into these concepts, let’s get familiar with some terminology.An Elasticsearch index is a logical namespace to organize your data (like a database). An Elasticsearch index has one or more shards (default is 5). A shard is a Lucene index which actually stores the data and is a search engine in itself. Each shard can have zero or more replicas (default is 1). An Elasticsearch index also has “types” (like tables in a database) which allow you to logically partition your data in an index. All documents in a given “type” in an Elasticsearch index have the same properties (like schema for a table).Elasticsearch Index ~ Database Types ~ Tables Mapping ~ SchemaNOTE: The analogies above are for equivalence purposes only and not for equality. I would recommend reading this blog to help decide when to choose an index or a type to store data.Now, that we are familiar with the terms in Elasticsearch world, let’s see the different kinds of roles nodes can have.An instance of Elasticsearch is a node and a group of nodes form a cluster. Nodes in an Elasticsearch cluster can be configured in three different ways:Master NodeData NodeClient Node:The node in the Elasticsearch cluster that you connect with as a client is called the coordinating node. The coordinating node routes the client requests to the appropriate shard in the cluster. For read requests, the coordinating node selects a different shard every time to serve the request in order to balance the load.Before we start reviewing how a CRUD request sent to the coordinting node propagates through the cluster and is executed by the engine, let’s see how Elasticsearch stores data internally to serve results for full-text search at low latency.Elasticsearch uses Apache Lucene, a full-text search library written in Java and developed by Doug Cutting (creator of Apache Hadoop), internally which uses a data structure called an inverted index designed to serve low latency search results. A document is the unit of data in Elasticsearch and an inverted index is created by tokenizing the terms in the document, creating a sorted list of all unique terms and associating a list of documents with where the word can be found.It is very similar to an index at the back of a book which contains all the unique words in the book and a list of pages where we can find that word. When we say a document is indexed, we refer to the inverted index. Let’s see how inverted index looks like for the following two documents:Doc 1: Insight Data Engineering Fellows Program Doc 2: Insight Data Science Fellows ProgramIf we want to find documents which contain the term “insight”, we can scan the inverted index (where words are sorted), find the word “insight” and return the document IDs which contain this word, which in this case would be Doc 1 and Doc 2.To improve searchability (e.g., serving same results for both lowercase and uppercase words), the documents are first analyzed and then indexed. Analyzing consists of two parts:By default, Elasticsearch uses Standard Analyzer, which usesThere are many other analyzers available and you can read about them in the docs.In order to serve relevant search results, every query made on the documents is also analyzed using the same analyzer used for indexing.NOTE: The standard analyzer also uses stop token filter but it is disabled by default.As the concept of inverted index is clear now, let’s review CRUD operations. We’ll begin with writes.(C)reateWhen you send a request to the coordinating node to index a new document, the following set of operations take place:shard = hash(document_id) % (num_of_primary_shards)The figure below shows how the write request and data flows.(U)pdate and (D)eleteDelete and Update operations are also write operations. However, documents in Elasticsearch are immutable and hence, cannot be deleted or modified to represent any changes. Then, how can a document be deleted/updated?Every segment on disk has a .del file associated with it. When a delete request is sent, the document is not really deleted, but marked as deleted in the .del file. This document may still match a search query but is filtered out of the results. When segments are merged (we’ll cover segment merging in a follow up post), the documents marked as deleted in the .del file are not included in the new merged segment.Now, let’s see how updates work. When a new document is created, Elasticsearch assigns a version number to that document. Every change to the document results in a new version number. When an update is performed, the old version is marked as deleted in the .del file and the new version is indexed in a new segment. The older version may still match a search query, however, it is filtered out from the results.After the documents are indexed/updated, we would like to perform search requests. Let’s see how search requests are executed in Elasticsearch.Read operations consist of two parts:Let’s see how each phase works.Query PhaseIn this phase, the coordinating node routes the search request to all the shards (primary or replica) in the index. The shards perform search independently and create a priority queue of the results sorted by relevance score (we’ll cover relevance score later in the post). All the shards return the document IDs of the matched documents and relevant scores to the coordinating node. The coordinating node creates a new priority queue and sorts the results globally. There can be a lot of documents which match the results, however, by default, each shard sends the top 10 results to the coordinating node and the coordinating creates a priority queue sorting results from all the shards and returns the top 10 hits.Fetch PhaseAfter the coordinating node sorts all the results to generate a globally sorted list of documents, it then requests the original documents from all the shards. All the shards enrich the documents and return them to the coordinating node.The figure below shows how the read request and the data flows.As mentioned above, the search results are sorted by relevance. Let’s review how relevance is defined.The relevance is determined by a score that Elasticsearch gives to each document returned in the search result. The default algorithm used for scoring is tf/idf (term frequency/inverse document frequency). The term frequency measures how many times a term appears in a document (higher frequency == higher relevance) and inverse document frequency measures how often the term appears in the entire index as a percentage of the total number of documents in the index (higher frequency == less relevance). The final score is a combination of the tf-idf score with other factors like term proximity (for phrase queries), term similarity (for fuzzy queries), etc.These CRUD operations are supported by some internal data structures and techniques which are very important to understand how Elasticsearch works. In a follow up post, I will be going through some of these concepts and some of the gotchas while using Elasticsearch.Interested in transitioning to a career in data engineering? Find out more about the Insight Data Engineering Fellows Program in New York and Silicon Valley, apply today, or sign up for program updates.Already a data scientist or engineer?Find out more about our Advanced Workshops for Data Professionals. Register for two-day workshops in Apache Spark and Data Visualization, or sign up for workshop updates.",30/06/2016,0,47.0,30.0,965.0,478.0,5.0,13.0,0.0,17.0,en
3771,The 5 Clustering Algorithms Data Scientists Need to Know,Towards Data Science,George Seif,20000.0,11.0,1319.0,"Want to be inspired? Come join my Super Quotes newsletter. 😎Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.In Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Today, we’re going to look at 5 popular clustering algorithms that data scientists need to know and their pros and cons!K-Means is probably the most well-known clustering algorithm. It’s taught in a lot of introductory data science and machine learning classes. It’s easy to understand and implement in code! Check out the graphic below for an illustration.K-Means has the advantage that it’s pretty fast, as all we’re really doing is computing the distances between points and group centers; very few computations! It thus has a linear complexity O(n).On the other hand, K-Means has a couple of disadvantages. Firstly, you have to select how many groups/classes there are. This isn’t always trivial and ideally with a clustering algorithm we’d want it to figure those out for us because the point of it is to gain some insight from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration.An illustration of the entire process from end-to-end with all of the sliding windows is shown below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.In contrast to K-means clustering, there is no need to select the number of clusters as mean-shift automatically discovers this. That’s a massive advantage. The fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data-driven sense. The drawback is that the selection of the window size/radius “r” can be non-trivial.DBSCAN is a density-based clustered algorithm similar to mean-shift, but with a couple of notable advantages. Check out another fancy graphic below and let’s get started!DBSCAN poses some great advantages over other clustering algorithms. Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises, unlike mean-shift which simply throws them into a cluster even if the data point is very different. Additionally, it can find arbitrarily sized and arbitrarily shaped clusters quite well.The main drawback of DBSCAN is that it doesn’t perform as well as others when the clusters are of varying density. This is because the setting of the distance threshold ε and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold ε becomes challenging to estimate.One of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. We can see why this isn’t the best way of doing things by looking at the image below. On the left-hand side, it looks quite obvious to the human eye that there are two circular clusters with different radius’ centered at the same mean. K-Means can’t handle this because the mean values of the clusters are very close together. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center.Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! Taking an example in two dimensions, this means that the clusters can take any kind of elliptical shape (since we have a standard deviation in both the x and y directions). Thus, each Gaussian distribution is assigned to a single cluster.To find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation), we will use an optimization algorithm called Expectation–Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. Then we can proceed with the process of Expectation–Maximization clustering using GMMs.There are 2 key advantages to using GMMs. Firstly GMMs are a lot more flexible in terms of cluster covariance than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster’s covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support mixed membership.Hierarchical clustering algorithms fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm stepsHierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can’t do this. These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of O(n³), unlike the linear complexity of K-Means and GMM.There are your top 5 clustering algorithms that a data scientist should know! We’ll end off with an awesome visualization of how well these algorithms and a few others perform, courtesy of Scikit Learn! Very cool to see how the different algorithms compare and contrast with different data!",05/02/2018,0,5.0,7.0,593.0,413.0,8.0,5.0,0.0,1.0,en
3772,IoT Learning Algorithms and Predictive Maintenance — Part III: Few-shot Learning,IoT & Data Science,Record Evolution,104.0,18.0,2474.0,"The article tackles smart data processing of the Internet of Things (IoT) in a predictive maintenance context and relates this to recent developments in semi-supervised learning. While written with an eye towards a non-expert audience, the article references recent scientific publications. We leave it to the curious and technically oriented reader to expand their knowledge on the ideas we have sketched out (see References). We aim to be informative and open minds to stimulating discussions on IoT and data analytics.We cover the topic of IoT Learning Algorithms and Predictive Maintenance in a series of three articles. In PART I, we present a simple case study in detail and discuss some learning algorithms related to it. In PART II, we focus on IoT data analytics and applications to IoT design for predictive maintenance. In PART III (the current article), we review recent literature on semi-supervised learning and compare the foundations of different methods. We introduce real-life cases in which few-shot learning may provide an efficient technique for smart IoT analytics and data streaming. At the end of this article, you will find a glossary of terms and suggestions for further reading.Before diving into techniques to deal with rare events and anomalies, let’s define them. A rare event is an event in time with marginal statistics, that is, an event occurring too few times with respect to the norm (the norm being the average time scale of the background). A good example is an earthquake. An anomaly is a rare event that has different features than the events in the norm distribution, meaning that an anomaly is generated by a process different from the processes attributable to the norm. An anomaly is often seen as the analog of an outlier, defined by Hawkins as follows: “An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism”. In other words, an outlier is a data point that has many different properties compared to the natural state of the dataset.Rare events often refer to catastrophes or breakdowns. So the cost of a rare event often is huge. On the other hand, it is tricky to predict a rare event. Straightforward methods fail in cases of extreme imbalance and …..the training data will often have imbalanced data (the number of normal data will be much larger than the number of abnormal data). We need to find a way to deal with imbalanced data. One easy solution is sampling (sampling up the number of abnormal data and sampling down the number of normal data). Another solution to be taken into consideration is applying weights to labels.In Part I, we presented a hypothetical hierarchical solution to the anomaly detection problem, in which case one also runs into imbalance issues.Anomaly detection is a vast topic. The following chart demonstrates a plethora of models:Here we consider a case where we have large amounts of training data with rare anomalies hidden in them. We are trying to approach a new case with supervised learning. Later, we will also include updating the set of labels on the fly using semi-supervised techniques.We touched upon the topic of anomalies and dealt with a concrete example in Part I of this article series. Anomalies can be detected by hand via a simple statistical criterion (as they can be conceptualized as outliers). On the other hand, the goal is to automate the detection process using evolving machine intelligence and the learning structure of certain anomalies.In Part II of this article series, we talked about network security. There we presented security protocols and mechanisms. Here we want to enter the discussion of smart security in an industrial context. A comprehensive list of anomaly detection algorithms is provided in the following table [12]:We want to indicate that in the referenced publication [12], the authors have presented a complete review of classical anomaly detection results. Here we only cover algorithms related to network security. Below we introduce details from Table 1:Below we sketch out some classical methods of anomaly detection and their properties:As mentioned above, one important aspect of anomalies is that they are rare compared to mainstream behavior. Thus, we need to come up with strategies to overcome the data imbalance in samples. In this sense, one-shot learning is a powerful semi-supervised learning method that can be adapted to IoT analytics. The main idea is that we have a central network model that copies itself to given devices and identifies a new category. The central model will be updated by singular examples detected by the edge device and will be reused in newly installed IoT devices. In summary, we have four steps:This workflow can be optimized by increasing the number of training examples (meaning with given experience) for available resources at a given moment. In general, inheritance or transfer of knowledge is important for such smart IoT settings to work efficiently. Unfiltered data reported continuously wouldn’t just consume storage resources and consume redundant energy but will also create noise in the model where one needs the data to train a new model. The message is that Big Data collected randomly doesn’t necessarily lead to knowledge. Thus, we need smart local IoT analytics. Specifically, this can be accomplished by few-shot learning because the decision to report or act must only sometimes involve very very few examples.Here we suggest a selection of models that implement semi-supervised learning. The few-shot problem has been studied from multiple perspectives, including the similarity-matching [17], optimization [3], metric learning [14, 16,18], and hierarchical graphical models [15].A survey by Tassilo Klein classifies the Semi-supervised Deep Learning algorithms. Below we present this classification and the corresponding references:A. Data Level Approach:B. Parameter Level Approach:C. Combining both:The need for similarity of data as a way of evaluation is ubiquitous to machine learning. However, this is no longer feasible in complex cases that involve hand-crafted metrics. This is why one can train a metric with given parameters to represent this proximity. Such a metric is crucial for implementing clustering algorithms such as k-nearest neighbors or k-means [18]. General-purpose metrics such as Euclidean metrics can be applied to structural numerical data. Nevertheless, a good metric that captures the peculiarities of a given dataset is essential as generic metrics usually fail in this task.Apart from the examples above, there are neural network-independent methods such as the Metric or Kernel Learning using prototypical networks [6], metric learning [16].With GANs, you can create additional samples from existing data that resemble the given samples but are not the same. Using this method, you can generate more anomaly data starting from a small set of anomalies.arxiv.orghttps://arxiv.org/abs/1810.01392Why do we need meta-learning a.k.a learning of learning? There seem to be two main reasons why gradient-based optimization fails in the face of a few labeled examples:Meta-learning suggests framing the learning problem at two levels. The first is the quick acquisition of knowledge within each separate task presented. This process is guided by the second mechanism, which involves a slower extraction of information learned across all tasks. [9]Meta-learning has been presented in simple charts in [19]. The reader can refer to relevant publications to study the technicalities of the two optimization mechanisms in meta-learning applied to neural networks.First of all, conventional optimization with gradient descent and variants is simply minimizing the distance between predicted and true labels while satisfying a regularization scheme (to reduce the number of parameters). The parameters of the gradient descent, i.e. learning rate, are given and fixed for the whole process.Learning: Minimize the distance between predicted and grand truth by using a loss-function metric.The idea of meta-learning is optimizing learning parameters as well as the network parameters via a second process. For example, the learning rate of the loss optimizer can be adapted in such a way that its learning slows down following the part of the network we are looking at. This can be achieved by additional dynamical variables (for details, please see [19] and the references therein).Meta-learning: Training the hyperparameters or the optimizer. Optimize the optimizer!Additional processes to follow are:Despite the whole buzz about Deep Learning with its data-hungry training epochs and yet non-intuitive implicit mechanisms, one can simply ask if there is an easier way of doing semi-supervised learning. The answer is: yes and no! At the conceptual level, SVMs are really powerful tools to implement semi-supervised learning. In practice, however, they may lead to NP-hard problems.“While deep neural networks have shown outstanding results in a wide range of applications, learning from a very limited number of examples is still a challenging task. Despite the difficulties of few-shot learning, metric-learning techniques showed the potential of the neural networks for this task. While these methods perform well, they don’t provide satisfactory results. In this work, the idea of metric-learning is extended with Support Vector Machines (SVM) working mechanism, which is well known for generalization capabilities on a small dataset.” [27]In this section, we look at a few methods that have been introduced over the past years:arxiv.org2. Methods from recent articles:arxiv.org3. Out-of-distribution Examples: “Several previous works seek to address these problems by giving deep neural network classifiers a means of assigning anomaly scores to inputs. These scores can then be used for detecting out-of-distribution (OOD) examples (Hendrycks & Gimpel, 2017; Lee et al., 2018; Liu et al., 2018)”4. Outlier Exposure: “ We propose leveraging diverse, realistic datasets for this purpose, with a method we call Outlier Exposure (OE). OE provides a simple and effective way to consistently existing methods for OOD detection… Utilizing Auxiliary Datasets. Outlier Exposure uses an auxiliary dataset entirely disjoint from test-time data to teach the network better representations for anomaly detection.”5. https://arxiv.org/abs/1805.09411…..Having discussed the mechanism of one cycle of learning and deployment, we need to start thinking about building a flexible system to implement such cycles in a continuum in the long term.Batch or offline learning algorithms take batches of training data to train a model. Batch learning is an approach whereby all the data is ingested at once to build a model. Online learning algorithms take an initial guess model and then pick up observations from the training population to recalibrate the weights on each input parameter. Below are several trade-offs in using the two algorithms:In cases where we deal with huge data, we are left with no choice but to use online learning algorithms. The only other option is to do a batch learning on a smaller sample.In this article series, we have outlined some challenges in dealing with a smart IoT environment. Our goal is to make IoT devices collaborative and to keep on improving their pseudo-perception of the world. To do so, however, we need an infrastructure for device and data management.First, we need to build a swarm environment for IoT devices with a flexible setup, data, and meta-data collection mechanisms. The Record Evolution platform provides a frontend where one can manage IoT devices and deploy apps in them. Another crucial component is the data infrastructure needed to create pipes of data inflow and transmission to a learning mechanism. The platform’s data science studio is a transparent and user-friendly cloud data platform handling data acquisition, data transformation, and data visualization. The next step is using cloud resources and flexible semi-supervised algorithms to transfer the collected data into the intelligence. On top of this, we need to re-deploy and update according to the needs of the local IoT environment. The platform’s IoT development studio can handle this task.In our analysis, semi-supervised anomaly detection is the way to go due to the structural properties of real IoT data sets (the imbalance between known and unknown). Unsupervised learning is always a solution when labeled data isn’t available. However, one should also exploit transfer learning [12, 13, 20] as a healthy state as it is rather stereotypical and is modeled by an abundance of data, meaning that we will have a plethora of labeled states. This can simply help detect less common states and characterize them. Later on, such new states can be learned, and hence the boundaries of the known anomaly universe can be extended. We believe few-shot learning is the method that captures the specifics of the problem at hand.In this three-piece article series, we covered the topic of an end-to-end IoT service and infrastructure. Our main focus was on how to implement smart IoT in a world of dynamic heterogeneous data. Overall, these 45 minutes of reading aim to reach both non-experts and a technically oriented readership.I. IoT is crucial for predictive maintenance. To accomplish this, we need to refer to edge computing and smart algorithms.II. IoT requires data and device management infrastructure. Deployment and update cycles are part of the intelligence inheritance process.III. Inductive semi-supervised learning is a good candidate for a smart IoT algorithm. While there are various options when choosing the exact learning algorithm, we mostly presented Deep Neural Networks and Support Vector Machines.1. Article: Generative Adversarial Residual Pairwise Networks for One-shot Learning2. Article: Efficient K-shot Learning with Regularized Deep Networks3. Article: Optimization as a Model for Few-shot Learning4. Article: Low-shot Visual Recognition by Shrinking and Hallucinating Features5. Article: Model-Agnostic Meta-learning for Fast Adaptation of Deep Networks6. Article: Prototypical Networks for Few-shot Learning7. Article: Meta-SGD: Learning to Learn Quickly for Few-shot Learning8. Article: Deep Learning for IoT Big Data and Streaming Analytics: A Survey9. Article: Meta-learning a Dynamical Language Model10. Article: Low-shot Learning with Large-scale Diffusion11. Article: Meta-learning for Semi-supervised Few-shot Classification12. Article: Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications13. Article: Machine Learning for the Internet of Things Data Analysis: A Survey14. Article: Matching Networks for One-shot Learning15. Article: One-shot Learning with a Hierarchical Nonparametric Bayesian Model16. Article: Metric Learning With Adaptive Density Discrimination17. Article: Siamese Neural Networks for One-shot Image Recognition18. Article: A Survey on Metric Learning for Feature Vectors and Structured Data19. Blog by Thomas Wolf: Meta-learning20. Blog by Tassilo Klein: Deep Few-shot Learning21. Blog by Abhinav Khushraj: IoT-based Predictive Maintenance22. Blog: The Value That IoT Brings23. Book by Bishop: Pattern Recognition and Machine Learning24. Article: A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data25. Article: Imitation Networks: Few-shot Learning of Neural Networks from Scratch26. Article: Learning to Remeber Rare Events27. Article: Make SVM Great Again With Siamese Kernel For Few-shot Learning (authors undisclosed)28. Presentation: WAMP (Web Application Messaging Protocol)29. Wiki: Anomalies in Statistics. The Definition Given by Wikipedia30. Article: The Internet Of Things: New Interoperability, Management, and Security Challenges31. Book: Foundations of Time-Frequency Analysis32. Wiki: Intro to PID Controllers33. Web: https://mqtt.org/faq34. Web: http://customerthink.com/top-5-surprising-facts-everyone-should-read-about-iot/35. Blog: https://blog.timescale.com/why-sql-beating-nosql-what-this-means-for-future-of-data-time-series-database-348b777b847a?gi=85a48c95088736. Blog: Record Evolution PlatformThis article is part of a three-piece article series. See Part I and Part II here:IoT Learning and Predictive Maintenance-IIoT Learning and Predictive Maintenance-IIThis glossary only covers the concepts presented in our argument. This is not an exhaustive account of machine learning in an IoT setting. You can refer to reviews [13],[12], and [8] for a complete picture.",15/02/2019,0,184.0,23.0,1331.0,634.0,13.0,16.0,0.0,100.0,en
3773,"Machine Learning week 1: Cost Function, Gradient Descent and Univariate Linear Regression",Medium,Lachlan Miller,2000.0,5.0,751.0,"I have started doing Andrew Ng’s popular machine learning course on Coursera. The first week covers a lot, at least for someone who hasn’t touched much calculus for a few yearsThese three topics were a lot to take in. I’ll talk about each in detail, and how they all fit together, with some python code to demonstrate.Edit May 4th: I published a follow up focusing on how the Cost Function works here, including an intuition, how to calculate it by hand and two different Python implementations. I can do gradient descent and then bring them together for linear regression soon.First, the goal of most machine learning algorithms is to construct a model: a hypothesis that can be used to estimate Y based on X. The hypothesis, or model, maps inputs to outputs. So, for example, say I train a model based on a bunch of housing data that includes the size of the house and the sale price. By training a model, I can give you an estimate on how much you can sell your house for based on it’s size. This is an example of a regression problem — given some input, we want to predict a continuous output.The hypothesis is usually presented asThe theta values are the parameters.Some quick examples of how we visualize the hypothesis:This yields h(x) = 1.5 + 0x. 0x means no slope, and y will always be the constant 1.5. This looks like:How aboutThe goal of creating a model is to choose parameters, or theta values, so that h(x) is close to y for the training data, x and y. So for this dataI will try and find a line of best fit using linear regression. Let’s get started.We need a function that will minimize the parameters over our dataset. One common function that is often used is mean squared error, which measure the difference between the estimator (the dataset) and the estimated value (the prediction). It looks like this:It turns out we can adjust the equation a little to make the calculation down the track a little more simple. We end up with:Let’s apply this const function to the follow data:For now we will calculate some theta values, and plot the cost function by hand. Since this function passes through (0, 0), we are only looking at a single value of theta. From here on out, I’ll refer to the cost function as J(ϴ).For J(1), we get 0. No surprise — a value of J(1) yields a straight line that fits the data perfectly. How about J(0.5)?The MSE function gives us a value of 0.58. Let’s plot both our values so far:J(1) = 0J(0.5) = 0.58I’ll go ahead and calculate some more values of J(ϴ).And if we join the dots together nicely…We can see that the cost function is at a minimum when theta = 1. This makes sense — our initial data is a straight line with a slope of 1 (the orange line in the figure above).We minimized J(ϴ) by trial and error above — just trying lots of values and visually inspecting the resulting graph. There must be a better way? Queue gradient descent. Gradient Descent is a general function for minimizing a function, in this case the Mean Squared Error cost function.Gradient Descent basically just does what we were doing by hand — change the theta values, or parameters, bit by bit, until we hopefully arrived a minimum.We start by initializing theta0 and theta1 to any two values, say 0 for both, and go from there. Formally, the algorithm is as follows:where α, alpha, is the learning rate, or how quickly we want to move towards the minimum. If α is too large, however, we can overshoot.Quickly summarizing:We have a hypothesis:which we need fit to our training data. We can use a cost function such Mean Squared Error:which we can minimize using gradient descent:Which leads us to our first machine learning algorithm, linear regression. The last piece of the puzzle we need to solve to have a working linear regression model is the partial derivate of the the cost function:Which turns out to be:Which gives us linear regression!With the theory out of the way, I’ll go on to implement this logic in python in the next post.Edit May 4th: I published a follow up focusing on how the Cost Function works here, including an intuition, how to calculate it by hand and two different Python implementations. I can do gradient descent and then bring them together for linear regression soon.",10/01/2018,1,0.0,14.0,636.0,359.0,21.0,1.0,0.0,5.0,en
3774,DeepMind’s Latest A.I. Health Breakthrough Has Some Problems,OneZero,Julia Powles,1200.0,8.0,1808.0,"Google-affiliated artificial intelligence firm DeepMind has been pushing into the healthcare sector for some time. Last week the London-based company synchronized the release of a set of new research articles — one with the U.S. Department of Veterans Affairs, and three with a North London hospital trust known as the Royal Free.In one paper, published in the journal Nature, with co-authors from Veterans Affairs and University College London, DeepMind claimed its biggest healthcare breakthrough to date: that artificial intelligence (A.I.) can predict acute kidney injury (AKI) up to two days before it happens.AKI — which occurs when the kidneys suddenly stop functioning, leading to a dangerous buildup of toxins in the bloodstream — is alarmingly common among hospital patients in serious care, and contributes to hundreds of thousands of deaths in the United States each year. DeepMind’s bet is that if it can successfully predict which patients are likely to develop AKI well in advance, then doctors could stop or reverse its progression much more easily, saving lives along the way.Beyond the headlines and the hope in the DeepMind papers, however, are three sober facts.First, nothing has actually been predicted — and certainly not before it happens. Rather, what has happened is that DeepMind has taken a windfall dataset of historic incidents of kidney injury in American veterans, plus around 9,000 data-points for each person in the set, and has used a neural network to figure out a pattern between the two.Second, that predictive pattern only works some of the time. The accuracy rate is 55.8% overall, with a much lower rate the earlier the prediction is made, and the system generates two false positives for every accurate prediction.Third, and most strikingly of all: the study was conducted almost exclusively on men — or rather, a dataset of veterans that is 93.6% male. Given the A.I. field’s crisis around lack of diversity and amplification of bias and discrimination, that fact is very important — and astonishingly understated.A DeepMind spokesperson responded to this point by stating “the dataset is representative of the VA population and, as with all deep learning models, it would need further, representative data before being used more widely.” But this depoliticizes how DeepMind has cast the study and its results — not as a tool for potential use with American veterans, or even as a tool that provides indicative results for men, but as a groundbreaking innovation with a general application.Beyond these very significant deficiencies are a number of missed opportunities in DeepMind’s analysis. Some are foundational.Kidney injury affects 13.4% of the patients in the Veterans Affairs dataset–a rate two-thirds the 20% average the study presents for hospitalized U.S. patients in general. The difference is interesting in such a specific population as veterans, and suggests there may be something relevant in the VA’s patient characteristics as well as doctor’s choices and clinical practice. But the Nature study is presented with such an absence of context and explanation as to how VA clinicians actually detected and sought to prevent kidney injury, or the features of the population and its variability, that this basic and essential information is impossible to interrogate.Similarly, the researchers make no attempt to explain the A.I. model they used. How did it work? Why did they decide to construct the model in the way they did? Why was this a good conceptual fit for this particular dataset, and how effectively could it generalize to a wider population? How did the A.I. address the needs of specific patient types, and what impact would this algorithm have on them?A spokesperson said that DeepMind aimed to justify all of the decisions made in the VA study through supplemental information and a non-peer reviewed protocol paper. However, none of these questions were answered with precision and, as a result, the study offers few meaningful insights into either renal medicine or A.I. prediction. The study is littered with unexplained choices that may have been medically instructive, as well as omissions of details (such as 36 salient features discovered by the deep learning model, and what they mean) and outliers — distractions from a clean model, perhaps, but all representing real patients, at the end of the day.But even if all these missed opportunities and deficiencies were addressed, there’s a much larger narrative to DeepMind’s U.S. health research. And that’s where the remainder of the newly released papers come in.Veterans are far from DeepMind’s first attempt–and, quite plausibly, far from its first choice–in predicting kidney injury. The company has been trying to tackle avoidable patient harm since at least 2015, when it first struck a deal with the Royal Free London NHS Foundation Trust that gave it the fully identified health records of over 1.6 million patients.By 2016, and as a direct result of receiving such a treasure trove of private information, DeepMind was embroiled in a major data scandal over legality. In 2017, the U.K.’s data watchdog ruled that patient rights had been breached in several major respects for what turned out to be DeepMind’s gain. The whole saga caused significant reputational damage that has simmered ever since, and by late 2018, Google moved unilaterally to absorb and rehabilitate DeepMind’s healthcare arm into its own Google Health division–a move that remains incomplete because none of DeepMind’s healthcare partners have agreed to transfer their contracts fully to Google.DeepMind’s work with Veteran Affairs points to a crucial question. Clearly the dataset that DeepMind holds on patients in Britain is three times as large and significantly more diverse than the dataset on U.S. veterans. So why would DeepMind choose to do the research that led to the Nature paper with American veterans instead of Royal Free patients?DeepMind rationalizes the choice as simply one of working with different partners for different projects. But the more plausible answer could be that the legal and reputational risk was perceived to be greater in the U.K., particularly around reusing the controversial Royal Free patient data.Nevertheless, the Veteran Affairs data isn’t absent complication, as a point of comparison. Although individual patients were de-identified in the United States before being transferred to DeepMind, with 9,000 data-points per person, collected across a period of up to 15 years, it remains likely that at least one of the patients would be capable of being reidentified with expert methods. That’s all that’s required to bring processing within the scope of personal data, and therefore the European Union’s General Data Protection Regulation.Two other legacies remain from the initial data scandal between DeepMind and Royal Free. First, despite ongoing privacy concerns implicated by a decidedly dubious legal basis for processing, the full data trove has remained in DeepMind’s possession, with the blessing of the U.K. regulator. (DeepMind analogizes itself to a clinical data storage system, ready to serve up records on each and every Royal Free patient, even if those patients haven’t set foot in the hospital in nearly a decade and have no identifiable need for care.) This demonstrates, as does the gift from Veterans Affairs, that DeepMind is able to gain and maintain access to incredibly valuable datasets in a way not practically realizable and defensible by others. Second, both DeepMind and the Royal Free have been determined to make a good news story of it all, overlooking any inconvenient concerns.It was in this spirit that in early 2017, Royal Free starting pushing the clinical deployment of Streams, DeepMind’s clinical smartphone app built off the back of the data transfer and designed to interface with patient data and tests and to generate push alerts. Importantly, and ironically, Streams is not an A.I. tool. It is driven by standard tests and standard formulae. But DeepMind is an A.I. company, so Streams has always been destined to become an A.I. tool.In a dramatic evaluation in Nature Digital Medicine, the use of Streams is shown to have no detectable beneficial impact on patient outcomes, despite all the fanfare.The other papers coordinated to be released with the Veterans Affairs study therefore make a connection that has been years coming. Simultaneously, if rather shakily and with multiple deficiencies, some kind of A.I. model for predicting kidney injury–even if it is a model for American veteran males–is presented alongside a set of evaluations on Streams as a clinical tool.Those evaluation papers, however, give a decidedly lukewarm impression on Streams. In a dramatic evaluation in Nature Digital Medicine, the use of Streams is shown to have no statistically significant beneficial impact on patient’s clinical outcomes, despite all the fanfare.An associated user study in the Journal of Medical Internet Research (JMIR) — on 19 of the total set of 47 Royal Free clinicians who shared the six iPhones that carried Streams — in fact tells us that the app, for all its promises, creates more work, more anxiety, and probably would require hiring more people to monitor and respond to alerts, often unnecessarily.But finally, an evaluation paper about costs, also in the JMIR, claims that assuming no change in staffing (and therefore simply expecting clinicians to absorb the workload and anxiety identified in the user study), Streams could deliver a mean of less than $2,600 cost savings per patient. Delving into the supplementary file containing data to support the cost estimate, it seems that the control hospital that did not use Streams also saw significant reductions in the major cost contributors to that figure, at several statistically significant levels, in a way that deserves comparison and could challenge the central claim.But to DeepMind, perhaps these are extraneous details, given the kicker of the whole exercise. “We did not include the costs of providing the technology, and therefore, it is not possible to judge whether or not it would be cost saving overall,” states the paper, co-written by DeepMind co-founder Mustafa Suleyman. “Our results suggest that the digitally enabled care pathway would be cost saving, provided provision of the technology costs less than around £1,600 [$1,945] per patient spell.”The authors of the JMIR study about costs could not be reached for comment, but a DeepMind spokesperson emphasized that, although the Nature Digital Medicine study demonstrated no significant improvement in clinical outcomes through the use of Streams, the evaluations detailed improvements in the reliability and speed of AKI recognition, the time frames in which some key treatments and specialist care were delivered, as well as a claimed reduction in healthcare costs.Streams, then, seems typical of DeepMind’s way of working. It offers few overall gains in clinical outcomes, creates anxiety and additional workload for physicians, and was built on the back of deeply controversial access to patients’ data. Whatever Google and DeepMind are planning to do in the United States, they need to overhaul their attitude to the most basic priorities of rights, explanations, and costs to humans, not machines. Those come first, before profit — or rushing to proclaim that A.I. has a central place in the future of medicine.",06/08/2019,0,1.0,9.0,389.0,276.0,2.0,0.0,0.0,24.0,en
3775,Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation (with Python Code),Analytics Vidhya,Pulkit Sharma,762.0,13.0,2117.0,"I am fascinated by self-driving cars. The sheer complexity and mix of different computer vision techniques that go into building a self-driving car system is a dream for a data scientist like me.So, I set about trying to understand the computer vision technique behind how a self-driving car potentially detects objects. A simple object detection framework might not work because it simply detects an object and draws a fixed shape around it.That’s a risky proposition in a real-world scenario. Imagine if there’s a sharp turn in the road ahead and our system draws a rectangular box around the road. The car might not be able to understand whether to turn or go straight. That’s a potential disaster!Instead, we need a technique that can detect the exact shape of the road so our self-driving car system can safely navigate the sharp turns as well.The latest state-of-the-art framework that we can use to build such a system? That’s Mask R-CNN!So, in this article, we will first quickly look at what image segmentation is. Then we’ll look at the core of this article — the Mask R-CNN framework. Finally, we will dive into implementing our own Mask R-CNN model in Python. Let’s begin!We learned the concept of image segmentation in part 1 of this series in a lot of detail. We discussed what is image segmentation and its different techniques, like region-based segmentation, edge detection segmentation, and segmentation based on clustering.I would recommend checking out that article first if you need a quick refresher (or want to learn image segmentation from scratch).I’ll quickly recap that article here. Image segmentation creates a pixel-wise mask for each object in the image. This technique gives us a far more granular understanding of the object(s) in the image. The image shown below will help you to understand what image segmentation is:Here, you can see that each object (which are the cells in this particular image) has been segmented. This is how image segmentation works.We also discussed the two types of image segmentation: Semantic Segmentation and Instance Segmentation. Again, let’s take an example to understand both of these types:All 5 objects in the left image are people. Hence, semantic segmentation will classify all the people as a single instance. Now, the image on the right also has 5 objects (all of them are people). But here, different objects of the same class have been assigned as different instances. This is an example of instance segmentation.Part one covered different techniques and their implementation in Python to solve such image segmentation problems. In this article, we will be implementing a state-of-the-art image segmentation technique called Mask R-CNN to solve an instance segmentation problem.Mask R-CNN is basically an extension of Faster R-CNN. Faster R-CNN is widely used for object detection tasks. For a given image, it returns the class label and bounding box coordinates for each object in the image. So, let’s say you pass the following image:The Fast R-CNN model will return something like this:The Mask R-CNN framework is built on top of Faster R-CNN. So, for a given image, Mask R-CNN, in addition to the class label and bounding box coordinates for each object, will also return the object mask.Let’s first quickly understand how Faster R-CNN works. This will help us grasp the intuition behind Mask R-CNN as well.Once you understand how Faster R-CNN works, understanding Mask R-CNN will be very easy. So, let’s understand it step-by-step starting from the input to predicting the class label, bounding box, and object mask.Similar to the ConvNet that we use in Faster R-CNN to extract feature maps from the image, we use the ResNet 101 architecture to extract features from the images in Mask R-CNN. So, the first step is to take an image and extract features using the ResNet 101 architecture. These features act as an input for the next layer.Now, we take the feature maps obtained in the previous step and apply a region proposal network (RPM). This basically predicts if an object is present in that region (or not). In this step, we get those regions or feature maps which the model predicts contain some object.The regions obtained from the RPN might be of different shapes, right? Hence, we apply a pooling layer and convert all the regions to the same shape. Next, these regions are passed through a fully connected network so that the class label and bounding boxes are predicted.Till this point, the steps are almost similar to how Faster R-CNN works. Now comes the difference between the two frameworks. In addition to this, Mask R-CNN also generates the segmentation mask.For that, we first compute the region of interest so that the computation time can be reduced. For all the predicted regions, we compute the Intersection over Union (IoU) with the ground truth boxes. We can computer IoU like this:IoU = Area of the intersection / Area of the unionNow, only if the IoU is greater than or equal to 0.5, we consider that as a region of interest. Otherwise, we neglect that particular region. We do this for all the regions and then select only a set of regions for which the IoU is greater than 0.5.Let’s understand it using an example. Consider this image:Here, the red box is the ground truth box for this image. Now, let’s say we got 4 regions from the RPN as shown below:Here, the IoU of Box 1 and Box 2 is possibly less than 0.5, whereas the IoU of Box 3 and Box 4 is approximately greater than 0.5. Hence. we can say that Box 3 and Box 4 are the region of interest for this particular image whereas Box 1 and Box 2 will be neglected.Next, let’s see the final step of Mask R-CNN.Once we have the RoIs based on the IoU values, we can add a mask branch to the existing architecture. This returns the segmentation mask for each region that contains an object. It returns a mask of size 28 X 28 for each region which is then scaled up for inference.Again, let’s understand this visually. Consider the following image:The segmentation mask for this image would look something like this:Here, our model has segmented all the objects in the image. This is the final step in Mask R-CNN where we predict the masks for all the objects in the image.Keep in mind that the training time for Mask R-CNN is quite high. It took me somewhere around 1 to 2 days to train the Mask R-CNN on the famous COCO dataset. So, for the scope of this article, we will not be training our own Mask R-CNN model.We will instead use the pretrained weights of the Mask R-CNN model trained on the COCO dataset. Now, before we dive into the Python code, let’s look at the steps to use the Mask R-CNN model to perform instance segmentation.It’s time to perform some image segmentation tasks! We will be using the mask rcnn framework created by the Data scientists and researchers at Facebook AI Research (FAIR).Let’s have a look at the steps which we will follow to perform image segmentation using Mask R-CNN.First, we will clone the mask rcnn repository which has the architecture for Mask R-CNN. Use the following command to clone the repository:Once this is done, we need to install the dependencies required by Mask R-CNN.Here is a list of all the dependencies for Mask R-CNN:You must install all these dependencies before using the Mask R-CNN framework.Next, we need to download the pretrained weights. You can use this link to download the pre-trained weights. These weights are obtained from a model that was trained on the MS COCO dataset. Once you have downloaded the weights, paste this file in the samples folder of the Mask_RCNN repository that we cloned in step 1.Finally, we will use the Mask R-CNN architecture and the pretrained weights to generate predictions for our own images.Once you’re done with these four steps, it’s time to jump into your Jupyter Notebook! We will implement all these things in Python and then generate the masks along with the classes and bounding boxes for objects in our images.Sp, are you ready to dive into Python and code your own image segmentation model? Let’s begin!To execute all the code blocks which I will be covering in this section, create a new Python notebook inside the “samples” folder of the cloned Mask_RCNN repository.Let’s start by importing the required libraries:Next, we will define the path for the pretrained weights and the images on which we would like to perform segmentation:If you have not placed the weights in the samples folder, this will again download the weights. Now we will create an inference class which will be used to infer the Mask R-CNN model:What can you infer from the above summary? We can see the multiple specifications of the Mask R-CNN model that we will be using.So, the backbone is resnet101 as we have discussed earlier as well. The mask shape that will be returned by the model is 28X28, as it is trained on the COCO dataset. And we have a total of 81 classes (including the background).We can also see various other statistics as well, like:You should spend a few moments and understand these specifications. If you have any doubts regarding these specifications, feel free to ask me in the comments section below.Next, we will create our model and load the pretrained weights which we downloaded earlier. Make sure that the pretrained weights are in the same folder as that of the notebook otherwise you have to give the location of the weights file:Now, we will define the classes of the COCO dataset which will help us in the prediction phase:Let’s load an image and try to see how the model performs. You can use any of your images to test the model.This is the image we will work with. You can clearly identify that there are a couple of cars (one in the front and one in the back) along with a bicycle.It’s prediction time! We will use the Mask R-CNN model along with the pretrained weights and see how well it segments the objects in the image. We will first take the predictions from the model and then plot the results to visualize them:Interesting. The model has done pretty well to segment both the cars as well as the bicycle in the image. We can look at each mask or the segmented objects separately as well. Let’s see how we can do that.I will first take all the masks predicted by our model and store them in the mask variable. Now, these masks are in the boolean form (True and False) and hence we need to convert them to numbers (1 and 0). Let’s do that first:Output:This will give us an array of 0s and 1s, where 0 means that there is no object at that particular pixel and 1 means that there is an object at that pixel. Note that the shape of the mask is similar to that of the original image (you can verify that by printing the shape of the original image).However, the 3 here in the shape of the mask does not represent the channels. Instead, it represents the number of objects segmented by our model. Since the model has identified 3 objects in the above sample image, the shape of the mask is (480, 640, 3). Had there been 5 objects, this shape would have been (480, 640, 5).We now have the original image and the array of masks. To print or get each segment from the image, we will create a for loop and multiply each mask with the original image to get each segment:This is how we can plot each mask or object from the image. This can have a lot of interesting as well as useful use cases. Getting the segments from the entire image can reduce the computation cost as we do not have to preprocess the entire image now, but only the segments.Below are a few more results which I got using our Mask R-CNN model:Looks awesome! You have just built your own image segmentation model using Mask R-CNN — well done.I love working with this awesome Mask R-CNN framework. Perhaps I will now try to integrate that into a self-driving car system. 🙂Image segmentation has a wide range of applications, ranging from the healthcare industry to the manufacturing industry. I would suggest you try this framework on different images and see how well it performs. Feel free to share your results with the community.In case you have any questions, doubts or feedback regarding the article, do post them in the comments section below.Originally published at https://www.analyticsvidhya.com on July 22, 2019.",22/07/2019,11,9.0,7.0,638.0,421.0,19.0,2.0,0.0,9.0,en
3776,Stepwise Regression Tutorial in Python,Towards Data Science,Ryan Kwok,12.0,9.0,1762.0,"How do you find meaning in data? In our mini project, my friend @ErikaSM and I seek to predict Singapore’s minimum wage if we had one, and documented that process in an article over here. If you have not read it, do take a look.Since then, we have had comments on our process and suggestions to develop deeper insight into our information. As such, this follow-up article outlines two main objectives, finding meaning in data, and learning how to do stepwise regression.In the previous article, we discussed how the talk about a minimum wage in Singapore has frequently been a hot topic for debates. This is because Singapore uses a progressive wage model and hence does not have a minimum wage.The official stance of the Singapore Government is that a competitive pay structure will motivate the labour force to work hard, aligned with the value of Meritocracy embedded in Singapore culture. Regardless of the arguments for or against minimum wages in Singapore, the poor struggle to afford necessities and take care of themselves and their families.We took a neutral stance acknowledging the validity of both sides of the argument and instead presented a comparison of a prediction of Singapore’s minimum wage using certain metrics across different countries. The predicted minimum wage was also contrasted with the wage floors in the Progressive Wage Model (PWM) across certain jobs to spark some discussion about whether the poorest are earning enough.We used data from Wikipedia and World Data to collect data on minimum wage, cost of living, and quality of life. The quality of life dataset includes scores in a few categories: Stability, Rights, Health, Safety, Climate, Costs, and Popularity.The scores across the indicators and categories were fed into a linear regression model, which was then used to predict the minimum wage using Singapore’s statistics as independent variables. This linear model was coded on Python using sklearn, and more details about the coding can be viewed in our previous article. However, I will also briefly outline the modelling and prediction process in this article as well.The predicted annual minimum wage was US$20,927.50 for Singapore. A brief comparison can be seen in this graph below.Our professor encouraged us to use stepwise regression to better understand our variables. From this iteration, we incorporated stepwise regression to assist us in dimensionality reduction not only to produce a simpler and more effective model, but to derive insights in our data.So what exactly is stepwise regression? In any phenomenon, there will be certain factors that play a bigger role in determining an outcome. In simple terms, stepwise regression is a process that helps determine which factors are important and which are not. Certain variables have a rather high p-value and were not meaningfully contributing to the accuracy of our prediction. From there, only important factors are kept to ensure that the linear model does its prediction based on factors that can help it produce the most accurate result.In this article, I will outline the use of a stepwise regression that uses a backwards elimination approach. This is where all variables are initially included, and in each step, the most statistically insignificant variable is dropped. In other words, the most ‘useless’ variable is kicked. This is repeated until all variables left over are statistically significant.Before proceeding to analyse the regression models, we first modified the data to reflect a monthly wage instead of annual wage. This was because we recognised that most people tend to view their wages in months rather than across the entire year. Expressing our data as such would allow our audience to better understand our data. However, it is also worth noting that this change in scale would not affect the modelling process or the outcomes.Looking at our previous model, we produced the statistics to test the accuracy of the model. But before that, we would first have to specify the relevant X and Y columns, and obtain that information from the datafile.Next, to gather the model statistics, we would have to use the statmodels.api library. Here, a function is created which grabs the columns of interest from a list, and then fits an ordinary least squares linear model to it. The statistics summary can then be very easily printed out.Here we are concerned about the column “P > |t|”. Quoting some technical explanations from the UCLA Institute for Digital Research and Education, this column gives the 2-tailed p-value used in testing the null hypothesis.“Coefficients having p-values less than alpha are statistically significant. For example, if you chose alpha to be 0.05, coefficients having a p-value of 0.05 or less would be statistically significant (i.e., you can reject the null hypothesis and say that the coefficient is significantly different from 0).”In other words, we would generally want to drop variables with a p-value greater than 0.05. As seen from the initial summary above, the least statistically significant variable is “Safety” with a p-value of 0.968. Hence, we would want to drop “Safety” as a variable as shown below. The new summary is shown below as well.This time, the new least statistically significant variable is “Health”. Similarly, we would want to remove this variable.We continue this process until all p-values are below 0.05.Finally, we find that there are 5 variables left, namely Workweek, GDP per Capita, Cost of Living Index, Rights, and Popularity. Since each of the p-values are below 0.05, all of these variables are said to be statistically significant.We can now produce a linear model based on this new set of variables. We can also use this to predict Singapore’s minimum wage. As seen, the predicted monthly minimum wage is about $1774 USD.This is the most important part of the process. Carly Fiorina, former CEO of Hewlett-Packard, once said: “The goal is to turn data into information, and information into insight.” This is exactly what we aim to achieve.“The goal is to turn data into information, and information into insight.” ~ Carly Fiorina, former CEO of Hewlett-PackardFrom just looking at the variables, we would have easily predicted which were statistically significant. For example, the GDP per Capita and Cost of Living Index would logically be good indicators of the minimum wage in a country. Even the number of hours in a workweek would make sense as an indicator.However, we noticed that “Rights” was still included in the linear model. This spurred us to first look at the relationship between Rights and Minimum Wage. Upon plotting the graph, we found this aesthetically pleasing relationship.Initially, we wouldn’t have considered Rights to be correlated to Minimum Wage since the more obvious candidates of GDP and Cost of Living stood out more as contributors to the minimum wage level. This made us reconsider how we understood minimum wage and compelled us to dig deeper.From World Data, “Rights” involved civil rights, and revolved mainly around people’s participation in politics and corruption. We found that the Civil Rights Index includes democratic participation by the population and measures to combat corruption. This index also involves public perception of the government including data from Transparency.org.“In addition, other factors include democratic participation by the population and (with less emphasis) measures to combat corruption. In order to assess not only the measures against corruption, but also its perception by the population, the corruption index based on Transparency.org was also taken into account.”This forced us to consider the correlation between Civil Rights and minimum wage. Knowing this information, we did further research and found several articles that might explain this correlation.American civil rights interest group, The Leadership Conference on Civil and Human Rights, released a report about why minimum wage is a civil and human rights issue and the need for stronger minimum wage policy to reduce inequality and ensure that individuals and families struggling in low-paying jobs are paid fairly. It hence makes sense as a country with more democratic participation is also likely to voice concerns about minimum wage, forcing a discussion and consequently increasing it over time.The next variable we looked at was Popularity. We first searched how this was measured from World Data.“The general migration rate and the number of foreign tourists were therefore evaluated as indicators of a country’s popularity. A lower rating was also used to compare the refugee situation in the respective country. A higher number of foreign refugees results in higher popularity, while a high number of fleeing refugees reduces popularity.”At first glance, it seems like there is no correlation. However, if we consider China, France, USA, and Spain as outliers, the majority of the data points seem to better fit an exponential graph. This raises two questions. Firstly, why is there a relationship between Popularity and Minimum Wage? Secondly, why are these four countries outliers?To be very honest, this stumped us. We simply could not see any way where popularity could be correlated to a minimum wage. Nevertheless, there was an important takeaway: that popularity is somehow statistically significant in predicting a minimum wage of a country. While we might not be the people to discover that relationship, this gives insight into our otherwise less meaningful data.It is important to bring back the quote from Carly Fiorina, “The goal is to turn data into information, and information into insight.” We as humans require tools and methods to convert data into information, and experience/knowledge to convert that information into insight.We first used Python as a tool and executed stepwise regression to make sense of the raw data. This let us discover not only information that we had predicted, but also new information that we did not initially consider. It is easy to guess that Workweek, GDP, and Cost of Living would be strong indicators of the minimum wage. However, it is only through regression that we discovered that Civil Rights and Popularity are also statistically significant.In this case, there were research online that we found that could possibly explain this information. This resulted in new insight that minimum wage is actually seen as a human right, and an increase in democratic participation can possibly result in more conversations about a minimum wage and hence increasing it.However, it is not always possible to find meaning in data that easily. Unfortunately, we, as university students, may not be the best people to offer probable explanations to our information. This is seen in our attempts to explain the relationship between Popularity and Minimum Wage. However, it is within our capacity to take this information and spread it to the world, leaving it as an open ended question for discussions to flourish.That is how we can add value to the world using data.",09/03/2021,6,7.0,0.0,1108.0,758.0,8.0,1.0,0.0,10.0,en
3777,Credit Scoring with Machine Learning,Passion for Data Science,Hongri Jia,195.0,7.0,1252.0,"The credit score is a numeric expression measuring people’s creditworthiness. The banking usually utilizes it as a method to support the decision-making about credit applications. In this blog, I will talk about how to develop a standard scorecard with Python (Pandas, Sklearn), which is the most popular and simplest form for credit scoring, to measure the creditworthiness of the customers.Nowadays, creditworthiness is very important for everyone since it is regarded as an indicator for how dependable an individual is. In various situations, service suppliers need to evaluate customers’ credit history first, and then decide whether they will provide the service or not. However, it is time-consuming to check the entire personal portfolios and generate a credit report manually. Thus, the credit score is developed and applied for this purpose because it is time-saving and easily comprehensible.The process of generating the credit score is called credit scoring. It is widely applied in many industries especially in the banking. The banks usually use it to determine who should get credit, how much credit they should receive, and which operational strategy can be taken to reduce the credit risk. Generally, it contains two main parts:Here I will introduce the most popular credit scoring method called scorecard. There are two main reasons why the scorecard is the most common form for credit scoring. First, it is easy to interpret to people who has no related background and experience such as the clients. Second, the development process of the scorecard is standard and widely understood, which means the companies don’t have to spend much money on it. A sample scorecard is shown below. I will talk about how to use it later.Now I’m going to give some details about how to develop a scorecard. The data set I used here is from the Kaggle competition. The detailed information is listed in the Figure-2. The first variable is the target variable, which is a binary categorical variable. And the rest of the variables are the features.After gaining an insight into the data set, I start to apply some feature engineering methods on it. First, I check each feature if it contains missing values, and then impute the missing values with median.Next, I do the outlier treatment. Generally, the methods used for outliers depends on the type of outliers. For example, if the outlier is due to mechanical error or problems during measurement, it can be treated as missing data. In this data set, there are some extremely large value, but they are all reasonable values. Thus, I apply top and bottom coding to deal with them. In Figure-3, you can see after applying the top coding, the distribution of the feature is more normal.According to the sample scorecard shown in Figure-1, it is obvious that each feature should be grouped into various attributes (or groups). There are some reasons for grouping the features.Binning is a proper method used for this purpose. After the treatment, I assign each value to the attribute in which it should be, which also means all numeric values are converted to categorical. Here is a example for the outcome of binning.After grouping all the features, the feature engineering is completed. Next step is to calculate the weight of evidence for each attribute and the information value for each characteristics (or feature). As mentioned before, I have used binning to convert all numeric value into categorical. However, we cannot fit model with these categorical values, so we have to assign some numeric values to these groups. The purpose of the Weight of Evidence (WoE) is exactly to assign a unique value to each group of categorical variables. The Information Value (IV) measures predictive power of the characteristic, which is used for feature selection. The formula of WoE and IV is given below. Here the “Good” means the customer won’t have serious delinquency or target variable is equal to 0, and “Bad” means the customer will have serious delinquency or target variable is equal to 1.Usually, characteristics analysis reports are produced to get WoE and IV. Here I define a function in Python to generate the reports automatically. As an example, the characteristics analysis report for “Age” is shown in Figure-5.Then I make a bar chart to compare the IV of all the features. In the bar chart, you can see the last two features “NumberOfOpenCreditLinesAndLoans” and “NumberRealEstateLoansOrLines” have pretty low IV, so here I choose other eight feature for model fitting.After the feature selection, I replace the attributes with the corresponding WoE. Until now, I get the proper data set for the model training. The model used for developing scorecard is logistic regression, which is a popular model for binary classification. I apply cross validation and grid search to tune the parameters. Then, I use the test data set to check the prediction accuracy of the model. Since the Kaggle won’t give the values for target variable, I have to submit my result online to obtain the accuracy. To show the effect of data processing, I train the model with raw data and the processed data. Based on the result given by the Kaggle, the accuracy is improved from 0.693956 to 0.800946 after the data processing.The final step is calculating the scorecard point for each attribute and produce the final scorecard. The score for each attribute can be calculated with the formula:Score = (β×WoE+ α/n)×Factor + Offset/nWhere:β — logistic regression coefficient for characteristics that contains the given attributeα — logistic regression intercept WoE — Weight of Evidence value for the given attributen — number of characteristics included in the modelFactor, Offset — scaling parameterThe first four parameters have already been calculated is the previous part. The following formulas are used for calculating factor and offset.Here, pdo means points to double the odds and the bad rate has been already calculated in the characteristics analysis reports above. If a scorecard has the base odds of 50:1 at 600 points and the pdo of 20 (odds to double every 20 points), the factor and offset would be:Factor = 20/Ln(2) = 28.85Offset = 600- 28.85 × Ln (50) = 487.14When finishing all the calculation, the process of developing the scorecard is done. Part of the scorecard is shown in Figure-7.When you have new customers coming, you just need to find the correct attribute in each characteristics according to the data and get the score. The final credit score can be calculated as the sum of the score of each characteristics. For instance, the bank has a new applicant for credit card with age of 45, debt ratio of 0.5 and monthly income of 5000 dollars. The credit score should be: 53 + 55 + 57 = 165.To develop a more accurate scorecard, people usually have to consider more situations. For example, there are some individuals identified as “Bad” in the population but their application is approved, while there will be some “Good” persons that have been declined. Thus, reject inference is supposed to be involved in the development process. I don’t do this part because it requires the data set of rejected cases which I don’t have in my data. If you want know more about this part, I highly recommend you to read Credit Risk Scorecards — Developing and Implementing Intelligent Credit Scoring written by Naeem Siddiqi.If you are interested in my work or have some problems about it, please feel free to contact me. At the meantime, if you want to know more about what students learn from WeCloudData’s data science courses, check out this website:www.weclouddata.com",02/04/2018,0,1.0,1.0,773.0,405.0,10.0,3.0,0.0,1.0,en
3778,"If you’re looking for a way to use Gensim to setup a doc2vec model, I found the following works... ",Medium,Justin Davies,132.0,1.0,98.0,"If you’re looking for a way to use Gensim to setup a doc2vec model, I found the following works rather well for my use case.from gensim.models.doc2vec import LabeledSentencefrom os import listdirfrom os.path import isfile, joinimport gensimimport DocIterator as DocItdocLabels = []docLabels = [f for f in listdir(“/Users/justin/DeepLearning/suck/GBP_USD/train/neu”) if f.endswith(‘.txt’)]data = []for doc in docLabels:with open(“/Users/justin/DeepLearning/suck/GBP_USD/train/neu/” + doc, ‘r’) as f:data.append(f.read())it = DocIt.DocIterator(data, docLabels)model = gensim.models.Doc2Vec(size=300, window=10, min_count=5, workers=3,alpha=0.04, min_alpha=0.005) # use fixed learning ratemodel.build_vocab(it)for epoch in range(100):print(“Epoch “ + str(epoch))model.train(it)print(model.docvecs.most_similar([“6605c7c39fc7d99889fc047488dc9e33.txt”], topn=10))model.alpha -= 0.002 # decrease the learning rateprint(model.alpha)model.min_alpha = model.alpha # fix the learning rate, no decamodel.train(it)print(model.docvecs.most_similar([“6605c7c39fc7d99889fc047488dc9e33.txt”], topn=10))model.save(“doc2vec.model”)",06/06/2016,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,en
3779,Creating and training a U-Net model with PyTorch for 2D & 3D semantic segmentation: Model building [2/4],Towards Data Science,Johannes Schmidt,157.0,6.0,954.0,"In the previous chapter we built a dataloader that picks up our images and performs some transformations and augmentations so that they can be fed in batches to a neural network like the U-Net. In this part, we focus on building a U-Net from scratch with the PyTorch library. The goal is to implement the U-Net in such a way, that important model configurations such as the activation function or the depth can be passed as arguments when creating the model.The U-Net is a convolutional neural network architecture that is designed for fast and precise segmentation of images. It has performed extremely well in several challenges and to this day, it is one of the most popular end-to-end architectures in the field of semantic segmentation.We can split the network into two parts: The encoder path (backbone) and the decoder path. The encoder captures features at different scales of the images by using a traditional stack of convolutional and max pooling layers. Concretely speaking, a block in the encoder consists of the repeated use of two convolutional layers (k=3, s=1), each followed by a non-linearity layer, and a max-pooling layer (k=2, s=2). For every convolution block and its associated max pooling operation, the number of feature maps is doubled to ensure that the network can learn the complex structures effectively.The decoder path is a symmetric expanding counterpart that uses transposed convolutions. This type of convolutional layer is an up-sampling method with trainable parameters and performs the reverse of (down)pooling layers such as the max pool. Similar to the encoder, each convolution block is followed by such an up-convolutional layer. The number of feature maps is halved in every block. Because recreating a segmentation mask from a small feature map is a rather difficult task for the network, the output after every up-convolutional layer is appended by the feature maps of the corresponding encoder block. The feature maps of the encoder layer are cropped if the dimensions exceed the one of the corresponding decoder layers.In the end, the output passes another convolution layer (k=1, s=1) with the number of feature maps being equal to the number of defined labels. The result is a u-shaped convolutional network that offers an elegant solution for good localization and use of context. Let’s take a look at the code.This code is based on https://github.com/ELEKTRONN/elektronn3/blob/master/elektronn3/models/unet.py (c) 2017 Martin Drawitsch, released under MIT License, which implements a configurable (2D/3D) U-Net with user-defined network depth and a few other improvements of the original architecture. They themselves actually used the 2D code from Jackson Huang https://github.com/jaxony/unet-pytorch.Here is a simplified version of the code — saved in a file unet.py:I will not go into detail here, but rather just mention important design choices. It can be useful to view the architecture in repeating blocks in the encoder but also in the decoder path. As you can see in unet.py the DownBlock and the UpBlock help to build the architecture. Both use smaller helper functions that return the correct layer, depending on what arguments are passed , e.g. if a 2D (dim=2) or 3D (dim=3) network is wanted. The number of blocks is defined by the depth of the network.A DownBlock generally has the following scheme:A UpBlock has the following layers:For our Unet class we just need to combine these blocks and make sure that the correct layers from the encoder are concatenated to the decoder (skip pathways). These layers have to be cropped if their sizes do not match with the corresponding layers from the decoder. In such cases, the autocrop function is used. For merging, I concatenate along the channel dimension (see Concatenate). Instead of transposed convolutions we could also use upsampling layers (interpolation methods) that are followed by a 1x1 or 3x3 convolution block to reduce the channel dimension. Using interpolation generally gets rid of the checkerboard artifact. For 3D input consider using trilinear interpolation.At the end we just need to think about the parameter initialization. By default, the weights are initialized with torch.nn.init.xavier_uniform_ and the biases are initialized with zeros using torch.nn.init.zeros_.For details and the available parameter options, I encourage you to take a look at the code. Feel free to change the code to your needs or expand e.g. the number of activation functions.Let’s create such a model and use it to make a prediction on some random input:This will give us:To check weather our model is correct, we can get the model’s summary with this package pytorch-summary:which prints out a summary like this:To ensure correct semantic concatenations, it is advised to use input sizes that return even spatial dimensions in every block but the last in the encoder. For example: An input size of 120² gives intermediate output shapes of [60², 30², 15²] in the encoder path for a U-Net with depth=4 . A U-Net with depth=5 with the same input size is not recommended, as a maxpooling operation on odd spatial dimensions (e.g. on a 15² input) should be avoided.To make our lives easier, we can numerically compute the maximum network depth for a given input dimension with a simple function:This will outputwhich tells us that that we can design a U-Net as deep as this without having to worry about semantic mismatches. Conversely, we can also numerically determine the possible input shapes dimensions for a given depth:This will outputwhich tells us that we can have 3 different input shapes with such a level 8 U-Net architecture. But I dare to say that such a network with this input size is probably not useful in practice.In this part we created a configurable UNet model for the purpose of semantic segmentation. Now that we have built our model, it is time to create a training loop in the next chapter.",02/12/2020,7,0.0,0.0,953.0,256.0,3.0,0.0,0.0,7.0,en
3780,The best explanation of Convolutional Neural Networks on the Internet!,TechnologyMadeEasy,Harsh Pokharna,2600.0,5.0,726.0,"CNNs have wide applications in image and video recognition, recommender systems and natural language processing. In this article, the example that I will take is related to Computer Vision. However, the basic concept remains the same and can be applied to any other use-case!For a quick recap of Neural Networks, here’s a very clearly explained article series.CNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output. The whole network has a loss function and all the tips and tricks that we developed for neural networks still apply on CNNs. Pretty straightforward, right?So, how are Convolutional Neural Networks different than Neural Networks?What do we mean by this?Unlike neural networks, where the input is a vector, here the input is a multi-channeled image (3 channeled in this case).There are other differences that we will talk about in a while.Before we go any deeper, let us first understand what convolution means.We take the 5*5*3 filter and slide it over the complete image and along the way take the dot product between the filter and chunks of the input image.For every dot product taken, the result is a scalar.So, what happens when we convolve the complete image with the filter?I leave it upon you to figure out how the ‘28’ comes. (Hint: There are 28*28 unique positions where the filter can be put on the image)The convolution layer is the main building block of a convolutional neural network.The convolution layer comprises of a set of independent filters (6 in the example shown). Each filter is independently convolved with the image and we end up with 6 feature maps of shape 28*28*1.Suppose we have a number of convolution layers in sequence. What happens then?All these filters are initialized randomly and become our parameters which will be learned by the network subsequently.I will show you an example of a trained network.Take a look at the filters in the very first layer (these are our 5*5*3 filters). Through back propagation, they have tuned themselves to become blobs of coloured pieces and edges. As we go deeper to other convolution layers, the filters are doing dot products to the input of the previous convolution layers. So, they are taking the smaller coloured pieces or edges and making larger pieces out of them.Take a look at image 4 and imagine the 28*28*1 grid as a grid of 28*28 neurons. For a particular feature map (the output received on convolving the image with a particular filter is called a feature map), each neuron is connected only to a small chunk of the input image and all the neurons have the same connection weights. So again coming back to the differences between CNN and a neural network.Parameter sharing is sharing of weights by all neurons in a particular feature map.Local connectivity is the concept of each neural connected only to a subset of the input image (unlike a neural network where all the neurons are fully connected)This helps to reduce the number of parameters in the whole system and makes the computation more efficient.I will not be talking about the concept of zero padding here as the idea is to keep it simple. Interested people can read about it separately!A pooling layer is another building block of a CNN.Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently.The most common approach used in pooling is max pooling.We have already discussed about convolution layers (denoted by CONV) and pooling layers (denoted by POOL).RELU is just a non linearity which is applied similar to neural networks.The FC is the fully connected layer of neurons at the end of CNN. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks and work in a similar way.I hope you understand the architecture of a CNN now. There are many variations to this architecture but as I mentioned before, the basic concept remains the same. In case you have any doubts/feedback, please comment.You can follow me to read more TechnologyMadeEasy articles!And if you want your friends to read this too, click share!References: http://cs231n.github.io/convolutional-networks/#overview",28/07/2016,0,14.0,14.0,708.0,376.0,10.0,0.0,0.0,3.0,en
3781,"Key takeaways — O’reilly AI London Conference, Oct 9–11, 2018 ",Medium,Debmalya Biswas,132.0,8.0,1725.0,"Key takeaways — O’reilly AI London Conference, Oct 9–11, 2018I had an opportunity to attend the O’reilly AI London Conference, Oct 9–11, 2018. Given our short attention span these days, let me try a more clickbait style approach for the takeaways :)Key takeaways1. AI Gurus are the new rock stars and there was never a better time to be in this field. There continues to be tremendous interest in Enterprise AI. This was the first O’reilly AI Conference in Europe and it was ‘Sold Out’. Including this one, O’reilly now organizes 5 AI Conferences all over the world in a year, and all of them get ‘Sold Out’. The situation is still better than say NIPS, which got Sold Out in a couple of weeks. (“The meeting has sold out. The waitlist is now full” — NIPS 2018 website.)2. There were all the usual suspects at the conference: IBM, Amazon, Google, Microsoft, Intel – mostly prompting their Cloud offerings. It is interesting see the ‘Head of ..’, ‘VP of …’ come one after the other touting their respective platforms, without commenting on how their offering differs from what the previous speaker was presenting.In my opinion, there is hardly any differentiation between the ML/DL offerings of the different Cloud platforms today. All of them are basically providing a managed service out of algorithms which are anyway available (for free) in the research/open source domain. Whatever proprietary 5% differentiation exists, it can probably lead to 5% higher accuracy for a very specific use-case. The point however is that if you had such a strategic use-case for which that 5% mattered, you would probably develop it in-house rather than going to the Cloud. Note that I am not saying that Cloud ML offerings do not have any value. On the contrary, they are great for fast experimentation and showing early results; however, it is not worth going into lengthy debates/RFIs/RFPs to assess if Google Cloud is better than Watson.3. Do NOT go into every Google or Facebook talk thinking it will improve your IQ and give you groundbreaking insights into how AI is practiced at these companies. Having followed such talks at AI conferences for some time now, I think I have finally uncovered their underlying operational pattern.On the one hand, these companies are becoming more open than ever; at least in the field of AI. A few years ago, one could have hardly imagined Facebook sharing the algorithmic details of their ML based services and the underlying infrastructure, Amazon/Uber sharing details of their forecasting algorithms, etc. However, this is exactly the type of information that you would find on their respective blogs today — a few recommended ones to follow:Apple Machine Learning JournalGoogle AI BlogUber EngineeringFacebook ResearchThe presentations however are a different story. It would seem that once these blogs are published, a common set of slides are issued internally which are then presented by different employees at different venues. For instance, the following talk at the conference:Learning at Facebook: An Infrastructure Perspective — Yangqing Jia (Facebook), Dmytro Dzhulgakov (Facebook)Is basically based on their paper/blog post published last year.Plus, there are of course some (e.g. the ones below) which have absolutely nothing new or interesting to offer and would most likely not have even earned a slot if the presenters were not from Google.What is ML Ops? Solutions and best practices for applying DevOps to production ML services — Kaz Sato (Google)The future of conversational UI — Alice Zimmermann (Google)4. On a more technical note, Deep Learning (DL) methods for NLP and Computer Vision (Image Classification, Object Detection, etc.) seem to have reached a saturation point – there is nothing fundamentally new happening in these fields. The focus is rather on enterprise tools in the form of more Cloud services offering these APIs, mature Open Source frameworks; leading to their wider adoption in enterprise settings.5. As such, the focus on AI Research, and consequently of this conference has shifted to the following topics:We will explore the above topics in more detail in the rest of this report.6. DL for Forecasting: For years, Forecasting research has focused on statistical methods, e.g. ARIMA; which are quite mature and widely used. Recently, given the time-step nature of Recurrent Neural Networks (RNNs), and it variant Long Short-term Memory (LSTM) networks, DL Researchers have start challenging the foothold of statistical methods for Forecasting. While the jury is still out on this topic, and the best algorithm for your problem will always depend on your data characteristics; there is growing consensus that a hybrid: (statistical + ML/DL) model works best.“Interestingly, one winning entry to the M4 Forecasting Competition was a hybrid model that included both hand-coded smoothing formulas inspired by a well known the Holt-Winters method and a stack of dilated long short-term memory units (LSTMs).”The above observation was presented by a very interesting talk by Uber:Forecasting at Uber: Machine learning approaches — Andrea Pasqua (Uber)which actually is summary of their blog post:Forecasting at Uber: An IntroductionOther interesting talks on this topic included the below presentation by SAS, where they explored different architectures of combining statistical and ML/DL models.Business forecasting using hybrid approach: A new forecasting method using deep learning and time series — Pasi Helenius (SAS), Larry Orimoloye (SAS)The below presentation provided a great overview of recent research papers in this field:Deep prediction: A year in review for deep learning for time series — Aileen Nielsen (Skillman Consulting)Among the cited papers was the below one by Amazon, which provides some interesting insights into their probabilistic forecasting algorithms:DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks — Valentin Flunkert, David Salinas, Jan Gasthaus (Amazon)For an introduction to this topic, there was a very good tutorial presented at the conference by a team of Microsoft Data Scientists.Recurrent neural networks for time series forecasting — Yijing Chen (Microsoft), Dmitry Pechyoni (Microsoft), Angus Taylor (Microsoft), Vanja Paunic (Microsoft)an overview of RNN architectures for Time Series Forecasting. It does provide a performance comparison of the architectures as well. All the notebooks are available on their GitHub page and can be used as a starting point to implement the different architectures.7. AIOps: DevOps to operationalize AI products/modelsAs Enterprise AI usage mature, there is increasing focus on AIOps/MLOps to set up the right DevOps practices to efficiently support AI use-cases from experimentation to production scale deployments.There were some interesting knowledge sharing presentations on this topic by Zalando and LinkedIn.Architecting AI applications - Mikio Braun (Zalando SE)TonY: Native support of TensorFlow on Hadoop - Jonathan Hung (LinkedIn), Keqiu Hu (LinkedIn), Anthony Hsu (LinkedIn)The major announcement in this area was of course the recent announcement of PyTorch 1.0, and how it compares (and will compete) w.r.t. TensorFlow.PyTorch 1.0 - bringing research and production together - Dmytro Dzhulgakov (Facebook)The significance of PyTorch 1.0 comes from FB’s earlier positioning that they were using different frameworks for research/experimentation and production – PyTorch for Research and Caffee2 for Production – relying and investing in the ONNX toolchain to perform model transformation between the 2 frameworks.PyTorch 1.0 promises to unify the two worlds allowing the usage of 1 framework for both experimentation and production. Coming from FB, there is and will be significant interest in PyTorch 1.0. At this stage however, in my opinion, it is still early days and PyTorch seems to lack the maturity and widespread adoption (in terms of the availability of 3rd party open source libraries leveraging TensorFlow) of TensorFlow. TensorFlow has also been investing in improving its debugging and visualization capabilities; and there is no need to abandon all your TensorFlow notebooks and start migrating them to PyTorch 1.0 just yet.8. Explainable AIThis refers to the requirement that when a ML/DL model takes a decision, it is possible to explain the parameters underlying that decision. In the interests of taking unbiased and fair decisions, and not least the “Right to Explainability” clause in GDPR; there is considerable interest in the topic and there were quite a few related presentations at the conference.How to build privacy and security into deep learning models — Yishay Carmiel (IntelligentWire)Building safe artificial intelligence with OpenMined — Andrew Trask (OpenMined)Protecting your secrets — Katharine Jarmul (KIProtect)For me, the standout announcement/presentation on this topic was the one below by IBM:Trust and transparency of AI for the enterprise (sponsored by IBM Watson)- Ruchir Puri (IBM)It features a business-oriented dashboard to help explain AI-powered recommendations or decisions, and tools to mitigate bias early in the data collection and management phases. For instance, the below dashboard shows a “Policy Age” bias creeping into a Claims Approval process. For more technical details, please refer to Ruchir Puri’s blog on this topic.As I have long said, IBM Watson may not be the smartest, but it is one of the most enterprise ready platforms out there. And, with the release of this service; they have once again validated this view. It is not very clear which algorithms/models are supported by the “explainability” layer, and other cloud platforms will most likely catch-up soon; but the point is that they were the first to bring to market this enterprise handy feature and it has value even if it works only for the most basic ML based classification/segmentation models.9. GANs for synthetic data generationSynthetic data generation, esp. using Generative Adversarial Networks (GANs), remains an area of active research to address the shortage of training data to train DL models.Performance evaluation of GANs in a semisupervised OCR use case - Florian Wilhelm (iNovex)How to augment sparse training sets with synthetic data - Daeil Kim (AI.Reverie)10. Reinforcement Learning (RL)I will keep this short. Let me know if you made it this far are still looking for more insights ☺ Suffice it to say that with the current saturation setting into DL methods, there is quite a bit of expectation that RL will be the next big thing in AI.Deep reinforcement learning: How to avoid the hype and make it work for you - Dr. Sid J Reddy (Conversica)The really positive development in this space is the growing availability of RL frameworks (below + a few others by OpenAI, Facebook and Microsoft/Bonsai) that will allow non-specialists to leverage RL - having the same effect that TensorFlow, Keras and Caffe/PyTorch had on DL adoption in the last couple of years.Reinforcement Learning Coach - Gal Novik (Intel AI)Building reinforcement learning applications with Ray - Richard Liaw (UC Berkeley RISELab), Eric Liang (UC Berkeley RISELab)",18/11/2018,0,11.0,0.0,570.0,285.0,3.0,1.0,0.0,31.0,en
3782,Machine Learning — Word Embedding & Sentiment Classification using Keras,Towards Data Science,Javaid Nabi,978.0,9.0,1288.0,"In the previous post, we discussed various steps of text processing involved in Nature Language Processing (NLP) and also implemented a basic Sentiment Analyzer using some of the classical ML techniques.Deep learning has demonstrated superior performance on a wide variety of tasks including NLP, Computer Vision, and Games. To explore further, we will discuss and use some of the advanced NLP techniques, based on Deep Learning, to create an improved Sentiment Classifier.Sentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about.The input X is a piece of text and the output Y is the sentiment which we want to predict, such as the star rating of a movie review.If we can train a system to map from X to Y based on a labelled data set like above, then such a system can be used to predict sentiment of a reviewer after watching a movie.In this post we will focus on below tasks:Deep learning text classification model architectures generally consist of the following components connected in sequence:The IMDB movie review set can be downloaded from here. This dataset for binary sentiment classification contains set of 25,000 highly polar movie reviews for training, and 25,000 for testing. The dataset after initial pre-processing is saved to movie_data.csv file. First we load the IMDb dataset, the text reviews are labelled as 1 or 0 for positive and negative sentiment respectively.The word embeddings of our dataset can be learned while training a neural network on the classification problem. Before it can be presented to the network, the text data is first encoded so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API provided with Keras. We add padding to make all the vectors of same length (max_length). Below code converts the text to integer indexes, now ready to be used in Keras embedding layer.The Embedding layer requires the specification of the vocabulary size (vocab_size), the size of the real-valued vector space EMBEDDING_DIM = 100, and the maximum length of input documents max_length .We are now ready to define our neural network model. The model will use an Embedding layer as the first hidden layer. The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset during training of the model.The summary of the model is:We have used a simple deep network configuration for demonstration purpose. You can try out different configuration of the network and compare the performance. The embedding param count 12560200 = (vocab_size * EMBEDDING_DIM). Maximum input length max_length = 2678. The model during training shall learn the word embeddings from the input text. The total trainable params are 12,573,001.Now let us train the model on training set and cross validate on test set. We can see from below training epochs that the model after each epoch is improving the accuracy. After a few epochs we reach validation accuracy of around 84%. Not bad :)We can test our model with some sample reviews to check how it is predicting the sentiment of each review. First we will have to convert the text review to tokens and use model to predict as below.The output gives the prediction of the word either to be 1 (positive sentiment) or 0 (negative sentiment).Value closer to 1 is strong positive sentiment and a value close to 0 is a strong negative sentiment. I can clearly see that the model prediction is wrong for test_sample_7 and is doing reasonably well for rest of the samples.In the above approach we learn word embedding as part of fitting a neural network model.There is another approach to building the Sentiment clarification model. Instead of training the embedding layer, we can first separately learn word embeddings and then pass to the embedding layer. This approach also allows to use any pre-trained word embedding and also saves the time in training the classification model.We will use the Gensim implementation of Word2Vec. The first step is to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, removing stop words etc. The word2vec algorithm processes documents sentence by sentence.we have 50000 review lines in our text corpus. Gensim’s Word2Vec API requires some parameters for initialization.i. sentences – List of sentences; here we pass the list of review sentences.ii. size – The number of dimensions in which we wish to represent our word. This is the size of the word vector.iii. min_count – Word with frequency greater than min_count only are going to be included into the model. Usually, the bigger and more extensive your text, the higher this number can be.iv. window – Only terms that occur within a window-neighborhood of a term, in a sentence, are associated with it during training. The usual value is 4 or 5.v. workers– Number of threads used in training parallelization, to speed up trainingAfter we train the model on our IMDb dataset, it builds a vocabulary size = 134156 . Let us try some word embeddings the model learnt from the movie review dataset.The most similar words for word horrible are:Try some math on the word vectors — woman+king-man=?Let us find the odd word woman, king, queen, movie = ?This is very interesting to see the word embeddings learned by our word2vec model form the text corpus. The next step is to use the word embeddings directly in the embedding layer in our sentiment classification model. we can save the model to be used later.Since we have already trained word2vec model with IMDb dataset, we have the word embeddings ready to use. The next step is to load the word embedding as a directory of words to vectors. The word embedding was saved in file imdb_embedding_word2vec.txt. Let us extract the word embeddings from the stored file.The next step is to convert the word embedding into tokenized vector. Recall that the review documents are integer encoded prior to passing them to the Embedding layer. The integer maps to the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the Embedding layer such that the encoded words map to the correct vector.Now we will map embeddings from the loaded word2vec model for each word to the tokenizer_obj.word_index vocabulary and create a matrix with of word vectors.We are now ready with the trained embedding vector to be used directly in the embedding layer. In the below code, the only change from previous model is using the embedding_matrix as input to the Embedding layer and setting trainable = False, since the embedding is already learned.Look closely, you can see that model total params = 13,428,501 but trainable params = 12801. Since the model uses pre-trained word embedding it has very few trainable params and hence should train faster.To train the sentiment classification model, we use VALIDATION_SPLIT= 0.2, you can vary this to see effect on the accuracy of the model.Finally training the classification model on train and validation test set, we get improvement in accuracy with each epoch run. We reach 88% accuracy with just around 5 epochs.You can try to improve the accuracy of the model by changing hyper-parameters, running more epochs etc,. Also, you can use some other pre-trained embeddings prepared on very large corpus of text data that you can directly download.In this post we discussed in detail the architecture of Deep Learning model for sentiment classification. We also trained a word2vec model and used it as a per-trained embedding for sentiment classification.Thanks for reading, if you liked it, please give a clap to it.http://ruder.io/deep-learning-nlp-best-practiceskeras.iomachinelearningmastery.comHands-On NLP with Python, By Rajesh Arumugam, Rajalingappaa Shanmugamani July 2018",04/10/2018,4,6.0,6.0,692.0,268.0,24.0,5.0,0.0,11.0,en
3783,Computer Vision: Instance Segmentation with Mask R-CNN,Towards Data Science,Renu Khandelwal,3900.0,8.0,702.0,"This is the fourth part in the series on Computer vision journey. In this article we will explore Mask R-CNN to understand how instance segmentation works with Mask R-CNN and then predict the segmentation for an image with Mask R-CNN using KerasPart 1- CNN, R-CNN, Fast R-CNN, Faster R-CNNPart 2 — Understanding YOLO, YOLOv2, YOLO v3Part 3- Object Detection with YOLOv3 using KerasWhat is instance segmentation and how is different from semantic segmentation?Semantic Segmentation detects all the objects present in an image at the pixel level. Outputs regions with different classes or objectsSemantic segmentation groups pixels in a semantically meaningful way. Pixels belonging to a person, road, building, fence, bicycle, cars or trees are grouped separately.Instance Segmentation is identifying each object instance for every known object within an image.Instance segmentation assigns a label to each pixel of the image. It is used for tasks such as counting the number of objectsInstance segmentation requiresMask R-CNN extends Faster R-CNN.What’s different in Mask R-CNN and Faster R-CNN?Mask R-CNN has an additional branch for predicting segmentation masks on each Region of Interest (RoI) in a pixel-to pixel mannerFaster R-CNN is not designed for pixel-to-pixel alignment between network inputs and outputs.Faster R-CNN has two outputsMask R-CNN has three outputsWhat’s similar between Mask R-CNN and Faster R-CNN?-Both Mask R-CNN and Faster R-CNN have a branch for classification and bounding box regression.-Both use ResNet 101 architecture to extract features from image.-Both use Region Proposal Network(RPN) to generate Region of Interests(RoI)How does Mask R-CNN work?Mask R-CNN model is divided into two partsMask R-CNN uses anchor boxes to detect multiple objects, objects of different scales, and overlapping objects in an image. This improves the speed and efficiency for object detection.Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect.To predict multiple objects or multiple instances of objects in an image, Mask R-CNN makes thousands of predictions. Final object detection is done by removing anchor boxes that belong to the background class and the remaining ones are filtered by their confidence score. We find the anchor boxes with IoU greater than 0.5. Anchor boxes with the greatest confidence score are selected using Non-Max suppression explained belowIoU computes intersection over the union of the two bounding boxes, the bounding box for the ground truth and the bounding box for the predicted box by algorithmWhen IoU is 1 that would imply that predicted and the ground-truth bounding boxes overlap perfectly.To detect an Object once in an image, Non-Max suppression considers all bounding boxes with IoU >0.5What if I have multiple bounding boxes with IoU greater than 0.5?For this we use MatterPort Mask R-CNN. The Mask R-CNN model used in this example is pre-trained on the COCO data set.Step 1: Clone the Mask R-CNN repositoryStep 2: Download the pre-trained weights for COCO model from MatterPort. Place the file in the Mask_RCNN folder with name “mask_rcnn_coco.h5”Step 3: Import the required librariesStep 4: We Create a myMaskRCNNConfig class that inherits from Mask R-CNN Config class.As I am using CPU hence setting the GPU_COUNT=1COCO dataset has 80 labels so we set the NUM_CLASSES to 80 + 1 (for background)Step 5: Create an instance of the myMaskRCNNConfig classStep 6 :Initialize Mask R-CNN model for “inference” using the Config instance that we createdStep 7: Load the weights for the Mask R-CNN. These are the pre-trained weights for COCO data setStep 8: Define 80 classes that the coco model and 1 for background(BG)Step 9: Function for drawing boxes for objected detected in the imageStep 10: We finally make the prediction and draw bounding boxes around the detected objectsLoading the image and then converting it to a numpy arraywe now make the predictionResult is a dictionary for the image that we passed into the detect() function.Dictionary has keys for the bounding boxes, masks, class and the scores. Each key points to a list for multiple possible objects detected in the image.The keys of the dictionary areStep 11: Visualize the results by drawing bounding box around the region of interest(rois)Drawing the maskTo find the number of object and classesCode for prediction using Mask R-CNN available at githubMask R-CNN paperhttps://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272http://www.cs.toronto.edu/~urtasun/courses/CSC2541/08_instance.pdfmachinelearningmastery.com",31/07/2019,13,48.0,20.0,661.0,384.0,11.0,7.0,0.0,14.0,en
3784,Traveling santa Problem — An incompetent algorist’s attempt,Medium,Hrishikesh Huilgolkar,186.0,4.0,693.0,"Kaggle announced the Traveling santa problem in the christmas season. I joined in excitedly.. but soon realized this is not an easy problem. Solving this problem would require expertise on data structures and some good familiarity with TSP problems and its many heuristic algorithms. I had neither.. I had to find a way to deal with this problem. I compenseted my lack of algorithmic expertise with common sense, logic and intuition. I finished 65th out of 356 total competitors.I did some research on packaged TSP solvers and top TSP algorithms. I found concorde but I could not get it to work on my ubuntu machine. So I settled with LKH which uses Lin-Kernighan heuristic for solving TSP and related problems. I wrote scripts for file conversions and for running LKH.LKH easily solved my tsp problem in around 30 hours. But it was just one path. I still had to figure out how to make it find the second path.A simple idea to get 2 disjoint paths is to generate first path and then make weight of those edges infinite and run LKH on the problem again. But this required the problem to be in Distance Matrix format.Then I found a major problem.Problem: Ram too lowCreating distance matrix for 150,000 points was unimaginable.It would requirememory for one digit * 150,000 * 150,000assuming memory for one digit = 8 bytesmemory required = 8*150,000²which is 167 GB!(Correct me if I am wrong)Solution:A simple solution was to divide the map in manageable chunks.I used scipy’s distance matrix creation function scipy.spatial.distance.pdist() It creates distance matrix from coordinates.The matrix created by pdist is in compressed form (a flattened matrix of upper diagonal elements. scipy.spatial.distance.squareform() can create a square distance matrix from compressed matrix but that would waste a lot of ram.So I created a custom function which divided compressed matrix by rows so LKH can read it.Input:(coordinates)1 12 34 1output of pdist:(compressed upper column)1 2 4Output of squreform():(Uncompressed square matrix)0 1 21 0 42 4 0Output of my function which processed compressed matrix:(Upper diagonal elements)[[1,2],[4]]Lots of ram saved!I tried using Manhattan distance instead of euclidean distance. But after dividing the problem in grids, time taken by distance calculation was manageable so I stuck with euclidean distance.Through trial and error, I found that on my laptop with 4 GB ram, a 6 by 6 grid in the above format was manageable for both creating distance matrix and for LKH.I ran LKH on resulting distance matrices and joined the individual solutions.I joined the resulting solutions in different combination for both paths so as to avoid common paths.I got 7,415,334 with this method.I tried time limit on LKH algorithm. From 40,000 seconds I reduced it to 300, 20, 5 ,1 seconds but It made the results slightly worse.MingleThe solution above was good but It could have been better. The problem was that the first path was so good that the second path struggled to find good path. The difference between the two paths was big.Path1 ~= 6.2MPath2 ~= 7.4MFor a long time I thought this would require either solving both paths simultaneously or using genetic algorithm or similar algorithm to combine both paths. Both were pretty difficult to implement.Then I got a simple idea. My map was divided in 36 squares. If I combine 18 squares of first path and 18 squares of second path, I will have a path whose distance will be approximately average of the two paths.I tried this trick and used different combinations of the two paths squares and got the best score of 6,807,498For new path1, select blue squres from old path1 and grey square from old path2Use remaining squares for new path2.Remove cross linesMy squares were joined in a zigzag manner. I removed the zig-zag lines for a further improvement.I scored 6,744,291 which was my best score.Another idea was to make end point of one square and the beginning point of next square as near as possible but I couldn’t implement the idea before deadline.My score was around 200,000 points away from the first place which was 6,526,972. Not bad!Public repo: https://bitbucket.org/hrishikeshio/traveling-santa (More documentation for source code coming soon)Originally published at www.blogicious.com on January 19, 2013.",19/01/2013,0,7.0,3.0,202.0,145.0,4.0,0.0,0.0,4.0,en
3785,Data without context is meaningless (and boring),SlideMagic,Jan Schultink,1100.0,1.0,212.0,"The quarter is done, and here comes the day-long sales results presentation. Excel is pasted into PowerPoint, creating huge decks through which senior management has to sit through. Sales organizes by channel: small restaurants sales, growth; large restaurants sales, growth, supermarkets sales, growth. Marketing presents by brands: brand 1 sales, growth, brand 2 sales, growth.If you are a marketing manager, looking at the Q3 sales and growth figures of a particular brand is really interesting. All the numbers of the previous quarters are more or less in your head. For the production manager though, going through these pages is mental torture, as she does not have the historical context readily available. (Read more about the Curse of Knowledge here)The solution is the opposite of what I preach for bullet point charts: instead of breaking up slides into multiple pages, condense lots of data in 1 chart, but make it comparable. Put the quarter growth rates of all brands on a page and compare them. List the historical brand growth rates of the past 8 quarters on one page and see what is going on. There is no problem showing a massive amount of data on 1 slide, as long as it is about the same variable that is compared across different dimensions.",01/09/2011,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,en
3786,K-Fold Cross Validation for Deep Learning Models using Keras,The Owl,Siladittya Manna,230.0,4.0,780.0,"with a little help from sklearnMachine Learning models often fails to generalize well on data it has not been trained on. Sometimes, it fails miserably, sometimes it gives somewhat better than miserable performance. To be sure that the model can perform well on unseen data, we use a re-sampling technique, called Cross-Validation.We often follow a simple approach of splitting the data into 3 parts, namely, Train, Validation and Test sets. But this technique does not generally work well for cases when we don’t have a large datasets. When we have limited data, dividing the dataset into Train and Validation sets may casue some data points with useful information to be excluded from the training procedure, and the model fails to learn the data distrubution properly.So, what different do we do in K-Fold cross validation do?K-Fold CV gives a model with less bias compared to other methods. In K-Fold CV, we have a paprameter ‘k’. This parameter decides how many folds the dataset is going to be divided. Every fold gets chance to appears in the training set (k-1) times, which in turn ensures that every observation in the dataset appears in the dataset, thus enabling the model to learn the underlying data distribution better.The value of ‘k’ used is generally between 5 or 10. The value of ‘k’ should not be too low or too high. If the value of ‘k’ is too low (say k = 2), we will have a highly biased model. This case is similar to that of splitting the dataset into training and validation sets, hence the bias will be high and variance low. If the value of ‘k’ is large (say k = n (the number of observations)), then this approach is called Leave One Out CV (LOOCV). In this case, bias will be low but the variance will be high and the model will overfit, resulting in the model to fail in generalizing over the test set.Another approach is to shuffle the dataset just once prior to splitting the dataset into k folds, and then split, such that the ratio of the observations in each class remains the same in each fold. Also the test set does not overlap between consecutive iterations. This approach is called Stratified K-Fold CV. This approach is useful for imbalanced datasets.Importing Required LibrariesHere, we are assuming that all the images in Train set are in the folder train and the labels of corresponding image files are in a csv file, say training_labels.csv, which has two columns, filename and label.The column filename either contains only the name of the image file or the whole path to the image file. In case it does not contain the whole file path, otherwise we have to pass the path to the directory in which the images are stored as the directory argument. If one wants to use the method flow_from_directory, then one have to move image files in and out of the folders for each class for any two of the k folds, k number of times, since this method requires the images belonging to one class are present in respective folders under a single master directory. Hence, we can add the path to the image file names, in the csv files.Read the training_labels.csv file and creating the instancesWe don’t need to create X, because as mentioned in the documentation page for StratifiedKFold , it is sufficient to provide only the labels Y to generate the splits and hence we can put np.zeros(n_samples) instead of X.Create an instance of the ImageDataGenerator classIt can be seen, that the validation_split argument is not given any value as we will be creating the validation set using one of the k splits.We also need to save the best model in each fold.Auxiliary function for getting model name in each of the k iterationsGetting the folds and creating the data generatorsThis piece of code is shown only for K-Fold CV. For Stratified K-Fold CV, just replace kf with skf.create_new_model() function return a model for each of the k iterations. New data generators are created in each iteration as the training data and the validation data changes. checkpoint callback is also created with each iteration because to save the best model ineach iteration, we need to give a different file name for the model file in each iteration, otherwise it will be overwritten by the files in the successive iterations. For evaluating the model in each iteration, the weights of the best model is loaded before the model.evaluate() is run. The performance of the models on the validation set is stored at the end.We can now get the average performance of the model from the list VALIDATION_ACCURACY.REFERENCES :",20/03/2020,2,45.0,42.0,400.0,387.0,2.0,1.0,0.0,6.0,en
3787,"Gotta catch them all, but which one first?",Medium,Knoyd,147.0,4.0,622.0,"For this blog post, we decided to jump on the PokémonGO hype and add a bit of science into the craze. Our goal is to give you the optimal portfolio of Pokémon to train, so you can be as effective as possible against a wide variety of opponents. As each Pokémon has its strengths and weaknesses, we created clusters of Pokémon with similar characteristics and looked at the few selected ones allowing the player to compete against as many different enemies as possible.We used the Pokémon API fan service available on the internet to find all the information about the little creatures.The data we used consists of:The data is available for 811 Pokémon. Although we have done the analysis for all the Pokémon, in this post, we focus only on the first 150 Pokémon as those are the ones that are available in PokémonGO. These datasets were transformed and 457 features were created for each Pokémon.Our goal was to identify the optimal portfolio of Pokémon, which every trainer should have in order to cover almost all the types. This will increase the probability of winning in a random fight (provided that the Pokémon have the same CP). The approach we took was through clustering all of the Pokémon and assigning them to different clusters. Afterwards finding the Pokémon, which are the nearest to the center of each of the clusters to have our portfolio as diverse as possible.Using the dendrogram and the elbow rule we have selected 4 as the optimal number of clusters. This also makes sense as it is a reasonable number of Pokémon, that one can manage to train to a strong level. The output of the K-means clustering can be seen in the graphic below. The clusters are slightly overlapping as the dimension was reduced to 2 using the principal components analysis.Using the unsupervised random forest algorithm we have also explored the features, that create the biggest difference between the Pokémon.The highest importance by far has the feature avg_accuracy — the average accuracy of all moves where accuracy is available. The following features with similar importance areclass_no_physical — number of moves with physical damage, target_avg_pp_all_opponents — average value of power points of moves, which can be used against all opponents, type_avg_pp_normal — average value of power points of moves, which can be used against Pokémon with type normal.Below you can find a couple of options for your optimal portfolio. We explain what makes each cluster of Pokémon unique and give you 5 alternatives with the first being the ideal selection for this cluster and so on.Group of mostly grass, bug and poison Pokémon. This is the group with the most weaknesses but they are doing well against other Pokémon from same kinds. The best choice in this group is Bellsprout.The second group consists of mostly flying, normal and a few fire Pokémon. The representatives of this group are on average very strong against steel and fairy Pokémon. Do not use against psychics. The best candidate to have is Pidgey.Psychic, fairy and electric group. Strong against ice, grass, ghost, fire and fighting Pokémon. They are on average very weak against dragons and poison counterparts. The most valuable Pokémon from this cluster is Clefairy.Mostly water, rock and dragon Pokémon. Strong against fire and normal kinds. Not very effective against flying and fairy types. Shelder is the must have from this group.This is it for our Pokémon analysis. We hope it helps you get a kick-ass portfolio and become a legendary master trainer. So when you head out for the next hunt remember to keep your eyes open for our suggested candidates. You don't wanna end up in the next gym battle unprepared, do you?This article was originally published at: https://www.knoyd.com/blog/pokemongo",08/08/2016,0,5.0,6.0,816.0,646.0,7.0,1.0,0.0,7.0,en
3788,"GloVe, ELMo & BERT",Towards Data Science,Ryan Burke,137.0,10.0,2232.0,"One of the most challenging tasks for machine learning models is finding the best way to to generate numeric representations for words so the model can use that information in its calculations.In computer vision tasks, the red channel in a color (RGB) image will always refer to the red channel, and the green channel to the green channel. Text, however, is heavily based on context, such that the same word can take on multiple meanings depending on its use. Pandas, for example, can refer to cute and fuzzy bears or a Python data analysis library.This is further complicated when considering sentences and paragraphs. Consider the following:Pandas are cute and fuzzy. They don’t use Pandas data analysis library because they are bears.Now I realize that is not the beginning of a Pulitzer prize winning story, but the point is to bring your attention the word They in the second sentence. Who are they? To answer this, there has to be a memory component built in that allows the algorithm to generate representations of Pandas (in the 1st sentence) similar to They in the second sentence and dissimilar to Pandas (in the 2nd sentence).Today I am going to go through examples using an open-source natural language processing library, Spark NLP, to perform a text classification task using state-of-the-art algorithms that have made attempts to deal with the challenges (among others) mentioned above.I will be comparing the results from the following modelsBefore we get into our text classification challenge, a little description about Spark NLP:Spark NLP is an open-source natural language processing library, built on top of Apache Spark and Spark ML. It provides an easy API to integrate with ML Pipelines and it is commercially supported by John Snow Labs. Spark NLP’s annotators utilize rule-based algorithms, machine learning and some of them Tensorflow running under the hood to power specific deep learning implementations.I encourage everyone to take a look at Spark NLP’s Medium page and their Github for in depth tutorials on a multitude of NLP tasks.If you have your notebooks open and ready, you can start working with Spark NLP by installing and importing the necessary packages using the code below.Et voilà! Now we are ready to get to work.For today’s challenge, we will be working with the dataset found here. You can also follow along with my notebook at this repo.To begin, you can read the .csv file using the following code which outputs a sample of the dataframe:We’re interested in classifying our text column by target, where 0 = no disaster and 1 = disaster. Using the code below will allow us see the distribution of our target variable.There are 1211 null values, which are not going to be useful for our classification. We can remove them from the dataframe using the following code.Next, we will go through the preprocessing steps that I used transform the raw text.Each step in the Spark NLP pipeline requires certain columns as inputs and produces new columns of a different type as outputs. In this section we will go over the preprocessing pipeline which was consistent throughout this project. See links for a full description of all Spark NLP transformers and annotators including all modifiable parameters.This is the entry point into our pipeline which transforms our raw text into a Document type. This annotator uses the input column text, which contains our tweets, and outputs a new column document. I also chose the cleanup mode shrink to essentially remove the whitespace (new lines, tabs, multiple blank spaces).2. Tokenizer()The tokenizer breaks our sentences down into individual components (e.g., words or expressions). The input to this annotator is the document column and it will output a new column, token. I modified the parameter SplitChars to split tokens joined by a hyphen, and I also modified the parameter ContextChars to remove certain characters from tokens as seen in the code below.3. Normalizer()The normalizer is a text cleaning annotator that accepts input from our token column and outputs and normalized column. The parameter CleanupPatterns was used to remove all punctuation while keeping all alphanumeric characters.4. StopWordsCleaner()The stopwords annotator removes a list of predefined words based on a library. This library is the same one used by Spark ML and scikit-learn, and the complete list can be found here. These words are typically words that are so commonly used that they wouldn’t offer any utility in a text classification problem (e.g., and, am, be, do, for, get, no).5. Lemmatizer()Lemmatization refers to a process of normalizing text with the goal to reduce variability by transforming derivationally similar words to a base form (e.g., writes, wrote, written, writer → write). You’ll notice below that Spark NLP has a pretrained lemma library antbnc which is based on words in the British National Corpus.Below you can see the full code for the preprocessing pipeline. In the final block, the pipeline is defined as the sequence of steps that we have covered that starts with the raw text and finishes with the lemma.Next, we will go through an example to see how raw text is transformed at each step of our preprocessing pipeline. To do so, we will use Spark NLPs LightPipeline briefly described below.LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.Below you can see that fitting the train dataframe, which consisted of over 7,000 tweets, took 15.2 seconds.Suppose we wanted to save this preprocessing model and use it for another task. Below, we create a Spark dataframe using an example text and then transform it using the same pipeline. Although the text was ~1/7000 the size of the training dataframe, it took 1.55 seconds (1/10) of the time.Enter the LightPipeline. As you can see below, at 121 ms, this increased the speed by nearly 13%.Now, to visualize how this preprocessing pipeline will influence our dataframe, we can use the following code.And to make it easier to read, I plugged the results into a table.Just as a note, on line 10 you see <comma>. I had to replace the actual comma when I created a gist because it interpreted this as another column.With that said, we can see how the normalizer removes the punctuation. The stopwords cleaner removed words such as as, she, the, etc… Finally, the lemmatizer converted words to their lemma, such as sat → sit, watching → watch, caught → catch.Up to now, we have cleaned up our dataframe, but it still contains text. In the next section we will look at the various pretrained embeddings used to convert our clean text into a numerical representation.As mentioned above, this section will briefly describe the different embeddings used to convert our cleaned text. It’s beyond the scope of this article to go into great detail into the mechanics of each, but I will include links to the original papers for those who want to look a little deeper.Word embeddings generate multi-dimensional vector representations for words. The goal is to generate similar representations for words with similar meanings. For example, tsunami, hurricane, and tornado may have similar representations, but what about twister? This is an example of the importance of context. Twister could refer to a game or a tornado, but surely game and tornado shouldn’t have similar representations. Let’s take a quick look at how each model creates embeddings.According to the creators, GloVe is:essentially a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaningConsider the following fictitious example of the co-occurence probabilities for our targets disaster and no disaster and four words (wind, damage, burger, driving). To interpret this, a ratio of 1 would mean there is no difference between the numerator, P(k|disaster), and denominator, P(k|no disaster). As values become larger than one, the probability increases that it would be associated with a disaster. Conversely, as values become smaller than one, the probability increases that it would be associated with no disaster.From our example, damage has the strongest association with our target disaster, while burger has the strongest association with the target no disaster. Both wind and driving are close to one, meaning there is no strong association to either target.One limitation of GloVe is that it doesn’t take into account in what context the word is being used. Consider these two sentences:The creators of ELMo found a way to deal with this issue giving rise to contextualized word embeddings. Here, no fixed embeddings are generated. Instead, the entire sentence is read prior to assigning an embedding. Using a bidirectional LSTM, ELMo learns both next and previous words.Both GloVe and ELMo are pretrained on an unsupervised task on a large body of text. A key difference is that with GloVe we are using the learned vectors for some downstream task. With ELMo we are using the learned vectors and the pretrained model as components in some downstream task.If NLP researchers weren’t so fond of the Muppets, I think a better name than BERT would have be Frankenstein because of the way it incorporates so many of the recent breakthroughs. For an extremely detailed and helpful guide, I encourage you to check out this article.BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently — including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al)Although ELMo uses bidirectional LSTMs, the fact that it concatenates the forward and backward models limits the ability of the representation to take advantage of both contexts simultaneously.BERT uses a masked-language objective, meaning that words are randomly hidden and replaced by a mask. Using a transformer, the masked words are then predicted using unmasked words surrounding it (to the left and right).In the next section, we will compare how well the embeddings from these models can be used to classify disaster from no disaster.Before the results are unveiled, I wanted to take a moment to introduce the classifier that was used. ClassifierDL is a generic classifier built by John Snow Labs inside TensorFlow that supports up to 100 classes. Key parameters that I set were:It’s also worth noting that I split the original dataframe into train and validation sets using:Below you can find the entire pipeline which is an extension of the preprocessing pipeline I described above. As you can see, GloVe word embeddings were used as input to another Spark NLP annotator SentenceEmbeddings which were then used for classification using the ClassifierDL.Predictions were then made on the validation set using the code below, and a sample of the predictions are presented in the subsequent table.Of the ten predictions shown, the one error perfectly highlights the limitation of GloVe embeddings. Because of the lack of context, I want some tsunami takeout was classified as a disaster.Using sklearn, we can see how well the GloVe embeddings could classify our text. In fact, with an accuracy of 81%, it did pretty good!2. ELMoNext, let’s see if ELMo embeddings were able to improve classification accuracy. To use the pretrained ELMo embeddings, simply plug the following into the pipeline above as a replacement for the GloVe embeddings.Using all the same code from above, predictions were made on the validation set and are presented below. Here we see that ELMo was able to learn the context of the word tsunami which GloVe missed. Good job ELMo!Despite this, the model accuracy increased by only 1%.3. BERTFinally, let’s see how BERT embeddings performed on our classification task. Just like we did above, simply plug the following into the same spot.Once again, a sample of the predictions for the validation set are presented. Interestingly, we see that BERT did not correctly classify the sentence, Meet Brinco your own personal earthquake and tsunami early warning beacon.I’m going to have to take BERT’s side on this one, however. In my opinion, the tweet is describing a disaster detection system not a disaster.Either way, the end result with BERT embeddings did not improve the accuracy beyond what ELMo achieved.Today we covered several examples of text classification using Spark NLP. We focused on 3 word embedding models, providing step-by-step instructions on how to include them in your own classification task:Although the results didn’t vary a great deal, we saw some examples of how the different embedding models operate. Specifically, we were able to see that GloVe embeddings lacked context. It was unable to differentiate tsunami the restaurant from the actual disaster.This project focused on word embeddings. In a future post, we will explore Spark NLPs sentence embedding options, such as Universal Sentence Encoder (USE) and BERT sentence embeddings.Thanks for reading!NER with BERT in Spark NLPText Classification in Spark NLP with Bert and Universal Sentence EncodersHow to Get Started with SparkNLP in 2 WeeksThe Illustrated TransformerVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)Language Modeling II: ULMFiT and ELMoWord Embedding (Part II) : Intuition and (some) maths to understand end-to-end GloVe model",16/03/2021,0,42.0,49.0,1400.0,937.0,1.0,7.0,0.0,32.0,en
3789,AI Safety and the Scaling Hypothesis,Towards Data Science,Jeremie Harris,93000.0,4.0,399.0,"Editor’s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds.When OpenAI announced the release of their GPT-3 API last year, the tech world was shocked. Here was a language model, trained only to perform a simple autocomplete task, which turned out to be capable of language translation, coding, essay writing, question answering and many other tasks that previously would each have required purpose-built systems.What accounted for GPT-3’s ability to solve these problems? How did it beat state-of-the-art AIs that were purpose-built to solve tasks it was never explicitly trained for? Was it a brilliant new algorithm? Something deeper than deep learning?Well… no. As algorithms go, GPT-3 was relatively simple, and was built using a by-then fairly standard transformer architecture. Instead of a fancy algorithm, the real difference between GPT-3 and everything that came before was size: GPT-3 is a simple-but-massive, 175B-parameter model, about 10X bigger than the next largest AI system.GPT-3 is only the latest in a long line of results that now show that scaling up simple AI techniques can give rise to new behavior, and far greater capabilities. Together, these results have motivated a push toward AI scaling: the pursuit of ever larger AIs, trained with more compute on bigger datasets. But scaling is expensive: by some estimates, GPT-3 cost as much as $5M to train. As a result, only well-resourced companies like Google, OpenAI and Microsoft have been able to experiment with scaled models.That’s a problem for independent AI safety researchers, who want to better understand how advanced AI systems work, and what their most dangerous behaviors might be, but who can’t afford a $5M compute budget. That’s why a recent paper by Andy Jones, an independent researcher specialized in AI scaling, is so promising: Andy’s paper shows that, at least in some contexts, the capabilities of large AI systems can be predicted from those of smaller ones. If the result generalizes, it could give independent researchers the ability to run cheap experiments on small systems, which nonetheless generalize to expensive, scaled AIs like GPT-3. Andy was kind enough to join me for this episode of the podcast.Here were some of my favourite take-homes from the conversation:You can follow Andy on Twitter here, or me on Twitter here.",02/06/2021,0,1.0,6.0,1400.0,787.0,1.0,3.0,0.0,15.0,en
3790,Подводные камни UNet в Unity 5,Medium,Pavel Shestakov,38.0,2.0,447.0,"Итак, вы решили подключить свой локальный пул геймобъектов, прочли документацию на сайте, нашли необходимые методы и решили , что сейчас все заработает. Возможно в вашем случае это действительно будет так, если вы до этого не регистрировали ни одного префаба для спаунинга по сети.Дело в том, что в Unity при спауне геймобъекта приватным методом ClientScene.OnObjectSpawn сначала проверяется наличие объекта в списке зарегистрированных (через инспектор компоненты NetworkManager или напрямую через добавление геймобъектов в словарь NetworkManager.spawnPrefabs), и только если необходимый геймобъект не найден, то идет в работу словарь хендлеров (делегатов SpawnDelegate) для спауна, в который и записывается ваш делегат с помощью методов ClientScene.RegisterSpawnHandler (или аналогичным ClientScene.RegisterPrefab с перегружаемыми делегатами) на основе ключа assetId структуры типа NetworkHash128.Аналогичный словарь существует и для уничтожения предмета на сцене UnSpawnDelegate. Но он вызывается постоянно для любого геймобъекта с действительным assetId.Сам assetId отыскать не сложно. Он назначается автоматически для любого геймобъекта, на котором висит скрипт NetworkIdentity, который можно получить следующим образом:return GetComponent<NetworkIdentity>().assetIdОчень неприятный баг заключается в следующем. Если вы решили написать расширенную реализацию своих сетевых сообщений (универсальную, оптимальную или вы просто любитель быдлописи), то будьте аккуратны с абстрактными классами. Сериализовать нужно явно, только перегрузкой базовых методов Serialize и Deserialize. И данная перегрузка обязательна должна присутствовать в теле класса любого сообщения. Если вы захотите скрыть от посторонних глаз эти методы (например через реализацию своих методов OnMySerialize и OnMyDeserialize) — увы, Unity об этом не узнает никогда. Возможно это связано с постобработкой скриптов и подходом использования Mono/Cecil (который активно интегрирован в процесс компиляции вашей Assembly-CSharp.dll для сетевых сообщений и аннотаций таких как SyncVar и ей аналогичных).Вы сделали класс MyPlayer и унаследовали его от NetworkBehaviour, чтоб использовать все прелести Command и Rpc. Но в какой-то момент поняли, что предлагаемый функционал вам недостаточен и вам просто “кровь-из-жопы” необходимо написать свой сериализатор для этой компоненты. Вы потратили несколько дней и таки замутили оный. Допустим этот объект вообще ничего не делает, но данные внутри него могут изменяться (например мешок с лутом, и сервер каждые 5 мин обновляет его содержимое). В игровом мире он не перемещается/не вращается, его нельзя уничтожить или подобрать. Так вот если вы используете свой сериализатор (а значит вы знакомы с dirtyBits, инкрементной синхронизацией и вот этим вот всем), то данные на клиенте не обновятся, даже если сервере изменились данные.Все дело в том, что ваш NetworkBehaviour “уснул”, точнее у него не изменился syncVarDirtyBits. Да, это те же самые “грязные” биты, только глобально для синхронизации конкретного NetworkBehaviour. Любая компонента, зависимая от NetworkIndentity, например NetworkTransform изменяет значения этих битов при любом изменения одного из своих полей (будь то координаты положения или вращения). Поэтому вам необходимо руками проставлять хотя бы в значение 1u хотя бы вот так:if (networkBehaviour != null && networkBehaviour.GetDirtyBit() == 0 && _dirtyBits != 0)networkBehaviour.SetDirtyBit(1u);где _dirtyBits — это ваши изменения.",17/10/2016,0,29.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,ru
3791,Real-time face recognition: training and deploying on Android using Tensorflow lite — transfer learning,Medium,Saidakbar P,93.0,15.0,3017.0,"For the last couple of weeks, I have been experimenting with mobilenet models for object detection on Android devices. Since I took a Deep learning course in the past semester, I knew that those mobilenet models could be trained for detecting other objects as well. Moreover, available guides such as this object detection tutorial and this Android deployment tutorial rely on the older version of the Tensorflow framework — Tensorflow Mobile, which is being deprecated as of February 2019. Instead, Tensorflow Lite will be the main framework for mobile devices in the future and Lite version has moved from the contribution stage to the core of Tensorflow.Additionally, there are recent articles that manually annotate images for training purposes. However, we will implement opencv DNN for automatic face detection and annotation before we can feed those images to a mobilenet model for transfer learning.What will you learn?Note that other types of object recognition are also possible, but object annotation can be time-consuming. Therefore, I focus on face recognition for automatic preparation of images.For the last few years, the field of Artificial Intelligence (AI) has gained popularity among computer scientists and is attracting more enthusiasts from other fields due to its novelty and how much productivity AI can bring to businesses. Although we have not reached the level of general AI that gained consciousness, scientists from different fields are contributing to its development implementing the latest achievements in technology. The rise of data sharing platforms such as Facebook, Google services and other similar services are fueling the field of AI. Along with such big data availability, we are also witnessing exponential growth in computing power. This brings exciting venues for data exploration. At the time of writing, we already have self-driving cars, machine translators (NLP), Image classifiers that can pass Captcha. There has been a significant development in AI as a research field [1]. Google and Nvidia are supporting the development by realizing open source libraries and hardware support.One of the exciting fields of AI is Computer Vision, specifically, Object Detection. Recognizing objects in real-time relies on a neural network model, which is usually trained in huge datasets and powerful hardware. In this respect, Google has the capacity to train such models with a vast amount of images available on the internet. Fortunately, since Google made their machine learning framework — Tensorflow — publicly available, enthusiasts and experts started experimenting and many neural network models supporting TensorFlow framework were released as open source. The official GitHub webpage of TensorFlow also provides state-of-the-art pre-trained models for object detection. There are models that are specifically designed for running on mobile devices — mobilenet models.If you are reading this article, then I assume you have some programming experience in python, Tensorflow and familiar with deep learning concepts. If you are not familiar with the latter, start with this great tutorial.Since training a neural network, especially, the one that deals with images requires a powerful GPU for computations, we need to train our model in the cloud, not on our laptop. For comparison, one epoch of ssd_mobilenet_v2 for ~3000 images took 6 minutes on my core i7 4th gen laptop whereas Nvidia K80 GPU training took two seconds to complete one epoch.Fortunately, we have free options: Kaggle with 9 hours limit and Google collab with 12 hours limit. I will be personally using Kaggle website because public notebooks can be accessed as a web page and once the training is started (committed), we can close the browser. I recommend opening an account on Kaggle if you don’t have one. On the other hand, Google Collab only works when you keep the browser tab open and with our training that lasts around 9 hours, this is not a good option.My Kaggle notebooks for part 1 (version 2)and part 2 are extensions of this article. They reproduce all the steps in the following two sections to generate a retrained tflite model. The third part only requires Android Studio for preparing and serving the demo app.Before we can start fine-tuning our neural network model, we need to prepare a set of images that has faces of people and show/annotate where the face is. Face detection is a well-researched area and deserves a book of its own. We will not discuss the details of how face detection works here, but if you are curious, check this introduction to face detection. We refer to face detection as annotating faces with boxes as shown below for our model training purposes.When it comes to the annotation of faces with boxes, there are several face detection techniques. Four popular face detection methods for Python and their accuracies are discussed in this OpenCV article. The simplest method is Haar Cascade Face Detector. In part 1 of my Kaggle notebook, I employed Haar Cascades in the beginning. However, the shortcomings of Haar Cascades such as the inability to detect non-frontal faces and clipping chins for annotations set me back from using it for further image annotations. At this point, I turned to OpenCV DNN, a better face detector with full face annotation (without clipping chins) and higher accuracy for non-frontal face detection. The below code returns coordinates of a detected face in the image. Note that we will be dealing with only single person photos so that each photo contains only one specific person. This is also the reason for our automation of face annotation — we want to recognize who the person is later with our model.As we can observe in my notebook, non-frontal face images are also recognized with OpenCV DNN. We will implement this method of annotation for all of our images (later used for training).Once we know how to get coordinates of faces from images, we have to save those coordinates for later use.The above function creates a text file for each person containing coordinates of their faces in images. We have 62 classes (people) and 3023 images in total, each class containing at least 20 images. So, we will obtain 62 text files for the training and the same amount of text files for test datasets. Dividing the LFW dataset into train/test set is also provided in my notebook.At this point, we have photos in LFW/train/ and LFW/test/ folders; labels are in Labels/train/ and Labels/test/ folders.Now we are ready to train our model, right? Not yet! Images and text files should be converted to TFRecord file format first. Why? In short, it is about performance, less storage, and efficient data pipeline (details). The official documentation illustrates how to create a *.tfrecord file considering we have our labels and images. We will modify it for our use case.Make sure you have the object_detection library:For creating a tfrecord file, the above function is reading a *.jpg image and corresponding text file line to that *.jpg. Reading and obtaining a text file line with corresponding face coordinates occurs at txtlines = read_txt(person, photo) . The function for read_txt is:All of our functions are ready. The last step for creating a tfrecord file is to call the above create_tf_example function:Note that the function is also creating object_label.pbtxt and labels.txt. For the former, this is required in the training process for referring to which image corresponds to which id and name. For the latter, labels.txt is used for real-time inference on Android. Furthermore, they should follow the format as specified in the code. To illustrate, object_label.pbtxt has the following format for each item:Finally, our train and test sets will be saved in tfrecord format. Make sure you define labels folder before calling the save_tf() function. In my case, I had:After running this entire notebook as explained above, we will have:These four files are used in the training part of our notebook. Creating tfrecord is the easy part and takes around two minutes to complete in Jupyter Notebook. Now comes the hard part, which took me many hours of debugging and running. The notebook code in Part 2 will take at least 9 hours on Kaggle with K80 GPU for our model to achieve noticeable results.This section may seem overwhelming at first, but if you follow the notebook codes, you should get a working model at the end.Before we start the training process, we need to download Tensorflow object detection library. In Part 2 of my notebook, we access the python command line interface directly from jupyter notebook to perform downloading and installing additional libraries. For example:This line clones the ‘models’ directory to our local directory. Note that, we are piping all the outputs resulting from the command to models.txt instead of showing them in the notebook. We can later refer to that text file if some actions do not finish successfully.After downloading the ‘models’, we need to install protobuf to convert some files from ‘models’ directory into *.py format:Then, change your directory to models/research/ to convert files and set this folder as an environment variable (otherwise, training will fail):The object detection library is ready.We need to prepare the model at this stage. Download ssd_mobilenet_v2_coco and its configuration from Model Zoo (last trained on 2018.03.29).Note: above model uses float datatype for calculations. ssd_mobilenet_v2_coco_quantized is a uint8 version (8-bit unsigned integer) that does not use float (32/64 bits) type for calculations. Therefore, the accuracy of the quantized models are lower but they run faster on mobile devices. Since I am using 20 samples for each person, initial training of the quantized model was inaccurate for the demonstration in this project. Therefore, I went with a float type model. If you have more than 100 images for each person, then you can experiment with quantized models. Version 17 of my notebook is done with a quantized model.After adding them to our Kaggle kernel, we have to change the downloaded *.config file as follows:The model and config file are also ready. Now we can finally start training:Since we are performing these commands within the notebook, make sure there is no space after every equal sign. Otherwise, you will get ‘File Not Found’ error.model_dir should indicate to the output location and the ‘training’ folder should exist. Logs are piped to train.txt if required for debugging. Note that with 8000 steps, training will take around 8–9 hours on Kaggle with a K80 GPU. Therefore, if you think your code will not run successfully at the first try (which happened to me all the time), change the num_steps to 1 to see how the training process goes. You may see several warnings coming from Tensorflow, this is due to deprecated libraries that are being called by the object detection ‘models’. Hopefully, they will also update their object detection library soon.If the training completes successfully, you should see several model.ckpt-******.data-00000-of-00001 where * is the step number in the ‘training’ folder. Those are checkpoints. We will use the last checkpoint with the highest step number to obtain a final model later. Additionally, *.index, *.meta, and checkpoint text files are created, which are needed for creating a frozen graph model or further training to work around Kaggle’s 9-hour limit.Freezing a graph for Tensorflow lite is the process of obtaining a model from the last checkpoint. The frozen graph will contain architecture, variables, and weights of the neural networks.We use export_tflite_ssd_graph.py file from the object detection library here to freeze the model’s graph. config file, *.index, *.meta, and .ckpt files should be in the same directory to freeze the model.After freezing the graph, the above command will produce tflite_graph.pb and tflite_graph.pbtxt. We are interested in tflite_graph.pb since it contains neural network weights for inference.The last step is to convert that tflite_graph.pb to a*.tflite file format. *.tflite is a Tensorflow lite model format that can be directly served with an app on Android without any other modification. The function for converting a frozen graph to a tflite format is already implemented in the Tensorflow core library, so we do not have show the directory of the source code. Instead, we can call tflite_convert as follows:If you have an older version of Tensorflow, you might need to indicate the location of tflite_convert file or update your Tensorflow to the version 1.12.graph_def_file indicates to the frozen graph we obtained from the previous step. For output file, the shown directory must exist before we can perform the above code. Regarding the other parameters, input_arrays are normalized as we saved our image properties in a normalized format in tf_example.py from part 1; output_arrays are coordinates of the predicted face locations; input_shape is the dimensions of our photo: one photo with 300x300 pixels size and 3 RGB colors. Interestingly, this is also a bottleneck for the face detection model — the smaller the face in the photo, the lower the chances of correctly recognizing the person. In fact, we can observe how accurately the model predicts the person’s name by checking the train.txt from our outputs. Average precision (AP) for the medium area is 0.427 and for a large area AP is 0.855 (for IoU between 0.5:0.95 — the percentage of the predicted box overlapping with the actual face location).This means this model with only 300x300 pixels of input can only correctly identify (85.5% of the time) a person’s name if the shown photo is full face, not when the face is a small part of the photo. However, for demonstration purposes, we can accept it for now. I have an idea about how we can work around this by using two models on Android— OpenCV DNN for face detection and one more image classification model from mobilenet trained on face recognition. The result is DNN finds the face in real-time and feeds that cropped full face image to the mobilenet classifier and we get a much higher accuracy even in small areas of the photo. However, this is going to be another project for the future.If you want to see how the model is performing, the function for running an inference using the frozen model is shown at the end of my notebook.Our trained model is ready for deployment as a part of an Android app. From the output section of our notebook we will need 62faces_float.tflite (our trained model) and labels.txt from our previous notebook. These two files are all we need to serve them on an Android app.In this section, I will discuss the quick way of serving the model. You need to have the latest version of Android Studio and download the TF Lite Android App demo. Extract the demo app, then import as a project in Android Studio.When Android Studio asks for updating the wrapper, select ‘yes’.By default, the demo app comes with Speech synthesis, Image Classification, and Object detection apps —three different apps. Moreover, the models for each of the apps are downloaded online automatically, which will make the size of the app around 163 MB. To avoid downloading these models and installing other apps, we have to modify some of the files shown above.First, we have to change download-models.gradle in Gradle Scripts. In the following line remove all the links:So we will have def models = [] .Next, open AndroidManifest.xml from the ‘manifests’ folder and remove two activities (apps):and:After removing these activities, we will only have DetectorActivity, which is the app we need. Other apps will not be installed.Additionally, you can delete SpeechActivity.java and ClassifierActivity.java files from the java folder.Now we need to copy our labels.txt and 62faces_float.tflite to assets folder. From the navigation panel in Android Studio, right-click on ‘assets’ folder and click on ‘Show in Explorer’ which opens the folder with assets and other folders. Copy-paste labels.txt and 62faces_float.tflite inside the assets folder.Reminder: Our class labels start from 1 and 0 is reserved. For this reason, open labels.txt and add ??? to the first line on top. The next following lines are our labels/person names (Edit in Notepad++ to keep the formatting).The last step is to change file names in DetectorActivity.java. From Android Studio, navigate to java folder and open DetectorActivity.java. Change the following three lines:To the following:Remember, our model is not quantized, so we set TF_OD_API_IS_QUANTIZED to false. Make sure model name and labels are correctly indicated.Finally, our demo app is ready to be compiled. First Build->Make Project then Build->Build Bundles->Build APK. After the building process is completed, you can find the app location in Andoid Studio logs.Great! If you have done all the steps correctly, the app should run without issues on your Android device and recognize some faces from our dataset. Note that the model is not very accurate and needs more samples and training for better predictions.During the last two weeks, I have spent most of my time digging in blogs and documentation for Tensorflow. Thanks to the available blogs (though outdated) and great Tensorflow documentation, I was finally able to bring a working app based on my trained model that detects and recognizes faces. Along the way, you, the reader, and I learned how to annotate faces from images, create tfrecords, train an existing model for face detection and deploy on Android. All of this was done without installing tons of apps except for Android Studio and using Kaggle website. Although ssd_mobilenet models are not optimized for detecting faces from a long distance, they can be accurate in close distance face photos. Limitations are obvious for those models: they accept only 300x300 pixel images and face recognition requires higher resolution photos of the faces to be highly accurate. I propose that we continue the development of this facial recognition app by combining two models: OpenCV DNN for instant face detection on the app interface, then feeding this detected face to another neural network classifier, which only accepts the cropped face photo from OpenCV DNN in 300x300. By combining two models, we can use quantized versions to receive the real-time results as fast as the above float model but with much higher accuracy. For the time being, our demo app has proven the capabilities of Tensorflow lite.Thank you for reading!Please, leave your feedback and applauses are welcome!Further reading:What is AI by GURU99, 2019: https://www.guru99.com/artificial-intelligence-tutorial.htmlRecent developments in AI, 2019. Lecture by Lex Fridman — MIT: https://youtu.be/53YvP6gdD7UWhere AI is headed by Karen Hao, 2019: https://www.technologyreview.com/s/612768/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/DARPA perspective on AI: https://www.darpa.mil/about-us/darpa-perspective-on-ai",25/02/2019,11,30.0,7.0,634.0,543.0,6.0,3.0,0.0,53.0,en
3792,Decision Tree in Machine Learning,Towards Data Science,Prince Yadav,101.0,9.0,1000.0,"A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules. Below diagram illustrate the basic flow of decision tree for decision making with labels (Rain(Yes), No Rain(No)).Decision tree is one of the predictive modelling approaches used in statistics, data mining and machine learning.Decision trees are constructed via an algorithmic approach that identifies ways to split a data set based on different conditions. It is one of the most widely used and practical methods for supervised learning. Decision Trees are a non-parametric supervised learning method used for both classification and regression tasks.Tree models where the target variable can take a discrete set of values are called classification trees. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Classification And Regression Tree (CART) is general term for this.Throughout this post i will try to explain using the examples.Data comes in records of forms.The dependent variable, Y, is the target variable that we are trying to understand, classify or generalize. The vector x is composed of the features, x1, x2, x3 etc., that are used for that task.ExampleWhile making decision tree, at each node of tree we ask different type of questions. Based on the asked question we will calculate the information gain corresponding to it.Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the purest daughter nodes. A commonly used measure of purity is called information. For each node of the tree, the information value measures how much information a feature gives us about the class. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes are pure, or until the information gain is 0.Lets try querying questions and its outputs.Now we will try to Partition the dataset based on asked question. Data will be divided into two classes at each steps.Algorithm for constructing decision tree usually works top-down, by choosing a variable at each step that best splits the set of items. Different algorithms use different metrices for measuring best.First let’s understand the meaning of Pure and Impure.Pure means, in a selected sample of dataset all data belongs to same class (PURE).Impure means, data is mixture of different classes.Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set.If our dataset is Pure then likelihood of incorrect classification is 0. If our sample is mixture of different classes then likelihood of incorrect classification will be high.Calculating Gini Impurity.ExampleCode for Above StepsNow build the Decision tree based on step discussed above recursively at each node.Let’s build decision tree based on training data.OutputFrom above output we can see that at each steps data is divided into True and False rows. This process keep repeated until we reach leaf node where information gain is 0 and further split of data is not possible as nodes are Pure.Advantage of Decision TreeDisadvantage of Decision TreeOverfitting is one of the major problem for every model in machine learning. If model is overfitted it will poorly generalized to new samples. To avoid decision tree from overfitting we remove the branches that make use of features having low importance. This method is called as Pruning or post-pruning. This way we will reduce the complexity of tree, and hence imroves predictive accuracy by the reduction of overfitting.Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set. There are 2 major Pruning techniques.An alternative method to prevent overfitting is to try and stop the tree-building process early, before it produces leaves with very small samples. This heuristic is known as early stopping but is also sometimes known as pre-pruning decision trees.At each stage of splitting the tree, we check the cross-validation error. If the error does not decrease significantly enough then we stop. Early stopping may underfit by stopping too early. The current split may be of little benefit, but having made it, subsequent splits more significantly reduce the error.Early stopping and pruning can be used together, separately, or not at all. Post pruning decision trees is more mathematically rigorous, finding a tree at least as good as early stopping. Early stopping is a quick fix heuristic. If used together with pruning, early stopping may save time. After all, why build a tree only to prune it back again?Suppose you need to select a flight for your next travel. How do we go about it? We check first if the flight is available on that day or not. If it is not available, we will look for some other date but if it is available then we look for may be the duration of the flight. If we want to have only direct flights then we look whether the price of that flight is in your pre-defined budget or not. If it is too expensive, we look at some other flights else we book it!There are many more application of decision tree in real life. You can check this and this for more applications of decision tree.From this article i tried to explains basics of decision tree and how basically it works. You can find the source code used in this article at github.Hope you liked this article. For any changes, suggestion please message me directly on this article or on LinkedIn. Happy Learning — Cheers :)",14/11/2018,11,22.0,3.0,1087.0,992.0,3.0,6.0,0.0,7.0,en
3793,DeepMind’s Latest A.I. Health Breakthrough Has Some Problems,OneZero,Julia Powles,1200.0,8.0,0.0,"Google-affiliated artificial intelligence firm DeepMind has been pushing into the healthcare sector for some time. Last week the London-based company synchronized the release of a set of new research articles — one with the U.S. Department of Veterans Affairs, and three with a North London hospital trust known as the Royal Free.In one paper, published in the journal Nature, with co-authors from Veterans Affairs and University College London, DeepMind claimed its biggest healthcare breakthrough to date: that artificial intelligence (A.I.) can predict acute kidney injury (AKI) up to two days before it happens.AKI — which occurs when the kidneys suddenly stop functioning, leading to a dangerous buildup of toxins in the bloodstream — is alarmingly common among hospital patients in serious care, and contributes to hundreds of thousands of deaths in the United States each year. DeepMind’s bet is that if it can successfully predict which patients are likely to develop AKI well in advance, then doctors could stop or reverse its progression much more easily, saving lives along the way.Beyond the headlines and the hope in the DeepMind papers, however, are three sober facts.First, nothing has actually been predicted — and certainly not before it happens. Rather, what has happened is that DeepMind has taken a windfall dataset of historic incidents of kidney injury in American veterans, plus around 9,000 data-points for each person in the set, and has used a neural network to figure out a pattern between the two.Second, that predictive pattern only works some of the time. The accuracy rate is 55.8% overall, with a much lower rate the earlier the prediction is made, and the system generates two false positives for every accurate prediction.Third, and most strikingly of all: the study was conducted almost exclusively on men — or rather, a dataset of veterans that is 93.6% male. Given the A.I. field’s crisis around lack of diversity and amplification of bias and discrimination, that fact is very important — and astonishingly understated.A DeepMind spokesperson responded to this point by stating “the dataset is representative of the VA population and, as with all deep learning models, it would need further, representative data before being used more widely.” But this depoliticizes how DeepMind has cast the study and its results — not as a tool for potential use with American veterans, or even as a tool that provides indicative results for men, but as a groundbreaking innovation with a general application.Beyond these very significant deficiencies are a number of missed opportunities in DeepMind’s analysis. Some are foundational.Kidney injury affects 13.4% of the patients in the Veterans Affairs dataset–a rate two-thirds the 20% average the study presents for hospitalized U.S. patients in general. The difference is interesting in such a specific population as veterans, and suggests there may be something relevant in the VA’s patient characteristics as well as doctor’s choices and clinical practice. But the Nature study is presented with such an absence of context and explanation as to how VA clinicians actually detected and sought to prevent kidney injury, or the features of the population and its variability, that this basic and essential information is impossible to interrogate.Similarly, the researchers make no attempt to explain the A.I. model they used. How did it work? Why did they decide to construct the model in the way they did? Why was this a good conceptual fit for this particular dataset, and how effectively could it generalize to a wider population? How did the A.I. address the needs of specific patient types, and what impact would this algorithm have on them?A spokesperson said that DeepMind aimed to justify all of the decisions made in the VA study through supplemental information and a non-peer reviewed protocol paper. However, none of these questions were answered with precision and, as a result, the study offers few meaningful insights into either renal medicine or A.I. prediction. The study is littered with unexplained choices that may have been medically instructive, as well as omissions of details (such as 36 salient features discovered by the deep learning model, and what they mean) and outliers — distractions from a clean model, perhaps, but all representing real patients, at the end of the day.But even if all these missed opportunities and deficiencies were addressed, there’s a much larger narrative to DeepMind’s U.S. health research. And that’s where the remainder of the newly released papers come in.Veterans are far from DeepMind’s first attempt–and, quite plausibly, far from its first choice–in predicting kidney injury. The company has been trying to tackle avoidable patient harm since at least 2015, when it first struck a deal with the Royal Free London NHS Foundation Trust that gave it the fully identified health records of over 1.6 million patients.By 2016, and as a direct result of receiving such a treasure trove of private information, DeepMind was embroiled in a major data scandal over legality. In 2017, the U.K.’s data watchdog ruled that patient rights had been breached in several major respects for what turned out to be DeepMind’s gain. The whole saga caused significant reputational damage that has simmered ever since, and by late 2018, Google moved unilaterally to absorb and rehabilitate DeepMind’s healthcare arm into its own Google Health division–a move that remains incomplete because none of DeepMind’s healthcare partners have agreed to transfer their contracts fully to Google.DeepMind’s work with Veteran Affairs points to a crucial question. Clearly the dataset that DeepMind holds on patients in Britain is three times as large and significantly more diverse than the dataset on U.S. veterans. So why would DeepMind choose to do the research that led to the Nature paper with American veterans instead of Royal Free patients?DeepMind rationalizes the choice as simply one of working with different partners for different projects. But the more plausible answer could be that the legal and reputational risk was perceived to be greater in the U.K., particularly around reusing the controversial Royal Free patient data.Nevertheless, the Veteran Affairs data isn’t absent complication, as a point of comparison. Although individual patients were de-identified in the United States before being transferred to DeepMind, with 9,000 data-points per person, collected across a period of up to 15 years, it remains likely that at least one of the patients would be capable of being reidentified with expert methods. That’s all that’s required to bring processing within the scope of personal data, and therefore the European Union’s General Data Protection Regulation.Two other legacies remain from the initial data scandal between DeepMind and Royal Free. First, despite ongoing privacy concerns implicated by a decidedly dubious legal basis for processing, the full data trove has remained in DeepMind’s possession, with the blessing of the U.K. regulator. (DeepMind analogizes itself to a clinical data storage system, ready to serve up records on each and every Royal Free patient, even if those patients haven’t set foot in the hospital in nearly a decade and have no identifiable need for care.) This demonstrates, as does the gift from Veterans Affairs, that DeepMind is able to gain and maintain access to incredibly valuable datasets in a way not practically realizable and defensible by others. Second, both DeepMind and the Royal Free have been determined to make a good news story of it all, overlooking any inconvenient concerns.It was in this spirit that in early 2017, Royal Free starting pushing the clinical deployment of Streams, DeepMind’s clinical smartphone app built off the back of the data transfer and designed to interface with patient data and tests and to generate push alerts. Importantly, and ironically, Streams is not an A.I. tool. It is driven by standard tests and standard formulae. But DeepMind is an A.I. company, so Streams has always been destined to become an A.I. tool.In a dramatic evaluation in Nature Digital Medicine, the use of Streams is shown to have no detectable beneficial impact on patient outcomes, despite all the fanfare.The other papers coordinated to be released with the Veterans Affairs study therefore make a connection that has been years coming. Simultaneously, if rather shakily and with multiple deficiencies, some kind of A.I. model for predicting kidney injury–even if it is a model for American veteran males–is presented alongside a set of evaluations on Streams as a clinical tool.Those evaluation papers, however, give a decidedly lukewarm impression on Streams. In a dramatic evaluation in Nature Digital Medicine, the use of Streams is shown to have no statistically significant beneficial impact on patient’s clinical outcomes, despite all the fanfare.An associated user study in the Journal of Medical Internet Research (JMIR) — on 19 of the total set of 47 Royal Free clinicians who shared the six iPhones that carried Streams — in fact tells us that the app, for all its promises, creates more work, more anxiety, and probably would require hiring more people to monitor and respond to alerts, often unnecessarily.But finally, an evaluation paper about costs, also in the JMIR, claims that assuming no change in staffing (and therefore simply expecting clinicians to absorb the workload and anxiety identified in the user study), Streams could deliver a mean of less than $2,600 cost savings per patient. Delving into the supplementary file containing data to support the cost estimate, it seems that the control hospital that did not use Streams also saw significant reductions in the major cost contributors to that figure, at several statistically significant levels, in a way that deserves comparison and could challenge the central claim.But to DeepMind, perhaps these are extraneous details, given the kicker of the whole exercise. “We did not include the costs of providing the technology, and therefore, it is not possible to judge whether or not it would be cost saving overall,” states the paper, co-written by DeepMind co-founder Mustafa Suleyman. “Our results suggest that the digitally enabled care pathway would be cost saving, provided provision of the technology costs less than around £1,600 [$1,945] per patient spell.”The authors of the JMIR study about costs could not be reached for comment, but a DeepMind spokesperson emphasized that, although the Nature Digital Medicine study demonstrated no significant improvement in clinical outcomes through the use of Streams, the evaluations detailed improvements in the reliability and speed of AKI recognition, the time frames in which some key treatments and specialist care were delivered, as well as a claimed reduction in healthcare costs.Streams, then, seems typical of DeepMind’s way of working. It offers few overall gains in clinical outcomes, creates anxiety and additional workload for physicians, and was built on the back of deeply controversial access to patients’ data. Whatever Google and DeepMind are planning to do in the United States, they need to overhaul their attitude to the most basic priorities of rights, explanations, and costs to humans, not machines. Those come first, before profit — or rushing to proclaim that A.I. has a central place in the future of medicine.",06/08/2019,0,1.0,0.0,1024.0,693.0,1.0,0.0,0.0,0.0,en
3794,DBSCAN clustering for data shapes k-means can’t handle well (in Python),Towards Data Science,Gabriel Pierobon,324.0,5.0,439.0,"In this post I’d like to take some content from Introduction to Machine Learning with Python by Andreas C. Müller & Sarah Guido and briefly expand on one of the examples provided to showcase some of the strengths of DBSCAN clustering when k-means clustering doesn’t seem to handle the data shape well. I’m going to go right to the point, so I encourage you to read the full content of Chapter 3, starting on page 168 if you would like to expand on this topic. I’ll be quoting the book when describing the working of the algorithm.This is how k-means work in a visual representation:One issue with k-means clustering is that it assumes that all directions are equally important for each cluster. This is usually not a big problem, unless we come across with some oddly shape data.In this example, we will artificially generate that type of data. With the code below, provided by the authors of the book (with some minor changes in number of clusters), we can generate some data that k-means won’t be able to handle correctly:As you can see, we have arguably 5 defined clusters with a stretched diagonal shape.Let’s apply k-means clustering:What we can see here is that k-means has been able to correctly detect the clusters at the middle and bottom, while presenting trouble with the clusters at the top, which are very close to each other. The authors say: “these groups are stretched toward the diagonal. As k-means only considers the distance to the nearest cluster center, it can’t handle this kind of data”Let’s see how DBSCAN clustering can help with this shape:Some highlights about DBSCAN clustering extracted from the book:This is an example of how clustering changes according to the choosing of both parameters:The parameter eps is somewhat more important, as it determines what it means for points to be close. Setting eps to be very small will mean that no points are core samples, and may lead to all points being labeled as noise. Setting eps to be very large will result in all points forming a single cluster.Let’s get back to our example and see how DBSCAN deals with it:After twisting eps and min_samplesfor some time, I got some fairly consistent clusters, still including some noise points.Lastly, considering we created the data points explicitly defining 5 clusters, we can mesure performance using adjusted_rand_score. This is not frequent since in real cases we don’t have cluster labels to begin with (thus our need to apply clustering techinques). Since in this case we do have labels, we can measure performance:There you have it! DBSCAN scores 0.99 while k-means only gets 0.76",30/09/2018,6,5.0,10.0,559.0,383.0,5.0,4.0,0.0,3.0,en
3795,How Does Back-Propagation in Artificial Neural Networks Work?,Towards Data Science,Anas Al-Masri,384.0,10.0,1512.0,"Ever since the world of Machine Learning was introduced to non-linear functions that work recursively (i.e. Artificial Neural Networks), the applications of which boomed noticeably. In this context, proper training of a Neural Network is the most important aspect of making a reliable model. This training is usually associated with the term “Back-propagation”, which is highly vague to most people getting into Deep Learning. Heck, most people in the industry don’t even know how it works — they just know it does!Back-propagation is the essence of neural net training. It is the practice of fine-tuning the weights of a neural net based on the error rate (i.e. loss) obtained in the previous epoch (i.e. iteration). Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization.So how does this process work, with the vast simultaneous mini-executions involved? Let’s learn by example!In order to make this example as subjective as possible, we’re just going to touch on related concepts (e.g. loss functions, optimization functions, etc.) without explaining them, as these topics deserve their own series.Imagine that we have a deep neural network that we need to train. The purpose of training is to build a model that performs the XOR (exclusive OR) functionality with two inputs and three hidden units, such that the training set (truth table) looks something like the following:Moreover, we need an activation function that determines the activation value at every node in the neural net. For simplicity, let’s choose an identity activation function:We also need a hypothesis function that determines what the input to the activation function is. This function is going to be the typical, ever-famous:Let’s also choose the loss function to be the usual cost function of logistic regression, which looks a bit complicated but is actually fairly simple:Furthermore, we’re going to use the Batch Gradient Descent optimization function to determine in what direction we should adjust the weights to get a lower loss than the one we currently have. Finally, the learning rate will be 0.1 and all the weights will be initialized to 1.Let’s finally draw a diagram of our long-awaited neural net. It should look something like this:The leftmost layer is the input layer, which takes X0 as the bias term of value 1, and X1 and X2 as input features. The layer in the middle is the first hidden layer, which also takes a bias term Z0 of value 1. Finally, the output layer has only one output unit D0 whose activation value is the actual output of the model (i.e. h(x)).It is now the time to feed-forward the information from one layer to the next. This goes through two steps that happen at every node/unit in the network:1- Getting the weighted sum of inputs of a particular unit using the h(x) function we defined earlier.2- Plugging the value we get from step 1 into the activation function we have (f(a)=a in this example) and using the activation value we get (i.e. the output of the activation function) as the input feature for the connected nodes in the next layer.Note that units X0, X1, X2 and Z0 do not have any units connected to them and providing inputs. Therefore, the steps mentioned above do not occur in those nodes. However, for the rest of the nodes/units, this is how it all happens throughout the neural net for the first input sample in the training set:and same goes for the rest of the units:As we mentioned earlier, the activation value (z) of the final unit (D0) is that of the whole model. Therefore, our model predicted an output of 1 for the set of inputs {0, 0}. Calculating the loss/cost of the current iteration would follow:The actual_y value comes from the training set, while the predicted_y value is what our model yielded. So the cost at this iteration is equal to -4.According to our example, we now have a model that does not give accurate predictions (it gave us the value 4 instead of 1) and that is attributed to the fact that its weights have not been tuned yet (they are all equal to 1). We also have the loss, that is equal to -4. Back-propagation is all about feeding this loss backwards in such a way that we can fine-tune the weights based on which. The optimization function (Gradient Descent in our example) will help us find the weights that will — hopefully — yield a smaller loss in the next iteration. So let’s get to it!If feeding forward happened using the following functions:Then feeding backward will happen through the partial derivatives of those functions. There is no need to go through the working of arriving at these derivatives. All we need to know is that the above functions will follow:where Z is just the z value we obtained from the activation function calculations in the feed-forward step, while delta is the loss of the unit in the layer.I know it’s a lot of information to absorb in one sitting, but I suggest you take your time and really understand what is going on at every step before going further.Now we need to find the loss at every unit/node in the neural net. Why is that? Well, think about it this way, every loss the the deep learning model arrives to is actually the mess that was caused by all the nodes accumulated into one number. Therefore, we need to find out which node is responsible for most of the loss in every layer, so that we can penalize it in a sense by giving it a smaller weight value and thus lessening the total loss of the model.Calculating the delta of every unit can be problematic. However, thanks to Mr. Andrew Ng, he gave us the shortcut formula for the whole thing:where values delta_0, w and f’(z) are those of the same unit’s, while delta_1 is the loss of the unit on the other side of the weighted link. For example:You can think of it this way, in order to get the loss of a node (e.g. Z0), we multiply the value of its corresponding f’(z) by the loss of the node it is connected to in the next layer (delta_1), by the weight of the link connecting both nodes.This is exactly how back-propagation works. We do the delta calculation step at every unit, back-propagating the loss into the neural net, and finding out what loss every node/unit is responsible for.Let’s calculate those deltas and get it over with!There are a few things to notice here:All that is left now is to update all the weights we have in the neural net. This follows the Batch Gradient Descent formula:Where W is the weight at hand, alpha is the learning rate (i.e. 0.1 in our example) and J’(W) is the partial derivative of the cost function J(W) with respect to W. Again, there’s no need for us to get into the math. Therefore, let’s use Mr. Andrew Ng’s partial derivative of the function:Where Z is the Z value obtained through forward-propagation, and delta is the loss at the unit on the other end of the weighted link:Now we use the Batch Gradient Descent weight update on all the weights, utilizing our partial derivative values that we obtain at every step. It is worth emphasizing on that the Z values of the input nodes (X0, X1, and X2) are equal to 1, 0, 0, respectively. The 1 is the value of the bias unit, while the zeroes are actually the feature input values coming from the data set. One last note is that there is no particular order to updating the weights. You can update them in any order you want, as long as you don’t make the mistake of updating any weight twice in the same iteration.In order to calculate the new weights, let’s give the links in our neural nets names:New weight calculations will happen as follows:It is important to note here that the model is not trained properly yet, as we only back-propagated through one sample from the training set. Doing all we did all over again for all the samples will yield a model with better accuracy as we go, trying to get closer to the minimum loss/cost at every step.It might not make sense to you that all the weights have the same value again. However, training the model on different samples over and over again will result in nodes having different weights based on their contributions to the total loss.The theory behind Machine Learning can be really difficult to grasp if not tackled the right way. One example of this would be Back-propagation, whose effectiveness is visible in most real-world Deep Learning applications, but it is never examined. Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa.",30/01/2019,13,11.0,3.0,882.0,464.0,7.0,1.0,0.0,2.0,en
3796,"Tesla’s Big Plans, DeepMind Pays For Itself, Internet Drones, and Moore’s Law",Emergent // Future,Matt Kiser,2300.0,4.0,494.0,"Issue 17 This week we review Elon Musk’s big plans for Tesla, how Google uses DeepMind to save millions of dollars, why Zuckerberg is building a fleet of internet drones, and check in on Moore’s Law death watch. Plus, projects to try at home, and our top reads from the past week.Not a subscriber? Join the Emergent // Future newsletter here.You might have heard: Elon Musk outlined his masterplan for Tesla in blog post. For the past 10-years, Tesla’s vision had been to do:Now, Musk is doubling-down on solar power, Tesla trucks, self-driving cars, and car-sharing — he wants your car to make you money when you aren’t using it. The company has already started developing electric and autonomous trucks and buses.tl;dr “We’re not an electric car company; we’re a futuristic logistics company and manufacturer.” h/t FusionBut did you know: Mercedes is testing CityPilot, their semi-autonomous bus program in Amsterdam? The bus recently completed a 12-mile test trip that connected Amsterdam’s Schiphol airport with the nearby town of Haarlem.Along the route, the bus, which is fully networked and can communicate with traffic lights and other city infrastructure, had to stop at traffic lights, pass through tunnels, and navigate pedestrians.ALSO: Why GM Is Holding Back Its Bleeding-Edge Tech For Autonomous VehiclesGoogle’s latest DeepMind experiment has improved the power usage efficiency in their data centers by 15%, and cut the cost used for cooling by 40%.Compared to five years ago, Google now gets 3.5 times the computing power out of the same amount of energy.Essentially, the company Google acquired in 2014 for more $600 million, is now paying for itself. Now that’s electric.Facebook just completed the first test flight of their internet drone, Aquila. The drone is part of Zuckerberg’s plan to bring the internet to all 7 billion people on Earth by launching high-altitude, solar-powered drones that beam internet access to the ground.At cruising altitude, Aquila used 2,000 watts of energy — the equivalent output of five strong cyclists. We wonder: could DeepMind improve upon that?Facebook HQ says 60% of the global population doesn’t have internet access. And, as many as 1.6 billion of those unconnected live in remote locations with no access to mobile broadband networks.By 2021 Moore’s Law will be dead. That’s the word from the Semiconductor Industry Association, which says that transistors will stop shrinking.All is not lost, however. Processors could continue to fulfill Moore’s Law by increasing in vertical density.Meanwhile, Nvidia unveiled their new flagship graphics card: the $1,200 Titan X with 12GB GDDR5X memory.The company claims it’s 60% faster than previous Titan X. It’s no surprise that Nvidia calls their new processor: “The Ultimate. Period.”To put Moore’s Law in perspective: 69 years ago, scientists built the first transistor. Today, there’s a graphics card with 12 billion of them.Emergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.Originally published at blog.algorithmia.com on July 27, 2016.",27/07/2016,0,16.0,28.0,1200.0,630.0,1.0,3.0,0.0,31.0,en
3797,Transfer learning from pre-trained models,Towards Data Science,Pedro Marcelino,320.0,14.0,1810.0,"This article teaches you how to use transfer learning to solve image classification problems. A practical example using Keras and its pre-trained models is given for demonstration purposes.Deep learning is fast becoming a key instrument in artificial intelligence applications (LeCun et al. 2015). For example, in areas such as computer vision, natural language processing, and speech recognition, deep learning has been producing remarkable results. Therefore, there is a growing interest in deep learning.One of the problems where deep learning excels is image classification (Rawat & Wang 2017). The goal in image classification is to classify a specific picture according to a set of possible categories. A classic example of image classification is the identification of cats and dogs in a set of pictures (e.g. Dogs vs. Cats Kaggle Competition).From a deep learning perspective, the image classification problem can be solved through transfer learning. Actually, several state-of-the-art results in image classification are based on transfer learning solutions (Krizhevsky et al. 2012, Simonyan & Zisserman 2014, He et al. 2016). A comprehensive review on transfer learning is provided by Pan & Yang (2010).This article shows how to implement a transfer learning solution for image classification problems. The implementation proposed in this article is based on Keras (Chollet 2015), which uses the programming language Python. Following this implementation, you will be able to solve any image classification problem quickly and easily.The article has been organised in the following way:Transfer learning is a popular method in computer vision because it allows us to build accurate models in a timesaving way (Rawat & Wang 2017). With transfer learning, instead of starting the learning process from scratch, you start from patterns that have been learned when solving a different problem. This way you leverage previous learnings and avoid starting from scratch. Take it as the deep learning version of Chartres’ expression ‘standing on the shoulder of giants’.In computer vision, transfer learning is usually expressed through the use of pre-trained models. A pre-trained model is a model that was trained on a large benchmark dataset to solve a problem similar to the one that we want to solve. Accordingly, due to the computational cost of training such models, it is common practice to import and use models from published literature (e.g. VGG, Inception, MobileNet). A comprehensive review of pre-trained models’ performance on computer vision problems using data from the ImageNet (Deng et al. 2009) challenge is presented by Canziani et al. (2016).Several pre-trained models used in transfer learning are based on large convolutional neural networks (CNN) (Voulodimos et al. 2018). In general, CNN was shown to excel in a wide range of computer vision tasks (Bengio 2009). Its high performance and its easiness in training are two of the main factors driving the popularity of CNN over the last years.A typical CNN has two parts:Figure 1 shows the architecture of a model based on CNN. Note that this is a simplified version, which fits the purposes of this text. In fact, the architecture of this type of model is more complex than what we suggest here.One important aspect of these deep learning models is that they can automatically learn hierarchical feature representations. This means that features computed by the first layer are general and can be reused in different problem domains, while features computed by the last layer are specific and depend on the chosen dataset and task. According to Yosinski et al. (2014), ‘if first-layer features are general and last-layer features are specific, then there must be a transition from general to specific somewhere in the network’. As a result, the convolutional base of our CNN — especially its lower layers (those who are closer to the inputs) — refer to general features, whereas the classifier part, and some of the higher layers of the convolutional base, refer to specialised features.When you’re repurposing a pre-trained model for your own needs, you start by removing the original classifier, then you add a new classifier that fits your purposes, and finally you have to fine-tune your model according to one of three strategies:Figure 2 presents these three strategies in a schematic way.Unlike Strategy 3, whose application is straightforward, Strategy 1 and Strategy 2 require you to be careful with the learning rate used in the convolutional part. The learning rate is a hyper-parameter that controls how much you adjust the weights of your network. When you’re using a pre-trained model based on CNN, it’s smart to use a small learning rate because high learning rates increase the risk of losing previous knowledge. Assuming that the pre-trained model has been well trained, which is a fair assumption, keeping a small learning rate will ensure that you don’t distort the CNN weights too soon and too much.From a practical perspective, the entire transfer learning process can be summarised as follows:As mentioned before, models for image classification that result from a transfer learning approach based on pre-trained convolutional neural networks are usually composed of two parts:Since in this section we focus on the classifier part, we must start by saying that different approaches can be followed to build the classifier. Some of the most popular are:In this example, we will see how each of these classifiers can be implemented in a transfer learning solution for image classification. According to Rawat and Wang (2017), ‘comparing the performance of different classifiers on top of deep convolutional neural networks still requires further investigation and thus makes for an interesting research direction’. So it will be interesting to see how each classifier performs in a standard image classification problem.You can find the full code of this example on my GitHub page.In this example, we will use a smaller version of the original dataset. This will allow us to run the models faster, which is great for people who have limited computational power (like me).To build a smaller version of the dataset, we can adapt the code provided by Chollet (2017) as shown in Code 1.The convolutional base will be used to extract features. These features will feed the classifiers that we want to train so that we can identify if images have dogs or cats.Once again, the code provided by Chollet (2017) is adapted. Code 2 shows the code used.The first solution that we present is based on fully-connected layers. This classifier adds a stack of fully-connected layers that is fed by the features extracted from the convolutional base.To keep it simple (and fast), we will use the solution proposed by Chollet (2018) with slight modifications. In particular, we will use the Adam optimizer instead of the RMSProp because Stanford says so (what a beautiful argumentum ad verecundiam).Code 3 shows the code used, while Figures 5 and 6 present the learning curves.Brief discussion of results:The difference between this case and the previous one is that, instead of adding a stack of fully-connected layers, we will add a global average pooling layer and feed its output into a sigmoid activated layer.Note that we are talking about a sigmoid activated layer instead of a softmax one, which is what is recommended by Lin et al. (2013). We are changing to the sigmoid activation because in Keras, to perform binary classification, you should use sigmoid activation and binary_crossentropy as the loss (Chollet 2017). Therefore, it was necessary to do this small modification to the original proposal of Lin et al. (2013).Code 4 shows the code to build the classifier. Figure 7 and 8 show the resulting learning curves.Brief discussion of results:In this case, we will train a linear support vector machines (SVM) classifier on the features extracted by the convolutional base.To train this classifier, a traditional machine learning approach is preferable. Consequently, we will use k-fold cross-validation to estimate the error of the classifier. Since k-fold cross-validation will be used, we can concatenate the train and the validation sets to enlarge our training data (we keep the test set untouched, as we did in the previous cases). Code 5 shows how data was concatenated.Finally, we must be aware that the SVM classifier has one hyperparameter. This hyperparameter is the penalty parameter C of the error term. To optimize the choice of this hyperparameter, we will use exhaustive grid search. Code 6 presents the code used to build this classifier, while Figure 9 illustrates the learning curves.Brief discussion of results:In this article, we:I hope that you feel motivated to start developing your deep learning projects on computer vision. This is a great field of study and new exciting findings are coming out everyday.I’d be glad to help you, so let me know if you have any questions or improvement suggestions!1. Bengio, Y., 2009. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1), pp.1–127.2. Canziani, A., Paszke, A. and Culurciello, E., 2016. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678.3. Chollet, F., 2015. Keras.4. Chollet, F., 2017. Deep learning with python. Manning Publications Co..5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 248–255). Ieee.6. He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770–778).7. Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097–1105).8. LeCun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. nature, 521(7553), p.436.9. Lin, M., Chen, Q. and Yan, S., 2013. Network in network. arXiv preprint arXiv:1312.4400.10. Pan, S.J. and Yang, Q., 2010. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10), pp.1345–1359.11. Rawat, W. and Wang, Z., 2017. Deep convolutional neural networks for image classification: A comprehensive review. Neural computation, 29(9), pp.2352–2449.12. Simonyan, K. and Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.13. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. and Wojna, Z., 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818–2826).14. Tang, Y., 2013. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239.15. Voulodimos, A., Doulamis, N., Doulamis, A. and Protopapadakis, E., 2018. Deep learning for computer vision: A brief review. Computational intelligence and neuroscience, 2018.16. Yosinski, J., Clune, J., Bengio, Y. and Lipson, H., 2014. How transferable are features in deep neural networks?. In Advances in neural information processing systems (pp. 3320–3328).17. Zeiler, M.D. and Fergus, R., 2014, September. Visualizing and understanding convolutional networks. In European conference on computer vision (pp. 818–833). Springer, Cham.Thanks to João Coelho for reading drafts of this.You can find more about me and my projects at pmarcelino.com. Also, you can sign up for my newsletter to receive my latest updates on Humans, Machines, and Science.",23/10/2018,0,40.0,16.0,749.0,583.0,10.0,11.0,0.0,16.0,en
3798,Serving PyTorch Models on AWS Lambda with Caffe2 & ONNX,Medium,michaelulin,60.0,10.0,1449.0,"Code available here: https://github.com/michaelulin/pytorch-caffe2-aws-lambdaHaving worked with PyTorch, I love the flexibility and ease of development of the framework versus other platforms. As PyTorch is still early in its development, I was unable to find good resources on serving trained PyTorch models, so I’ve written up a method here that utilizes ONNX, Caffe2 and AWS Lambda to serve predictions from a trained PyTorch model. I hope that you find it to be useful.How to effectively deploy a trained PyTorch modelUsing ONNX, Facebook and Microsoft’s recently released platform for Neural Network interoperability, we can convert a model trained in PyTorch to Caffe2 and then serve predictions with that model from AWS Lambda.ONNX enables models trained in PyTorch to be used in Caffe2 (and vice versa). Eventually the framework will support Microsoft’s CNTK framework, but as of October 2017, the support hasn’t been released yet.AWS Lambda is AWS’s serverless platform. After converting our PyTorch model to Caffe2, we can serve predictions from AWS Lambda, which makes it easy to scale and serve predictions via an API. AWS Lambda has a number of limitations that we have to work with (including limiting all files and code to a 50mb zip file).The first step is to train and save a PyTorch model that you want to serve predictions from. For this example, we can just use one of the pretrained models that’s included with torchvision. For a good tutorial on training a PyTorch model, see the PyTorch site here: http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.htmlAfter you’ve trained your model, save it so that we can convert it to an ONNX format for use with Caffe2.Next, we’ll need to set up an environment to convert PyTorch models into the ONNX format. We’ll need to install PyTorch, Caffe2, ONNX and ONNX-Caffe2. I strongly recommend just using one of the docker images from ONNX. It has everything you need already set up and makes it very simple to execute the script below. The ONNX docker image is available here: https://github.com/onnx/onnxI won’t go into setting up everything on your own here, but if you’re feeling up to it, there are a couple things that I would note:Once you have everything set up, either through docker (highly recommended) or through your own set up, save your pre-trained model from the previous step somewhere you can access it and run the following script.This loads the model into PyTorch, converts the model to an ONNX format, tests loading the model via ONNX-Caffe2 and tests whether the output from the converted Caffe2 model matches the PyTorch model.For a more complete tutorial on converting models from PyTorch to Caffe2, see the PyTorch website: http://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.htmlAfter you have the .proto file and upload it to S3, we can start serving predictions using AWS Lambda.Once you have your model converted to ONNX, we can set up our AWS Lambda function to serve predictions with it. The current limitations of AWS Lambda require that we fit all of our code and the necessary libraries to run into a 50mb or smaller zip file. This is why we’re using Caffe2 rather than PyTorch to serve our predictions (due to its much smaller footprint).PyTorch takes up over 300mb of disk space, but we can fit Caffe2 and all of the necessary libraries into a 38mb zip file.If you want, you can download the deps.zip file from the github repo, add your own python script and run it on AWS Lambda. You can find that here: https://github.com/michaelulin/pytorch-caffe2-aws-lambdaTo create your own zip file for AWS Lambda, launch a new EC2 instance based on the AWS Lambda AMI located here: http://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.htmlThis is the base image that your Lambda function will run on, so you can test how the Lambda function will work and what libraries/packages are available.Once that’s up, run the following script to install all the necessary packages and libraries on the instance, install Caffe2 and add it all to a zip file.After creating the zip file, you just need to add the python script that will be run via AWS Lambda. You can use the following test script. The script will download the trained ONNX model from S3 and save it to the /tmp space available to the Lambda function. 500mb of tmp space is available, so we should be able to load most trained models.This script loads the pre-trained ONNX model, loads it into Caffe2 and runs a test prediction. The handler function is the one that will be run by AWS Lambda. Inputs will be available from the event variable. In this example, we generate a random numpy array with the same size as the input, run it through the model and return the prediction as a string. In a real deployment, you could fetch the input from the event variable.Save this script and add it to the deps.zip file (or whatever your zip file is called). In this example, I’ve added the above script (test.py) to the deps.zip file via the following command:zip -9 -q -r ~/deps.zip test.pyOnce you’ve added your script to the zip file, save the zip file to S3 or your local computer so that you can upload it to AWS Lambda.Here’s how to deploy the Lambda function via the AWS management consoleNavigate to the Lambda from the management console then click on create function.When selecting the blueprint, select “Author from Scratch”.On the next step, name your function and then select a role. For this example, you’ll need to select or create a role that has the ability to read from the S3 bucket where your ONNX model is saved as well as the ability to create logs and log events (for writing the AWS Lambda logs to Cloudwatch). The following IAM policy should work, but you may need to modify it for your purposes.{ “Version”: “2012–10–17”, “Statement”: [ { “Effect”: “Allow”, “Action”: [ “logs:CreateLogGroup”, “logs:CreateLogStream”, “logs:PutLogEvents”], “Resource”: “arn:aws:logs:*:*:*” }, { “Effect”: “Allow”, “Action”: [ “s3:GetObject” ], “Resource”: “arn:aws:s3:::*” } ]}After setting up the role, upload the function code either by uploading the zip file from S3 or from your computer. Select the Python 2.7 runtime, and change the name of the Handler to your function.Finally, configure a test event and save the function. For this example, it doesn’t matter what the test event is as the function does not use information from the event.That’s it. The test event should run and you should see an output like the one below.Finally, you’ll likely want to be able to trigger your Lambda function via API. AWS API Gateway makes it easy to enable this functionality. To set it up, click on triggers and then Add Trigger. Click on the empty box and select API Gateway from the dropdown menu.Once selected, give your API a name. In this example, it’s Test. Enter a value for the deployment stage and configure security. You can leave the API open so that anyone with the url can use it. I recommend that you secure it with an API key or IAM access. Here I’ll set up the API with a key. So, select Open with access key for the security option.Once that’s setup, navigate to the API Gateway console. On the left side of the page, select the Stages tab and then select your deployment stage (in this case “prod”). This will display your Invoke URL for calling your API.The default setup is to call your API from any method (GET, POST, etc.). In this example, we’ll stick with the default.After you’ve noted this, select Usage Plans on the left and create a new usage plan. You can select the settings for throttling this API key or setting a quota.Next, add your API and stage to the usage plan.Finally, you can create a new API key and associate it with the Usage Plan and API or use a pre-existing API key for this API.Once you’ve set that up, you’re done. You can now serve predictions via API. The code below demonstrates how to call the API via the requests library in Python. Since the API Gateway is setup to accept any method, you can call the API with any method. Here I’ll use the POST method. The url is the invoke url followed by the stage name, “prod”, followed by the resource name “test”.You can pass the API key in the headers by providing a value to the ‘x-api-key’ key and you can pass values to the event variable in the lambda function via the json= variable. The event variable in the lambda function is a dictionary with the values passed here available to the function via the ‘body’ key.That’s it. I hope you’ve found this to be helpful. Please let me know if you have any feedback. Good luck with your Lambda functions!",09/10/2017,5,57.0,31.0,839.0,331.0,10.0,1.0,0.0,8.0,fr
3799,[談理解] 電競賽評也能告訴我們如何設計智慧系統的解釋機制？,人機共生你我它,Chi-Lan Yang | 楊期蘭,209.0,7.0,80.0,"「蟲苔已經撲到人家的臉上了！」 「快要滿人口啦！應該要開戰了喔，因為其實人口滿，你剛剛把人家斷炊，這邊是一個很好的時機點可以來壓制」電競賽評每天在做的事就是分析許多專業玩家打game的過程，帶著觀眾理解這些專業玩家每一步背後的策略，仔細想想，這些賽評帶領觀眾理解專業玩家的方式，是不是也跟使用者透過一個解釋機制來理解黑盒子般的智慧系統類似？電競賽評是專家行為的詮釋者，從他們身上，能帶給我們什麼智慧代理系統設計的啟發？來自美國Oregon State University的研究團隊發現了這個關聯，透過分析賽評們對於電競的即時評論，試圖了解：當解釋機制（賽評）在說明智慧系統運作（專業玩家動作）時，需要哪些線索來搞懂智慧系統的行為、對使用者說明時需要包含哪些資訊、以及要怎麼說出這些難懂的資訊才能幫助使用者搞懂智慧系統這個黑盒子。在眾多線索中，哪些資訊才是賽評需要的呢？研究者分析賽評切換的畫面，發現遊戲賽評會不斷的蒐集玩家當下的表現、所處的環境、產能狀況或統計資料（例：擊殺比例）以及賽評不斷切換視角（例：畫面轉到不同地點、切換成不同玩家的視角）來幫助自己解釋這些玩家為什麼在此時此刻會做出特定的行為。透過分析賽評如何理解專業玩家，我們可以知道當設計解釋機制的時候，需要想辦法讓使用者需要知道系統已做、能做哪些事，就如同賽評會說出「蟲苔已經撲到人家的臉上了」或告訴觀眾「滿人口應該就可以開戰了」，藉由這些資訊來讓觀眾理解玩家做出特定行為的意圖。除此之外，智慧系統的解釋機制也需要告訴使用者現在系統已經看到、聽到或取得哪些資訊，以自駕車來說，在操作面板上對駕駛顯示目前系統偵測到周圍環境哪些資訊、已經分別執行過哪些步驟，幫助駕駛理解系統做決策的過程；或是讓使用者知道智慧系統做了哪些事、能做哪些事，例如透過系統協助保安人員判斷某位入境者是否為非法移民時，智慧測謊系統需要對決策者顯示目前它根據哪些不同面向的資訊得到某個判斷、呈現某個分析結果潛在的誤差來源可能是什麼等，藉此幫助決策者判斷應該怎麼參考測謊系統的分析結果。研究者們更進一步分析這些賽評們在評論比賽時說的話，企圖了解賽評怎麼傳遞出這些了解專業玩家的「解釋」，進而讓觀眾理解專業玩家們的一舉一動，只要了解這個之後，我們就能知道智慧系統的解釋裡面需要包含哪些成分，才能夠滿足使用者的好奇心。分析結果發現，在激戰模式的時間限制下，賽評會向觀眾解釋玩家做了什麼（“他正朝牆壁走過去”）、接下來可能會發生什麼（“看起來準備要追殺了”）、玩家們如何做出某個舉動（“他嘗試向對手反擊”）以及評價玩家的某個決定（“他這個打法最大的缺點就是太慢”）。當遊戲進入激戰時，短時間內往往會有好幾個事件接續發生，這時候賽評們不一定能給出最精確的解釋，但卻會提供以上這四類解釋來幫助觀眾了解玩家為什麼做出某些行為。我們把這些專業玩家當作智慧系統，而把賽評們當作智慧系統的解釋機制，從上面的分析結果可以知道，當智慧系統在短時間內需要提供解釋時：「系統做了什麼」、「系統接下來可能會做什麼」、「系統怎麼產生這個結果」、「分析系統某個決定的優劣」，這些內容能夠幫助使用者在時間、資源有限的情況下對於系統行為有大致的理解。最後，研究者們想知道賽評們用什麼方式說出這些解釋來幫助觀眾理解玩家行為，進而讓我們了解系統的解釋應該要怎麼設計才能被使用者看懂。分析結果發現，在有時間限制的情況下，如果想要有效率的描述專家行為，這些解釋應該包含一個主詞或動作搭配三種性質的描述：主詞/動作+「空間」特性，像是距離遠近、區域、大小、配置等（“哦門口只留一隻”）、主詞/動作+「時間」特性，像是進展、順序、速度、重複性等資訊（“這場比賽已經打了20分鐘囉”）、與主詞/動作+「質量」特性（“人口差到了70”）這三種特質的描述。如果我們以自駕車系統例子對應上面三種描述[2]，系統提供駕駛的解釋也許可以是：空間：光達感測器掃描的範圍是方圓OOO公尺時間：系統依序分析了車道線、行人、道路障礙物、彎道質量：前方行人走路的速度比駕駛本人平時速度慢很多，請留意這些關於如何為智慧系統設計解釋機制的相關研究還在持續進行中 (關鍵字：explainable AI, interpretable machine learning)，還有有許多需要被解決的問題，像是資訊過量(information overload)、如何得到系統解釋、如何針對不同族群呈現系統解釋等；另外，智慧系統在不同使用情境下也需要配合不同場域的習慣設計，像是醫療、自駕車、智慧工廠、智慧家庭、犯罪防治各種不同情境中，智慧系統都需要有不同的設計解釋機制，過程中需要擁有領域知識(domain knowledge)的專家一起參與設計，我也還在持續探索中～這一系列文章的目的在於讓讀者們留意「智慧系統的解釋機制」這個主題，相信具有不同專業背景的大家能一起從不同切入點來讓跟我們生活密不可分的智慧系統更加親民。最後，如果有對這個領域或主題熟悉的讀者，若有不同觀點或是對於內容有任何評論或想法，歡迎留言公開討論～ ლ(╹◡╹ლ)【延伸閱讀】”Explainable Machine Learning Models for Healthcare AI”： https://youtu.be/4pgLsDzrlB8作者：楊期蘭、劉淨感謝陳美伶、沈奕超、黃振瑋提供編輯建議— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —*本篇是擷取原始論文中部分內容搭配筆者想分享的概念所架構而成，部分研究細節與討論並未完全呈現，鼓勵有興趣的讀者直接參考原文深入了解細節。本篇目的在於讓讀者了解人機互動領域中如何切入人與智慧系統互動的主題。內文並非逐字翻譯，亦不能取代原文 [1]。— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —",20/12/2018,0,22.0,9.0,1156.0,906.0,3.0,1.0,0.0,7.0,zh-tw
3800,Artistic Style Transfer with Convolutional Neural Network,"Data Science Group, IITR",Manjeet Singh,226.0,6.0,1049.0,"We all have used apps like Prisma and Lucid, but ever wondered how these things works? Like we give a photo from our camera roll and select a design to mix both the images and we get a new image which has the content of our input image and style of the design image. In the world of deep learning this is called style transfer.Style transfer is the technique of recomposing images in the style of other images. It all started when Gatys et al. published an awesome paper on how it was actually possible to transfer artistic style from one painting to another picture using convolutional neural networks..Here are some examples :“Neural networks are everywhere. I do not expect that they will take away the bread of artists and designers, but it took my phone a minute to make quite interesting art work from several mediocre pictures.”Convolutional Neural Networks (CNNs) are a category of Neural Network that have proven very effective in areas such as image recognition and classification. CNNs have been successful in computer vision related problems like identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.CNN is shown to be able to well replicate and optimize these key steps in a unified framework and learn hierarchical representations directly from raw images.If we take a convolutional neural network that has already been trained to recognize objects within images then that network will have developed some internal independent representations of the content and style contained within a given image.Here is an example of CNN hierarchy from VGG net where shallow layers learns low level features and as we go deeper into the network these convolutional layers are able to represent much larger scale features and thus have a higher-level representation of the image content.All winning architectures of ImageNet Large Scale Visual Recognition Challenge in recent years have been some form of convolutional neural network — with the most recent winners even being able to surpass human level performance!In 2014, the winner of the ImageNet challenge was a network created by Visual Geometry Group (VGG) at Oxford University, achieving a classification error rate of only 7.0%. Gatys et. al use this network — which has been trained to be extremely effective at object recognition — as a basis for trying to extract content and style representations from images.We can construct images whose feature maps at a chosen convolution layer match the corresponding feature maps of a given content image. We expect the two images to contain the same content — but not necessarily the same texture and style.Given a chosen content layer l, the content loss is defined as the Mean Squared Error between the feature map F of our content image C and the feature map P of our generated image Y.When this content-loss is minimized, it means that the mixed-image has feature activation in the given layers that are very similar to the activation of the content-image. Depending on which layers we select, this should transfer the contours from the content-image to the mixed-image.We will do something similar for the style-layers, but now we want to measure which features in the style-layers activate simultaneously for the style-image, and then copy this activation-pattern to the mixed-image.One way of doing this, is to calculate the Gram-matrix(a matrix comprising of correlated features) for the tensors output by the style-layers. The Gram-matrix is essentially just a matrix of dot-products for the vectors of the feature activations of a style-layer.If an entry in the Gram-matrix has a value close to zero then it means the two features in the given layer do not activate simultaneously for the given style-image. And vice versa, if an entry in the Gram-matrix has a large value, then it means the two features do activate simultaneously for the given style-image. We will then try and create a mixed-image that replicates this activation pattern of the style-image.If the feature map is a matrix F, then each entry in the Gram matrix G can be given by:The loss function for style is quite similar to out content loss, except that we calculate the Mean Squared Error for the Gram-matrices instead of the raw tensor-outputs from the layers.As with the content representation, if we had two images whose feature maps at a given layer produced the same Gram matrix we would expect both images to have the same style, but not necessarily the same content. Applying this to early layers in the network would capture some of the finer textures contained within the image whereas applying this to deeper layers would capture more higher-level elements of the image’s style. Gatys et. al found that the best results were achieved by taking a combination of shallow and deep layers as the style representation for an image.We can see that the best results are achieved by a combination of many different layers from the network, which capture both the finer textures and the larger elements of the original image.Using a pre-trained neural network such as VGG-19, an input image (i.e. an image which provides the content), a style image (a painting with strong style elements) and a random image (output image), one could minimize the losses in the network such that the style loss (loss between the output image style and style of ‘style image’), content loss (loss between the content image and the output image) and the total variation loss (which ensured pixel wise smoothness) were at a minimum. In such cases, the output image generated from such a network, resembled the input image and had the stylist attributes of the style image.The total loss can then be written as a weighted sum of the both the style and content losses.we will minimize our total loss by Adam optimizer. As our loss go down we will go close to our goal of producing a style transfer image Y.Audio/Music style transfers have already made some progress and several more use cases pertaining to unique human tasks like the style of playing chess etc. are also being explored, using more generalized frameworks of style transfer.One suggestion is that do not miss out references, by reading them only you can understand algorithm properly.Hit ❤ if this makes you little bit more intelligent.Co-authors: Nishant Raj and Ashutosh Singh",04/09/2017,0,12.0,0.0,534.0,364.0,11.0,1.0,0.0,13.0,en
3801,How to predict Quora Question Pairs using Siamese Manhattan LSTM,ML Review,Elior Cohen,1000.0,9.0,1269.0,"The article is about Manhattan LSTM (MaLSTM) — a Siamese deep network and its appliance to Kaggle’s Quora Pairs competition.I will do my best to explain the network and go through the Keras code (if you are only here for the code, scroll down :)Full code on GithubIn the past few years, deep learning is all the fuss in the tech industry.To keep up on things I like to get my hands dirty implementing interesting network architectures I come across in article readings.Few months ago I came across a very nice article called Siamese Recurrent Architectures for Learning Sentence Similarity.It offers a pretty straightforward approach to the common problem of sentence similarity.Named MaLSTM (“Ma” for Manhattan distance), its architecture is depicted in figure 1 (diagram excludes the sentence preprocessing part). Notice that since this is a Siamese network, it is easier to train because it shares weights on both sides.(I will be using Keras, so some technical details are related to the implementation)So first of all, what is a “Siamese network”? Siamese networks are networks that have two or more identical sub-networks in them.Siamese networks seem to perform well on similarity tasks and have been used for tasks like sentence semantic similarity, recognizing forged signatures and many more.In MaLSTM the identical sub-network is all the way from the embedding up to the last LSTM hidden state.Word embedding is a modern way to represent words in deep learning models. More about it can be found in this nice blog post.Essentially it’s a method to give words semantic meaning in a vector representation.Inputs to the network are zero-padded sequences of word indices. These inputs are vectors of fixed length, where the first zeros are being ignored and the nonzeros are indices that uniquely identify words.Those vectors are then fed into the embedding layer. This layer looks up the corresponding embedding for each word and encapsulates all them into a matrix. This matrix represents the given text as a series of embeddings.I use Google’s word2vec embedding, same as in the original paper.The process is depicted in figure 2.We have two embedded matrices that represent a candidate of two similar questions. Then we feed them into the LSTM (practically, there is only one) and the final state of the LSTM for each question is a 50-dimensional vector. It is trained to capture semantic meaning of the question.In figure 1, this vector is denoted by the letter h.If you don’t entirely understand LSTMs, I suggest reading this wonderful post.By now we have the two vectors that hold the semantic meaning of each question. We put them through the defined similarity function (below)and since we have an exponent of a negative the output (the prediction in our case) will be between 0 and 1.The optimizer of choice in the article is the Adadelta optimizer, which can be read about in this article. We also use gradient clipping to avoid the exploding gradient problem. You may find a nice explanation of the gradient clipping in this video from the Udacity deep learning course.This is where I will diverge a little from the original paper. For the sake of simplicity, I do not use a specific weight initialization scheme and do not pretrain it on a different task.Other parameters such as batch size, epochs, and the gradient clipping norm value are chosen by me.My full implementation can be found in this Jupyter notebook — keep following this post to see only the significant parts.Here (and in the notebook) I’ve excluded all of the data analysis part, again to keep things simple and the article readable.We get the data as raw text, so our first mission is to take the text and convert it into lists of word indices.When first opening the training data files in pandas, this is what you get.Our columns of interest are question1, question2, and is_duplicate which are self-explanatory.The training data is stored in train_df and the test data in test_df and both are pandas DataFrames.Only difference between train_df and test_df is that the latter doesn’t have the is_duplicate columnNext, I created a helper function named text_to_word_list(text) which takes a string as input and outputs a list where each entry is a single word from the text and does some preprocessing (removing specific signs etc).Now our aim is to have the ability to turn a word into its embedding given by word2vec, in order to do that we will need to build:We also use gensim.models.KeyedVectors to load the word2vec embeddings.Throughout the code only 2 functions of this class will be used, .vocab which will hold all of the word2vec words and .word_vec(word) which takes a word and returns its embedding.Finally we will use nltk's English stopwords and store them in stops.Creating vocabulary and inverse_vocabulary:So now we have vocabulary, inverse_vocabulary and both train_df and test_df converted to word indices, screenshot below.Notice we start at index 1, index 0 is reserved for the zero padding.Also, notice I do not exclude stopwords if they have embeddings, I will later give them a random representation — this is done for the sake of simplicity. A far better approach will be to train your own embeddings to better capture the context of the problem.Our next goal is to create the embedding matrix.We will assign each word its word2vec embedding and leave the unrecognized ones (less than 0.5%) random.Also, we keep the first index all zeros.Great, we have our embedding matrix in place.In order to prepare our data for use in Keras we have to do two things:We will also create a validation dataset, to measure our model using scikit-learn’s train_test_split function — it keeps the labels distribution between the datasets by default. In max_seq_length we have the length of the longest question, and here is the codeitertools.product simply gives all the combinations between the two lists.Now we create the model itself.Most of the code is pretty clear but I would like to take a moment to talk about Keras Merge layer.The Merge layer allows us to merge elements with some built-in methods, but also supports custom methods. This is where it comes in handy since we need to “merge” our two LSTMs output using the MaLSTM similarity function.First let’s define the MaLSTM similarity function.Now lets build the model (using functional API)Don’t you love it how simple Keras is?Next we define the optimizer and compile our model.Now all that is left is to train it!I launched it on my local machine, which has a GTX 1070.The whole script and including preparation and training took about 21 hours.To properly evaluate the model performance, lets plot training data vs validation data accuracy and lossAccuracy:Loss:So just like that out of the box, seems that MaLSTM is doing OK, getting an 82.5% accuracy rate on the validation data.This article and the code as well was written with simplicity in mind.To achieve state of the art results tuning and adjusting to your specific use case will always be needed.Here are some thoughts of mine where can we go from here:The double-edged sword of deep learning is that this list is infinite. I’ve put there a tiny bit of possible things that keep us within the MaLSTM architecture.If you want to explore further and to get the best results possible, I advise you to look at the discussion about the competition — they achieved some really impressive results there using various models and ensembling.The purpose of this post was to put into work a good article that implements some things that you don’t really see in tutorials and stuff, I hope this post and the code has taught you a thing or two. Happy coding :)",07/06/2017,8,32.0,9.0,1152.0,696.0,7.0,3.0,0.0,11.0,en
3802,Use-cases of Google’s Universal Sentence Encoder in Production,Towards Data Science,Sambit Mahapatra,760.0,6.0,825.0,"Before building any Deep Learning model in Natural Language Processing (NLP), text embedding plays a major role. The text embedding converts text (words or sentences) into a numerical vector.Why do we convert texts into vectors?A vector is an array of numbers of a particular dimension. A vector of size 5×1 contain 5 numbers and we can think of it as a point in 5D space. If there are two vectors each of dimension 5, they can be thought of two points in a 5D space. Thus we can calculate how close or distant those two vectors are, depending on the distance measure between them.Hence, lots of efforts in machine learning research are bring put to converting data into a vector as once data is converted into a vector, we can say two data points are similar or not by calculating their distance. Techniques like Word2vec and Glove do that by converting a word to vector. Thus the corresponding vector of “cat” will be closer to “dog” than “eagle”. But while embedding a sentence, along with words the context of the whole sentence needs to be captured in that vector. This is where the “Universal Sentence Encoder” comes into the picture.The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering, and other natural language tasks. The pre-trained Universal Sentence Encoder is publicly available in Tensorflow-hub. It comes with two variations i.e. one trained with Transformer encoder and other trained with Deep Averaging Network (DAN). The two have a trade-off of accuracy and computational resource requirement. While the one with Transformer encoder has higher accuracy, it is computationally more intensive. The one with DNA encoding is computationally less expensive and with little lower accuracy.Here, we will go with the transformer encoder version. It worked well for us while running it along with 5 other heavy deep learning models in a 5 GB ram instance. Also, we could be able to train a classifier with 1.5 Million data using this version of Universal Sentence Encoder at embedding level. Few use cases of Universal Sentence Encoder I have come across are :Let’s see how to use the pre-trained Universal Sentence Encoder available at Tensorflow-hub, for above-mentioned use cases in python.First, let’s import the required libraries:While using in production, we need to download the pre-trained Universal Sentence Encoder to local so that each time we call it won’t be downloaded.Here, “../sentence_wise_email/module/module_useT” is the folder where the sentence encoder files are downloaded. The encoder is optimized for greater-than-word length text, hence can be applied to sentences, phrases or short paragraphs.For example (official site example):The output it gives :As it can be seen whether it is a word, sentence or phrase, the sentence encoder is able to give an embedding vector of size 512.How to use in Rest APIWhile using it in Rest API, you have to call it multiple times. Calling the module and session, again and again, will be very time-consuming. (~16s for each call from our testing). One thing can be done is to call the module and create the session at the start, and continue reusing it. (The first call takes ~16s and then consecutive calls in ~.3s).The output is a matrix of dimension 5*512. (each sentence is a vector of size 512). Since the values are normalized, the inner product of encodings can be treated as a similarity matrix.The output is:As it can be seen here, the similarity between “we are sorry for the inconvenience” and “we are sorry for the delay” is 0.87 (row-1, col-2), while the similarity between “we are sorry for the inconvenience” and “we will get you the best possible rate” is 0.46 (row-1, col-5), which is amazing. There are other ways of finding the similarity score from encodings like cosine similarity, Manhattan distance etc. (code is there in my Github repo mentioned at the end of the article).Removing duplicate textsWhile working on a question-answer verification system, one of the major problems was the repeating of answer statements. Traversing through the available data at us, we found the sentences having the semantic similarity score > 0.8 (calculated through the inner product of encodings as mentioned above) are actually duplicate statements so we removed them.Now the unique messages were:Basically, it discarded the statements “we are sorry for the delay” and “we regret for your inconvenience” as they are the duplicate of sentence 1.Performing classification by finding semantically similar sentencesWhile building an answer evaluation system, we came across the problem of detecting the follow-up statements. But there was not enough data to train a statistical model. We could solve the problem, thanks to the Universal Sentence Encoder. What approach we followed is, create a matrix of encodings of all the available data. Then get the encoding of user input and see if it is more than 60% similar to any of the available data, take it as a follow-up. A simple greetings identification can be:github.comhttps://tfhub.dev/google/universal-sentence-encoder-large/3arxiv.org",24/01/2019,10,7.0,0.0,432.0,288.0,1.0,1.0,0.0,5.0,en
3803,Understanding Bidirectional RNN in PyTorch,Towards Data Science,Ceshine Lee,1500.0,3.0,408.0,"Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. The outputs of the two networks are usually concatenated at each time step, though there are other options, e.g. summation.This structure allows the networks to have both backward and forward information about the sequence at every time step. The concept seems easy enough. But when it comes to actually implementing a neural network which utilizes bidirectional structure, confusion arises…The first confusion is about the way to forward the outputs of a bidirectional RNN to a dense neural network. For normal RNNs we could just forward the outputs at the last time step, and the following picture I found via Google shows similar technique on a bidirectional RNN.But wait… if we pick the output at the last time step, the reverse RNN will have only seen the last input (x_3 in the picture). It’ll hardly provide any predictive power.The second confusion is about the returned hidden states. In seq2seq models, we’ll want hidden states from the encoder to initialize the hidden states of the decoder. Intuitively, if we can only choose hidden states at one time step(as in PyTorch), we’d want the one at which the RNN just consumed the last input in the sequence. But if the hidden states of time step n (the last one) are returned, as before, we’ll have the hidden states of the reversed RNN with only one step of inputs seen.Keras provides a wrapper for bidirectional RNNs. If you take a look at line 292 in wrappers.py:github.comYou’d find that by default the outputs of the reversed RNN is ordered backward as time step (n…1). Keras will reverse it when return_sequences is true (it’s false by default). So if we’re taking one time step output, Keras will take the one at time step n for normal RNN and the one at time step 1 for reverse RNN. This pretty much confirms that figure 2 shows flawed structure.With the first confusion sorted out. We are now interested in how to use bidirectional RNNs correctly in PyTorch:The above notebook answered the two confusions we had (assuming batch_firstis false):(Side note) The output shape of GRU in PyTorch when batch_firstis false:output (seq_len, batch, hidden_size * num_directions)h_n (num_layers * num_directions, batch, hidden_size)The LSTM’s one is similar, but return an additional cell state variable shaped the same as h_n.",13/11/2017,0,7.0,5.0,579.0,317.0,2.0,1.0,0.0,4.0,en
3804,Appearance of The Principate [Pt. II],Medium,Daniel Voshart,960.0,4.0,265.0,"Using the neural-net tool Artbreeder, Photoshop and historical references, I have created photoreal depictions of Roman Emperors. Scroll down to see each emperor.ON CREATIVE COMMONS & COPYRIGHT: Faces can be shared non-watermarked at 200 pixels max height OR 512 pixels with the digital mosaic watermark with Attribution-NonCommercial-ShareAlike. Please link back to this page. Continuation of this project depends on prints, licensing and commissions.*CONCISE UPDATE (July 31st) replacing a July 27th CLARIFICATION: ‘TheApricity’, a tertiary source, has been removed entirely. I knew it to be unreliable prior to starting this project but kept here for posterity and debate. It is now clear to me they have distorted primary and secondary sources to push a pernicious white supremacist agenda. I am instead quoting Davide Cocci who has provided, what I believe to be, a more reliable translation.A primary Greek text by John Malalas has also been removed. Three Emperors on this page: Vitellius, Titus and Domitian were given blond hair based on this source. Had I known better, I would have defaulted to brown hair and likely would have chosen a darker skin tone.68–69 (Died aged 72 — Murdered by Praetorian Guard in coup led by Otho)69–69 (Died aged 36 — Committed suicide after losing Battle of Bedriacum to Vitellius)69–69 (Died aged 54 — Murdered by Vespasian’s troops)69–79 (Died Aged 69 — Natural causes)79–81 (Died aged 41 — Natural causes)Father: Vespasian.Brother: Domitian.81–96 (Died aged 44 — Assassinated by court officials)Father: VespasianBrother: Titus←PREVIOUS [Pt I] 27 BC–68 AD: Julio-Claudian dynastyNEXT [Pt III] 96–192: Nerva–Antonine dynastyABOUT THE AUTHORThis is a quarantine project by Daniel Voshart. 🖼️ Prints available here.",24/07/2020,0,54.0,28.0,1022.0,525.0,12.0,7.0,0.0,59.0,en
3805,Basics of the Classic CNN,Towards Data Science,Chandra Churh Chatterjee,119.0,8.0,1420.0,"Convolutional neural networks. Sounds like a weird combination of biology and math with a little CS sprinkled in, but these networks have been some of the most influential innovations in the field of computer vision and image processing.The Convolutional neural networks are regularized versions of multilayer perceptron (MLP). They were developed based on the working of the neurons of the animal visual cortex.Let’s say we have a color image in JPG form and its size is 480 x 480. The representative array will be 480 x 480 x 3. Each of these numbers is given a value from 0 to 255 which describes the pixel intensity at that point. RGB intensity values of the image are visualized by the computer for processing.The idea is that you give the computer this array of numbers and it will output numbers that describe the probability of the image being a certain class (.80 for a cat, .15 for a dog, .05 for a bird, etc.). It works similar to how our brain works. When we look at a picture of a dog, we can classify it as such if the picture has identifiable features such as paws or 4 legs. In a similar way, the computer is able to perform image classification by looking for low-level features such as edges and curves and then building up to more abstract concepts through a series of convolutional layers. The computer uses low-level features obtained at the initial levels to generate high-level features such as paws or eyes to identify the object.Contents of a classic Convolutional Neural Network: -1.Convolutional Layer.2.Activation operation following each convolutional layer.3.Pooling layer especially Max Pooling layer and also others based on the requirement.4.Finally Fully Connected Layer.1.Input to a convolutional layerThe image is resized to an optimal size and is fed as input to the convolutional layer.Let us consider the input as 32x32x3 array of pixel values2. There exists a filter or neuron or kernel which lays over some of the pixels of the input image depending on the dimensions of the Kernel size.Let the dimensions of the kernel of the filter be 5x5x3.3. The Kernel actually slides over the input image, thus it is multiplying the values in the filter with the original pixel values of the image (aka computing element-wise multiplications).The multiplications are summed up generating a single number for that particular receptive field and hence for sliding the kernel a total of 784 numbers are mapped to 28x28 array known as the feature map.**Now if we consider two kernels of the same dimension then the obtained first layer feature map will be (28x28x2).•Let us take a kernel of size (7x7x3) for understanding. Each of the kernels is considered to be a feature identifier, hence say that our filter will be a curve detector.The sum of the multiplication value that is generated is = 4*(50*30)+(20*30) = 6600 (large number)The sum of the multiplication value that is generated is = 0 (small number).1. The value is much lower! This is because there wasn’t anything in the image section that responded to the curve detector filter. Remember, the output of this convolution layer is an activation map. So, in the simple case of a one filter convolution (and if that filter is a curve detector), the activation map will show the areas in which there at most likely to be curved in the picture.2. In the previous example, the top-left value of our 26 x 26 x 1 activation map (26 because of the 7x7 filter instead of 5x5) will be 6600. This high value means that it is likely that there is some sort of curve in the input volume that caused the filter to activate. The top right value in our activation map will be 0 because there wasn’t anything in the input volume that caused the filter to activate. This is just for one filter.3. This is just a filter that is going to detect lines that curve outward and to the right. We can have other filters for lines that curve to the left or for straight edges. The more filters, the greater the depth of the activation map, and the more information we have about the input volume.In the picture, we can see some examples of actual visualizations of the filters of the first conv. layer of a trained network. Nonetheless, the main argument remains the same. The filters on the first layer convolve around the input image and “activate” (or compute high values) when the specific feature it is looking for is in the input volume.1.When we go through another conv. layer, the output of the first conv. layer becomes the input of the 2nd conv. layer.2. However, when we’re talking about the 2nd conv. layer, the input is the activation map(s) that result from the first layer. So each layer of the input is basically describing the locations in the original image for where certain low-level features appear.3. Now when you apply a set of filters on top of that (pass it through the 2nd conv. layer), the output will be activations that represent higher-level features. Types of these features could be semicircles (a combination of a curve and straight edge) or squares (a combination of several straight edges). As you go through the network and go through more conv. layers, you get activation maps that represent more and more complex features.4. By the end of the network, you may have some filters that activate when there is handwriting in the image, filters that activate when they see pink objects, etc.1.The way this fully connected layer works is that it looks at the output of the previous layer (which as we remember should represent the activation maps of high-level features) and the number of classes N (10 for digit classification).2. For example, if the program is predicting that some image is a dog, it will have high values in the activation maps that represent high-level features like a paw or 4 legs, etc. Basically, an FC layer looks at what high level features most strongly correlate to a particular class and has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes.3. The output of a fully connected layer is as follows [0 .1 .1 .75 0 0 0 0 0 .05], then this represents a 10% probability that the image is a 1, a 10% probability that the image is a 2, a 75% probability that the image is a 3, and a 5% probability that the image is a 9 (Softmax approach) for digit classification.§We know kernels also known as feature identifiers, used for identification of specific features. But how the kernels are initialized with the specific weights or how do the filters know what values to have.Hence comes the important step of training. The training process is also known as backpropagation, which is further separated into 4 distinct sections or processes.•Forward Pass•Loss Function•Backward Pass•Weight UpdateThe Forward Pass:For the first epoch or iteration of the training the initial kernels of the first conv. layer is initialized with random values. Thus after the first iteration output will be something like [.1.1.1.1.1.1.1.1.1.1], which does not give preference to any class as the kernels don’t have specific weights.The Loss Function:The training involves images along with labels, hence the label for the digit 3 will be [0 0 0 1 0 0 0 0 0 0], whereas the output after a first epoch is very different, hence we will calculate loss (MSE — Mean Squared Error)The objective is to minimize the loss, which is an optimization problem in calculus. It involves trying to adjust the weights to reduce the loss.The Backward Pass:It involves determining which weights contributed most to the loss and finding ways to adjust them so that the loss decreases. It is computed using dL/dW, where L is the loss and the W is the weights of the corresponding kernel.The weight update:This is where the weights of the kernel are updated using the following equation.Here the Learning Rate is chosen by the programmer. Larger value of the learning rate indicates much larger steps towards optimization of steps and larger time to convolve to an optimized weight.Finally, to see whether or not our CNN works, we have a different set of images and labels (can’t double dip between training and test!) and pass the images through the CNN. We compare the outputs to the ground truth and see if our network works!",31/07/2019,0,15.0,0.0,565.0,229.0,15.0,2.0,0.0,0.0,en
3806,Understanding Logistic Regression step by step,Towards Data Science,Gustavo Chávez,250.0,6.0,837.0,"Logistic Regression is a popular statistical model used for binary classification, that is for predictions of the type this or that, yes or no, A or B, etc. Logistic regression can, however, be used for multiclass classification, but here we will focus on its simplest application.As an example, consider the task of predicting someone’s gender (Male/Female) based on their Weight and Height.For this, we will train a machine learning model from a data set of 10,000 samples of people’s weight and height. The data set is taken from the Conway & Myles Machine Learning for Hackers book, Chapter 2, and can it can be directly downloaded here.This is a preview of what the data looks like:Each sample contains three columns: Height, Weight, and Male.There are 5,000 samples from males, and 5,000 samples for females, thus the data set is balanced and we can proceed to training.The Python’s scikit-learn code to train a logistic regression classifier and make a prediction is very straightforward:The general workflow is:The logistic regression classifier can be derived by analogy to the linear regression hypothesis which is:However, the logistic regression hypothesis generalizes from the linear regression hypothesis in that it uses the logistic function:The result is the logistic regression hypothesis:The function g(z) is the logistic function, also known as the sigmoid function.The logistic function has asymptotes at 0 and 1, and it crosses the y-axis at 0.5.Since our data set has two features: height and weight, the logistic regression hypothesis is the following:The logistic regression classifier will predict “Male” if:This is because the logistic regression “threshold” is set at g(z)=0.5, see the plot of the logistic regression function above for verification.For our data set the values of θ are:To get access to the θ parameters computed by scikit-learn one can do:With the coefficients at hand, a manual prediction (that is, without using the function clf.predict()) would simply require to compute the vector productand to check if the resulting scalar is bigger than or equal to zero (to predict Male), or otherwise (to predict Female).As an example, say we want to predict the gender of someone with Height=70 inches and Weight = 180 pounds, like at line 14 at the script LogisticRegression.py above, one can simply do:Since the result of the product is bigger than zero, the classifier will predict Male.A visualization of the decision boundary and the complete data set can be seen here:As you can see, above the decision boundary lie most of the blue points that correspond to the Male class, and below it all the pink points that correspond to the Female class.Also, from just looking at the data you can tell that the predictions won’t be perfect. This can be improved by including more features (beyond weight and height), and by potentially using a different decision boundary.Logistic regression decision boundaries can also be non-linear functions, such as higher degree polynomials.The scikit-learn library does a great job of abstracting the computation of the logistic regression parameter θ, and the way it is done is by solving an optimization problem.Let’s start by defining the logistic regression cost function for the two points of interest: y=1, and y=0, that is, when the hypothesis function predicts Male or Female.Then, we take a convex combination in y of these two terms to come up with the logistic regression cost function:The logistic regression cost function is convex. Thus, in order to compute θ, one needs to solve the following (unconstrained) optimization problem:There is a variety of methods that can be used to solve this unconstrained optimization problem, such as the 1st order method gradient descent that requires the gradient of the logistic regression cost function, or a 2nd order method such as Newton’s method that requires the gradient and the Hessian of the logistic regression cost function — this was the method prescribed in the scikit-learn script above.For the case of gradient descent, the search direction is the negative partial derivative of the logistic regression cost function with respect to the parameter θ:In its most basic form, gradient descent will iterate along the negative gradient direction of θ (known as a minimizing sequence) until reaching convergence.Notice that the constant α is usually called the learning rate or the search step and that it has to be carefully tuned to reach convergence. Algorithms such as backtracking line search aid in the determination of α.In summary, these are the three fundamental concepts that you should remember next time you are using, or implementing, a logistic regression classifier:1. Logistic regression hypothesis2. Logistic regression decision boundary3. Logistic regression cost functionFor a discussion of the Logistic regression classifier applied to a data set with more features (using Python too) I recommend this Medium post of I am a postdoctoral fellow at the Lawrence Berkeley National Laboratory, where I work at the intersection of machine learning and high-performance computing.If you find this article interesting, feel free to say hello over LinkedIn, I’m always happy to connect with other professionals in the field.And as always: comments, questions, and shares are highly appreciated! ❤️",21/02/2019,1,19.0,34.0,1338.0,319.0,19.0,3.0,0.0,10.0,it
3807,Object Detection & Segmentation with Python,Medium,Apdullah Yayik,13.0,4.0,489.0,"It was announced by FAIR (facebook artificial intelligence research) last year that the Mask RCNN structure using the resnet50 infrastructure was successfully implemented on MS COCO and Balloon datasets and valuable resuts were obtained (see dedicated github page). In addition, the trained weights were also released for researchers and practitionars to make transfer learning to solve different problems with reasonable cost(see matterport github page).In my another article I have explaineed how to make transfer learning with such released MS COCO weights to deletect an locate weapons (see article here)At the end of this reading this article, you will see succesful object recognition and segmentation in video and images taken randomly from the outerside of the world.It will be a quicker and easier way to run python codes on Google Colab, where the required packages are already available. In this sense, you can review and execute my notebook on Colab.If you need to run on your local computer consider the required packages belowInstall cython and Python API for MS COCO data. These will be used at prepocessing.You can clone Mask RCNN python code from matterport page.Do not forget to add mrcnn folder that contains configurations and coco folder to python path.Import libraries and assign values for needed directories. Note that, here in “logs”folder optimized weights are saved in .h5 format with corresponding time stamp after each epoch while training. In this application weights released by matterport (COCO_MODEL_PATH) will be used so this folder will not used. Also “image” folder contains images for testing, we will not use it to see results from any image from anywhere.Configuration of dataset loading, augmentation, prediction and, masking choises of test images can be made in this code. The utmost important parameters for learning can be changed in base configuration class ‘Config’ that is in /mrcnn/config.py file.‘config’ object inhereted from ‘InterferenceConfig’ class takes parmeters from ‘CocoConfig’ class in coco.py file and overrides for your choice.We will use the parameteres that were used while training to able to make accurate feedforward.Here model is created with considering configuration above, architecture of resnet50 is structured using Tensorflow and Keras libraries (Note that modellib is alias model library of /mrcnn/model.py) Then weights are loaded.MS COCO dataset has 81 number of classes. In which language you want the masking to be made, you can change them without breaking the order.And, we are coming to end for image masking. Read image and predict.I provide some gorgeus photos masked with Mask RCNN.Now, lets make a video example. Start by cloning a sample video from github page.Each frame in the video is being processed in the same way just here, see the whole code in my colab notebook)You can check the number of listed masked frames to verify the accuracy of the process above.Masked frames can be combined to make masked video using the make_frame method with this python code:make_video(outvid, images, fps=30)Congrats! You can check and download masked video in ‘mytestimagesvideos’ folder.!ls ./mytestimagesvideos",27/08/2019,0,2.0,0.0,620.0,264.0,15.0,0.0,0.0,5.0,en
3808,Computer Vision — A journey from CNN to Mask R-CNN and YOLO -Part 1,Towards Data Science,Renu Khandelwal,3900.0,12.0,1503.0,"In this article we will explore and understand the architecture and workings of different computer vision algorithm CNN, Region-based CNN(R-CNN), Fast R-CNN, Faster R-CNN. In the next article, we will explore Mask R-CNN and YOLO(You only look once)What is the purpose of Computer Vision?Computer vision is a subfield of AI. It is used to enable computers to understand, identify and generate intelligent understanding of the digital images the same way human vision does.What does Computer Vision do?Using Computer vision we can identifyWhen we view an image, we scan the image. We may view an image from left to right or top to bottom to understand the different features of the image. Our brain combines different local features that we scanned to classify the image. This is exactly how CNN works.CNN takes input as an image “x”, which is a 2-D array of pixels with different color channels(Red,Green and Blue-RGB).To the input image we apply different filters or feature detector to output feature maps. Filters or feature detectors are spatially small compared to the input image. These filters extend through the full depth of the input image.Multiple convolutions are performed in parallel by applying nonlinear function ReLU to the convolutional layer.Multiple feature detector identifies different things like edge detection, different shapes, bends or different colors etc.We apply Pooling to the convolutional layer. We can apply Min Pooling, Max Pooling or Average Pooling. Max pooling function provides better performance compared to min or average pooling.Pooling helps with Translational Invariance. Translational invariance means that when we change the input by a small amount the pooled outputs does not change.Invariance of image implies that even when an image is rotated, sized differently or viewed in different illumination an object will be recognized as the same object.In the next step, we flatten the pooled layer to input it to a fully connected(FC) neural network.We use a softmax activation function for multi class classification in the final output layer of the fully connected layer.For a binary classification we use a sigmoid activation function in the final output layer of the fully connected layer.Strength of CNNCNN is used forLimitations of CNNSo how do we identify multiple objects present in an image and draw bounding boxes around all the different objects?We now explore Region-based CNN’s that will help solve the problem of multiple objects present in an image and draw bounding boxes around all the different objects.R-CNN is used for classification as well as objection detection with bounding boxes for multiple objects present in an imageHow does R-CNN work?R-CNN works on a premise that only a single object of interest will dominate in a given region.R-CNN uses selective search algorithm for object detection to generate region proposals.so what forms a region in an image?Regions in an image can be identified byFigure(a), spoons, bowls are in different scales. Figure(b), kittens are distinguishable based on colors and not texture. Figure(c), Chameleon is distinguishable by texture, but not color. Figure(d), Wheels are part of the car, but not similar in color or texture. They are part of an enclosure.What is selective search and how will we use it to identify multiple objects in an image?Step 1: Generate initial sub-segmentation. We generate as many regions, each of which belongs to at most one object.Step 2: Recursively combine similar regions into larger ones. Here we use Greedy algorithm.This yields a hierarchy of successively larger regions, just like we wantStep 3: Use the generated regions to produce candidate object locations.Now that we know how Selective Search works, let’s get into the details of R-CNNR-CNN combines region proposal with CNN.Region proposals is a set of candidate detection available to the detector. CNN runs the sliding windows over the entire image however R-CNN instead select just a few windows. R-CNN uses 2000 regions for an image.Region proposals run an algorithm called a segmentation algorithm which uses selective search.but how does object detection in R-CNN work?To all scored regions in an image, apply a greedy non-maximum suppression.Non-Max suppression rejects a region if it has an intersection-over union (IoU) overlap with a higher scoring selected region larger than a learned threshold.Our objective with object detection is to detect an object just once with one bounding box. However, with object detection, we may find multiple detections for the same objects. Non-Max suppression ensures detection of an object only onceTo understand Non-Max suppression, we need to understand IoU.IoU computes intersection over the union of the two bounding boxes, the bounding box for the ground truth and the bounding box for the predicted box by algorithmWhen IoU is 1 that would imply that predicted and the ground-truth bounding boxes overlap perfectly.To detect an Object once in an image, Non-Max suppression considers all bounding boxes with IoU >0.5What if I have multiple bounding boxes with IoU greater than 0.5?For e.g. if we have three rectangles with the 0.6 and the 0.7 and 0.9. For IoU to identify the vehicle in the image below, Non-Max Suppression will keep the bounding box with IoU 0.9 and will suppress the remaining bounding boxes of 0.6 and 0.7 IoU.For the car in the image below, Non-Max Suppression will keep IoU with 0.8 and suppress or remove IoU bounding box with 0.7Biggest Challenges with R-CNN is that Training is slow and expensiveWhat makes training slow and expensive in R-CNN ?•CNN for feature extraction•Linear SVM classifier for identifying objects•Regression model for tightening the bounding boxesSo how do we make the algorithm more efficient and fast?Few things to be improved in R-CNN would beAll this is done in Fast R-CNN.Fast R-CNN is a fast framework for object classification and object detection with deep ConvNetsFast R-CNN network takes image and a set of object proposals as an input.Unlike R-CNN, Fast R-CNN uses a single deep ConvNet to extract features for the entire image once.We also create a set of ROI(Region of Interest) for the image using selective search. Region of interest (RoI) layer extracts a fixed-length feature vector from the feature map for each object proposal for object detection. RoI layer is a special-case of the spatial pyramid pooling layer with only one pyramid levelFully Connected layers(FC) needs fixed-size input. Hence we use ROI Pooling layer to warp the patches of the feature maps for object detection to a fixed size.ROI pooling layer is then fed into the FC for classification as well as localization. RoI pooling layer uses max pooling. It converts features inside any valid region of interest into a small feature map.Fully connected layer branches into two sibling output layersFast R-CNN uses selective search as a proposal method to find the Regions of Interest, which is slow and time consuming process. Not suitable for large real-life data setsFaster R-CNN does not use expensive selective search instead uses Region Proposal Network.It is a single, unified network for object detectionFaster R-CNN consists of two stagesRegion Proposal Network takes an image of any size as input and outputs a set of rectangular object proposals each with an objectness score. It does this by sliding a small network over the feature map generated by the convolutional layerRPN shares computation with a Fast R-CNN object detection network.Feature generated from RPN is fed into two sibling fully connected layers — a box-regression layer for the bounding box and a box-classification layer for object classification.RPN is efficient and processes 10 ms per image to generate the ROI’s.An anchor is centered at the sliding window in question and is associated with a scale and aspect ratio. Faster R-CNN uses 3 scales and 3 aspect ratio, yielding 9 anchors at each sliding windows.Anchors help with translational invariance.At each sliding window location, we simultaneously predict multiple region proposals. The number of maximum possible proposals for each location is denoted as k.Reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate the probability of object or not object for each proposalFaster R-CNN is composed of 3 different neural networksFaster R-CNN takes image as an input and is passed through the Feature network to generate the feature map.RPN uses the feature map from the Feature network as an input to generate the rectangular boxes of object proposals and the objectness score.The predicted region proposals from RPN are then reshaped using a RoI pooling layer. Warped into a fixed vector size.Warped fixed-size vector is then fed into two sibling fully connected layers, a regression layer to predict the offset values for the bounding box and a classification layer for object classificationWe started with a simple CNN used for image classification and object detection for a single object in the image.R-CNN is used for image classification as well as localization for multiple objects in an image.R-CNN was slow and expensive so Fast R-CNN was developed as a fast and more efficient algorithm. Both R-CNN and Fast R-CNN used selective search to come up with regions in an image.Faster R-CNN used RPN(Region Proposal Network) along with Fast R-CNN for multiple image classification, detection and segmentation.In the next article, we will explore YOLO and Mask R-CNN.References:http://vision.stanford.edu/teaching/cs231b_spring1415/slides/ssearch_schuyler.pdfhttps://arxiv.org/pdf/1406.4729.pdfhttps://arxiv.org/pdf/1506.01497.pdfhttps://arxiv.org/pdf/1311.2524.pdfhttp://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdfhttp://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdfhttps://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdfhttp://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf",22/07/2019,0,67.0,13.0,693.0,379.0,20.0,20.0,0.0,9.0,en
3809,How to build a Recurrent Neural Network in TensorFlow (1/7),Medium,Erik Hallström,2000.0,7.0,1467.0,"Dear reader,This article has been republished at Educaora and has also been open sourced. Unfortunately TensorFlow 2.0 changed the API so it is broken for later versions. Any help to make the tutorials up to date are greatly appreciated. I also recommend you looking into PyTorch.In this tutorial I’ll explain how to build a simple working Recurrent Neural Network in TensorFlow. This is the first in a series of seven parts where various aspects and techniques of building Recurrent Neural Networks in TensorFlow are covered. A short introduction to TensorFlow is available here. For now, let’s get started with the RNN!It is short for “Recurrent Neural Network”, and is basically a neural network that can be used when your data is treated as a sequence, where the particular order of the data-points matter. More importantly, this sequence can be of arbitrary length.The most straight-forward example is perhaps a time-series of numbers, where the task is to predict the next value given previous values. The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has “seen” at time-steps before. This state-vector is the encoded memory of the RNN, initially set to zero.The best and most comprehensive article explaining RNN:s I’ve found so far is this article by researchers at UCSD, highly recommended. For now you only need to understand the basics, read it until the “Modern RNN architectures”-section. That will be covered later.Although this article contains some explanations, it is mostly focused on the practical part, how to build it. You are encouraged to look up more theory on the Internet, there are plenty of good explanations.We will build a simple Echo-RNN that remembers the input data and then echoes it after a few time-steps. First let’s set some constants we’ll need, what they mean will become clear in a moment.Now generate the training data, the input is basically a random binary vector. The output will be the “echo” of the input, shifted echo_step steps to the right.Notice the reshaping of the data into a matrix with batch_size rows. Neural networks are trained by approximating the gradient of loss function with respect to the neuron-weights, by looking at only a small subset of the data, also known as a mini-batch. The theoretical reason for doing this is further elaborated in this question. The reshaping takes the whole dataset and puts it into a matrix, that later will be sliced up into these mini-batches.TensorFlow works by first building up a computational graph, that specifies what operations will be done. The input and output of this graph is typically multidimensional arrays, also known as tensors. The graph, or parts of it can then be executed iteratively in a session, this can either be done on the CPU, GPU or even a resource on a remote server.The two basic TensorFlow data-structures that will be used in this example are placeholders and variables. On each run the batch data is fed to the placeholders, which are “starting nodes” of the computational graph. Also the RNN-state is supplied in a placeholder, which is saved from the output of the previous run.The weights and biases of the network are declared as TensorFlow variables, which makes them persistent across runs and enables them to be updated incrementally for each batch.The figure below shows the input data-matrix, and the current batch batchX_placeholder is in the dashed rectangle. As we will see later, this “batch window” is slided truncated_backprop_length steps to the right at each run, hence the arrow. In our example below batch_size = 3, truncated_backprop_length = 3, and total_series_length = 36. Note that these numbers are just for visualization purposes, the values are different in the code. The series order index is shown as numbers in a few of the data-points.Now it’s time to build the part of the graph that resembles the actual RNN computation, first we want to split the batch data into adjacent time-steps.As you can see in the picture below that is done by unpacking the columns (axis = 1) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. The reason for using the variable names “plural”_”series” is to emphasize that the variable is a list that represent a time-series with multiple entries at each step.The fact that the training is done on three places simultaneously in our time-series, requires us to save three instances of states when propagating forward. That has already been accounted for, as you see that the init_state placeholder has batch_size rows.Next let’s build the part of the graph that does the actual RNN computation.Notice the concatenation on line 6, what we actually want to do is calculate the sum of two affine transforms current_input * Wa + current_state * Wb in the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias b is broadcasted on all samples in the batch.You may wonder the variable name truncated_backprop_length is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch.This is the final part of the graph, a fully connected softmax layer from the state to the output that will make the classes one-hot encoded, and then calculating the loss of the batch.The last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically — the computation graph is executed once for each mini-batch and the network-weights are updated incrementally.Notice the API call to sparse_softmax_cross_entropy_with_logits, it automatically calculates the softmax internally and then computes the cross-entropy. In our example the classes are mutually exclusive (they are either zero or one), which is the reason for using the “Sparse-softmax”, you can read more about it in the API. The usage is to havelogits is of shape [batch_size, num_classes] and labels of shape [batch_size].There is a visualization function so we can se what’s going on in the network as we train. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch.It’s time to wrap up and train the network, in TensorFlow the graph is executed in a session. New data is generated on each epoch (not the usual way to do it, but it works in this case since everything is predictable).You can see that we are moving truncated_backprop_length steps forward on each iteration (line 15–19), but it is possible have different strides. This subject is further elaborated in this article. The downside with doing this is that truncated_backprop_length need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data. Otherwise there might a lot of “misses”, as you can see on the figure below.Also realize that this is just simple example to explain how a RNN works, this functionality could easily be programmed in just a few lines of code. The network will be able to exactly learn the echo behavior so there is no need for testing data.The program will update the plot as training progresses, shown in the picture below. Blue bars denote a training input signal (binary one), red bars show echos in the training output and green bars are the echos the net is generating. The different bar plots show different sample series in the current batch.Our algorithm will fairly quickly learn the task. The graph in the top-left corner shows the output of the loss function, but why are there spikes in the curve? Think of it for a moment, answer is below.The reason for the spikes is that we are starting on a new epoch, and generating new data. Since the matrix is reshaped, the first element on each row is adjacent to the last element in the previous row. The first few elements on all rows (except the first) have dependencies that will not be included in the state, so the net will always perform badly on the first batch.This is the whole runnable program, just copy-paste and run. After each part in the article series the whole runnable program will be presented. If a line is referenced by number, these are the line numbers that we mean.In the next post in this series we will be simplify the computational graph creation by using the native TensorFlow RNN API.",10/11/2016,0,1.0,9.0,1283.0,708.0,7.0,0.0,0.0,9.0,en
3810,Text Classification in Spark NLP with Bert and Universal Sentence Encoders,Towards Data Science,Veysel Kocaman,450.0,11.0,1722.0,"Natural language processing (NLP) is a key component in many data science systems that must understand or reason about a text. Common use cases include text classification, question answering, paraphrasing or summarising, sentiment analysis, natural language BI, language modeling, and disambiguation.NLP is essential in a growing number of AI applications. Extracting accurate information from free text is a must if you are building a chatbot, searching through a patent database, matching patients to clinical trials, grading customer service or sales calls, extracting facts from financial reports or solving for any of these 44 use cases across 17 industries.Text classification is one of the main tasks in modern NLP and it is the task of assigning a sentence or document an appropriate category. The categories depend on the chosen dataset and can range from topics.Every text classification problem follows similar steps and is being solved with different algorithms. Let alone classical and popular Machine Learning classifiers like Random Forest or Logistic Regression, there are more than 150 deep learning frameworks proposed for various text classification problems.There are several benchmark datasets being used in Text Classification problems and the latest benchmarks can be tracked on nlpprogress.com. Here is the basic statistics regarding these datasets.A simple text classification application usually follows these steps:In this article, we will build a text classification model in Spark NLP using Universal Sentence Embeddings. Then we will compare this with other ML and DL approaches and text vectorization methods.There are several text classification options in Spark NLP:As discussed thoroughly in our seminal article about Spark NLP, all these text processing steps before ClassifierDL can be implemented in a pipeline that is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. That is, the data are passed through the fitted pipeline in order. Each stage’s transform() method updates the dataset and passes it to the next stage. With the help of Pipelines, we can ensure that training and test data go through identical feature processing steps.Here are the available word/sentence embeddings and language models in Saprk NLP.Before building any Deep Learning model in Natural Language Processing (NLP), text embedding plays a major role. The text embedding converts text (words or sentences) into a numerical vector.Basically, the text embedding methods encode words and sentences in fixed-length dense vectors to drastically improve the processing of textual data. The idea is simple: Words that occur in the same contexts tend to have similar meanings.Techniques like Word2vec and Glove do that by converting a word to vector. Thus the corresponding vector of “cat” will be closer to “dog” than “eagle”. But while embedding a sentence, along with words the context of the whole sentence needs to be captured in that vector. This is where the “Universal Sentence Encoder” comes into the picture.The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering, and other natural language tasks. The pre-trained Universal Sentence Encoder is publicly available in Tensorflow-hub . It comes with two variations i.e. one trained with Transformer encoder and the other trained with Deep Averaging Network (DAN).Spark NLP also use Tensorflow-hub version of USE that is wrapped in a way to get it run in the Spark environment. That is, you can just plug and play this embedding in Spark NLP and train a model in a distributed fashion.Instead of averaging the word embeddings of each word in a sentence to get a sentence embeddings, USE generates embeddings for the sentence with no further calculation.In this article, we will use the AGNews dataset, one of the benchmark datasets in Text Classification tasks, to build a text classifier in Spark NLP using USE and ClassifierDL annotator, the latest classification module added to Spark NLP with version 2.4.4.ClassifierDL is the very first multi-class text classifier in Spark NLP and it uses various text embeddings as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) that is built inside TensorFlow and supports up to 50 classes.That is, you can build a text classifier with Bert, Elmo, Glove and Universal Sentence Encoders in Spark NLP with this ClassiferDL.Lets start coding !Stating with loading necessary packages and starting a Spark session.We can then download AGNews data set from Github repo.AGNews dataset has 4 classes: World, Sci/Tech, Sports, BusinessNow we can feed this data frame to Spark NLP DocumentAssembler, which is the entry point to Spark NLP for any Spark datagram.That’s all. We take the dataset, feed into, and then get the sentence embeddings from USE and then train in ClassifierDL. With zero text preprocessing or cleaning!Now let's start the training (fitting). We will train for 5 epochs using .setMaxEpochs() param in ClassiferDL . This will take around 10 min to finish in Colab environment.When you run this, Spark NLP will write the training logs to annotator_logs folder in your home directory. Here is how you can read the logs.As you can see, we achieved above 90% validation accuracy in less than 10 min with no text preprocessing, which is usually the most time consuming and laborious step in any NLP modeling.Now let's get the predictions ontest set. We will use the test set that we downloaded above.Here is how we can get the test metrics through classification_report in sklearn library.We achieved 89.3% test set accuracy! Looks nice!(After 2.6.0 version, Spark NLP introduced BertSentenceEmbeddings annotator and more than 30 pretrained sentence embeddings models for Electra and Bert, in various sizes. )Now let’s use BertSentenceEmbeddings to train a model on the same dataset.We just load a small Bert sentence embeddings with L8 and 512 dimension and use that instead of USE. As you can see it is almost 8 times smaller the size of USE with the power of Bert. And here what we get after the training.Almost the same metrics as what we get with USE . It’s like AutoML on scale with a few lines of code !As it is the case in any text classification problem, there are a bunch of useful text preprocessing techniques including lemmatization, stemming, spell checking and stopwords removal, and nearly all of the NLP libraries in Python have the tools to apply these techniques except spell checking. At the moment, the Spark NLP library is the only available NLP library out there to have a spell checking capability out of the box.Let's apply these steps in a Spark NLP pipeline and then train a text classifier with Glove word embeddings. We will at first apply several text preprocessing steps (normalize by just keeping the alphabetical letters, removing stopwords and then lemma)and then get word embeddings per token (lemma of a token) and then average the word embeddings in each sentence to get a sentence embeddings per row.Regarding all these text preprocessing tools in Spark NLP and more, you can find detailed instructions and code samples in this Colab notebook.Then we can fit on the train set.and get the test metrics.Now we have 88% test set accuracy! Even after all these text cleaning steps, we couldn’t beat Universal Sentence Embeddings + ClassifierDL :-) This is mainly due to the fact that USE is usually performs better on raw text compared to the cleaned version as it is already trained on raw sentences and when we apply text preprocessing we introduce some noise that was not seen while the USE is being trained and the sentence consistency was compromised while cleaning.In order to train the same classifier with BERT , we can replace glove_embeddings stage with bert_embeddings stage in the same pipeline we built above. You can find more information about how we implement and leverage Bert in Spark NLP at this link.As discussed thoroughly in one of our previous articles, LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don’t even need to convert the input text to DataFrame in order to feed it into a pipeline that’s accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.LightPipelines are easy to create and also save you from dealing with Spark Datasets. They are also very fast and, while working only on the driver node, they execute parallel computation. Let’s see how it applies to our case described above:You can also save this trained model to your disk and then use later in another Spark pipeline with ClassifierDLModel.load().In this article, we trained a multi-class text classification model in Spark NLP using popular word embeddings and Universal Sentence Encoders, and then achieved a decent model accuracy in less than 10 min train time. The entire code can be found at this Github repo (Colab compatible). We also prepared another notebook to cover nearly all the possible Text Classification combinations in Spark NLP and Spark ML (CV, TfIdf, Glove, Bert, Elmo, USE, LR, RF, ClassiferDL, DocClassifier).We also started to give online Spark NLP trainings for both public and enterprise (healthcare) versions. Here is the link to all the public Colab notebooks that would take you from zero to hero in a few hours step by step.John Snow Labs will be organizing virtual Spark NLP trainings and here is the link to the next training:https://events.johnsnowlabs.com/online-training-spark-nlpWe hope that you already read the previous articles on our official Medium page, joined our slack channel, and started to play with Spark NLP. Here are the links for the other articles. Don’t forget to follow our page and stay tuned!Introduction to Spark NLP: Foundations and Basic Components (Part-I)Introduction to: Spark NLP: Installation and Getting Started (Part-II)Spark NLP 101 : Document AssemblerSpark NLP 101: LightPipelineNamed Entity Recognition (NER) with BERT in Spark NLP",12/04/2020,8,10.0,6.0,1256.0,695.0,15.0,2.0,0.0,35.0,en
3811,DBScan on trajectories — Determining Eps and MinPts,Medium,Marcio Geovani Jasinski,46.0,3.0,401.0,"In the last post I’ve applied DBScan to remove noises from a trajectory. However, to achieve an acceptable result from original trajectory I tried several parameters before end up witth:Usually data analysis cannot afford such strategy since it would take too long to clean up big amount of data if every trajectory demands a human evaluation. The good news is that this process can be automated. Actually, the original DBScan paper from Ester et al. brings a section about determining the parameters Eps and MinPts using a heuristic approach.The basic idea is process data evaluating the k-th nearest neighbor of each point and sort them descending. Usually the result will point out a threshold value where clusters will appear on the right side of the chart while noises will be on the left (see Figure 1).Figure 1 — The k-th distance principle to separate noises and clusters (Ester et all. 1996)The separation of noises and clusters can be detected on the first “valley” on the chart. However is not that easy to detect the “valley” by an automated process. In this post I will not cover the “valley” detection since I’m interested in point out the concept.I applied Ester heuristic on same trajectory from previous post using two strategies:The first view of both execution looks like Figure 2. There is a huge noise which actually make the chart very hard to read. For sure, this is the cause of max speed above 700 km/h.Figure 2 — The k6-distance result with all noisesIn order to get a better look I’ve created new charts shown on Figure 3 and Figure 4 without bigger noise. Figure 3 is related to k6-distance indicates a threshold of 42 m instead 40 m. This means exactly same result as my previous output:Figure 3 — The k6-distance heuristic evaluationEster et al. suggests to use a k4-distance evaluation since it gives a good balance of cost and benefits. The result from such input is shown on Figure 3.Figure 4 — The k4-distance heuristic evaluationUsing k4-distance, the heuristic indicate following input: minPts= 4 and Eps = 30. The result of DBScan from mentioned parameters are presented below:It seems to a good result considering the possibility of automation. Moreover, max speed might be related to another issued I did not mention yet: stops. But this one I think is a good topic for the next post.Posted 4th November 2015 at thechaoscomputing.blogspot.com.",11/06/2017,0,4.0,3.0,320.0,150.0,4.0,4.0,0.0,3.0,en
3812,TikTok’s new feature — anime filter got million posts in 3 days,Medium,Vicente Luego,6.0,2.0,278.0,"TikTok is an application which has been used for talking materials between teens and startups for years now. Most of time we only see how it creates from a new emerging industry instead of investigating what makes the customer retention high as 39%.TikTok’s Chinese version, Douyin, published a new filter in its app this week. Within 3 days, they gathered millions of posts which used this filter. The filter names as “Anime Change”. The main character is to change the video into animation.It’s not a new one to be honest, many applications have launched a similar filter before, such as B612. But the difference here is to change a video and to make the result acceptable.The technology used behind is one called Generative Adversarial Networks, AKA GAN. GAN is used for generating a likely photograph by machine learning. It’s a concept raised by Ian Goodfellow in 2014, and became popular in 2018.TikTok team was acutely aware of the potential behind GAN and started to develop their animation function immediately. A special team was created in September of 2019, they used months of time to generate a model and spent most of their time to optimize it, made the animation more acceptable by clients.Finally we saw a successful case from TikTok and soon the feature will be added in the rest 30+ TikTok versions. The episode showed to us again how important the long-term operation of an application is and how hard to prolong the product life cycle. It’s still unknown how the market will be and where will TikTok heads for, but in the end clients should be happy to see more interesting apps and features to use.",28/06/2020,0,0.0,0.0,687.0,483.0,2.0,0.0,0.0,0.0,en
3813,"AlphaFold-based databases and fully-fledged, easy-to-use, online AlphaFold interfaces poised to revolutionize biology",Towards Data Science,LucianoSphere,476.0,22.0,4807.0,"Not only computational but also experimental biology. Thoughts on the future of data science niches in biology.In a recent story I covered the release of the academic paper describing AlphaFold’s version 2 and its source code, and I showed you how scientists around the world were starting to apply the program to their favorite proteins through Google Colab notebooks, for free and without any hardware needs. These notebooks are rapidly evolving to enable more features, allowing anybody to model not only isolated proteins but also complexes of multiple proteins, and including known structures of related proteins and multiple sequence alignments to improve the program’s results. Moreover, Deepmind and the European Bioinformatics Institute started to upload AlphaFold-calculated models for “all” proteins, already having covered 20 full organisms and available for free download. Scientists trying the program and the database of models report on Twitter several success stories that anticipate how these and related technologies will disrupt the area of structural biology. Not only computational but also experimental structural biology, as the predicted models facilitate experimental determination of protein structures.In this story published last week I covered the formal release of the details of AlphaFold 2, the CASP14-winning program for structure prediction developed by Google’s Deepmind, in a peer-reviewed article in the journal Nature and also of its code in GitHub. I also showed you how despite the huge size of this model, its complex dependencies on other libraries and hardware needs, scientists around the world were already running the program right online thanks to Google Colab notebooks developed by some very kind scientists. But history unveils very fast in these times, so many additional exciting news saw the light since my previous article.After a quick recap of what proteins are, why biologists are interested in knowing their structures, how they can be experimentally determined or predicted by computers, and how AlphaFold 2 works, I develop on the breaking news: enhanced Colab notebooks that expose AlphaFold’s most advanced features, a growing database of free models precomputed by AlphaFold 2 for a major fraction of all proteins known from sequenced genomes, successful applications that are already happening, and outcomes of “experiments” testing the limits of the program. Last, I discuss what all this entails for the future of biology and what niches of structural biology and bioinformatics will likely flourish as the result of these new technologies brought about by AlphaFold and all the previous academic work that led Deepmind to master it.Table of contentsBackground: Proteins, why biologists are interested in knowing their structures, how structures can be determined through experiments or predicted by computer programs, and how AlphaFold 2 worksIn a nutshell, proteins are linear chains of multiple amino acids, each of which consists in a constant unit of 4 non-hydrogen atoms plus a sidechain of variable size, ranging from none to around 20 atoms. The amino acids are connected through the constant unit, called backbone, to form a polypeptide that does not remain random but rather acquires one or more arrangements in space. That is, they fold into 3D structures. What exact structure a protein will adopt in 3D depends essentially on the identity of the amino acid sidechains, i.e. its amino acid sequence. Very briefly and simplifying definitions that are quite more complex, amino acid sequences are encoded by genes; the collection of genes of an organism is its genome; and the collection of proteins encoded in a genome is the proteome.To be more precise, and this will be important later, the polypeptide actually can fold into multiple substructures, each of which is called a domain. (In principle it is these domains, not necessarily whole proteins, what AlphaFold has mastered -because that’s what CASP keeps track of, mainly.) And moreover, certain proteins or regions of proteins do not actually fold into well-defined 3D structures, rather remaining “disordered”. Disordered regions can be just small, connecting well-folded domains, or quite long which can in turn have some biological relevance (most times) or not; besides, some proteins are totally, “intrinsically” disordered. I know all this went a bit beyond the classical definition of proteins and protein structures for non-biologists, but it will be important for discussions later on in the article. Predicting not only the structures of proteins but also their disordered regions and how proteins move, is all key to modern structural bioinformatics.Why do biologists want to know the structures of proteins? As briefly touched upon in the introduction to my other story, knowing the structures of proteins allows advancing biotechnology and pharma. Medicaments are small molecules that bind to specific pockets in protein structures, modulating their structures with a positive physiological consequence. For example a small molecule can target a protein that controls how cells divide to attack a cancer. Another small molecule may interfere with an essential bacterial protein thus killing it. And the list goes on. Knowing a protein’s structure also helps us to understand how it performs its function, so we can then change it (by introducing mutations in the encoding gene) to adapt its use in say some biotechnological process such as fermentations, oil degumming, etc.We can easily sequence genes and whole genomes, but passing from amino acid sequences to actual 3D structures is not straightforward. In the best case, when biologists want to know the structure of a new protein, they can check if other proteins of similar sequence have their structure solved (the Protein Data Bank is a free database where academics deposit and find all experimentally determined structures). If there is no known structure that can be used to reliably model the new protein by homology, then two main options stand: either attempting experimental determination of the new protein, or applying a prediction method that does not rely in homology to know structures. Experimental structure determination is in most cases tedious, expensive and labor-intensive, and very often fails. There are three main techniques to solve protein structures experimentally: X-ray diffraction of protein crystals, which requires your protein to produce well-diffracting crystals, Nuclear Magnetic Resonance spectroscopy which has severe limitations in tractable size and solution conditions, and cryo-electron microscopy which is developing very rapidly but is still quite limited to rather large, well-defined proteins or complexes and for many proteins it doesn’t yet produce atomic resolution but just some blobs of atom densities. On the other hand, predicting or “modeling” protein structures without any homolog proteins of known structures is (or kind of “was”) an extremely hard problem, that now got easier thanks not only to AlphaFold but also to several technologies that preceded it.The field of predicting protein structures without known structures for related proteins is what CASP has been tracking for over a quarter of a century. You can see in my last week story how these predictions were rather bad for a long time, until methods for detection of contacts between pairs of amino acids were introduced that helped to guide folding of protein models. These methods essentially exploit alignments of sequences similar to the one under study, seeking for pairs of amino acids that change together and inferring when the co-variation reflects the two amino acids being in contact in the 3D structure. This was by CASP11 and 12, and then for CASP13 some academic groups and also Deepmind rerouted similar alignment-based analyses through machine learning models to predict not only contacts but also distances and orientations between residues, which redounded in better constraints for folding protein models. Then in CASP14 many groups pushed this a bit forward, gaining some prediction capability, but Alphafold 2 mastered the problem through a couple of novel ideas. While the details are in their paper, to me its most interesting ingredients are (i) the novel way how they treat the input sequence alignments and the structures known for related proteins; (ii) the fact that they represent the protein folding problem within the network, i.e. they don’t use external folding as all academics were doing by CASP14 and even AlphaFold 1 was doing by CASP13; and the fact that everything from sequence or alignment input to 3D model output flows in a single, huge, end-to-end differentiable network.Enhanced Colab notebooks that expose AlphaFold’s features needed to get the most of it: multiple sequence alignments, known structures of related proteins, and oligomerization states.As I discussed in my previous story, the AlphaFold 2 model is huge, and many researchers feared that they would never be able to have at hand the hardware resources needed to run it. However, less than a week after release some cool, very kind scientists put up Google Colab notebooks where anybody with a Google account could run AlphaFold on its favorite protein sequence. The early notebooks were quite simple, allowing only the modeling of individual protein chains, which after all is what AlphaFold was designed to achieve and mainly tested in the most popular track of CASP. But quickly, scientists started to add more of AphaFold’s features into the notebooks, such that users can now run essentially any protein with full control and inputs.The main additions were two that are very important for AlphaFold (and any other modern structure prediction program) to perform well. One is the possibility of calculating a multiple sequence alignment from the input sequence, to be fed into the program so that it can extract structural information from it. CASP12 showed that proteins for which more sequences could be found were in average modeled better. CASP13 showed the same trend but also found that programs could work with less sequences. CASP14 showed that programs could work with even fewer sequences, but they (AlphaFold 2 included) still needed them for high-quality predictions. In the early Google Colab notebooks users run the program with single input sequences and no alignments at all. This most often resulted in mid-quality to poor models, as indicated by the LDDT plots (which predict the expected quality of the model at each amino acid). Users could also provide their own alignments, but good alignments for structure prediction have some special requirements. The new Colab notebooks take this into account; moreover, they exploit some ad hoc methods that have been optimized for this, and search for protein sequences in multiple databases that add up to over 20 million sequences.The second very important addition is the possibility of fishing out “templates” i.e. experimental structures of proteins that probably share structural features with the protein one wants to model, and pass these templates to AlphaFold. This is of course very helpful for modeling, to the extent that until recently, modeling based on this kind of homology was the only procedure that guaranteed some success. The higher the similarity between template and target, the better. But of course for many targets, there are no good structures that kind be used as templates.The best Colab notebooks around are these two by Sergey Ovchinnikov, Martin Steinegger, Milot Mirdita and Minkyung Baek:https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynbhttps://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2_complexes.ipynb#scrollTo=g-rPnOXdjf18Besides the options for building sequence alignments and using templates, this notebook includes two additional interesting features: the possibility of doing a final refinement through molecular simulations, and the possibility of modeling multiple copies of the protein together. The former feature is important to optimize small problems, and makes sense only when the models look already quite reliable. The option for modeling multiple copies of the protein together makes sense when you know or suspect that your protein might actually be oligomeric, as I describe in the next section.To complete this short description of this Colab notebook, it delivers 5 models and a plot of predicted quality estimates, all easily downloadable. Although there might be some improvements and a few further extensions to this notebook, I think the main new interesting features will come up from the different tests that scientists are doing to test AlphaFold 2’s limits.Pushing the limits: AlphaFold is so good that it can do a few things that it wasn’t probably even devised to, but it also gets stuck at some long-standing problemsScientists quickly began to test the limits of the program by forcing it to make predictions that in principle it was not much expected to accomplish well. Surprisingly, some of the outcomes of these tests were quite positive!First, already known from the CASP14 assessment (which I remind you runs blindly and independently of the people doing structure predictions), AlphaFold could not only fold correctly the individual domains that make up proteins but also their relative arrangements. That is, it could get whole structures right, at least for well-structured proteins for which large alignments could be built and for which some experimental structures were available for similar proteins. The CASP14 assessment also found that AlphaFold 2 can not only model correctly the backbone of the protein, but also the amino acid side chains. Historically, the problem was so difficult that CASP used to evaluate essentially only the backbone, but by CASP13 when I was an assessor it became obvious that the top programs (including AlphaFold 1 and some academic programs) were so good at modeling the backbone that side chains should be considered too. Now for most target proteins AlphaFold 2 modeled the sidechains very well.Next, and this was reported by several scientists running AlphaFold 2 themselves, the program seems to have a made a huge advancement in predicting the complexes formed by two or more proteins. In biology, such complexes are essential to transmit information between proteins, and in many cases because the truly functional units are assemblies of multiple proteins that do not perform any function if isolated. Some actually aren’t even stable without their partners. Protein complexes come in two main flavors: the so called homo-oligomers which consist on multiple copies of the same molecule, and hetero-oligomers which consist in two or more different proteins. CASP has a specific track dedicated to complexes, which has historically shown that this remains a very difficult problem -on which many are working, of course. And there is even another contest specific for protein complex prediction, called CAPRI, where it wouldn’t surprise me to soon see Deepmind competing too.More specifically, what scientists have discovered is that if they input two concatenated protein sequences into AlphaFold 2 then there is a good chance that it will return quite reasonable complexes. This is by no means trivial, and has been under study for some years, for two reasons. One is that concatenating proteins means concatenating them throughout the whole input multiple sequence alignment, and this is by itself no easy task. Second, coevolution signals between pairs of proteins do exist, but they are usually weaker and, for the case of homo-oligomers, inter-molecular signals (which contain information about the complex by two proteins) are very hard to disentangle from intra-molecular signals (which contain information about how each protein folds).All this I just told you remains at the moment anecdotal, just on twitter and not peer-reviewed, but increasingly more cases of successful protein complex predictions show up. I am sure that right now some are groups benchmarking this further at a larger scale, and that we will soon see a peer-reviewed work assessing AlphaFold’s capability to predict complexes.What structural features cannot AlphaFold 2 yet model? That’s of course also important. First, it can model quite well the short disordered loops that connect structured pieced within well-folded domains, or that connect separate domains. But it cannot model the longer loops at all, so it’s better to just try predictions by leaving them out, especially if they are terminal. After all, long disordered regions are very easy to identify from the sequence only. The good thing is that LDDT plots predict very well that these loops are inaccurate. Always pay attention to LDDT plots.Talking about LDDT, it is important to bear in mind that it is a quite local estimate of model quality. This is very important because two regions distant in sequence may have high LDDT scores, indicating a likely good quality around these amino acids, yet be totally off in their relative orientations and distances. In my own tests I saw this especially happening when an oligomeric protein is modeled as a monomer. If the program is told that the protein is monomeric, it will of course try to satisfy all contacts and short distance restraints by deforming the monomer. The overall shape of the model will thus be wrong, even though the LDDT estimate at each amino acid is quite high. A new run indicating how many copies of the protein must be included in the prediction will hopefully resolve that problem.Another problem I have seen is with proteins that are mostly water-soluble but contain elements that traverse membranes. Membranes are laminar arrangements of lipids, which do not like water so they self-assemble into, ehm, membranes, to hide themselves from water. Some proteins exist only integrated into membranes, these ones are mostly modeled quite well. Others fulfill or their roles in solution; these ones are also mostly modeled correctly. But others are soluble yet have small elements, usually helicoidal, that insert into membranes. In the tests I did, such helices tend to give AlphaFold problems, apparently because it attempts to pack them with the folded part. Somehow this is not all wrong, because that is likely what this element would do if the protein were in solution without any membranes to bind to! Reality is actually more complicated, because such scenario usually renders the protein intractable so experimentalists cannot solve the full structures of these types of proteins; rather, in these cases they only solve the structures of the soluble domains.And one more problem I have seen reported and seen myself is a quite strong bias towards 3D arrangements that are available in the Protein Data Bank. This is perfectly ok, because AlphaFold was trained on this database, and every cautious, knowleadgable user will probably realize about these limitation. What is the problem, exactly? Well, many proteins bind smaller molecules or even elemental ions as parts of their normal function, and for this they adopt different structures. Very often, the structure that has the small molecule or ion bound is more stable and so this is the one that ends up in the Protein Data Bank. The other molecule may be studied with other experimental methods that do not give atomic detail but often do reveal substantial differences between the small molecule-bound and free states. When you then train a model to predict structures, it will most likely predict this state because that is what it knows, even if you provide just a sequence and do not indicate that any ion or small molecule is bound. In fact neither AlphaFold nor any other structure prediction program provides input fields for “relevant small molecules that bind”. Likewise, they do not consider any other source of structural variability, if the multiple options are not reflected in the Protein Data Bank. A typical example of all this, that I saw people discussing in Twitter for AlphaFold 2, is proteins that bind metal ions like say copper or zinc: the amino acids involved in binding the ions are usually only structured in the bound form, but floppy in the unbound forms. A user was surprised that AlphaFold was predicting the ion-bound form even though it had not provided any information about bound ions. That’s perfectly expected, because for this kind of proteins the Protein Data Bank is dominated vastly by ion-bound forms, as ion-free forms are often difficult to characterize.The summary for this section on limitations is clear: Users still need to know their chemistry and biology, and use them to interpret what new a model is telling, what parts can be trusted, and why it is telling what it is telling. That’s why like with any other protein model, it is essential to look at the quality estimates and the templates and alignments used by the program, of course always keeping in mind what’s known about the protein.Deepmind and the European Bioinformatics Institute joined forces to compute protein models for “all” proteins with AlphaFold 2. Models for 20 organisms are already available for free download.The latest news for the community of biologists was that Deepmind and the European Bioinformatics Institute joined efforts to attempt modeling all proteins of all known genomes. This means around 20 million structural models, of which they have already released 350,000 from human and 20 other species. This means cutting time and no need for expertise to run the program (although the Colab notebooks already make it very easy!).The models are accessible for free at https://alphafold.ebi.ac.uk/. For the organisms whose proteomes have been processed, there are direct download links at https://alphafold.ebi.ac.uk/download. Users can search the database through a variety of keywords, from free text to protein identifiers widely used in biology. But to me the best tool is one that allows you to search models by comparing amino acid sequences at https://www.ebi.ac.uk/Tools/sss/fasta/. Why? Because your exact protein of interest may not have been modeled, but there might be a model for a highly similar one, that you can use to easily model your protein by homology.One important note with the models provided by this database is that they are all based assuming monomeric states, and I have seen this causing problems in many proteins that I know are dimeric, trimeric, etc. AlphaFold has no way to know (or guess) this. Perhaps it would be good that Deepming + EBI to provide models assuming different oligomerization states? For the moment, you better run calculations for oligomers to complement what EBI offers. Likewise, as shown above the runs are biased to produce models that resemble what is already there in the Protein Data Bank; this means that if you input the sequence of a protein that is only stable for experimental characterization with a ligand bound, you will most likely get a model that corresponds to this form, even though strictly speaking you wanted to model the protein alone. On finishing this article I found that EBI does stress out all these limitations, in its note at https://www.ebi.ac.uk/about/news/opinion/alphafold-potential-impactsSome concrete successful applications of AlphaFold 2 and related technologiesThe wave of structure prediction methods using alignments and templates processed through machine learning methods had already given a couple of practical surprises in earlier editions of CASP, but AlphaFold 2 brought monopolized these cases in CASP14. Namely, the models themselves helped to complete the interpretation of experimental data that was waiting to be used to round up an experimental structural determination, in a procedure called molecular replacement where draft models are used to interpret X-ray diffraction data. Once that was done, the model (and others) could be finally assessed; of course this one was quite close to the experimental structure.Similar reports have been around outside of CASP for a few years. For example, a structure resolved in this recent work through experimental methods relied substantially on partial models built using programs that academics wrote -in the process setting the bases for AlphaFold 1, and some actually not much different from it.Another field that has already benefitted from this kind of models, and probably the one that will benefit the most from AlphaFold 2, is that of cryo-electron microscopy. In this technique, the main experimental data obtained is essentially a 3D map of electron densities. When this resolution is high enough, special programs (making a long process short, but of course it actually takes a lot of human intervention) can fit amino acids inside and in this way obtain the experimental structures. But high resolution maps are relatively rare compared to the amount of data produced. Thus, for the larger number of proteins inspected by cryo-electron microscopy it is very hard to thread the amino acid sequence to determine the structure. But with high-quality models from programs like AlphaFold one can -talking fast and superficially- simply fit the model inside the electron density map to “validate” it and the best case distort it a bit to better match the experimental data. This is nothing new, it is done quite routinely but it was so far limited to proteins that were easy to model. Now, AlphaFold 2 will allow to apply this same trick to almost all proteins. Moreover, cryo-electron microscopy usually works best for assemblies of several proteins, so AlphaFold’s purported capacity to also model them will be useful if confirmed.And these applications were only to structural biology, i.e. the part of biology that deals with structural details at atomic level. But having access to models will also impact directly on other areas of biology such as cell biology, where even gross models of protein structures are often enough to interpret the results of experiments and to design new ones.The future of biology and data science niches within biology, and impacts on machine learning science itselfThe direct implications of AlphaFold and similar programs are of course the improved ways to model protein structures. As I showed with some examples above this is by no means the end of experimental structural biology, because none of these programs can get details sufficiently right and because they are inherently biased to what is available in the Protein Data Bank. On the contrary, in the last section I made the point that the models coming out from these programs are of inmense help in experimental structure solving. This means that AlphaFold and other modeling technologies will by no means replace structural biologists, but rather make their job easier and let them focus on more complicated aspects and systems.The early 2000s had several genome projects under the spotlight, as they promised to “crack the code of life”. Despite the many advancements that DNA sequencing and whole genomes brought, from the point of structural biology it was obvious that having these genomes available was not enough. The era of solving structures soon started, but this was far less efficient and more expensive than sequencing DNA. Efforts on structural genomics consortia were set to prioritize which proteins were most important to solve structures for, trying to speed up the rate of completion of the Protein Data Bank. Now, the possibility of modeling proteins quite accurately (given sufficient information is available and all the limits discussed in this article plus some others for sure!) makes the dream of structural coverage of the proteome much more feasible. Of course, this fulfilment is due in part to the work done during the structural genomics projects, which filled the Protein Data bank with structural data for programs to learn. And also to the millions of protein sequences obtained throughout years of DNA sequencing proteins, that now allow for the data-rich alignments required by protein modeling programs.The impact on structural biology is not only on predicting the structures of proteins and assisting their experimental determination, but also on designing them, even from scratch. Neural networks that predict protein structures from intermediate predictions of contacts, distances and orientations were recently repurposed into iterative methods that optimize protein sequences to obtain a given protein structure. Protein design is a field in itself, I recommend this review to learn about different methods including those based on neural networks.The other big impact the Deepmind work will likely have is much broader, and has to do with all the technologies developed and put together into AlphaFold 2. As I described already, this is not an improvement of AlphaFold 1 but rather a whole redesign that involves many smaller (yet big!) new tools for the machine learning and data science communities. Taken from their paper, these new developments include:Each of these new “small” inventions and improvements to existing components of neural networks are of potential use in other problems of biology and more broadly of computer sciences. Deepmind said they are working on improving prediction of protein complexes and also on small molecule binding, all key, still open problems in biology. And they are already working on many other areas of biology and clinic, for example in automatically detecting tumors in medical images just to mention one example.Academics are more excited than ever to apply these and new ideas to other problems. Niches of chemistry and biology beyond protein structure prediction include predicting the effects of mutations on disease, automated interpretation of human-written texts for annotation into databases, automated image analyses to detect, delimit, extract and identify objects, better automation of chemical calculations, and many more. We can hence expect that the technologies used to build AlphaFold will likely impact, and inspire impacts, on many fields of science and engineering in the near future.Links and further readsLiked this article and want to tip me? [Paypal] -Thanks!I am a nature, science, technology, programming, and DIY enthusiast. Biotechnologist and chemist, in the wet lab and in computers. I write about everything that lies within my broad sphere of interests. Check out my lists for more stories. Become a Medium member to access all stories by me and other writers, and subscribe to get my new stories by email (original affiliate links of the platform).",26/07/2021,0,20.0,12.0,936.0,526.0,5.0,3.0,0.0,37.0,en
3814,"Background Removal in Real-Time Video Chats using TensorflowJS, Part 1",Medium,Jean-Marc Beaujour,67.0,4.0,660.0,"An app that removes and replaces in real-time the background in webcam video streams, and all from within the browser! No need for a green screen or a uniform background. This project was made during my 4 weeks at the AI Program of Insight Data Science (Palo Alto).Try it here!There is a trend in AI to move from Centralized Cloud Computing to Edge Computing [1], in particular for real time services application for which Centralized Cloud Computing suffers from higher latency. Furthermore, Edge Computing AI might provide solutions for privacy conscientious consumers [2]. One tool that is likely to help this trend is TensorflowJS (TFJS), in brief Tensorflow in Javascript wrapper. TFJS enables to create AI apps, which training and prediction can be conducted on the client side [3], [4].In this demo, I limit the use case to a web-conference type of video stream with a single person in front of the webcam. At the frame level, that is basically a portrait. The goal is to perform a semantic segmentation on each frame where the person is labeled class 1 and the background class 0. The model is trained on a computer with a NVidia GPU, and the inference is performed on the browser of various devices.I used the Flickr portrait mask dataset (Shen et al. 2016): 1900 portrait images and their corresponding masks. The dataset has a variety of portrait’ backgrounds (indoor, outdoor, uniform), a variety of subjects/persons and of facial expressions. The average of all masks shows that in most portraits, the person stands in the center of the frame (see below).The data preparation is relatively basic: cropping and resizing the original image and the masks.The model used is a U-Net type of architecture with the separable convolutional blocks of MobileNet. It is inspired from the MobileUnet implementation. The input is a RGB image (224 x 224) and the output is a binary image of same size. For this 1st implementation, I replaced the BiLinearUpsampling2D, a custom layer in MobileUnet, by UpSampling2D which is supported by TFJS. The model has a small footprint: 6 million parameters and ~28Mb.The loss function is a log-loss, which is good enough for this first implementation.The training takes a bit less than 2hrs. The model performs relatively well on the test data (from the same dataset). The use of the UpSampling2D layer in place of the BiLinearUpsampling2D layer results in pixelation at the mask edges.On my MacBookPro, the inference on a single frame takes a bit more than 300ms, which suggests a frame rate of ~3fps. That’s quite small, considering that a smooth video requires at least 15–20 fps.The trained model (weight + architecture) is then converted to TFJS format and uploaded in a bucket of Google Cloud Storage.At first, the model is downloaded on the client-side. The video from the webcam is captured and rendered on the html page. Each frames are converted to a tensor and are fed to the model: a binary tensor is output. With a few tensor manipulations, the background can be changed to a set of pre-selected image backgrounds (the drop-down menu). The inference is conducted on a frame-by-frame basis.The segmentation works relatively well even on images that the model has never seen. Even though the model was trained on static images without temporal correlations, the results on the live video stream are pretty promising.The frame rate is obviously device dependent:Note that the frame with the hidden background (right) is a bit darker than the original image (left): that’s due to the the crude method I used to superpose the synthetic background, the mask and the original image. I am working on improving that piece, and that will be in Part 2 of the story.Try a live demo here, and let me know what you think!…In Part2 (coming soon) I will be discussing some improvements made to the model to increase the frame rate on the macBook Pro, and data augmentation schemes to improve generalization. Stay tuned!…https://github.com/jmlb/insight_project",27/06/2018,0,14.0,11.0,703.0,320.0,5.0,1.0,0.0,10.0,en
3815,Chatbots were the next big thing: what happened?,The Startup,Justin Lee,9100.0,11.0,2238.0,"Oh, how the headlines blared:“…the 2016 bot paradigm shift is going to be far more disruptive and interesting than the last decade’s move from Web to mobile apps.”Chatbots were The Next Big Thing.Our hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start socializing with machines.And why wouldn’t they be? All the road signs pointed towards insane success.Messaging was huge! Conversational marketing was a sizzling new buzzword! WeChat! China!Plus, it was becoming clear that supply massively exceeded demand when it came to those pesky, hard-to-build apps.At the Mobile World Congress 2017, chatbots were the main headliners. The conference organizers cited an ‘overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots’.In fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:“Will a single platform emerge to dominate the chatbot and personal assistant ecosystem?”One year on, we have an answer to that question.No.Because there isn’t even an ecosystem for a platform to dominate.Chatbots weren’t the first technological development to be talked up in grandiose terms and then slump spectacularly.The age-old hype cycle unfolded in familiar fashion…Reverential TechCrunch articles were written.Prophetic thought leaders like Chris Messina chimed in.Silicon Valley salivated at the prospect of talking to smart automation.Messenger began to overflow with bots.Slack went through exponential growth and even launched a fund for bot investment.Expectations built, built, and then….. It all kind of fizzled out.The predicted paradim shift didn’t materialize.And apps are, tellingly, still alive and well.We look back at our breathless optimism and turn to each other, slightly baffled:“is that it? THAT was the chatbot revolution we were promised?”Digit’s Ethan Bloch sums up the general consensus:“I’m not even sure if we can say ‘chatbots are dead,’ because I don’t even know if they were ever alive.”According to Dave Feldman, Vice President of Product Design at Heap, chatbots didn’t just take on one difficult problem and fail: they took on several and failed all of them.Bots can interface with users in different ways. The big divide is text vs. speech. In the beginning (of computer interfaces) was the (written) word.Users had to type commands manually into a machine to get anything done.Then, graphical user interfaces (GUIs) came along and saved the day. We became entranced by windows, mouse clicks, icons. And hey, we eventually got color, too!Meanwhile, a bunch of research scientists were busily developing natural language (NL) interfaces to databases, instead of having to learn an arcane database query language.Another bunch of scientists were developing speech-processing software so that you could just speak to your computer, rather than having to type. This turned out to be a whole lot more difficult than anyone originally realised:… so many different accents, people speaking too fast or too slowly, mumbling, umming and aahing.Did you say “recognise speech” or “wreck a nice beach”?The next item on the agenda was holding a two-way dialog with a machine. Here’s an example dialog (dating back to the 1990s) with VCR setup system:User: “Hello.”Agent: “Hello, what’s your name?”User: “Candy.”Agent: “Hello, Candy!”User: “Let’s set the clock.”Agent: “What is the time?”User: “The time is 11:00 AM.”Agent sets the clock to 11:00 AM.Pretty cool, right? The system takes turns in collaborative way, and does a smart job of figuring out what the user wants.It was carefully crafted to deal with conversations involving VCRs, and could only operate within strict limitations.Modern day bots, whether they use typed or spoken input, have to face all these challenges, but also work in an efficient and scalable way on a variety of platforms.Basically, we’re still trying to achieve the same innovations we were 30 years ago.Here’s where I think we’re going wrong:An oversized assumption has been that apps are ‘over’, and would be replaced by bots.By pitting two such disparate concepts against one another (instead of seeing them as separate entities designed to serve different purposes) we discouraged bot development.You might remember a similar war cry when apps first came onto the scene ten years ago: but do you remember when apps replaced the internet?It’s said that a new product or service needs to be two of the following: better, cheaper, or faster. Are chatbots cheaper or faster than apps? No — not yet, at least.Whether they’re ‘better’ is subjective, but I think it’s fair to say that today’s best bot isn’t comparable to today’s best app.Plus, nobody thinks that using Lyft is too complicated, or that it’s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot — and having the bot fail.A great bot can be about as useful as an average app. When it comes to rich, sophisticated, multi-layered apps, there’s no competition.That’s because machines let us access vast and complex information systems, and the early graphical information systems were a revolutionary leap forward in helping us locate those systems.Modern-day apps benefit from decades of research and experimentation. Why would we throw this away?But, if we swap the word ‘replace’ with ‘extend’, things get much more interesting.Today’s most successful bot experiences take a hybrid approach, incorporating chat into a broader strategy that encompasses more traditional elements.The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response.Does my product need a bot? Are existing platforms able to support its functionality? Do I have the patience to build a bot that’s capable of doing what I want it to?Another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these.For plenty of companies, bots just aren’t the right solution. The past two years are littered with cases of bots being blindly applied to problems where they aren’t needed.Building a bot for the sake of it, letting it loose and hoping for the best will never end well:The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input.The advantage of this approach is that it’s pretty easy to list all the cases that they are designed to cover. And that’s precisely their disadvantage, too.That’s because these bots are purely a reflection of the capability, fastidiousness and patience of the person who created them; and how many user needs and inputs they were able to anticipate.Problems arise when life refuses to fit into those boxes.According to recent reports, 70% of the 100,000+ bots on Facebook Messenger are failing to fulfil simple user requests. This is partly a result of developers failing to narrow their bot down to one strong area of focus.When we were building GrowthBot, we decided to make it specific to sales and marketers: not an ‘all-rounder’, despite the temptation to get overexcited about potential capabilties.Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly.A competent developer can build a basic bot in minutes — but one that can hold a conversation? That’s another story. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like.In an ideal world, the technology known as NLP (natural language processing) should allow a chatbot to understand the messages it receives. But NLP is only just emerging from research labs and is very much in its infancy.Some platforms provide a bit of NLP, but even the best is at toddler-level capacity (for example, think about Siri understanding your words, but not their meaning.)As Matt Asay outlines, this results in another issue: failure to capture the attention and creativity of developers.“Consumer interest was never going to materialize until machine intelligence could get anywhere near human intelligence.User interest depends upon AI that makes talking with a bot worthwhile for consumers.”And conversations are complex. They’re not linear. Topics spin around each other, take random turns, restart or abruptly finish.Today’s rule-based dialogue systems are too brittle to deal with this kind of unpredictability, and statistical approaches using machine learning are just as limited. The level of AI required for human-like conversation just isn’t available yet.And in the meantime, there are few high-quality examples of trailblazing bots to lead the way. As Dave Feldman remarked:“Should Slack, Facebook, Google, Microsoft, Kik, and others have built their own built-in bots to lead the way?Should they have gotten more proactive with their bot funds and incubators, hiring mentors to educate participants in the Way of the Bot, or supplying engineering and design resources? Funded Strategic Bot Initiatives at high-profile partners?In my opinion yes, yes, and yes. When it comes to platforms, developers are the users; and we don’t rely on our users to understand why or how to use our products. We have to show them.”Once upon a time, the only way to interact with computers was by typing arcane commands to the terminal. Visual interfaces using windows, icons or a mouse were a revolution in how we manipulate informationThere’s a reasons computing moved from text-based to graphical user interfaces (GUIs). On the input side, it’s easier and faster to click than it is to type.Tapping or selecting is obviously preferable to typing out a whole sentence, even with predictive (often error-prone ) text. On the output side, the old adage that a picture is worth a thousand words is usually true.We love optical displays of information because we are highly visual creatures. It’s no accident that kids love touch screens. The pioneers who dreamt up graphical interface were inspired by cognitive psychology, the study of how the brain deals with communication.Conversational UIs are meant to replicate the way humans prefer to communicate, but they end up requiring extra cognitive effort. Essentially, we’re swapping something simple for a more-complex alternative.Sure, there are some concepts that we can only express using language (“show me all the ways of getting to a museum that give me 2000 steps but don’t take longer than 35 minutes”), but most tasks can be carried out more efficiently and intuitively with GUIs than with a conversational UI.Aiming for a human dimension in business interactions makes sense.If there’s one thing that’s broken about sales and marketing, it’s the lack of humanity: brands hide behind ticket numbers, feedback forms, do-not-reply-emails, automated responses and gated ‘contact us’ forms.Facebook’s goal is that their bots should pass the so-called Turing Test, meaning you can’t tell whether you are talking to a bot or a human. But a bot isn’t the same as a human. It never will be.A conversation encompasses so much more than just text.Humans can read between the lines, leverage contextual information and understand double layers like sarcasm. Bots quickly forget what they’re talking about, meaning it’s a bit like conversing with someone who has little or no short-term memory.As HubSpot team pinpointed:Bots provide a scalable way to interact one-on-one with buyers. Yet, they fail when they don’t deliver an experience as efficient and delightful as the complex, multi-layered conversations people are accustomed to having with other humans on messaging apps.People aren’t easily fooled, and pretending a bot is a human is guaranteed to diminish returns (not to mention the fact that you’re lying to your users).And even those rare bots that are powered by state-of-the-art NLP, and excel at processing and producing content, will fall short in comparison.And here’s the other thing. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans.But is that how humans prefer to interact with machines?Not necessarily.At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure.In a way, those early-adopters weren’t entirely wrong.People are yelling at Google Home to play their favorite song, ordering pizza from the Domino’s bot and getting makeup tips from Sephora. But in terms of consumer response and developer involvement, chatbots haven’t lived up to the hype generated circa 2015/16.Not even close.Computers are good at being computers. Searching for data, crunching numbers, analyzing opinions and condensing that information.Computers aren’t good at understanding human emotion. The state of NLP means they still don’t ‘get’ what we’re asking them, never mind how we feel.That’s why it’s still impossible to imagine effective customer support, sales or marketing without the essential human touch: empathy and emotional intelligence.For now, bots can continue to help us with automated, repetitive, low-level tasks and queries; as cogs in a larger, more complex system. And we did them, and ourselves, a disservice by expecting so much, so soon.But that’s not the whole story.Yes, our industry massively overestimated the initial impact chatbots would have. Emphasis on initial.As Bill Gates once said:We always overestimate the change that will occur in the next two years and underestimate the change that will occur in the next ten. Don’t let yourself be lulled into inaction.The hype is over. And that’s a good thing. Now, we can start examining the middle-grounded grey area, instead of the hyper-inflated, frantic black and white zone.I believe we’re at the very beginning of explosive growth. This sense of anti-climax is completely normal for transformational technology.Messaging will continue to gain traction. Chatbots aren’t going away. NLP and AI are becoming more sophisticated every day.Developers, apps and platforms will continue to experiment with, and heavily invest in, conversational marketing.And I can’t wait to see what happens next.",05/06/2018,0,14.0,17.0,897.0,604.0,12.0,1.0,0.0,35.0,en
3816,Reinforcement Learning w/ Keras + OpenAI: Actor-Critic Models,Towards Data Science,Yash Patel,589.0,13.0,2666.0,"Quick RecapLast time in our Keras/OpenAI tutorial, we discussed a very fundamental algorithm in reinforcement learning: the DQN. The Deep Q-Network is actually a fairly new advent that arrived on the seen only a couple years back, so it is quite incredible if you were able to understand and implement this algorithm having just gotten a start in the field. As with the original post, let’s take a quick moment to appreciate how incredible results we achieved are: in a continuous output space scenario and starting with absolutely no knowledge on what “winning” entails, we were able to explore our environment and “complete” the trials.Put yourself in the situation of this simulation. This would essentially be like asking you to play a game, without a rulebook or specific endgoal, and demanding you to continue to play until you win (almost seems a bit cruel). And not only that: the possible result states you could reach with a series of actions is infinite (i.e. continuous observation space)! Yet, the DQN converges surprising quickly in tackling this seemingly impossible task by maintaining and slowly updating value internally to actions.Even More Complex EnvironmentThe step up from the previous MountainCar environment to the Pendulum is very similar to that from CartPole to MountainCar: we are expanding from a discrete environment to continuous. The Pendulum environment has an infinite input space, meaning that the number of actions you can take at any given time is unbounded. Why is DQN no longer applicable in this environment? Wasn’t our implementation of it completely independent of the structure of the environment actions?While it was indepedent of what the actions were, the DQN was fundamentally premised on having a finite output space. After all, think about how we structured the code: the prediction looked to assign a score to each of the possible actions at each time step (given the current environment state) and simply taking the action that had the highest score. We had previously reduced the problem of reinforcement learning to effectively assigning scores to actions. But, how would this be possible if we have an infinite input space? We would need an infinitely large table to keep track of all the Q values!So, how do we go about tackling this seemingly impossible task? After all, we’re being asked to do something even more insane than before: not only are we given a game without instructions to play and win, but this game has a controller with infinite buttons on it! Let’s see why it is that DQN is restricted to a finite number of actions.The reason stems from how the model is structured: we have to be able to iterate at each time step to update how our position on a particular action has changed. That’s exactly why we were having the model predict the Q values rather than directly predicting what action to take. If we did the latter, we would have no idea how to update the model to take into account the prediction and what reward we received for future predictions. So, the fundamental issue stems from the fact that it seems like our model has to output a tabulated calculation of the rewards associated with all the possible actions. What if, instead, we broke this model apart? What if we had two separate models: one outputting the desired action (in the continuous space) and another taking in an action as input to produce the Q values from DQNs? That seems to solve our problems and is exactly the basis of the actor-critic model!Actor-Critic Model TheoryAs we went over in previous section, the entire Actor-Critic (AC) method is premised on having two interacting models. This theme of having multiple neural networks that interact is growing more and more relevant in both RL and supervised learning, i.e. GANs, AC, A3C, DDQN (dueling DQN), and so on. Getting familiar with these architectures may be somewhat intimidating the first time through but is certainly a worthwhile exercise: you’ll be able to understand and program some of the algorithms that are at the forefront of modern research in the field!Getting back to the topic at hand, the AC model has two aptly named components: an actor and a critic. The former takes in the current environment state and determines the best action to take from there. It is essentially what would have seemed like the natural way to implement the DQN. The critic plays the “evaluation” role from the DQN by taking in the environment state and an action and returning a score that represents how apt the action is for the state.Imagine this as a playground with a kid (the “actor”) and her parent (the “critic”). The kid is looking around, exploring all the possible options in this environment, such as sliding up a slide, swinging on a swing, and pulling grass from the ground. The parent will look at the kid, and either criticize or complement here based on what she did, taking the environment into account. The fact that the parent’s decision is environmentally-dependent is both important and intuitive: after all, if the child tried to swing on the swing, it would deserve far less praise than if she tried to do so on a slide!Brief Aside: Chain Rule (Optional)The main point of theory you need to understand is one that underpins a large part of modern-day machine learning: the chain rule. It would not be a tremendous overstatement to say that chain rule may be one of the most pivotal, even though somewhat simple, ideas to grasp to understand practical machine learning. In fact, you could probably get away with having little math background if you just intuitively understand what is conceptually convenyed by the chain rule. I’ll take a very quick aside to describe the chain rule, but if you feel quite comfortable with it, feel free to jump to the next section, where we actually see what the practical outline for developing the AC model looks like and how the chain rule fits into that plan.Pictorially, this equation seems to make very intuitive sense: after all, just “cancel out the numerator/denominator.” There’s one major problem with this “intuitive explanation” though: the reasoning in this explanation is completely backwards! It is important to remember that math is just as much about developing intuitive notation as it is about understanding the concepts. And so, people developing this “fractional” notation because the chain rule behaves very similarly to simplifying fractional products. So, people who try to explain the concept just through the notation are skipping a key step: why is it that this notation is even applicable? As in, why do derivatives behave this way?The underlying concept is actually not too much more difficult to grasp than this notation. Imagine we had a series of ropes that are tied together at some fixed points, similar to how springs in series would be attached. Let’s say you’re holding one end of this spring system and your goal is to shake the opposite end at some rate 10 ft/s. You could just shake your end at that speed and have it propagate to the other end. Or you could hook up some intermediary system that shakes the middle connection at some lower rate, i.e. at 5 ft/s. In that case, you’d only need to move your end at 2 ft/s, since whatever movement you’re making will be carried on from where you making the movement to the endpoint. This is because the physical connections force the movement on one end to be carried through to the end. Note: of course, as with any analogy, there are points of discrepancy here, but this was mostly for the purposes of visualization.In a very similar way, if we have two systems where the output of one feeds into the input of the other, jiggling the parameters of the “feeding network,” will shake its output, which will propagate and be multiplied by any further changes through to the end of the pipeline.AC Model OverviewTherefore, we have to develop an ActorCritic class that has some overlap with the DQN we previously implemented, but is more complex in its training. Because we’ll need some more advanced features, we’ll have to make use of the underlying library Keras rests upon: Tensorflow. Note: You can definitely implement this in Theano as well, but I haven’t worked with it in the past and so have not included its code. Feel free to submit expansions of this code to Theano if you choose to do so to me!The model implementation will consist of four main parts, which directly parallel how we implemented the DQN agent:AC ParametersFirst off, just the imports we’ll be needing:The parameters are very similar to those in the DQN. After all, this actor-critic model has to do the same exact tasks as the DQN except in two separate modules. We also continue to use the “target network hack” that we discussed in the DQN post to ensure the network successfully converges. The only new parameter is referred to as “tau” and relates to a slight change in how the target network learning takes place in this case:The exact use of the tau parameter is explained more in the training section that follows but essentially plays the role of shifting from the prediction models to the target models gradually. Now, we reach the main points of interest: defining the models. As described, we have two separate models, each associated with its own target network.We start with defining actor model. The purpose of the actor model is, given the current state of the environment, determine the best action to take. Once again, this task has numeric data that we are given, meaning there is no room or need to involve any more complex layers in the network than simply the Dense/fully-connected layers we’ve been using thus far. And so, the Actor model is quite simply a series of fully connected layers that maps from the environment observation to a point in the environment space:The main difference is that we return the a reference to the Input layer. The reason for this will be more clear by the end of this section, but briefly, it is for how we handle the training differently for the actor model.The tricky part for the actor model comes in determining how to train it, and this is where the chain rule comes into play. But before we discuss that, let’s think about why it is any different than the standard critic/DQN network training. After all, aren’t we simply going to fit as in the DQN case, where we fit the model according to the current state and what the best action would be based on current and discounted future rewards? The problem lies in the question: if we’re able to do what we asked, then this would be a solved issue. The issue arises in how we determine what the “best action” to take would be, since the Q scores are now calculated separately in the critic network.So, to overcome this, we choose an alternate approach. Rather than finding the “best option” and fitting on that, we essentially do hill climbing (gradient ascent). For those not familiar with the concept, hill climbing is a simple concept: from your local POV, determine the steepest direction of incline and move incrementally in that direction. In other words, hill climbing is attempting to reach a global max by simply doing the naive thing and following the directions of the local maxima. There are scenarios you could imagine where this would be hopelessly wrong, but more often than not, it works well in practical situations.As a result, we want to use this approach to updating our actor model: we want to determine what change in parameters (in the actor model) would result in the largest increase in the Q value (predicted by the critic model). Since the output of the actor model is the action and the critic evaluates based on an environment state+action pair, we can see how the chain rule will play a role. We’ll want to see how changing the parameters of the actor will change the eventual Q, using the output of the actor network as our “middle link” (code below is all in the “__init__(self)” method):We see that here we hold onto the gradient between the model weights and the output (action). We’ve also scaled it by the negation of self.actor_critic_grad (since we want to do gradient ascent in this case), which is held by a placeholder. For those unfamiliar with Tensorflow or learning for the first time, a placeholder plays the role of where you “input data” when you run the Tensorflow session. I won’t go into details about how it works, but the tensorflow.org tutorial goes through the material quite beautifully.Moving on to the critic network, we are essentially faced with the opposite issue. That is, the network definition is slightly more complicated, but its training is relatively straightforward. The critic network is intended to take both the environment state and action as inputs and calculate a corresponding valuation. We do this by a series of fully-connected layers, with a layer in the middle that merges the two before combining into the final Q-value prediction:The main points of note are the asymmetry in how we handle the inputs and what we’re returning. For the first point, we have one extra FC (fully-connected) layer on the environment state input as compared to the action. I did so because that is the recommended architecture for these AC networks, but it probably works equally (or marginally less) well with the FC layer slapped onto both inputs. As for the latter point (what we’re returning), we need to hold onto references of both the input state and action, since we need to use them in doing updates for the actor network:Here we set up the missing gradient to be calculated: the output Q with respect to the action weights. This is directly called in the training code, as we will now look into.AC TrainingThe last main part of this code that is different from the DQN is the actual training. We do, however, make use of the same basic structure of pulling episodes from memory and learning from those. Since we have two training methods, we have separated the code into different training functions, cleanly calling them as:Now we define the two train methods. The, however, is very similar to that from the DQN: we are simply finding the discounted future reward and training on that. The only difference is that we’re training on the state/action pair and are using the target_critic_model to predict the future reward rather than the actor:As for the actor, we luckily did all the hard work before! We already set up how the gradients will work in the network and now simply have to call it with the actions and states we encounter:As mentioned, we made use of the target model. And so, we have to update its weights at every time step. However, we only do so slowly. More concretely, we retain the value of the target model by a fraction self.tau and update it to be the corresponding model weight the remainder (1-self.tau) fraction. We do this for both the actor/critic, but only the actor is given below (you can see the critic in the full code at the bottom of the post):AC PredictionThis is identical to how we did it in the DQN, and so there’s not much to discuss on its implementation:Prediction CodeThe prediction code is also very much the same as it was in previous reinforcement learning algorithms. That is, we just have to iterate through the trial and call predict, remember, and train on the agent:Complete CodeWith that, here is the complete code used for training against the “Pendulum-v0” environment using AC (Actor-Critic)!Boy, that was long: thanks for reading all the way through (or at least skimming)! Keep an eye out for the next Keras+OpenAI tutorial!",31/07/2017,12,11.0,16.0,393.0,281.0,5.0,1.0,0.0,0.0,en
3817,Machine Learning Project: Predicting Boston House Prices With Regression,Towards Data Science,Victor Roman,2100.0,17.0,2245.0,"In this project, we will develop and evaluate the performance and the predictive power of a model trained and tested on data collected from houses in Boston’s suburbs.Once we get a good fit, we will use this model to predict the monetary value of a house located at the Boston’s area.A model like this would be very valuable for a real state agent who could make use of the information provided in a dayly basis.You can find the complete project, documentation and dataset on my GitHub page:https://github.com/rromanss23/Machine_Leaning_Engineer_Udacity_NanoDegree/tree/master/projects/boston_housingThe dataset used in this project comes from the UCI Machine Learning Repository. This data was collected in 1978 and each of the 506 entries represents aggregate information about 14 features of homes from various suburbs located in Boston.The features can be summarized as follows:This is an overview of the original dataset, with its original features:For the purpose of the project the dataset has been preprocessed as follows:We’ll now open a python 3 Jupyter Notebook and execute the following code snippet to load the dataset and remove the non-essential features. Recieving a success message if the actions were correclty performed.As our goal is to develop a model that has the capacity of predicting the value of houses, we will split the dataset into features and the target variable. And store them in features and prices variables, respectivelyIn the first section of the project, we will make an exploratory analysis of the dataset and provide some observations.Calculate StatisticsData Science is the process of making some assumptions and hypothesis on the data, and testing them by performing some tasks. Initially we could make the following intuitive assumptions for each feature:We’ll find out if these assumptions are correct through the project.Scatterplot and HistogramsWe will start by creating a scatterplot matrix that will allow us to visualize the pair-wise relationships and correlations between the different features.It is also quite useful to have a quick overview of how the data is distributed and wheter it cointains or not outliers.We can spot a linear relationship between ‘RM’ and House prices ‘MEDV’. In addition, we can infer from the histogram that the ‘MEDV’ variable seems to be normally distributed but contain several outliers.Correlation MatrixWe are going to create now a correlation matrix to quantify and summarize the relationships between the variables.This correlation matrix is closely related witn covariance matrix, in fact it is a rescaled version of the covariance matrix, computed from standardize features.It is a square matrix (with the same number of columns and rows) that contains the Person’s r correlation coefficient.To fit a regression model, the features of interest are the ones with a high correlation with the target variable ‘MEDV’. From the previous correlation matrix, we can see that this condition is achieved for our selected variables.In this second section of the project, we will develop the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model’s performance through the use of these tools and techniques helps to reinforce greatly the confidence in the predictions.Defining a Performace MetricIt is difficult to measure the quality of a given model without quantifying its performance on the training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement.For this project, we will calculate the coefficient of determination, R², to quantify the model’s performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how “good” that model is at making predictions.The values for R² range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable.A model can be given a negative R2 as well, which indicates that the model is arbitrarily worse than one that always predicts the mean of the target variable.Shuffle and Split DataFor this section we will take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.Training and TestingYou may ask now:What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm?It is useful to evaluate our model once it is trained. We want to know if it has learned properly from a training split of the data. There can be 3 different situations:1) The model didn´t learn well on the data, and can’t predict even the outcomes of the training set, this is called underfitting and it is caused because a high bias.2) The model learn too well the training data, up to the point that it memorized it and is not able to generalize on new data, this is called overfitting, it is caused because high variance.3) The model just had the right balance between bias and variance, it learned well and is able predict correctly the outcomes on new data.In this third section of the project, we’ll take a look at several models’ learning and testing performances on various subsets of training data.Additionally, we’ll investigate one particular algorithm with an increasing 'max_depth' parameter on the full training set to observe how model complexity affects performance.Graphing the model's performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone.Learning CurvesThe following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased.Note that the shaded region of a learning curve denotes the uncertainty of that curve (measured as the standard deviation). The model is scored on both the training and testing sets using R2, the coefficient of determination.Learning the DataIf we take a close look at the graph with the max depth of 3:Complexity CurvesThe following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves — one for training and one for validation.Similar to the learning curves, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the performance_metric function.Bias-Variance TradeoffIf we analize how the bias-variance vary with the maximun depth, we can infer that:Best-Guess Optimal ModelFrom the complexity curve, we can infer that the best maximum depth for the model is 4, as it is the one that yields the best validation score.In addition, for more depth although the training score increases, validation score tends to decrease which is a sign of overfitting.In this final section of the project, we will construct a model and make a prediction on the client’s feature set using an optimized model from fit_model.Grid SearchThe grid search technique exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter, which is a dictionary with the values of the hyperparameters to evaluate. One example can be:In this example, two grids should be explored: one with a linear kernel an C values of [1,10,100,1000], and the second one with an RBF kernel, and the cross product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001].When fitting it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.K-fold cross-validation is a technique used for making sure that our model is well trained, without using the test set. It consist in splitting data into k partitions of equal size. For each partition i, we train the model on the remaining k-1 parameters and evaluate it on partition i. The final score is the average of the K scores obtained.When evaluating different hyperparameters for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance.To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.However, by partitioning the available data into three sets (training, validating and testing sets), we drastically reduce the number of samples which can be used for learning the model, and the resulting model may not be sufficiently well trained (underfitting).By using k-fold validation we make sure that the model uses all the training data available for tunning the model, it can be computationally expensive but allows to train models even if little data is available.The main purpose of k-fold validation is to get an unbiased estimate of model generalization on new data.Fitting a ModelThe final implementation requires that we bring everything together and train a model using the decision tree algorithm.To ensure that we are producing an optimized model, we will train the model using the grid search technique to optimize the 'max_depth'parameter for the decision tree. The 'max_depth' parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction.In addition, we will find your implementation is using ShuffleSplit() for an alternative form of cross-validation (see the 'cv_sets'variable). The ShuffleSplit() implementation below will create 10 ('n_splits') shuffled sets, and for each shuffle, 20% ('test_size') of the data will be used as the validation set.Making PredictionsOnce a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data.In the case of a decision tree regressor, the model has learned what the best questions to ask about the input data are, and can respond with a prediction for the target variable.We can use these predictions to gain information about data where the value of the target variable is unknown, such as data the model was not trained on.Optimal ModelThe following code snippet finds the maximum depth that return the optimal model.Predicting Selling PricesImagine that we were a real estate agent in the Boston area looking to use this model to help price homes owned by our clients that they wish to sell. We have collected the following information from three of our clients:To find out the answers of these questions we will execute the folowing code snippet and discuss its output.From the statistical calculations done at the beginning of the project we found out the following information:Given these values, we can conclude:And our initial assumptions of the features are confirmed:Model’s SensitivityAn optimal model is not necessarily a robust model. Sometimes, a model is either too complex or too simple to sufficiently generalize to new data.Sometimes, a model could use a learning algorithm that is not appropriate for the structure of the data given.Other times, the data itself could be too noisy or contain too few samples to allow a model to adequately capture the target variable — i.e., the model is underfitted.The code cell below run the fit_model function ten times with different training and testing sets to see how the prediction for a specific client changes with respect to the data it's trained on.We obtained a range in prices of nearly 70k$, this is a quite large deviation as it represents approximately a 17% of the median value of house prices.Model’s ApplicabilityNow, we use these results to discuss whether the constructed model should or should not be used in a real-world setting. Some questions that are worth to answer are:Data collected from 1978 is not of much value in today’s world. Society and economics have changed so much and inflation has made a great impact on the prices.The dataset considered is quite limited, there are a lot of features, like the size of the house in square feet, the presence of pool or not, and others, that are very relevant when considering a house price.Given the high variance on the prince range, we can assure that it is not a robust model and, therefore, not appropiate for making predictions.Data collected from a big urban city like Boston would not be applicable in a rural city, as for equal value of feaures prices are much higher in the urban area.In general it is not fair to estimate or predict the price of an indivual home based on the features of the entire neighborhood. In the same neighborhood there can be huge differences in prices.Throughout this article we made a machine learning regression project from end-to-end and we learned and obtained several insights about regression models and how they are developed.This was the first of the machine learning projects that will be developed on this series. If you liked it, stay tuned for the next article! Which will be an introduction to the theory and concepts regarding to classification algorithms.If you liked this post then you can take a look at my other posts on Data Science and Machine Learning here.If you want to learn more about Machine Learning, Data Science and Artificial Intelligence follow me on Medium, and stay tuned for my next posts!",20/01/2019,13,71.0,60.0,639.0,295.0,13.0,16.0,0.0,4.0,en
3818,"AdaBelief Optimizer: fast as Adam, generalizes as well as SGD",Towards Data Science,Kaustubh Mhaisekar,13.0,8.0,961.0,"All types of neural networks and many machine learning algorithms optimize their loss functions using gradient-based optimization algorithms. There are several such optimization algorithms, or optimizers, that exist and are used to train models - RMSprop, Stochastic Gradient Descent(SGD), Adaptive Moment Estimation(Adam) and so many more.There are two primary metrics to look at while determining the efficacy of an optimizer:Adaptive algorithms like Adam have a good convergence speed, while algorithms like SGD generalize better.But recently researchers from Yale introduced a novel AdaBelief optimizer (AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients) that combines many benefits of existing optimization methods:We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the “belief” in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step.To understand what this means and how AdaBelief works, we first need to take a look at Adam (Adam: A Method for Stochastic Optimization), the optimizer AdaBelief is derived from.The Adam Optimizer is one of the most widely used optimizer to train all kinds of neural networks. It basically combines the optimization techniques of momentum and RMS prop. Let me explain how it works in short:Notations used here:So what’s happening here is we have a loss function f(θ) that is to be minimized by finding the optimal values of θ such that the loss function achieves its minima. In order to do that, we use gradient descent where we basically compute the gradients of the function and keep subtracting them from the weights until we reach the minimum.To make this descent faster, we combine two optimization techniques:In Adam, we simply combine these two to form an optimizer that can capture the characteristics of both these optimization algorithms to get this update direction:Note: m-t and v-t here are used after bias correction, to get better fitting early on in the training.The addition of ε is to prevent the denominator from being equal to 0.Also here m-t is known as the first moment, and v-t is known as the second moment, hence the name “Adaptive moment estimation”.Now that you know how Adam works, let’s look at AdaBelief.As you can see, the AdaBelief optimizer is extremely similar to the Adam optimizer, with one slight difference. Here instead of using v-t, the EMA of gradient squared, we have this new parameter s-t:And this s-t replaces v-t to form this update direction:Let us now see what difference does this one parameter make and how does it affect the performance of the optimizer.s-t is defined as the EMA of (g-t - m-t)², that is, the square of the difference between the gradient and the EMA of the gradient(m-t). This means that AdaBelief takes a large step when the value of the gradient is close to its EMA, and a small step when the two values are different.Let’s look at this graph here to better understand AdaBelief’s advantage over Adam -In the given graph, look at region 3:In region 3, the value of g-t is going to be big as the curve is really steep in that area. The value of v-t is going to be big as well, and thus if we used Adam here, the step size in this region is going to be really small as v-t is in the denominator.But, in AdaBelief, we calculate s-t as the moving average of the difference between the gradient and its moving average squared. And since both of these values are really close, the value of s-t is going to be really small this time, thus if we use AdaBelief, since s-t is really small and is in the denominator, we will end up taking big steps in this region, as an ideal optimizer should.We see that AdaBelief can take care of regions with “Large gradient, small curvature” cases, while Adam can’t.Also, note that in the graph regions 1 and 2 can be used to demonstrate the advantage of AdaBelief and Adam over optimizers such as momentum or SGD in the following way:Let us gain some more intuition by looking at this 2D example, consider a loss function - f(x,y) = |x| + |y|:Here the blue arrows represent the gradients and the x on the right side is the optimal point. As you can see, the gradient in the x-direction is always 1, while in the y-direction it keeps oscillating between 1 and -1.So in Adam, v-t for x and y directions will always be equal to 1 as it considers only the amplitude of the gradient and not the sign. Hence Adam will take the same sized steps in both x and y directions.But in AdaBelief, both the amplitude and sign of the gradient is considered. So in the y-direction s-t will be equal to 1, in the x direction it will become 0, thus taking much larger steps in the x-direction than the y-direction.Here are some video examples created by the authors of the original paper to demonstrate the performance of AdaBelief - AdaBelief Optimizer, Toy examplesHere are some experimental results comparing the performance of AdaBelief with other optimizers on different neural networks like CNNs, LSTMs, and GANs presented by the authors of the original paper:2. Time Series Modeling:3. Generative Adversarial Network:I hope you understood and enjoyed all of the concepts explained in this post. Please feel free to reach out for any kind of questions or doubts.Thanks for reading!",19/12/2020,0,52.0,0.0,712.0,363.0,11.0,7.0,0.0,26.0,en
3819,How to Train a BERT Model From Scratch,Towards Data Science,James Briggs,6500.0,7.0,1243.0,"Many of my articles have been focused on BERT — the model that came and dominated the world of natural language processing (NLP) and marked a new age for language models.For those of you that may not have used transformers models (eg what BERT is) before, the process looks a little like this:Now, this is a great approach, but if we only ever do this, we lack the understanding behind creating our own transformers models.And, if we cannot create our own transformer models — we must rely on there being a pre-trained model that fits our problem, this is not always the case:So in this article, we will explore the steps we must take to build our own transformer model — specifically a further developed version of BERT, called RoBERTa.There are a few steps to the process, so before we dive in let’s first summarize what we need to do. In total, there are four key parts:Once we have worked through each of these sections, we will take the tokenizer and model we have built — and save them both so that we can then use them in the same way we usually would with from_pretrained.As with any machine learning project, we need data. In terms of data for training a transformer model, we really are spoilt for choice — we can use almost any text data.And, if there’s one thing that we have plenty of on the internet — it’s unstructured text data.One of the largest datasets in the domain of text scraped from the internet is the OSCAR dataset.The OSCAR dataset boasts a huge number of different languages — and one of the clearest use-cases for training from scratch is so that we can apply BERT to some less commonly used languages, such as Telugu or Navajo.Unfortunately, the only language I can speak with any degree of competency is English — but my girlfriend is Italian, and so she — Laura, will be assessing the results of our Italian-speaking BERT model — FiliBERTo.So, to download the Italian segment of the OSCAR dataset we will be using HuggingFace’s datasets library — which we can install with pip install datasets. Then we download OSCAR_IT with:Let’s take a look at the dataset object.Great, now let’s store our data in a format that we can use when building our tokenizer. We need to create a set of plaintext files containing just the text feature from our dataset, and we will split each sample using a newline \n.Over in our data/text/oscar_it directory we will find:Next up is the tokenizer! When using transformers we typically load a tokenizer, alongside its respective transformer model — the tokenizer is a key component in the process.When building our tokenizer we will feed it all of our OSCAR data, specify our vocabulary size (number of tokens in the tokenizer), and any special tokens.Now, the RoBERTa special tokens look like this:So, we make sure to include them within the special_tokens parameter of our tokenizer’s train method call.Our tokenizer is now ready, and we can save it file for later use:Now we have two files that define our new FiliBERTo tokenizer:And with those, we can move on to initializing our tokenizer so that we can use it as we would use any other from_pretrained tokenizer.We first initialize the tokenizer using the two files we built before — using a simple from_pretrained:Now our tokenizer is ready, we can try encoding some text with it. When encoding we use the same two methods we would typically use, encode and encode_batch.From the encodings object tokens we will be extracting the input_ids and attention_mask tensors for use with FiliBERTo.The input pipeline of our training process is the more complex part of the entire process. It consists of us taking our raw OSCAR training data, transforming it, and loading it into a DataLoader ready for training.We’ll start with a single sample and work through the preparation logic.First, we need to open our file — the same files that we saved as .txt files earlier. We split each based on newline characters \n as this indicates the individual samples.Then we encode our data using the tokenizer — making sure to include key parameters like max_length, padding, and truncation.And now we can move onto creating our tensors — we will be training our model through masked-language modeling (MLM). So, we need three tensors:If you’re not familiar with MLM, I’ve explained it here.Our attention_mask and labels tensors are simply extracted from our batch. The input_ids tensors require more attention however, for this tensor we mask ~15% of the tokens — assigning them the token ID 3.In the final output, we can see part of an encoded input_ids tensor. The very first token ID is 1 — the [CLS] token. Dotted around the tensor we have several 3 token IDs — these are our newly added [MASK] tokens.Next, we define our Dataset class — which we use to initialize our three encoded tensors as PyTorch torch.utils.data.Dataset objects.Finally, our dataset is loaded into a PyTorch DataLoader object — which we use to load our data into our model during training.We need two things for training, our DataLoader and a model. The DataLoader we have — but no model.For training, we need a raw (not pre-trained) BERTLMHeadModel. To create that, we first need to create a RoBERTa config object to describe the parameters we’d like to initialize FiliBERTo with.Then, we import and initialize our RoBERTa model with a language modeling (LM) head.Before moving onto our training loop we need to set up a few things. First, we set up GPU/CPU usage. Then we activate the training mode of our model — and finally, initialize our optimizer.Finally — training time! We train just as we usually would when training via PyTorch.If we head on over to Tensorboard we’ll find our loss over time — it looks promising.Now it’s time for the real test. We set up an MLM pipeline — and ask Laura to assess the results. You can watch the video review at 22:44 here:We first initialize a pipeline object, using the 'fill-mask' argument. Then begin testing our model like so:“ciao come va?” is the right answer! That’s as advanced as my Italian gets — so, let’s hand it over to Laura.We start with “buongiorno, come va?” — or “good day, how are you?”:The first answer, “buongiorno, chi va?” means “good day, who is there?” — eg nonsensical. But, our second answer is correct!Next up, a slightly harder phrase, “ciao, dove ci incontriamo oggi pomeriggio?” — or “hi, where are we going to meet this afternoon?”:And we return some more positive results:Finally, one more, harder sentence, “cosa sarebbe successo se avessimo scelto un altro giorno?” — or “what would have happened if we had chosen another day?”:We return a few good more good answers here too:Overall, it looks like our model passed Laura’s tests — and we now have a competent Italian language model called FiliBERTo!That’s it for this walkthrough of training a BERT model from scratch!We’ve covered a lot of ground, from getting and formatting our data — all the way through to using language modeling to train our raw BERT model.I hope you enjoyed this article! If you have any questions, let me know via Twitter or in the comments below. If you’d like more content like this, I post on YouTube too.Thanks for reading!🤖 70% Discount on the NLP With Transformers Course*All images are by the author except where stated otherwise",06/07/2021,2,7.0,20.0,1370.0,764.0,4.0,4.0,0.0,4.0,en
3820,Credit Card Fraud Detection using Autoencoders in H2O,Towards Data Science,Maneesha Rajaratne,236.0,8.0,712.0,"Frauds in the finance field are very rare to be identified. Because of that, it can do a severe damage to the financial field. It is estimated that fraud costs at least $80 billion a year across all lines of insurance. If there is a small possibility of detecting fraudulent activities, that can do a major impact on annual losses. That is why financial companies invest in machine learning as a preemptive approach to tackling fraud.The benefits of using a machine learning approach are that,The best way to detect frauds is anomaly detection.Anomaly detection is a technique to identify unusual patterns that do not conform to the expected behaviors, called outliers. It has many applications in business from fraud detection in credit card transactions to fault detection in operating environments. Machine learning approaches for Anomaly detection;Today we will be using Autoencoders to train the model.Most of us are not familiar with this model. Autoencoders is an unsupervised Neural Network. It is a data compression algorithm which takes the input and going through a compressed representation and gives the reconstructed output.As for the dataset we will be using Credit Card Transaction dataset provided by Kaggle: https://www.kaggle.com/mlg-ulb/creditcardfraudThe dataset includes 284,807 transactions. among them, 492 transactions are labeled as frauds. Because of this, the dataset is highly imbalanced. It contains only numerical variables. Feature ‘Time’ contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature ‘Amount’ is the transaction Amount, this feature can be used for example-dependent cost-sensitive learning. Feature ‘Class’ is the response variable and it takes value 1 in case of fraud and 0 otherwise.You can find my Kaggle Kernel here: https://www.kaggle.com/maneesha96/credit-card-fraud-detection-using-autoencodersFull code: https://github.com/Mash96/Credit-Card-Fraud-DetectionThen Let's get started!!!We will be using H2O as the ML platform today. You can find more info here: https://www.h2o.aiInitialize H2O serverLoading dataset using pandas data frameChecking for null values in the datasetIn order to proceed we need to convert the pandas data frame to H2O data frameThe Time variable is not giving an impact on the model prediction. This can figure out from data visualization. Before moving on to the training part, we need to figure out which variables are important and which are not. So we can drop the unwanted variables.Split the data frame as training set and testing set keeping 80% for the training set and rest to the testing set.Our dataset has a lot of non-fraud transactions. Because of this for the model training, we only send non-fraud transactions. So that the model will learn the pattern of normal transactions.When building the model, 4 fully connected hidden layers were chosen with, [14,7,7,14] number of nodes for each layer. First two for the encoder and last two for the decoder.Variable Importance : In H2O there is a special way of analyzing which variables are giving higher impact on the model.VisualizationThe testing set has both normal and fraud transactions in it. The Autoencoder will learn to identify the pattern of the input data. If an anomalous test point does not match the learned pattern, the autoencoder will likely have a high error rate in reconstructing this data, indicating anomalous data. So that we can identify the anomalies of the data. To calculate the error, it uses Mean Squared Error(MSE)The accuracy is 0.9718Since the data is highly imbalanced, it cannot be measured only by using accuracy. Precision vs Recall was chosen as the matrix for the classification task.Precision: Measuring the relevancy of obtained results.[ True positives / (True positives + False positives)]Recall: Measuring how many relevant results are returned.[ True positives / (True positives + False negatives)]True Positives — Number of actual frauds predicted as fraudsFalse Positives — Number of non-frauds predicted as fraudsFalse Negatives — Number of frauds predicted as non-frauds.We need to find a better threshold that can separate the anomalies from normal. This can be done by calculating the intersection of the Precision/Recall vs Threshold graph.Our model is catching most of the fraudulent data. In Autoencoders, it gives a good accuracy. But if we look into Precision and Recall of the dataset, it is not performing enough. As I mentioned earlier, there are other anomaly detection methods that perform well in highly imbalanced datasets.I have tried more methods on this dataset. So I will see you soon with those. :)https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd",17/11/2018,27,17.0,4.0,737.0,430.0,18.0,2.0,0.0,5.0,en
3821,Backpropagation: The Simple Proof,Towards Data Science,Essam Wisam,15.0,10.0,1812.0,"What sets artificial neural networks apart from other machine learning algorithms is how they can efficiently deal with big data and how they assume very little about your dataset.Your neural network doesn’t care if your classification data isn’t linearly separable via a kernel or if the trend followed by your regression data is a roller coaster. As long that your dataset is some continuous mapping from one finite space (x) to another (y) then you can approximate that mapping to any degree of accuracy depending on your architecture. This follows from them being universal approximators as proven by the Universal Approximation Theory. The point is that back, when neural networks first showed up in the 40s, there was no fast way to make use of this aspect of neural networks. It wasn’t until 1970 that Backpropagation — a fast training algorithm for neural networks was published in its modern form.We’ll spend most of our time in this story understanding how such algorithm works from a mathematical perspective.Before we delve into its proof. I’d like to make sure that you’re aware of two things. First, that given a multivariable function f(x, y) (e.g. the surface shown in the figure above) then all it takes to minimize f(x, y) is to find ∂f/∂x and ∂f/∂y and then iteratively use gradient descent to update x and y till we reach the minimum. The same applies if our multivariable function is some cost function in millions of variables (weights and biases) if we wish to minimize it then all it takes is to find the partial derivative of the function with respect to each of the parameters and then use gradient descent to iteratively update it’s parameters in order to minimize the loss. The second thing that I want to you to be aware of is the chain rule. Consider f(x) with x being a function in some variable w, then to find ∂f/∂w we write ∂f/∂w = (∂f/∂x)×(∂x/∂w) this is simply the chain rule, and it can be generalized for f(x₁, x₂, x₃) with each xᵢ being a function in w by writing it for each and then adding: ∂f/∂y = (∂f/∂x₁)×(∂x₁/∂w) + (∂f/∂x₂)×(∂x₂/∂w) + (∂f/∂x₃)×(∂x₃/∂w) this applies no matter how many xᵢ’s we have which justifies using Σ while writing the expression.Now our objective is clear, we’re given a dataset, an arbitrary neural network, a cost function J and we need to find the derivative of J with respect to every parameter (weight/bias) in the neural network and backpropagation allows us to do just that, one observation at a time. Which means that if our loss function isthen to find its derivative with respect to some weight or bias we can find the derivative offor each individual training example (xᵢ, yᵢ) and then add them all up and divide by d. Like if we introduced the derivative inside the sum. Going forward, the formula above is what we’re alluding to when we write J.Now what remains before we begin the proof is to decide on a notation.For weights let wᴸₘₙ be the weight from the mₜₕ neuron in the (L-1)ₜₕ layer to the nₜₕ neuron in the Lₜₕ layer. So for example the weight going from the third neuron in the input layer to second neuron in the hidden layer would be w⁰₃₂ We’ll later look back with favor on making the first hidden layer have L = 0 but bear with me for now.Besides weights we also have activations and biases which are specific to the neuron. we’ll denote the activation of the mₜₕ neuron in the Lₜₕ layer by aᴸₘ and for the bias we’ll do bᴸₘ.Notice that if we call the activation function h then we can writeThis exactly captures what happens inside an arbitrary neuron. A linear combination followed by an activation. Now it might serve us well if we call the linear combination z, so that the following is trueNow what, shall we launch into the proof? Well, since we’re going to be implementing this (perhaps, in another story), it’s so helpful if we also talk about how we can write what we presented so far in vector/matrix notation.For each layer in the neural network that isn’t the input layer we can define the following:Wᴸ : The weight matrix involving weights from the (L-1)ₜₕ layer to the Lₜₕ layer. If the (L-1)ₜₕ layer has M neurons and the Lₜₕ layer has N neurons then this has dimensions M*N. Like you’d imagine, the element Wᴸ[m,n] will simply be wᴸₘₙ that we defined earlier.bᴸ : The bias vector for any layer would have the bias of each of its neurons, bᴸ[m] = bᴸₘaᴸ : The activations vector for any layer would have the activations of each of its neurons, aᴸ[m] = aᴸₘzᴸ : The pre-activations vector for any layer would have the linear combinations of each of its neurons, zᴸ[m] = zᴸₘUsing this new notation, if we let the function h be applied on vectors element-wise then we can writeRemember that due to our notation each column in W represents the weights into a specific neuron in the Lₜₕ layer. This fact along with how matrix multiplication works should drive the formula above home for you.Also note the that for L = 0 (first hidden layer) the above relation holds if and only if we substitute x for the activation. The only reason we didn’t give L = 0 for the input layer is that this would make Wᴸ, bᴸ, etc. undefined for L = 0. We might as well let the final layer have L = H so we can easily highlight it in the proof. This also makes H the no. of hidden layers in our network.Let’s start by finding ∂J/∂wᴸₘₙ we can make use of the fact that zᴸₙ is a function in wᴸₘₙ by using the chain rule in the following wayBy substituting zᴸₙ with its definition and considering that bᴸₙ isn’t a function in wᴸₘₙ we getNow because the only weight that will survive the differentiation in the sum is that for which i = m, we can writeFor most of our time in the proof we’ll be trying to find the term in red; thus, it’s worth giving a name. Let’s call it δᴸₙ. Notice that each neuron has its own δᴸₙ. For a whole layer we can write the vector δᴸ like we did for bᴸ, aᴸ and zᴸ.So our result in index notation becomesand in vector notationWhich directly follows from the definition of the outer product and our result in index notation.Now let’s find ∂J/∂bᴸₙ using the analogous fact that zᴸₙ is a function in bᴸₙBy substituting zᴸₙ with its definition and considering that the sum is no function in bᴸₙ we getNow this meansor in vector notationSo concluding the proof really boils down to just finding delta. Remember how our cost function looks like?y(x) is really just the activations of the last layer, so we can writeBecause J is explicitly a function in aᴴ, this should tempt you to find δᴴₙ by using the chain rule and involving aᴴ.By this, we’re implicitly assuming that aᴴₙ is only a function in zᴴₙ in that layer which is true for most cost function that you’ve came across but not for soft-max because then any aₙ is a function of all the zₙ’s in the layer. In that case we would need to take them into consideration by using the multivariable version of the chain rule. But we’ll do that for another time.Both of these terms are easy to find. For the firstBecause this applies for any neuron in the last layer, we can write the vector form asNow for the second term, we haveand in vector form it’s simply h’(zᴴ) where h’ would be applied element-wise on the vector zᴴ.Combining both of these in index notation yieldsBecause of how it looks in index notation if we use ⵙ for the element-wise product of two vectors then we can realize the formula in vector notation by writingBut this only speaks for the last layer when we seek finding δ for any neuron in our network (δᴸₙ).Because we know δ for the last layer, the one way to use it to find the δ’s for other layers is to find a relation that relates the δ of the current layer L with the δ of the next layer. Let’s try that out.We need the sum here because every zₘ in the next layer is a function of zₙ (h(zₙ) goes into all of them, right?). For the next term, by substituting z with its definition and considering that aᴸᵢ is only a function in zᴸₙ when i = n, we can write it as:Now recalling that aᴸₙ=h(zᴸₙ) we can writeBy substituting in δᴸₙ, in index notation we getNotice that what we’re doing in the sum is equivalent to dot product between the row in the weight matrix that corresponds to the neuron and the whole delta vector (column). This is exactly what happens behind the scenes if we multiply the weight matrix by the delta vector except that because it occurs for all rows in W we get a vector that considers the quantity all neurons. And since for each we simply multiply by h(zᴸₙ) we can make use again of ⵙ as an element-wise product and write:Take a deep breath. This concludes the proof.To compile all the equations that we’ve derived in index notation we can writeAnd in vector notation (layer by layer)The proof helps us only arrive at the equations; the algorithm is what employs them. It considers one example at a time and goes as follows:1- Find aᴸ and zᴸ for layers 0 through H by feeding an example into the network. (Use the first two equations.) This is known as the “forward pass”.2- Compute δᴸ for layers H through 0 using the formulae for δᴴ, δᴸ respectively. (Use the third equation.)3- Simultaneously compute ∂J/∂Wᴸ and ∂J/∂bᴸ for layers H through 0 as once we have δᴸ we can find both of these. (Use the last two equations.) This is known as the “backward pass”.4- Repeat for more examples until the weights and biases of the network can be updated through gradient descent (depends on your batch size):As you might’ve noticed, steps 2 and 3 are what give the algorithm its name; we have to back propagate in order to find the parameters. In the next story we might consider implementing this in Python. Till then. Au revoir.References:[1] “File:Kernel Machine.Png — Wikimedia Commons”. Commons.Wikimedia.Org, 2011, https://commons.wikimedia.org/wiki/File:Kernel_Machine.png.[2] Pixabay, 2015, https://pixabay.com/de/vectors/paraboloid-mathematik-schale-fl%c3%a4che-696804/. Accessed 21 Sept 2021.[3] “File:Multilayerneuralnetworkbigger English.Png — Wikimedia Commons”. Commons.Wikimedia.Org, 2010, https://commons.wikimedia.org/wiki/File:MultiLayerNeuralNetworkBigger_english.png.[4] Bout, William. “File:San Francisco Museum Of Modern Art, San Francisco, United States (Unsplash Rkjf2bmrljc).Jpg — Wikimedia Commons”. Commons.Wikimedia.Org, 2017, https://commons.wikimedia.org/wiki/File:San_Francisco_Museum_of_Modern_Art,_San_Francisco,_United_States_(Unsplash_RkJF2BMrLJc).jpg.[5] “Woman Writing On A Whiteboard”. Pexels, 2021, https://www.pexels.com/photo/woman-writing-on-a-whiteboard-3862130/. Accessed 21 Sept 2021.Study Resources:Nielsen, Michael. Neural Networks And Deep Learning. 2019, CHP. 1,2.",22/09/2021,0,138.0,120.0,1302.0,257.0,35.0,0.0,0.0,9.0,en
3822,Basics of Image Classification with PyTorch,Heartbeat,John Olafenwa,652.0,12.0,2519.0,"Many deep learning frameworks have been released over the past few years. Among them, PyTorch from Facebook AI Research is very unique and has gained widespread adoption because of its elegance, flexibility, speed, and simplicity. Most deep learning frameworks have either been too specific to application development without sufficient support for research, or too specific for research without sufficient support for application development.However, PyTorch blurs the line between the two by providing an API that’s very friendly to application developers while at the same time providing functionalities to easily define custom layers and fully control the training process, including gradient propagation. This makes it a great fit for both developers and researchers.Chief of all PyTorch’s features is its define-by-run approach that makes it possible to change the structure of neural networks on the fly, unlike other deep learning libraries that rely on inflexible static graphs.In this post, you’ll learn from scratch how to build a complete image classification pipeline with PyTorch. Get ready for an exciting ride!Installing PyTorch is a breeze thanks to pre-built binaries that work well across all systems.INSTALL ON WINDOWSCPU Only:pip3 install http://download.Pytorch.org/whl/cpu/torch-0.4.0-cp35-cp35m-win_amd64.whlpip3 install torchvisionWith GPU Supportpip3 install http://download.Pytorch.org/whl/cu80/torch-0.4.0-cp35-cp35m-win_amd64.whl pip3 install torchvisionINSTALL ON LINUXCPU Only:pip3 install torch torchvisionWith GPU Supportpip3 install http://download.Pytorch.org/whl/cpu/torch-0.4.0-cp35-cp35m-linux_x86_64.whlpip3 install torchvisionINSTALL ON OSXCPU Only:pip3 install torch torchvisionWith GPU SupportVisit Pytorch.org for instructions regarding installing with gpu support on OSX.Note: To run experiments in this post, you should have a cuda capable GPU. If you don’t, NO PROBLEM! Visit colab.research.google.com to get a cloud based gpu accelerated vm for free.The models we’ll be using in this post belong to a class of neural networks called Convolutional Neural Networks (CNN). A CNN is primarily a stack of layers of convolutions, often interleaved with normalization and activation layers. The components of a convolutional neural network is summarized below.CNN — A stack of convolution layersConvolution Layer — A layer to detect certain features. Has a specific number of channels.Channels — Detects a specific feature in the image.Kernel/Filter — The feature to be detected in each channel. It has a fixed size, usually 3 x 3.To briefly explain, a convolution layer is simply a feature detection layer. Every convolution layer has a specific number of channels; each channel detects a specific feature in the image. Each feature to detect is often called a kernel or a filter. The kernel is of a fixed size, usually, kernels of size 3 x 3 are used.For example, a convolution layer with 64 channels and kernel size of 3 x 3 would detect 64 distinct features, each of size 3 x 3.Models are defined in PyTorch by custom classes that extend the Module class. All the components of the models can be found in the torch.nn package. Hence, we’ll simply import this package. Here we’ll build a simple CNN model for the purpose of classifying RGB images from the CIFAR 10 dataset. The CIFAR10 dataset consists of 50,000 training images and 10,000 test images of size 32 x 32.In the code above, we first define a new class named SimpleNet, which extends the nn.Module class. In the constructor of this class, we specify all the layers in our network. Our network is structured as convolution — relu — convolution — relu — pool — convolution — relu — convolution — relu — linear.To clarify what is happening in each layer, let’s go over them one by one.Given that our input would be RGB images which have 3 channels (RED-GREEN-BLUE), we specify the number of in_channels as 3. Next we want to apply 12 feature detectors to the images, so we specify the number of out_channels to be 12. Here we use the standard 3 x 3 kernel size (defined simply as 3). The stride is set to 1, and should always be so, unless you plan to reduce the dimension of the images. By setting the stride to 1, the convolution would move 1 pixel at a time. Lastly, we set the padding to be 1: this ensures our images are padded with zeros to keep the input and output size the same.Basically, you need not worry much about the stride and padding at present. Keep your focus on the in_channels and out_channels.Note that the out_channels in this layer, serves as the in_channels in the next layer, as seen below.Hungry for more? Join over 14,000 machine learners and data scientists who receive the latest and greatest in deep learning in their inbox each week.This is the standard ReLU activation function, it basically thresholds all incoming features to be 0 or greater. In simple English, when you apply relu to the incoming features, any number less than 0 is changed to zero, while others are kept the same.This layer reduces the dimension of the image by setting the kernel_size to be 2, reducing our image width and height by a factor of 2. What it essentially does is take the maximum of the pixels in a 2 x 2 region of the image and use that to represent the entire region; hence 4 pixels become just one.The final layer of our network would almost always be the linear layer. It’s a standard, fully connected layer that computes the scores for each of our classes — in this case ten classes.Note that we have to flatten the entire feature map in the last conv-relu layer before we pass it into the image. The last layer has 24 output channels, and due to 2 x 2 max pooling, at this point our image has become 16 x 16 (32/2 = 16). Our flattened image would be of dimension 16 x 16 x 24. We do this with the code:In our linear layer, we have to specify the number of input_features to be 16 x 16 x 24 as well, and the number of output_features should correspond to the number of classes we desire.Note the simple rule of defining models in PyTorch. Define layers in the constructor and pass in all inputs in the forward function.That hopefully gives you a basic understanding of constructing models in PyTorch.The code above is cool but not cool enough — if we were to write very deep networks, it would look cumbersome. The key to cleaner code is modularity. In the above example, we could put convolution and relu in one single separate module and stack much of this module in our SimpleNet.To do that, we first define a new module as belowConsider the above as a mini-network meant to form a part of our larger SimpleNet.As you can see above, this Unit consists of convolution-batchnormalization-relu.Unlike in the first example, here I included BatchNorm2d before ReLU. Batch Normalization essentially normalizes all inputs to have zero mean and unit variance. It greatly boosts the accuracy of CNN models.Having defined the unit above, we can now stack many of them together.That’s a whole 15 layer network, made up of 14 convolution layers, 14 ReLU layers, 14 batch normalization layers, 4 pooling layers, and 1 Linear layer, totalling 62 layers! This was made possible through the use of sub-modules and the Sequential class.The above code is made up of a stack of the unit and the pooling layers in between.Notice how I made the code more compact by putting all layers except the fully connected layer into a sequential class. This further simplifies the code in the forward function.Also the AvgPooling layer after the last unit computes the average of all activations in each channel. The output of the unit has 128 channels, and after pooling 3 times, our 32 x 32 images have become 4 x 4. We apply the AvgPool2D of kernel size 4, turning our feature map into 1 x 1 x 128.Consequently, the linear layer would have 1 x 1 x 128 = 128 input features.We also flatten the output of the network to have 128 features.Data loading is very easy in PyTorch thanks to the torchvision package. To demonstrate this, I’ll be loading the CIFAR10 dataset that we’ll make use of in this tutorial.First we need three additional import statementsTo load the dataset we do the following:We do this for the training set as below:First we pass an array of transformations using transform.Compose. RandomHorizontalFlip randomly flips the images horizontally. RandomCrop randomly crops the images. Below is an example of horizontal flipping.Lastly, the two most important; ToTensor converts the images into a format usable by PyTorch. Normalize with the values given below would make all our pixels range between -1 to +1. Note that when stating the transformations, ToTensor and Normalize must be last in the exact order as defined above. The primary reason for this is that the other transformations are applied on the input which is a PIL image, however, this must be converted to a PyTorch tensor before applying normalization.Data Augmentation helps the model to classify images properly irrespective of the perspective from which it is displayed.Next, we load the training set using the CIFAR10 class, and finally we create a loader for the training set, specifying a batch size of 32 images.This is repeated for the test set as below, except that the transformations only include ToTensor and Normalize. We do not apply other types of transformations on the test set.The first time you run this code, the dataset of about 170 mb would be downloaded to your system.Training neural networks with PyTorch is a very explicit process that gives you full control over what happens during training. Let’s go over the process step by step.You should import the Adam optimizer as:from torch.optim import AdamStep 1: Instantiate the Model, create the optimizer and Loss functionStep 2: Write a function to adjust learning rates# Create a learning rate adjustment function that divides the learning rate by 10 every 30 epochsThis function essentially divides the learning rate by a factor of 10 after every 30 epochs.Step 3: Write functions to save and evaluate the model.To evaluate the accuracy of the model on the test set, we iterate over the test loader. At each step, we move the images and labels to the GPU, if available and wrap them up in a Variable. The images are passed into the model to obtain predictions. The maximum prediction is picked and then compared to the actual class to obtain the accuracy. Finally we return the average accuracy.Step 4: Write the training functionThe training function above is highly annotated; however, you might still be confused by a few things. It’s important to review exactly what happens above in detail.First we loop over the loader for the training set:Next, if GPU support is available, we move both the images and labels to the GPU:The next line is to clear all currently accumulated gradients.This is important because weights in a neural network are adjusted based on gradients accumulated for each batch, hence for each new batch, gradients must be reset to zero, so images in a previous batch would not propagate gradients to a new batch.In the next steps, we pass our images into the model. It returns the predictions, and then we pass both the predictions and actual labels into the loss function.We call loss.backward() to propagate the gradients, and then we call optimizer.step() to modify our model parameters in accordance with the propagated gradients.These are the main steps in the training.The rest of the code is to compute the metrics:Here we retrieve the actual loss and then obtain the maximum predicted class. Finally, we sum up the number of correct predictions in the batch and add it to the total train_acc.After each epoch, we call the learning rate adjustment function, compute the average of the training loss and training accuracy, find the test accuracy, and log the results.More importantly, we keep track of the best accuracy, and if the current test accuracy is greater than our current best, we’d call the save models function.The complete code on GitHub is here:Run this code — you should have over 90% test accuracy after about 35 epochs.After models are trained, they can be used to perform inference on new images.To perform inference, you need to go through the following steps:To illustrate this, we’ll use the SqueezeNet model with pre-trained ImageNet weights. This allow us to take nearly any image and get the prediction for it. Since the ImageNet model has 1000 classes, a lot of different kinds of objects are supported.Torchvision provides predefined models, covering a wide range of popular architectures.First, import all needed packages and classes and create an instance of the SqueezeNet model.Note that, in the above code, by setting pre trained to be true, the SqueezeNet model would be downloaded the first time you run this function. The size of the model is just 4.7 mb.Next, create a prediction function as below:The code above contains the same components we used during training and evaluation. See the comments in the above code for clarity.Finally, in the main function that runs our prediction, we should download an image from the web and store it on disk. We should also download the class map that maps all class indexes to actual class names. This is because our model would return the index of the predicted class, depending on how the class names are encoded, the actual names would then be retrieved from the index-class map.Afterwards, we run the predict function using the saved image, and we use the saved class map to obtain the exact class name.Here is the complete code for inference:The image in the example above is the picture of this bird:The image was taken from the ImageAI repository. If you want to use your own custom network, i.e. the SimpleNet you just created, to perform inference, all you need to do is to replace the model loading section with this.Note that if your model was trained on ImageNet, then your num_classes must be 1000 instead of 10.All other aspects of the code remain the same with just one difference — if we’re running prediction with a model trained on cifar10, then in the transforms, change transforms.CenterCrop(224) to transforms.Resize(32)However, if your model was trained on ImageNet, this change should not be done.Hope you have had a nice ride with PyTorch! This post is the first in a series I’ll be writing on PyTorch. Stay connected for more and give a clap!You can always reach to me on twitter: @johnolafenwaDiscuss this post on Hacker NewsEditor’s Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We’re committed to supporting and inspiring developers and engineers from all walks of life.Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don’t sell ads.If you’d like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Comet Newsletter), join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.",17/05/2018,11,40.0,24.0,1057.0,525.0,6.0,2.0,0.0,27.0,en
3823,Music Genre Classification Using CNN,Clairvoyant Blog,Arsh Chowdhry,10.0,8.0,1160.0,"“If Music is a Place — then Jazz is the City, Folk is the Wilderness, Rock is the Road, Classical is a Temple.” — Vera NazarinWe’ve all used some music streaming app to listen to music. But what is the app's logic for creating a personalized playlist for us?One general example of logic is by having a Music Genre Classification System.Music genre classification forms a basic step for building a strong recommendation system.The idea behind this project is to see how to handle sound files in python, compute sound and audio features from them, run Machine Learning Algorithms on them, and see the results.In a more systematic way, the main aim is to create a machine learning model, which classifies music samples into different genres. It aims to predict the genre using an audio signal as its input.The objective of automating the music classification is to make the selection of songs quick and less cumbersome. If one has to manually classify the songs or music, one has to listen to a whole lot of songs and then select the genre. This is not only time-consuming but also difficult. Automating music classification can help to find valuable data such as trends, popular genres, and artists easily. Determining music genres is the very first step towards this direction.For this project, the dataset that we will be working with is GTZAN Genre Classification dataset which consists of 1,000 audio tracks, each 30 seconds long. It contains 10 genres, each represented by 100 tracks.The 10 genres are as follows:The dataset has the following folders:Let’s now dive into the code part of the project…This returns the Data Types in the Dataframe. It is used to help us understand what data we’re dealing with in the dataset.First, we’ll drop the first column ‘filename’ as it is unnecessary:Through this code:data, sr = librosa.load(audio_recording)It loads and decodes the audio as a time series y.sr = sampling rate of y. It is the number of samples per second. 20 kHz is the audible range for human beings. So it is used as the default value for sr. In this code we are using sr as 45600Hz.Librosa is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems. By using Librosa, we can extract certain key features from the audio samples such as Tempo, Chroma Energy Normalized, Mel-Freqency Cepstral Coefficients, Spectral Centroid, Spectral Contrast, Spectral Rolloff, and Zero Crossing Rate. For further information on this, please visit here.2. Python.display.AudioWith the help of IPython.display.Audio we can play audio in the notebook. It is a library used for playing the audio in the jupyterlab. The code is below:The following methods are used for visualizing the audio files we have:Waveforms are visual representations of sound as time on the x-axis and amplitude on the y-axis. They are great for allowing us to quickly scan the audio data and visually compare and contrast which genres might be more similar than others. For deeper knowledge on this please visit here.A spectrogram is a visual way of representing the signal loudness of a signal over time at various frequencies present in a particular waveform. Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time.Spectrograms are sometimes called sonographs, voiceprints, or voicegrams. When the data is represented in a 3D plot, they may be called waterfalls. In 2-dimensional arrays, the first axis is frequency while the second axis is time.The vertical axis represents frequencies (from 0 to 10kHz), and the horizontal axis represents the time of the clip.Spectral Rolloff is the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lieslibrosa.feature.spectral_rolloff computes the rolloff frequency for each frame in a signal.It is a powerful tool for analyzing music features whose pitches can be meaningfully categorized and whose tuning approximates to the equal-tempered scale. One main property of chroma features is that they capture harmonic and melodic characteristics of music while being robust to changes in timbre and instrumentation. For more information visit here.Zero crossing is said to occur if successive samples have different algebraic signs. The rate at which zero-crossings occur is a simple measure of the frequency content of a signal. Zero-crossing rate is a measure of the number of times in a given time interval/frame that the amplitude of the speech signals passes through a value of zero.Through the librosa library, we can get the count of zero crossings in the audio:Preprocessing of data is required before we finally train the data. We will try and focus on the last column that is ‘label’ and will encode it with the function LabelEncoder() of sklearn.preprocessing.We can’t have text in our data if we’re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model. To convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class. For further information visit here.fit_transform(): Fit label encoder and return encoded labels.Standard scaler is used to standardize features by removing the mean and scaling to unit variance.The standard score of sample x is calculated as:z = (x - u) / sStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data.Now comes the last part of the music classification genre project. The features have been extracted from the raw data and now we have to train the model. There are many ways through which we can train our model. Some of these approaches are:For this blog, we will be using CNN Algorithm for training our model. We chose this approach because various forms of research show it to have the best results for this problem.The following chart gives a clear view of why CNN algorithm is used:For further information please visit this site.For the CNN model, we had used the Adam optimizer for training the model. The epoch that was chosen for the training model is 600.All of the hidden layers are using the RELU activation function and the output layer uses the softmax function. The loss is calculated using the sparse_categorical_crossentropy function.Dropout is used to prevent overfitting.We chose the Adam optimizer because it gave us the best results after evaluating other optimizers.The model accuracy can be increased by further increasing the epochs but after a certain period, we may achieve a threshold, so the value should be determined accordingly.The accuracy we achieved for the test set is 92.93 percent which is very decent.So we come to the conclusion that Neural Networks are very effective in machine learning models. Tensorflow is very useful in implementing Convolutional Neural Network (CNN) that helps in the classifying process.Hope you enjoyed the post and found it informative.https://towardsdatascience.com/extract-features-of-music-75a3f9bc265dhttps://medium.com/bisa-ai/music-genre-classification-using-convolutional-neural-network-7109508ced47https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8",07/05/2021,0,25.0,3.0,1369.0,440.0,27.0,3.0,0.0,13.0,en
3824,Multi-label Text Classification using BERT – The Mighty Transformer,HuggingFace,Kaushal Trivedi,1000.0,7.0,1270.0,"The past year has ushered in an exciting age for Natural Language Processing using deep neural networks. Research in the field of using pre-trained models have resulted in massive leap in state-of-the-art results for many of the NLP tasks, such as text classification, natural language inference and question-answering.Some of the key milestones have been ELMo, ULMFiT and OpenAI Transformer. All these approaches allow us to pre-train an unsupervised language model on large corpus of data such as all wikipedia articles, and then fine-tune these pre-trained models on downstream tasks.Perhaps the most exciting event of the year in this area has been the release of BERT, a multilingual transformer based model that has achieved state-of-the-art results on various NLP tasks. BERT is a bidirectional model that is based on the transformer architecture, it replaces the sequential nature of RNN (LSTM & GRU) with a much faster Attention-based approach. The model is also pre-trained on two unsupervised tasks, masked language modeling and next sentence prediction. This allows us to use a pre-trained BERT model by fine-tuning the same on downstream specific tasks such as sentiment classification, intent detection, question answering and more.In this article, we will focus on application of BERT to the problem of multi-label text classification. Traditional classification task assumes that each document is assigned to one and only on class i.e. label. This is sometimes termed as multi-class classification or sometimes if the number of classes are 2, binary classification.On other hand, multi-label classification assumes that a document can simultaneously and independently assigned to multiple labels or classes. Multi-label classification has many real world applications such as categorising businesses or assigning multiple genres to a movie. In the world of customer service, this technique can be used to identify multiple intents for a customer’s email.We will use Kaggle’s Toxic Comment Classification Challenge to benchmark BERT’s performance for the multi-label text classification. In this competition we will try to build a model that will be able to determine different types of toxicity in a given text snippet. The types of toxicity i.e. toxic, severe toxic, obscene, threat, insult and identity hate will be the target labels for our model.Google Research recently open-sourced the tensorflow implementation of BERT and also released the following pre-trained models:We will use the smaller Bert-Base, uncased model for this task. The Bert-Base model has 12 attention layers and all text will be converted to lowercase by the tokeniser. We are running this on an AWS p3.8xlarge EC2 instance which translates to 4 Tesla V100 GPUs with total 64 GB GPU memory.I personally prefer using PyTorch over TensorFlow, so we will use excellent PyTorch port of BERT from HuggingFace available at https://github.com/huggingface/pytorch-pretrained-BERT. We have converted the pre-trained TensorFlow checkpoints to PyTorch weights using the script provided within HuggingFace’s repo.Our implementation is heavily inspired from the run_classifier example provided in the original implementation of BERT.The data will be represented by class InputExample.We will convert the InputExample to the feature that is understood by BERT. The feature will be represented by class InputFeatures.BERT-Base, uncased uses a vocabulary of 30,522 words. The processes of tokenisation involves splitting the input text into list of tokens that are available in the vocabulary. In order to deal with the words not available in the vocabulary, BERT uses a technique called BPE based WordPiece tokenisation. In this approach an out of vocabulary word is progressively split into subwords and the word is then represented by a group of subwords. Since the subwords are part of the vocabulary, we have learned representations an context for these subwords and the context of the word is simply the combination of the context of the subwords. For more details regarding this approach please refer Neural Machine Translation of Rare Words with Subword Unitshttps://arxiv.org/pdf/1508.07909.P.S. This in my opinion is as important a breakthrough as BERT itself.We will adapt BertForSequenceClassification class to cater for multi-label classification.The primary change here is the usage of Binary cross-entropy with logits (BCEWithLogitsLoss) loss function instead of vanilla cross-entropy loss (CrossEntropyLoss) that is used for multiclass classification. Binary cross-entropy loss allows our model to assign independent probabilities to the labels.The model summary is shows the layers of the model alongwith their dimensions.The training loop is identical to the one provided in the original BERT implementation in run_classifier.py. We trained the model for 4 epochs with batch size of 32 and sequence length as 512, i.e. the maximum possible for the pre-trained models. The learning rate was kept to 3e-5, as recommended in the original paper.We had the opportunity to use multiple GPUs. so we wrapped the Pytorch model inside DataParallel module. This allows us to spread our training job across all the available GPUs.We did not use half precision FP16 technique as for some reason, binary crosss entropy with logits loss function did not support FP16 processing. This doesn’t really affect the end result, it simply takes a bit longer to train.We adapted the accuracy metric function to include a threshold, which is set to 0.5 as default.For multi-label classification, a far more important metric is the ROC-AUC curve. This is also the evaluation metric for the Kaggle competition. We calculate ROC-AUC for each label separately. We also use micro-averaging on top of individual labels’ roc-auc scores.I would recommend reading this excellent blog to get a deeper insight on the roc-auc curve.We ran a few experiments with a few variations but more of less got similar results. The outcome is as listed below:Training Loss: 0.022, Validation Loss: 0.018, Validation Accuracy: 99.31%ROC-AUC scores for the individual labels:toxic: 0.9988severe-toxic: 0.9935obscene: 0.9988threat: 0.9989insult: 0.9975identity_hate: 0.9988Micro ROC-AUC: 0.9987The result seems to be quite encouraging as we seems to have created a near perfect model for detecting toxicity of a text comment. Now lets see how we score against the Kaggle leaderboard.We ran inference logic on the test dataset provided by Kaggle and submitted the results to the competition. The following was the outcome:We scored 0.9863 roc-auc which landed us within top 10% of the competition. To put this result into perspective, this Kaggle competition had a price money of $35000 and the 1st prize winning score is 0.9885.The top scores are achieved by teams of dedicated and highly skilled data scientists and practitioners. They use various techniques as such ensembling, data augmentation and test-time augmentation in addition to what we have done so far.We have tried to implement the multi-label classification model using the almighty BERT pre-trained model. As we have shown the outcome is really state-of-the-art on a well-known published dataset. We were able to build a world class model that can be used in production for various industries, especially in customer service.For us, the next step will be to fine tune the pre-trained language models by using the text corpus of the downstream task using the masked language model and next sentence prediction tasks. This will be an unsupervised task and hopefully will allow the model to learn some of our custom context and terminologies. This is similar technique used by ULMFiT. I will share The outcome in another blog so do watch out for it.I have shared most of the code for this implementation in the code gist. However I will merge my changes back to HuggingFace’s github repo.I would encourage you all to implement this technique on your own custom datasets and would love to hear some stories.I would love to hear back from all. Also please feel free to contact me using LinkedIn or Twitter.I have made available the jupyter notebook for this article. Note that this is an interim option and this work will be merged into HuggingFace’s awesome pytorch repo for BERT.nbviewer.jupyter.orggithub.com",27/01/2019,0,21.0,9.0,700.0,228.0,3.0,5.0,0.0,19.0,en
3825,Google DeepMind-style datacenter optimization AI model (on the cheap),Medium,commander,23.0,6.0,1225.0,"There was news recently in bloomberg about how google was able to cut electricity usage in its datacenter by using an AI scheme made by DeepMind (of AlphaGo fame). Earlier this week, i decided to make a quick-and-dirty implemetation in python and share it here for anyone interested in a practical example of what exactly they did. First lets take a quick look at why one would want to make such a thing...Datacenters (and indeed any other large scale structures that use a lot of energy) need to be carefully optimized for efficiency as even a 10% - 15% saving on the electricity bill can add up to millions of dollars a year. The biggest challenge here is that even though there are certain simple steps that anyone can take to reduce energy use (don’t use a very low server room set-point, use free-cooling when possible, etc…) one can never actually predict quantitatively what the effect of changing variable x by z% will have on total consumption. This is because there simply are too many variables that affect the net consumption of a datacenter (chillers, AHUs, compressors, condensers, fans, outside conditions, latitude, etc…) and its impossible to actually write down a formula that can quantify all these relationships.However, as long as you have a lot of data, ML is perfect for learning complex relationships between multiple features and outcomes. So, first let’s have a look at the available data. (I work in the industry so I was fortunate enough to have these data handy).For my feature set, I decided to use 9 input variables from my datacenter, which are: chiller load, pump load, AHU load, condenser load, IT load, outside air temperature, outside air humidity, wind speed, and wind direction. Loads are in kW, temperatures in degree F, humidity in percentage, wind speed in miles/hour and wind direction degrees. Furthermore, all metered data is measured at 5 min intervals, while outisde conditions are recorded on-change. Note: the DeepMind model uses 19 inputs, the list of which can be found in their technical paper. While DeepMind chose to optimize the data center’s PUE, I decided its easier to just optimize the total data center electricity consumption. But one can use any relevant time series data here, depending on what one wishes to model.Its important to have good quality data. At the very least, data points should line up (time wise)and be of the same size. IoT sensors are typically bad at this, so you’ll have to do some wranggling to get everything nice and orderly. My sensor data have random missing values and false peaks at different time stamps. For the purposes of this simple demo, I decided to resample the data to a common index of timestamps from start_time to end_time spaced 5 minutes apart, which I generated algorithmically using the datetime package in python 3.5. Then all sensor data were resampeled into a pandas dataframe to this common index by using nearest matching values from the raw data, and using interpolate/ffill/bfill (take your pick) where data is missing. Here’s sample code:where the sample_to_std() function implements something like the following:note: there are more efficient ways of writing these functions, this is only meant for clarity.Next, we combine all input data into one data frame using concat:We can then confirm that the input dataframe behaves as expected:Before we actually build a deep learning model, its worth running a simple regression to try to predict the output just so we establish a baseline score. Here I use a simple linear regression using the linear_model module from scikit-learn. First let’s setup the cross-validation scheme using 10 iterations, and a train/test split of 80/20:next, we’ll set up a function that will allow us to test and score different types regression algorithms in just one line:The main thing to note here is that we’re using the Pipeline function from scikit-learn to string together a StandardScaler and a regressor. Now all we have to do is setup a regressor and pass it to the test_algo() function like so:You’ll note from the test_algo() function that we are using mean absolute error to score these functions. For the linear model, I get a mean absolute error of 16.48.Let’s try another quick one just for fun. This time, a K nearest neighbor regressor from scinkit-learn:The plot shows a scatter plot of predicted Vs actual consumption on the test portion of the data, and overlays a simple y = x line as a visual guide to show the match. This nearest neighbor model returns a mean absolute error of 5.78 kW. Not bad given that total consumption is in the range of 300 to 500 kW. With the baseline established, let’s move on to the neural networks.I will use the Keras neural networks library to build my model. It is dead simple to work with, and uses a Theano backend (TensorFlow backend also available). Unfortunately, Keras doesn’t accept data in the pandas DataFrame format, so we’ll have to convert our data to numpy array format. Thankfully, pandas makes this exceedingly easy. It takes just one line of code:Aside from changing variable names, our cross validation and testing scheme will not change at all. Let’s start by building a basic perceptron. Keras makes this easy by providing a Sequential() class that easily allows one to stack on many layers of neurons. Since we will use a scikit-learn wrapper to make the scoring easy and compatible with the rest of our code, we’ll first write a container function for our Keras model:The first layer we added is a Dense (i.e. fully connected) layer with 9 units, input shape of 9 (same as out feature length), initialized using the ‘normal’ scheme and with relu activation. We then simply add the output layer with 1 unit with no activation (since we’d like to see the regressed raw predictions). The function simply returns this model after a model.compile() step where we specify the loss function to optimize and a suitable optimizer. The different options are enumerated on the Keras guide. The sk-learn wrapper class we use is called KerasRegressor and has the following configuration:Running this model yields a mean absolute error of 14.07, which isn’t too bad but seems to be outperformed by the nearest neighbor estimator. This suggests that there might be room for improvement. I leave further optimizations to the reader, but one obvious way to do this would be to expand the topology of the neural net by adding more layers (depth) or by using wider layers (width) or both, like so:This model has 4 layers (30, 40, 40, 30 units respectively). With this configuration, I was able to achieve a mean absolute error of 8.34 kW.update: here are results for a model with 4 hidden layers with 50 units each.It seems that the nearest neighbor estimator is hard to beat in this case.Note: I’m only using 5 months of data (44065 time stamps in total) on 9 features, whereas the DeepMind model uses 2 years worth of data (same 5 min resolution) on 19 features. Their neural net uses 5 hidden layers with 50 units each. So I expect their code to be more robust. They also clean their data and take care to eliminate feature collinearity. But even with faily unclean data and a quick-and-dirty ANN model, its nice to get single digit errors :-)Cheers,",17/08/2016,11,9.0,11.0,1244.0,695.0,3.0,0.0,0.0,4.0,en
3826,The Volcker metric known as inventory aging… and thoughts of Whisky,Acuity Derivatives,Acuity Derivatives,21.0,2.0,397.0,"Inventory Aging is a rather innocuous looking member of the band of (now) seven metrics that, under the Volcker rule, banking entities with significant trading assets and liabilities are required to calculate daily and report monthly.As written, the metric description seems straightforward enough:Inventory Aging generally describes a schedule of the trading desk’s aggregate assets and liabilities and the amount of time that those assets and liabilities have been held. [It] should measure the age profile of the trading desk’s assets and liabilities and must include two schedules, an asset- aging schedule and a liability-aging schedule.The graphic below broadly outlines the processes of asset/liability tagging, matching, sorting and netting of trades involved in generating an inventory aging schedule.Straightforward enough for standardized instruments where the product can be unambiguously described such that its quantities (i.e. how much is held and for how long it has been held) can be reasonably easily established. For OTC derivatives not so much; and for complex OTC derivatives, well that is when one’s thoughts wander to Whisky.And an analogy…© Acuity Derivatives LLC, 2014A complex OTC derivative represents a complex composite of stochastic interactions between a great many variables, that together unambiguously describe it. Not unlike Whisky. And like Whisky, an unambiguous description is almost impossible to be had; but is best approximated via a robust classification scheme. At the TSAM Technology and Operational Strategy conference in June this Whisky analogy was used — perhaps to distraction in a late presentation session on a hot summer afternoon.The TSAM presentation itself may be viewed and downloaded below.[gview file=”http://acuityderivatives.com/wp-content/uploads/2014/11/AcuityDerivatives-ClassificationPresentation-s.pdf""]As has been described in earlier discussions of the topic, classification attempts to arrange traded financial derivatives into product classes or groups based on similar or related properties (similarity of properties as meaningful within some context). Here, risk and valuation materiality form the classification context.In this context, classification is less about a normative way of naming products and more about a framework for ordering the properties of financial derivative contracts in such a way that they can be grouped around the types of risk sensitive behavior they are likely to exhibit; so risk similar products are treated consistently e.g. for netting within a Volcker metrics or similar risk reporting framework that relies on aggregation and similarity.Inventory Aging is a metric that teases out the subtlety of this complexity. In the post-regulatory world of aggregated risk, there are several more.",01/08/2014,0,1.0,1.0,896.0,856.0,2.0,0.0,0.0,0.0,en
3827,Prior over functions: Gaussian process,Towards Data Science,Jehill Parikh,109.0,8.0,1380.0,"In this post we discuss working of Gaussian process. Gaussian process fall under kernel methods, and are model free. Gaussian process are specially useful for low data regimen to “learn” complex functions. We shall review a very practical real world application (not related to deep learning or neural networks). The discussion follows from the talks of subject matter experts Prof Neil Lawrence and Prof Richard Tuner.Background reading:Multivariate gaussian distribution: A Gaussian distribution can be specified using a mean (u), variance (σ2) and probability distribution function (PDF) as shown belowIf we have more than one independent gaussian distribution we can combine them. The combined PDF is also Gaussian i.e. a multivariate Gaussian. E.g. of a two Gaussian is shown below for two Gaussian (i.e. bivariate)In general PDF of multivariate gaussian can be specific by means, variance (or standard deviation) and covariance matrix (Σ)Properties of Multivariate (MV) gaussianIf you would like read the proof or just want the equations, please see them here or here and summary is here. These are very important to review as most math related to gaussian processes is application of these fundamental results.Bayes rules: states that posterior probability is prior probability time the likelihood i.e.Posterior, P(H|E) = (Prior P(H) * likelihood P(E|H))| Evidence P(E)Where H is some hypothesis and E is evidence.Gaussian process: We start our discussion with linear regression approach. For some input (x), we can writef(x): One form of this equation is slope intercept form (ignore noise term, we generally, we add it to weights w)Now lets change w1 (slope) and w2 (intercept) and see what the results are (demos/gifs may take time to load)We got a range of lines! What are we trying to achieve in parametric models such e.g. here, linear regression, is to estimates w (or beta’s for linear regression) which best describe our data, see the maths here.In Bayesian linear regression, we place priors of weight, w1 and w2, and then obtain posterior, once we observe (x,y) i.e. training setUsing baye’s rule this isw1 and w2 can be combined into a matrix and be drawn from a bivariate Gaussian distribution. In general, “n” number of weights (beta) “w” can be drawn for a multivariate distribution gaussianPosterior: weights “w” which best describe our training set (x,y)Likelihood: function which is mostly like i.e. p(y|x,w) for a given weight and training input xMarginal likelihood is normalisation constant which is an integral over all priors and all likelihoodsAfter obtaining posterior “w”, we can employ them on new data i.e. data not in our training set (x,y), these are obtained from marginalisation property of the Gaussian.For new observation we have x* and want to obtain (f*) or y*, which using Bayesian ruleLinear to Nonlinear: So far we assumed that our function is linear and w want to obtain “w”. For non-linear and general case, we can replace x, with ϕ(x), to transforms x to a feature space where we can work with linearly. And then apply Bayesian inferences to obtain f* or y* using ϕ(x*), Loosely speaking transformation is what changes the linear model to gaussian process. Maths although relatively straight forward it is hard to assess what is going on. It generally ends by specifying a kernel function or a covariance function which work on input X. How the above equations are related to a kernel function is really hard to understand, and requires some effort. To make things simpler we turn to intuition. The example discussed here is from Richard Tuner’s tutorialAs Dr Tuner explains lets start with two correlated Gaussian’s y1 and y2 , plot their height (of random draws) on a chart to obtain a new representation. If we look at the two 2D gaussian from the “top” above it can produces a contour plot. We are assuming here that these two gaussian are very highly correlated as show by the covariance matrix, means are zero.Now we draw a few samples from these two Gaussian and plot it as show belowNow we fix one of the gaussian then draw samples from the second correlated gaussian, this limits the second samples from a smaller gaussian shown in red.We can repeat it for 20 dimensional gaussian, now the covariance matrix is the represented by a “function” where correlation is strong diagonally but weak otherwiseHere fixing the two points we are controlling the “wriggling”In general this can be done for an infinite dimensional gaussian. Where each of the “y” is specified a via a kernel function K which work on the any input (x), plus so noise. I is identity matrix. Here we just select the necessary section of the covariance matrix KThe Kernel function or the covariance function K is the “trick” that allows us to apply the Bayesian framework we developed with linear model to any function. It is related to weigh space view of Bayesian linear regression to “function space” view via a ϕ: basis or activation function which transform input x to a function spaceϕ(x) is related to kernel function as a result of the technical condition we impose on covariance matrix (Σ). Σ is positive semidefinite and thus, can we can apply SVD, and as a result we can writeThis Kernel trick allows to work directly with inputs in the function space. Therefore kernel function is more important as it works directly on the inputs we don’t need to specify weights “w” or ϕ(x)The most common kernel function are shown belowIn the function space view, we writeGenerally mean m(x) is zero: you can think of this work on batch normalised data.Definition: A gaussian process is defined by a collection of (infinite) random variable, specified via a covariance function K.Prior: When we draw prior samples from a GP we can obtain arbitrary function samples, as shown below.Posterior: With our training dataset (x,y) we can then obtain the posterior (y or f(x), since y=f(x)+noise). Here we limiting to the functions which only pass through our training data points, see the fixing of “y"" example in two animationsPredictive distribution is also can obtained by using relevant sub matrix from the infinite covariance matrix specific via K. The input here is X* to obtain the likelihoods (y and f*) .As everything is gaussian, we can apply marginalisation property, to obtain the final gaussian using Bayes rule, this gives mean prediction function and uncertainty via the variance.Intuition: Loosely speaking the intuition to keep in mind for posterior and predictive distribution, is simpler to figures where we control wriggling, where red dot’s are our training examples (x,y), we have “fixed” them, and select from other Gaussian which can vary. At the time of predict we even wriggle “less” as we have fix more data points (x plus x*,y) so reduce uncertainties further.There are many technicalities which have been skipped so it is important to read Rasmussen & Williams to appreciate all of them, we have therefore follow the same convention. This post was just intended to provide an overview to this rich subject and to facilitate reading of literature we make extensive use of their work.Implementation: In practice we have to choose the right kernel (covariance function) with it parameters and apply cholskey decomposition but this is computational expensive. A note on implementation issue available here. To over come such limitation Variational methods can also be employed in frameworks like tensorflow probability.Applications: GP allow estimate of uncertainty in complex functions, and thus are widely used. The one outlined below is a personal favourite, in medical imaging.GP’s are central to post-processing of diffusion MRI data. The fsl tool “eddy” is based on GP. It is a vital step to obtain accurate connections (white matter tracks) of human brain. An example (low resolution) showing major white tracks is shown belowThe model free methods also employed for reducing MRI scan acquisition to reduce scan time in research. Here the MRI diffusion signal is model via a gaussian process, this model is then employed to reconstruct the missing acquisitions in the k-q space as show here. This opens the door for much sophisticate acquisitions which can’t be routinely performed clinically as they are too long and only limited for research work. Due to specialised nature where one needs a lot domain knowledge we not discussing technical details, but do read the paper to get flavour of the work and complexities.References:",07/09/2019,0,23.0,20.0,644.0,290.0,30.0,2.0,0.0,23.0,en
3828,1000x Faster Spelling Correction algorithm (2012),Medium,Wolf Garbe,412.0,6.0,1105.0,"Update1: An improved SymSpell implementation is now 1,000,000x faster.Update2: SymSpellCompound with Compound aware spelling correction. Update3: Benchmark of SymSpell, BK-Tree und Norvig’s spell-correct.Recently I answered a question on Quora about spelling correction for search engines. When I described our SymSpell algorithm I was pointed to Peter Norvig’s page where he outlined his approach.Both algorithms are based on Edit distance (Damerau-Levenshtein distance). Both try to find the dictionary entries with smallest edit distance from the query term.If the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. But SymSpell uses a different way to search the dictionary, resulting in a significant performance gain and language independence. Three ways to search for minimum edit distance in a dictionary:1. Naive approachThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. This exhaustive search is inordinately expensive.Source: Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze: Introduction to Information Retrieval.The performance can be significantly improved by terminating the edit distance calculation as soon as a threshold of 2 or 3 has been reached.2. Peter NorvigGenerate all possible terms with an edit distance (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary. For a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time.Source: Peter Norvig: How to Write a Spelling Corrector.This is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and language dependent (because the alphabet is used to generate the terms, which is different in many languages and huge in Chinese: a=70,000 Unicode Han characters)3. Symmetric Delete Spelling Correction (SymSpell) Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step. Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time.This is three orders of magnitude less expensive (36 terms for n=9 and d=2) and language independent (the alphabet is not required to generate deletes). The cost of this approach is the pre-calculation time and storage space of x deletes for every original dictionary entry, which is acceptable in most cases.The number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinations without repetitions, and k=n-d), E.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 dictionary entries we need to additionally store 1,500,000 deletes.The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup by using deletes only instead of deletes +transposes + replaces + inserts. It is six orders of magnitude faster (for edit distance=3) and language independent.Remark 1: During the precalculation, different words in the dictionary might lead to same delete term: delete(sun,1)==delete(sin,1)==sn. While we generate only one new dictionary entry (sn), inside we need to store both original terms as spelling correction suggestion (sun,sin)Remark 2: There are four different comparison pair types:The last comparison type is required for replaces and transposes only. But we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance (bank==bnak and bank==bink, but bank!=kanb and bank!=xban and bank!=baxn).Remark 3: Instead of a dedicated spelling dictionary we are using the search engine index itself. This has several benefits:Remark 4: We have implemented query suggestions/completion in a similar fashion. This is a good way to prevent spelling errors in the first place. Every newly indexed word, whose frequency is over a certain threshold, is stored as a suggestion to all of its prefixes (they are created in the index if they do not yet exist). As we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost. Multiple terms are sorted by the number of results stored in the index.ReasoningThe SymSpell algorithm exploits the fact that the edit distance between two terms is symmetrical:We are using variant 3, because the delete-only-transformation is language independent and three orders of magnitude less expensive.Where does the speed come from?Computational Complexity The SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1).Comparison to other approaches BK-Trees have a search time of O(log dictionary_size), whereas the SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size. Tries have a comparable search performance to our approach. But a Trie is a prefix tree, which requires a common prefix. This makes it suitable for autocomplete or search suggestions, but not applicable for spell checking. If your typing error is e.g. in the first letter, than you have no common prefix, hence the Trie will not work for spelling correction.If you need a very fast auto-complete then try my Pruning Radix TrieApplication Possible application fields of the SymSpell algorithm are those of fast approximate dictionary string matching: spell checkers for word processors and search engines, correction systems for optical character recognition, natural language translation based on translation memory, record linkage, de-duplication, matching DNA sequences, fuzzy string searching and fraud detection.For a single user or for small edit distances other algorithms might just be fine. But for search engines and search as a service search API where you have to serve thousands of concurrent users, while still maintaining a latency of a few milliseconds, and where spelling correction is not even the main procession task, but only one of many components in query preprocessing, you need the fastest spelling correction you can get.Source codeThe C# implementation of the Symmetric Delete Spelling Correction algorithm is released on GitHub as Open Source under the MIT License:https://github.com/wolfgarbe/symspellPortsThere are ports in C++, Crystal, Go, Java, Javascript, Python, Ruby, Rust, Scala, Swift available.Originally published at seekstorm.com on June 7, 2012.",07/06/2012,0,32.0,3.0,1024.0,681.0,1.0,4.0,0.0,19.0,en
3829,Simple and multiple linear regression with Python,Towards Data Science,Amanda Iglesias Moreno,1700.0,11.0,1905.0,"Linear regression is an approach to model the relationship between a single dependent variable (target variable) and one (simple regression) or more (multiple regression) independent variables. The linear regression model assumes a linear relationship between the input and output variables. If this relationship is present, we can estimate the coefficients required by the model to make predictions on new data.In this article, you will learn how to visualize and implement the linear regression algorithm from scratch in Python using multiple libraries such as Pandas, Numpy, Scikit-Learn, and Scipy. Additionally, we will measure the direction and strength of the linear relationship between two variables using the Pearson correlation coefficient as well as the predictive precision of the linear regression model using evaluation metrics such as the mean square error.Now! Let’s get started 💜The dataset used in this article was obtained in Kaggle. Kaggle is an online community of data scientists and machine learners where it can be found a wide variety of datasets. The dataset selected contains the height and weight of 5000 males and 5000 females, and it can be downloaded at the following link:www.kaggle.comThe first step is to import the dataset using Pandas. Pandas is a Python open source library for data science that allows us to work easily with structured data, such as csv files, SQL tables, or Excel spreadsheets. After importing csv file, we can print the first five rows of our dataset, the data types of each column as well as the number of null values.As we can easily observe, the dataframe contains three columns: Gender, Height, and Weight. The Gender column contains two unique values of type object: male or female. A float data type is used in the columns Height and Weight. Since the dataframe does not contain null values and the data types are the expected ones, it is not necessary to clean the data .To better understand the distribution of the variables Height and Weight, we can simply plot both variables using histograms. Histograms are plots that show the distribution of a numeric variable, grouping data into bins. The height of the bar represents the number of observations per bin.The previous plots depict that both variables Height and Weight present a normal distribution. It can also be interesting as part of our exploratory analysis to plot the distribution of males and females in separated histograms.The previous plots show that both height and weight present a normal distribution for males and females. Although the average of both distribution is larger for males, the spread of the distributions is similar for both genders. Pandas provides a method called describe that generates descriptive statistics of a dataset (central tendency, dispersion and shape).Exploratory data analysis consists of analyzing the main characteristics of a data set usually by means of visualization methods and summary statistics. The objective is to understand the data, discover patterns and anomalies, and check assumption before we perform further evaluations. After performing the exploratory analysis, we can conclude that height and weight are normal distributed. Males distributions present larger average values, but the spread of distributions compared to female distributions is really similar.But maybe at this point you ask yourself: There is a relation between height and weight? Can I use the height of a person to predict his weight?The answer of both question is YES! 😃 💪 Let’s continue ▶️ ▶️A scatter plot is a two dimensional data visualization that shows the relationship between two numerical variables — one plotted along the x-axis and the other plotted along the y-axis. Matplotlib is a Python 2D plotting library that contains a built-in function to create scatter plots the matplotlib.pyplot.scatter() function.The following plot shows the relation between height and weight for males and females. The visualization contains 10000 observations that is why we observe overplotting. Overplotting occurs when the data overlap in a visualization, making difficult to visualize individual data points. In this case, the cause is the large number of data points (5000 males and 5000 females). Another reason can be a small number of unique values; for instance, when one of the variables of the scatter plot is a discrete variable.In the following plot, we have randomly selected the height and weight of 500 women. This plot has not overplotting and we can better distinguish individual data points. As we can observe in previous plots, weight of males and females tents to go up as height goes up, showing in both cases a linear relation.Simple linear regression is a linear approach to modeling the relationship between a dependent variable and an independent variable, obtaining a line that best fits the data.y =a+bxwhere x is the independent variable (height), y is the dependent variable (weight), b is the slope, and a is the intercept. The intercept represents the value of y when x is 0 and the slope indicates the steepness of the line. The objective is to obtain the line that best fits our data (the line that minimize the sum of square errors). The error is the difference between the real value y and the predicted value y_hat, which is the value obtained using the calculated linear equation.error = y(real)-y(predicted) = y(real)-(a+bx)We can easily obtain this line using Numpy. Numpy is a python package for scientific computing that provides high-performance multidimensional arrays objects. The numpy function polyfit numpy.polyfit(x,y,deg) fits a polynomial of degree deg to points (x, y), returning the polynomial coefficients that minimize the square error. In the following lines of code, we obtain the polynomials to predict the weight for females and males.The following plot depicts the scatter plots as well as the previous regression lines.Seaborn is a Python data visualization library based on matplotlib. We can easily create regression plots with seaborn using the seaborn.regplot function. The number of lines needed is much lower in comparison to the previous approach.The previous plot presents overplotting as 10000 samples are plotted. The plot shows a positive linear relation between height and weight for males and females. For a better visualization, the following figure shows a regression plot of 300 randomly selected samples.Scikit-learn is a free machine learning library for python. We can easily implement linear regression with Scikit-learn using the LinearRegression class. After creating a linear regression object, we can obtain the line that best fits our data by calling the fit method.The values obtained using Sklearn linear regression match with those previously obtained using Numpy polyfit function as both methods calculate the line that minimize the square error. As previously mentioned, the error is the difference between the actual value of the dependent variable and the value predicted by the model. The least square error finds the optimal parameter values by minimizing the sum S of squared errors.Once we have fitted the model, we can make predictions using the predict method. We can also make predictions with the polynomial calculated in Numpy by employing the polyval function. The predictions obtained using Scikit Learn and Numpy are the same as both methods use the same approach to calculate the fitting line.Correlation measures the extent to which two variables are related. The Pearson correlation coefficient is used to measure the strength and direction of the linear relationship between two variables. This coefficient is calculated by dividing the covariance of the variables by the product of their standard deviations and has a value between +1 and -1, where 1 is a perfect positive linear correlation, 0 is no linear correlation, and −1 is a perfect negative linear correlation.We can obtain the correlation coefficients of the variables of a dataframe by using the .corr() method. By default, Pearson correlation coefficient is calculated; however, other correlation coefficients can be computed such as, Kendall or Spearman.A rule of thumb for interpreting the size of the correlation coefficient is the following:In previous calculations, we have obtained a Pearson correlation coefficient larger than 0.8, meaning that height and weight are strongly correlated for both males and females.We can also calculate the Pearson correlation coefficient using the stats package of Scipy. The function scipy.stats.pearsonr(x, y) returns two values the Pearson correlation coefficient and the p-value.As can be observed, the correlation coefficients using Pandas and Scipy are the same:We can use numerical values such as the Pearson correlation coefficient or visualization tools such as the scatter plot to evaluate whether or not linear regression is appropriate to predict the data. Another way to perform this evaluation is by using residual plots. Residual plots show the difference between actual and predicted values. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate.We can use Seaborn to create residual plots as follows:As we can see, the points are randomly distributed around 0, meaning linear regression is an appropriate model to predict our data. If the residual plot presents a curvature, the linear assumption is incorrect. In this case, a non-linear function will be more suitable to predict the data.Simple linear regression uses a linear function to predict the value of a target variable y, containing the function only one independent variable x₁.y =b ₀+b ₁x ₁After fitting the linear equation to observed data, we can obtain the values of the parameters b₀ and b₁ that best fits the data, minimizing the square error.Previously, we have calculated two linear models, one for men and another for women, to predict the weight based on the height of a person, obtaining the following results:So far, we have employed one independent variable to predict the weight of the person Weight = f(Height) , creating two different models. Maybe you are thinking 💭 ❓ Can we create a model that predicts the weight using both height and gender as independent variables? The answer is YES! 😄 ⭐️ And here is where multiple linear regression comes into play!Multiple linear regression uses a linear function to predict the value of a target variable y, containing the function n independent variable x=[x₁,x₂,x₃,…,xₙ].y =b ₀+b ₁x ₁+b₂x₂+b₃x₃+…+bₙxₙWe obtain the values of the parameters bᵢ, using the same technique as in simple linear regression (least square error). After fitting the model, we can use the equation to predict the value of the target variable y. In our case, we use height and gender to predict the weight of a person Weight = f(Height,Gender).There are two types of variables used in statistics: numerical and categorical variables.Multiple linear regression accepts not only numerical variables, but also categorical ones. To include a categorical variable in a regression model, the variable has to be encoded as a binary variable (dummy variable). In Pandas, we can easily convert a categorical variable into a dummy variable using the pandas.get_dummies function. This function returns a dummy-coded data where 1 represents the presence of the categorical variable and 0 the absence.To avoid multi-collinearity, we have to drop one of the dummy columns.Then, we can use this dataframe to obtain a multiple linear regression model using Scikit-learn.After fitting the linear equation, we obtain the following multiple linear regression model:If we want to predict the weight of a male, the gender value is 1, obtaining the following equation:For females, the gender has a value of 0.If we compare the simple linear models with the multiple linear model, we can observe similar prediction results. The gender variable of the multiple linear regression model changes only the intercept of the line. 🙌Thanks for reading 🙌 😍 😉 🍀",27/07/2019,0,165.0,6.0,496.0,336.0,19.0,8.0,0.0,20.0,en
3830,"NER, SpaCy and Lasagne",The Startup,Paul Ellis,15.0,8.0,1466.0,"One of the great things about NER is trying to find those critters! I recently completed a project where one of the pre-requisites was to identify a location from large text fields containing randomly entered data.Of course if there’s no control during the input of data then chaos reigns but we are where we are and if someone wants to put their homemade recipe for lasagne in an address field then hey it’s going to get messy but we’ll keep the lecture notes on data entry for another time and place.For now we need to extract something that at least resembles a location and feed, no more food analogies please, the results into Geopy to grab coordinates that would be used to create cracking, as Iolo Williams would say, maps in Tableau or Seaborne because rest assured the customer always looks for that breath-taking image of distribution across infinity and beyond, particularly when an incorrect coordinate is returned.So let’s jump in and review the data. The following is an extract taken from a field that we shall label Campus and as you can see the text contains utf-8 encoding with various combinations of words and punctuation:So how can Named Entity Recognition (NER) help us with this? The answer lays in the interpretation and identification in a grammatical sentence of distinct entities. For example, if we examine the following sentence:‘I will meet Bruce Springsteen for a cup of coffee at Gregg’s in Caerphilly at 11:30 AM tomorrow.’NER attempts to interpret the sentence as distinct entities:‘I will meet (Bruce Springsteen) Person for a cup of (coffee) Food at (Gregg’s) Location in (Caerphilly) Geographic Location at (11:30 AM) Time (tomorrow) Date.We are in effect labelling the data. ‘This is called chunking or shallow parsing. Shallow parsing aims to derive higher order units composed of the grammatical atoms, like nouns, verbs, adjectives, and so on.’ (Rao & McMahan 2019[1])We could of course create a program that through a series of contrived rules would attempt to label entities but this would be wholly inefficient especially when a strict set of rules would have to encompass the nuances of speech and language. So a process of observation needs to be adopted whereby machine learning algorithms are used to detect patterns in language data to create models which are subsequently used to predict the structure and syntax of new text data.To paraphrase Vasiliev, a statistical model contains estimates for the probability distribution of linguistic units, such as words and phrases that maps all of the possible outcomes of that variable to their probabilities of occurrence. An NLP model will include linguistic annotations, such as part-of-speech tags and syntactic annotations, and word vectors.The next step is to use spaCy’s NLP API to classify the Campus description.Using spaCy’s Named Entity Recognition classifier we would expect the output to distinguish the tokenised data into distinct labels, for example:https://spacy.io/usage/linguistic-featuresThe displacy utility, which colour codes labels, was used to distinguish the entities along with the field contents.The results gathered from the execution of NER on a limited sample is quite weak but we would expect this as a first parse although towns like Watford or Stockport and the city of Derby should all have been classified as a geopolitical entity (GPE) whereas as we can see the GPE classification was only partially performed on ‘Ashton’ and not ‘Ashton-on-Mersey’ as the hyphenated compound structure of the word appears to have been ignored by the API as no dependency was applied.So let’s build a distinct NLP model for NER that could categorise locations within our Campus description. The end-to-end process for the creation of the NLP NER model is outlined in the following diagram:https://spacy.io/usage/training#nerAs per the diagram the first step is to obtain Training data from the many examples within the Campus field which would potentially represent Campus descriptions both for English and non-English text.‘To prepare training examples, you convert raw text data into a training example containing a sentence and each token’s annotations. During the training process, spaCy uses the training examples to correct the model’s weights: the goal is to minimize the error (called the loss) of the model prediction. Put simply, the algorithm calculates the relationship between the token and its annotation to determine the likelihood that a token should be assigned that annotation.’ (Vasilliev 2020[2])Note that for this demonstration I have adopted the en language along with installing and importing the relevant spaCy components.So, onward to the training data.It’s recommended that you create a large number of training examples that will provide a holistic representation to efficiently model the data.To ensure that the field boundaries are distinct prior to training the model spaCy provides the library bilou_tags_offsets. In the example below the first 8 characters, including space, are defined as an organisation, characters for the ORG tag ‘University’ range from 8,18 and exclude the double space requiring only 1 additional space for word tokenisation. Finally, the Geography, Planning and Environment, GPE, token extends to 1 character after Belfast.BILUO is an acronym which relates specifically to the position of the tokens:spaCy adopted the BILUO character offset scheme to determine ‘the cost of each action given the current state of the entity recognizer. The costs are then used to calculate the gradient of the loss, to train the model.’ https://spacy.io/api/annotationBILUO is a variant of BMEWO but as the following diagram illustrates, there are several standards:‘The BMEWO encoding further distinguishes end-of-entity (E_X) tokens from mid-entity tokens (M_X), and adds a whole new tag for single-token entities (W_X). I believe the BMEWO encoding was introduced in Andrew Borthwick’s NYU thesis and related papers on “max entropy” named entity recognition around 1998, following Satoshi Sekine’s similar encoding for decision tree named entity recognition.’https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/Sorry, we digress. Upon defining the text, we created examples of labels. ‘The GoldParseobject collects the annotated training examples, also called the gold standard. It’s initialized with the Docobject it refers to, and keyword arguments specifying the annotations, like tags or entities. Its job is to encode the annotations, keep them aligned and create the C-level data structures required for efficient access.’ https://spacy.io/usage/trainingFor our model the following GoldParse objects were assigned to part of speech tags:It was also possible to update the gold-standard annotations using the following commands:The process that we used to build the model was iterative and based upon the randomly shuffling the training data through the NLP model. For the first model we adopted a range of both 100 and 1000 iterations while letting spaCy create a new distinct optimizer for this model. We set the dropout rate to 0.5 ‘a rate at which to randomly “drop” individual features and representations. This makes it harder for the model to memorize the training data. For example, a 0.25 dropout means that each feature or internal representation has a 1/4 likelihood of being dropped.’ https://spacy.io/usage/trainingThe iterative sequence of events to create the model are outlined below:Docs — Pass the randomly shuffled training dataGold — Pass the dictionary of annotations that we had previously created and updatedDrop — Set our dropout rate to 0.5. Essentially the dropout rate makes it more difficult for the model to memorise the data which has been submitted.SGD — For this initial model we omitted sgd opting for spaCy to create the necessary optimiser.The model was saved to the operating system, creating the directory ‘model’ with the following sub-directories:To test the model, we loaded the new ‘model’ into Jupyter Notebooks:Our initial test was undertaken on the training data to ensure that the model had at least been successful.The entities and tokens reflected, in the most part, the data used to train the model.We then applied the model to a sample of the Campus data:There were a number of obvious discrepancies, for example, ‘the’ was classified as GPE.We revisited the training data and added several more examples as per the overall process outlined in the model creation.Re-running the model ensured further clarity in regards to GPE entities but as this was an iterative approach it highlighted the need to ensure that the training data reflected all Campus data.At this point it would be useful to reflect how far we have travelled in the classification process. Our initial spaCy NLP en_core_web_sm model struggled to classify GPE tags within the Campus description.Whereas our trained NLP model provided greater distinction of entities within the Campus field.Whilst executing the model it was evident that updating the training data had a positive impact on the model’s ability to classify data.However, would there be further gains if we adopted a tailored optimiser for our model? We’ll take a look in the next article, once I’ve finished my lasagne.Paul[1] Rao, Delip and McMahan, Brian “Natural Language Processing with PyTorch Build Intelligent Language Applications Using Deep Learning.” O’Reilly (2019). 9781491978238[2] Vasilliev, Yuli. “Natural Language Processing with Python and spaCy.” No Starch Press (2020)",02/02/2021,0,6.0,0.0,554.0,266.0,26.0,0.0,0.0,14.0,en
3831,Word2vec with PyTorch: Implementing the Original Paper,Towards Data Science,Olga Chernytska,142.0,13.0,1986.0,"Word Embeddings is the most fundamental concept in Deep Natural Language Processing. And word2vec is one of the earliest algorithms used to train word embeddings.In this post, I want to go deeper into the first paper on word2vec — Efficient Estimation of Word Representations in Vector Space (2013), which as of now has 24k citations, and this number is still growing.Our plan is the following:I am attaching my Github project with word2vec training. We will go through it in this post.Today we are reviewing only the first paper on word2vec. However, there are several later papers, describing the evolution of word2vec:I believe, if you understand the first paper, you’ll easily catch the ideas described in later papers. So let’s go!Disclosure. Wor2vec is already an old algorithm and there are more recent options (for instance, BERT). This post is for those, who have just started their journey into Deep NLP, or for those, who are interested in reading and implementing papers.Contents — What is word2vec? — Model Architecture — Data — Data Preparation — Text Processing with PyTorch — Training Details — Retrieving Embeddings — — Visualization with t-SNE — — Similar Words — — King — Man + Woman = Queen — What’s Next?Here is my 3-sentence explanation:If you are not familiar with the concept of word embeddings, below are the links to several great resources. Read through skipping the details but grasping the intuition behind it. And come back to my post for the word2vec details and coding.Better now?Word embeddings are used literally in every NLP task — text classification, named-entity recognition, question answering, text summarization, etc. Everywhere. Models do not understand words and letters, they understand numbers. That’s where word embeddings come in handy.Word2vec is based on the idea that a word’s meaning is defined by its context. Context is represented as surrounding words.Think about it. Assume, you are learning a new language. You are reading a sentence and all the words there are familiar to you, except one. You’ve never seen this word before, but you can easily tell its part of speech, right? And sometimes, even guess its meaning. That’s because the information from surrounding words helps you.For the word2vec model, context is represented as N words before and N words after the current word. N is a hyperparameter. With larger N we can create better embeddings, but at the same time, such a model requires more computational resources. In the original paper, N is 4–5, and in my visualizations below, N is 2.There are two word2vec architectures proposed in the paper:For instance, the CBOW model takes “machine”, “learning”, “a”, “method” as inputs and returns “is” as an output. The Skip-Gram model does the opposite.Both CBOW and Skip-Gram models are multi-class classification models by definition. Detailed visualizations below should make it clear.What is happening in the black box?The initial step would be to encode all words with their IDs. ID is an integer (index) that identifies word position in the vocabulary. “Vocabulary” is a term to describe a set of unique words in the text. This set may be all words in the text or just the most frequent ones. More on that in Section “Data Preparation”.Word2vec model is very simple and has only two layers:The difference between CBOW and Skip-Gram models is in the number of input words. CBOW model takes several words, each goes through the same Embedding layer, and then word embedding vectors are averaged before going into the Linear layer. The Skip-Gram model takes a single word instead. Detailed architectures are in the images below.Where are the word embeddings?We train the models that are not going to be used directly. We don’t want to predict a word from its context or context from a word. Instead, we want to get word vectors. It turns out that these vectors are weights of the Embedding layer. More details on that are in Section “Retrieving Embeddings”.Word2vec is an unsupervised algorithm, so we need only a large text corpus. Originally, word2vec was trained on Google News corpus, which contains 6B tokens.I’ve experimented with smaller datasets available in PyTorch:When training word embedding for the commercial/research task — choose the dataset carefully. For instance, if you’d like to classify Machine Learning papers, train word2vec on scientific texts about Machine Learning. If you’d like to classify fashion articles, a dataset of fashion news would be a better fit. That’s because the word “model” means “approach” and “algorithm” in the Machine Learning domain, but “person” and “woman” in the fashion domain.When reusing trained word embeddings, pay attention to the dataset they were trained on and whether this dataset is appropriate for your task.The main step in data preparation is to create a vocabulary. The vocabulary contains the words for which embeddings will be trained. Vocabulary may be the list of all the unique words within a text corpus, but usually, it is not.It is better to create vocabulary:Such filtering makes much sense because, with a smaller vocabulary, the model is faster to train. On the other hand, you probably do not want to use embedding for words that appeared only once within the text corpus, as these embedding may not be good enough. To create good word embeddings the model should see a word several times and in different contexts.Each word in the vocabulary has its unique index. Words in vocabulary may be sorted alphabetically or based on their frequencies, or may not — it should not affect the model training. Vocabulary is usually represented as a dictionary data structure:Punctuation marks and other special symbols may be also added to the vocabulary, and we train embeddings for them as well. You may lowercase all the words, or train separate embeddings for the words `apple` and `Apple`; in some cases, it may be useful.Depending on what you want your vocabulary (and word embeddings) to be like — preprocess the text corpus appropriately. Lowercase or not, remove punctuation or not, and tokenize text.For my model:So, before words go into the model, they are encoded as IDs. The ID corresponds to the word index in the vocabulary. Words that are not in the vocabulary (out-of-vocabulary words) are encoded with some number, for instance, 0.The full code for training word2vec is here. Let’s go through important steps.Models are created in PyTorch by subclassing from nn.Module. As described previously, both CBOW and Skip-Gram models have 2 layers: Embedding and Linear.Below is the model class for CBOW, and here is for Skip-Gram.Pay attention, there is no Softmax activation in the Linear Layer. That’s because PyTorch CrossEntropyLoss expects predictions to be raw, unnormalized scores. While in Keras you can customize what the input to CrossEntropyLoss would be — raw values or probabilities.Model input is word ID(s). Model output is an N-dimensional vector, where N is vocabulary size.EMBED_MAX_NORM is the parameter to restrict word embedding norms (to be 1, in our case). It works as a regularization parameter and prevents weights in Embedding Layer grow uncontrollably. EMBED_MAX_NORM is worth experimenting with. What I’ve seen: when restricting embedding vector norm, similar words like “mother” and “father” have higher cosine similarity, comparing to when EMBED_MAX_NORM=None.We create vocabulary from the dataset iterator using the PyTorch function build_vocab_from_iterator. WikiText-2 and WikiText103 datasets have rare words replaced with token <unk>, we add this token as a special symbol with ID=0 and all out-of-vocabulary words also encode with ID=0.Dataloader we create with collate_fn. This function implements the logic of how to batch individual samples. When looping through PyTorch WikiText-2 and WikiText103 datasets, each sample retrieved is a text paragraph.For instance, in collate_fn for CBOW we “say”:Pay attention, the number of final batches (Xs and Ys) when we call collate_fn will be different from parameter batch_size specified in Dataloader, and will vary for different paragraphs.Code for collate_fn for CBOW is below, for Skip-Gram — is here.And here is how to used collate_fn with PyTorch Dataloader:I’ve also created a class Trainer, that is used for model training and validation. It contains a typical PyTorch train and validation flow, so for those who have experience with PyTorch, it will look pretty straightforward.If you want to understand the code better — I recommend you clone my repository and play with it.Word2vec is trained as a multi-class classification model using Cross-Entropy loss.You choose batch size to fit into the memory. Just remember: that batch size is the number of dataset paragraphs, which will be processed into input-output pairs, and this number will be much larger.The paper optimizer is AdaGrad, but I’ve used a more recent one — Adam.I’ve skipped the paper part with Hierarchical Softmax and used just plain Softmax. No Huffman tree was used either to build a vocabulary. Hierarchical Softmax and Huffman tree — are tricks for speeding up the training. But PyTorch has a lot of optimization under the hood, so training is already fast enough.As recommended in the paper, I’ve started with a learning rate of 0.025 and decreased it linearly every epoch until it reaches 0 at the end of the last epoch. Here PyTorch LambdaLR scheduler helps a lot; and here is how I used it.The authors trained the model for only 3 epochs in most experiments (but on a very large dataset). I’ve experimented with the number smaller and larger and decided to stick with 5 epochs.For WikiText-2 dataset:For WikiText103 dataset:The full procedure is described in this notebook.Word embeddings are stored in the Embedding layer. Embedding layer size is (vocab_size, 300), which means there we have embedding for all the words in the vocabulary.When trained on the WikiText-2 dataset both CBOW and Skip-Gram models have weights in the Embedding layer of size (4099, 300), where each row is a word vector. Here is how to get Embedding layer weights:And here is how to get words in the same order as in the embedding matrix:Before using embedding in your model, it’s worth checking whether they were trained properly. There are several options for that:You may use sklearn t-SNE and plotly to create a 2-component visualization like the one below. Here numeric strings are in green and they form 2 separate clusters.After zooming in, we may see that this division of numeric strings into 2 clusters makes much sense. The top left cluster is formed from years, while the lower right — from plain numbers.You may explore this plotly visualization to find even more interesting relations. And the code for creating this visualization is here.Word similarity is calculated as cosine similarity between word vectors. The higher the cosine similarity, the more similar words are assumed to be, obviously.For instance, for the word “father” here are the most similar words in the vocabulary:Code for finding similar words is here.According to the paper, a properly trained word2vec model can solve equations “king — man + woman = ?” (answer: “queen”), or “bigger — big + small = ?” (answer: “smaller”), or “Paris — France + Germany = ?” (answer: “Berlin”).Equations are solved by performing mathematical operations on the word vectors: vector(“king”) — vector(“man”) + vector(“woman”). And the final vector should be the closest to the vector(“queen”).Unfortunately, I could not reproduce that part. My CBOW and Skip-Gram models trained on WikiText-2 and WikiText103 are not able to catch this kind of relation (code here).The closest vectors to vector(“king”) — vector(“man”) + vector(“woman”) are:The closest vectors to vector(“bigger”) — vector(“big”) + vector(“small”) are:Sometimes, correct words are within the top5, but never the closest ones. I assume there are two possible reasons for that:Dataset size really matters while training word embedding. And authors also mentioned that in the paper.I hope this post helps you to build a foundation in Deep NLP, so you can move on to more advanced algorithms. For me, it was very interesting to dig deep into the original paper and train everything from scratch. Recommended.Originally published at https://notrocketscience.blog on September 29, 2021.If you’d like to read more tutorials like this, subscribe to my blog “Not Rocket Science” — Telegram and Twitter.",29/09/2021,10,12.0,23.0,1368.0,634.0,10.0,14.0,0.0,40.0,en
3832,A short introduction to StyleGAN,Analytics Vidhya,Praveenkumar,10.0,5.0,746.0,"Generative models(GAN) have always been the niche and hard-to-master domain of the Deep learning space. Control over distinct features of output image has been a challenging research topic. StyleGAN is an approach that addresses this aspect. It distances itself from the conventional architectures of GAN and introduces a novel approach to generate high-resolution synthetic images along with a fair control over the distinct features of the image.With StyleGAN, seamless realistic new images can be generated and changing something called Style vector gives fantastic real looking animations. Some examples can be found here:drive.google.comAlias-Free GAN is an improved version of StyleGAN which makes animations even more seamless, a short introduction follows in the latter part of this article.Understanding the architecture of StyleGAN can give us some insight into why it works so well. Below is the description of the architecture of StyleGAN from the original paper.Authors point six important aspects in the architecture of StyleGAN:1. Baseline Progressive GANA progressive GAN is a training method where GAN architecture is grown slowly in a stable fashion during the training. At first, a base model with generator and discriminator is trained with small images and once the training stabilizes new blocks of layers are added which are capable of taking in larger images, and the model is trained again. This method is repeated until the desired image size is achieved.StyleGAN uses this training methodology.2. Tuning (incl. bilinear up/down)Several methods are used to improve results, and one of them is bilinear sampling. Here, after each upsampling layer and before each downsampling layer, activations are filtered with a second-order binomial filter.This technique replaces the conventional upsampling used in Progressive GAN.3. Add mapping and stylesNext, instead of feeding the latent space directly to the synthesis network (authors refer to the generator of StyleGAN by synthesis network), it is passed through eight fully connected layers, and the output is called the Style vector. Style vector is later incorporated into each block of the synthesis network, after the convolutional layers. This is done via a method called adaptive instance normalization or AdaIN.AdaIN maps the feature map of the preceding convolutional layer into Gaussian distribution and then adds the Style vector.4. Remove traditional inputInstead of using a conventional latent space as input, a fixed value is used as input to the synthesis network. This input is learned during the training.Personally, I felt this was the most intriguing part of the architecture introduced by the authors.5. Add noise inputsNext, single-channel images consisting of uncorrelated Gaussian noise, or simply referred to as noise in the paper, is fed to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned per feature scaling factors and then added to the output of the corresponding convolution.6. Addition Mixing regularizationFinally, the Mixing regularization technique prevents the network from assuming that adjacent styles are correlated. It involves training the network with two random latent space nodes for a certain percentage of generated images, instead of one latent space. During the generation of these images, latent nodes are switched at a randomly selected point in the synthesis networkThis method is also referred to as style mixing.ResultsThe StyleGAN can generate high-quality images and also let us control the style of the generated image.These videos give an insight into the capabilities of the model:drive.google.comStyle Vectors give control over the different facial features. The use of different style vectors at different points in the synthesis network results in the change of different aspects of the generated image.Blocks of synthesis network with lower resolution (4x4, 8x8) control high-level aspects such as pose, general hairstyle, face shape, and eyeglassesMiddle resolutions (16x16, 32x32) control smaller scale facial features, hairstyle, eyes open/closedHigher resolutions (64x64, 1024x1024) affect mainly the colour scheme and microstructureThe video below demonstrates these capabilities:https://drive.google.com/file/d/1O4lPOWXl7HSpbhdQJcYx16rje5gIzO9k/view?usp=sharing/The main motivation behind this improvement was something called “texture sticking”.The video below gives an insight into the problem:The problem of alias occurred because of poor signal processing and is addressed in this paper by implementing a few architectural changes to StyleGAN. These changes ensure that unwanted information does not leak into the hierarchical synthesis process and the model still matches the Fréchet Inception Distance(FID) score of StyleGAN(Version 2).Alias Free GAN has been launched recently and I haven't had time to go through it in detail. Expect this article to be extended with details about Alias Free GAN soon….or perhaps a new article, in which case I will add a link here.Hope you enjoyed the article!Feel free to connect:www.linkedin.comReferences:arxiv.orgarxiv.orgmachinelearningmastery.com",01/08/2021,0,7.0,5.0,666.0,511.0,7.0,1.0,0.0,9.0,en
3833,CSS Box Model and Positioning. VGG Virtual Internship Assignment. ,Medium,Chijioke Nwagwu,17.0,3.0,429.0,"CSS Box Model and PositioningVGG Virtual Internship Assignment.The CSS box model is crucial and fundamental to understand as far as layout and positioning are concerned in styling of a web page. This is so because every element in HTML generate a box around it and these boxes have properties that can be illustrated using what is popularly know as the CSS Box Model. You can view the box model from the developer tool by simply right clicking on an element on the web page then click on “inspect”.Once you are in the developer tools menu, ensure the “Elements” tab and “Styles” tab are selected (might be slightly different for other browsers). Then scroll down, you will see the box model for the element you are inspecting as shown below.From the image above, we can see that for a given element, the box model has three properties that defines it; padding, border and margin. If you would picture a web page in your mind using the box model, you would see the elements in form of the cells of a table arranged vertically and horizontally as the case may be. And based on this, let’s try to define the properties of the box model.Margin: This is the vertical or horizontal space between one cell and another cell (cell here represents the boundary of an element).Border: This is the demarcation of an element. It is invisible by default but can be made to be displayed using appropriate property-value pairs.Padding: This is the space inside the border between the border and the content of the element.Having considered the box model, we are now ready to look at CSS positioning. Positioning simply refers to the placement of elements on a web page using the various positioning methods. Css positioning methods include;Absolute: Position is calculated from the top left of the web page.Relative: Position is calculated with respect to the page order. (the preceding element is the reference point)Fixed: This is similar to absolute positioning but in addition, when used on an element, the element does not move relative to the page movement. So for an element with absolute positioning, such and element can be scrolled into and out of view while for an element with fixed positioning, it is just stationary.Sticky: This makes and element to act like it is switching it’s positioning style between relative and fixed depending on the scroll position of the user according to the values set.Using some of the techniques briefly outlined above, I have created a simple accessibility compliant contact form here: https://wisestme.github.io/VGG_Virtual_Internship/Assignment/form.htmlLink to repo: https://github.com/wisestme/VGG_Virtual_Internship/tree/master/AssignmentThank you.",21/01/2020,0,8.0,8.0,874.0,338.0,2.0,0.0,0.0,2.0,en
3834,Text Summarization Using Deep Neural Networks,Towards Data Science,Shivam Duseja,8.0,9.0,1628.0,"The amount of textual data being produced every day is increasing rapidly both in terms of complexity as well as volume. Social Media, News articles, emails, text messages (the list goes on..), generate massive information and it becomes cumbersome to go through lengthy text materials (and boring too!). Thankfully with the advancements in Deep Learning, we can build models to shorten long pieces of text and produce a crisp and coherent summary to save time and understand the key points effectively.We can broadly classify text summarization into two types:1. Extractive Summarization: This technique involves the extraction of important words/phrases from the input sentence. The underlying idea is to create a summary by selecting the most important words from the input sentence2. Abstractive Summarization: This technique involves the generation of entirely new phrases that capture the meaning of the input sentence. The underlying idea is to put a strong emphasis on the form — aiming to generate a grammatical summary thereby requiring advanced language modeling techniques.In this article, we will use PyTorch to build a sequence 2 sequence (encoder-decoder) model with simple dot product attention using GRU and evaluate their attention scores. We will further look into metrics like — BLEU, ROUGE for evaluating our model.Dataset used :We will work on the wikihow dataset that contains around 200,000 long sequence pairs of articles and their headlines. This dataset is one of the large-scale datasets available for summarization with the length of articles varying considerably. These articles are quite diverse in their writing style which makes the summarization problem more challenging and interesting.For more information on the dataset: https://arxiv.org/abs/1810.09305Pre-processing and cleaning is an important step because building a model on unclean and messy data will in turn produce messy results. We will apply the below cleaning techniques before feeding our data to the model:We will first define the contractions in the form of a dictionary:Before feeding the training data to our deep model, we will represent each word as a one-hot vector. We will then need a unique index per word to use it as the input and output of the network.In order to do so, we will create a helper class Lang which has word-> index and index-> word mappings along with the count of each word. To read data in our model, we have created pairs of our input and output in the form of a list (Pairs-> [Input, Output])We will be using a seq2seq(encoder-decoder architecture) model with a simple dot product attention. The underlying idea of choosing this architecture is that we have a many-to-many problem at hand(n number of words as input and m number of words as output). The figure below shows the detailed architecture diagram for this model.There are four major components in this architecture:Encoder: The encoder layer of the seq2seq model extracts information from the input text and encodes it into a single vector, that is a context vector. Basically, for each input word, the encoder generates a hidden state and a vector , using this hidden state for the next input word.We have used GRU(Gated Recurrent Unit) for the encoder layer in order to capture long term dependencies - mitigating the vanishing/exploding gradient problem encountered while working with vanilla RNNs.The GRU cell reads one word at a time and using the update and reset gate, computes the hidden state content and cell state.I found the below two links useful for getting more information on the working of GRU:Empirical Evaluation of Gated Recurrent Neural Networks on Sequence ModelingGated Recurrent Units (GRU)Decoder: The decoder layer of a seq2seq model uses the last hidden state of the encoder i.e. the context vector and generates the output words. The decoding process starts once the sentence has been encoded and the decoder is given a hidden state and an input token at each step/time. At the initial time stamp/state the first hidden state is the context vector and the input vector is SOS(start-of-string). The decoding process ends when EOS(end-of-sentence) is reached. The SOS and EOS tokens are explicitly added at the start and end of each sentence respectively.Attention Mechanism: Using the encoder-decoder architecture, the encoded context vector is passed on to the decoder to generate an output sentence. Now what if the input sentence is long and a single context vector cannot capture all the important information. This is where attention comes into picture!!!The main intuition of using attention is to allow the model to focus/pay attention on the most important part of the input text. As a blessing in disguise, it also helps to overcome the vanishing gradient problem.There are different types of attention — additive, multiplicative, however, we will use the basic dot product attention for our model.1. Attention scores are first calculated by computing the dot product of the encoder(h) and decoder(s) hidden state2. These attention scores are converted to a distribution(α) by passing them through the Softmax layer.3. Then the weighted sum of the hidden states (z) is computed.4. This z is then concatenated with s and fed through the softmax layer to generate the words using ‘Greedy Algorithm’ (by computing argmax)In this architecture, instead of directly using the output of last encoder’s hidden state, we are also feeding the weighted combination of all the encoder hidden states. This helps the model to pay attention to important words across long sequences.Supporting EquationsTeacher Forcing: In general, for recurrent neural networks, the output from a state is fed as an input to the next state. This process causes slow convergence thereby increasing the training time.What is Teacher ForcingTeacher forcing addresses this slow convergence problem by feeding the actual value/ground truth to the model. The basic intuition behind this technique is that instead of feeding the decoders predicted output as an input for the next state, the ground truth or the actual value is fed to the model. If the model predicts a wrong word it might lead to a condition wherein all the further words that are predicted are incorrect. Hence, teacher forcing feeds the actual value thereby correcting the model if it predicts/generates a wrong word.Teacher forcing is a fast and effective way to train RNNs, however, this approach may result in more fragile/unstable models when the generated sequences vary from what was seen during the training process.To deal with such an isssue, we will follow an approach that involves randomly choosing to use the ground truth output or the generated output from the previous time step as an input for current time step.Below code snippet shows the dynamic implementation of Teacher ForcingFor our text summarization problem, there can be multiple correct answers and as we do not have a single correct output, we can evaluate our model using different parameters like — Recall, Precision, F-score. Below are some of the metrics that we will use:BLEU(Bilingual Evaluation Understudy): The cornerstone of this metric is precision having values between [0,1], wherein 1 represents a perfect match and 0 represents a complete mismatch. This metric is basically calculated by comparing the number of machine-generated words that are a part of the reference sentence with respect to the total number of words in the machine-generated output. Let’s understand this with the help of an example:Reference Sentence: The door is lockedMachine Output: the the theBLEU Score=1The above machine output is an extreme condition, however, in order to overcome this problem, the BLEU score is calculated in a way that each word in the generated sentence will be clipped to the number of times, that word occurs in the reference sentence. This basically makes sure that if one word is generated more number of times than it is present in the reference sentence– it is not considered while evaluating the similarity.After applying the above rule, we get the modified BLEU Score as 1/3Let’s take a look at another extreme example:Reference Sentence: The door is lockedMachine Output: theBLEU Score: 1 (even after applying the above rule)In such a scenario, where the length of the generated sentence is less than the reference sentence, a Brevity penalty(BP) is introduced, i.e. there is a penalty to the BLEU score, if the generated sentence is smaller than the reference sentence, however, when the generated sentence is longer than the reference sentence no penalty is introduced.Brevity Penalty is defined as-where, r is the effective reference corpus lengthc is the length of generated/candidate sentenceThis metric was first proposed by Kishore Papineni in 2002. Refer the below link for more details around this metric: BLEU: a Method for Automatic Evaluation of Machine TranslationROUGE (Recall-Oriented Understanding for Gisting Evaluation):ROUGE is basically a recall-oriented measure that works by comparing the number of machine-generated words that are a part of the reference sentence with respect to the total number of words in the reference sentence.This metric is more intuitive in the sense that every time we add a reference to the pool, we expand our space of alternating summaries. Hence, this metric should be preferred when we have multiple references.ROUGE implementation is pretty similar to BELU, however, there are other underlying implementations like LCS (Longest common subsequence) and skip-gram, etc. We can directly use the ROUGE-N implementation using the python library ROUGE.For more details, you can refer to the below paper: ROUGE: A Package for Automatic Evaluation of SummariesWe have seen a precision-based metric(BLEU) and a recall oriented metric(ROUGE), however, if we want our evaluation on the basis of both recall and precision, we can use F-Score as an evaluation measure.Result: The implementation code can be found on my Github.The model ran on Google Colab Pro(T4 & P100 GPU - 25GB with high memory VMs) for ~6–7 hours and it seemed to work fine on shorter summaries (~50 words). However, the model can be optimized by further tuning the hyperparameters (learning rate, optimizer, loss function, hidden layers, momentum, iterations, etc.)",29/08/2020,2,70.0,43.0,895.0,401.0,6.0,3.0,0.0,11.0,en
3835,Probability concepts explained: Maximum likelihood estimation,Towards Data Science,Jonny Brooks-Bartlett,10000.0,8.0,1616.0,"In this post I’ll explain what the maximum likelihood method for parameter estimation is and go through a simple example to demonstrate the method. Some of the content requires knowledge of fundamental probability concepts such as the definition of joint probability and independence of events. I’ve written a blog post with these prerequisites so feel free to read this if you think you need a refresher.Often in machine learning we use a model to describe the process that results in the data that are observed. For example, we may use a random forest model to classify whether customers may cancel a subscription from a service (known as churn modelling) or we may use a linear model to predict the revenue that will be generated for a company depending on how much they may spend on advertising (this would be an example of linear regression). Each model contains its own set of parameters that ultimately defines what the model looks like.For a linear model we can write this as y = mx + c. In this example x could represent the advertising spend and y might be the revenue generated. m and c are parameters for this model. Different values for these parameters will give different lines (see figure below).So parameters define a blueprint for the model. It is only when specific values are chosen for the parameters that we get an instantiation for the model that describes a given phenomenon.Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.The above definition may still sound a little cryptic so let’s go through an example to help understand this.Let’s suppose we have observed 10 data points from some process. For example, each data point could represent the length of time in seconds that it takes a student to answer a specific exam question. These 10 data points are shown in the figure belowWe first have to decide which model we think best describes the process of generating the data. This part is very important. At the very least, we should have a good idea about which model to use. This usually comes from having some domain expertise but we wont discuss this here.For these data we’ll assume that the data generation process can be adequately described by a Gaussian (normal) distribution. Visual inspection of the figure above suggests that a Gaussian distribution is plausible because most of the 10 points are clustered in the middle with few points scattered to the left and the right. (Making this sort of decision on the fly with only 10 data points is ill-advised but given that I generated these data points we’ll go with it).Recall that the Gaussian distribution has 2 parameters. The mean, μ, and the standard deviation, σ. Different values of these parameters result in different curves (just like with the straight lines above). We want to know which curve was most likely responsible for creating the data points that we observed? (See figure below). Maximum likelihood estimation is a method that will find the values of μ and σ that result in the curve that best fits the data.The true distribution from which the data were generated was f1 ~ N(10, 2.25), which is the blue curve in the figure above.Now that we have an intuitive understanding of what maximum likelihood estimation is we can move on to learning how to calculate the parameter values. The values that we find are called the maximum likelihood estimates (MLE).Again we’ll demonstrate this with an example. Suppose we have three data points this time and we assume that they have been generated from a process that is adequately described by a Gaussian distribution. These points are 9, 9.5 and 11. How do we calculate the maximum likelihood estimates of the parameter values of the Gaussian distribution μ and σ?What we want to calculate is the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we’ll make our first assumption. The assumption is that each data point is generated independently of the others. This assumption makes the maths much easier. If the events (i.e. the process that generates the data) are independent, then the total probability of observing all of data is the product of observing each data point individually (i.e. the product of the marginal probabilities).The probability density of observing a single data point x, that is generated from a Gaussian distribution is given by:The semi colon used in the notation P(x; μ, σ) is there to emphasise that the symbols that appear after it are parameters of the probability distribution. So it shouldn’t be confused with a conditional probability (which is typically represented with a vertical line e.g. P(A| B)).In our example the total (joint) probability density of observing the three data points is given by:We just have to figure out the values of μ and σ that results in giving the maximum value of the above expression.If you’ve covered calculus in your maths classes then you’ll probably be aware that there is a technique that can help us find maxima (and minima) of functions. It’s called differentiation. All we have to do is find the derivative of the function, set the derivative function to zero and then rearrange the equation to make the parameter of interest the subject of the equation. And voilà, we’ll have our MLE values for our parameters. I’ll go through these steps now but I’ll assume that the reader knows how to perform differentiation on common functions. If you would like a more detailed explanation then just let me know in the comments.The above expression for the total probability is actually quite a pain to differentiate, so it is almost always simplified by taking the natural logarithm of the expression. This is absolutely fine because the natural logarithm is a monotonically increasing function. This means that if the value on the x-axis increases, the value on the y-axis also increases (see figure below). This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.Taking logs of the original expression gives us:This expression can be simplified again using the laws of logarithms to obtain:This expression can be differentiated to find the maximum. In this example we’ll find the MLE of the mean, μ. To do this we take the partial derivative of the function with respect to μ, givingFinally, setting the left hand side of the equation to zero and then rearranging for μ gives:And there we have our maximum likelihood estimate for μ. We can do the same thing with σ too but I’ll leave that as an exercise for the keen reader.No is the short answer. It’s more likely that in a real world scenario the derivative of the log-likelihood function is still analytically intractable (i.e. it’s way too hard/impossible to differentiate the function by hand). Therefore, iterative methods like Expectation-Maximization algorithms are used to find numerical solutions for the parameter estimates. The overall idea is still the same though.Well this is just statisticians being pedantic (but for good reason). Most people tend to use probability and likelihood interchangeably but statisticians and probability theorists distinguish between the two. The reason for the confusion is best highlighted by looking at the equation.These expressions are equal! So what does this mean? Let’s first define P(data; μ, σ)? It means “the probability density of observing the data with model parameters μ and σ”. It’s worth noting that we can generalise this to any number of parameters and any distribution.On the other hand L(μ, σ; data) means “the likelihood of the parameters μ and σ taking certain values given that we’ve observed a bunch of data.”The equation above says that the probability density of the data given the parameters is equal to the likelihood of the parameters given the data. But despite these two things being equal, the likelihood and the probability density are fundamentally asking different questions — one is asking about the data and the other is asking about the parameter values. This is why the method is called maximum likelihood and not maximum probability.Least squares minimisation is another common method for estimating parameter values for a model in machine learning. It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the least squares method. For a more in-depth mathematical derivation check out these slides.Intuitively we can interpret the connection between the two methods by understanding their objectives. For least squares parameter estimation we want to find the line that minimises the total squared distance between the data points and the regression line (see the figure below). In maximum likelihood estimation we want to maximise the total probability of the data. When a Gaussian distribution is assumed, the maximum probability is found when the data points get closer to the mean value. Since the Gaussian distribution is symmetric, this is equivalent to minimising the distance between the data points and the mean value.If there is anything that is unclear or I’ve made some mistakes in the above feel free to leave a comment. In the next post I plan to cover Bayesian inference and how it can be used for parameter estimation.Thank you for reading.",04/01/2018,0,0.0,19.0,1376.0,537.0,14.0,0.0,0.0,7.0,en
3836,Build K-Means from scratch in Python,Medium,Rishit Dagli,198.0,6.0,704.0,"K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically. The “Choosing K” section below describes how the number of groups can be determined.This story covers:The algorithm can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group.This is a versatile algorithm that can be used for any type of grouping. Some examples of use cases are:Behavioral segmentation:Inventory categorization:Sorting sensor measurements:Detecting bots or anomalies:In addition, monitoring if a tracked data point switches between groups over time can be used to detect meaningful changes in the data.K-Means is actually one of the simplest unsupervised clustering algorithm. Assume we have input data points x1,x2,x3,…,xn and value of K(the number of clusters needed). We follow the below procedure:Here Let point a=(a₁,a₂) ; b=(b₁,b₂) and D be the Euclidean distance between the points thenMathematically speaking, if each cluster centroid is denoted by cᵢ, then each data point x is assigned to a cluster based onarg (min (cᵢ ϵ c) D(cᵢ,x)²)For finding the new centroid from clustered group of points —So it’s really problematic if you choose the wrong K, so this is what I am talking about.So this was a clustering problem for 3 cluster, K = 3 but if I choose K = 4, I will get something like this and this will definitely not give me the kind of accuracy of predictions, I want my model too. Again if I choose K = 2, I get something like this —Now this isn’t a good prediction boundary too. You or me can figure out the ideal K easily in this data as it is 2 dimensional, we could figure out the K for 3 dimensional data too, but lets say you have 900 features and you reduce them to around a 100 features, you cannot visually spot out the ideal K (these problems usually occur in gene data). So, you want a proper way for figuring or I would say guessing an approximate K. To do this we have may algorithms like cross validation, bootstrap, AUC, BIC and many more. But we will discuss one of the most simple and easy to comprehend algorithm called “elbow point”.The basic idea behind this algorithm is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster. So average distortion will decrease. The lesser number of elements means closer to the centroid. So, the point where this distortion declines the most is the elbow point and that will be our optimal K.For the same data shown above this will be the cost(SSE — sum of squared errors) vs K graph created —So, our ideal K is 3. Let us take another example where ideal K is 4. This is the data-And, this is the cost vs K graph created-So, we easily infer that K is 4.Here we will use the libraries, Matplotlib to create visualizations and Numpy to perform calculation.So first we will just plot our data —To make our work organized we will build a class and proceed as in the steps above and also define a tolerance or threshold value for the SSE’s.Now we can do something like:Next, you can also use the predict function. So that was about K-Means and we are done with the code 😀Hi everyone I am Rishit DagliTwitterWebsiteIf you want to ask me some questions, report any mistake, suggest improvements, give feedback you are free to do so by emailing me at —",29/10/2019,3,2.0,9.0,761.0,412.0,12.0,8.0,0.0,4.0,en
3837,Using ResNet for ECG time-series data,Towards Data Science,Sanne de Roever,19.0,7.0,1328.0,"Recurrent neural networks like plain RNN or more advanced models like LSTM and GRU used to be the goto models for deep-learning practitioners venturing into the time series domain. NLP, providing an abundance of sequence data, provided a willing subject. But transformer architectures like BERT and GPT have definitely taken over in the domain. Apart from these transformer architectures, CNN’s have also made a come-back or advance in the time-series domain. Are CNN’s good at modelling time-series?How good are CNN’s at modelling time-series?To answer this question tthis post replicates an article called “ECG Heartbeat Classification: A Deep Transferable Representation” [1] that applies ResNet, a CNN based architecture, to electrocardiogram (ECG) data. To round it of transfer learning is applied to the problem.Keras code is provided in the form of a notebook that can be readily executed with for example Google Colab here.This post is structured as follows:The article with the original study uses two sets of ECG data:(Both datasets are available on Kaggle, see the notebook for details.)Both datasets contain standardized ECG signals. Each observation has 187 time-steps per heartbeat. An example observation plotted in 2D renders:In the original MIT-BIH data set one of the following labels is assigned to each observation:In the Kaggle data set, that happens to be the source of the original study, these labels have been fused into 5 categories. The data set provides both a training and test datasets of lengths 87554 and 21892 respectively. Not too shabby!The PTB dataset (of length 14552) merely assigns a normal and abnormal label to the observations; the distribution of labels can be found below.In the original study the weights of the model fitted to the MIT-BIH data set are transferred to the PTB data set. Since the MIT-BIH data set is much larger than the PTB data set this could make sense.Before the replication kicks off, first a short ResNet refresher is provided.The goals of ResNet is to allow for the optimization of deep networks with more layers. To do so ResNet introduces skip connections. Skip connections are created by literally adding the signal from an earlier layer to a later layer. A nice illustration is provided in the original article:What is happening here? The signal from the first convolutional layer is added to the signal of the 3rd convolutional layer in the first ResNet block. (Note that the signal is added before the ReLU. The pool layer at the end of the block is not strictly part of the ResNet block, its is specific to this application.)It turns out that adding a these skip connections allows for deep-learning networks.One note on the convolutions. The input data, although depicted in two dimensions, is one dimensional. The convolutions therefore will also be one dimensional; the filters will be moving from left to right.Lets code!Before evaluating the transfer learning setup of the original study, first a baseline multi-layer perceptron (MLP) and a ResNet model without transfer learning are fit on the PTB data set. Why complicate things if it is not necessary? One could argue that a deep-learning model is in itself a complication. In this case one would argue that a neural network easily fits to the data, giving a very efficient implementation.The baseline MLP network consists of 3 hidden ReLU layers of with 15, 10 and 5, followed by a sigmoid actived layer. Keras produces this picture of the model:The MLP almost resolved the training data at 97.5% training accuracy. This was about the maximum possible with this architecture. Despite using some L2 regularization, over-fitting on the training data occurred, as can be seen below.The final validation accuracy was 95.2% with a test accuracy of 95.1%. The somewhat small validation data set of 10% seems to work well. Below is a confusion matrix generated using the test data.For good measure the MLP model was optimized using a cyclical learning rate, with the learning rates being estimated using a learning rate finder. This practice was popularized by fast.ai. Applying this using both with SGD and Adam, the effect of this setup seemed to lift the accuracy about 1%, i.e. from 93.7% to 94.7% on the validation data set. Interesting enough to keep in mind for the future. Unfortunately the support in Keras for these tools is not standardized yet.Where the MLP could not fully resolve the training data, the ResNet went through the data like a hot knife through butter. The training data was completely resolved, as was almost the validation data.Surely you are joking Mr. Feynman? Zooming in the last bit of the graph, some over-fitting can be observed due to the increasing validation loss.For neural networks it is not unusual to resolve the training data set, it is considered a good starting practice in fact. But the validation metrics were also very good with a 99.7% accuracy. In the discussion section a hypothesis is generated that could explain these results.For the transfer learning model, the ResNet model was first fit on the larger MIT-BIH data set. The transfer learning setup is simple: the last fully connected layers were retrained on the PTD data set. For the curious, the code snippet that makes this happen is depicted below.Using the summary function, it can be verified that the number of trainable parameters has significantly decreased.The results on the MIT-BIH are good, 98.5% accuracy. After applying transfer learning, the test accuracy on the PTB data-set resulted in 96.4%. The original paper reports an accuracy of 95.9% after transfer learning, these numbers are in the same ballpark.Although it is tempting, the initial almost perfect validation and test scores of the ResNet without transfer learning on the PTB data set should be interpreted with extreme caution. A plausible explanation for these scores could be that due to the randomization of the train, validation and test sets, the network actually memorized the ECG patterns of the people the samples were taken from. The correct way to deal with this would be to make sure that the data in the validation and tests sets do no contain data from people present in the train data. In this setup generalization over people is actively tested. The features to make this happen are not present in the data though.Not corroborating this hypothesis are the results of the ResNet on the MIT-BIH data-set: the accuracy on the test set turned out to be 98.5%. Even with the skewed classes this is a very good result. Note that the model has to predict 5 categories instead of 2, which is a harder task.The data ECG used is highly cyclical, luckily! This allows the time-series to be formatted in smaller ‘tiles’ where supposedly distinctive heart activity patterns are present. Intuitively CNN networks fit really well in this setting.As far as the transfer learning goes, the assumption that transfer learning can improve results is not unreasonable. The underlying assumption is that the basic components that make up the shape of the signal are shared across the signal with different labels. In this case the results do not corroborate its use though. The results might be improved by retraining more layers at the end of the network, by setting the weights of the layers to trainable one step at a time.A comparison with LSTM or GRU networks has not been made; this could the subject at a later date.Although possibly some methodological flaws entered this replication study, a case for applying CNN’s to time-series has been made. For ECG data the search is for repetitive patterns across a set of repeating signals: exactly what CNN networks are good at.[1] Kachuee, M., Fazeli, S., Sarrafzadeh, M.: Ecg heartbeat classification: A deep trans-ferable representation. In: 2018 IEEE International Conference on Healthcare In-formatics (ICHI). pp. 443–444. IEEE (2018).[2] Moody GB, Mark RG. The impact of the MIT-BIH Arrhythmia Database. IEEE Eng in Med and Biol 20(3):45–50 (May-June 2001).[3] Bousseljot R, Kreiseler D, Schnabel, A. Nutzung der EKG-Signaldatenbank CARDIODAT der PTB über das Internet. Biomedizinische Technik, Band 40, Ergänzungsband 1 (1995) S 317.",29/06/2020,0,0.0,1.0,406.0,319.0,11.0,3.0,0.0,3.0,en
3838,Understanding UNET,Medium,Kirill Bondarenko,65.0,6.0,834.0,"How to understand U-Net in the most simple way.Hello everyone!In this article I want to explain in simple way the one of the most popular models structures to solve image segmentation task — UNET.If you haven’t heard about it and haven’t seen its architecture, it’s not a problem, because in this article I will start with a simple structure and at the end will be traditional UNET. Let’s start.UNET model was created for medicine purpose to find tumors in lungs or brain, but nowadays it has got much wider usage field.For example your task is to find rectangles on images, no matter what color or shape they are.We have a red one and yellow one rectangles on a green background. This is an input for UNET model.We need to define positive regions on the image where we have rectangles as 1 and negative regions as 0 (like binary classification). If we change red and yellow colors pixels values to 1 and green region to 0 we will get a gray scaled image or binary mask or target (supervised learning term):UNET model will learn to find these white regions on the images. It works with any kind of objects: cats, humans, cars, buildings, roads etc.MiUNET or mini UNET is a simplified and trimmed version of the original UNET. (and somewhere changed)If you want to understand something difficult, the rightest way is to simplify and investigate every step to explain it in the simplest words even for a child. — ancient mathematician Kiryusha, 200 b.c.First of all let’s take a data from the previous paragraph and add there shapes.Simplified architecture will consist of the four parts: convolution, upsampling, concatenation and again convolution (but with other purpose).Now, let’s go through the each stage and investigate it.Let’s create this layer with kernel of convolution = 3x3x3Stride = 1Padding = 0And number of filters = 64And the same for MaxPooling (filter shape 3x3x64).Convolution output shape = (10–3 + 2*0)/1 +1 = 8After convolution we get 8x8x64 image (tensor).Applying MaxPooling we get (8–3 + 2*0)/1 +1 = 6 and the image shape will be 6x6x64.Basically, we did this:We reduced width and height, but increased depth from 3 channels RGB image to 64 layers depth image.If you can’t operate with convolutional layers in easy way, don’t worry about it and learn by hard Andrew Ng whole course about convolutions. No other way.2. UpsamplingWhat is upsampling ? We need to increase our image width and height to 8x8 (explained further) and there are few methods how to do upsampling, here will be shown “nearest neighbor”.The key idea of this method might be shown just with a simple figure:We duplicate pixels values for each layer without any weights and other complex operations.3. ConcatenationHere is an answer why we need to do upsampling up to 8, not to 10.If you look at the picture in the beginning of this paragraph with general structure of the model, you will see, that convolution layer has two outputs with shapes 8x8x64 where one goes to maxpooling and another one goes to concatenation operation.Concatenation is made by third axis (depth):Here we concatenated output of the first one convolutional layer and output of upsampling layer and as the result we get 8x8x128 tensor.4. ConvolutionHere we go again. But now we will use convolution to increase width and height and reduce depth.We will apply here two convolutional layers without maxpooling.First one: F= 3x3x128, P=2, S=1, number of filters = 64.We get output: 10x10x64.Second one: F=3x3x64, P=1, S=1, number of filters = 1.Result: 10x10x1Great, now let’s look at the whole model.In this way we created a network that can work with images data and make segmentation task with returning same shape masks.You may try it in Python using keras:In this condition your model will be able to work with any initial image data shape (height and width) and return the same ones.On the left part of the model structure are blocks of convolutional layers + ReLU activations and MaxPooling layers. In this example initial image has a shape 572x572x1 , at the bottom the shape will be 28x28x1024 after convolutions and poolings.On the right part of the model is going a process of reducing depth and increasing height and width. Going from the bottom to the up: 28x28x1024 →56x56x1536 (the lowest concatenation and first upsampling) →54x54x512 (convolution to reduce depth and reduce a bit W and H) →104x104x768 (second upsampling) →102x102x256 (convolution to reduce depth) →100x100x256 →200x200x384 →198x198x128 →196x196x128 →392x392x192 → 390x390x64 →388x388x64 → 388x388x2 .In this model output has depth = 2. It may be a reference to the number of classes to segment, but in our task of binary segmentation output should be 388x388x1 as a binary gray scale mask.Now you can see that UNET architecture isn’t difficult and trust me, you will spend much more time to prepare right masks and training data at all for the real world tasks.I hoped, this article has helped you in understanding UNET.Good luck !With best wishes, Bondarenko K., machine learning engineer.",02/07/2019,1,20.0,0.0,774.0,422.0,16.0,2.0,0.0,4.0,en
3839,Presenting Goibibo Insights,Backstage,Goibibo Tech,142.0,1.0,127.0,"Its now been quite some time for Goibibo in business, which means that there is a huge amount of data that we have generated over this period. As a part of converting this data to information, we present to you our new initiative — Goibibo Insights.As the name suggests,Insights aims to give you interesting trends across the travel industry as seen by the large data we crunch at Goibibo. We believe this will further assist you in fine-tuning your travel plans. After all, this is your data — we have simply organised it and given it back.Read on for the first series of insights with the info-graphics.Insights are publicly shared on our Group portal (IbiboGroup) and also with the press.Originally published at goibibo.github.io on December 9, 2012.",09/12/2012,0,4.0,3.0,500.0,608.0,1.0,0.0,0.0,2.0,en
3840,Understanding LSTM and its diagrams,ML Review,Shi Yan,1700.0,7.0,1331.0,"I just want to reiterate what’s said here:colah.github.ioI’m not better at explaining LSTM, I want to write this down as a way to remember it myself. I think the above blog post written by Christopher Olah is the best LSTM material you would find. Please visit the original link if you want to learn LSTM. (But I did create some nice diagrams.)Although we don’t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit. We make decisions by reasoning and by experience. So do computers, we have the logic units, CPUs and GPUs and we also have memories.But when you look at a neural network, it functions like a black box. You feed in some inputs from one side, you receive some outputs from the other side. The decision it makes is mostly based on the current inputs.I think it’s unfair to say that neural network has no memory at all. After all, those learnt weights are some kind of memory of the training data. But this memory is more static. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.The naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.A lot of times, you need to process data that has periodic patterns. As a silly example, suppose you want to predict christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.Theoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.Then later, LSTM (long short term memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.At a first sight, this looks intimidating. Let’s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the “memory” of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.Therefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.The way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.The first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.The second valve is the new memory valve. New memory will come in through a T shaped joint like above and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.On the LSTM diagram, the top “pipe” is the memory pipe. The input is the old memory (a vector). The first cross ✖ it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.Then the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the ✖ below the + sign.After these two operations, you have the old memory C_t-1 changed to the new memory C_t.Now lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it’s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.Now the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.The new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.These two ✖ signs are the forget valve and the new memory valve.And finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.The above diagram is inspired by Christopher’s blog post. But most of the time, you will see a diagram like below. The major difference between the two variations is that the following diagram doesn’t treat the memory unit C as an input to the unit. Instead, it treats it as an internal thing “Cell”.I like the Christopher’s diagram, in that it explicitly shows how this memory C gets passed from the previous unit to the next. But in the following image, you can’t easily see that C_t-1 is actually from the previous unit. and C_t is part of the output.The second reason I don’t like the following diagram is that the computation you perform within the unit should be ordered, but you can’t see it clearly from the following diagram. For example to calculate the output of this unit, you need to have C_t, the new memory ready. Therefore, the first step should be evaluating C_t.The following diagram tries to represent this “delay” or “order” with dash lines and solid lines (there are errors in this picture). Dash lines means the old memory, which is available at the beginning. Some solid lines means the new memory. Operations require the new memory have to wait until C_t is available.But these two diagrams are essentially the same. Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:This is the forget gate (valve) that shuts the old memory:This is the new memory valve and the new memory:These are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big “Cell”):This is the output valve and output of the LSTM unit:",13/03/2016,0,0.0,0.0,967.0,653.0,15.0,0.0,0.0,2.0,en
3841,Fast and Faster Region-based Convolutional Network,Medium,Pavan Gurram,4.0,7.0,987.0,"Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.This post contains the details of Fast R-CNN and Faster R-CNN, which are the incremental improvements of R-CNN( aka “slow R-CNN”).Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks.The main contribution of Fast-R-CNN is the RoI pooling followed by a two-headed fully connected network.An input image is passed through CNN(set of convolutional and maxpooling layers). At the end of the CNN, without a top layer, a feature map is generated by filter.We also use methods, like the selective search(greedily merges super-pixels based on engineered low-level features), to get Regions Proposals(Given an input image, find all possible places where objects can be located).Then, for each proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map.Each feature vector is fed into a sequence of fully connected layers which produces :1.2. Selective SearchSelective Search is to group regions together to form an object. The regions may form an object because of only colour, only texture, or because parts are enclosed. Furthermore, lighting conditions such as shading and the colour of the light may influence how regions form an object.Bottom-up grouping is a popular approach to segmentation, hence we adapt it for selective search. Because the process of grouping itself is hierarchical, we can naturally generate locations at all scales by continuing the grouping process until the whole image becomes a single region.Below example shows how pixels are grouped into regions.1.3. Region of Interest PoolingIt is a widely used operation in object detection task using CNN. It was first proposed by Ross Girshick in April 2015 and it achieves a significant speedup of both training and testing.The RoI pooling layer takes two inputs:For every region of interest from the input list, it takes a section of the input feature map that corresponds to it and scales it to some pre-defined size. The scaling is done by:We will get a fixed size feature maps from list of rectangles with different sizes as output.We’re going to perform region of interest pooling on a single 8×8 feature map, one region of interest and an output size of 2×2.State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.Faster R-CNN = Fast R-CNN + RPN (Region Proposal Network)It is composed of two modules:Firstly, the input image is passed through convolutional network which generate feature maps.The feature maps are taken by RPN and outputs set of rectangular object proposals, each with an objectness score.We model this process with a fully convolutional network which produces:The RPN takes the convolutional feature map and generates proposals over the image.2.2.1. Anchors:We slide a small network over the convolutional feature map output by last convolutional layer to generate region proposals.An anchor is centered at the sliding window and is associated with a scale and aspect ratio. By default we use 3 scales and 3 aspect ratios yielding 9 anchors.Suppose if you have a feature map of 38 x 56 x 9 anchors, sampled at a stride of 16, that’s a lot of proposals! (38 x 56 x 9) / 16 = 1192 proposals for a single image. It is not possible to keep all of them, there’s probably only a few interesting things on an image.Thus, the RPN sorts the proposals to find the ones with the corresponding highest probability. Because we have so many proposals, there’s bound to be ones that recognize the same object. We apply Non-max suppression to only keep the most confident proposal, and remove everything else.RPN takes all the anchors and outputs good proposals for objects.It does this by having two different outputs for each of the anchors.2.2.2. Convolutional Implementation of RPNFor the classification layer, we output two predictions per anchor: the score of it being background (not an object) and the score of it being foreground (an actual object).For the regression, or bounding box adjustment layer, we output 4 predictions: the (x​center​​​​, y​center​​​​, width​​, height)​​ which we will apply to the anchors to get the final proposals.2.2.3. Non Maximum Suppression(NMS)NMS is a post-processing algorithm responsible for merging all detections that belong to the same object.Few RPN proposals highly overlap with each other. To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. Too low, you may end up missing proposals for objects, too high, you could end up with too many proposals for the same object. A value commonly used is 0.6.NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals.2.2.4. Loss function:A binary class label is assigned for each anchor to train RPN. We assign a positive label to two kinds of anchors:Anchors that are neither positive nor negative do not contribute to the training objective.The overall loss of the RPN is a combination of the classification loss and the regression loss.The classification loss is log loss over two classes (object vs. not object). For regression task, instead of using a simple L1 or L2 loss, the paper suggests using Smooth L1 loss.Regression loss is activated only for positive anchors and is disabled otherwise.2.2.5. ROI PoolingWe get different sized proposed regions (feature maps) from RPN. To bring them to the same size, we use RoI pooling.As explained in section 1.3, RoI Pooling splits the input feature map into a fixed number of roughly equal regions, and then apply Max-Pooling on every region.These are the models proved to solve complex computer vision problems.New models are currently being built, not only for object detection, but for semantic segmentation, 3D-object detection, and more, that are based on this models.Here is my LinkedIn profile.Faster R-CNN : https://arxiv.org/pdf/1506.01497.pdfFast R-CNN : https://arxiv.org/pdf/1504.08083.pdf",27/11/2019,0,11.0,0.0,898.0,522.0,11.0,8.0,0.0,5.0,en
3842,GAN 2.0: NVIDIA’s Hyperrealistic Face Generator,SyncedReview,Synced,24000.0,4.0,713.0,"Look at the two pictures below. Can you tell which is a photograph and which was generated by AI?The truth is… wait for for it… both images are AI-generated fakes, products of American GPU producer NVIDIA’s new work with generative adversarial networks (GANs). The research was published today in the paper A Style-Based Generator Architecture for Generative Adversarial Networks, which proposes a new generator architecture that has achieved state-of-the-art performance in face generation.Since GANs were introduced in 2014 by Google Researcher Ian Goodfellow, the tech has been widely adopted in image generation and transfer. After some early wiry failures, GANs have made huge breakthroughs and can now produce highly convincing fake images of animals, landscapes, human faces, etc. Researchers know what GANs can do, however a lack of transparency in their inner workings means GAN improvement is still achieved mainly through trial-and-error. This allows only limited control over the synthesized images.The NVIDIA paper proposes an alternative generator architecture for GAN that draws insights from style transfer techniques. The system can learn and separate different aspects of an image unsupervised; and enables intuitive, scale-specific control of the synthesis.Here’s how it works: Given an input facial image, the style-based generator can learn its distribution and apply its characteristics on a novel synthesized image. While previous GANs could not control what specific features they wanted to regenerate, the new generator can control the effect of a particular style — for example high-level facial attributes such as pose, identity, shape — without changing any other features. This enables better control of specific features such as eyes and hair styles. Below is a video demo of how GAN-generated images vary from one to another given different inputs and styles.Behind the new feature is a technique NVIDIA calls “style-mixing.” From the paper: “To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training. When generating such an image, we simply switch from one latent code to another — an operation we refer to as style mixing — at a randomly selected point in the synthesis network.”Stochastic variation is another key property allowing GANs to realize the randomization of detailed facial features, such as the placement of facial hair, stubble density, freckles, pores, etc. The paper proposes adding per-pixel noise after each convolution layer. The added noise does not affect the overall composition or the high-level attributes of images, and changing noise in different layers produces matching stochastic variation results.To quantify interpolation quality and disentanglement, the paper proposes two new, automated methods — perceptual path length and linear separability — that are applicable to any generator architecture.Researchers saw impressive results using the new generator to forge images of bedrooms, cars, and cats with the Large-scale Scene Understanding (LSUN) dataset.Alongside today’s paper, NVIDIA has also released a huge new dataset of human faces. Flickr- Faces-HQ (FFHQ) contains 70,000 high-quality images at 1024 resolution. The dataset will soon be available to the public.The paper’s first author is Tero Karras, a principal research scientist at NVIDIA Research with a primary research interest in deep learning, generative models, and digital content creation. His paper Progressive Growing of GANs for Improved Quality, Stability, and Variation, or known as ProgressiveGAN has won accolades and was accepted by ICLR 2018.Synced, as a natural fan of deep learning and GAN, has noticed that more than a few papers on GANs have picked up momentum and prompted discussions this year. DeepMind researchers proposed BigGAN two months ago, and the model achieved an Inception Score (IS) of 166.3 — a more than 100 percent improvement over the previous state of the art (SotA) result of 52.52. Meanwhile a team of Tsinghua University and Cardiff University researchers introduced CartoonGAN to simulate the styles of Japanese anime maestri from snapshots of real world scenery.Click this link to view the new NVIDIA paper.Journalist: Tony Peng | Editor: Michael Sarazen2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon. Apply for Insight Partner Program to get a complimentary full PDF report.Follow us on Twitter @Synced_Global for daily AI news!We know you don’t want to miss any stories. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates.",15/12/2018,0,17.0,6.0,700.0,425.0,4.0,0.0,0.0,10.0,en
3843,Automated Feature Engineering Tools,Analytics Vidhya,Rajneesh Jha,6.0,6.0,1265.0,"Feature Engineering is a technique to convert raw data columns to something meaningful which can help in predicting the outcomes in a machine learning task. Feature Engineering can be a very tedious and often the most time taking in machine learning life cycle.But to our rescue comes some of the cool tools which automates the whole feature engineering process and creates a large pool of features in a very short span for both classification and regression tasks.We have found following tools which automates the whole feature engineering process and creates large number of features for both relation and non-relational data. While some of them only performs feature engineering, we have some tools which also perform feature selection. Many a times these tools can generate a lot of features which may be correlated among themselves and not have any predictive power to improve the resultant metric, so it’s better to drop such features and thus making feature selection an integral part of feature engineering process.Here is the list of some of the best tools available :-1. FeatureTools2. AutoFeat3. TsFresh4. Cognito5. OneBM6. ExploreKit7. PyFeatFeatureTools :-One of the most popular Python library for automated feature engineering is FeatureTools, which generates a large feature set using “deep feature synthesis”. This library is targeted towards relational data, where features can be created through aggregations (e.g. given some customers (data table 1) and their associated loans (in table 2), a new feature could be the sum of each customer’s loans), or transformations (e.g. time since the last loan payment). DFS requires structured and relational data for creating new features.There are 2 main components of FeatureTools :-· Entity and Entity-Set : Entity can be thought of as a single data-frame while Entity-Set is combination of more than one data-frames.· Primitives : These are basic operations like mean, mode, max etc. that can be applied to the data. It can either be a Transformation or Aggregation.Advantages :-1) Most popular and hence lot of resources are available.2) We can specify the variable types.3) Custom primitives can be created.4) Expanding Features with respect to time can be created.5) Best at handling relational database.Limitations :-1) Creates large number of features leading to curse of dimensionality.2) For database which are not relational we will have to use normalization.3) Does not have support for unstructured data.4) Features extracted are basic statistical features which are aggregated independently of other columns of target variable.AutoFeat :AutoFeat is one of the python library which automates feature engineering and feature selection along with fitting a Linear Regression model. They generally fir Linear Regression model to make the process more explainable.The strategy followed by AutoFeat is somewhat orthogonal to that of FeatureTools: It is not meant for relational data, found in many business application areas, but was rather built with scientific use cases in mind, where e.g. experimental measurements would instead be stored in a single table. For this reason, AutoFeat also makes it possible to specify the units of the input variables to prevent the creation of physically nonsensical features.Here is the feature selection process used by AutoFeat :Advantages :-· Only open source framework for general purpose automated feature engineering which does not care about relational data.· Also does feature selection to reduce the dimensionality problem.· Does not create physically non-sensical features and hence useful in logistic data.Limitations :-· Not good at handling relational data.· Only make simpler features like ratios, products and other basic transformations.· Does not consider feature interaction for making new features.· Only fit automated model for regression data not for classification problem.TsFresh :TsFresh, which stands for “Time Series Feature extraction based on scalable hypothesis tests”, is a Python package for time series analysis that contains feature extraction methods and a feature selection algorithm. Currently, it automatically extracts 64 features from time series data that describe both basic and complex characteristics of a time series (such as the number of peaks, average value, maximum value, time reversal symmetry statistic, etc.), and those features can be used to build regression or classification based machine learning models.Advantages :· Best open source python tool available for time series classification and regression.· Can be easily integrated with FeatureTools.Limitations :· Can only be used for time-series data that too only good for supervised learning.PyFeat :PyFeat is a practical and easy to use toolkit implemented in Python for extracting various features from proteins, DNAs and RNAs.It can only be used with genome data, hence making it not so useful for everyday classification and regression task but find its usefulness in pharma industries.ExploreKit :ExploreKit is based on the intuition that highly informative features often result from manipulations of elementary ones, they identify common operators to transform each feature individually or combine several of them together. it uses these operators to generate many candidate features, and chooses the subset to add based on the empirical performance of models trained with candidate features added.Generation and Selection process is as follows:Advantages :· Uses meta learning to rank candidate features rather than running feature selection on all created features which can sometimes be very large.Limitations :· No open source implementation either in Python or R.OneBM :OneBM works directly with multiple raw tables in a database. It joins the tables incrementally, following different paths on the relational graph. It automatically identifies data types of the joint results, including simple data types (numerical or categorical) and complex data types (set of numbers, set of categories, sequences, time series and texts), and applies corresponding pre-defined feature engineering techniques on the given types. In doing so, new feature engineering techniques could be plugged in via an interface with its feature extractor modules to extract desired types of features in specific domain. it supports data scientists by automating the most popular feature engineering techniques on different structured and unstructured data.Feature selection is used to remove irrelevant features extracted in the prior steps. First, duplicated features are removed. Second, if the training and test data have an implicit order defined by a column, e.g. timestamp, then drift features are detected by comparing the distribution between the value of features in the training and a validation set. If two distributions are different, the feature is identified as a drift feature which may cause over-fitting. Drift features are all removed from the feature set. Besides, we also employ Chi-square hypothesis testing to test whether there exists a dependency between a feature and the target variable. Features that are marginally independent from the target variable are removed.Advantages :· Works well with both relational as well as non-relational data.· Generates simple as well as complex features as compared to FeatureTools.· Tested in Kaggle competitions where it out performed state of the art models.· Can be used to create feature for big data also.Limitations :· No open source implementation.Cognito :Cognito is a system that automates feature engineering but from a single database table. In each step, it recursively applies a set of predefined mathematical transformations on the table’s columns to obtain new features from the original table. In doing so, the number of features is exponential in the number of steps. Therefore, a feature selection strategy was proposed to remove redundant features. it improves prediction accuracy on UCI datasets. Since it does not support relational databases with multiple tables, in order to use Cognito, data scientists need to get as input one table produced from raw data of multiple tables.Limitations :· No open source implementation.· Extra efforts needed with relational data.We can use these techniques to come with large number of features without investing much time and focus more on other aspects of machine learning which is modelling and at scale deployment.",22/02/2020,0,47.0,0.0,655.0,425.0,2.0,0.0,0.0,2.0,en
3844,Clustering documents with Python,Towards Data Science,Dimitris Panagopoulos,117.0,4.0,491.0,"Natural Language Processing has made huge advancements in the last years. Currently, various implementations of neural networks are cutting edge and it seems that everybody talks about them. But, sometimes a simpler solution might be preferable. After all, one should try to walk before running. In this short article, I am going to demonstrate a simple method for clustering documents with Python. All code is available at GitHub (please note that it might be better to view the code in nbviewer).We are going to cluster Wikipedia articles using k-means algorithm. The steps for doing that are the following:2. represent each article as a vector,3. perform k-means clustering,4. evaluate the result.Using the wikipedia package it is very easy to download content from Wikipedia. For this example, we will use the content of the articles for:The content of each Wikipedia article is stored wiki_list while the title of each article is stored in variable title.Since we are going to use k-means, we need to represent each article as a numeric vector. A popular method is to use term-frequency/inverse-document-frequency (tf-idf). Put it simply, with this method for each word w and document d we calculate:tf(w,d): the ratio of the number of appearances of w in d divided with the total number of words in d.idf(w): the logarithm of the fraction of the total number of documents divided by the number of documents that contain w.tfidf(w,d)=tf(w,d) x idf(w)It is recommended that common, stop words are excluded. All the calculations are easily done with sklearn’s TfidfVectorizer.(To be honest, the calculation of sklearn is slightly different. You can read about in the documentation.)Each row of variable X is a vector representation of a Wikipedia article. Hence, we can use X as input for the k-means algorithm.First, we must decide on the number of clusters. Here, we will use the elbow method.The plot is almost a straight line, probably because we have to few articles. But at a closer examination a dent appears for k=4 or k=6. We will try to cluster into 6 groups.Since we have used only 10 articles, it is fairly easy to evaluate the clustering just by examining what articles are contained in each cluster. That would be difficult for a large corpus. A nice way is to create a word cloud from the articles of each cluster.It might seem odd that swimming is not in the same cluster as basketball and tennis. Or that AI and Machine Learning are not in the same group with Data Science. That is because we choose to create 6 clusters. But by looking at the word clouds we can see that articles on basket and tennis have words like game, player, team, ball in common while the article on swimming does not.At the top of the GitHub page, there is a button that will allow you to execute the notebook in Google’s Colab. You can easily try a different number of clusters. The results might surprise you!",05/08/2020,5,0.0,9.0,627.0,352.0,3.0,3.0,0.0,5.0,en
3845,How I got my first android job without a degree or experience,Medium,Mo Hajr,35.0,7.0,1243.0,"It’s a great time to work as an android developer, as Millions of android devices activated every day a huge demand for android developers is required.Begging as an android developer can be extremely challenging too, so in this post, I will try to elaborate all the basic requirements and skills anyone needs to land a job as an android developer without the need of a degree or experience.So below is a list of all generalized requirements based on my little experience as an android developer and my researching for junior-level positions , the requirements will always vary from company to another and you will hardly find any two job descriptions exactly the same but these requirements will be good to start with.You might consider that this a lot of things but you can build up these skills faster than what you think.From this day and on this language will be your new (girlfriend or boyfriend), 95% if it’s not 100% of the apps that you will make will be fully written in this language.you should be really comfortable using java programming language what I mean by comfortable is that you should have a well-rounded knowledge of the basic syntax. You should know things like variables, loops, lists, Maps and object-oriented concepts like class vs static methods and inheritance.If You don’t know java yet I would recommend Udacity It’s one of my favorite platforms as I started my journey as an android developer with them and till that day am still using their platform for learning.You can learn Java while you are making android apps with this course Android Development for beginnersthere is no app without a UI and XML is your tool to create your layouts in your android app, You need have a basic understanding of the android layouts and views , as if you saw a layout you can immediately tell how and what you will use to draw it , also the ability to make responsive layouts that supports different densities from the smallest phones to the largest tablets.Again the Android Development for beginners teach you XML while you are making android apps.I can talk at a stretch about this , It will be something that each day while you are developing ,learning something new in it, I will mention the basic and essential concepts that every developer starting should be aware ofAgain, this goes without saying as a developer you should be really good at searching and finding what you need like searching for bugs , most of the developers I met copy and paste is their lifestyle, they don’t bother themselves to know what the code actually does, don’t be like that …… try to understand everything that you copy read a lot about it, in order to have the full picture and to know the infrastructure of what exactly happens in behind.“What is the difference between us and the users if we don’t know exactly how it works”Developer.android website has a huge documentation and guides and code snippets that can help along the way when you start implemnting ,learning or understanding anything about the android sdk, this should be the first place you look in and if you felt that you still don’t understand you can then start searching for blogposts or video tutouials about it.Getting data from the web is something really essential , there will be alot of times when you need to provide your users with fresh and renewable data.I would recommend focusing on JSON/REST APIs as all the time since I Start as an android developer I didn’t use XML/SOAP They are similar in the concepts but still different in the way of requesting and parsing the data but just focus on JSON/REST those are really enough, Also working with 3rd party APIs like facebook , google ..Etc, is a huge plus for you to land a job and it really facilitate a lot of things in the development process for you to implement for example the login process.Maybe you will think that your technical skills are the most important and to some extent this is right but actually I was able to land the job because of my non-technical skills as I had no degree , no prior experience and am also in a foreign country so pretty much anyone in that situation will think that he doesn’t have any chance but actually you do.the reason I been able to get the role even though I didn’t have the required technical skills, is that I showed how passionate I’m about the companies product and how eager I’m to learn any new tool or technology and do whatever it takes to finish the task.It’s simple as that if you like what you doing then for sure you will have the passion about it , the problem lies in showing and expressing it, you have to show in the interview how passionate you are about what the company does , also when you encounter a question that you don’t know or a technology that you never heard about that’s totally okay but you should never reply with I Don’t know ,show the intention that you would learn it or your previous projects didn’t simply require you to touch this area.2.CommunicationThe Ability to express to collaborate and communicate with teams One of the most important things , as most of the developers tend to be more introverts and having hard times to express themselves in a clear way, even if you are the only developer you will have to engage with the marketing or operations team at some point and working beside them to some extent.So you have to make sure that can explain what you are working on for both technical and non-technical people without having hard times doing it.this is not something essential or required for you to develop android but many jobs will ask you to work on their backend system as well , As when I was having my interview in my current position they ask me if I have experience with php but didn’t have it as skill and still managed to get the job , there is a lot of backend technologies like (Node.js, PHP, Django.. etc)but you will need to learn one as start , you don’t need to have deep understanding of it as you are not a backend developer , you just need to have an overall understanding of how it works and have to read it and deal with it if needed.this is not something required too , but it will be really a plus for you if you know how to deal with version control as it facilitate a lot for you specially when you are working with teams , again you don’t need to be expert in it , you just need to know the basics of it as creating and using repositories and committing and pushing your changes.this would play a huge role for you as it will speaks for you and your experience , It doesn’t have to be something really good or fancy but even a simple app published on Google Play demonstrates that you have a baseline of knowledge about the entire app development lifecycle.Thanks a lot for reading all the way till here , I hope that you found the article useful . Any feedback or correction for my information that I provided would be highly appreciated it.",07/10/2016,0,11.0,26.0,788.0,426.0,8.0,1.0,0.0,4.0,en
3846,Machine Learning —Fundamentals,Towards Data Science,Javaid Nabi,978.0,13.0,2175.0,"This article introduces the basics of machine learning theory, laying down the common concepts and techniques involved. This post is intended for the people starting with machine learning, making it easy to follow the core concepts and get comfortable with machine learning basics.In 1959, Arthur Samuel, a computer scientist who pioneered the study of artificial intelligence, described machine learning as “the study that gives computers the ability to learn without being explicitly programmed.”Alan Turing’s seminal paper (Turing, 1950) introduced a benchmark standard for demonstrating machine intelligence, such that a machine has to be intelligent and responsive in a manner that cannot be differentiated from that of a human being.Machine Learning is an application of artificial intelligence where a computer/machine learns from the past experiences (input data) and makes future predictions. The performance of such a system should be at least human level.A more technical definition given by Tom M. Mitchell’s (1997) : “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” Example:In order to perform the task T, the system learns from the data-set provided. A data-set is a collection of many examples. An example is a collection of features.Machine Learning is generally categorized into three types: Supervised Learning, Unsupervised Learning, Reinforcement learningIn supervised learning the machine experiences the examples along with the labels or targets for each example. The labels in the data help the algorithm to correlate the features.Two of the most common supervised machine learning tasks are classification and regression.When we have unclassified and unlabeled data, the system attempts to uncover patterns from the data . There is no label or target given for the examples. One common task is to group similar examples together called clustering.Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal. For example, maximize the points won in a game over many moves.Regression is a technique used to predict the value of a response (dependent) variables, from one or more predictor (independent) variables.Most commonly used regressions techniques are: Linear Regression and Logistic Regression. We will discuss the theory behind these two prominent techniques alongside explaining many other key concepts like Gradient-descent algorithm, Over-fit/Under-fit, Error analysis, Regularization, Hyper-parameters, Cross-validation techniques involved in machine learning.In linear regression problems, the goal is to predict a real-value variable y from a given pattern X. In the case of linear regression the output is a linear function of the input. Letŷ be the output our model predicts: ŷ = WX+bHere X is a vector (features of an example), W are the weights (vector of parameters) that determine how each feature affects the prediction andb is bias term. So our task T is to predict y from X, now we need to measure performance P to know how well the model performs.Now to calculate the performance of the model, we first calculate the error of each example i as:we take the absolute value of the error to take into account both positive and negative values of error.Finally we calculate the mean for all recorded absolute errors (Average sum of all absolute errors).Mean Absolute Error (MAE) = Average of All absolute errorsMore popular way of measuring model performance is usingMean Squared Error (MSE): Average of squared differences between prediction and actual observation.The mean is halved (1/2) as a convenience for the computation of the gradient descent [discussed later], as the derivative term of the square function will cancel out the 1/2 term. For more discussion on the MAE vs MSE please refer [1] & [2].The main aim of training the ML algorithm is to adjust the weights W to reduce the MAE or MSE.To minimize the error, the model while experiencing the examples of the training set, updates the model parameters W. These error calculations when plotted against the W is also called cost function J(w), since it determines the cost/penalty of the model. So minimizing the error is also called as minimization the cost function J.When we plot the cost function J(w) vs w. It is represented as below:As we see from the curve, there exists a value of parameters W which has the minimum cost Jmin. Now we need to find a way to reach this minimum cost.In the gradient descent algorithm, we start with random model parameters and calculate the error for each learning iteration, keep updating the model parameters to move closer to the values that results in minimum cost.repeat until minimum cost: {}In the above equation we are updating the model parameters after each iteration. The second term of the equation calculates the slope or gradient of the curve at each iteration.The gradient of the cost function is calculated as partial derivative of cost function J with respect to each model parameter wj, j takes value of number of features [1 to n]. α, alpha, is the learning rate, or how quickly we want to move towards the minimum. If α is too large, we can overshoot. If α is too small, means small steps of learning hence the overall time taken by the model to observe all examples will be more.There are three ways of doing gradient descent:Batch gradient descent: Uses all of the training instances to update the model parameters in each iteration.Mini-batch Gradient Descent: Instead of using all examples, Mini-batch Gradient Descent divides the training set into smaller size called batch denoted by ‘b’. Thus a mini-batch ‘b’ is used to update the model parameters in each iteration.Stochastic Gradient Descent (SGD): updates the parameters using only a single training instance in each iteration. The training instance is usually selected randomly. Stochastic gradient descent is often preferred to optimize cost functions when there are hundreds of thousands of training instances or more, as it will converge more quickly than batch gradient descent [3].In some problems the response variable is not normally distributed. For instance, a coin toss can result in two outcomes: heads or tails. The Bernoulli distribution describes the probability distribution of a random variable that can take the positive case with probability P or the negative case with probability 1-P. If the response variable represents a probability, it must be constrained to the range {0,1}.In logistic regression, the response variable describes the probability that the outcome is the positive case. If the response variable is equal to or exceeds a discrimination threshold, the positive class is predicted; otherwise, the negative class is predicted.The response variable is modeled as a function of a linear combination of the input variables using the logistic function.Since our hypotheses ŷ has to satisfy 0 ≤ ŷ ≤ 1, this can be accomplished by plugging logistic function or “Sigmoid Function”The function g(z) maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification. The following is a plot of the value of the sigmoid function for the range {-6,6}:Now coming back to our logistic regression problem, Let us assume that z is a linear function of a single explanatory variable x. We can then express z as follows:And the logistic function can now be written as:Note that g(x) is interpreted as the probability of the dependent variable.g(x) = 0.7, gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).The input to the sigmoid function ‘g’ doesn’t need to be linear function. It can very well be a circle or any shape.We cannot use the same cost function that we used for linear regression because the Sigmoid Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.In order to ensure the cost function is convex (and therefore ensure convergence to the global minimum), the cost function is transformed using the logarithm of the sigmoid function. The cost function for logistic regression looks like:Which can be written as:So the cost function for logistic regression is:Since the cost function is a convex function, we can run the gradient descent algorithm to find the minimum cost.We try to make the machine learning algorithm fit the input data by increasing or decreasing the models capacity. In linear regression problems, we increase or decrease the degree of the polynomials.Consider the problem of predicting y from x ∈ R. The leftmost figure below shows the result of fitting a line to a data-set. Since the data doesn’t lie in a straight line, so fit is not very good (left side figure).To increase model capacity, we add another feature by adding term x² to it. This produces a better fit ( middle figure). But if we keep on doing so ( x⁵, 5th order polynomial, figure on the right side), we may be able to better fit the data but will not generalize well for new data. The first figure represents under-fitting and the last figure represents over-fitting.When the model has fewer features and hence not able to learn from the data very well. This model has high bias.When the model has complex functions and hence able to fit the data very well but is not able to generalize to predict new data. This model has high variance.There are three main options to address the issue of over-fitting:Regularization can be applied to both linear and logistic regression by adding a penalty term to the error function in order to discourage the coefficients or weights from reaching large values.The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified linear regression error function:where lambda is our regularization parameter.Now in order to minimize the error, we use gradient descent algorithm. We keep updating the model parameters to move closer to the values that results in minimum cost.repeat until convergence ( with regularization): {}With some manipulation the above equation can also be represented as:The first term in the above equation,will always be less than 1. Intuitively you can see it as reducing the value of the coefficient by some amount on every update.The cost function of the logistic regression with Regularization is:repeat until convergence ( with regularization): {}The regularization term used in the previous equations is called L2 or Ridge regularization.The L2 penalty aims to minimize the squared magnitude of the weights.There is another regularization called L1 or Lasso:The L1 penalty aims to minimize the absolute value of the weightsDifference between L1 and L2 L2 shrinks all the coefficient by the same proportions but eliminates none, while L1 can shrink some coefficients to zero, thus performing feature selection. For more details read this.Hyper-parameters are “higher-level” parameters that describe structural information about a model that must be decided before fitting model parameters, examples of hyper-parameters we discussed so far:Learning rate alpha , Regularization lambda.The process to select the optimal values of hyper-parameters is called model selection. if we reuse the same test data-set over and over again during model selection, it will become part of our training data and thus the model will be more likely to over fit.The overall data set is divided into:The training set is used to fit the different models, and the performance on the validation set is then used for the model selection. The advantage of keeping a test set that the model hasn’t seen before during the training and model selection steps is that we avoid over-fitting the model and the model is able to better generalize to unseen data.In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is to use cross-validation, which is illustrated in Figure below.Below Cross-validation steps are taken from here, adding here for completeness.These are the steps for selecting hyper-parameters using K-fold cross-validation:Cross-validation allows us to tune hyper-parameters with only our training set. This allows us to keep the test set as a truly unseen data-set for selecting final model.We’ve covered some of the key concepts in the field of Machine Learning, starting with the definition of machine learning and then covering different types of machine learning techniques. We discussed the theory behind the most common regression techniques (Linear and Logistic) alongside discussed other key concepts of machine learning.Thanks for reading.[1] https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d[2] https://towardsdatascience.com/ml-notes-why-the-least-square-error-bf27fdd9a721[3] https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3[4] https://elitedatascience.com/machine-learning-iteration#micro",15/08/2018,2,35.0,50.0,519.0,137.0,30.0,3.0,0.0,7.0,en
3847,Activation Functions in Neural Networks,Towards Data Science,SAGAR SHARMA,3700.0,5.0,660.0,"It’s just a thing function that you use to get the output of node. It is also known as Transfer Function.It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).The Activation Functions can be basically divided into 2 types-FYI: The Cheat sheet is given below.As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range.Equation : f(x) = xRange : (-infinity to infinity)It doesn’t help with the complexity or various parameters of usual data that is fed to the neural networks.The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like thisIt makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.The main terminologies needed to understand for nonlinear functions are:Derivative or Differential: Change in y-axis w.r.t. change in x-axis.It is also known as slope.Monotonic function: A function which is either entirely non-increasing or non-decreasing.The Nonlinear Activation Functions are mainly divided on the basis of their range or curves-The Sigmoid Function curve looks like a S-shape.The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.The function is monotonic but function’s derivative is not.The logistic sigmoid function can cause a neural network to get stuck at the training time.The softmax function is a more generalized logistic activation function which is used for multiclass classification.tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.The function is differentiable.The function is monotonic while its derivative is not monotonic.The tanh function is mainly used classification between two classes.Both tanh and logistic sigmoid activation functions are used in feed-forward nets.The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.Range: [ 0 to infinity)The function and its derivative both are monotonic.But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.It is an attempt to solve the dying ReLU problemCan you see the Leak? 😆The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.When a is not 0.01 then it is called Randomized ReLU.Therefore the range of the Leaky ReLU is (-infinity to infinity).Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.Happy to be helpful. Support me.So, follow me on Medium, LinkedIn to see similar posts.Any comments or if you have any questions, write them in the comment.Clap it! Share it! Follow Me!theffork.comhackernoon.comtowardsdatascience.comtowardsdatascience.comtowardsdatascience.comtowardsdatascience.commedium.com",06/09/2017,0,44.0,0.0,719.0,412.0,11.0,1.0,0.0,10.0,en
3848,Train Object Detection AI with 6 lines of code,DeepQuestAI,Moses Olafenwa,3500.0,9.0,1052.0,"Step-by-step tutorial on training object detection models on your custom datasetObject detection is one of the most profound aspects of computer vision as it allows you to locate, identify, count and track any object-of-interest in images and videos. Object detection is used extensively in many interesting areas of work and study such as:A number of pre-collected object detection datasets such as Pascal VOC, Microsoft’s COCO, Google’s Open Images are readily available along with their pre-trained models for detection and identifying only a fix set of items.However, the challenge with using these public datasets and pre-trained models is that they do not provide a convenient way for you to easily train new object detection models to detect and identify your desired object(s) of interest. Since the past one year that I published my first Object Detection article “Object Detection with 10 lines of code”, I have received thousands of request from developers, teams, students and researchers who desire to detect their own objects-of-interest in images and videos, beyond the 80 classes provided in the COCO dataset and 600 classes provided in the Open Images dataset.I am most glad to announce that with the latest release of ImageAI v2.1.0, support for training your custom YOLOv3 models to detect literally any kind and number of objects is now fully supported, and that is what we will guide you to do in this tutorial. Let’s get started.github.comFor the purpose of this tutorial, we will be using Google Colab to train on a sample dataset we have provided. Follow the steps below.Step 1 — Preparing your datasetFor your custom detection training, you have to provide sample images ( your image dataset ) for training your model and validating the model after training for accuracy. ImageAI detection training supports the Pascal VOC format for your custom dataset. For the purpose of this tutorial, we have provided a sample dataset for the Hololens Mixed Reality headset, on which we will train a model that can detect and identify the Hololens in pictures and videos. You can download the sample dataset via the link below.https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/headset.zipIf you want to train on your own images for your custom object(s), follow the instructions below.medium.comStep 2 — Installing ImageAI and DependenciesGo to https://colab.research.google.com and create a new notebook. Ensure you change the runtime for your new notebook to a GPU. Then:i. TensorFlowii. Other Dependenciesiii. ImageAIpip install imageai --upgradeStep 3 — Initiate your detection model trainingTo ensure that our trained custom models have better detection accuracy, we will be using transfer learning from a pre-trained YOLOv3 model in the training. ImageAI provides the option to train with and without transfer learning. I will strongly recommend you use transfer learning except you have thousands of object samples in your dataset.SIMPLE! The above 6-lines of code is all you need to initiate the training on your custom dataset. Now let’s break down the code to its part:— object_names_array: This is an array of the names of all the objects in your dataset. Please note that if your custom dataset annotation has more than one object, simple set the values as shown in the example below— batch_size: This is the batch size for the training. Kindly note that the larger the batch size, the better the detection accuracy of the saved models. However, due to memory limits on the Nvidia K80 GPU available on Colab, we have to keep this value as 4. The batch size can be values of 8, 16 and so on.— num_experiments: This is the number of times we want the training code to iterate on our custom dataset.— train_from_pretrained_model: This is used to leverage transfer learning using the pretrained YOLOv3 model we downloaded earlier.Once the training starts,Step 4 — Evaluate your modelsIn the sample log shown above, new models are saved based on the decrease in the validation loss (E.g — loss: 5.2569) . In most cases, the lower the loss, the more accurate the model will be detecting objects in images and videos. However, some models may experience overfitting and still have lower losses. To ensure that you pick the best model for your custom detection, ImageAI allows you to evaluate the mAP (mean Average Precision, read more about it here) of all the trained models saved in the hololens/models folder.The higher the mAP, the better the detection accuracy of the model.Simple run the code below on the models saved during the training.When you run the above code, you get a result like the example below.Let’s breakdown the evaluation code:— model_path: This is the path to the folder containing our models. It can also be the filepath to a specific model.— json_file: This is the path to the detection_config.json file saved during the training.— iou_threshold: This is our desired minimum Intersection over Union value for the mAP computation. It can be set to values between 0.0 to 1.0— object_threshold: This is our desired minimum class score for the mAP computation. It can be set to values between 0.0 to 1.0.— nms_threshold: This is our desired Non-maximum suppression for the mAP computation.Step 5 — Detecting our custom Object in an imageNow that we have trained our custom model to detect the Hololens headset, we will use the best model saved as well as the detection_config.json file generated to detect the object in an image.https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/hololens-ex-60--loss-2.76.h5https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/detection_config.jsonWhen we run the above code, we get the result below.— RESULT —VOILA! Now we have been able to successfully train a new detection model to detect the Hololens Mixed Reality headset.If you enjoyed this article, give it as many claps as you will like to. Also share with friends and colleagues as well.The Colab Notebook containing all the codes in this tutorial is available via the link below.colab.research.google.comOn a final note, ImageAI also allows you to use your custom detection model to detect objects in videos and perform video analysis as well. See the documentations and the sample codes links provided below.github.comVideo Detection Documentation:github.comExample codes:github.comIf you will like to know everything about how object detection works with links to more useful and practical resources, visit the Object Detection Guide linked below.www.fritz.aiThe ImageAI project and this tutorial brought to you by DeepQuest AI.At DeepQuest AI, our mission we chose to accept is to advance and democratize Artificial Intelligence, and make it accessible to every individual and organization on the planet.deepquestai.com",01/08/2019,10,74.0,5.0,825.0,574.0,4.0,12.0,0.0,16.0,en
3849,Diving into Abstractive Text Summarization — Part 1,Intel Student Ambassadors,Vincenzo Santopietro,30.0,6.0,825.0,"Text summarization is nowadays one of the most studied research topics in natural language processing (NLP) and has its applications in almost all domains of the internet, for example, e-shops, search engines and news websites that use summaries to give readers an overview of what a particular article might talk about.Text Summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text.[1]Text summarization algorithms can be classified into two main categories:Extractive text summarization algorithms are capable of extracting key sentences from a text without modifying any word [2][3]. Abstractive summarization, instead, involves a complex process understanding the language, the context and generating new sentences. This frees the model of the constraint of using pre-written text but involves using large-scale data during training.A lot of algorithms for both extractive and abstractive text summarization are based on Recurrent Neural Networks(RNN). Furthermore, using RNNs in an Encoder-Decoder manner leads us to the well known Sequence-To-Sequence (Seq2Seq) architecture [4], which is one of the most used and best-performing approaches in machine translation. Most of the current advancements have been performed on very short summaries and documents: a lot of algorithms tend to perform worse when a big document has to be summarized in more than a few words. The majority of state-of-the-art algorithms use pre-trained word embeddings, for a better understanding of the concepts expressed in a text.Nowadays the amount of textual data is massive, and this led researchers to develop algorithms capable of representing words mathematically, in order to solve complex problems. What are word embeddings? Wikipedia states that a word embeddingis the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension.Word embeddings are learned in order to make these vectors capture all the semantical, hierarchical and contextual pieces of information in a word. The process of choosing the best embeddings for a project is often a trial-and-error methodology. By the way, some of the most famous and effective techniques for learning word embeddings are Word2Vec [5] and GloVe [6].Let’s focus on GloVe: it is an unsupervised learning algorithm developed at Stanford University for generating word embeddings by factorizing a global word-word co-occurrence matrix from a corpus. Resulting embeddings show interesting linear substructures.Embeddings of words like Verizon and Vodafone are very close to each other, while man and woman are quite distant.For this project I’ve used Keras, an Open Source Deep Learning Library written in Python, running on top of Intel® Optimization for TensorFlow. In practice, you can download GloVe pre-trained vectors here and add them to your model in Keras using an Embedding Layer.Once we set up a dictionary which maps words to vectors, we can build the embedding weights matrix that’ll be set in the model.We can now create the Embedding layer in Keras using the following command:There are plenty of datasets for text summarization. The most famous (and large) is the GigaWord Dataset, which can be purchased here and consists of nearly 10 million documents and over 4 billion tokens. If you want to do some experiments on publicly available datasets first, there are also the BBC News Summary and the Amazon Fine Food Reviews datasets.We can easily build a vanilla Seq2Seq model for abstractive text summarization using Keras, in order to train the model and evaluate its performance. A bidirectional LSTM is used for the Encoder, while the Decoder is made of a single unidirectional LSTM. The model has been trained on a single node of an Intel® Xeon® platform.Making a machine measure how much a predicted summary is “good” may seem quite strange. Furthermore, applying the standard accuracy measure is not the right choice for summarization. In 2004 Lin published a paper about a new package with more than just one metric used for evaluating automatic summarization and machine translation algorithms, called Rouge: Recall-Oriented Understudy for Gisting Evaluation [7]. The most common metrics from ROUGE are:There are a few GitHub repositories with Python implementations of the ROUGE metrics that you can use in your own projects, like this and this.We’ve discovered what is text summarization, how words can be represented mathematically in a vector space, a Seq2Seq model used for abstractive summarization, some of the datasets available and a set of metrics used to evaluate summaries. In Part. II the training process either on a single node or on multiple nodes will be explained in detail and some results will be shown.This article is one of a series of posts done as part of an Intel AI Incubator project for which I am part of. The project makes use of Intel® Optimization for TensorFlow and Scikit-Learn. Intel® Xeon® Scalable processors were used to train the model.",14/02/2019,4,0.0,15.0,640.0,497.0,2.0,3.0,0.0,15.0,en
3850,Introduction to ResNets,Towards Data Science,Connor Shorten,1910.0,6.0,1010.0,"This Article is Based on Deep Residual Learning for Image Recognition from He et al. [2] (Microsoft Research): https://arxiv.org/pdf/1512.03385.pdfIn 2012, Krizhevsky et al. [1] rolled out the red carpet for the Deep Convolutional Neural Network. This was the first time this architecture was more successful that traditional, hand-crafted feature learning on the ImageNet. Their DCNN, named AlexNet, contained 8 neural network layers, 5 convolutional and 3 fully-connected. This laid the foundational for the traditional CNN, a convolutional layer followed by an activation function followed by a max pooling operation, (sometimes the pooling operation is omitted to preserve the spatial resolution of the image).Much of the success of Deep Neural Networks has been accredited to these additional layers. The intuition behind their function is that these layers progressively learn more complex features. The first layer learns edges, the second layer learns shapes, the third layer learns objects, the fourth layer learns eyes, and so on. Despite the popular meme shared in AI communities from the Inception movie stating that “We need to go Deeper”, He et al. [2] empirically show that there is a maximum threshold for depth with the traditional CNN model.He et al. [2] plot the training and test error of a 20-layer CNN versus a 56-layer CNN. This plot defies our belief that adding more layers would create a more complex function, thus the failure would be attributed to overfitting. If this was the case, additional regularization parameters and algorithms such as dropout or L2-norms would be a successful approach for fixing these networks. However, the plot shows that the training error of the 56-layer network is higher than the 20-layer network highlighting a different phenomenon explaining it’s failure.Evidence shows that the best ImageNet models using convolutional and fully-connected layers typically contain between 16 and 30 layers.The failure of the 56-layer CNN could be blamed on the optimization function, initialization of the network, or the famous vanishing/exploding gradient problem. Vanishing gradients are especially easy to blame for this, however, the authors argue that the use of Batch Normalization ensures that the gradients have healthy norms. Amongst the many theories explaining why Deeper Networks fail to perform better than their Shallow counterparts, it is sometimes better to look for empirical results for explanation and work backwards from there. The problem of training very deep networks has been alleviated with the introduction of a new neural network layer — The Residual Block.The picture above is the most important thing to learn from this article. For developers looking to quickly implement this and test it out, the most important modification to understand is the ‘Skip Connection’, identity mapping. This identity mapping does not have any parameters and is just there to add the output from the previous layer to the layer ahead. However, sometimes x and F(x) will not have the same dimension. Recall that a convolution operation typically shrinks the spatial resolution of an image, e.g. a 3x3 convolution on a 32 x 32 image results in a 30 x 30 image. The identity mapping is multiplied by a linear projection W to expand the channels of shortcut to match the residual. This allows for the input x and F(x) to be combined as input to the next layer.Equation used when F(x) and x have a different dimensionality such as 32x32 and 30x30. This Ws term can be implemented with 1x1 convolutions, this introduces additional parameters to the model.An implementation of the shortcut block with keras from https://github.com/raghakot/keras-resnet/blob/master/resnet.py. This shortcut connection is based on a more advanced description from the subsequent paper, “Identity Mappings in Deep Residual Networks” [3].Another great implementation of Residual Nets in keras can be found here →The Skip Connections between layers add the outputs from previous layers to the outputs of stacked layers. This results in the ability to train much deeper networks than what was previously possible. The authors of the ResNet architecture test their network with 100 and 1,000 layers on the CIFAR-10 dataset. They test on the ImageNet dataset with 152 layers, which still has less parameters than the VGG network [4], another very popular Deep CNN architecture. An ensemble of deep residual networks achieved a 3.57% error rate on ImageNet which achieved 1st place in the ILSVRC 2015 classification competition.A similar approach to ResNets is known as “highway networks”. These networks also implement a skip connection, however, similar to an LSTM these skip connections are passed through parametric gates. These gates determine how much information passes through the skip connection. The authors note that when the gates approach being closed, the layers represent non-residual functions whereas the ResNet’s identity functions are never closed. Empirically, the authors note that the authors of the highway networks have not shown accuracy gains with networks as deep as they have shown with ResNets.The architecture they used to test the Skip Connections followed 2 heuristics inspired from the VGG network [4].Overall, the design of a 34-layer residual network is illustrated in the image below:In Conclusion, the Skip Connection is a very interesting extension to Deep Convolutional Networks that have empirically shown to increase performance in ImageNet classification. These layers can be used in other tasks requiring Deep networks as well such as Localization, Semantic Segmentation, Generative Adversarial Networks, Super-Resolution, and others. Residual Networks are different from LSTMs which gate previous information such that not all information passes through. Additionally, the Skip Connections shown in this article are essentially arranged in 2-layer blocks, they do not use the input from same layer 3 to layer 8. Residual Networks are more similar to Attention Mechanisms in that they model the internal state of the network opposed to the inputs. Hopefully this article was a useful introduction to ResNets, thanks for reading!References[1] Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. 2012.[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. 2015.[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Identity Mappings in Deep Residual Networks. 2016.[4] Karen Simonyan, Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. 2014.",24/01/2019,0,2.0,5.0,627.0,438.0,8.0,2.0,0.0,3.0,en
3851,NLP — Text Summarization using NLTK: TF-IDF Algorithm,Towards Data Science,Akash Panchal,293.0,10.0,651.0,"In the Article Text summarization in 5 steps using NLTK, we saw how we summarize the text using Word Frequency Algorithm.Bonus: See in Action with Streamlit AppNow, we’ll summarize the text using Tf-IDF Algorithm.Note that, we’re implementing the actual algorithm here, not using any library to do the most of the tasks, we’re highly relying on the Math only.In a simple language, TF-IDF can be defined as follows:A High weight in TF-IDF is reached by a high term frequency(in the given document) and a low document frequency of the term in the whole collection of documents.TF-IDF algorithm is made of 2 algorithms multiplied together.Term frequency (TF) is how often a word appears in a document, divided by how many words there are.TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)Term frequency is how common a word is, inverse document frequency (IDF) is how unique or rare a word is.IDF(t) = log_e(Total number of documents / Number of documents with term t in it)Example, Consider a document containing 100 words wherein the word apple appears 5 times. The term frequency (i.e., TF) for apple is then (5 / 100) = 0.05.Now, assume we have 10 million documents and the word apple appears in one thousand of these. Then, the inverse document frequency (i.e., IDF) is calculated as log(10,000,000 / 1,000) = 4.Thus, the TF-IDF weight is the product of these quantities: 0.05 * 4 = 0.20.Easy, right? We’ll use the same formula to generate the summary.Oh Yeah, I Love Math.Perquisites Python3, NLTK library of python, Your favourite text editor or IDEWe’ll tokenize the sentences here instead of words. And we’ll give weight to these sentences.We calculate the frequency of words in each sentence.The result would be something like this:Here, each sentence is the key and the value is a dictionary of word frequency.We’ll find the TermFrequency for each word in a paragraph.Now, remember the definition of TF,TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)Here, the document is a paragraph, the term is a word in a paragraph.Now the resultant matrix would look something like this:If we compare this table with the table we’ve generated in step 2, you will see the words having the same frequency are having the similar TF score.This again a simple table which helps in calculating IDF matrix.we calculate, “how many sentences contain a word”, Let’s call it Documents per words matrix.This is what we get now,i.e, the word resili appears in 2 sentences, power appears in 4 sentences.We’ll find the IDF for each word in a paragraph.Now, remember the definition of IDF,IDF(t) = log_e(Total number of documents / Number of documents with term t in it)Here, the document is a paragraph, the term is a word in a paragraph.Now the resultant matrix would look something like this:Compare the same with the TF matrix and see the difference.Now we have both the matrix and the next step is very easy.TF-IDF algorithm is made of 2 algorithms multiplied together.In simple terms, we are multiplying the values from both the matrix and generating new matrix.Scoring a sentence is differs with different algorithms. Here, we are using Tf-IDF score of words in a sentence to give weight to the paragraph.This gives the table of sentences and their respected score:Similar to any summarization algorithms, there can be different ways to calculate a threshold value. We’re calculating the average sentence score.We get the following score as an average:Algorithm: Select a sentence for a summarization if the sentence score is more than the average score.For the threshold, we’ve used 1.3x of the average score. You can play with such variables to generate the summary as you like.Original text:Summarized text:Voila! You’ve just summarized the text using the infamous Tf-IDF algorithm. Show off to your friends now. 😎Note: This is an extractive text summarization technique.github.compythonizr.com",10/06/2019,9,33.0,7.0,1400.0,929.0,1.0,1.0,0.0,7.0,en
3852,Gaussian Mixture Model clustering: how to select the number of components (clusters),Towards Data Science,Vincenzo Lavorini,230.0,7.0,1261.0,"If you landed on this post, you probably already know what a Gaussian Mixture Model is, so I will avoid the general description of the this technique.But if you are not aware of the details, you can just see the GMM as a k-means which is able to form stretched clusters, like the ones you can see in Figure 2.All the code used for this post is in this notebook. In the same repository you can find the data to fully replicate the results you see plotted.Now: suppose you are in the situation depicted in Figure 1, you want to discern how many clusters we have (or, if you prefer, how many gaussians components generated the data), and you don’t have information about the “ground truth”. A real case, where data do not have the nicety of behaving good as the simulated ones.At a first look, one could scream “Three main cluster plus two minor!”. And maybe it’s just correct, but here we want to check for an automated method for finding the “right” number of clusters.It’s to be noted that this “right” number is something fluffy anyway, since the specificity of each problem can lead one to decide to overcome the decision of the automated algorithm.But let me introduce the GMM results for the configuration we just announced (five clusters). In the plot below (Figure 2) we fitted a GMM with five components to our data. And we have our first issue: same data, same model, but different results.The guilty for this behavior is the fitting procedure: the Expectation-Maximization (EM) algorithm. This algorithm only guarantee that we land to a local optimal point, but it do not guarantee that this local optima is also the global one. And so, if the algorithm starts from different initialization points, in general it lands into different configurations.While there are other procedures to fit the GMM, we want to stick on this one: the other methods are more complex and require more hyperparameters to be set; and this exceed the goals for this post.We, then, have to take in account of the non-deterministic nature of our guy.The easiest way of dealing with it is to run the fitting procedure several times, and consider the mean value and the standard deviation for each configuration. Put simple: we account for an error on each fit.Since we don’t know the ground truth of our cluster generators, i.e. we are not aware of the original distribution which generated the data, our choices about the performance evaluation of the clustering process are limited and quite noisy.Nonetheless, we will explore three different techniques.This score, as clearly stated by the SKLearn developers, consider two measures:i.e. it checks how much the clusters are compact and well separated. The more the score is near to one, the better the clustering is.Since we already know that the fitting procedure is not deterministic, we run twenty fits for each number of clusters, then we consider the mean value and the standard deviation of the best five runs. The results are in Figure 3.Well, it turns out that we get the best score with five cluters. We have also to consider that also the four cluster configuration is almost equally good, if we consider the standard deviation (the ‘error’) of both the configurations. So, this score do not gives us a clear result.Here we form two datasets, each with an half randomly choose amount of data. We will then check how much the GMMs trained on the two sets are similar, for each configuration.Since we are talking about distributions, the concept of similarity is embedded in the Jensen-Shannon (JS) metric. The lesser is the JS-distance between the two GMMs, the more the GMMs agree on how to fit the data.All the credits on the (beautiful!) way the Jensen-Shannon metric is calculated go to Dougal, and to its post on StackOverflow.Following this technique, the configuration with three clusters is the most conservative; this do not necessarily means that it’s the good configuration, since we are just talking about reproducibility of the results between the two sets.The interesting part of this plot is the sudden change in the distance we see after passing the six cluster configuration: both the mean distance and its associated error show a huge increase.This means that starting from seven clusters, the GMMs of the two sets are much more different (since the distance is much bigger), and also less stable (since the error band is much bigger).We can say that the good configuration, which takes in account both of the amount of information included (=biggest possible number of clusters) and on the stability of the fitting procedure (=lowest possible GMMs distance), is the one which considers six cluster.This criterion gives us an estimation on how much is good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have, and by extension, the true, unknown, distribution. In order to avoid overfitting, this technique penalizes models with big number of clusters.Following this criterion, the bigger the number of clusters, the better should be the model. Which means that the penalty BIC criteria gives to complex models do not save us from overfit. Or, in more prosaic terms, this score sucks. At least in this basic form.But before screaming and trashing out this technique, we can notice two things. The first is that the curve is fairly smooth and monotone. The second is that the curve follows different slopes in different part of it. Starting from these two observations, the temptation to check where the BIC curve change slope is big. So let’s check it!Technically, we have to calculate the gradient of the BIC scores curve. Intuitively, the concept of gradient is simple: if two consecutive points have the same value, their gradient is zero. If they have different values, their gradient can be eighter negative, if the second point has a lower value, or positive otherwise. The magnitude of the gradient tells us how much the two values are different.As expected, all the gradients have negative values. But we see more clearly that starting from a cluster size of seven the gradient become almost constant, i.e. the original function has a more gentle decrease, i.e. there is no much gain in increasing the number of clusters. In short, this technique suggest us to use six clusters.We have explored three different techniques to choose the right number of clusters which can be discerned in this dataset. The results are in the table below:Even if the decisions are similar, there is no a clear value on which all the strategies agree. In this specific case, this means that the GMM is not a good model to cluster our data.Another sympton you can catch is that both the Silhouette and the gradient of BIC show a second value which is almost as good as the choose one: 4 is almost as good as 5 for the Silhouette, and 5 is almost as good as 6 for the gradient of BIC scores.One could say to just take five (in medio stat virtus) as the right number of clusters, but this could not be the optimal choice. And it turns out that actually it was not for my specific issue.This because the clusters do not show a clear symmetric (oval-like) shape, and so they can not be approximated by a model composed of 2-d gaussians like the one you see in Figure 7, which instead are symmetric.I hope you enjoy the reading!",21/11/2018,0,14.0,3.0,932.0,503.0,9.0,1.0,0.0,7.0,en
3853,Understanding Optimization Algorithms in Machine Learning,Towards Data Science,Supriya Secherla,14.0,5.0,752.0,"Mathematics behind two important optimization techniques in machine learningOptimization is the process where we train the model iteratively that results in a maximum and minimum function evaluation. It is one of the most important phenomena in Machine Learning to get better results.Why do we optimize our machine learning models? We compare the results in every iteration by changing the hyperparameters in each step until we reach the optimum results. We create an accurate model with less error rate. There are different ways using which we can optimize a model. In this article, let’s discuss two important Optimization algorithms: Gradient Descent and Stochastic Gradient Descent Algorithms; how they are used in Machine Learning Models, and the mathematics behind them.2. MAXIMA AND MINIMAMaxima is the largest and Minima is the smallest value of a function within a given range. We represent them as below:Global Maxima and Minima: It is the maximum value and minimum value respectively on the entire domain of the functionLocal Maxima and Minima: It is the maximum value and minimum value respectively of the function within a given range.There can be only one global minima and maxima but there can be more than one local minima and maxima.3. GRADIENT DESCENTGradient Descent is an optimization algorithm and it finds out the local minima of a differentiable function. It is a minimization algorithm that minimizes a given function.Let’s see the geometric intuition of Gradient Descent:Let’s take an example graph of a parabola, Y=X²Here, the minima is the origin(0, 0). The slope here is Tanθ. So the slope on the right side is positive as 0<θ<90 and its Tanθ is a positive value. The slope on the left side is negative as 90<θ<180 and its Tanθ is a negative value.One important observation in the graph is that the slope changes its sign from positive to negative at minima. As we move closer to the minima, the slope reduces.So, how does the Gradient Descent Algorithm work?Objective: Calculate X*- local minimum of the function Y=X².4. LEARNING RATELearning Rate is a hyperparameter or tuning parameter that determines the step size at each iteration while moving towards minima in the function. For example, if r = 0.1 in the initial step, it can be taken as r=0.01 in the next step. Likewise it can be reduced exponentially as we iterate further. It is used more effectively in deep learning.What happens if we keep r value as constant:In the above example, we took r=1. As we calculate the points Xᵢ, Xᵢ+₁, Xᵢ+₂,….to find the local minima, X*, we can see that it is oscillating between X = -0.5 and X = 0.5.When we keep r as constant, we end up with an oscillation problem. So, we have to reduce the “r” value with each iteration. Reduce the r value as the iteration step increases.Important Note: Hyperparameters decide the bias-variance tradeoff. When r value is low, it could overfit the model and cause high variance. When r value is high, it could underfit the model and cause high bias. We can find the correct r value with Cross Validation technique. Plot a graph with different learning rates and check for the training loss with each value and choose the one with minimum loss.5. GRADIENT DESCENT IN LOGISTIC REGRESSIONThe formula for the optimal plane in logistic regression after applying sigmoid function is:Apply Gradient Descent Algorithm on Logistic Regression:We’ll calculate W₀, W₁, W₂, …., Wᵢ-₁, Wᵢ to find W*. When (Wᵢ-₁ — Wᵢ) is small i.e., when Wᵢ-₁, Wᵢ converge, we declare W* = WᵢThe disadvantage of Gradient Descent:When n(number of data points) is large, the time it takes for k iterations to calculate the optimum vector becomes very large.Time Complexity: O(kn²)This problem is solved with Stochastic Gradient Descent and is discussed in the next section.5. STOCHASTIC GRADIENT DESCENT(SGD)In SGD, we do not use all the data points but a sample of it to calculate the local minimum of the function. Stochastic basically means Probabilistic. So we select points randomly from the population.Here, m is the sample of data selected randomly from the population, nTime Complexity: O(km²). m is significantly lesser than n. So, it takes lesser time to compute when compared to Gradient Descent.6. CONCLUSIONIn this article, we discussed Optimization algorithms like Gradient Descent and Stochastic Gradient Descent and their application in Logistic Regression. SGD is the most important optimization algorithm in Machine Learning. Mostly, it is used in Logistic Regression and Linear Regression. It is extended in Deep Learning as Adam, Adagrad.7. REFERENCES[1] Maxima and Minima: https://en.wikipedia.org/wiki/Maxima_and_minima[2] Gradient Descent: https://en.wikipedia.org/wiki/Gradient_descent",18/06/2021,0,13.0,13.0,1400.0,1086.0,8.0,4.0,0.0,11.0,en
3854,"HRNet explained: Human Pose Estimation, Semantic Segmentation and Object Detection",Towards Data Science,Hucker Marius,1400.0,10.0,1628.0,"Outline of HRNet explained:If you know already the basics (CNN + Areas of Application), skip down to section 3 or section 4.HRNet is a state-of-the-art algorithm in the field of semantic segmentation, facial landmark detection, and human pose estimation. It has shown superior results in semantic segmentation on datasets like PASCAL Context, LIP, Cityscapes, AFLW, COFW, and 300W.But first, let’s understand what the fields mean and what kind of algorithm hides behind HRNet.Semantic Segmentation is used to categorize structures of an image into certain classes. This is done by labeling each pixel with a certain class [3]. In the example below all pixels representing the cyclist are a class person and all pixels representing the bicycle are class bicycle [3]. The aim of image segmentation is to let the algorithm segment the image into classes and hence certain structures.What is semantic segmentation used for?There are various use cases and image segmentation is just at the start. It is applied in autonomous driving, medical image diagnostic, handwriting recognition.Facial landmark Detection is used to recognize and localize certain regions of a face such as the nose, mouth, eyes, or eyebrows [2]. In the following image you can see that by using OpenCV it is possible to detect eyebrows, nose, and mouth (visible through the red dots on the left image).What is facial landmark detection used for?You might know face swap filters in Snapchat or Instagram. Or some other filters that change your eyes, nose, or mouth. All these filters work with face landmark detection to detect where a certain part of your face is located. Furthermore, it is used for face morphing and head pose estimation.Human Pose Estimation is similar to face landmark detection except that it is applied to the whole body and that it has more to do with motion. Instead of facial regions, it detects semantic key points such as left should, right knee, etc. [4]. In the image below it is well depicted how the lines draw the body and recognize certain points, often joints, such as hip, knee, shoulder and elbow.In what applications is pose estimation integrated?It’s applied in training robots, e.g. humanoid robots. To learn certain movements the movements of persons are learned. Just a couple of days ago Tesla announced that it wants to become a key player in the field of humanoid robots, which shows that this is an important topic of the future which might change whole industries.Furthermore, it is applied in interactive motion games, such as Microsoft Kinect or other VR applications [4].All these applications and methods rely on Convolutional Neural Networks, which are the fundament of the HRNet.medium.comA Convolutional Neural Network (often ConvNet or CNN) is a special case of a Neural Network often applied in the field of Computer Vision. The Deep Learning algorithm takes an image as input and assigns importance (made up of bias and weight) to its objects [5].Okay, that’s pretty superficial. Let’s see why CNNs are state-of-the-art.First of all, Convolutional Neural Networks as used in HRNet are similar to ordinary Neural Networks. Both are based on Neurons and have weights and biases. The main difference is the architecture. Normal neural networks do not scale well and are hence not useful for images. Images are not flat as most people might think. An image is three-dimensional, e.g. 32x32x3. 32x32 are the pixels that make an image 2-dimensional and the x3 are the colors of each channel (RGB). In ordinary Neural Networks, every single nod of a (hidden) layer is connected to all nodes in the previous or next layer. For an image of the size 32x32x3, this would lead to 3072(=32x32x3) weights. And often images are not that small, so scale it further to high-resolution images (e.g. 1000x1000x3), and the number of weights and connections to build a fully connected neural network explodes and slows down the neural network [7].Here is where Convolutional Neural Networks come into place. Instead of building flat layer, it arranges the neurons into a 3D object with the dimensions depth, height, and width [7]. One of the major advantages is that it keeps the dimensions for the output. An ordinary NN would reduce the dimensions to a flat output.What kind of layers does a ConvNet have?Convolution Layer — computes output of local regions of an image using filtersPooling Layer — performs downsampling along the spatial dimensionsFully-Connected-Layer — computes the final class scoresFor demonstration purposes let’s assume the input image is of the size 5x5x1 as in the gif below. The magic in a convolutional layer happens through a filter (also called kernel). In our example, the filter has the size 3x3x1. This kernel/filter moves in little steps over the whole image (in our case in 9 steps) and builds a convolved feature. This reduces the dimensionality and produces an activation map [5].To cover all features and shapes not only one but different filters are applied (e.g. 5 different filters). The network will learn then filters that activate when they recognize a certain type of visual shape, e.g. a corner or a line. In an image with higher resolution, a filter of 5x5x1 might detect other shapes than a filter of size 30x30x1 does.Each filter produces an activation map that recognizes certain shapes and patterns. All these activation maps are stacked along the depth dimension and result in the output matrix [6].A pooling layer is normally used between convolutional layers. It’s used for the reduction of the necessary parameters and the spatial size. This helps again to decrease the required computational power. There are two types of pooling: max pooling and average pooling [7].Max Pooling takes the maximum value of the filter/kernel, while average pooling calculates the average of all values of the kernel.It’s a layer as you might know it from a conventional basic neural network. A fully connected layer has full connections to all activations of the previous layer and in a CNN it can be used as the final layer used for the classification [7].Now you should know the basics to dive into the HRNet.HRNet stands for High-Resolution Network, which refers to the high resolution of the images being processed. “Strong high-resolution representations play an essential role in pixel and region labeling problems, e.g., semantic segmentation, human pose estimation, facial landmark detection, and object detection.” [1] With growing pixels and more video-based problems high resolution might play a growing role in the future.The network behind HRNet is called HRNetV1 and it “maintains high-resolution representations by connecting high-to-low resolution convolutions in parallel, where there are repeated multiscale fusions across parallel convolutions.”[1]To answer it let’s take a closer look at the following image.You can see four blocks colored in light blue. Each represents a multi-resolution block as said above, a block that connects “high-to-low resolution convolutions in parallel”[1]. Parallel processing is demonstrated by multiple channel-map-lines below each other. The yellow channel map represents the highest resolution, while the red small channel maps represent the lowest resolution. The fourth block processes 4 resolutions in parallel.What happens in a multi-resolution group convolution?It “is a simple extension of the group convolution” [this would be a single line of boxes, e.g. only the yellow line],“ which divides the input channels into several subsets of channels and performs a regular convolution over each subset over different spatial resolutions separately.”[1]At the end of each stage, you can see a full connection to the multi-resolution group of the next stage, which looks like the image on the left side. This part of the HRNet is called multi-resolution convolution.“It resembles the multi-branch full-connection manner of the regular convolution.”[1]In other words, it connects regular convolutions (which would be just a single line) of one stage with all parallel convolutions of the next stage.What is the advantage of the HRNet and why does it perform better than previous models (AlexNet, GoogleNet, VGGNet, ResNet, DenseNet)?Semantically strong & spatially precise:In the HRNetV1 only the high-resolution representations result in the output (as can be seen below). Hence, subsets from low-resolution convolutions are lost and not fully included in the output.That’s why the inventors of the HRNet built on a modification that allows to output all subsets from high to low resolutions.“The benefit is that the capacity of the multi-resolution convolution is fully explored.”[1]This is done by upsampling the low resolutions representations to the highest resolution and concatenating all resulting subsets. This model is called HRNetV2 and is mainly used for estimating segmentation maps/facial landmark heatmaps.Another model is the HRNetV2, which was made for object detection. Here the authors attached one more step: “We construct a multi-level representation by downsampling the high-resolution representation with average pooling to multiple levels.”[1]HRNet and its variants (HRNetV1, HRNetV2, HRNetV2p) are state-of-the-art for many machine learning disciplines in the field of computer vision. The authors have changed the conventional architecture for convolutions, which had a serial procedure, to a parallel architecture with multiple group convolutions. This architecture allowed high resolutions and improves precision and the semantical connection.HRNet showed once more that thinking out of the box is necessary to achieve state-of-the-art results. The authors did not try to improve the existing serial approaches like in AlexNet and co., but they tried to rebuild it completely with an even smarter approach that lead to higher results. I am pretty sure this was only the beginning of even more complex architectures that will substitute HRNet one day.medium.comSources2. Shakhadri, S. (2021). Facial Landmark Detection Simplified With Opencv. https://www.analyticsvidhya.com/blog/2021/07/facial-landmark-detection-simplified-with-opencv/3. Jordan, J. (2018). An overview of semantic image segmentation. https://www.jeremyjordan.me/semantic-segmentation/4. Odemakinde, E. (2021). Human Pose Estimation with Deep Learning — Ultimate Overview in 2021. https://viso.ai/deep-learning/pose-estimation-ultimate-overview/5. Saha, S. (2018). A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way. https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a536. Microsoft Research Blog (2020). High-Resolution Network: A universal neural architecture for visual recognition. https://www.microsoft.com/en-us/research/blog/high-resolution-network-a-universal-neural-architecture-for-visual-recognition/7. No author (n.d.). Convolutional Neural Networks (CNNs / ConvNets) https://cs231n.github.io/convolutional-networks/",07/10/2021,0,22.0,13.0,911.0,463.0,14.0,3.0,0.0,25.0,en
3855,No home for iPad on Apple.com,cvil.ly,Craig Villamor,1300.0,1.0,77.0,"Have you noticed that there’s no spot in the Apple.com navigation for the iPad? I tried navigating to iPhone, iPod+iTunes and Mac and could not find iPad in any of those locations. I wonder when they plan to address this?[caption id=”” align=”aligncenter” width=”576"" caption=”No home for iPad in Apple.com IA”][/caption]Update: Now there is a home for iPad! Note that they also separated iPod & iTunes.[caption id=”” align=”aligncenter” width=”604"" caption=”Now there is a home for iPad on Aplle.com”][/caption]",24/02/2010,0,1.0,0.0,849.0,466.0,2.0,0.0,0.0,0.0,en
3856,Attention — Seq2Seq Models,Towards Data Science,Pranay Dugar,92.0,6.0,922.0,"Sequence-to-sequence (abrv. Seq2Seq) models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014).A Seq2Seq model is a model that takes a sequence of items (words, letters, time series, etc) and outputs another sequence of items.In the case of Neural Machine Translation, the input is a series of words, and the output is the translated series of words.Now let's work on reducing the blackness of our black box. The model is composed of an encoder and a decoder. The encoder captures the context of the input sequence in the form of a hidden state vector and sends it to the decoder, which then produces the output sequence. Since the task is sequence based, both the encoder and decoder tend to use some form of RNNs, LSTMs, GRUs, etc. The hidden state vector can be of any size, though in most cases, it's taken as a power of 2 and a large number (256, 512, 1024) which can in some way represent the complexity of the complete sequence as well as the domain.RNNs by design, take two inputs, the current example they see, and a representation of the previous input. Thus, the output at time step t depends on the current input as well as the input at time t-1. This is the reason they perform better when posed with sequence related tasks. The sequential information is preserved in a hidden state of the network and used in the next instance.The Encoder, consisting of RNNs, takes the sequence as an input and generates a final embedding at the end of the sequence. This is then sent to the Decoder, which then uses it to predict a sequence, and after every successive prediction, it uses the previous hidden state to predict the next instance of the sequence.Drawback: The output sequence relies heavily on the context defined by the hidden state in the final output of the encoder, making it challenging for the model to deal with long sentences. In the case of long sequences, there is a high probability that the initial context has been lost by the end of the sequence.Solution: Bahdanau et al., 2014 and Luong et al., 2015 papers introduced and a technique called “Attention” which allows the model to focus on different parts of the input sequence at every stage of the output sequence allowing the context to be preserved from beginning to end.To put it in very simple terms, since the issue was that a single hidden state vector at the end of the encoder wasn’t enough, we send as many hidden state vectors as the number of instances in the input sequence. So here is the new representation:Well, that sounds pretty simple, doesn’t it? Let’s bring in some more complexity. How exactly does the Decoder use the set of hidden state vectors? Until now, the only difference between the two models is the introduction of the hidden states of all the instances of the input during the decoding phase.Another valuable addition to creating the Attention based model is the context vector. This is generated for every time instance in the output sequences. At every step, the context vector is a weighted sum of the input hidden states as given below:But how is the context vector used in the prediction? And how are the weights a1, a2, a3 decided? Let's go one question at a time, simpler one first — the context vector.The generated context vector is combined with the hidden state vector by concatenation and this new attention hidden vector is used for predicting the output at that time instance. Note that this attention vector is generated for every time instance in the output sequence and now replaces the hidden state vector.Now we get to the final piece of the puzzle, the attention scores.Again, in simple terms, these are the output of another neural network model, the alignment model, which is trained jointly with the seq2seq model initially. The alignment model scores how well an input (represented by its hidden state) matches with the previous output (represented by attention hidden state) and does this matching for every input with the previous output. Then a softmax is taken over all these scores and the resulting number is the attention score for each input.Hence, we now know which part of the input is most important for the prediction of each of the instances in the output sequence. In the training phase, the model has learned how to align various instances from the output sequence to the input sequence. Below is an illustrated example of a machine translation model, shown in a matrix form. Note that each of the entries in the matrix is the attention score associated with the input and the output sequence.So now we have the final and complete modelAs we can see, the black box that we started with, has now turned white. Below is a pictorial summarization:I hope you found this useful and easily understandable. In case of any corrections or any kind of feedback, I’d love to hear from you. Please comment here and let me know.jalammar.github.ioskymind.aiarxiv.orggithub.comblog.googlemedium.comarxiv.orgdatascience.stackexchange.comSutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014.Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).https://www.guidelight.com.au/the-guidelight-psychology-blog/?category=Life+Coaching",13/07/2019,0,2.0,15.0,992.0,628.0,12.0,0.0,0.0,14.0,es
3857,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,Emergent // Future,Arthur Juliani,12000.0,6.0,1189.0,"For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1–3). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:Eq 1. Q(s,a) = r + γ(max(Q(s’,a’))This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:(Thanks to Praneet D for finding the optimal hyperparameters for this approach)Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.Eq2. Loss = ∑(Q-target - Q)²Below is the Tensorflow walkthrough of implementing our simple Q-Network:While the network learns to solve the FrozenLake problem, it turns out it doesn’t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!If you’d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on Twitter @awjliani.More from my Simple Reinforcement Learning with Tensorflow series:",26/08/2016,0,3.0,16.0,954.0,467.0,2.0,1.0,0.0,19.0,en
3858,Read Text from Image with One Line of Python Code,Towards Data Science,Dario Radečić,28000.0,5.0,819.0,"Dealing with images is not a trivial task. To you, as a human, it’s easy to look at something and immediately know what is it you’re looking at. But computers don’t work that way.Tasks that are too hard for you, like complex arithmetics, and math in general, is something that a computer chews without breaking a sweat. But here the exact opposite applies — tasks that are trivial to you, like recognizing is it cat or dog in an image are really hard for a computer. In a way, we are a perfect match. For now at least.While image classification and tasks that involve some level of computer vision might require a good bit of code and a solid understanding, reading text from a somewhat well-formatted image turns out to be a one-liner in Python —and can be applied to so many real-life problems.And in today’s post, I want to prove that claim. There will be some installation to go though, but it shouldn’t take much time. These are the libraries you’ll need:I don’t want to prolonge this intro part anymore, so why don’t we jump into the good stuff now.Now, this library will only be used to load the images(s), you don’t actually need to have a solid understanding of it beforehand (although it might be helpful, you’ll see why).According to the official documentation:OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Being a BSD-licensed product, OpenCV makes it easy for businesses to utilize and modify the code.[1]In a nutshell, you can use OpenCV to do any kind of image transformations, it’s fairly straightforward library.If you don’t already have it installed, it’ll be just a single line in terminal:And that’s pretty much it. It was easy up until this point, but that’s about to change.What the heck is this library? Well, according to Wikipedia:Tesseract is an optical character recognition engine for various operating systems. It is free software, released under the Apache License, Version 2.0, and development has been sponsored by Google since 2006.[2]I’m sure there are more sophisticated libraries available now, but I’ve found this one working out pretty well. Based on my own experience, this library should be able to read text from any image, provided that the font isn’t some bulls*** that even you aren’t able to read.If it can’t read from your image, spend more time playing around with OpenCV, applying various filters to make the text stand out.Now the installation is a bit of a pain in the bottom. If you are on Linux it all boils down to a couple of sudo-apt get commands:I’m on Windows, so the process is a bit more tedious.First, open up THIS URL, and download 32bit or 64bit installer:The installation by itself is straightforward, boils down to clicking Next a couple of times. And yeah, you also need to do a pip installation:Is that all? Well, no. You still need to tell Python where Tesseract is installed. On Linux machines, I didn’t have to do so, but it’s required on Windows. By default, it’s installed in Program Files.If you did everything correctly, executing this cell should not yield any error:Is everything good? You may proceed.Let’s start with a simple one. I’ve found a couple of royalty-free images that contain some sort of text, and the first one is this:It should be the easy one, and there exists a possibility that Tesseract will read those blue ‘objects’ as brackets. Let’ see what will happen:My claim was true. It’s not a problem though, you could easily address those with some Python magic.The next one could be more tricky:I hope it won’t detect that ‘B’ on the coin:Looks like it works perfectly.Now it’s up to you to apply this to your own problem. OpenCV skills could be of vital importance here if the text blends with the background.Reading text from an image is a pretty difficult task for a computer to perform. Think about it, the computer doesn’t know what a letter is, it only works only with numbers. What happens behind the hood might seem like a black box at first, but I encourage you to investigate further if this is your area of interest.I’m not saying that PyTesseract will work perfectly every time, but I’ve found it good enough even on some trickier images. But not straight out of the box. Some image manipulation is required to make the text stand out.It’s a complex topic, I know. Take it one day at a time. One day it will be second nature to you.Loved the article? Become a Medium member to continue learning without limits. I’ll receive a portion of your membership fee if you use the following link, with no extra cost to you.medium.com[1] https://opencv.org/about/[2] https://en.wikipedia.org/wiki/Tesseract_(software)",28/10/2019,3,9.0,9.0,758.0,320.0,7.0,1.0,0.0,9.0,en
3859,Interpreting recurrent neural networks on multivariate time series,Towards Data Science,André Ferreira,164.0,24.0,5123.0,"In this article, we’ll explore a state-of-the-art method of machine learning interpretability and adapt it to multivariate time series data, a use case which it wasn’t previously prepared to work on. You’ll find explanations to core concepts, on what they are and how they work, followed by examples. We’ll also address the main ideas behind the proposed solution, as well as a suggested visualization of instance importance.It’s not just hype anymore, machine learning is becoming an important part of our lives. Sure, there aren’t any sentient machines nor Scarlett Johansson ear lovers (shoutout to Her) out there, but the evolution of these algorithms is undeniable. They can ride cars, assist in medical prognosis, predict stock, play videogames at a pro level and even generate melodies or images! But these machine learning models aren’t flawless nor foolproof. They can even be misleading, showing an incorrect probability in a sample that is very different from the training data (I recommend having a look at Christian Perone’s presentation addressing uncertainty [1]). Thus, especially in critical applications such as in diagnosing a patient or deciding a company’s strategy, it’s important to at least have some understanding of how the model got to its output value so that users can confirm if it’s trustable or not. Furthermore, in the case of a high-performance model coupled with an adequate interpretation, it can lead to surprising revelations, such as the impact of a gene in the diagnosis of a disease or a certain time of the year on sales.So it’s a no-brainer to apply interpretability techniques on all that is machine learning, right? Well, more or less. While simpler models like linear regression and decision trees are straightforward to analyze, more complex models such as a neural network aren’t self-explanatory, particularly in scenarios of high dimensionality of data and parameters. Some architecture changes have been suggested to make neural networks easier to interpret, such as attention weights. However, not only do these approaches require increasing the number of parameters and altering the model’s behavior (which could worsen its performance), they may not give us the full picture (attention weights only indicate the relative importance of each feature, not if it impacted the output positively or negatively). As such, there has been this tradeoff between performance and interpretability, where in order to be able to interpret the model, it would have to be simple enough or specially adapted in some way, restricting its potential.Fortunately, research has been growing on perturbation-based methods, a family of interpretability techniques that apply changes in the input data (i.e. perturbations) to calculate importance scores, usually without requiring a specific model architecture. This means that these methods can be model-agnostic, making every possible model interpretable and with that eliminating the performance/interpretability tradeoff (albeit with some caveats that we’ll address later). So, let’s go through some of the main concepts behind modern perturbation-based interpretability techniques.Shapley values are a concept from game theory, first introduced by Lloyd Shapley in 1953 (I know that I said “modern”, but bear with me here), which defined a way to calculate each player’s contribution in a cooperative game. It all comes down to a single equation. Consider a total of N players, i the player whose contribution we’re calculating, φi player i’s contribution, S a subset of players excluding i (with |S| meaning the number of players in subset S) and v the function that outputs the total payoff for the set of input players. To calculate player i’s contribution, we calculate the following equation:In other words, each player’s contribution is determined by the weighted average of that player’s marginal contributions, over all possible combinations of players. Note that by combination I mean a subset of players in the game, regardless of their order, and by marginal contribution I mean how the payoff changes when that specific player joins in, in the current combination. Now that we understood the marginal contribution part, there’s still that messy stuff on the left. These seemingly complex weights can actually give rise to a simple equivalent version of the Shapley values equation:In this equation, we iterate through all possible permutations (R) of the full list of players, instead of just using the unique marginal contributions. Note that by permutation I mean the order in which players are added (e.g. player 1 starts the game, then player 2 joins in, followed by player 3, etc). In this case, it now has the symbol PiR (sorry, can’t really write in equation form in Medium text), which represents all the players that appeared before i, in the current order R. This equivalence means that the weights are set in a way that takes into account how many times a unique marginal contribution appears, on all possible orders of the players. Moreover, it’s the way those weights are defined that allow Shapley values to fulfill a set of properties that ensure a fair and truthful distribution of contributions throughout the players. In order to keep this post reasonably short, I’m not going to list them here, but you can check Christoph Molnar’s Interpretable Machine Learning book [2] if you want to know more.To illustrate this, as a football fan, imagine the following scenario:Imagine we have three strikers (i.e. players that play forward in the field, with the main objective to score or assist in as many goals as possible). Let’s call them B, L, and V. Let G be the function that, for a set of strikers in play, outputs how many goals are scored by the team. With that in mind, imagine that we have the following goals scored when each set of players are on the field:Think that in this game all players will eventually be playing, it’s just a matter of when each one goes in (beginning on the starting squad or joining in the first or second substitution). As such, we have 6 possible scenarios of them getting in the game, to which we need to calculate marginal contributions:As a last step towards the Shapley values, we just need to apply one of the Shapley values equation (weighted average of unique marginal contributions or average of all orders’ marginal contributions) on each player:Notice how I calculated the Shapley values through both equations that I showed before, with both leading to the same results. Also, as a consequence of one the method’s properties (efficiency), the sum of all the Shapley values equals the payoff of the grand coalition, i.e. the payoff when all the players are in the game, G(B, L, V).Now, I’m not trying to play games with you by explaining an unrelated 50’s theory. You see, if we replace the idea of “players” with “feature values” and “payoff” with “model output”, we got ourselves an interpretability method. There are just two issues that we need to address to make this useful in machine learning explainability:In 2017, Scott Lundberg and Su-In Lee published the paper “A Unified Approach to Interpreting Model Predictions” [3]. As the title suggests, they proposed a new method to interpret machine learning models that unifies previous ones. They found out that 7 other popular interpretability methods (LIME, Shapley sampling values, DeepLIFT, QII, Layer-Wise Relevance Propagation, Shapley regression values, and tree interpreter) all follow the same core logic: learn a simpler explanation model from the original one, through a local linear model. Because of this, the authors call them additive feature attribution methods.What is this local linear model magic? Essentially, for each sample x that we want to interpret, using model f’s output, we train a linear model g, which locally approximates f on sample x. However, the linear model g doesn’t directly use x as input data. Rather, it converts it to x’, which represents which features are activated (for instance, x’i = 1 means that we’re using feature i, while x’i = 0 means that we’re “removing” feature i), much like the case of selecting combinations of players. As such, and considering that we have M features and M+1 model coefficients (named φ), we get the following equation for the interpreter model:And, having the mapping function hx that transforms x’ into x, the interpreter model should locally approximate model f by obeying to the following rule, whenever we get close to x’ (i.e. z’ ≈ x’):Knowing that the sample x that we want to interpret naturally has all features available (in other words, x’ is a vector of all ones), this local approximation dictates that the sum of all φ should equal the model’s output for sample x:So, these equations are all jolly fun, but now what? The trick is in what these φ coefficients represent and how they’re calculated. Each coefficient φ, being this a linear model, relates to each feature’s importance on the model. For instance, the bigger the absolute value of φi is, the bigger the importance of feature i is on the model. Naturally, the sign of φ is also relevant, as a positive φ corresponds to a positive impact on the model’s output (the output value increases) and the opposite occurs for a negative φ. An exception here is φ0. There is no feature 0, so it is not associated with any feature in particular. In fact, if we have an all zeroes vector z’ as an input, the output of the interpreter model will be g(0) = φ0. In theory, it should correspond to the output of the model when no feature is present. Practically, what is done in SHAP is that φ0 assumes the average model output on all the data, so that it represents a form of starting point for the model before adding the impact of each feature. Because of this, we can see each of the remaining coefficients as each feature’s push on the base output value (φ0) onto a bigger or smaller output (depending on the coefficient’s sign), with the sum of all of these feature related coefficients resulting in the difference between the output on the current sample and the average output value. This characteristic allows us to create interesting force plots with the SHAP package, as you can see in the example below of a model that predicts if a football team has a player win the Man of the Match award. To see more about this example, check Kaggle’s tutorial on SHAP values [5].There’s an additional interesting aspect of the φ coefficients. For everyone except φ0, which again doesn’t correspond to any single feature importance, the authors defend that there is only one formula that simultaneously accomplishes three desirable properties:1: Local accuracyWhen approximating the original model f for a specific input x, local accuracy requires the explanation model to at least match the output of f for the original input x.2: MissingnessIf the simplified inputs (x’) represent feature presence, then missingness requires features missing in the original input to have no impact.3: ConsistencyLet fx(z’) = f(hx(z’)) and z’ \ i denote setting z’i = 0. For any two models f and f’, iffor all inputs z’ ∈ {0, 1}^M , then φ(f’, x) ≥ φ(f, x).And the formula that fulfills these three properties is… Brace yourselves for a Deja Vu…Looks familiar? It’s essentially the Shapley values formula adapted to this world of binary z’ values. When before we had combinations of players represented by S, we now have combinations of activated features represented by z’. The weights and the marginal contributions are still there. It’s not a coincidence that these coefficients are called SHAP values, where SHAP stands for SHapley Additive exPlanation.Moving on from the definition of SHAP values, we still have the same two issues to solve: efficiency and how to represent missing values. These are both addressed through sampling and expectation of the missing features. That is, we fix the values of the features being used (those that have z’ = 1, whose values correspond to zs) and we then integrate through sample values for the remaining, removed features (those that have z’ = 0, whose values correspond to zs‒), in a fixed number of iterations. By going through several different values of the deactivated features and getting their averages, we are reducing those features’ influence on the output. We also speed up the process as we don’t need to go through every possible iteration.Knowing of this need for integration of random samples, we need to define two sets of data every time we run SHAP:SHAP also speeds up the process by applying regularization on the importance values. In other words, some low importance features will assume a SHAP value of 0. Additionally, considering the unification with other interpretability techniques, SHAP has various versions implemented that have model-specific optimizations, such as Deep Explainer for deep neural networks, and Tree Explainer for decision trees and random forests.And alongside this unification of methods and optimizations, SHAP also has ready to use visualizations, such as the force plots shown earlier, dependence contribution plots and summary plots, as seen below in the same football example.I know how you might be feeling now:And you have reasons for it, considering SHAP’s well-founded theory and the practicality of its python implementation. But, unfortunately, there are still some problems:But don’t despair. I’ve found a workaround for this that we’ll address next.As I’ve mentioned, SHAP has multiple versions of an interpreter model, based on other interpretability methods that it unifies. One of them is the Kernel Explainer, which is based on the popular LIME [6]. It has some clear similarities, such as using a linear model that locally approximates the original model as an interpreter, and using a simplified input x’, where values of 1 correspond to the feature’s original value being used and values of 0 represent the feature being missing. Furthermore, LIME and its SHAP counterpart (Kernel Explainer) don’t assume any specific model component or characteristics, such as a decision tree or backpropagation of gradients, which makes them completely model-agnostic. The main difference relies on the linear model’s loss function. While LIME defines the loss function, its associated weighting kernel and regularization term heuristically, which the SHAP paper’s authors defend that breaks the local accuracy and/or consistency properties, SHAP’s Kernel Explainer uses the following fixed parameters objective function:The squared loss is adequate here, as we want g to approximate f as best as possible. Regarding the weighting kernel, one way to see the validity of these parameters is that the weight is infinitely big when:Not only do these parameters provide an advantage over LIME, by guaranteeing compliance with the three desirable properties, but also the joint estimation of all SHAP values through linear regression gives better sample efficiency than classic Shapley equations.Since we’re training a linear regression model, the only major parameter that needs to be defined, besides setting the data to interpret and the background data from where SHAP gets random samples, is the number of times to re-evaluate the model when explaining each prediction (named as nsamples in the python code). As with other machine learning models, it’s important to have a big enough number of training iterations, so as to get a well-fitted model and with low variance through different training sessions. keep in mind that, in this case, we don’t need to worry about overfitting, as we really just want to interpret the model on a specific sample, not use the resulting interpreter on other data afterwards, so we really want a large number of model re-evaluations.There’s naturally a downside to this Kernel Explainer method. Since we need to train a linear regression model on each prediction that we want to explain, using several iterations to train each model and without specific model type optimizations (like for example Deep Explainer and Tree explainer have), Kernel Explainer can be very slow to run. It all depends on how much data you want to explain, how much background data to sample from, the number of iterations (nsamples) and your computation setting, but it can take some hours to calculate SHAP values.In short, if Kernel Explainer was a meme, it would be this:As Kernel Explainer should work on all models, only needing a prediction function on which to do the interpretation, we could try it with a recurrent neural network (RNN) trained on multivariate time series data. You can also try it yourself through the simple notebook that I made [7]. In there, I created the following dummy dataset:Essentially, you can imagine it as being a health dataset, with patients identified by subject_id and their clinical visits by ts. The label might indicate if they have a certain disease or not and variables from 0 to 3 could be symptoms. Notice how Var3 and Var2 were designed to be particularly influential, as the label is usually activated when they drop below certain levels. Var0 follows along but with less impact, and lastly Var1 is essentially random. I also already included an “output” column, which indicates the prediction probability of being label 1 for a trained RNN model. As you can see, in general, it does a good job predicting the label, with 0.96 AUC and 1 AUC on the training (3 patients) and test (1 patient) sets.For the prediction function, which the SHAP Kernel Explainer will use, I just need to make sure the data is in a float type PyTorch tensor and set the model’s feedforward method:As it’s a small dataset, I defined both the background and the test sets as the entire dataset. This isn’t recommended for bigger datasets, as you’ll want to use a small percentage of the data as the samples that you want to interpret, due to the slowness of the Kernel Explainer, and avoid having the same data inside both the background and test sets.However, when we compare the resulting sum of the SHAP coefficients with the output, they don’t exactly match, breaking the local accuracy axiom:Why is it like this? Shouldn’t this version of SHAP be able to interpret all kinds of models? Well, part of the issue is that, looking at the code, you see that SHAP expects tabular data. There’s even a comment on the documentation that mentions this need for 2D data:X : numpy.array or pandas.DataFrame or any scipy.sparse matrix A matrix of samples (# samples x # features) on which to explain the model’s output.In our case, the data is three-dimensional (# samples x # timestamps x # features). This would still be fine if we used a simpler model, one that only uses a single instance to predict the label. However, as we’re using a RNN, which accumulates memory from previous instances in its hidden state, it will consider the samples as being separate sequences of one single instance, eliminating the use of the model’s memory. In order to fix this issue, I had to change the SHAP Kernel Explainer code, so that it includes the option to interpret recurrent models on multivariate sequences. There were more subtle changes needed, but the core addition was this part of the code:As you can see, it’s code that is only executed if we previously detected the model as being a recurrent neural network, it loops through each subject/sequence separately and passes through the model’s hidden state, its memory of the previous instances in the current sequence.After these changes, we can now go to the same example and see that the sums of SHAP coefficients correctly match the real outputs!Now, if we look at the summary bar plot, we can see what is the overall ranking of the feature importance:Looks good! Var2 and Var3 are correctly recognized as the most important features, followed by a less relevant Var0 and a much less significant Var1. Let’s see an example of the time series of patient 0, to confirm if it’s, in fact, interpreting well individual predictions:All checks out with our dataset as, initially, when Var2 and Var3 have high values, it decreases the output, while in the final stages, with Var2 and Var3 lower, the output increases. We can confirm this further by getting the force plot on timestamp 3, where the output probability rises to above 50%:As SHAP Kernel Explainer is not really fast (don’t elude yourself if you run the previous notebook, it’s fast there because the dataset is very, very small), every chance we have to make it faster should be considered. Naturally, a computationally heavy part of the process is the iteration through multiple combinations of samples from the background data, when training the interpreter model. So, if we could reduce the number of samples used, we would be able to get a speedup. Now think about this: could we represent the missing features by just a single reference value? If so, what would it be?I’d say that even a formula given in the SHAP paper gives us a hint (the one in the “Efficiency” subsection earlier in the “SHAP” part of this post). If we’re integrating over samples to get the expected values of the missing features, why not directly use the average values of those features as a reference value? And this is even easier to do if, in the preprocessing phase, we normalized the data into z-scores. This way, we just need to use an all zeroes vector as the sole background sample, as zero represents each feature’s average value.Rerunning the Kernel Explainer with just the zero reference values instead of all the previous background data, we get the following summary bar plot:We got similar results to the case of using all the dataset as background data! There are small differences in the ranking of feature importance, with Var3 slightly surpassing Var2 as the most relevant feature and a bigger importance given to Var1, although it's still the least relevant. Beyond the fact that there are these differences, it's actually interesting since I intended to have Var3 as being the most important feature. You can also check the more detailed time series and force plots in the notebook, but they’re also close to what we got before.Of course, the previous example uses a toy dataset with an unrealistically small size, just because experimentation is fast and we know exactly what to expect. I’ve also been working on a real health dataset, concerning the check-ups of over 1000 ALS diagnosed patients. Unfortunately, as this dataset isn’t public, I can’t share it nor incorporate it in an open notebook to make the results reproducible. But I can share with you some preliminary results.To give some context, I trained an LSTM model (a type of recurrent neural network) to predict if a patient will need non-invasive ventilation in the next 3 months, a common procedure done mainly when respiratory symptoms aggravate.Running the modified SHAP Kernel Explainer on this model gives us the following visualizations:Some interesting notes here:This interpretation was done on the time series of 183 patients, with a maximum sequence length of 20 and using zero as the reference value. I ran the code in Paperspace [8], on a C7 machine (12 CPU cores, 30GB of RAM). It took around 5 hours to finish, while the alternative of using 50 background samples was estimated to take around 27 hours. Now you believe me when I say it’s slow right? At least our reference value trick still manages to improve the running time.While it does seem like we got to the happy ending, there’s still something off here. The article is about interpreting models on multivariate time series data, however we’ve only addressed feature importance scores. How do we know what instances/timestamps were most relevant? In other words, how do we know the instance importance scores? Unfortunately, right now, SHAP doesn’t have that built-in for multivariate sequential data. For that, we’ll need to come up with our own solution.As we discussed, SHAP is mainly designed to deal with 2D data (# samples x # features). I showed you a change in the code that allows it to consider different sequences’ features separately, but it’s a workaround that only leads down to feature importance, without interpreting the relevance of instances as a whole. One could still think if it would somehow be possible to apply SHAP again, fooling it to consider instances as features. However, would that be practical or even make sense? First of all, we already have the slow process of using SHAP to do feature importance, so applying it again to get instance importance could make the whole interpretation pipeline impractically slow. Then, considering how SHAP works by going through combinations of subsets of features, removing the remaining ones (in this case, removing instances), it seems like it would be unrealistic. In real-world data where temporal dynamics are relevant, I don’t think it would make sense to consider synthetic samples where there are multiple gaps between time events. Considering all of this, in my opinion, we should go for a simpler solution.An initial approach comes rather naturally: just remove the instance of which we want to get an importance score and see how it affects the final output. To do this, we subtract the original output by the output of the sequence without the respective instance, so that we get a score whose sign indicates in which direction that instance pushes the output (if the new output has a lower value, that means the instance has a positive effect; vice-versa for the higher value). We could call this an occlusion score, as we’re blocking an instance in the sequence.While it makes sense, it’s likely not enough. When keeping track of something along time, such as a patient’s disease progression, there tend to be certain moments where something new happens that can have repercussions or be repeated in the following events. For instance, if we were predicting the probability of worsening symptoms, we could have a patient that starts very ill but, after successful treatment, gets completely cured, with a near-zero probability of getting sick again. If we were to only apply the previous method of instance occlusion, all instances of the patient after the treatment could receive similar scores, although it’s clear that the moment that he received the treatment is, in fact, the crucial one. In order to address this, we can take into account the variation in the output brought in by the instance. That is, we compare the output at the instance being analyzed with the one immediately before it, like if we were calculating a derivative.Of course, the occlusion score might still be relevant in many scenarios, so the ideal solution is to combine both scores in a weighted sum. Considering the more straightforward approach of occlusion, and some empirical analysis, I’ve picked the weights to be 0.7 for occlusion and 0.3 for output variation. And since these changes in the output can be somewhat small, usually not exceeding a change of 25 percentage points in the output, I think that we should also apply a nonlinear function on the result, so as to amplify high scores. For that, I’ve chosen the tanh function, as it keeps everything in the range of -1 to 1, and added a multiplier of 4 inside, so that a change of 25 percentage points in the output gets very close to the maximum score of 1.Despite the combination of two scores and the application of a nonlinear function, the solution remains simple and fast. For comparison, applying this method on the same 183 patients data and the same computation setting as in the previous feature importance example, this took around 1 minute to run, while feature importance demanded 5 hours.Having this instance importance formulation, we can visualize the scores, even in multiple sequences simultaneously. I’ve been working on the time series plot that you see below, inspired by Bum Chul Kwon et al. paper “RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records” [9], where each row corresponds to a sequence (in my case, a patient’s time series), instances are represented by circles whose color indicates its importance (intensity indicates score magnitude, color indicates sign), and we can also add simple prediction meters, which give us a sense of what is the final output value.Notice that patient 125 is the same as in the ALS example of feature importance. It adequately indicates timestamp 9 as a relevant, output increasing instance, while timestamp 10 reduces that output value.Even though I hope that you like the logic and the plot that I propose for instance importance, I’m not quite ready to share this code with you, as it still needs fine-tuning. As part of my master’s thesis, I’m creating an API called Model Interpreter, which contains ready to use implementations of what we discussed here (SHAP for multivariate time series, instance importance scores and plot) and more. I hope to be able to make it public on GitHub until October. Meanwhile, feel free to explore more on these topics and to do your own experimentations! Remember also that my modified version of SHAP [10] is public, so you can already use that.If there’s something you should take away from this post, even if you just powered through the images and the memes, it’s this:Although it’s still slow, the proposed modified SHAP can explain any model, even if trained on multivariate time series data, with desirable properties that ensure a fair interpretation and implemented visualizations that allow intuitive understanding; albeit an instance importance calculation and plot is still missing from the package.There’s still work to be done in machine learning explainability. Efficiency and ease of use need to be addressed, and there’s always room for improvement, as well as the proposal of alternatives. For instance, SHAP still has some assumptions, like feature independence and local linearizability of models, which might not always be true. However, I think that we already have a good starting point from SHAP’s strong theoretical principles and its implementation. We’re finally reaching a time when we can not only get results from complex algorithms, but also ask them why and get a realistic, complete response.[1] C. Perone, Uncertainty Estimation in Deep Learning (2019), PyData Lisbon July 2019[2] C. Molnar, Interpretable Machine Learning (2019), Christopher Molnar’s webpage[3] S. Lundberg and S. Lee, A Unified Approach to Interpreting Model Predictions (2017), NIPS 2017[4] S. Lundberg, SHAP python package (2019), GitHub[5] D. Becker, Kaggle’s tutorial on SHAP values (2019), Kaggle[6] M. Ribeiro et al., “Why Should I Trust You?”: Explaining the Predictions of Any Classifier (2016), CoRR[7] A. Ferreira, GitHub repository with a simple demonstration of SHAP on a dummy, multivariate time series dataset (2019), GitHub[8] Paperspace cloud computing service[9] B. Chul Kwon et al., RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records (2018), IEEE VIS 2018[10] A. Ferreira, Modified SHAP, compatible with multivariate time series data (2019), GitHub[11] R. Astley, Revolutionary new AI blockchain IoT technology (2009), YouTube",01/08/2019,0,17.0,96.0,1183.0,316.0,39.0,5.0,0.0,25.0,en
3860,How I went from zero coding skills to data scientist in 6 months,Towards Data Science,Kate Marie Lewis,3700.0,8.0,1769.0,"I had just walked away from 8 years of study and hard work with no plan. You might be wondering why someone would do that. My boss was crushing my spirit and knew that I needed to make a change.My boyfriend suggested becoming a data scientist. I said ‘you're crazy!’ I didn’t know the first thing about programming. Surely he was overestimating what I was capable of. Imposter syndrome strikes again.About two weeks later my friend Anna suggested the exact same thing, I thought about it some more and began to entertain the idea. Why not? I decided to become a beginner again and reinvent myself as a data scientist.I wanted to learn at my own pace so I decided to take online courses. I figured that with a PhD in Neuroscience I probably had enough formal training to get a data science job. I just needed the practical skills.This story will describe the 4 different courses that I took and how they led to a data science job at a healthcare startup in Silicone Valley.At the time, most of the online courses I came across were free. So I challenged myself to gain the skills I needed without spending any money. What can I say, I am pretty stingy 😜When I quit my post doc at UCSF I had zero programming experience. I had used statistics throughout all my research, but only on a small scale. All the datasets I had previously analysed were ones I had generated myself in the lab. Therefore the number of observations were very small. I needed to learn to code and analyse data on a much larger scale.When I decided I wanted to become a data scientist, the first thing I wanted to learn was how to write computer code. Since I had never coded before it was a complete unknown. I figured that if I really hated writing code, then data science would not be a great fit for me. So it seemed like a good place to start.I was lucky that my partner Ben has worked in many areas of tech and was able to point me in the right direction. He suggested that Python might suit me best. Python is excellent for data analysis, versatile and deals well with large datasets, so that is where I began.To begin learning to code I used Codecademy. I started with Introduction to Python, but I am not sure if the course I completed still exists as it was back in 2014. If I was to use Codecademy to start learning python now I would probably choose the Analyze Data with Python course.I found Codecademy an excellent starting point. The main advantage for me was to be able to write code right in my browser. Installing programming environments on my computer correctly is still my Achilles heel. So I was pleased to avoid it when starting off. It was comforting to know that if my code didn’t work it was because of the syntax and not because of an environment setup error.I also liked how you could do a few minutes of work at a time with Codecademy. If I had some spare time I would log in and do a few problems because it was all there waiting for me. This piecemeal progression meant that I was not too intimidated to get stuck into it.At the time that I completed the course, there were only a handful of Codecademy courses on offer and this one was free. I was so amazed at the quality of free courses available online.Once I had learned the basics of Python, I needed to start to level up my stats experience and learn to analyse data on a larger scale.Second I took the Coursera Data Science Specialisation from Johns Hopkins. At the time, you could do an honour code certificate version for free and only had to pay if you wanted the verified certificate.For me, the verified certificate did not seem important. Instead, I needed to be able to demonstrate the skills taught in the course during tech interviews. So I took the free version of the specialisation.One drawback for me was that this series of courses is taught in R. R is an excellent programming language for statistical analysis and is favoured by academia. However, I wanted to learn Python for data science. I thought Python would be more useful in the startups where I wanted to work.I looked into a few data analysis courses in Python but they seemed to assume quite a bit of knowledge that I did not yet have. I believe most of these courses were aimed at software engineers who were wanting to transition into data science. So they assumed that you had solid programming skills and already knew how to set up your python environment.The main aspect that I liked about the Coursera Data Science Specialisation is that it started from the very beginning. There were step by step instructions on how to install R and R studio in the first course. This made it easy to tackle the subsequent courses knowing that there would not be any technical issues.Another facet of the Johns Hopkins Data Science specialisation that suited me is that it was taught by the Public Health department. My health science domain expertise made it easy for me to follow the examples that they set out. They had examples using air quality impacts on asthma and other datasets related to healthcare. Therefore I could focus on the course content rather than figuring out the scenarios presented for data analysis.This series of courses really set me up well with a base level understanding in the main aspects of data science work. It touched on programming in R, basic data cleaning, analysis, regression and machine learning. I really enjoyed learning to code and how to use code to analyse data, so that encouraged me to continue learning.At this stage of my retraining, I started asking people in my network if they could introduce me to other people who had made the transition from academia to data science in San Francisco. A few were able to connect me so I set up as many informational interviews as I could.A friend introduced me to a data scientist from Modcloth who had taken a similar pathway to me. She used to be a neuroscientist and I found her advice particularly helpful.Her major recommendation was to learn SQL.SQL was not covered at all in the Coursera data science specialisation from Johns Hopkins. She said that most of her day to day work was querying databases. She had to extract insights for the business development and marketing teams. Only a small portion of her time was spent doing statistical analysis and machine learning.I took her advice and started a self-paced SQL course with Stanford Online. Of all the courses that I did, this was my favourite. I enjoyed it because the teacher was excellent and used simple examples to explain the concepts. She also explained each concept in multiple different ways.I have since recommended this course to so many people because I think that a good foundation in SQL is essential for any data scientist. Data science courses I have come across do not cover how to get the data from a database using SQL. I think this is a huge oversight. Most courses have a CSV of data prepared for the student to use, but in my experience that is rarely the case in industry data science jobs.Once I had completed the Stanford SQL course I started applying for data science positions. By that stage, I was living back in Australia and started doing Skype interviews with startups in the San Francisco bay area. Whilst interviewing I wanted to continue developing my skills.I then took the Foundations of data analysis course using R by edX. It was so helpful to revise a lot of the concepts that I had already learned in the Coursera course.I am a firm believer that learning concepts from different teachers can provide new insights. It was much easier to follow the statistics and machine learning concepts learning them the second time around. I felt like I got a deeper understanding through this course.While I was finishing off the course, I was successful in one of my interviews with Amino, a healthcare startup in San Francisco and proceeded to get a working visa and move to the USA.I think I was successful in that final interview because I had passable coding skills and a decent statistical understanding, but more importantly I had healthcare domain knowledge, experimental design and scientific method expertise.In my opinion, it was these additional aspects that put my application over the top and led this startup to take a chance on me. I was very junior and required a lot more on the job training. I think that all the courses I did were just enough to get the hiring team to consider me and that my experience specific to the healthcare space got me over the line.So if you are looking to change career paths into data science, I would recommend looking for a company where your existing domain knowledge is valuable.The major gap in my knowledge that I wish I had filled before commencing my new data science job was using git from the command line. I had never used the terminal or command line before and I had no idea how to use git to commit my code to the company’s Github repository.It took several engineers quite a bit of time to get me up to speed. I would have liked to at least have an idea of how to use it before I started so that I would not have wasted their valuable time. My colleagues were awesome and didn’t seem to mind teaching me but I did feel like a bit of a burden in the first few days.I did eventually catch up and found Learn Code the Hard Way Command Line extremely useful.If you are thinking of following a similar pathway into data science I would encourage you to go for it! It was absolutely the right choice for me. Different people learn in different ways, but if you have the self-discipline to study and complete what you start it is certainly feasible to teach yourself data science through online courses. If that is your goal I wish you the best of luck and would be happy to answer any questions if I can.In addition to data, my other passion is painting. You can find my wildlife art at www.katemarielewis.comtowardsdatascience.comtowardsdatascience.com",24/01/2020,0,0.0,0.0,425.0,213.0,2.0,0.0,0.0,12.0,en
3861,20 ideas for better data visualization,UX Collective,Taras Bakusevych,7300.0,7.0,1020.0,"Applications we design are becoming increasingly data-driven. The need for quality data visualization is high as ever. Confusing and misleading graphics are all around us, but we can change this by following these simple rules.Choosing the wrong chart type, or defaulting to the most common type of data visualization could confuse users or lead to data misinterpretation. The same data set can be represented in many ways, depending on what users would like to see. Always start with a review of your data set and user interview.You can learn more on how to pick the right representation for your data, and how to design effective dashboards in my article about Dashboard design.When using horizontal bars, plot negatives values on the left side and positive on the right side of a baseline.Do not plot negative and positive values on the same side of the baseline.Truncation leads to misrepresentation. On the example below, looking at the chart on the left, you can quickly conclude that value B is more than 3 times greater than D when in reality the difference is far more marginal. Starting at zero baseline ensures that users get a much more accurate representation of data.For line charts always limiting the y-axis scale to start at zero may render the chart almost flat. As the main goal for a line chart is to represent the trend, it's important to adapt the scale based on the data set for a given period and keep the line occupying two-thirds of the y-axis range.The line chart is composed of “markers” that are connected by lines, often used to visualize a trend in data over intervals of time — a time series. This helps to illustrate how values change over time and works really well with short time intervals, but when data updates infrequently this may cause confusion.Ex. Using a line chart to represent yearly revenue, if values are updated monthly will open the chart to interpretation. Users may assume the lines connecting the “markers” are representing actual values when in reality true revenue numbers at that specific time are unknown. In such scenarios using a vertical bar chart can be a better option.Smoothed line charts may be visually pleasing but they misrepresent the actual data behind them, also excessively thick lines obscure the real “markers” positions.Often, to save space for your visualization, you may be inclined to use dual-axis charts when there are two data series with the same measure, but different magnitudes. Not only are those charts hard to read, but they also represent a comparison between 2 data series in completely misleading way. Most users will not pay close attention to the scales and just scan the chart, drawing wrong conclusions.A pie chart is one of the most popular and often misused charts. In most cases, a bar chart is a much better option. But if you decided on a pie chart here are a few recommendations on how to make it work:Without proper labeling, no matter how nice is your graph — it won’t make sense. Labeling directly on the chart is super helpful for all viewers. Consulting the legend requires time and mental energy to link the values and corresponding segments.Putting the values on top of slices may cause multiple problems, from readability issues to challenges with thin slices. Instead, add black labels with clear links to each segment.There are several ways commonly accepted in ordering pie slices:The same recommendation holds true for many other charts. Do not default to alphabetical sorting. Place the largest values on top (for horizontal bar charts) or left (for vertical bar charts) to ensure the most important values take the most prominent space, reducing the eye movements, and time required to read a chart.A pie chart in general is not the easiest chart to read, as it's very hard to compare similar values. When we take the middle out and create a donut chart, we free as space to display additional information but sacrifice clarity, taken to extremes it renders the chart useless.Unnecessary styling is not only distracting, it may cause misinterpretation of the data and users making false impressions. You should avoid:Color is an integral part of effective data visualization, consider those 3 color palette types when designing:A Qualitative color palette works best for the display of categorical variables. Colors assigned should be distinct to ensure accessibility.A Sequential color palette works best for numeric variables that need to be placed in a specific order. Using hue or lightness or a combination of both, you create a continuous color set.A Divergent color palette is a combination of two sequential palettes with a central value in the middle(usually zero). Often divergent color palettes will communicate positive and negative values. Make sure color also matches the notion of “negative” and “positive” performance.Check out a handy tool — ColorBrewer that can help you generate various color palettes.According to the National Eye Institute, about 1 in 12 humans are color blind. Your charts are only successful if they are accessible to a broad audience.Make sure typography is communicating information and helping users focus on data, rather than distracting from it.This simple trick will ensure users will be able to scan the chart much more effectively, without straining their neck)If your task is to add interactive charts to web and mobile projects, one of the first questions you should ask is what charting library will we use? Modern charting libraries have many of the previously mentioned interactions and rules baked in. Designing based on a defined library will ensure ease of implementation and will give you a ton of interaction ideas.Help users explore by changing parameters, visualization type, timeline. Draw conclusions to maximize value and insight. In the example below, you can see the IOS Health app using a combination of various kinds of data presentation to its benefit.For all who would like to learn more about this topic, I highly recommend reading “The Wall Street Journal Guide to Information Graphics: The Dos and Don’ts of Presenting Data, Facts, and Figures” by Dona M. Wong. Many of the ideas in this article are inspired by this book.",18/08/2021,0,6.0,0.0,1340.0,650.0,22.0,5.0,0.0,4.0,en
3862,Blog 9- Information Architecture,Medium,Joshua Holmes,68.0,4.0,668.0,"1. What is the purpose of metadata? What are the categories of metadata?Metadata provides definitions about the data they are attached to. It can include descriptive information about the context, quality, condition and characteristics.Metadata is broken into three categories; structural (describes information about the document), descriptive (enables the document to be identified) and administrative (identifies the relationship of the document to the business context).2. What is a controlled vocabulary? How is a controlled vocabulary beneficial to a web site and/or organisation?A controlled vocabulary is a list of equivalent terms in the form of a synonym ring or a list of preferred terms in the form of an authority file.This is beneficial as it helps categorise information, establish a sites navigation, the basis of personalisation features, helps CMS preparation, and gets users and organisations using the same language.3. List the four main types of controlled vocabularies.4. What is the purpose of a synonym ring? Give examples of terms that would be considered equivalent under a synonym ring. What might happened during a search if you didn’t use a synonym ring? (Give an example.)The synonym rings’ main purpose is to connect a set of words that are equivalent for the purpose of retrieval.For example, ‘kitchenaid’ could be ‘kitchen aid’, ‘blender’, ‘food processor’, ‘cuisinart’ or ‘cuisinart’.Without a synonym ring, the search could omit relevant results as they are written using equivalent words.5. What is the purpose of an authority file? Describe how an authority file can educate users during search.An authority file defines proper names for a set of entities within a limited domain. It contains preferred or acceptable terms.It can educate users by showing them what a term means or helps them further understand what it is they are searching for.6. Create an authority file for abbreviations of the Australian states and territories (Queensland, New South Wales, South Australia, Australian Capital Territory, Victoria, Western Australia, Northern Territory).7. Describe the purpose of a classification scheme. How does a web site benefit from a classification scheme?A classification scheme is a hierarchical arrangement of preferred terms which organises objects according to some principals.A website benefits by having a succinct and easily understood navigation system based off a classification scheme.8. Create a classification scheme for several major dog breeds based on whether the dog is toy, small, medium, large, or giantDog > toy > brown > long hair > Yorkshire TerrierDog > small > tri-colour > short hair > BeagleDog > medium > black & white > medium hair > Border CollieDog > large > brown > medium hair > German ShepherdDog > giant > red-brown > medium hair > St Bernard11. What topics would the following codes retrieve under Dewey Decimal Classification:a. 025.524Generalities (000) > Library & information services (020) > Library operations (025) > Services to users (025.5) > Information search and retrieval (025.524)b. 787.87092The Arts (700) > Music (780) > Stringed Instruments (787) > Guitars (787.87)c. 641.623Technology (Applied sciences) (600) > Home economics & family living (640) > Food & drink (641) > Cooking specific material (641.6) > Cooking with beer (641.623)d. 634.772Technology (Applied sciences) (600) >Agriculture (630) > Orchards, fruits, forestry (634) > Bananas (634.772)e. 522.29Natural sciences & mathematics (500) > Astronomy & allied sciences (520) > Techniques, equipment, materials (522) > Movable nonspace telescopes (522.29)16. What is the purpose of thesauri? How is a thesaurus beneficial to searching on a web site?Thesauri provides meaning gap filling between concepts and words.It is beneficial in helping a user to find what they are looking for even if they don’t know exactly what to search.19. Consider the preferred term “car”. List a variant term(s), broader term, narrower term, and related term(s).VT- van, sedanBT- vehicleNT- 3 door car, four wheel drive, Holden carRT- truck, road20. Consider the preferred term “sword”. List a variant term(s), broader term, narrower term, and related term(s).VT- single edge bladeBT- weaponNT- short sword, long swordRT- arrow, knife30. Design a controlled vocabulary for your proposed Drupal web site.Technology related conferences and events.Mobile, DesktopAndroid, iOS, Web, WindowsSecurity, Programming, EntrepreneurshipConference, Hackathon, Forum",18/10/2015,0,18.0,0.0,740.0,702.0,2.0,2.0,0.0,2.0,en
3863,Understanding Dropout,Konvergen.AI,Roan Gylberth,171.0,6.0,1031.0,"One particular layer that is useful, yet mysterious when training neural networks is Dropout. Dropout is created as a regularization technique, that we can use to reduce the model capacity so that our model can achieve lower generalization error. The intuition is easy, we didn’t use all neurons but only turn on some neuron in each training iteration with probability p. But how does dropout works, and is it the same as the implementation?When training neural networks, our models are prone to over-fitting, achieving very low error on training data but have a much higher error on the testing data. To counter this, we use regularization technique to lower the model capacity so that we can reduce the gap of training and testing error and our model can have similar performance in both training and testing phase.If we recall how neural networks learn, it is learning by altering the weight parameters to minimize the loss function L. Hence, our optimization task is to minimize L.Regularization works by adding some weighted regularization function in the loss so our model is better conditioned and more stable.Where 𝜆 is the regularization parameters and R(𝛳) is the regularization function. A popular example of regularization technique is L2 Regularization or weight decay which use l2 norm of the weights as the regularization function, specificallywhere w is the weights. We often opt to leave the biases from the equation because it could hurt our model badly and leads to under-fitting.One particular method to reduce generalization error is by combining several different models, which we often call model ensemble or model averaging. This makes sense because, in one model, we can have errors in one part of the test data, while another model has errors in another part of the test data. Thus, by combining several models we can get a more robust result since the parts that are already correct in most models won’t change and the error will be reduced. So, the averaged models will perform at least as well as any of its members.For example, we train k models which each model have error e_i. Suppose that the errors have variance v and covariance c. Then the error from averaging all k models isand the expected squared error of the average model error isWe can see that if we have perfectly correlated errors, then we have v=c and the mean squared error of the models reduced to v. Also if we have perfectly uncorrelated errors, then we have c=0 and the mean squared error becomes (1/k)v. This shows that the expected squared error is inversely proportional to the ensemble size.Unfortunately, training many models, especially deep neural networks are computationally expensive. The problems didn’t just come at training phase, running inference on (exponentially) many models are computationally expensive too. This makes averaging many deep neural networks become unpractical, especially when the resources are limited. Srivastava et al. acknowledges this challenge and comes up with a way to get an approximation of that process.There are two phases that we need to understand, i.e., training and testing phase.The intuition for dropout training phase are quite simple. We turn off some neurons at training time to make the networks different at each training iteration. The way we turn off the neurons can be viewed in the figure above. We multiply each input y_i to a neuron r_i that is a two-points distribution that outputs either 0 or 1 with Bernoulli distribution. So, without dropout, the forward pass in training phase iswhere the input of the activation function f is just the sum of weights w multiplied by the input y. Now, with dropout, the forward pass in the training phase becomewhere we can see in the second line, we add a neuron r which either keep the neuron by multiplying the input with 1 with probability p or shut down the neuron by multiplying the input with 0 with probability 1-p, then do the same forward pass as without dropout.For a neural networks with n neurons, the training phase can be seen as training an ensemble of 2^n possible different neural networks, since there are two possibilities of the state of each neuron. At the testing phase, we can use the same methods of averaging as model ensemble, that is we run the inference in all possible neural networks and average the results. However, doing this is not feasible and computationally expensive. Hence, Srivastava et al. propose a way to get the approximation of the average, that is by doing inference in the neural networks without dropout.To get the approximation, we have to make sure that the output of the inference is the same the expected value of the inference while training. Suppose that the output of a neuron is z and the dropout probability p(r) is p, then the expected value of the neuron with dropout ishence, to get the same output in the testing as the expected output of the training, we can scale the weights of each neuron in testing phase by p. This method is called weight scaling inference.One example of the most straightforward dropout implementation is the one introduced in cs231n lecture notes about neural networks. I will use an adaptation of their implementation while elaborating the important parts.This is the implementation of dropout in three layered DNN with ReLU as the activation function. See that we apply dropout before the input come to the hidden layer 2 and the output layer. Since that the Bernoulli distribution is a special case of Binomial distribution where n=1, we can use numpy.random.binomial to create the dropout mask.Notice that in the prediction phase we multiply each layer with the keep_prob, this is the implementation of the weight scaling inference described above. However, running this for each layers can increase prediction time. So, there is another implementation that avoids this by scaling the weights by 1/keep_prob in training time, which usually called the inverted dropout.Notice that instead of scaling the output by keep_prob in the prediction, we scale the weight by 1/keep_prob in the training phase. In this way, the expected value of the outputs is already z, so there is no scaling needed in the prediction phase.",21/07/2019,0,2.0,30.0,700.0,197.0,10.0,1.0,0.0,3.0,en
3864,TokenGo запускает ICO!,Medium,TokenGo Platform_RU,294.0,4.0,652.0,"Приветствую вас, друзья! TokenGo запускает ICO!Мы долго шли к этому дню, к этому волнующему событию. До момента запуска ICO платформы TokenGo остались считанные часы.В первую очередь я хочу сказать спасибо всем тем, кто сегодня с нами! С кем-то мы знакомы уже несколько месяцев, успели пообщаться, обсудить будущее и подружиться, кто-то присоединяется только сейчас, изучает White Paper, читает темы на форумах, задает вопросы в Telegram-чате. И это очень здорово, что наше сообщество постоянно растет, укрепляется, и каждый участник вносит свой вклад в строительство экосистемы TokenGo. Отдельно хочу поздравить инвесторов, записавшихся в White List. Уверен, что полученный вами уникальный бонус вас обязательно порадует!А в TokenGo все продолжает идти по плану. Совсем недавно мы провели переговоры с нашими китайскими товарищами, и могу сказать, что они завершились крайне удачно! Достигнута договоренность о представлении TokenGo на местах, и мы ожидаем начало активного инвестирования примерно через пару недель с момента запуска ICO. В помощь новым друзьям, уже сейчас мы перевели наш сайт на китайский язык, заканчиваем перевод White Paper, размещаем темы на самых популярных китайских форумах, подключаем WeChat (аналог нашего Telegram).Аналогичным образом мы продолжаем работать и вести переговоры с потенциальными инвесторами со всего земного шара, и уверены в собственном успехе! Я думаю, что вы все успели заметить, как в последнее время ускорился приток участников! Мы усиливаем рекламу по всем направлениям и каналам. Задача наших маркетологов заключается в том, чтобы ежедневно отслеживать конверсию каждого из каналов, собирать статистику и готовить необходимые публикации для выстраивания дальнейшего диалога со своей целевой аудиторией.Как все уже успели заметить, для нас очень важно ваше мнение. Мы стараемся учитывать все пожелания и предложения. Анализировать собственные ошибки и делать работу в TokenGo максимально приятной и эффективной, ведь от того, как сейчас общими усилиями будет заложен фундамент платформы, напрямую зависит ее последующее полноформатное функционирование. Специально к этапу ICO по вашим просьбам мы внесли несколько новых важных обновлений:- в целях исключения визуальных скачков собранной в Ethereum суммы (из-за постоянных изменений курса), мы приняли решение ежедневно производить фиксацию курса в течение дня по первой сделке между каждой из валют и ETH. Это позволит сделать статистику сбора более понятной и наглядной.- мы добавили возможность осуществлять покупку токенов за фиатные деньги! Теперь Вы можете выбрать, как Вы хотите оплатить токены — одной из пяти популярных криптовалют или же с помощью привычных банковских карт. В ближайшее время мы планируем подключить также дополнительные платежные системы. Разумеется, как и ранее, Вы можете заранее просчитать необходимую сумму на встроенном калькуляторе.- мы значительно расширили штат администраторов в официальном Telegram чате и модераторов по Баунти кампании. Кроме этого, у нас появились sales-менеджеры, которые круглосуточно готовы помочь любому участнику, имеющему какие-либо проблемы с инвестированием.Друзья, впереди у нас много дел и свершений. Мы встали на правильный путь и обязательно пройдем его вместе с вами. Выполним Дорожную карту, организуем удобную для всех и приносящую пользу Экосистему TokenGo с востребованным и растущим в цене токеном GPT.А еще мы обязательно продолжим общаться. Через этот блог, через видео и онлайн-конференции. Через стримы на Facebook. Кстати, в данный момент у меня утверждается большая программа на апрель и май месяцы по участию в различных конференциях. Вся информация будет обязательно указана в официальном блоге TokenGo. Буду рад увидеть вас вживую!В завершении я хочу пожелать всем нам терпения, благоразумия, силы воли, чтобы дойти этот путь достойно! И, конечно же, удачи и успеха! Я уверен, что вместе у нас всё обязательно получится!Сокращение от GoPowerGPTСтарт ICO 27.02.2018 00:00 UTCICO завершение 30.04.2018 23:59 UTCSoft Cap10000 ETHHard Cap250000 ETHКурс1 GPT = 0.00057143 ETHПринимаем ETH, BTC, BCH, LTC, DASHОбщая эмиссия: 700 000 000Доступно для покупки: 600 000 000Стратегические партнеры и ранние инвесторы: 50 000 000Команда разработчиков: 30 000 000Баунти Кампания: 20 000 000Регуляция: Все нераспроданные токены будут уничтоженыПовышение стоимости: GPT 0,5% в день начиная с первого дня ICOБольше о TokenGo:Как работает TokenGo. Примеры использованияТокенизация бизнеса на платформе TokenGoКонсенсус TokenGoЭкосистема TokenGoХарвестинг TokenGoОснователь TokenGo Антон Бендерский о перспективах платформы.Основатель TokenGo Антон Бендерский о допущенных ошибках и текущих достижениях.Официальный сайт TokenGoRU ветка TokenGo на bitcointalkОфициальный RU блог TokenGoОфициальный EN блог TokenGoWhitepaper RUWhitepaper ENGithubTwitterFacebookTelegram чатTelegram канал",26/02/2018,0,3.0,0.0,1300.0,782.0,2.0,0.0,0.0,22.0,ru
3865,Tips to avoid the pitfall of over fitting in Linear Regression ,Medium,karthic Rao,745.0,2.0,49.0,Tips to avoid the pitfall of over fitting in Linear Regression8. The choice of the model has to be based on the observation from training error and test error . Also its tricky to make choice of right features to come to make build the model for your predictions.,10/01/2016,0,1.0,0.0,1400.0,1112.0,1.0,1.0,0.0,0.0,en
3866,Step by step -Understand the architecture of Region proposal network (R-CNN),Medium,Pallawi,432.0,9.0,1783.0,"This blog is written to explain the evolution of object detection models in simple words and self-explanatory diagrams. This blog can be helpful to every individual who is entering into the field of computer vision and data science or has taken up a project which requires solving an object detection problem.We all must have heard about Faster R-CNN and there are high chances that you found this blog when you searched for the keyword “Faster R-CNN” as it has been among the state of arts used in many fields since January 2016.A strong object detection architecture like Faster RCNN is built upon the successful research like R-CNN and Fast R-CNN. To honestly enjoy working, troubleshooting and pursuing the dream of creating your own model which can one day be called a state of art my friend I would always recommend reading the research papers in chronological order.Therefore I have tried my best to explain all of the three architectures so that we do not miss on the basics. One day we will build our architecture and contribute to the field we are passionate about.I will also talk about the stories of my struggles while reading these papers and the attitude we must have to read fearlessly. To keep the spirit high, I want to state the quote which kept me motivated throughout reading the papers and finally writing this blog for you all. Reading is important because if you can read, you can learn anything about everything and everything about anything.-Tomie DepaolaThe paper that talks about R-CNN is Rich feature hierarchies for accurate object detection and semantic segmentation which is popularly known as R-CNN. It was published in the year 2014.How it all started?In the year 2004 papers like Distinctive Image Features from Scale-Invariant Keypoints and Histograms of Oriented Gradients for Human Detection describes the use of SIFT and HOG features to address visual recognition task. But if we look at performance on the canonical visual recognition task, PASCAL VOC object detection challenge, it is generally acknowledged that progress has been slow during 2010–2012, with small gains obtained by building ensemble systems and employing minor variants of successful methods.Then in the year, 2012 ImageNet Classification with Deep Convolutional Neural Networks made its mark in the field of CNN’s by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Their success resulted from training a large CNN on 1.2 million labelled images, together with a few twists on CNN (e.g., max(x; 0) rectifying nonlinearities and “dropout” regularization).The significance of the ImageNet result was vigorously debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?The R-CNN paper answers this question by bridging the gap between image classification and object detection. This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.A challenge faced in detection is that labelled data is scarce and the amount currently available is insufficient for training a large CNN.The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning. The second principle contribution of this paper is to show that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain-specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. During the experiments, fine-tuning for detection improved the mAP performance by 8 percentage points. After fine-tuning, R-CNN achieved an mAP of 54% on VOC 2010 compared to 33% for the highly-tuned, HOG-based deformable part model (DPM). The authors of R-CNN also mentions the paper DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition which is a black box feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaptation.As R-CNN operates on regions it is natural to extend it to the task of semantic segmentation. With minor modifications, the authors also achieved competitive results on the PASCAL VOC segmentation task, with an average segmentation accuracy of 47.9% on the VOC 2011 test set.We know that recognition occurs several stages downstream, which suggests that there might be hierarchical, multi-stage processes for computing features that are even more informative for visual recognition.To achieve this result, R-CNN focused on two problems: localizing objects with a deep network and training a high-capacity CNN model with only a small quantity of annotated detection data. In order to maintain high spatial resolution, CNNs those days typically had only had two convolutional and pooling layers.Units high up in R-CNN network, which had five convolutional layers, have very large receptive fields (195 x195 pixels) and strides (32x32 pixels) in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge.R-CNN solve the CNN localization problem by operating within the “recognition using regions” paradigm, which has been successful for both object detection and semantic segmentation.R-CNN generates 2000 category-independent region proposals for the input image, these 2000 regions are proposed using the selective search.Selective Search is a method for finding a large set of possible object locations in an image, independent of the class of the actual object. It works by clustering image pixels into segments and then performing hierarchical clustering to combine segments from the same object into object proposals.The selective search uses multiple image processing algorithms to output these proposals.Input — RGB imageOutput — 2000 proposed regions which consist of background and the object classesThese regions are converted to bounding boxes. These bounding boxes have precise information about pixel locations of proposed regions on the imageRegardless of the size or aspect ratio of the candidate region, the authors warped all pixels in a tight bounding box to the required size. A simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region’s shape.Input — 2000 proposed regions which consist of background and the object classesOutput- 2000 warped images of fixed sizeDuring those times when we had limited domain-specific annotated dataset and computation capabilities, the authors adopted a method called as Supervised pre-training where they pre-trained the CNN on a large auxiliary dataset (ILSVRC2012 classification) using image-level annotations only (bounding box labels were not available for this data). This could be considered as a way to use pre-trained weights.Input — Classification dataset which exhibited feature similarities with object classes to be used for Domain-specific training and has ample amount if data to be trained.Output- A CNN when given an image can classify those images into classes based on the object that it has in those images.After the Supervised pre-training, the next step was a domain-specific fine-tuning. where the pre-trained CNN was trained with the 2000 fixed size warped images.Each warped bounding box is then forward propagated through a CNN network. During the forward propagation, a convolution neural network does feature extraction from each of the proposed regions. So from every input image, 2000 warped images would enter the CNN sequentially (one at a time). Now one can imagine the time the network will consume to process a single image.Then features are extracted by CNN and warped into a fixed-length feature vector. Then each fixed-length feature vector is classified using category-specific linear Support vector machine(set of class-specific linear SVMs).To fine-tune the CNN they used stochastic gradient descent (SGD) at a starting learning rate of 0.001. During the training, they treated all region proposals (warped images) with 0.5 IoU overlap with a ground-truth box as positives for that box’s class and the rest as negatives.During backpropagation, they uniformly sample(sampling is called as a process of picking data from a pool of dataset) 32 positive windows (overall classes) and 96 background windows to construct a mini-batch of size 128. They biased the sampling towards positive windows because they are extremely rare compared to the background.Once the domain-specific CNN is trained and we start getting the classification and bounding boxes. But we know that the layers of the CNN were not efficient to deliver high precision and recall. So to make the model robust SVM’s were trained. The SVM was trained only after the domain-specific CNN was trained with satisfactory performance.To train the SVM the 2000 regions were proposed by selective search, then these regions were warped and then sent through the trained domain-specific CNN which extracts features from 2000 warped regions and then convert them into feature vectors. These feature vectors (output from CNN) were used as input to train SVM. The confidence threshold that was used to train SVM as a positive and negative object class was 0.3. The trained CNN would output the class of every feature vector. Based on this prediction information by CNN the vectors would be used as input to train class specific linear SVM's. One SVM for one class. Example: If a CNN classifies the feature vector belonging to a class “cat” then that feature vector would be used to train the CAT SVM.During training the class-specific SVM’s a class-specific bounding box regressor is also trained simultaneously. The input to the regressor is the feature vectors computed by domain-specific CNN. The regressor helps in predicting tight bounding boxes.SVM input- Class-specific feature vectors predicted by the trained domain-specific model.SVM Output — Precisely classified feature vectorsStep-1 Selective search on the test image to extract around 2000 region proposals.Step-2 Each proposal is warped and forward propagate it through the domain-specific CNN in order to compute features.Step-3 Then, for each class, the authors score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-over union (IoU) overlap with a higher scoring selected region larger than a learned threshold.Hope I was able to help you understand how R-CNN works. I have taken some efforts to create self-explanatory diagrams. I searched and read R-CNN from many sources but always found a very high-level diagram with precise literature. But since there has bee so much advancement done over this model that many people read and write about it. I have tried my best to formulate it on this page. Certainly, this model is not used these days but if you want to build your own model someday I would highly recommend you to read all the initial days research paper. You can read about other models like Fast, Faster-RCNN, YOLO V1, V2, V3 and Single-shot detector in my next blogs.If you found this blog helpful, please give a clap so that others may find it.d2l.aitowardsdatascience.com",05/09/2020,0,12.0,0.0,1103.0,631.0,4.0,0.0,0.0,16.0,en
3867,Invest in Unchainet |Heterogeneous Cloud Computing Infrastructure,Medium,Unchainet,11.0,2.0,275.0,"Unchainet is aiming to provide a decentralized cloud platform connecting providers with spare computing resources and clients who need them. Research shows 30% of servers in private data centers consume energy but are not being used. We are working to provide easy-to-install software for companies with private data centers, hosting companies and individuals so they can easily connect to the Unchainet network and start earning money on a transparent and efficient marketplace. Unchainet clients will include existing partners and all other cloud users. Our platform’s important differentiation from competing decentralized cloud platforms is familiar open source technology and bridging interfaces which completely removes friction associated with staff training and allows easy migration from existing platforms.The team plans to use blockchain technology (QoS chain and UNET chain) to bring trust and transparency between clients and providers and fast peer-to-peer contracts and transactions. It’s just a backbone technology for platform users, but very important for the community to see the network size, volumes of contracts and tamper-proof quality of providers. Computing resources are traded in UNET cryptocurrency. All UNETs will be distributed during ICO, and new clients will need to purchase them from token holders through exchanges. Growing number of contracts on the platform will raise the demand for UNETs. UNET starts as the ERC20 token, but our roadmap includes building custom blockchain based on modularity of Tendermint blockchain and fast speeds of Red Belly blockchain while achieving better decentralization on algorithm layer (Proof of Beneficial Work algorithm) and even geo-distribution of voting nodes. This will allow us to migrate UNETs to our native blockchain and launch modular blockchain as a service offering for 3rd party DApps.",28/08/2018,0,1.0,0.0,1200.0,628.0,1.0,0.0,0.0,2.0,en
3868,Coding is a Trap. Get Out.,TekLit,Justin,1800.0,3.0,576.0,"The looming threat to the average programmer.Let’s face it. Unless you are talented enough for Google to hire you, you are probably limited to developing APIs, websites, or customizing an ERP-like business system.If toiling day after day, adding mundane features to boring systems isn’t enough for you, you have the added task of keeping up with the frameworks and tools that will evolve with or without you. It is easy to only focus on what you are working on without building new skills. But, should you do that, it will be much harder to find work when your job is rendered obsolete.Chasing skills to remain economically viable is just the beginning of the trap. Coding tends to pay significantly higher than most jobs, making a move to a project management position or management position is often lateral in compensation.It may be only a short while after switching jobs that the newness wears off, and that nice pay raise doesn’t feel like enough money to you anymore.Should you decide that switching companies will make you happy, most other software development jobs are similar or even worse. It may be only a short while after switching jobs that the newness wears off, and that nice pay raise doesn’t feel like enough money to you anymore. Now you’ve exacerbated the situation.Once you have switched jobs, yielding a nice salary increase, you will likely not find another job with the same pay level. You’ve probably grown accustomed to a comfortable lifestyle that requires a higher level of income.You’ve acquired a mortgage, or perhaps an expensive apartment in your dream neighborhood. Maybe you’ve bought a nice car and wracked up a few credit cards. Now your 30% debt-to-income ratio is thousands of dollars higher than most of your fellow Americans’. Or maybe you married someone who grew accustomed to the perks of an above-average earning spouse.Now your 30% debt-to-income ratio is thousands of dollars higher than most of your fellow Americans’.You could start snowballing your debt payments, save a little money and take a different job with a lower salary. Would you be happy starting all over? You could continue business as usual, but would you keep up with the cultish trends of the trade enough to maintain the status quo?If you do manage to skill up with your remaining time after long hours and tight deadlines, how long will it be before the powerful monopolies of the tech industry replace you (or at least most of you)? What happens when they figure out AI can do 80% of the heavy lifting?Writing a program used to mean that you created something to change the world or at least a little piece of it. How long will the magic last while making the same tedious API calls or adding a new field to a web form? How long before your days in your home office run together and building another Raspberry Pi gadget isn’t satisfying?How long before your days in your home office run together and building another Raspberry Pi gadget isn’t satisfying?For billions of impoverished people finally connected to the interwebs, coding is the ticket to a good life, possibly even a “green card.” For many American computer science majors, it’s a fast track to a prison disguised as a passion.There are a few simple yet elusive ways to prevent being trapped. But you can figure them out on your own, you problem solver, you!Justin McClainImage Credit: Einar Storsul from Pixabay",10/07/2021,0,0.0,2.0,700.0,438.0,1.0,0.0,0.0,0.0,en
3869,In case you missed it: My Webinar on Model-Based Machine Learning,emaasit,Daniel Emaasit,253.0,1.0,74.0,"In case you missed my free webinar on “Model-Based Machine Learning”, here is the recording.Apologies for the poor quality of the video. Domino Data Lab’s webinar platform suffered a service degradation while recording the event. The webinar slides may be found below.[slideshare id=64647075&doc=3rdpresentationpaperreview-160803065711]If you have any questions, please do not hesitate to contact me. Finally, I would like to thank Daniel Enthoven and Daniel Chalef from Domino Data Lab for setting up this webinar.",03/08/2016,0,1.0,0.0,1011.0,603.0,1.0,0.0,0.0,3.0,en
3870,Instance Segmentation in Google Colab with Custom Dataset,HackerNoon.com,RomRoc,133.0,5.0,712.0,"This article proposes an easy and free solution to train a Tensorflow model for instance segmentation in Google Colab notebook, with a custom dataset.Previous article was about Object Detection in Google Colab with Custom Dataset, where I trained a model to infer bounding box of my dog in pictures. The protagonist of my article is again my dog: in this case we take a step forward, we identify not only the bounding box, we make even pixel wise classification.Compared to previous article, we hold the same characteristics:These features allow anybody following this tutorial to create an instance segmentation model, and test it in Google Colab or export the model to run in a local machine.Source code of this article, including the sample dataset, is available in my Github repo.There are various open source frameworks to implement instance segmentation, you can find an overview in this presentation of Stanford University.We discard solutions that are not based on Tensorflow, such as Facebook Detectron based on Caffe2, because we decided to train the model in Google Colab, that is already integrated with Tensorflow.One of the most popular frameworks, easy to use and well documented, is Matterport Mask R-CNN. From my tests it’s one of the simplest and most robust implementations available.In addition, a big effort I faced with other implementations is to convert the annotations output file to framework input format. To be clear, once you create the pixel annotations of dataset with a graphical tool, you should convert it to input format defined by training framework.Matterport developed this task in a clear article, demonstrating how to transform annotations file to Matterport Mask R-CNN format.In previous article we created bounding box annotations to obtain object detection model, now we are going to train instance segmentation model, therefore we create pixel level mask annotations to define the boundaries of the objects in dataset. Among various available tools, I chose an intuitive and well done tool: VGG Image Annotator (VIA) by University of Oxford, you can see documentation in the official page of the project. Furthermore, it’s easy to integrate VIA with Matterport framework.This tool doesn’t need any installation, you just download the package and open the via.htmlfile with a modern browser.It’s important to create a good dataset to achieve a well performing trained model. Taking pictures of objects with different lighting conditions, from various angles and in different contexts, are good principles to obtain a well generalized model, and avoid overfitting.At the end of the annotation process, I created “images.zip” file with the following structure:Lastly I uploaded zip file into Google Drive, to use it during the training and test process. I included the dataset file in my Gitub repo, having pixel wise annotations of dog images.All the steps are in Google Colab notebook included in my repo. In my example, training process last about half an hour for 5 epochs, to get a more accurate model you can increase the number of epochs and the dataset size.I selected Python3 GPU enabled environment, to use up to 12 hours of Tesla K80 GPU offered in Google Colab. Next steps in notebook are:Install required packages: install packages, repositories and environment variables for Matterport instance segmentation with Tensorflow.Download and extract dataset: download images.zip dataset in Google Colab filesystem, previously uploaded in Google Drive. Update fileId variable with Google Drive id of your image.zip dataset.Edit settings file: code in my repo is inspired by Matterport Splash of Color sample, to run with a different dataset you should replace occurrences of “balloon” and “Balloon” with the name of object.Train model: use pretrained weights to apply transfer learning in training process. Options are COCO and ImageNet.Training process outputs the structure of neural network and various parameters, like the network architecture (Resnet50 or Resnet101).Below the Tensorboard charts of training process:Finally we can run test dataset inference with trained model.Output includes inference data (image resolution, anchors shapes, …), and test images with bounding box, segmentation mask and confidence score.If you want to run instance segmentation on a single object class, you can make a few minor changes to my Github code and adapt it to your dataset.I hope you liked this article, in case leave some claps, it will encourage me to write other practical articles about machine learning for computer vision :)",18/09/2018,2,8.0,5.0,848.0,512.0,4.0,1.0,0.0,5.0,en
3871,"Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK.",Towards Data Science,Javed Shaikh,490.0,7.0,1249.0,"Latest Update:I have uploaded the complete code (Python and Jupyter notebook) on GitHub: https://github.com/javedsha/text-classificationDocument/Text classification is one of the important and typical task in supervised machine learning (ML). Assigning categories to documents, which can be a web page, library book, media articles, gallery etc. has many applications like e.g. spam filtering, email routing, sentiment analysis etc. In this article, I would like to demonstrate how we can do text classification using python, scikit-learn and little bit of NLTK.Disclaimer: I am new to machine learning and also to blogging (First). So, if there are any mistakes, please do let me know. All feedback appreciated.Let’s divide the classification problem into below steps:The prerequisites to follow this example are python version 2.7.3 and jupyter notebook. You can just install anaconda and it will get everything for you. Also, little bit of python and ML basics including text classification is required. We will be using scikit-learn (python) libraries for our example.The data set will be using for this example is the famous “20 Newsgoup” data set. About the data from the original website:The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.This data set is in-built in scikit, so we don’t need to download it explicitly.i. Open command prompt in windows and type ‘jupyter notebook’. This will open the notebook in browser and start a session for you.ii. Select New > Python 2. You can give a name to the notebook - Text Classification Demo 1iii. Loading the data set: (this might take few minutes, so patience)Note: Above, we are only loading the training data. We will load the test data separately later in the example.iv. You can check the target names (categories) and some data files by following commands.Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).Scikit-learn has a high level component which will create feature vectors for us ‘CountVectorizer’. More about it here.Here by doing ‘count_vect.fit_transform(twenty_train.data)’, we are learning the vocabulary dictionary and it returns a Document-Term matrix. [n_samples, n_features].TF: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) / #Total words, in each document.TF-IDF: Finally, we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency.We can achieve both using below line of code:The last line will output the dimension of the Document-Term matrix -> (11314, 130107).There are various algorithms which can be used for text classification. We will start with the most simplest one ‘Naive Bayes (NB)’ (don’t think it is too Naive! 😃)You can easily build a NBclassifier in scikit using below 2 lines of code: (note - there are many variants of NB, but discussion about them is out of scope)This will train the NB classifier on the training data we provided.Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.Performance of NB Classifier: Now we will test the performance of the NB classifier on test set.The accuracy we get is ~77.38%, which is not bad for start and for a naive classifier. Also, congrats!!! you have now written successfully a text classification algorithm 👍Support Vector Machines (SVM): Let’s try using a different algorithm SVM, and see if we can get any better performance. More about it here.The accuracy we get is~82.38%. Yipee, a little better 👌Almost all the classifiers will have various parameters which can be tuned to obtain optimal performance. Scikit gives an extremely useful tool ‘GridSearchCV’.Here, we are creating a list of parameters for which we would like to do performance tuning. All the parameters name start with the classifier name (remember the arbitrary name we gave). E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.Next, we create an instance of the grid search by passing the classifier, parameters and n_jobs=-1 which tells to use multiple cores from user machine.This might take few minutes to run depending on the machine configuration.Lastly, to see the best mean score and the params, run the following code:The accuracy has now increased to ~90.6% for the NB classifier (not so naive anymore! 😄) and the corresponding parameters are {‘clf__alpha’: 0.01, ‘tfidf__use_idf’: True, ‘vect__ngram_range’: (1, 2)}.Similarly, we get improved accuracy ~89.79% for SVM classifier with below code. Note: You can further optimize the SVM classifier by tuning other parameters. This is left up to you to explore more.This is the pipeline we build for NB classifier. Run the remaining steps like before. This improves the accuracy from 77.38% to 81.69% (that is too good). You can try the same for SVM and also while doing grid search.2. FitPrior=False: When set to false for MultinomialNB, a uniform prior will be used. This doesn’t helps that much, but increases the accuracy from 81.69% to 82.14% (not much gain). Try and see if this works for your data set.3. Stemming: From Wikipedia, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. E.g. A stemming algorithm reduces the words “fishing”, “fished”, and “fisher” to the root word, “fish”.We need NLTK which can be installed from here. NLTK comes with various stemmers (details on how stemmers work are out of scope for this article) which can help reducing the words to their root form. Again use this, if it make sense for your problem.Below I have used Snowball stemmer which works very well for English language.The accuracy with stemming we get is ~81.67%. Marginal improvement in our case with NB classifier. You can also try out with SVM and other algorithms.Conclusion: We have learned the classic problem in NLP, text classification. We learned about important concepts like bag of words, TF-IDF and 2 important algorithms NB and SVM. We saw that for our data set, both the algorithms were almost equally matched when optimized. Sometimes, if we have enough data set, choice of algorithm can make hardly any difference. We also saw, how to perform grid search for performance tuning and used NLTK stemming approach. You can use this code on your data set and see which algorithms works best for you.Update: If anyone tries a different algorithm, please share the results in the comment section, it will be useful for everyone.Please let me know if there were any mistakes and feedback is welcome ✌️Recommend, comment, share if you liked this article.http://scikit-learn.org/ (code)http://qwone.com/~jason/20Newsgroups/ (data set)",24/07/2017,14,46.0,15.0,936.0,327.0,2.0,2.0,0.0,14.0,en
3872,Deep Reinforcement Learning for Automated Stock Trading,Towards Data Science,Bruce Yang,872.0,15.0,1893.0,"Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.This blog is based on our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, presented at ICAIF 2020: ACM International Conference on AI in Finance.Our codes are available on Github.github.comOur paper is available on SSRN.papers.ssrn.comIf you want to cite our paper, the reference format is as follows:Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy. In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA.A most recent DRL library for Automated Trading-FinRL can be found here:github.comFinRL for Quantitative Finance: Tutorial for Single Stock TradingFinRL for Quantitative Finance: Tutorial for Multiple Stock TradingFinRL for Quantitative Finance: Tutorial for Portfolio AllocationElegantRL supports state-of-the-art DRL algorithms and provides user-friendly tutorials in Jupyter notebooks. The core codes <1,000 lines, using PyTorch, OpenAI Gym, and NumPy.towardsdatascience.comOne can hardly overestimate the crucial role stock trading strategies play in investment.Profitable automated stock trading strategy is vital to investment companies and hedge funds. It is applied to optimize capital allocation and maximize investment performance, such as expected return. Return maximization can be based on the estimates of potential return and risk. However, it is challenging to design a profitable strategy in a complex and dynamic stock market.Every player wants a winning strategy. Needless to say, a profitable strategy in such a complex and dynamic stock market is not easy to design.Yet, we are to reveal a deep reinforcement learning scheme that automatically learns a stock trading strategy by maximizing investment return.Our Solution: Ensemble Deep Reinforcement Learning Trading StrategyThis strategy includes three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG).It combines the best features of the three algorithms, thereby robustly adjusting to different market conditions.The performance of the trading agent with different reinforcement learning algorithms is evaluated using Sharpe ratio and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy.Existing works are not satisfactory. Deep Reinforcement Learning approach has many advantages.Reinforcement Learning is one of three approaches of machine learning techniques, and it trains an agent to interact with the environment by sequentially receiving states and rewards from the environment and taking actions to reach better rewards.Deep Reinforcement Learning approximates the Q value with a neural network. Using a neural network as a function approximator would allow reinforcement learning to be applied to large data.Bellman Equation is the guiding principle to design reinforcement learning algorithms.Markov Decision Process (MDP) is used to model the environment.Recent applications of deep reinforcement learning in financial markets consider discrete or continuous state and action spaces, and employ one of these learning approaches: critic-only approach, actor-only approach, or and actor-critic approach.1. Critic-only approach: the critic-only learning approach, which is the most common, solves a discrete action space problem using, for example, Q-learning, Deep Q-learning (DQN) and its improvements, and trains an agent on a single stock or asset. The idea of the critic-only approach is to use a Q-value function to learn the optimal action-selection policy that maximizes the expected future reward given the current state. Instead of calculating a state-action value table, DQN minimizes the mean squared error between the target Q-values, and uses a neural network to perform function approximation. The major limitation of the critic-only approach is that it only works with discrete and finite state and action spaces, which is not practical for a large portfolio of stocks, since the prices are of course continuous.2. Actor-only approach: The idea here is that the agent directly learns the optimal policy itself. Instead of having a neural network to learn the Q-value, the neural network learns the policy. The policy is a probability distribution that is essentially a strategy for a given state, namely the likelihood to take an allowed action. The actor-only approach can handle the continuous action space environments.3. Actor-Critic approach: The actor-critic approach has been recently applied in finance. The idea is to simultaneously update the actor network that represents the policy, and the critic network that represents the value function. The critic estimates the value function, while the actor updates the policy probability distribution guided by the critic with policy gradients. Over time, the actor learns to take better actions and the critic gets better at evaluating those actions. The actor-critic approach has proven to be able to learn and adapt to large and complex environments, and has been used to play popular video games, such as Doom. Thus, the actor-critic approach fits well in trading with a large stock portfolio.We track and select the Dow Jones 30 stocks (at 2016/01/01) and use historical daily data from 01/01/2009 to 05/08/2020 to train the agent and test the performance. The dataset is downloaded from Compustat database accessed through Wharton Research Data Services (WRDS).The whole dataset is split in the following figure. Data from 01/01/2009 to 12/31/2014 is used for training, and the data from 10/01/2015 to 12/31/2015 is used for validation and tuning of parameters. Finally, we test our agent’s performance on trading data, which is the unseen out-of-sample data from 01/01/2016 to 05/08/2020. To better exploit the trading data, we continue training our agent while in the trading stage, since this will help the agent to better adapt to the market dynamics.• State 𝒔 = [𝒑, 𝒉, 𝑏]: a vector that includes stock prices 𝒑 ∈ R+^D, the stock shares 𝒉 ∈ Z+^D, and the remaining balance 𝑏 ∈ R+, where 𝐷 denotes the number of stocks and Z+ denotes non-negative integers.• Action 𝒂: a vector of actions over 𝐷 stocks. The allowed actions on each stock include selling, buying, or holding, which result in decreasing, increasing, and no change of the stock shares 𝒉, respectively.• Reward 𝑟(𝑠,𝑎,𝑠′):the direct reward of taking action 𝑎 at state 𝑠 and arriving at the new state 𝑠′.• Policy 𝜋 (𝑠): the trading strategy at state 𝑠, which is the probability distribution of actions at state 𝑠.• Q-value 𝑄𝜋 (𝑠, 𝑎): the expected reward of taking action 𝑎 at state 𝑠 following policy 𝜋 .The state transition of our stock trading process is shown in the following figure. At each state, one of three possible actions is taken on stock 𝑑 (𝑑 = 1, …, 𝐷) in the portfolio.At time 𝑡 an action is taken and the stock prices update at 𝑡+1, accordingly the portfolio values may change from “portfolio value 0” to “portfolio value 1”, “portfolio value 2”, or “portfolio value 3”, respectively, as illustrated in Figure 2. Note that the portfolio value is 𝒑𝑻 𝒉 + 𝑏.We define our reward function as the change of the portfolio value when action 𝑎 is taken at state 𝑠 and arriving at new state 𝑠 + 1.The goal is to design a trading strategy that maximizes the change of the portfolio value 𝑟(𝑠𝑡,𝑎𝑡,𝑠𝑡+1) in the dynamic environment, and we employ the deep reinforcement learning method to solve this problem.State Space: We use a 181-dimensional vector (30 stocks * 6 + 1) consists of seven parts of information to represent the state space of multiple stocks trading environmentAction Space:A2C is a typical actor-critic algorithm which we use as a component in the ensemble method. A2C is introduced to improve the policy gradient updates. A2C utilizes an advantage function to reduce the variance of the policy gradient. Instead of only estimates the value function, the critic network estimates the advantage function. Thus, the evaluation of an action not only depends on how good the action is, but also considers how much better it can be. So that it reduces the high variance of the policy networks and makes the model more robust.A2C uses copies of the same agent working in parallel to update gradients with different data samples. Each agent works independently to interact with the same environment. After all of the parallel agents finish calculating their gradients, A2C uses a coordinator to pass the average gradients over all the agents to a global network. So that the global network can update the actor and the critic network. The presence of a global network increases the diversity of training data. The synchronized gradient update is more cost-effective, faster and works better with large batch sizes. A2C is a great model for stock trading because of its stability.DDPG is an actor-critic based algorithm which we use as a component in the ensemble strategy to maximize the investment return. DDPG combines the frameworks of both Q-learning and policy gradient, and uses neural networks as function approximators. In contrast with DQN that learns indirectly through Q-values tables and suffers the curse of dimensionality problem, DDPG learns directly from the observations through policy gradient. It is proposed to deterministically map states to actions to better fit the continuous action space environment.We explore and use PPO as a component in the ensemble method. PPO is introduced to control the policy gradient update and ensure that the new policy will not be too different from the older one. PPO tries to simplify the objective of Trust Region Policy Optimization (TRPO) by introducing a clipping term to the objective function.The objective function of PPO takes the minimum of the clipped and normal objective. PPO discourages large policy change move outside of the clipped interval. Therefore, PPO improves the stability of the policy networks training by restricting the policy update at each training step. We select PPO for stock trading because it is stable, fast, and simpler to implement and tune.Our purpose is to create a highly robust trading strategy. So we use an ensemble method to automatically select the best performing agent among PPO, A2C, and DDPG to trade based on the Sharpe ratio. The ensemble process is described as follows:Step 1. We use a growing window of 𝑛 months to retrain our three agents concurrently. In this paper, we retrain our three agents at every three months.Step 2. We validate all three agents by using a 3-month validation rolling window followed by training to pick the best performing agent which has the highest Sharpe ratio. We also adjust risk-aversion by using turbulence index in our validation stage.Step 3. After validation, we only use the best model with the highest Sharpe ratio to predict and trade for the next quarter.We use Quantopian’s pyfolio to do the backtesting. The charts look pretty good, and it takes literally one line of code to implement it. You just need to convert everything into daily returns.References:A2C:Volodymyr Mnih, Adrià Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. The 33rd International Conference on Machine Learning (02 2016). https://arxiv.org/abs/1602.01783DDPG:Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. International Conference on Learning Representations (ICLR) 2016 (09 2015). https://arxiv.org/abs/1509.02971PPO:John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. 2015. Trust region policy optimization. In The 31st International Conference on Machine Learning. https://arxiv.org/abs/1502.05477John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv:1707.06347 (07 2017). https://arxiv.org/abs/1707.06347",25/08/2020,3,179.0,8.0,1354.0,780.0,14.0,11.0,0.0,35.0,en
3873,The First Inaugural Firefox Census Results,The Official Unofficial Firefox Blog,Firefox,9400.0,4.0,613.0,"We did a bit of informal censusing last month to get to know our users in the best way possible: anonymously and collectively. You might have seen the survey, which we shared through email, our about:home page, and social media. You might have also noticed it came from our Bureau of Censusing (not an official team here), Department of Whimsy (also not an official department, but you better believe we’re doing some introspection now as to why not). It was totally voluntary and, like everything we do, about openness and transparency.So in that spirit, let’s look at the results! You can find the full report here, if you’re into that sort of thing. There were 44 questions and a ton of interesting ways to slice the data, so for the sake of everyone’s tl;dr, we’re going to share a few highlights.Demographics + LifestyleStarting with some basics, we had 123,183 people take this survey. 95.3% were Firefox users and 4.7% were not, so we’re really just looking at our users here.72% of our survey takers were either between 18–34 years old or 55+ (44% and 28% roughly for each of those age groups respectively).80.8% use PC, 12.4% use Mac, and 6.8% use Linux, though our Linux users pointed out that we should have made an option to pick more than one here because not everyone is monogamous to just one operating system.Speaking of our Linux folk, we found they tend to speak more languages than other OS users. 58% of Linux users speak more than one language, compared to 38% for Mac and Windows users.When it comes to where Firefox users can be found when they’re at home, the majority grew up, and still live, in suburban areas (45% and 46%, respectively). Firefox users who live in rural areas, however, are more likely to leave their home to seek their fame and fortune elsewhere.57% of Firefox users who grew up in rural areas moved to the suburbs or city when they grew up. Only 35% of suburban users left, and 32% left the city.This next stat might seem like a strange one to put under Demographics + Lifestyle, but when you spend as much time online as we do, tabs are a way of life. Surprisingly, the vast majority of Firefox users surveyed are tab-conservative: 76% had 1–5 tabs open at the time of taking the survey.To the 40% of Firefox users surveyed who have taken 11+ vacation days this year: we salute you. Roam free!To the 31% of Firefox users surveyed who have taken 0–2 vacation days this year: we salute you, too. Though we hope if you’re able you’ll take a day or two off soon for some self care.The Sorting HatFirefox users largely identify as Gryffindors at 48%.Respondents who identified as Slytherins were about 9% more likely than the other houses to be online for over 6 hours a day. Are those Slytherins spending that extra time online in comment sections? We may never know.Back to that loyal Gryffindor state of mind, though, we’re going to take a leap here and say Firefox users are probably good friends. 55% are willing to travel more than 2 hours to visit a friend for a day and 37% had their last phone call with a friend over a significant other, parent or coworker. That is heartwarming, especially because we thought no one wanted to talk on the phone anymore.And Now for Some Likes & Dislikes:[TL;DR intensifies]Likes:Dislikes:So that’s that. We’ve gotten feedback from all sorts — including our metrics team — and will definitely be conducting a census again next year! What questions (or answers) do you wish had been included? Let us know!",10/11/2016,0,3.0,0.0,1200.0,750.0,4.0,2.0,0.0,1.0,en
3874,"Audio Deep Learning Made Simple: Automatic Speech Recognition (ASR), How it Works",Towards Data Science,Ketan Doshi,2400.0,14.0,1828.0,"Over the last few years, Voice Assistants have become ubiquitous with the popularity of Google Home, Amazon Echo, Siri, Cortana, and others. These are the most well-known examples of Automatic Speech Recognition (ASR). This class of applications starts with a clip of spoken audio in some language and extracts the words that were spoken, as text. For this reason, they are also known as Speech-to-Text algorithms.Of course, applications like Siri and the others mentioned above, go further. Not only do they extract the text but they also interpret and understand the semantic meaning of what was spoken, so that they can respond with answers, or take actions based on the user's commands.In this article, I will focus on the core capability of Speech-to-Text using deep learning. My goal throughout will be to understand not just how something works but why it works that way.I have a few more articles in my audio deep learning series that you might find useful. They explore other fascinating topics in this space including how we prepare audio data for deep learning, why we use Mel Spectrograms for deep learning models and how they are generated and optimized.As we can imagine, human speech is fundamental to our daily personal and business lives, and Speech-to-Text functionality has a huge number of applications. One could use it to transcribe the content of customer support or sales calls, for voice-oriented chatbots, or to note down the content of meetings and other discussions.Basic audio data consists of sounds and noises. Human speech is a special case of that. So concepts that I have talked about in my articles, such as how we digitize sound, process audio data, and why we convert audio to spectrograms, also apply to understanding speech. However, speech is more complicated because it encodes language.Problems like audio classification start with a sound clip and predict which class that sound belongs to, from a given set of classes. For Speech-to-Text problems, your training data consists of:The goal of the model is to learn how to take the input audio and predict the text content of the words and sentences that were uttered.In the sound classification article, I explain, step-by-step, the transforms that are used to process audio data for deep learning models. With human speech as well we follow a similar approach. There are several Python libraries that provide the functionality to do this, with librosa being one of the most popular.We have now transformed our original raw audio file into Mel Spectrogram (or MFCC) images after data cleaning and augmentation.We also need to prepare the target labels from the transcript. This is simply regular text consisting of sentences of words, so we build a vocabulary from each character in the transcript and convert them into character IDs.This gives us our input features and our target labels. This data is ready to be input into our deep learning model.There are many variations of deep learning architecture for ASR. Two commonly used approaches are:Let’s pick the first approach above and explore in more detail how that works. At a high level, the model consists of these blocks:So our model takes the Spectrogram images and outputs character probabilities for each timestep or ‘frame’ in that Spectrogram.If you think about this a little bit, you’ll realize that there is still a major missing piece in our puzzle. Our eventual goal is to map those timesteps or ‘frames’ to individual characters in our target transcript.But for a particular spectrogram, how do we know how many frames there should be? How do we know exactly where the boundaries of each frame are? How do we align the audio with each character in the text transcript?The audio and the spectrogram images are not pre-segmented to give us this information.This is actually a very challenging problem, and what makes ASR so tough to get right. It is the distinguishing characteristic that differentiates ASR from other audio applications like classification and so on.The way we tackle this is by using an ingenious algorithm with a fancy-sounding name — it is called Connectionist Temporal Classification, or CTC for short. Since I am not ‘fancy people’ and find it difficult to remember that long name, I will just use the name CTC to refer to it 😃.CTC is used to align the input and output sequences when the input is continuous and the output is discrete, and there are no clear element boundaries that can be used to map the input to the elements of the output sequence.What makes this so special is that it performs this alignment automatically, without requiring you to manually provide that alignment as part of the labeled training data. That would have made it extremely expensive to create the training datasets.As we discussed above, the feature maps that are output by the convolutional network in our model are sliced into separate frames and input to the recurrent network. Each frame corresponds to some timestep of the original audio wave. However, the number of frames and the duration of each frame are chosen by you as hyperparameters when you design the model. For each frame, the recurrent network followed by the linear classifier then predicts probabilities for each character from the vocabulary.The job of the CTC algorithm is to take these character probabilities and derive the correct sequence of characters.To help it handle the challenges of alignment and repeated characters that we just discussed, it introduces the concept of a ‘blank’ pseudo-character (denoted by “-”) into the vocabulary. Therefore the character probabilities output by the network also include the probability of the blank character for each frame.Note that a blank is not the same as a ‘space’. A space is a real character while a blank means the absence of any character, somewhat like a ‘null’ in most programming languages. It is used only to demarcate the boundary between two characters.CTC works in two modes:Let’s explore these a little more to understand what the algorithm does. We’ll start with CTC Decoding as it is a little simpler.The Loss is computed as the probability of the network predicting the correct sequence. To do this, the algorithm lists out all possible sequences the network can predict, and from that it selects the subset that match the target transcript.To identify that subset from the full set of possible sequences, the algorithm narrows down the possibilities as follows:With these constraints in place, the algorithm now has a set of valid character sequences, all of which will produce the correct target transcript. eg. Using the same steps that were used during Inference, “-G-o-ood” and “ — Go-od-” will both result in a final output of “Good”.It then uses the individual character probabilities for each frame, to compute the overall probability of generating all of those valid sequences. The goal of the network is to learn how to maximize that probability and therefore reduce the probability of generating any invalid sequence.Strictly speaking, since a neural network minimizes loss, the CTC Loss is computed as the negative log probability of all valid sequences. As the network minimizes that loss via back-propagation during training, it adjusts all of its weights to produce the correct sequence.To actually do this, however, is much more complicated than what I’ve described here. The challenge is that there is a huge number of possible combinations of characters to produce a sequence. With our simple example alone, we can have 4 characters per frame. With 8 frames that gives us 4 ** 8 combinations (= 65536). For any realistic transcript with more characters and more frames, this number increases exponentially. That makes it computationally impractical to simply exhaustively list out the valid combinations and compute their probability.Solving this efficiently is what makes CTC so innovative. It is a fascinating algorithm and it is well worth understanding the nuances of how it achieves this. That merits a complete article by itself which I plan to write shortly. But for now, we have focused on building intuition about what CTC does, rather than going into how it works.After training our network, we must evaluate how well it performs. A commonly used metric for Speech-to-Text problems is the Word Error Rate (and Character Error Rate). It compares the predicted output and the target transcript, word by word (or character by character) to figure out the number of differences between them.A difference could be a word that is present in the transcript but missing from the prediction (counted as a Deletion), a word that is not in the transcript but has been added into the prediction (an Insertion), or a word that is altered between the prediction and the transcript (a Substitution).The metric formula is fairly straightforward. It is the percent of differences relative to the total number of words.So far, our algorithm has treated the spoken audio as merely corresponding to a sequence of characters from some language. But when put together into words and sentences will those characters actually make sense and have meaning?A common application in Natural Language Processing (NLP) is to build a Language Model. It captures how words are typically used in a language to construct sentences, paragraphs, and documents. It could be a general-purpose model about a language such as English or Korean, or it could be a model that is specific to a particular domain such as medical or legal.Once you have a Language Model, it can become the foundation for other applications. For instance, it could be used to predict the next word in a sentence, to discern the sentiment of some text (eg. is this a positive book review), to answer questions via a chatbot, and so on.So, of course, it can also be used to optionally enhance the quality of our ASR outputs by guiding the model to generate predictions that are more likely as per the Language Model.While describing the CTC Decoder during Inference, we implicitly assumed that it always picks a single character with the highest probability at each timestep. This is known as Greedy Search.However, we know that we can get better results using an alternative method called Beam Search.Although Beam Search is often used with NLP problems in general, it is not specific to ASR, so I’m mentioning it here just for completeness. If you’d like to know more, please take a look at my article that describes Beam Search in full detail.towardsdatascience.comHopefully, this now gives you a sense of the building blocks and techniques that are used to solve ASR problems.In the older pre-deep-learning days, tackling such problems via classical approaches required an understanding of concepts like phonemes and a lot of domain-specific data preparation and algorithms.However, as we’ve just seen with deep learning, we required hardly any feature engineering involving knowledge of audio and speech. And yet, it is able to produce excellent results that continue to surprise us!And finally, if you liked this article, you might also enjoy my other series on Transformers, Geolocation Machine Learning, and Image Caption architectures.towardsdatascience.comtowardsdatascience.comtowardsdatascience.comLet’s keep learning!",25/03/2021,0,4.0,17.0,628.0,382.0,14.0,18.0,0.0,16.0,en
3875,EagleView high-resolution image semantic segmentation with Mask-RCNN/DeepLabV3+ using Keras and ArcGIS Pro,Towards Data Science,Chunguang (Wayne) Zhang,89.0,8.0,1347.0,"Computer vision in Machine Learning provides enormous opportunities for GIS. Its tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] In the last several years, computer vision is increasingly shifting from traditional statistical methods to the state-of-art deep learning neural network techniques.In this blog, I will share several empirical practices using Keras and ESRI ArcGIS Pro tools with deep learning and transfer learning techniques to build a building footprint image segmentation network model from a super-high-resolution 3-inch of EagleView (Pictometry) imagery.In 2018, ESRI and Microsoft collaborated with the Chesapeake Conservancy to train a deep neural network model to predict land cover from 1-meter NAIP resolution aerial imagery data source. The neural network similar in architecture to Ronnenberger et al.’s U-net (2015), a commonly-used semantic segmentation model was used in that case. Each year, the GIS Core group in Cobb County Georgia receives 3-inch super-high-resolution ortho imagery from EagleView(Pictometry). Could deep learning models be applied to this super-high-resolution ortho imagery to classify land cover or extract building footprints? There are several challenges — Super-high-resolution imagery usually presents varieties of vegetation types and overlaps; buildings and trees creating heavy shadows in the images, which could potentially misclassify the true ground objects.In the beginning, I was very conservative as I decided to use a CPU-only laptop to train roughly 3800 images. Considering the complexity of land cover and building footprints, this is quite a small dataset for deep learning because if you read textbooks, often say deep learning requires a huge amount of training data for better performance. But it is also a realistic classification problem: in a real world-cases, even small-scale image data can be extremely hard to collect and expensive or sometimes almost impossible. Being able to use small datasets and train a powerful classifier is a key skill for a competent data scientist. After many tries and runs, the results turn out very promising especially with state-of-the-art Deeplabv3+ and Mask-RCNN models.Study Area and training image dataset preparationThe geographical area of Cobb County covers with 433 of 1 x 1 mile Pictometry image tiles at a resolution of 3-inch. The county GIS group has a building footprint polygon layer in certain areas. For training purposes, one image tile close to the center of the County was chosen for the image training dataset(fig. 1). The building footprint polygon feature layer was used to process as ground truth mask labels. The “Export Training Data for Deep Learning” in ArcGIS Pro 2.4 ver. of Geoprocessing tool was used to export images and masks for instance segmentation datasets(fig.2). The dimension of the output images are 512x512x3 and rotation is set to 90 degrees to generate more images to prevent overfitting and help the model generalize better.1. Training with Mask-RCNN modelThe resulting training datasets contain over 18000 images and labels. With further data processing to remove no labeling images, final datasets had over 15000 training images and labels. However, with CPU only 32-GB memory laptop, it is impossible to feed such large datasets into the Mask-RCNN model which requires huge memory for the training.The training strategy is to see how the proof of concept will work, so I gradually increased the datasets to feed into CNN with a trial of 3800 datasets.I used the impressive open-source implementation Mask-RCNN library that MatterPort built on Github here to train the model.Mask-RCNN efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition[5]. You can read the research paper to better understand the model. (fig. 3).There are three main functions need to be modified in the class (Utils.dataset) to load your own datasets into the framework. See below for the data loading implementation. The anchor's ratio is setup (16,32,64,128,256) to predict a smaller size of residential buildings. IMAGES_PER_GPU set to =1 so CPU can be used to train the model (fig. 4). An example of an image and mask (fig. 5).Here, the transfer learning technique was applied with model backbone ResNet-101. I trained the last fully connected layers first with epoch =5 to adapt the residential building class, then trained the full network for 35 epochs.At a 32-GB CPU, it took nearly 48 hours to finish the training process (fig. 6 and fig.7).Here are two inferences that original images were not used in training (fig.8 and fig.9). Interestingly to see the inference mask is more accurately delineating the building than original mask.Another interesting example of one image that was not used in training and mask inference result (fig.10 and fig.11).2. Training with Deeplabv3+ modelDeeplabv3+ is the latest state-of-art semantic image segmentation model developed by google research team. The distinctive of this model is to employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous Rates (fig.13).With the same training datasets from ArcGIS export training data for a deep learning tool, the images and masks were processed with augmentations and saved to a HDF5 compressed file for conveniently loading to training model (fig.14).I used Keras implementation of Deeplabv3+ on Github here. Below is the Keras training model with backbone Mobilenetv2 which has fewer parameters than the Xception model (fig.15).With only 5 epoch training runs, the result turns out very promising(fig.16).I run Python scripts for inference of an arbitrary cropped 2064x1463 dimension image that crop and process 16 (512x512x3 dim) images to obtain the inference rasters. (fig.19). With further inspection of the images and inference, we can see the effect of the building shadow can lower the accuracy of the edge of the buildings.With the same trained model to predict the 2019 same area cropped the image, the result is very similar with only minor localized differences(fig.20). The model can really help in future year image inference.After adding the above image and inference to the ArcGIS Pro. (fig.21)The above image raster was converted to the polygon feature and then use Regularize Building Footprint in ArcGIS Pro 3D analysis with appropriate parameters to regularize raw detection. (fig.22)Then I run python inference scripts with two complete 2018 tile images of 20,000 x 20,000 dimension about 3-mile away from each other. The scripts crop and process 1600 (512x512x3 dim) images for the inference. It took approximately one hour to finish each tile using 32GB RAM of CPU only laptop. see (fig. 23). There are missed classified buildings mostly because of using very small training dataset and trees covering on top of the buildings. Choosing several representative tiles as training dataset from different locations of the County could improve the accuracy of the result.Conclusion:Although it is a relatively small dataset, Mask-RCNN, and Deeplabv3+ deep learning models both present promising results for super-high-resolution image segmentation using transferred learning techniques. Due to the less accuracy of original building footprints ground truth feature polygons and the laptop CPU and memory limitation, the result of the performance may not surpass human digitizer in some image classifications and instance segmentation. However, the accuracy of this Deep learning training process can be further enhanced by increasing high-quality training datasets from different locations of the county and applying data variation augmentation methods. The model can be used in multi-year imagery to infer feature detection for comparison or even used for low-cost feature delineation with ArcGIS tools ModelBuilder to automate the business tasks. More importantly, the above deep learning training process can be applied to other types of image instances or segmentation cases. (please see my next blog)1.Reinhard Klette (2014). Concise Computer Vision. Springer. ISBN 978–1–4471–6320–6.2.Linda G. Shapiro; George C. Stockman (2001). Computer Vision. Prentice Hall. ISBN 978–0–13–030796–5.3.Tim Morris (2004). Computer Vision and Image Processing. Palgrave Macmillan. ISBN 978–0–333–99451–1.4.Bernd Jähne; Horst Haußecker (2000). Computer Vision and Applications, A Guide for Students and Practitioners. Academic Press. ISBN 978–013085198–7.5. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick.(2018). Mask-RCNN, https://arxiv.org/abs/1703.06870v36. Deeplabv3+ model, https://github.com/tensorflow/models/tree/master/research/deeplab7. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html8. http://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deep-learning.htm9. https://pro.arcgis.com/en/pro-app/tool-reference/3d-analyst/regularize-building-footprint.htm10.https://blogs.technet.microsoft.com/machinelearning/2018/03/12/pixel-level-land-cover-classification-using-the-geo-ai-data-science-virtual-machine-and-batch-ai/11. U-Net: Convolutional Networks for Biomedical Image segmentation:https//lmb.informatik.uni-freiburg.de/people/ronneber/u-net/",10/12/2019,0,7.0,43.0,942.0,544.0,26.0,0.0,0.0,33.0,en
3876,Everything You Need to Know about Logistic Regression,Medium,Uday Paila,12.0,7.0,997.0,"In this article, I will discuss all cases of Logistic Regression that are useful while applying.Let’s take x as an input feature vector, and y is a class (-1 or +1), then the probability of class given input vector represented by the below formula.and log loss isLoss with regularization for optimization isN is the number of data points we have, C is hyperparameter to control regularization. Above I added l2 regularization notation. x_i is the data points features and y_i(+1 or -1) is the label we have for x_i. This formulation works only for Binary classification.Also, we can represent the Loss function as below, and that works for multiclass formulation as well.Let’s take we have K classes and N number of data points. Now the Log loss function is represented as,Now loss with regularization is,1(y_i=j) = 1 if y_i=j else 0. so Here, y_i takes (0 or 1) not (-1 or +1).For the Binary case, we can calculate the probability of the equation above as P(y/x). For multiclass, we have to calculate the probability of y belongs to class c given input x using softmax function as below.Here, W_c is the weight vector for each class C. In the multiclass classification, we will get Weight vectors for each class. The above loss function for multiclass classification is Multinomial Log loss.If you want to know more about the math behind Logistic regression, please read this.sklearn.linear_model.LogisticRegression —Based on the solver, the optimization algorithm used will be changed. We can do multiclass classification by changing the multi_class parameter. It takes OvR or multinomial. Please check the below image to know about all solvers.Main Hyperparameters :C → C is a multiplicative constant to loss term. While optimization we have to find the minimum value of the loss(Log loss+regularization). If we increase the C value, Log loss has to decrease so that final loss will decrease, i.e log loss will decrease if C increases, so it may be overly certain about training data, so it overfits.sklearn.linear_model.SGDClassifier —We can use SGDClassifier with loss = ‘log’ to get logistic regression optimization. It uses the stochastic gradient descent algorithm as an optimization algorithm, and minimizes binary log loss so for multiclass classification, it uses only the OneVsRest algorithm.SGD optimization loss looks like belowMain Hyperparameters:alpha → Alpha increases, regularization strength increases, so it underfits.learning_rate → Learning rate required for the one-step of SGDTraining:O(mNd) for binary classification with SGD, where m is the number of epochs, N is the number of data points, d is the dimensionality of data, i.e number of features.O(KmNd) for multiclass classification if we are using One-vs-Rest. Here, K is the number of classes.Inference/Testing:O(d) for a binary classification.O(Kd) for a Multiclass classification with One-vs-Rest/Multinomial.Once we have done with training, we will get the Weight vectors and for classification, we will calculate the probability as discussed in loss functions. Based on that probability, we can classify the query point.To get the probability, we have to multiply W and features. If we get W as a sparse matrix, i.e if W contains zero values for some of the features, we can reduce that multiplication time so that time to classification will be reduced. We can get this using regularization. If we decrease the C value with L1 regularization, we can achieve this, but it may reduce the classification score as well.SGD is a batchwise optimization algorithm only, so we can load data batch-wise and train our SGDClassifier in the Sk-Learn package. In Sk-Learn implementation, we can get the batchwise data, and use the partial_fit method to train the algorithm on that batch.Sk-Learn uses the class_weight parameter to give weight to the loss of a particular class, so we can increase the weight of the imbalance class.Observation Independence → Observations should not come from repeated measurements.None or Little Multicollinearity → Multicollinearity is a phenomenon in which one feature can be linearly predicted from the other features.The linearity of independent variables and Log odds → It requires features that are linearly related to the log-odds/logits, i.e ln(P/(1-P)). it doesn’t mean a linear relationship between features and labels.Assumption of Homoscedasticity is not required In Logistic regressionPerturbation Test → Once after training we will get the weight vector. Now add some random noise to the data, and train the same model again and get the weight vector. If there is a significant change in the weights, we can say that multicollinearity may exist. Check this by re-running the same model 2–3 times.Variance Inflation Factor → VIFs start at 1 and have no upper limit. A value of 1 indicates that there is no correlation between this feature variable and any others. VIFs between 1 and 5 suggest that there is a moderate correlation, but it is not severe enough to warrant corrective measures. VIFs greater than 5 represent critical levels of multicollinearity where the coefficients are poorly estimated. You can calculate VIF using statsmodels.stats outliers_influence. variance_inflation_factor. You can go through these code snippets.Remove the features which are collinear except one or create some interaction features of those collinear features.Logistic Regression gives linear separable hyperplane/boundary, so if our data is non-linearly separable, Logistic Regression won’t work efficiently, so we can create some interaction variables or based on our features we can generate some other features to make our data linear. This involves trial-and-error, and creative thinking to generate new features.Yes, we can, but only if we optimize using SGDClassifier because it takes batchwise data, and optimizes.With Sk-Learn 0.22.1, you can only parallelize for multi-class classification with One-vs-Rest by mentioning n_jobs.If we need parallel training, you can use Spark-ML’s logistic regression implementation or we can write Logistic Regression using Tensorflow/Keras, and train using data parallelization. Building a Logistic Regression in Tensorflow is very easy, it requires one linear layer and one sigmoid layer only, please check here for the implementation.Once after training, we will get weights corresponding to every feature we have in the data, so based on weight vector we can interpret which feature is important for the corresponding class.References:",06/02/2020,0,28.0,12.0,604.0,158.0,9.0,2.0,0.0,12.0,en
3877,Gradient descent algorithms and adaptive learning rate adjustment methods,Towards Data Science,Manish Chablani,1700.0,5.0,553.0,"Here is a quick concise summary for reference. For more detailed explanation please read: http://ruder.io/optimizing-gradient-descent/Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ for the entire training dataset.Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example x(i) and label y(i)Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples.Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [1], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image below. This helps accelerate SGD in the relevant direction and dampens oscillationsWhile Momentum first computes the current gradient (small blue vector in Image 4) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasksImage 4: Nesterov update (Source: G. Hinton’s lecture 6c)Above methods adapt updates to the slope of our error function and speed up SGD in turn. Adagrad adapts updates to each individual parameter to perform larger or smaller updates depending on their importance.we set g(t,i) to be the gradient of the objective function w.r.t. to the parameter θi at time step t:One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.Adagrad’s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.Adadelta [6] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size ww.RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad’s radically diminishing learning rates. RMSprop divides the learning rate by an exponentially decaying average of squared gradients.Adaptive Moment Estimation is most popular today.ADAM computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentumAdam can be viewed as a combination of RMSprop and momentum. We have also seen that Nesterov accelerated gradient (NAG) is superior to vanilla momentum. Nadam (Nesterov-accelerated Adaptive Moment Estimation) [24] thus combines Adam and NAG.Credits: Sebastian Ruder (2016). An overview of gradient descent optimisation algorithms. arXiv preprint arXiv:1609.04747.",14/07/2017,0,0.0,2.0,700.0,233.0,6.0,1.0,0.0,8.0,en
3878,The Measure of a Measure,Artificial Intelligence in Plain English,Charlie Kufs,267.0,6.0,815.0,"If you can measure a phenomenon, you can analyze the phenomenon. But if you don’t measure the phenomenon accurately and precisely, you won’t be able to analyze the phenomenon accurately and precisely. So in planning a statistical analysis, once you have specific concepts you want to explore you’ll need to identify ways the concepts could be measured.Start with conventional measures, the ones everyone would recognize and know what you did to determine. Then, consider whether there are any other ways to measure the concept directly. From there, establish whether there are any indirect measures or surrogates that could be used in lieu of a direct measurement. Finally, if there are no other options, explore whether it would be feasible to develop a new measure based on theory.Keep in mind that developing a new measure or a new scale of measurement is more difficult for the experimenter and the personnel who have to generate the data, and less understandable for reviewers than using an established measure.Say, for example, that you wanted to assess the taste of various sources of drinking water. You might use standard laboratory analysis procedures to test water samples for specific ions known to affect taste, like iron and sulfate. These would be direct measures of water quality. An example of an indirect measure would be total dissolved solids, a general measure of water quality that includes many dissolved ions besides iron and sulfate. An example of a surrogate measure would be the water’s electrical conductivity, which is positively correlated to the quantity of dissolved ions in the water. Electrical conductivity is easier and less expensive to measure than dissolved solids, which is easier and less expensive to measure than specific analytes like iron and sulfate.Developing a new measure based on theory might also be useful. Sometimes it’s beneficial to think out of the box. That’s how sabermetrics got started. So for example, you might use professional taste testers to judge the tastes of the waters. Or, you might conduct comparison surveys of untrained individuals. Clearly, what you measure and how you measure it will have a great influence on your findings.Of the possible measures you identify, select scales-of-measurement and consider how difficult it would be to generate accurate and precise data. Measurement bias and variability are introduced into a data value by the very process of generating the data value. It’s like tuning an analog radio. Turn the tuning dial a bit off the station and you hear more static. That’s more variance in the station’s signal. Every measurement can be thought of consisting of three elements:Consider the examples of data types shown in the following table. For any particular data type, all three of these elements change over time. Benchmarks change when new measurement technologies are developed or existing meters, gauges and other devices become more accurate and precise. Standardized tests, like the SAT, change to safeguard the secrecy of questions. Likewise, processes change over time to improve consistency and to accommodate new benchmarks. Judgments improve when data collectors are trained and gain work experience. Such changes can create problems when historical and current data are combined because variance differences attributable to evolving measurement systems can produce misleading statistics.Understanding these three facets of measurements is important because it will help you select good measures and measurement scales for a phenomenon, as well as decide how to control extraneous variability in data collection. For example:There’s a special type of analysis aimed at evaluating measurement variance called Gage R&R. The R&R part refers to:Gage R&R is a fundamental type of analysis in industrial statistics, where meeting product specifications requires consistent measurements, but it can be used for any measurement system from medical testing to opinion surveys.Consider building some redundancy into your variables if there is more than one way to measure a concept. Sometimes one variable will display a higher correlation with your model’s dependant variable or help explain analogous measurements in a related measure. For example, redundant measures are often included in opinion surveys by using differently worded questions to solicit the same information. One question might ask “Did you like [something]?” and then a later question might ask “ Would you recommend [something] to your friends?” or “ Would you use [something] again in the future?” to assess consistency in a respondent’s opinion about a product.Finally, take into account your objective and the ultimate use of your statistical models. For example, if you want to predict some dependent variable, quantitative independent-variables would usually be preferable to qualitative-variables because they would provide more scale resolution. Furthermore, you could dumb-down a quantitative variable to a less finely divided scale or even a qualitative scale but you usually can’t go in the other direction. If you want your prediction model to be simple and inexpensive to use, don’t select predictors that are expensive and time-consuming to measure.Originally published at http://statswithcats.net on September 12, 2010.",12/09/2010,0,18.0,15.0,822.0,712.0,7.0,3.0,0.0,6.0,en
3879,DBSCAN Parameter Estimation Using Python,Medium,Tara Mullin,33.0,3.0,446.0,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an unsupervised machine learning technique used to identify clusters of varying shape in a data set (Ester et al. 1996). Another post I wrote goes into what DBSCAN is and when to use it. You can find it here. This post will focus on estimating DBSCAN’s two parameters:There is no automatic way to determine the MinPts value for DBSCAN. Ultimately, the MinPts value should be set using domain knowledge and familiarity with the data set. From some research I’ve done, here are a few rules of thumb for selecting the MinPts value:After you select your MinPts value, you can move on to determining ε. One technique to automatically determine the optimal ε value is described in this paper. This technique calculates the average distance between each point and its k nearest neighbors, where k = the MinPts value you selected. The average k-distances are then plotted in ascending order on a k-distance graph. You’ll find the optimal value for ε at the point of maximum curvature (i.e. where the graph has the greatest slope).Recently, I worked with a data set with 10 dimensions. Following the rules above, my minimum samples value should be at least 10. Ultimately, I decided to go with 20 (2*dim), as suggested in papers 1 and 2 listed under resources at the end of this post.After I selected my MinPts value, I used NearestNeighbors from Scikit-learn, documentation here, to calculate the average distance between each point and its n_neighbors. The one parameter you need to define is n_neighbors, which in this case is the value you choose for MinPts.Step 1: Import LibrariesStep 2: Calculate the average distance between each point in the data set and its 20 nearest neighbors (my selected MinPts value).Step 3: Sort distance values by ascending value and plotFor my data set, the sorted distances produces a k-distance elbow plot that looks like this:The ideal value for ε will be equal to the distance value at the “crook of the elbow”, or the point of maximum curvature. This point represents the optimization point where diminishing returns are no longer worth the additional cost. This concept of diminishing returns applies here because while increasing the number of clusters will always improve the fit of the model, it also increases the risk that overfitting will occur.Zooming in on my k-distance plot, it looks like the optimal value for ε is around 225. I ended up looping through combinations of MinPts and ε values slightly above and below the values estimated here to find the model of best fit.Resources that helped me estimate the parameters for my DBSCAN model and write this post:",10/07/2020,3,1.0,0.0,404.0,267.0,2.0,3.0,0.0,7.0,en
3880,U-Net for Semantic Segmentation on Unbalanced Aerial Imagery,Towards Data Science,Amirhossein Heydarian,38.0,5.0,729.0,"In this article, we review the problem of semantic segmentation on unbalanced binary masks. Focal loss and mIoU are introduced as loss functions to tune the network parameters. Finally, we train the U-Net implemented in PyTorch to perform semantic segmentation on aerial images. The training codes and PyTorch implementations are available through Github.The dataset used here is “Semantic segmentation of aerial imagery” which contains 72 satellite images of Dubai, the UAE, and is segmented into 6 classes. The classes include water, land, road, building, vegetation, and unlabeled.U-Net is a convolutional neural network that originally was presented for biomedical image segmentation at the Computer Science Department of the University of Freiburg. It is based on fully convolutional neural networks and has a modified and extended architecture to work with fewer training images and yield more precise segmentation.The primary concept is to use a contracting network followed by an expanding network, with upsampling operators in the expansive network replacing pooling operations. These layers increase the resolution of the output. Besides, an expansive convolutional network can learn to assemble a precise output based on encoded information.The network consists of a contracting path (left side) and an expansive path (right side), which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated convolutions, each followed by a rectified linear unit (ReLU) and a max-pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the features and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path.The purpose of this article is to review the effect of loss function on segmentation output results. Three different loss functions are used in the training procedure. But first, let us have a quick review of them.Let's assume p as the output value for each pixel in the image. In this case, we can define the studied loss functions as bellow:Cross-entropy (or log loss) calculates the logarithm value of the output and because we’re talking about images, it is the logarithm value of every pixel in the output tensor.The alpha term is a weight hyperparameter for different classes and is a way to balance the loss for unbalanced classes. The final equation for weighted cross-entropy loss is shown in Eq 1.Focal Loss presents a better solution to the unbalanced dataset problem. It adds an extra term to reduce the impact of correct predictions and focus on incorrect examples. The gamma is a hyperparameter that specifies how powerful this reduction will be.This loss influences the training of a network on the unbalanced dataset and can improve segmentation results.And finally, we get to IoU loss which is another option for unbalanced segmentation and has fewer hyperparameters than the others. But first, let's get familiar with this metric in Eq 3.In the equation, the nominator is the overlap between the predicted and ground-truth masks, and the denominator is the union of them. The IoU is calculated by dividing these two numbers, with values closer to one indicating more accurate predictions.The purpose of optimization is to maximize the IoU and, it has a value between 0 and 1, so the loss function is defined as:I trained the U-Net with all three loss functions on the mentioned dataset. It’s important to note that there were only 65 images for training and 7 for validation, so we can’t expect great results. But this number of data is enough for our purpose.As you can see, cross-entropy has a problem segmenting small areas and has the worst performance among these loss functions.Focal loss can achieve better results, especially in small regions, but it still needs some hyperparameter tuning through trial and error.Finally, we can see that IoU loss also does a great job in segmentation, both for small and large areas.Here you can see some other outputs:In this article, we reviewed the effect of loss function for segmentation on unbalanced images. We trained U-Net neural network to perform semantic segmentation aerial images using 3 different loss functions, cross-entropy loss, focal loss, and IoU loss.The results demonstrate that cross-entropy loss cannot handle unbalanced datasets. Even adding weight for different classes is not very effective. On the other hand, focal loss and IoU loss both represent better results for unbalanced image segmentation.You can also refer to the GitHub page to access the project and PyTorch implementations.",03/10/2021,0,4.0,0.0,602.0,279.0,11.0,0.0,0.0,7.0,en
3881,Dimensionality Reduction by Stochastic Gradient Descent,Analytics Vidhya,Learner Subodh,2.0,8.0,1598.0,"This post demonstrates the use of Stochastic Gradient Descent for Dimensionality Reduction.What is Dimensionality Reduction?Dimensionality reduction is the process of reducing a potentially large set of features F to a smaller set of features F’ to be considered in a given machine learning or statistics problem.In an unsupervised setting, dimensionality reduction is often used for exploratory data analysis, for example to visualize the distribution of high dimensional data in human-digestible two or three dimensions. In a supervised setting, the main use is to reduce the number of parameters a learning machine has to determine. In other words: The goal of dimensionality reduction is to overcome the curse of dimensionality.A straightforward approach, feature selection, is to select only certain features, i.e., to choose a selection F’ subset of F, according to some optimality criterion, e.g., related to the information content of the features in F’ in regard to the data. More general approaches find F’ by mapping the feature space Fs that is defined by F into a lower-dimensional space Fs’ corresponding to F’.Here, we will focus on the latter category in a supervised setting. In particular, we present a method to learn a linear mapping that approximately maximizes the distance between classes.What are the Benefits of applying Dimensionality Reduction on our Dataset?Related WorkArguably the two most well known dimensionality reduction techniques are the unsupervised principal component analysis (PCA) and Fisher’s supervised linear discriminant analysis (LDA).Briefly, PCA finds a linear mapping so that the squared projection error is minimized. The effect is that most of the data’s variance in the original feature space is also represented in the reduced space Fs’. While PCA does preserve variance, it does not necessarily preserve class separability in the projected space.LDA, on the other hand, finds a projection so that the scatter between samples of different classes is maximized while the scatter between samples of the same class is minimized. The projection does not necessarily represent the high dimensional variation of the data well, but it does preserve information necessary to separate the classes. However, LDA makes the implicit assumption that the data of a given class are normally distributed. Coupled with the restriction that the dimensionality of Fs’ is one less than the number of classes C, dim(Fs’) = C − 1, this means that too much information may be discarded and the classes are not separable in Fs’.There are also nonlinear methods that project the data onto a low dimensional manifold embedded in F. Examples of such methods include multidimensional scaling, Isomap, local Fisher discriminant analysis and t-distributed stochastic neighbor embedding.Dimensionality reduction also has close ties to metric learning, where the goal is to learn a function that realizes a distance from a given dataset. The metric is then to be used in a subsequent distance-based classifier like nearest centroid or k-nearest neighbor. Examples of metric learning methods include large margin nearest neighbor and information-theoric metric learning.Some other popular Dimensionality Reduction Techniquest-distributed Stochastic Neighborhood EmbeddingNeural AutoencoderForward Feature SelectionBackward Feature EliminationRatio of missing valuesLow variance in column valuesHigh correlation between two columnsMethodsOur method can be formalized as follows: Given a set of n-dimensional training data D subset of R^n and a partition of this set into two disjoint classes X and Y, the goal is to find a linear projection x’ = Ax into an m-dimensional space, where m << n, so that class separation is best preserved in the lower dimensional space. The method can also be easily extended to more than two classes.In other words, the goal is to find a matrix A belongs to R^(m×n) so that the squared Euclidean distance of two reduced feature vectors,is large if x and y belong to different classes, and small otherwise. Note that this goal is very similar to that of metric learning. However, here we are explicitly interested in the dimensionality reduction so that subsequent learning algorithms do not have to be distance-based. One could use the method large margin nearest neighbor or information-theoric metric learning and decompose the matrix M, but there is no guarantee that the resulting dimensionality reduction is optimal in the sense of our goal.We instead take inspiration from LDA and find the projection matrix A = argmin(A) J(A) that minimizes the distance between feature vectors of the same class while simultaneously maximizing the distance between samples of different classes, measured by the unweighted ratio between intra-class and inter-class distance (like LDA),Unfortunately, a closed form solution of the above minimization problem is not readily available. Therefore, we choose to iteratively optimize it instead. In particular, we employ the well known method of gradient descent to find a (local) minimum,where At and At denote the solution candidate and update at time step t, Gt := del(A)J(At) denotes the gradient of the target function with respect to A, and the hyper-parameter nt is the learning rate at step t.Stochastic Gradient DescentStochastic gradient descent (SGD) has become a popular tool to speed up the learning process of deep neural networks. The main insight of SGD is that, due to the linearity of the differential operator del(A), the gradient of an objective function that sums over several sub-goals, J(A)=summation(i=1 to N)Ji(A), can be written asIn empirical risk minimization, one of the main pillars in machine learning, the objective function typically takes this form, where each sub-goal corresponds to minimizing the loss on one single training example.SGD utilizes this structure by approximating the gradient with a random subsample of the training data at each step of the gradient descent,where K << N is the number of samples to consider each step and sigma-t is a random permutation of the set {1, . . . ,N}. The intuition is that, while the individual approximations of the gradient do not point in the same direction as the true gradient, the approximation error averages out when a large number of iterations are performed. SGD requires more iterations to converge than gradient descent, but each iteration is much less computationally expensive. Thus, the overall convergence speed is generally faster, especially with large datasets or when the Ji are expensive to compute.Unfortunately, the objective function in equation 1 does not have the required structure. Both the nominator and denominator, however, do. This suggests that the general idea is applicable regardless: At each step of the gradient descent, we sample a small subset of the data, so that an equal number of samples is drawn from both X and Y. The gradient Gt is approximated using only these samples.AdaGradStochastic gradient descent, like regular gradient descent, is very susceptible to the choice of learning rate. If the learning rate is too large, then good solutions may be missed; if it is too small, and the algorithm will converge very slowly. Ideally, the learning rate should be large in the beginning, but small when the intermediate solution is near an optimum in order not to oscillate around it.AdaGrad defines a learning rate schedule that takes previous parameter updates into account. The learning rate for the (ik)-th component of the gradient is chosen aswhere n is a global learning rate, r(tau) denotes the time steps before t, and g, ik denotes the (ik)-th component of the historical gradient G. The intuition is that larger updates will result in a smaller learning rates in the future, which effectively smoothes out extreme parameter updates due to locally steep gradients.Much like with the momentum method or the Newton-Raphson method, each parameter aik is associated with a different learning rate schedule, which means that the progress along each dimension of the parameter space evens out over time. A drawback is that, depending on the global learning rate, the learning rates eventually approach zero and the optimization may stop before a minimum is reached.ConclusionWe have presented a method for linear dimensionality reduction. The method takes inspiration from both PCA and Fisher LDA and tries to minimize the pairwise distances between samples of the same class while simultaneously maximizing the distance between samples of different classes. Instead of providing a closed form solution, we chose to employ the well known gradient descent optimization algorithm. In order to speed up computation, we used stochastic gradient descent with an AdaGrad learning rate schedule.The biggest issue with our approach is the bias to effectively reduce the features to a one-dimensional feature space. We can plan to tackle this issue by re-weighting the terms of the objective function and introducing non-linearities in the projection. As this will result in a severely more complicated gradient, we will re-evaluate usage of automatic differentiation tools.Another interesting research question concerns the use of step-wise dimensionality reduction: Instead of a direct reduction from n to m dimensions, the algorithm would first reduce the n features to n − 1 features, then the intermediate n − 1 features to n − 2 features, etc., until the desired number of features is reached. The hope is that each of the intermediate reductions is easier to solve well than the full dimensionality reduction — especially if non-linearities are introduced as well.Thank you!ReferencesJ. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.Kilian Q Weinberger, John Blitzer, and Lawrence K Saul. Distance metric learning for large margin nearest neighbor classification. In Advances in neural information processing systems, pages 1473–1480, 2005.Guided Linear Dimensionality Reduction by Stochastic Gradient Descent, Matthias Richter.Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000.If you like this article, don’t forget to leave a “clap”!Thank you for your time.www.linkedin.comgithub.com",27/08/2020,0,10.0,14.0,448.0,184.0,7.0,1.0,0.0,4.0,en
3882,How to Fine-Tune BERT Transformer with spaCy 3,Towards Data Science,Walid Amamou,289.0,7.0,760.0,"Since the seminal paper “Attention is all you need” of Vaswani et al, Transformer models have become by far the state of the art in NLP technology. With applications ranging from NER, Text Classification, Question Answering or text generation, the applications of this amazing technology are limitless.More specifically, BERT — which stands for Bidirectional Encoder Representations from Transformers— leverages the transformer architecture in a novel way. For example, BERT analyses both sides of the sentence with a randomly masked word to make a prediction. In addition to predicting the masked token, BERT predicts the sequence of the sentences by adding a classification token [CLS] at the beginning of the first sentence and tries to predict if the second sentence follows the first one by adding a separation token[SEP] between the two sentences.In this tutorial, I will show you how to fine-tune a BERT model to predict entities such as skills, diploma, diploma major and experience in software job descriptions. If you are interested to go a step further and extract relations between entities, please read our article on how to perform joint entities and relation extraction using transformers.Fine tuning transformers requires a powerful GPU with parallel processing. For this we use Google Colab since it provides freely available servers with GPUs.For this tutorial, we will use the newly released spaCy 3 library to fine tune our transformer. Below is a step-by-step guide on how to fine-tune the BERT model on spaCy 3 (video tutorial here). The code along with the necessary files are available in the Github repo.To fine-tune BERT using spaCy 3, we need to provide training and dev data in the spaCy 3 JSON format (see here) which will be then converted to a .spacy binary file. We will provide the data in IOB format contained in a TSV file then convert to spaCy JSON format.I have only labeled 120 job descriptions with entities such as skills, diploma, diploma major, and experience for the training dataset and about 70 job descriptions for the dev dataset.In this tutorial, I used the UBIAI annotation tool because it comes with extensive features such as:Using the regular expression feature in UBIAI, I have pre-annotated all the experience mentions that follows the pattern “\d.*\+.*” such as “5 + years of experience in C++”. I then uploaded a csv dictionary containing all the software languages and assigned the entity skills. The pre-annotation saves a lot of time and will help you minimize manual annotation.For more information about UBIAI annotation tool, please visit the documentation page and my previous post “Introducing UBIAI: Easy-to-Use Text Annotation for NLP Applications”.The exported annotation will look like this:In order to convert from IOB to JSON (see documentation here), we use spaCy 3 command:After conversion to spaCy 3 JSON, we need to convert both the training and dev JSON files to .spacy binary file using this command (update the file path with your own):To check the correct cuda compiler is installed, run: !nvcc --versionThe only thing we need to do is to fill out the path for the train and dev .spacy files. Once done, we upload the file to Google Colab.I suggest to debug your config file in case there is an error:P.S: if you get the error cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_INVALID_PTX: a PTX JIT compilation failed, just uninstall cupy and install it again and it should fix the issue.If everything went correctly, you should start seeing the model scores and losses being updated:At the end of the training, the model will be saved under folder model-best. The model scores are located in meta.json file inside the model-best folder:The scores are certainly well below a production model level because of the limited training dataset, but it’s worth checking its performance on a sample job description.To test the model on a sample text, we need to load the model and run it on our text:Below are the entities extracted from our sample job description:Pretty impressive for only using 120 training documents! We were able to extract most of the skills, diploma, diploma major, and experience correctly.With more training data, the model would certainly improve further and yield higher scores.With only a few lines of code, we have successfully trained a functional NER transformer model thanks to the amazing spaCy 3 library. Go ahead and try it out on your use case and please share your results. Note, you can use UBIAI annotation tool to label your data, we offer free 14 days trial.As always, if you have any comment, please leave a note below or email at admin@ubiai.tools!Follow us on Twitter @UBIAI5",01/03/2021,13,1.0,4.0,1059.0,639.0,5.0,9.0,0.0,20.0,en
3883,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,Emergent // Future,Arthur Juliani,12000.0,6.0,1184.0,"For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1–3). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:Eq 1. Q(s,a) = r + γ(max(Q(s’,a’))This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:(Thanks to Praneet D for finding the optimal hyperparameters for this approach)Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.Eq2. Loss = ∑(Q-target - Q)²Below is the Tensorflow walkthrough of implementing our simple Q-Network:While the network learns to solve the FrozenLake problem, it turns out it doesn’t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!If you’d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @More from my Simple Reinforcement Learning with Tensorflow series:",26/08/2016,0,3.0,16.0,954.0,467.0,2.0,1.0,0.0,19.0,en
3884,SGD: MNIST — Putting it all together,unpackAI,Becky Zhu,6.0,3.0,432.0,"There are 7 steps to train/get a model in deep learning like this chart:We now put it with the SGD together and look at them step by step:Step 1: InitializeIn this step, we initialize our parameters with random values and tell PyTorch that we want to track their gradients:We will do the following things in this step:Step 2: PredictIn this step，we will calculate the predictions to see how close our predictions to our targets. The code will like thispreds = f(time, params)Step 3: Calculate the LossIn this step, can change the weight by a little in the direction of the slope, calculate the loss and adjustment again, and repeat this a few times. We will get to the lowest point on the curve. We can use “mse”or “l1”to calculate.“mse”stands for *mean squared error*, and “l1” refers to the standard mathematical jargon for *mean absolute value* (in math it’s called the *L1 norm*).The code will like this:loss = mse(preds, speed)Step 4: Calculate the gradientsIn this step, we find the weights to make the loss and calculate the gradients. We can by use loss.backward module. The code will like this:loss.backward()Step 5: Step the weightsIn this step, we will update the weights and biases based on the gradient and learning rate. We put that all in a function and test it. The code will like thisdef apply_step(params, prn=True):preds = f(time, params)loss = mse(preds, speed)loss.backward()params.data -= lr * params.grad.dataparams.grad = Noneif prn: print(loss.item())return predsWe look at the accuracy to see whether it is improved.Step 6: Repeat the process — — OptimizerIn this step, we loop and perform many improvements to reach a good result of accuracy. We’ve created a general-purpose foundation we can build on. Then we will be to create an object that will handle the SGD step for us to repeat the process. We can create optimizer and learner to replace our linear model with a neural network. In PyTorch we can use nn.Linear and learner module.We pass in all the elements: the `DataLoaders`, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print to create a `Learner` and use fit（）function to view the results.Step 7: STOPIn this step, we will decide where to stop and get our “best”model. We would keep training until the accuracy of the model started getting worse, or we ran out of time. We will watch the training and validation losses and our metrics to decide when to stop.ConclusionBy these steps, we will train our deep learning model with SGD easier, since there are convenient pre-packaged pieces in fastai",06/06/2021,0,9.0,3.0,722.0,118.0,1.0,1.0,0.0,0.0,en
3885,Transformer Neural Network: Step-By-Step Breakdown of the Beast,Towards Data Science,Utkarsh Ankit,142.0,13.0,2351.0,"The Transformer Neural Network is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It was proposed in the paper “Attention Is All You Need” 2017 [1]. It is the current state-of-the-art technique in the field of NLP.Before directly jumping to Transformer, I will take some time to explain the reason why we use it and from where it comes into the picture. (If you want to skip this part then directly go to the Transformer topic, but I suggest you read it sequentially for better understanding).So, the story starts with RNN (Recurrent Neural Networks).What is RNN? How is it different from simple ANN? What is the major difference?RNNs are the Feed Forward Neural Networks that are rolled out over time.Unlike normal Neural Networks, RNNs are designed to take a series of inputs with no predetermined limit on size. “Series” as in any input of that sequence has some relationship with their neighbour’s or have some influence on them.Basic feedforward networks “remember” things too, but they remember the things they learnt during training. While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s).It is used in different types of models-1. ) Vector-Sequence Models- They take fixed-sized vectors as input and output vectors of any length, for example, in image captioning, the image is given as an input and the output describes the image.2. ) Sequence-Vector Model- Take a vector of any size and output a vector of fixed size. Eg. Sentiment analysis of a movie rates the review of any movie as positive or negative as a fixed size vector.3. ) Sequence-to-Sequence Model- The most popular and most used variant, take input as a sequence and give output as another sequence with variant sizes. Eg. Language translation, for time series data for stock market prediction.Its disadvantages-For Eg. “The clouds are in the ____.”It is obvious that the next word will be sky, as it is linked with the clouds. Here we see the distance between clouds and the predicted word is less so RNN can predict it easily.But, for another example,“I grew up in Germany with my parents, I spent many years and have proper knowledge about their culture, that’s why I speak fluent ____.”Here the predicted word is German, which is directly connected with Germany, but the distance between Germany and the predicted word is more in this case, so it is difficult for RNN to predict.So, Unfortunately, as that gap grows, RNNs become unable to connect, as their memory fades with distance.Long Short Term Memory- Special kind of RNN, specially made for solving vanishing gradient problems. They are capable of learning Long-Term Dependencies. Remembering information for long periods of time is practically their default behaviour, not something they struggle to learn!The LSTM Neurons have unlike normal neurons have a branch that allows to pass information and to skip the long processing of the current cell, this allows the memory to be retained for a longer period of time. It does improve the situation of the vanishing gradient problem but not that amazingly, like it will do good till 100 words, but for like 1,000 words, it starts to lose its grip.But like simple RNN it is also very slow to train, or even slower.They take input sequentially one by one, which is not able to use up GPU’s very well, which are designed for parallel computation.How can we parallelize sequential data?? (I will get back on this question.)For now, we are dealing with two issues-Solving the vanishing gradient issue:It answers the question of what part of the input we should focus on.I am going to explain attention in a slightly different way. Let’s take a situation-Suppose someone gave us a book of machine learning and asked us to give information about categorical cross-entropy. There are two ways of doing it, first, read the whole book and come back with the answer. Second, go to the index, find the ‘losses’ chapter, go to the cross-entropy part and read the part of Categorical Cross Entropy.What do you think is the faster method?Like in the first method, it may take a whole week to read the entire book. While in second, it will hardly take 5 mins. Furthermore, our information from the first method will be more vague and diverse as it is based on too much information, while the information from the second one will be accurate to the requirement.What did we do differently here?In the former case we didn’t focus on any part of the book specifically, whereas in the latter case, we focused our attention on the chapter of losses and then further focused our attention on the cross-entropy part where the concept of Categorical Cross Entropy is explained. Actually, this is the way most of us humans will do.Attention in neural networks is somewhat similar to what we find in humans. They focus on the high resolution in certain parts of the inputs while the rest of the input is in low resolution [2].Let’s say we are making an NMT(Neural Machine Translator),Check out this animation, this shows how a simple seq-to-seq model works.We see that for each step of the encoder or decoder, RNN is processing its inputs and generating output for that time step. In each time step, RNN updates its hidden state based on inputs and previous outputs it has seen. In the animation, we see that the hidden state is actually the context vector we pass along to the decoder.Time for “Attention”.The context vector turned out to be problematic for these types of models. Models have a problem while dealing with long sentences. Or say they were facing the vanishing gradient problem in long sentences. So, a solution came along in a paper [2], Attention was introduced. It highly improved the quality of machine translation as it allows the model to focus on the relevant part of the input sequence as needed.This attention model is different from the classic seq-to-seq model in two ways-The last step of decoders proceed as follow-This scoring exercise is done at each time step on the decoder side.Now when we bring the whole thing together:So, this is how Attention works.Eg. Working of Attention In an Image Captioning Problem:-Now, remember the question which I stated earlier-How can we parallelize sequential data??So, here comes our ammunition-A paper called “Attention Is All You Need” published in 2017 comes into the picture, it introduces an encoder-decoder architecture based on attention layers, termed as the transformer.One main difference is that the input sequence can be passed parallelly so that GPU can be utilized effectively, and the speed of training can also be increased. And it is based on the multi-headed attention layer, vanishing gradient issue is also overcome by a large margin. The paper is based on the application of transformer on NMT(Neural Machine Translator).So, here both of our problems which we highlighted before are solved to some level here.Like for example in a translator made up of simple RNN we input our sequence or the sentence in a continuous manner, one word at a time to generate word embeddings. As every word depends on the previous word, its hidden state acts accordingly, so it is necessary to give one step at a time. While in transformer, it is not like that, we can pass all the words of a sentence simultaneously and determine the word embedding simultaneously. So, how it is actually working, let’s see ahead-1. Encoder Block -It’s a fact that computers don’t understand words, it works on numbers, vectors or matrices. So, we do need to convert our words to a vector. But how this can be possible. So, here the concept of Embedding Space comes. It’s like an open space or dictionary where words of similar meanings are grouped together or are present close to each other in that space. This space is termed as embedding space, and here every word, according to its meaning, is mapped and assigned with a particular value. So, here we convert our words to vectors.But one other issue we will face is that every word in different sentences has different meanings. So, to solve this issue, we take the help of Positional Encoders. It is a vector that gives context according to the position of the word in a sentence.Word → Embedding → Positional Embedding → Final Vector, termed as Context.So, our input is ready, now it goes to the encoder block.Multi-Head Attention Part -Now the main essence of the transformer comes in, “Self Attention”.It focuses on the part that how relevant a particular word is w.r.t to other words in that sentence. It is represented as an attention vector. For every word, we can have an attention vector generated, which captures the contextual relationship between words in that sentence.The only problem it faces is that for every word it weighs its value much higher on itself in the sentence, be we are inclined towards its interaction with other words of that sentence. So, we determine multiple attention vectors per word and take a weighted average, to compute the final attention vector of every word.As we are using multiple attention vectors, it is called the Multi-Head Attention Block.Feed Forward Network -Now, the second step is the Feed Forward Neural Network. This is the simple feed-forward Neural Network that is applied to every attention vector, it’s the main purpose is to transform the attention vectors into a form that is acceptable by the next encoder or decoder layer.Feed Forward Network accepts attention vectors “one at a time”. And the best thing here is unlike the case of RNN, here each of these attention vectors is independent of each other. So, parallelization can be applied here, and that makes all the difference.Now we can pass all the words at the same time into the encoder block, and get the set of Encoded Vectors for every word simultaneously.2. Decoder Block -Now, like if we are training a translator for English to the French language, so for training we need to give an English sentence along with its translated French sentence for the model to learn. So, our English sentences pass through Encoder Block, and French sentences pass through the Decoder Block.At first, we have the Embedding layer and Positional encoder part which changes the words into respective vectors, It is similar to what we have seen in the Encoder part.Masked Multi-Head Attention Part -Now it will pass through the self-attention block, where attention vectors are generated for every word in French sentences to represent how much each word is related to every word in the same sentence. (Just like we saw in Encoder Part).But this block is called the Masked Multi-Head Attention Block, and I am going to explain in simple terms-For that, we need to know how the learning mechanism works. First, we give an English word, it will translate in its French version itself using previous results, then it will match and compare with the actual French translation (which we fed in the decoder block). After comparing both, it will update its matrix value. This is how it will learn after several iterations.What we observe is that we need to hide the next French word, so that at first it will predict the next word itself using previous results, without knowing the real translated word. For learning to take place, it will make no sense if it already knows the next French word. Therefore, we need to hide (mask) it.We can take any word from the English sentence, but we can only take the previous word of the French sentence to learn. So, while performing the parallelization with matrix operation, we make sure that the matrix should mask the words appearing later by transforming them into 0’s so that the attention network can’t use them.Now, the resulting attention vectors from the previous layer and the vectors from the Encoder Block are passed into another Multi-Head Attention Block. (this part is where the results from the encoder block also come into the picture. In the diagram also it is clearly visible that the results from the Encoder blocks are coming here.). That’s why it is called Encoder-Decoder Attention Block.As we have one vector of every word for each English and French sentence. This block actually does the mapping of English and French words and finds out the relation between them. So, this is the part where the main English to French word mapping happens.The output of this block is attention vectors for every word in English and French sentences. Each vector represents the relationship with other words in both languages.Now we pass each attention vector into a feed-forward unit, it will make the output vectors form into something which is easily acceptable by another decoder block or a linear layer.A linear layer is another feed-forward layer. It is used to expand the dimensions into numbers of words in the French language after translation.Now it is passed through a Softmax Layer, which transforms the input into a probability distribution, which is human interpretable.And the resulting word is produced with the highest probability after translation.Below is the example that was illustrated in Google’s AI Blog [6], I put it here for your reference.The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.The decoder operates similarly, but generates one word at a time, from left to right. It attends not only to the other previously generated words but also to the final representations generated by the encoder.So, this is how the transformer works, and it is now the state-of-the-art technique in NLP. It is giving wonderful results, using a self-attention mechanism and also solves the parallelization issue. Even Google uses BERT that uses a transformer to pre-train models for common NLP applications.References -",25/04/2020,0,44.0,25.0,1240.0,599.0,27.0,6.0,0.0,33.0,en
3886,Simple Image Classification using Convolutional Neural Network — Deep Learning in python.,Becoming Human: Artificial Intelligence Magazine,Venkatesh Tata,417.0,10.0,1776.0,"In this article we will be solving an image classification problem, where our goal will be to tell which class the input image belongs to. The way we are going to achieve it is by training an artificial neural network on few thousand images of cats and dogs and make the NN(Neural Network) learn to predict which class the image belongs to, next time it sees an image having a cat or dog in it.The key thing to understand while following this article is that the model we are building now can be trained on any type of class you want, i am using cat and dog only as a simple example for making you understand how convolutional neural networks work. For example, if there are any doctors reading this, after completing this article they will be able to build and train neural networks that can take a brain scan as an input and predict if the scan contains a tumour or not.So coming to the coding part, we are going to use Keras deep learning library in python to build our CNN(Convolutional Neural Network).Before we jump into building the model, i need you to download all the required training and test dataset by going into this drive by clicking here, download both the folders named “ test_set” and “training_set” into your working directory, it may take a while as there are 10,000 images in both folders, which is the training data as well as the test dataset. Make sure to create a new directory and name it “whatever_you_want” and paste the above downloaded dataset folders into it.Let’s see what does the folders you just downloaded have in them. First, the folder “training_set” contains two sub folders cats and dogs, each holding 8000 images of the respective category. Second, the folder “test_set” contains two sub folders cats and dogs, each holding 2000 images of respective category.The process of building a Convolutional Neural Network always involves four major steps.Step - 1 : ConvolutionStep - 2 : PoolingStep - 3 : FlatteningStep - 4 : Full connectionWe will be going through each of the above operations while coding our neural network. So first go to your working directory and create a new file and name it as “whatever_you_want”.py , but I am going to refer to that file as cnn.py, where ‘cnn’ stands for Convolutional Neural Network and ‘.py’ is the extension for a python file. You will be appending whatever code I write below to this file.First let us import all the required keras packages using which we are going to build our CNN, make sure that every package is installed properly in your machine, there is two ways os using keras, i.e Using Tensorflow backend and by Using Theano backend, but don’t worry, all the code remains the same in either cases. I tested the below code using Tensorflow backend.Let us now see what each of the above packages are imported for :In line 1, we’ve imported Sequential from keras.models, to initialise our neural network model as a sequential network. There are two basic ways of initialising a neural network, either by a sequence of layers or as a graph.In line 2, we’ve imported Conv2D from keras.layers, this is to perform the convolution operation i.e the first step of a CNN, on the training images. Since we are working on images here, which a basically 2 Dimensional arrays, we’re using Convolution 2-D, you may have to use Convolution 3-D while dealing with videos, where the third dimension will be time.In line 3, we’ve imported MaxPooling2D from keras.layers, which is used for pooling operation, that is the step — 2 in the process of building a cnn. For building this particular neural network, we are using a Maxpooling function, there exist different types of pooling operations like Min Pooling, Mean Pooling, etc. Here in MaxPooling we need the maximum value pixel from the respective region of interest.In line 4, we’ve imported Flatten from keras.layers, which is used for Flattening. Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector.And finally in line 5, we’ve imported Dense from keras.layers, which is used to perform the full connection of the neural network, which is the step 4 in the process of building a CNN.Now, we will create an object of the sequential class below:Let us now code the Convolution step, you will be surprised to see how easy it is to actually implement these complex operations in a single line of code in python, thanks to Keras.Let’s break down the above code function by function. We took the object which already has an idea of how our neural network is going to be(Sequential), then we added a convolution layer by using the “Conv2D” function. The Conv2D function is taking 4 arguments, the first is the number of filters i.e 32 here, the second argument is the shape each filter is going to be i.e 3x3 here, the third is the input shape and the type of image(RGB or Black and White)of each image i.e the input image our CNN is going to be taking is of a 64x64 resolution and “3” stands for RGB, which is a colour img, the fourth argument is the activation function we want to use, here ‘relu’ stands for a rectifier function.Now, we need to perform pooling operation on the resultant feature maps we get after the convolution operation is done on an image. The primary aim of a pooling operation is to reduce the size of the images as much as possible. In order to understand what happens in these steps in more detail you need to read few external resources. But the key thing to understand here is that we are trying to reduce the total number of nodes for the upcoming layers.We start by taking our classifier object and add the pooling layer. We take a 2x2 matrix we’ll have minimum pixel loss and get a precise region where the feature are located. Again, to understand the actual math behind Pooling, i suggest you to go learn from an external source, this tutorial concentrates more on the implementation part. We just reduced the complexity of the model without reducing it’s performance.It’s time for us to now convert all the pooled images into a continuous vector through Flattening. Flattening is a very important step to understand. What we are basically doing here is taking the 2-D array, i.e pooled image pixels and converting them to a one dimensional single vector.The above code is pretty self-explanatory. We’ve used flatten function to perform flattening, we no need to add any special parameters, keras will understand that the “classifier” object is already holding pooled image pixels and they need to be flattened.In this step we need to create a fully connected layer, and to this layer we are going to connect the set of nodes we got after the flattening step, these nodes will act as an input layer to these fully-connected layers. As this layer will be present between the input layer and output layer, we can refer to it a hidden layer.As you can see, Dense is the function to add a fully connected layer, ‘units’ is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes but the art of choosing the most optimal number of nodes can be achieved only through experimental tries. Though it’s a common practice to use a power of 2. And the activation function will be a rectifier function.Now it’s time to initialise our output layer, which should contain only one node, as it is binary classification. This single node will give us a binary output of either a Cat or Dog.You can observe that the final layer contains only one node, and we will be using a sigmoid activation function for the final layer.Now that we have completed building our CNN model, it’s time to compile it.From above :It’s time to fit our CNN to the image dataset that you’ve downloaded.But before we do that, we are going to pre-process the images to prevent over-fitting. Overfitting is when you get a great training accuracy and very poor test accuracy due to overfitting of nodes from one layer to another.So before we fit our images to the neural network, we need to perform some image augmentations on them, which is basically synthesising the training data. We are going to do this using keras.preprocessing library for doing the synthesising part as well as to prepare the training set as well as the test test set of images that are present in a properly structured directories, where the directory’s name is take as the label of all the images present in it. For example : All the images inside the ‘cats’ named folder will be considered as cats by keras.You can find the explanation of what each of the above parameters do here, in the keras documentation page. Butwhat you need to understand as a whole of whats happening above is that we are creating synthetic data out of the same images by performing different type of operations on these images like flipping, rotating, blurring, etc.Now lets fit the data to our model !In the above code, ‘steps_per_epoch’ holds the number of training images, i.e the number of images the training_set folder contains.And ‘epochs’, A single epoch is a single step in training a neural network; in other words when a neural network is trained on every training samples only in one pass we say that one epoch is finished. So training process should consist more than one epochs.In this case we have defined 25 epochs.The test_image holds the image that needs to be tested on the CNN. Once we have the test image, we will prepare the image to be sent into the model by converting its resolution to 64x64 as the model only excepts that resolution. Then we are using predict() method on our classifier object to get the prediction. As the prediction will be in a binary form, we will be receiving either a 1 or 0, which will represent a dog or a cat respectively.You can check out the code in my GitHub repository : https://github.com/venkateshtata/cnn_medium.And feel free to fork and send pull requests, if you have any great modifications or suggestions to the code i’ve wrote. Thank you.Note :I have started my own stie where I will be implementing latest research papers on computer vision and Artificial Intelligence. Please visit www.matrixbynature.com for more tutorials.",13/12/2017,12,43.0,0.0,541.0,324.0,4.0,1.0,0.0,8.0,en
3887,How to easily Detect Objects with Deep Learning on Raspberry Pi,NanoNets,Sarthak Jain,1400.0,10.0,1154.0,"Disclaimer: I’m building nanonets.com to help build ML with less data and no hardwareIf you’re impatient scroll to the bottom of the post for the Github ReposThe raspberry pi is a neat piece of hardware that has captured the hearts of a generation with ~15M devices sold, with hackers building even cooler projects on it. Given the popularity of Deep Learning and the Raspberry Pi Camera we thought it would be nice if we could detect any object using Deep Learning on the Pi.Now you will be able to detect a photobomber in your selfie, someone entering Harambe’s cage, where someone kept the Sriracha or an Amazon delivery guy entering your house.20M years of evolution have made human vision fairly evolved. The human brain has 30% of it’s Neurons work on processing vision (as compared with 8 percent for touch and just 3 percent for hearing). Humans have two major advantages when compared with machines. One is stereoscopic vision, the second is an almost infinite supply of training data (an infant of 5 years has had approximately 2.7B Images sampled at 30fps).To mimic human level performance scientists broke down the visual perception task into four different categories.Object detection has been good enough for a variety of applications (even though image segmentation is a much more precise result, it suffers from the complexity of creating training data. It typically takes a human annotator 12x more time to segment an image than draw bounding boxes; this is more anecdotal and lacks a source). Also, after detecting objects, it is separately possible to segment the object from the bounding box.Object detection is of significant practical importance and has been used across a variety of industries. Some of the examples are mentioned below:Object Detection can be used to answer a variety of questions. These are the broad categories:There are a variety of models/architectures that are used for object detection. Each with trade-offs between speed, size, and accuracy. We picked one of the most popular ones: YOLO (You only look once). and have shown how it works below in under 20 lines of code (if you ignore the comments).Note: This is pseudo code, not intended to be a working example. It has a black box which is the CNN part of it which is fairly standard and shown in the image below.You can read the full paper here: https://pjreddie.com/media/files/papers/yolo_1.pdfFor this task, you probably need a few 100 Images per Object. Try to capture data as close to the data you’re going to finally make predictions on.Draw bounding boxes on the images. You can use a tool like labelImg. You will typically need a few people who will be working on annotating your images. This is a fairly intensive and time consuming task.You can read more about this at medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab. You need a pretrained model so you can reduce the amount of data required to train. Without it, you might need a few 100k images to train the model.You can find a bunch of pretrained models hereThe process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train.To start training the model you can run:The docker image has a run.sh script that can be called with the following parametersYou can find more details at:github.comTo train a model you need to select the right hyper parameters.Finding the right parametersThe art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. There is some level of black magic associated with this, along with a little bit of theory. This is a great resource for finding the right parameters.Quantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)Small devices like Mobile Phones and Rasberry PI have very little memory and computation power.Training neural networks is done by applying many tiny nudges to the weights, and these small increments typically need floating point precision to work (though there are research efforts to use quantized representations here too).Taking a pre-trained model and running inference is very different. One of the magical qualities of Deep Neural Networks is that they tend to cope very well with high levels of noise in their inputs.Why Quantize?Neural network models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for the neural connections, since there are often many millions of these in a single model.The Nodes and Weights of a neural network are originally stored as 32-bit floating point numbers. The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer.The size of the files is reduced by 75%.Code for Quantization:Note: Our docker image has quantization built into it.You need the Raspberry Pi camera live and working. Then capture a new ImageFor instructions on how to install checkout this linkDownload ModelOnce your done training the model you can download it on to your pi. To export the model run:Then download the model onto the Raspberry Pi.Install TensorFlow on the Raspberry PiDepending on your device you might need to change the installation a littleRun model for predicting on the new ImageThe Raspberry Pi has constraints on both Memory and Compute (a version of Tensorflow Compatible with the Raspberry Pi GPU is still not available). Therefore, it is important to benchmark how much time do each of the models take to make a prediction on a new image.We have removed the need to annotate Images, we have expert annotators who will annotate your images for you.We automatically train the best model for you, to achieve this we run a battery of model with different parameters to select the best for your dataNanoNets is entirely in the cloud and runs without using any of your hardware. Which makes it much easier to use.Since devices like the Raspberry Pi and mobile phones were not built to run complex compute heavy tasks, you can outsource the workload to our cloud which does all of the compute for youGet your free API Key from http://app.nanonets.com/user/api_keyNote: This generates a MODEL_ID that you need for the next stepCollect the images of object you want to detect. You can annotate them either using our web UI (https://app.nanonets.com/ObjectAnnotation/?appId=YOUR_MODEL_ID) or use open source tool like labelImg. Once you have dataset ready in folders, images (image files) and annotations (annotations for the image files), start uploading the dataset.Once the Images have been uploaded, begin training the ModelThe model takes ~2 hours to train. You will get an email once the model is trained. In the meanwhile you check the state of the modelOnce the model is trained. You can make predictions using the model",20/03/2018,14,31.0,6.0,832.0,592.0,15.0,6.0,0.0,26.0,en
3888,An intuitive explanation of Beam Search,Towards Data Science,Renu Khandelwal,3900.0,6.0,657.0,"In this article, we will learn:medium.comtowardsdatascience.comIn this article, you will get a detailed explanation of how neural machine translation developed using sequence to sequence algorithm to find the most relevant words in sentences for a target language.What is Beam search?To understand the Beam search, we will use the neural machine translation use case of sequence to sequence.The sequence to sequence model uses an encoder and decoder framework with Long Short Term Memory(LSTM) or Gated Recurrent Unit(GRU) as the basic blocks.Encoder maps a source sequence encodes the source information and passes it to the decoder. The decoder takes the encoded data from the encoder as an input along with the start-of-string <START> token as the initial input to produce an output sequence.Here the source sequence is a sentence in Hindi, and the target sequence is generated in English. You don’t want any random English translation, but you would like the best and most likely words to be selected for the translation that matches the meaning of the Hindi sentence.I have examples in Hindi as that is the other language other than English that I know.How do you select the best and most likely words for the target sequence?A simple approach would be to have a vocabulary of words in the target language, say 10,000 words then, based on the source sentence, get a probability for each of the 10,000 target words.There could be multiple likely translations of the source sentence in the target language.Should you pick any translation randomly?Our goal is to pick the best and most likely translated word, so we choose the target word with the maximum probability based on the source sentence.Should you pick only one best translation?Greedy Search algorithm selects one best candidate as an input sequence for each time step. Choosing just one best candidate might be suitable for the current time step, but when we construct the full sentence, it may be a sub-optimal choice.The beam search algorithm selects multiple alternatives for an input sequence at each timestep based on conditional probability. The number of multiple alternatives depends on a parameter called Beam Width B. At each time step, the beam search selects B number of best alternatives with the highest probability as the most likely possible choices for the time step.Let’s take an example to understand this.We will select the beam width =3; our vocabulary of English words is 10,000.Step 1:Find the top 3 words with the highest probability given the input sentence. The number of most likely words are based on the beam widthGreedy Search will always consider only one best alternative.Step 2: Find the three best pairs for the first and second words based on conditional probabilityStep 3: Find the three best pairs for the first, second and third word based on the input sentence and the chosen first and the second wordWe continue this process, and we pick three sentences with the highest probability. The top 3 sentences can vary in length or maybe the same length.We finally pick the output of the decoder as the sentence with the highest probabilityWill a higher value for the beam width give a better translation?A higher beam width will give a better translation but would use a lot of memory and computational power.When we had a beamwidth of 3 and 10,000-word vocabulary, we evaluated 30,000 combinations at each time step, created three instances of the encoder-decoder, and max sentence length was 9. Creating multiple copies of encoder-decoder and calculating conditional probabilities for 30,000 words at each time step needs lots of memory and computation power.A lower beam width will result in more inferior quality translation but will be fast and efficient in terms of memory usage and computational powerBeam search is the most popular search strategy for the sequence to sequence Deep NLP algorithms like Neural Machine Translation, Image captioning, Chatbots, etc.Beam search considers multiple best options based on beamwidth using conditional probability, which is better than the sub-optimal Greedy search.https://www.youtube.com/watch?v=RLWuzLLSIgw",02/02/2020,0,19.0,5.0,820.0,310.0,8.0,4.0,0.0,6.0,en
3889,The Games That AI Won,Towards Data Science,Brandon Walker,1300.0,6.0,1368.0,"Some tasks that AI does are actually not impressive. Think about your camera recognizing and auto-focusing on faces in pictures. That technology has been around since 2001, and it doesn’t tend to excite people. Why not? Well, because you can do that too, you can focus your eyes on someone’s face very easily. In fact, it’s so easy you don’t even know how you do it. If AI can do it too, then who cares how it works? Though we may not explicitly understand how this AI works, its underlying mechanisms don’t do anything we can’t. At least, this is what I think most people are thinking.Games are just the opposite. Rather than games being an innate ability we have (like focusing your vision), you have an understanding of how and why you make decisions within a game. So when AI is making decisions about how a game is played, they are much more interesting because you could be in the same situation as the AI, making the same exact decisions. AI is popularly thought of as something that replaces human work. When AI plays a game, it feels closest to taking our position in the world, since games are things we have to consciously think about. Games that AI have played have thus captured the interest of news, here are the most important victories it has won.On February 10th, 1996, IBM’s Deep Blue faced off against Gary Kasparov and became the first computer program to defeat a current chess world champion under normal conditions (i.e. with a clock in place). Unfortunately for Deep Blue, over the remaining five games in the match, it would lose three games and draw twice.A second match was set for the next year, and Deep Blue’s team made upgrades to their software. On May 3rd Deep Blue opened with a loss. In the second game, Deep Blue had trouble calculating what to do, to prevent itself from wasting too much time it made a random move. The move was terrible, but this threw off Kasparov who assumed that Deep Blue could see further into the future of the game than he could. Rather than have a loss dragged out of him in front of millions of people, he resigned the second game (which is common in chess). In reality, Kasparov could have won the second game had he played it to completion, which was a small embarrassment to him, which led to him playing more recklessly in the following games, culminating in Deep Blue winning 2 games to Kasparov’s 1, tying 3.As ESPN journalist Jeremy Schaap put it, “people were following this all over the world, people who had no interest in chess, people who only had interest in this narrative of man versus machine.” The interest in this game was fed by Kasparov’s claim at the end of the second game that IBM was cheating, which he based on the difference in Deep Blue’s quality of play between the first and second game. Chess has been a barometer of intelligence since its invention in the 6th century. To think that humans can be beaten by computers would be frustrating since there were still many ways which computers are inferior to humans. Of course, IBM was not cheating and Deep Blue’s quality change was attributed to its random move.IBM’s public relations team had pumped up the event, and the company’s stock rose immensely. With a victory in the bag, IBM rejected a rematch, then dismantle Deep Blue and made its final resting place the Computer History Museum in Mountain View.Kasparov no longer believes that Deep Blue was cheating, and recognizes that not only did Deep Blue beat him, but Deep Blue itself can be beaten by even new programs running on smaller computers. This game was a big step in the narrative of Artificial v. Human Intelligence and it established games as a method of comparison.While eating lunch together, two IBM executives began commenting on the success of Ken Jennings on Jeopardy! who still holds the record for most consecutive games won. Perhaps in yearning for the success following Deep Blue’s victory, they created Watson, a question-answering system that was intended to beat Ken Jennings.Watson, named after IBM’s founder Thomas J. Watson, was created in 2005, but did not begin its famous appearance until 2011. Along with Ken Jennings ,Watson went up against Brad Rutter, who were considered the two best Jeopardy! players at the time. Ken was a programmer who had taken a few AI classes and accepted the challenge partially because he thought there was no way an AI system could possibly beat him.Watson won the two matches it played against Brad and Ken, earning $77,147 to their $24,000 and $21,600, respectively. Watson then went on to defeat two members of congress, Rush D. Hold Jr. (a former Jeopardy! contestant) and Bill Cassidy, yet another bipartisan loss.Unlike Deep Blue, Watson was spun off by IBM into many commercial products including Watson Text to Speech, Watson Speech to Text, Watson Natural Language Classifier, Watson Natural Language Understanding, Watson Assistant, and Watson Health, among others.While Watson was designed for a specific purpose, DeepMind Technologies wanted to create an AI system that need not be trained on a narrow field, moving us closer to the definition of General AI. The goal they set out on pursuing was beating the many Atari games with only one model that was not redesigned for any particular game.Using only the input of what was on the screen and being told to maximize the score, DeepMind was able to beat every game at and get better scores then even the best human players. There 2013 paper, Playing Atari with Deep Reinforcement Learning led Google to acquire them in 2014.Deep Blue’s advantage had been its ability to process large amounts of moves at once and had a clever way of evaluating which one was the best. For the Asian board game Go, this was simply not possible. The rules of the game are quite simple, but their are around 2 * 10¹⁷⁰ number of states the game can be in at any one time. As a reference this number is larger than the number of atoms in the universe. Thus, strategy within a game of Go is hard to define since for each state, there are many decisions to evaluate that each lead to more states to evaluate. With no brute force approach working, DeepMind’s AlphaGo learned to attach value up to a few periods into the future, then learned to make decisions based on that value.After defeating the European Go champion Fan Hui, AlphaGo was tasked with playing Lee Sedol, one of the highest ranking players ever. Beating Lee prompted him to retire, saying “Even if I become the number one, there is an entity that cannot be defeated.”AlphaZero was a generalized version of AlphaGo, built with the intention of winning Chess, Go and Shogi (a Japanese version of chess). Not only did AlphaZero beat AlphaGo, it was done by only playing simulated games against itself, having no examples of expert’s games to look at. At the start of these simulations it knew absolutely nothing. It mastered Chess after 9 hours of training, Shogi after 2, and Go after 34.While this was great for Google in attracting talent, AI professor Joanna Bryson has pointed out that the credit Google have been given for these achievements can give a massive negotiating advantage with governments seeking to regulate AI.AlphaZero was then transitioned into AlphaStar with the intention of beating the real-time strategy game Starcraft. In 2019 AlphaStar achieved a ranking in the top 0.2 percent of human players. This was the first time that an AI had ever topped an e-sport. AlphaStar is not perfect at the Starcraft, but it is still seen as a massive milestone for AI, since it works in real-time, with partial information, under complex rules. While Go was relatively complicated in the number of decisions it faced, the rules to Go are quite simple compared to Srarcraft. This more complicated environment that AlphaStar operates in is a good indicator that AI is ready for wider commercial use like in self-driving vehicles, robotics, and supply chains.",15/03/2020,0,0.0,5.0,411.0,418.0,2.0,0.0,0.0,4.0,en
3890,Clustering Liferay globally across data centers (GSLB) with JGroups and RELAY2,Medium,bitsofinfo,25.0,16.0,3438.0,"Recently I’ve have been looking into options to solve the problem of GSLB’ing (global server load balancing) a Liferay Portal instance.This article is a work in progress… and a long one. Jan Eerdekens states it correctly in his article, “Configuring a Liferay cluster is part experience and part black magic” …. however doing it across data-centers however is like wielding black magic across N black holes….Footnotes for this article are here: https://bitsofinfo.wordpress.com/2014/05/21/liferay-clustering-internals/The objective is a typical one.Hopefully this article will help others out there, point them in a new direction and give them some ideas on how to put something like this together.I’d like to note that this is not necessarily the ideal way to do this…. but just one way you can do it “out of the box” without extending Liferay in other ways. (I say this because there are many optimizations one could do for slimming down what Liferay sends around a cluster by default… i.e. lots of heavier object serializations as events happen) I’d also like to note that I am not a Liferay expert and I’m sure some things I am describing here are not 100% accurate. What is described below is the result of a lot of analysis of the uncommented, undocumented Liferay source code. I’m sure the developers at Liferay could provide additional insight, corrections and clarifications to what is stated below, but in lieu of design documentation, code-comments and the alike this is all that we in the community have to go on.For my use case I tested this with two data-centers. One home-grown bare-metal DC in “regionA” and the other in AWS “regionB”. Point being we have two DC’s that are geographically separated over a WAN; could be dedicated line, VPN tunnel; whatever; point being the bandwidth available between DC’s is nothing compared to within DC.Liferay has some light documentation for clustering, however this documentation is focused on a Liferay cluster within a single DC. I say “light documentation” because that is the nicest way I can state it. The Liferay project in general is quite void of any real documentation when it comes to how things work internally “under the hood” in the Liferay core codebase (design documents, code comments etc). If you want to know how things work, you have to crawl through tons of un-commented source code and figure it out for yourself.First off Liferay uses JGroups under the hood (specifically JGroups version 3.2.10 in Liferay 6.2 as of this post). JGroups is pretty much one of the de-facto “go-to’s” for building clusters in Java applications and has been around a very long time; many Java stacks use this. If you want to see a good example of a open-source project with good design documents that explain the internals, see JGroups (hint, hint Liferay guys) I’m not going to go much further into describing JGroups, you can do that on your own; as I’ll be using some JGroups terminology below.Liferay defines two primary JGroups channels for what Liferay calls “cluster link”. You enable this in your portal-ext.properties by setting cluster.link.enabled=true. By default all channels in Liferay are UDP (multicast); if you are trying to cluster Liferay in a DC that does not support multicast (like AWS) you will want to configure it to use unicast (see this article)Both of these channels are used for various things such as notifying other cluster members of a give nodes state (i.e. notifying they are up/down etc) and sending ClusterRequests to other nodes for invoking operations across the cluster. There does not seem to be any consistency as to why one is used over the other. For example streaming a Lucene index over from another node uses the control channel, which reindex() requests use the transport channel.Out of the box, if you bring up more than one node in a local DC (configured w/ UDP multicast), the Liferay nodes will automatically discover each other, peer up in to a cluster and begin to send data to each other when appropriate. For unicast, again you have you make some changes to your portal-ext.properties to use unicast, but effectively the result is the same.Great! Now what, we have have one DC with N local nodes that are all peered up with each other….. but how can this DC exchange state with DC2? Good question, when attempting to GSLB an application there are many considerations, specifically for Liferay the big ones that I noted that need to be addressed are below; note there are some more hidden in Liferay’s internals, but for the big picture lets just focus on these 🙂Liferay has the concept of a logical “master” and who is the “master” is determined by ownership of a named lock called “com.liferay.portal.cluster.ClusterMasterExecutorImpl” that resides in the “Lock_” table in the database. @see ClusterMasterTokenClusterEventListener. Note that the database is shared by all nodes across all DC’s (see database section below), and this presents a huge problem if clusters in separate DC’s (which are only locally aware of peer-DC-nodes by default w/ Jgroups) can’t talk to each other across DC’s; which is what this article is about. i.e. node1 in DC1 might acquire this Lock_ first, but the nodes in DC2 cannot communicate w/ the “master” because it is in an unreachable DC.This is an enormous topic on its own, but for this article lets keep it dumb simple. Leverage Liferay’s reader-writer database configuration. For example in DC1 setup your master instance for your database and designate as your “write” database, then configure two read-slave instances; one slave in DC1 and another in DC2. In your portal-ext.properties files for nodes in both DC’s configure “jdbc.write.url” to hit the master instance in DC1 and “jdbc.read.url” to hit whichever read-slave instance is local within each DC.Liferay leverages Ehcache for caching within the application. Clustering for Ehcache can be enabled by reading this article. The configuration relies on a separate JGroups channel within the Liferay cluster and you need to properly configure it for unicast if your DC does not support multicast just like previously described.Fire up two separate DC’s pointing to a database setup as previously described, go change some data in the app via a node in DC1, and because it is likely cached, when you view that same page/data in DC2 you won’t see the change visible in the UI. Why? Well because the clustering is only local to each DC. When model objects are updated in Liferay they are saved in the database, and then an event occurs (distributed via JGroups) that tells peer nodes to dump the cache entry…. but only local to that DC. So you say, “well why not just enable unicast for all nodes in every DC so they are aware of all other nodes in all other DCs?” You could, but imagine the cross-talk; no thanks. There are a few solutions to this, one will be described below (via RELAY2) and another could be provided by the ehcache-jms-wan-replicator project.Liferay’s default search index is implemented using Lucene. There are several ways to configure Lucene for a Liferay cluster… but for this article lets keep it simple and not setup Solr to avoid the complexity of having to GSLB that as well… and just enable “lucene.replicate.writes=true”. So each peer node within a local DC has its own Lucene index copy, and once again Liferay leverages JGroups (via ClusterExecutorImpl) triggered by all sorts of fancy AOP (see IndexableAdvice.java, @Indexable annotation, IndexWriterProxyBean + review search-spring.xml in the codebase) to essentially intercept a index write and broadcast a”reindex this thing” message to peer nodes in the local DC cluster. Note that Liferay sends the entire object w/ data to be reindexed to all peer nodes over the JGroups channel (which is not necessarily efficient over a WAN). Secondly, when you go to Server Administration in the GUI and hit the “reindex all index data” button, the server you invoke this on also invokes that operation against all peer nodes. Lastly, another hidden thing is that peer nodes will suck over the entire Lucene index via an HTTP call on startup from a donor node…..again we’ll touch on this later and the considerations to think about.Again, fire up two separate DC’s pointing to a database/caching setup as previously described. Go to control panel and add a new user in DC1. Great you see the new user in the user’s screen when accessing through a DC1 node. Go view the users from DC2’s perspective. You won’t see the user, nor can you find them in a search despite them being in the database. Why? Well two things, first that Lucene “reindex this thing” message did not make it to DC2, and secondly (unless at this point you have either RELAY2 setup OR ehcache-jms-wan-replicator configured) these screens are also reliant on what is in Ehcache and a combination of what is in the local Lucene index.This is most definitely a consideration for GSLB’ing Liferay across DC’s, however it really does not have anything to do w/ JGroups and RELAY2 in particular so I’m not going to discuss it here. I’ll point you in a direction… consider putting Liferay’s files in S3 and abstracting that with something like YAS3FS which is quite promising for building a distributed file store with local node CDN read performance. Much faster than Liferay’s S3 implementation and globally aware of changes to the FS.Liferay has two classes of “jobs” (implemented via Quartz); master jobs which run only on ONE node, the “master” node and “slave” jobs which can run on any node. Again who is the “master” job runner is determined by an entry in the Lock_ table named “com.liferay.portal.kernel.scheduler.SchedulerEngine”. Who gets the lock is essentially which server boots up and acquires it first. The same problem that exists as noted above with regards to slave->master communication exists here in separated DC to DC environments where the nodes in separate DC’s cannot talk to one another over a WAN. Liferay has a few job types denoted by com.liferay.portal.kernel.scheduler.StorageType of:Point being is that the job engine in Liferay has a dependency to be ably to communicate to all other nodes.If you have the “live.users.enabled=true” option set in your portal-ext.properties you can do user monitoring in the cluster and this as well, if clustering is enabled needs to see all nodes in the cluster. When a cluster node comes up it sends a ClusterMessageType.NOTIFY/UPDATE which goes to all nodes which in turn broadcast a local ClusterEvent.JOIN which is reacted to by sending a unicast ClusterRequest.EXECUTE for LiveUsers.getLocalClusterUsers() on the remote node to effectively sign-in the users on the other node locally on the requesting node. This appears to be there so that each node will locally reflect all logged in users across the cluster. Again this will be incomplete if node1 on DC1 cannot talk to node2 @ DC2.There are a few other functions in Liferay that appear to leverage the underlying Jgroups channels (i.e. EE Licensing, JarUtil, DataSourceFactoryUtil, SetupWizard, PortalManagerUtil.manage() (related to JMX?), PortalImpl.resetCDNHosts() and EhcacheStreamBootstrapCacheLoader. You can see my other notes article for these tidbits.It might be helpful to diagnose whats going on by tweaking the logging settings for Liferay. To do this you should read this article to permanently change your log settings (which will dump more info on bootup which is important). Don’t rely on changing your logging settings via the GUI screen as those are not permanently saved and are transient. Below is an example webapps/ROOT/WEB-INF/classes/META-INF/portal-log4j-ext.xml file I used to triage various issues on bootup related to clustering.So at this point it should be clear to anyone reading that we need a way to get these separated clusters in DC1 and DC2 to talk to one another. First off we could just change the control/transport channels in Liferay to force use TCP UNICAST and specifically list all nodes globally across DC1 and DC2. This would let every node know about every other node globally, however this won’t scale well as each node would talk to every other node. The other option is the RELAY2 functionality available in JGroups.Essentially what RELAY2 provides is a “bridge” between N different JGroups clusters that are physically separated by a WAN or other network arrangement. By adding RELAY2 into your JGroups stack you can somewhat easily ensure that ALL messages that are passed through a JGroups channel will be distributed over the bridge to all other JGroups clusters in other DCs. Think of RELAY2 as secondary “cluster” in addition to your local cluster, however only ONE node in your local JGroups cluster (the coordinator) is responsible for “relaying” all messages over the bridge to the other “coordinators” in the relay cluster, for distribution to the other coordinators local JGroups cluster. So “out of the box” this can let you ensure that all Liferay cluster operations that occur in DC1 get transmitted to DC2 transparently. So with this enabled, when we add that user to our local Lucene index across all local nodes via UDP in the local cluster, the coordinator node in DC2 will also receive that event and transmit it locally to all nodes in the DC2 cluster. Now when you view the “users” on node2 in DC2 you will see the data that was added by node1 in DC1One IMPORTANT caveat is that using the RPC building blocks in JGroups/RELAY2 does not effectively cascacde RPC calls over the bridge. See here and this thread. To Liferay’s credit they did not implement the “RPC” like method invocations across Liferay clusters’s using RPC in JGroups (i’m not sure why) but rather they serialize ClusterRequests which encapsulate what is to be invoked on the other side via method reflection, and just send these messages as serialized objects over the wire. Had they used RPC one would have to modify Liferay’s code to get these RPC invocations across the RELAY2 bridge.What I am describing below assumes you are just using the Liferay cluster defaults of a local UDP multicast cluster, again if you are in AWS you will just need to adjust your unicast TCP JGroups stack accordingly, the configuration is pretty much the same w/ regards to where RELAY2 in configured the stackIMPORTANT: this only covers the transport and control channels. If you want to enable this kind of relay bridge for the separate Ehcache channels that Liferay uses, you will repeat the process (described in the steps below) for the Ehcache JGroups channel definitions in Liferay as well… summary high-level steps below (note alternatively you could leave the ehcache jgroups configuration alone in liferay and just leverage the ehcache jms wan replicator.)Ok, first lets define our “relay clusters” for both the control and transport channels in Liferay. Note all the steps below need to be done for all nodes across all DC’s and you need to adjust certain things relative to what DC you are running in, particularly the “site” names in the RELAY2 configurations1. Create a file in your WEB-INF/classes dir called “relay2-transport.xml”.2. Create a file in your WEB-INF/classes dir called “relay2-control.xml”3. Next we need to configure the actual TCP relay clusters for both the control/transport relay configurations. The sample below is a TEMPLATE you can use and copy twice, once for “relay2_global_control_tcp.xml” and another for “relay2_global_transport_tcp.xml”. Change the TCP.bind_port and TCPPING.initialhosts appropriately for each file. For TCPPING.inititalhosts, you will want to list ONE known node that lives in each DC. These will be the initial relay coordinator nodes.4. At this point we have the relay cluster configuration defined. Now we need to adjust Liferay’s “control” and “transport” channel JGroups stack to utilize them. Open up portal-ext.properties and add the following to the control and transport channels. Note these are long lines, copied from portal.properties default to portal-ext.properties and then appending to the stack:IMPORTANT: be sure to set the “site=” property of RELAY2 to match the DC you are configuring this for and must match the “site” name in the relay2-transport.xml and relay2-control.xml files accordingly.5. Ok, now startup Liferay and adjust its startup java options to add the following. This is required so that the JGroups stacks can find the relay2-transport.xml and relay2-control.xml relay configuration files.6. In Liferay’s logs you should see some additional JGroups control/transport channels showing up that represent the bridge between the report sites.At this point you should now have two separate Liferay clusters in two separate data-centers “bridged” using JGroups RELAY2 so that many of the issues described in this article are resolved and Liferay cluster events/messages are received across both data-centers. Note that unless you also tweaked the Ehcache JGroups configuration (as noted earlier) or are using the ehcache jms wan replicator, the Ehcache clustered cache replication events will not be sent to the other DC’sThat said, again this is not necessarily the best way to do this. What is the best way is to be determined as this is an experiment in progress. There are many things that may or may not really be necessary to warrant utilizing RELAY2 as the means to bridge multiple separated Liferay clusters. Liferay can generate a lot of cluster traffic and if you are bridging over a WAN this may not be efficient, or could potentially block operations in DC1 while waiting on a coordinator or timeout in DC2, resulting in perceived slowness on the sender side depending on if Liferay is invoking things remotely via ClusterRequests synchronously or asynchronously.The latter could potentially be alleviated by tweaking your TCP bridge configuration to optimize the TCP “max_bundle_size” and “max_bundle_timeout” parameters. Doing so you could reduce the sheer amount of messages sent back and forth over the bridge. I.E. let the bridge queue up N messages or until the total amount of data to be sent is >= N(size); effectively “batching” the data to be sent around the WAN. Note when tweaking these configuration settings you may encounter this kind of warning, that you will need to adjust your OS settings accordingly :Potential alternatives to RELAY2 are implementing point-specific solutions for transmitting the most visible/critical information to other DC’s such as cache events in a batched optimized fashion by using something like the Ehcache JMS WAN replicator. Another example is writing an extension for Lifferay which would do something similar for batching “reindex()” requests across DC’s, so rather than relying on RELAY2 which would transmit full Lucene documents over the wire… one-by-one… and extension could be developed to batch these in an optimized fashion where only model ids, type and operation request are conveyed rather than the entire Lucene document.Also note that because Liferay streams the entire contents of its Lucene indexes to peer nodes… it is important to understand “who” Liferay considers a peer node. This is determined by calling its “control” JChannel’s, “receiver” who is Liferay’s BaseReceiver’s getView() method which should only return local DC peers, and not those across a RELAY2 bridge which should avoid it connecting to a node in a WAN separated DC to stream the index (note according to these docs, a JGroups channel stack with RELAY2 enabled does NOT return views that span bridges). If you didn’t use RELAY2 but instead manually configured a giant UNICAST cluster across WANs, one would have to consider how clusters boot up, because if you start one node in DC1, then node2 in DC1 would get its index update from node1(DC1) (which is fine because they would be on the same local network). However when node3 comes up in DC2, because it recognizes the “master” node (via the shared database Lock_ table entry) as living in DC1 it would have to stream its index over the WAN. I’d also like to note that Liferay WILL attempt to stream an index from a node in another DC if you do a manual “reindex” that is initiated by a remote DC, however this will fail due to the way the ClusterRequest to reindex() in other DC’s sends along the jgroups “address” (see the footnotes article section on “ClusterLoadingSyncJob” for more details.)There is also one other oddity I noted in “who” Liferay considers are “peers” when RELAY2 is used. See the section on ClusterMasterExecutorImpl in the footnotes for more information.Regardless, doing any of the latter customizations requires analysis of Liferays behavior/code to determine the impact on what “events” in Liferay could be missed by not using RELAY2 (which will catch everything). You also would then be responsible for keeping tabs on what changes as Liferay does new releases, modifies the way one particular “clusterable” action behaves or adds totally new features that send messages over the cluster.Hopefully this document will help others out there trying to solve this kind of problem and save others some valuable time!Originally published at bitsofinfo.wordpress.com on May 21, 2014.",21/05/2014,8,13.0,24.0,820.0,617.0,1.0,5.0,0.0,32.0,en
3891,Kuliah Itu Gak Penting,Medium,Andreas Yonathan,44.0,6.0,1071.0,"“Buat apa kuliah kalo kalah sukses atau gaji aja kalah gede dari lulusan SMA/SMK?”Serius, pasti banyak orang yang pernah terlintas pikiran brilian seperti diatas. Bahkan bisa jadi kamu yang membaca ini adalah satunya bukan? Tenang saja, kamu tidak sendirian karena saya juga pernah berpikir seperti itu kok hehehe :DYa, tidak bisa dipungkiri karena kita sering melihat contoh orang-orang terkenal yang sukses padahal mereka OD (baca: Out Dewe, kalau DO kan Drop Out, jelek kesannya ditendang, sementara OD gak perlu nunggu ditendang udah keluar-keluar sendiri hehehe :p) atau bahkan sekolah pun gak tamat. Contohnya Brad Pitt, Oprah Winfrey, Lady Gaga, John Lennon, Eminem. Nama-nama itu pasti sudah tidak asing lagi kan?Mungkin ada yang berpikir orang yang OD maupun DO tidak banyak berkontribusi pada dunia. Jika kamu berpikir seperti itu, sorry, YOU’VE GOT A BIG MISTAKE!Tanpa Travis Kalanick, dengan uang hanya 100 ribu tidak akan mungkin bisa bepergian dari ujung Jakarta ke ujung Jakarta menggunakan mobil pribadi bukan milik sendiri, apalagi gratis.Jika saja Steve Jobs tidak bosan dengan kuliah, kamu mungkin tidak bisa membawa laptop, tablet dan smartphone yang terkenal bergengsi dengan apel “kroak”nya.Ya bisa jadi kamu juga mungkin tidak akan bisa menikmati laptop tangguh yang terkenal dengan Alien****nya berkat Michael Dell. Tanpa Bill Gates, tidak ada “jendela” yang berarti kamu juga tidak akan bisa bermain DotA 2, apalagi menggunakan mouse khusus gaming tanpa Min-Liang Tan. Dan andai saja Mark Zuckerberg memilih melanjutkan kuliahnya, mungkin kamu tidak akan bisa berinteraksi dengan ribuan orang di seberang lautan melalui Facebook. Dan tentunya masih banyak lagi deretan orang-orang yang tidak selesai sekolah namun sukses besar menguasai dunia.Jadi gak usah kuliah ya?No, the answer is really a BIG NO if you don’t have any plan for your life, or at least you are a son of a *****! (baca: Noble, artinya Google Translate aja ya :p)Kenapa?Masalahnya tidak semua orang adalah anak seorang ningrat atau tidak sejenius Bill Gates ataupun Mark Zuckerberg.Banyak orang kira Drop Out kuliah bisa membuat hidup lebih baik. Namun setelah Drop Out mereka tidak tahu harus apa setelah selesai kuliah. Ujung-ujungnya kerja pontang-panting atau ikut orang, sementara gaji pas-pasan. Syukur-syukur bisa dapet UMK, malah yang terjadi banyak juga yang hanya bisa menerima gaji dibawahnya. Akhirnya hanya bisa menyesal karena apa yang didapatkan tidak semanis apa yang diimpikan semasa kuliah dulu.Lalu bagaimana dengan fenomena pengangguran intelektual? Ya, gak boleh nutup mata juga sama realita banyaknya lulusan S1 yang masih nganggur. Entah karena susah dapet kerja, ato memang males kerja dengan dalih “istirahat” eh malah kebablasan sampe gak kerasa udah 5 tahun nganggur.Justru seharusnya kita bisa berpikir jernih, lha wong yang udah S1 aja masih bisa nganggur kok, apalagi yang nggak punya “title”.Dari situlah kita akan tahu kenapa kuliah itu menjadi penting.Di kampus, kita akan dituntut untuk berpikir lebih kritis dan rasional dibandingkan sekolah. Kita dituntut untuk bisa mempresentasikan hasil pekerjaan kita sesuai bahasa kita sendiri. Puncaknya adalah saat tugas akhir atau skripsi, dimana kita dituntut untuk menenun ribuan kata dengan bahasa tingkat tinggi yang mungkin kita sendiri tidak mengerti arti sebenarnya, hingga mempresentasikannya di hadapan para dosen penguji yang siap menerkam kita kapan saja. Belum lagi ditambah tuntutan untuk aktif kegiatan kemahasiswaan dan berorganisasi yang manfaatnya baru akan terasa ketika nanti memimpin sebuah tim ataupun perusahaan.Ambil contoh, Bu Susi yang menjadi Menteri padahal SMA aja belum tamat. Pertanyaannya, berapa banyak yang tahu jika beliau tidak tamat bangku sekolah bukan karena bodoh atau sengaja OD? Berapa banyak yang tahu seberapa besar perjuangannya untuk belajar berbagai hal secara otodidak? Karena beliau tahu keterbatasan pendidikannya mau tidak mau membuatnya harus berusaha lebih keras agar bisa “survive” dalam kehidupan ini.Ambil lagi contoh lain, Mark Zuckerberg saat di kampus juga selalu bereksperimen hingga akhirnya menciptakan Facebook. Kesuksesannya bukan karena gelar, tapi karena pada saat di kampus dia terus mengasah bakat dan kemampuannya. Beruntungnya juga dia bisa menemukan orang-orang yang kini menjadi co-founder, itu pun semua juga berawal di lingkungan kampus. Bahkan dia berusaha membangun websitenya lebih baik karena dia tahu di kampusnya ada kompetitor.Kalo ngeliat contoh-contoh diatas berarti kita cukup kuliah aja gak perlu sampai tamat, kan?Oke, memang itu adalah contoh orang-orang yang kuliah tapi tidak tamat. Tapi pertanyaannya kenapa harus putus di tengah jalan jika bisa ditamatkan?Jangan patah semangat dulu. Sekarang, bagaimana kalau kita ambil contoh orang-orang yang menyelesaikan kuliahnya dan bisa sukses besar?Mungkin kita tidak akan mengenal konsep transportasi “hyperloop” yang muncul dari Elon Musk, seorang Tony Stark (Iron Man) versi dunia nyata yang menyelesaikan dua kali kuliah S1 (Ekonomi dan Fisika) sebelum memulai berbagai usahanya yang sangat berhubungan erat dengan Fisika. Mungkin kita tidak akan mengenal berbagai teknologi komputer yang bisa dibilang dimotori oleh Hewlett-Packard yang pertemanan keduanya berawal dari kegiatan bersama sesudah lulus dari kampus. Lebih nyesek lagi, kita tidak akan bisa “googling” yang sangat membantu di saat kebingungan, karena Google pun juga berawal dari Tugas Akhir di kampus.Persamaan dari orang-orang yang sukses besar tanpa selesai kuliah maupun yang selesai kuliah adalah mereka berpikir kritis, rasional, dan terus berusaha mengembangkan keahlian dan keterampilannya dengan berkarya ketika mereka masih berada di lingkungan kampus.Intinya adalah, kuliah memberikan kesempatan untuk membentuk pola pikir kritis dan rasional.Namun sebaliknya, pola pikir kritis dan rasional tidak akan terbentuk selama kuliah tidak dianggap penting.Bersyukurlah jika bisa berkuliah, karena masih banyak saudara kita di dalam negeri yang belum bisa mengenyam bangku kuliah. Meskipun ada berbagai alasan yang bikin males kuliah, tetap saja itu tidak berarti kuliah jadi gak penting. Percayalah bahwa kuliah itu masih memiliki lebih banyak manfaat daripada tidak pernah kuliah sama sekali. Dan buat yang pengen cari kerja, bicara realita saja sih, ini Indonesia dimana pekerjaan bergaji tinggi tetap lebih mudah didapatkan dengan gelar S1 dibandingkan yang hanya lulusan SMA. Kalo masih gak percaya, bandingin aja sendiri di situs-situs lowongan kerja online.Bahasa gampangnya sih, kalo kamu merasa udah pinter kayak om Bill Gates atau Mark Zuckerberg ya it’s okay kalo mau OD karena merasa kuliah udah gak penting lagi. Cuma, resiko ya ditanggung sendiri loh. Tapi kalo kamu pengen jadi kayak Elon Musk, lanjutlah kuliah yang serius dan tamatin. It’s your choice :)—Photo Courtesy of Dead Mailbox Society.Referensi:fundersandfounders.comwww.buzzfeed.commustsharenews.comtime.comalumni.stanford.eduwww.tribunnews.comnasional.tempo.coDisclaimer: I’m not saying that I prefer OD. I just wanna invite people to open their mind about the truth behind the myths about Drop Out and the relationship with Success People.PS: Berhubung ini tulisan pertama di Medium, jadi mohon maaf kalo masih agak “semrawut” dan dengan senang hati saya terima segala masukan yang membangun. Tapi maaf, bagi yang hanya ingin “buang kotoran” disini rasanya perlu disekolahkan lagi karena banyak toilet masih buka kok. :D",02/05/2017,0,3.0,9.0,918.0,775.0,2.0,0.0,0.0,12.0,id
3892,Word embeddings in 2020. Review with code examples,Towards Data Science,Rostyslav Neskorozhenyi,115.0,13.0,1607.0,"In this article we will study word embeddings — digital representation of words suitable for processing by machine learning algorithms.Originally I created this article as a general overview and compilation of current approaches to word embedding in 2020, which our AI Labs team could use from time to time as a quick refresher. I hope that my article will be useful to a wider circle of data scientists and developers. Each word embedding method in the article has a (very) short description, links for further study, and code examples in Python. All code is packed as Google Colab Notebook. So let’s begin.According to Wikipedia, Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.The most basic method for transforming words into vectors is to count occurrence of each word in each document. Such approach is called countvectorizing or one-hot encoding.The main principle of this method is to collect a set of documents (they can be words, sentences, paragraphs or even articles) and count the occurrence of every word in each document. Strictly speaking, the columns of the resulting matrix are words and the rows are documents.Another approach in countvectorizing is just to place 1 if the word is found in the document (no matter how often) and 0 if the word is not found in the document. In this case we get real ‘one-hot’ encoding.With a large corpus of documents some words like ‘a’, ‘the’, ‘is’, etc. occur very frequently but they don’t carry a lot of information. Using one-hot encoding approach we can decide that these words are important because they appear in many documents. One of the ways to solve this problem is stopwords filtering, but this solution is discrete and not flexible.TF-IDF (term frequency — inverse document frequency) can deal with this problem better. TF-IDF lowers the weight of commonly used words and raises the weight of rare words that occur only in current document. TF-IDF formula looks like this:Where TF is calculated by dividing number of times the word occurs in the document by the total number of words in the documentIDF (inverse document frequency), interpreted like inversed number of documents, in which the term we’re interested in occurs. N — number of documents, n(t) — number of documents with current word or term t.The most commonly used models for word embeddings are word2vec and GloVe which are both unsupervised approaches based on the distributional hypothesis (words that occur in the same contexts tend to have similar meanings).Word2Vec word embeddings are vector representations of words, that are typically learnt by an unsupervised model when fed with large amounts of text as input (e.g. Wikipedia, science, news, articles etc.). These representation of words capture semantic similarity between words among other properties. Word2Vec word embeddings are learnt in a such way, that distance between vectors for words with close meanings (“king” and “queen” for example) are closer than distance for words with complety different meanings (“king” and “carpet” for example).Word2Vec vectors even allow some mathematic operations on vectors. For example, in this operation we are using word2vec vectors for each word:king — man + woman = queenCheck how similar are vectors for words ‘woman’ and ‘man’.Check how similar are vectors for words ‘king’ and ‘woman’.Another word embedding method is Glove (“Global Vectors”). It is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. Then this matrix is factorized to a lower-dimensional (word x features) matrix, where each row now stores a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.Find similarity between King and Queen (higher value is better).Find similarity between King and carpet.Check if king — man + woman = queen. We will multiply vectors for ‘man’ and ‘woman’ by two, because subtracting one vector for ‘man’ and adding the vector for ‘woman’ will do little to the original vector for “king”, likely because those “man” and “woman” are related themselves.FastText is an extension of word2vec. FastText was developed by the team of Tomas Mikolov who created the word2vec framework in 2013.The main improvement of FastText over the original word2vec vectors is the inclusion of character n-grams, which allows computing word representations for words that did not appear in the training data (“out-of-vocabulary” words).Create an embedding for the word ‘king’.Get most similar words for the word ‘king’.Test model ability to create vectors for unknown words.Get most similar words for unknown word ‘king-warrior’.Unlike traditional word embeddings such as word2vec and GLoVe, the ELMo vector assigned to a token or a word depends on current context and is actually a function of the entire sentence containing that word. So the same word can have different word vectors under different contexts. Also ELMo representations are purely character based so they are not limited to any predefined vocabulary.Description from the official site:ELMo is a deep contextualized word representation that models both (1) complex characteristics of the word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.In order to send sentences to the model we need to split them into the arrays of words and pad arrays to the same length. Also we will create ‘mask’ array that will show whether element is a real word or a padding symbol (in our case — ‘_’). We will use ‘mask’ array for visualization later to show only real words.Create embeddings with ELMo:Convert Tensorflow tensors to numpy array.Visualize embeddings using PCA.At last it’s time for current state-of-the-art approach — Transformers. Famous GPT-2, BERT, CTRL are all Transformers-based and produce context-sensitive embeddings like ELMo. But unlike ELMo Transformers do not use RNN, they do not require to process words in sentence sequentially one-by-one. All words in the sentence are processed in parallel, this approach speeds up processing and solves vanishing gradient problem.Transformers use the attention mechanizm to describe the connections and dependencies of each specific word with all other words in the sentence. This mechanism and the main principles of Transformers described in detail in a beautifully illustrated article by Jay Alammar.For our example we will use brilliant Transformers library which contains the latest Transformers-based models (such as BERT, XLNet, DialoGPT or GPT-2).Let’s make some embeddings with BERT. Firstly we will need to install Transformers library.Now we import pytorch, the pretrained BERT model, and a BERT tokenizer that will do all the needed work of converting sentences into format appropriate for BERT (tokenizing itself and adding special tokens like [SEP] and [CLS]).Enter some sentences and tokenize them.Note that some tokens may look like this: [‘aa’, ‘##th’, ‘##ur’, ‘pen’, ‘##dra’, ‘##gon’]. This is because of the BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. BERT tokenizer uses vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. So, if the word is not mentioned in a vocabulary, that words is splitted into subwords and characters. The two hash signs (##) before some subwords shows that subword is part of a larger word and preceded by another subword.We will use tokenizer.encode_plus function, that will:Segment ID. BERT is trained on and expects sentence pairs using 1s and 0s to distinguish between the two sentences. We will encode each sentence separately so we will just mark each token in each sentence with 1.Now we can call BERT model and finally get model hidden states from which we will create word embeddings.Let’s examine what we’ve got.We will use last four hidden layers to create each word embedding.Concatenate four layers for each token to create embeddingsLet’s examine embeddings for the first sentence. Firstly we need to get ids of tokens we need to compare.We can see that word ‘king’ is placed at indexes 1 and 17. We will check distance between embeddings 1 and 17. Also, we will check if embedding for the word ‘arthur’ is closer to ‘king’ then to the word ‘table’.So we see that embeddings for two ‘kings’ are quite similar but not the same, and Archtur is closer to be a king than a table.Things may be simplier with simplerepresentations module. This module does all the work we did earlier — extracts needed hidden states from BERT and creates embeddings in a few lines of code.Check distaces between Archtur, king and table.Same results, less code.I hope that after reading this article you have formed an idea of the current approaches to word embeddings and began to understand how to quickly implement these approaches in Python. The world of NLP is diverse and there are many more models and methods for embeddings. In my article I focused on the most common and those that we ourselves often use in our work. You can find additional information in the References section.",24/07/2020,34,7.0,0.0,535.0,186.0,26.0,2.0,0.0,44.0,en
3893,Autoencoders — Introduction and Implementation in TF.,Towards Data Science,Manish Chablani,1700.0,4.0,597.0,"Autoencoders (AE) are a family of neural networks for which the input is the same as the output (they implement a identity function). They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation.A really popular use for autoencoders is to apply them to images. The trick is to replace fully connected layers by convolutional layers. These, along with pooling layers, convert the input from wide and thin (let’s say 100 x 100 px with 3 channels — RGB) to narrow and thick. This helps the network extract visual features from the images, and therefore obtain a much more accurate latent space representation. The reconstruction process uses upsampling and convolutions.The resulting network is called a Convolutional Autoencoder (CAE).Convolutional autoencoders can be useful for reconstruction. They can, for example, learn to remove noise from picture, or reconstruct missing parts.To do so, we don’t use the same image as input and output, but rather a noisy version as input and the clean version as output. With this process, the networks learns to fill in the gaps in the image.Let’s see what a CAE can do to replace part of an image of an eye. Let’s say there’s a crosshair and we want to remove it. We can manually create the dataset, which is extremely convenient.Now that our autoencoder is trained, we can use it to remove the crosshairs on pictures of eyes we have never seen!Lets go over a sample implementation using MNIST dataset in tensorflow.Notebook: https://github.com/mchablani/deep-learning/blob/master/autoencoder/Convolutional_Autoencoder.ipynbThe encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers. The decoder needs to convert from a narrow representation to a wide reconstructed image.Usually, you’ll see transposed convolution layers used to increase the width and height of the layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3x3 kernel, a 3x3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a 3x3 path in a transposed convolution layer. The TensorFlow API provides us with an easy way to create the layers, tf.nn.conv2d_transpose.However, transposed convolution layers can lead to artifacts in the final images, such as checkerboard patterns. This is due to overlap in the kernels which can be avoided by setting the stride and kernel size equal. In this Distill article from Augustus Odena, et al, the authors show that these checkerboard artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer. In TensorFlow, this is easily done with tf.image.resize_images, followed by a convolution. Odena et al claim that nearest neighbor interpolation works best for the upsamplingAutoencoders can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1. We’ll use noisy images as input and the original, clean images as targets.Note we are using sigmoid_cross_entropy_with_logits for loss. According to TF documentation: It measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.Model definition:Training:Credits: https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694",26/06/2017,2,12.0,10.0,1400.0,401.0,2.0,0.0,0.0,6.0,en
3894,Getting Started With Google Colab,Towards Data Science,Anne Bonner,6200.0,8.0,1250.0,"You know it’s out there. You know there’s free GPU somewhere, hanging like a fat, juicy, ripe blackberry on a branch just slightly out of reach.Beautiful lightning-fast speed waiting just for you.Wondering how on earth to get it to work? You’re in the right place!For anyone who doesn’t already know, Google has done the coolest thing ever by providing a free cloud service based on Jupyter Notebooks that supports free GPU. Not only is this a great tool for improving your coding skills, but it also allows absolutely anyone to develop deep learning applications using popular libraries such as PyTorch, TensorFlow, Keras, and OpenCV.Colab provides GPU and it’s totally free. Seriously!There are, of course, limits. (Nitty gritty details are available on their faq page, of course.) It supports Python 2.7 and 3.6, but not R or Scala yet. There is a limit to your sessions and size, but you can definitely get around that if you’re creative and don’t mind occasionally re-uploading your files…Colab is ideal for everything from improving your Python coding skills to working with deep learning libraries, like PyTorch, Keras, TensorFlow, and OpenCV. You can create notebooks in Colab, upload notebooks, store notebooks, share notebooks, mount your Google Drive and use whatever you’ve got stored in there, import most of your favorite directories, upload your personal Jupyter Notebooks, upload notebooks directly from GitHub, upload Kaggle files, download your notebooks, and do just about everything else that you might want to be able to do.It’s awesome.Working in Google Colab for the first time has been totally phenomenal and pretty shockingly easy, but it hasn’t been without a couple of small challenges! If you know Jupyter Notebooks at all, you’re pretty much good to go in Google Colab, but there are just a few little differences that can make the difference between flying off to freedom on the wings of free GPU and sitting at your computer, banging your head against the wall…This article is for anyone out there who is confused, frustrated, and just wants this thing to work!Create a folder for your notebooks(Technically speaking, this step isn’t totally necessary if you want to just start working in Colab. However, since Colab is working off of your drive, it’s not a bad idea to specify the folder where you want to work. You can do that by going to your Google Drive and clicking “New” and then creating a new folder. I only mention this because my Google Drive is embarrassingly littered with what looks like a million scattered Colab notebooks and now I’m going to have to deal with that.)If you want, while you’re already in your Google Drive you can create a new Colab notebook. Just click “New” and drop the menu down to “More” and then select “Colaboratory.”Otherwise, you can always go directly to Google Colab.You can rename your notebook by clicking on the name of the notebook and changing it or by dropping the “File” menu down to “Rename.”Want to use GPU? It’s as simple as going to the “runtime” dropdown menu, selecting “change runtime type” and selecting GPU in the hardware accelerator drop-down menu!You can easily start running code now if you want! You are good to go!Want to mount your Google Drive? Use:Then you’ll see a link, click on that, allow access, copy the code that pops up, paste it in the box, hit enter, and you’re good to go! If you don’t see your drive in the side box on the left, just hit “refresh” and it should show up.(Run the cell, click the link, copy the code on the page, paste it in the box, hit enter, and you’ll see this when you’ve successfully mounted your drive):Now you can see your drive right there on the left-hand side of the screen! (You may need to hit “refresh.”) Plus, you can reach your drive any time withIf you’d rather download a shared zip file link, you can use:For example:That will give you Udacity’s flower data set in seconds!If you’re uploading small files, you can just upload them directly with some simple code. However, if you want to, you can also just go to the left side of the screen and click “upload files” if you don’t feel like running some simple code to grab a local file.Google Colab is incredibly easy to use on pretty much every level, especially if you’re at all familiar with Jupyter Notebooks. However, grabbing some large files and getting a couple of specific directories to work did trip me up for a minute or two.I covered getting started with Kaggle in Google Colab in a separate article, so if that’s what interests you, please check that out!Imports are pretty standard, with a few exceptions.For the most part, you can import your libraries by running import like you do in any other notebook.PyTorch is different! Before you run any other Torch imports, you’ll want to run*** UPDATE! (01/29)*** Colab now supports native PyTorch!!! You shouldn’t need to run the code below, but I’m leaving it up just in case anyone is having any issues!Then you can continue with your imports. If you try to simply run import torch you’ll get an error message. I really recommend clicking on the extremely helpful links that pop up. If you do, you’ll get that code right away and you can just click on “INSTALL TORCH” to import it into your notebook. The code will pop up on the left-hand side of your screen, and then hit “INSERT.”Not able to simply import something else that you want with an import statement? Try a pip install! Just be aware that Google Colab wants an exclamation point before most commands.or:and:is useful too!I did find that Pillow can be sort of buggy, but you can solve that by runningIf you get anything below 5.3, go to the “runtime” dropdown menu, restart the runtime, and run the cell again. You should be good to go!It’s easy to create a new notebook by dropping “File” down to “New Python 3 Notebook.” If you want to open something specific, drop the “File” menu down to “Open Notebook…”Then you’ll see a screen that looks like this:As you can see, you can open a recent file, files from your Google Drive, GitHub files, and you can upload a notebook right there as well.The GitHub option is great! You can easily search by an organization or user to find files. If you don’t see what you’re looking for, try checking the repository drop-down menu!Saving your work is simple! You can do a good ol’ “command-s” or drop the “File” menu down to save. You can create a copy of your notebook by dropping “File” -> “Save a Copy in Drive.” You can also download your workbook by going from “File” -> “download .ipyb” or “download .py.”That should be enough to at least get you up and running on Colab and taking advantage of that sweet, sweet free GPU! Please let me know if you run into any other newbie problems that I might be able to help you with. I’d love to help you if I can!If you want to reach out or find more cool articles, please come and join me at Content Simplicity!If you’re new to data science, machine learning, and artificial intelligence, you might want to check out the ultimate beginner’s guide to NumPy!towardsdatascience.comOr you might be interested in one of these!Or maybe you want to know how to get your own posts noticed!medium.comThanks for reading! ❤️",01/01/2019,9,15.0,0.0,1128.0,655.0,20.0,1.0,0.0,24.0,en
3895,Your Guide to Natural Language Processing (NLP),Towards Data Science,Diego Lopez Yse,842.0,13.0,2471.0,"Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value extracted from it. In theory, we can understand and even predict human behaviour using that information.But there is a problem: one person may generate hundreds or thousands of words in a declaration, each sentence with its corresponding complexity. If you want to scale and analyze several hundreds, thousands or millions of people or declarations in a given geography, then the situation is unmanageable.Data generated from conversations, declarations or even tweets are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate. Nevertheless, thanks to the advances in disciplines like machine learning a big revolution is going on regarding this topic. Nowadays it is no longer about trying to interpret a text or speech based on its keywords (the old fashioned mechanical way), but about understanding the meaning behind those words (the cognitive way). This way it is possible to detect figures of speech like irony, or even perform sentiment analysis.Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.It is a discipline that focuses on the interaction between data science and human language, and is scaling to lots of industries. Today NLP is booming thanks to the huge improvements in the access to data and the increase in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let’s mention some examples:NLP is particularly booming in the healthcare industry. This technology is improving care delivery, disease diagnosis and bringing costs down while healthcare organizations are going through a growing adoption of electronic health records. The fact that clinical documentation can be improved means that patients can be better understood and benefited through better healthcare. The goal should be to optimize their experience, and several organizations are already working on this.Companies like Winterlight Labs are making huge improvements in the treatment of Alzheimer’s disease by monitoring cognitive impairment through speech and they can also support clinical trials and studies for a wide range of central nervous system disorders. Following a similar approach, Stanford University developed Woebot, a chatbot therapist with the aim of helping people with anxiety and other disorders.But serious controversy is around the subject. A couple of years ago Microsoft demonstrated that by analyzing large samples of search engine queries, they could identify internet users who were suffering from pancreatic cancer even before they have received a diagnosis of the disease. How would users react to such diagnosis? And what would happen if you were tested as a false positive? (meaning that you can be diagnosed with the disease even though you don’t have it). This recalls the case of Google Flu Trends which in 2009 was announced as being able to predict influenza but later on vanished due to its low accuracy and inability to meet its projected rates.NLP may be the key to an effective clinical support in the future, but there are still many challenges to face in the short term.The main drawbacks we face these days with NLP relate to the fact that language is very tricky. The process of understanding and manipulating language is extremely complex, and for this reason it is common to use different techniques to handle different challenges before binding everything together. Programming languages like Python or R are highly used to perform these techniques, but before diving into code lines (that will be the topic of a different article), it’s important to understand the concepts beneath them. Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms:Is a commonly used model that allows you to count all words in a piece of text. Basically it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles:Words are flowing out like endless rain into a paper cup,They slither while they pass, they slip away across the universeNow let’s count the words:This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”).To solve this problem, one approach is to rescale the frequency of words by how often they appear in all texts (not just the one we are analyzing) so that the scores for frequent words like “the”, that are also frequent across other texts, get penalized. This approach to scoring is called “Term Frequency — Inverse Document Frequency” (TFIDF), and improves the bag of words by weights. Through TFIDF frequent terms in the text are “rewarded” (like the word “they” in our example), but they also get “punished” if those terms are frequent in other texts we include in the algorithm too. On the contrary, this method highlights and “rewards” unique or rare terms considering all texts. Nevertheless, this approach still has no context nor semantics.Is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. Following our example, the result of tokenization would be:Pretty simple, right? Well, although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign phrases (e.g. laissez faire).Tokenization can remove punctuation too, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks.For deeper details on tokenization, you can find a great explanation in this article.Includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time.There is no universal list of stop words. These can be pre-selected or built from scratch. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on. Nevertheless it seems that the general trend over the past time has been to go from the use of large standard stop word lists to the use of no lists at all.The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. For example, if we are performing a sentiment analysis we might throw our algorithm off track if we remove a stop word like “not”. Under these conditions, you might select a minimal stop word list and add additional terms depending on your specific objective.Refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).The problem is that affixes can create or expand new forms of the same word (called inflectional affixes), or even create new words themselves (called derivational affixes). In English, prefixes are always derivational (the affix creates a new word as in the example of the prefix “eco” in the word “ecosystem”), but suffixes can be derivational (the affix creates a new word as in the example of the suffix “ist” in the word “guitarist”) or inflectional (the affix creates a new form of word as in the example of the suffix “er” in the word “faster”).Ok, so how can we tell the difference and chop the right bit?A possible approach is to consider a list of common affixes and rules (Python and R languages have different libraries containing affixes and methods) and perform stemming based on them, but of course this approach presents limitations. Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To offset this effect you can edit those predefined methods by adding or removing affixes and rules, but you must consider that you might be improving the performance in one area while producing a degradation in another one. Always look at the whole picture and test your model’s performance.So if stemming has serious limitations, why do we use it? First of all, it can be used to correct spelling errors from the tokens. Stemmers are simple to use and run very fast (they perform simple operations on a string), and if speed and performance are important in the NLP model, then stemming is certainly the way to go. Remember, we use it with the objective of improving our performance, not as a grammar exercise.Has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best” is changed to “good”), hence standardizing words with similar meaning to their root. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words.Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.For example, the words “running”, “runs” and “ran” are all forms of the word “run”, so “run” is the lemma of all the previous words.Lemmatization also takes into consideration the context of the word in order to solve other problems like disambiguation, which means it can discriminate between identical words that have different meanings depending on the specific context. Think about words like “bat” (which can correspond to the animal or to the metal/wooden club used in baseball) or “bank” (corresponding to the financial institution or to the land alongside a body of water). By providing a part-of-speech parameter to a word ( whether it is a noun, a verb, and so on) it’s possible to define a role for that word in the sentence and remove disambiguation.As you might already pictured, lemmatization is a much more resource-intensive task than performing a stemming process. At the same time, since it requires more knowledge about the language structure than a stemming approach, it demands more computational power than setting up or adapting a stemming algorithm.Is as a method for uncovering hidden structures in sets of texts or documents. In essence it clusters texts to discover latent topics based on their contents, processing individual words and assigning them values based on their distribution. This technique is based on the assumptions that each document consists of a mixture of topics and that each topic consists of a set of words, which means that if we can spot these hidden topics we can unlock the meaning of our texts.From the universe of topic modelling techniques, Latent Dirichlet Allocation (LDA) is probably the most commonly used. This relatively new algorithm (invented less than 20 years ago) works as an unsupervised learning method that discovers different topics underlying a collection of documents. In unsupervised learning methods like this one, there is no output variable to guide the learning process and data is explored by algorithms to find patterns. To be more specific, LDA finds groups of related words by:Unlike other clustering algorithms like K-means that perform hard clustering (where topics are disjointed), LDA assigns each document to a mixture of topics, which means that each document can be described by one or more topics (e.g. Document 1 is described by 70% of topic A, 20% of topic B and 10% of topic C) and reflect more realistic results.Topic modeling is extremely useful for classifying texts, building recommender systems (e.g. to recommend you books based on your past readings) or even detecting trends in online publications.At the moment NLP is battling to detect nuances in language meaning, whether due to lack of context, spelling errors or dialectal differences.On March 2016 Microsoft launched Tay, an Artificial Intelligence (AI) chatbot released on Twitter as a NLP experiment. The idea was that as more users conversed with Tay, the smarter it would get. Well, the result was that after 16 hours Tay had to be removed due to its racist and abusive comments:Microsoft learnt from its own experience and some months later released Zo, its second generation English-language chatbot that won’t be caught making the same mistakes as its predecessor. Zo uses a combination of innovative approaches to recognize and generate conversation, and other companies are exploring with bots that can remember details specific to an individual conversation.Although the future looks extremely challenging and full of threats for NLP, the discipline is developing at a very fast pace (probably like never before) and we are likely to reach a level of advancement in the coming years that will make complex applications look possible.Thanks Jesús del Valle , Jannis Busch and Sabrina Steinert for your valuable inputsInterested in these topics? Follow me on Linkedin or Twitter",15/01/2019,0,31.0,13.0,709.0,290.0,9.0,2.0,0.0,19.0,en
3896,PixelCNN’s Blind Spot,Towards Data Science,Jessica Dafflon,23.0,9.0,1792.0,"Written by Walter Hugo Lopez Pinaya, Pedro F. da Costa, and Jessica DafflonHi everybody! Today, we will continue the series about autoregressive models and we will focus on one of the biggest limitations of PixelCNNs (i.e., blind spots) and how to improve to fix it.SummaryFor each topic, the code is availiable in this repository.In the previous two posts, we introduced generative models, the concept behind PixelCNNs, and looked at how a coloured PixelCNN works. Recall that PixelCNNs are a type of generative models that learn the probability distribution of pixels, that means that the intensity of future pixels will be determined by previous pixels. In this blogpost series we implemented two PixelCNNs and noticed that the performance was not stellar. In the previous posts, we mentioned that one of the ways to improve the model’s performance was to fix the blind spot problem. So, in this post, we will introduce the concept of blind spot, discuss how PixelCNNs are affected, and present one solution for solving it — the Gated PixelCNN. Let’s start!As you recall from the previous posts, the PixelCNN learns the conditional distribution for all pixels in the image and uses this information to make predictions. Also recall that PixelCNNs will learn the distribution of the pixels from left to right and top to bottom. So to make sure that ‘future’ pixels (i.e., pixels to the right or below the pixel that is being predicted) cannot be used for the prediction of the given pixel, a mask is generally used (Figure 1A). As shown in Figure 1A, the mask zeros out the pixels ‘after’ the pixel currently being predicted, which corresponds to the pixel at the center of the mask. However, due to this choice, not all ‘past’ pixels will be used to compute the new point, and the loss of information will lead to the creation of blind spots.To understand the blind spot problem, let’s look at Figure 1B. In Figure 1B, the dark pink point (m) is the pixel we want to predict, as it is at the center of the filter. Because we are using a 3x3 mask (1A.), pixel m depends on l, g, h, i. On the other hand, those pixels depend on previous pixels. For example, pixel g depends on f, a, b, c, and pixel i depends on h, c, d, e. From figure 1B, we can also see that despite coming before pixel m, pixel j is never taken into account to compute the predictions for m. Similarly, if we want to make predictions for q, j, n, o are never considered (Figure 1C.). The fact that not all previous pixels will influence the prediction is called the blind spot problem.We will first start by looking at the implementation of PixelCNNs and how the blind spot will affect the results. The snippet below shows the implementation of the mask from a PixelCNN using the Tensorflow 2.0 framework.Looking at the receptive field (marked in yellow in Figure 2) of the original PixelCNN, we can see the blind spot and how it propagates over the different layers. In the second part of this blogpost, we will describe the next version of PixelCNN, the Gated PixelCNN, that introduces a new mechanism to avoid the creation of blind spots.In the last two blog posts, we introduced the PixelCNN; however, this model had low performance and suffered from the blind spot problem that we introduced above.To solve these issues, van den Oord et al. (2016) introduced the Gated PixelCNN. The Gated PixelCNN differs from the PixelCNN in two major ways:This new model solved the blind spot issue by splitting the convolution into two parts: the vertical and horizontal stacks. Let’s look into how the vertical and horizontal stacks work.Vertical and Horizontal StacksIn the vertical stack, the goal is to process the context information from all rows before the current one. The trick used to make sure that all previous information is used and that causality is kept (i.e., the currently predicted pixel should not be aware of the information to its right), is to shift the centre of the mask up by one row, respectively to the pixel being predicted. As illustrated in Figure 3, although the center of the vertical mask is the light green pixel (m), the information gathered by the vertical stack will not be used to predict it, but it will, instead, be used to predict the pixel in the row below it (r).However, using the vertical stack alone would create blind spots to the left of the black predicted pixel (m). To avoid this, the information gathered by the vertical stack is combined with information from the horizontal stack (p-q represented in blue in Figure 3), which predicts all pixels to the left of the predicted pixel (m). The combination between the horizontal and vertical stack solves two problems: (1) no information on the right of the predicted pixel will be used, (2) because we take into consideration as a block, we no longer have a blind spot.In van den Oord et al. (2016), the vertical stack is implemented so that the receptive field of each convolution has a 2x3 format. We implemented this by using a 3x3 convolution with the last row masked out. In the horizontal stack, the convolution layers associate the predicted value with the data from the current row of the pixel analysed. This can be implemented using a 1x3 convolution where we mask the future pixels to guarantee the causality condition of the autoregressive models. Similar to the PixelCNN, we implemented a type A mask (that is used in the first layer) and a type B mask (used in the subsequent layers).We adapted our previous masked convolutional layers to be able to implement these new configurations. The snippet below shows the implementation of the mask using the Tensorflow 2.0 framework.By adding the feature maps of these two stacks across the network, we get an autoregressive model with a consistent receptive field and does not produce blind spots (Figure 4).The second major improvement from the vanilla PixelCNNs to the Gated CNNs is the introduction of gated blocks and multiplicative units (in the form of the LSTM gates). Therefore, instead of using the rectified linear units (ReLUs) between the masked convolutions, like the original pixelCNN; Gated PixelCNN uses gated activation units to model more complex interactions between features. This gated activation units use sigmoid (as a forget gate) and tanh (as real activation). In the original paper, the authors suggested that this could be one reason PixelRNN (that used LSTM’s) outperformed PixelCNN as they are able to better capture the past pixels by means of recurrence — they can memorize past information. Therefore, Gated PixelCNN used the following:σ is the sigmoid non-linearity, k is the number of the layer, ⊙ is the element-wise product, ∗ is the convolution operator, and W are the weights from the previous layer. Let’s look in more detail at a single layer in the PixelCNN.The stacks and gates are the fundamental blocks of the Gated PixelCNN (Figure 5). But how are they connected, and how will the information be processed? We will break this down into 4 processing steps, which we will discuss in the sessions below.1. Calculate the vertical stack features mapsAs a first step, the input from the vertical stack is processed by our 3x3 convolution layer with the vertical mask. Then, the resulting feature maps pass through the gated activation units and are inputted in the next block’s vertical stack.2. Feeding vertical maps into horizontal stackFor our autoregressive model, it is necessary to combine the information of both vertical and horizontal stacks. For this reason, in each block, the vertical stack is also used as one of the inputs to the horizontal layer. Since the centre of each convolutional step of the vertical stack corresponds to the analysed pixel, we cannot just add the vertical information. This would break the causality condition of the autoregressive models as it would allow information of future pixels to be used to predict values in the horizontal stack. This is the case on the second illustration in Figure 8A, where the pixels on the right (or future) of the black pixel are used to predict it. For this reason, before we feed the vertical information to the horizontal stack, we shift it down using padding and cropping (Figure 8B.). By zero-padding the image and cropping the bottom of the image, we can ensure that the causality between the vertical and horizontal stack is maintained. We will delve into more detail about how cropping works in future posts, so do not worry if its details are not completely clear.3. Calculate horizontal feature mapsIn this step, we process the feature maps of the horizontal convolutional layer. In fact, the first step consists of summing the feature maps from the vertical to the outputs of the horizontal convolution layer. The output of this combination has the ideal receptive format, which considers the information of all previous pixels. Finally, the feature maps go through the gated activation units.4. Calculate the residual connection on the horizontal stackIn this last step, if the block is not the first one of the network, a residual connection will combine the output of the previous step (processed by a 1x1 convolution) and then fed into the horizontal stack of the next block. If it is the first block of the network, then there is no residual connection, and this step is skipped.Using Tensorflow 2. we implemented the scheme above as following:In summary, using the gated block, we solved the blind spots on the receptive field and improved the model performance.In Oord et al. 2016, the PixelCNN uses the following architecture: the first layer is a masked convolution (type A) with 7x7 filters. Then, 15 residuals blocks were used. Each block process the data with a combination of 3x3 layers convolutional layers with mask type B and standard 1x1 convolutional layers. Between each convolutional layer, there is a non-linearity ReLU. Finally, the residual blocks also included a residual connection.In the next post, we will take a look at how to improve even further the performance of the Gated PixelCNN. We will also introduce the conditional PixelCNN, so stay tuned!We trained both a PixelCNN and a Gated PixelCNN and compare the results below.When comparing the MNIST prediction for PixelCNN and Gated PixelCNN (Figure 11), we do not observe a great improvement for this dataset on the MNIST. Some of the numbers that were previously corrected predicted are now incorrectly predicted. However, this does not mean that PixelCNNs should not be taken into account. In the next blog post, we will discuss Gated PixelCNNs and PixelCNN++ and how they will improve the model’s performance. So stay tuned!https://youtu.be/1BURwCCYNEIhttps://openreview.net/pdf?id=Hyvw0L9elhttps://www.slideshare.net/suga93/conditional-image-generation-with-pixelcnn-decodershttps://sergeiturukin.com/2017/02/24/gated-pixelcnn.html",09/09/2021,0,7.0,22.0,1220.0,858.0,12.0,2.0,0.0,19.0,en
3897,Simple Introduction to Convolutional Neural Networks,Towards Data Science,"Matthew Stewart, PhD Researcher",5900.0,9.0,1457.0,"In this article, I will explain the concept of convolution neural networks (CNN’s) using many swan pictures and will make the case of using CNN’s over regular multilayer perceptron neural networks for processing images.Image AnalysisLet us assume that we want to create a neural network model that is capable of recognizing swans in images. The swan has certain characteristics that can be used to help determine whether a swan is present or not, such as its long neck, its white color, etc.For some images, it may be more difficult to determine whether a swan is present, consider the following image.The features are still present in the above image, but it is more difficult for us to pick out these characteristic features. Let us consider some more extreme cases.At least the color is consistent, right? Or is it…Can it get any worse? It definitely can.OK, enough with the swan pictures now. Let’s talk about neural networks. We’ve been basically talking about detecting features in images, in a very naïve way. Researchers built multiple computer vision techniques to deal with these issues: SIFT, FAST, SURF, BRIEF, etc. However, similar problems arose: the detectors were either too general or too over-engineered. Humans were designing these feature detectors, and that made them either too simple or hard to generalize.Representation Learning is a technique that allows a system to automatically find relevant features for a given task. Replaces manual feature engineering. There are several techniques for this:The Problem with Traditional Neural NetworksI will assume that you are already familiar with traditional neural networks called the multilayer perceptron (MLP). If you are not familiar with these, there are hundreds of tutorials on Medium outlining how MLPs work. These are modeled on the human brain, whereby neurons are stimulated by connected nodes and are only activated when a certain threshold value is reached.There are several drawbacks of MLP’s, especially when it comes to image processing. MLPs use one perceptron for each input (e.g. pixel in an image, multiplied by 3 in RGB case). The amount of weights rapidly becomes unmanageable for large images. For a 224 x 224 pixel image with 3 color channels there are around 150,000 weights that must be trained! As a result, difficulties arise whilst training and overfitting can occur.Another common problem is that MLPs react differently to an input (images) and its shifted version — they are not translation invariant. For example, if a picture of a cat appears in the top left of the image in one picture and the bottom right of another picture, the MLP will try to correct itself and assume that a cat will always appear in this section of the image.Clearly, MLPs are not the best idea to use for image processing. One of the main problems is that spatial information is lost when the image is flattened into an MLP. Nodes that are close together are important because they help to define the features of an image. We thus need a way to leverage the spatial correlation of the image features (pixels) in such a way that we can see the cat in our picture no matter where it may appear. In the below image, we are learning redundant features. The approach is not robust, as cats could appear in yet another position.Enter the Convolutional Neural NetworkI hope the case is clear why MLPs are a terrible idea to use for image processing. Now let us move on and discuss how CNN’s can be used to solve most of our problems.We analyze the influence of nearby pixels by using something called a filter. A filter is exactly what you think it is, in our situation, we take a filter of a size specified by the user (a rule of thumb is 3x3 or 5x5) and we move this across the image from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation.A filter could be related to anything, for pictures of humans, one filter could be associated with seeing noses, and our nose filter would give us an indication of how strongly a nose seems to appear in our image, and how many times and in what locations they occur. This reduces the number of weights that the neural network must learn compared to an MLP, and also means that when the location of these features changes it does not throw the neural network off.If you are wondering how the different features are learned by the network, and whether it is possible that the network will learn the same features (having 10 nose filters would be kind of redundant), this is highly unlikely to happen. When building the network, we randomly specify values for the filters, which then continuously update themselves as the network is trained. It is very very unlikely that two filters that are the same will be produced unless the number of chosen filters is extremely large.Some examples of filters, or kernels as we call them, are given below.After the filters have passed over the image, a feature map is generated for each filter. These are then taken through an activation function, which decides whether a certain feature is present at a given location in the image. We can then do a lot of things, such as adding more filtering layers and creating more feature maps, which become more and more abstract as we create a deeper CNN. We can also use pooling layers in order to select the largest values on the feature maps and use these as inputs to subsequent layers. In theory, any type of operation can be done in pooling layers, but in practice, only max pooling is used because we want to find the outliers — these are when our network sees the feature!Just to reiterate what we have found so far. We know that MLPs:The general idea of CNN’s is to intelligently adapt to the properties of images:CNN’s are also composed of layers, but those layers are not fully connected: they have filters, sets of cube-shaped weights that are applied throughout the image. Each 2D slice of the filters are called kernels. These filters introduce translation invariance and parameter sharing. How are they applied? Convolutions!A good question to have right now is what happens at the edges of the image? If we apply convolutions on a normal image, the result will be down-sampled by an amount depending on the size of the filter. What do we do if we don’t want this to happen? We can use padding.PaddingPadding essentially makes the feature maps produced by the filter kernels the same size as the original image. This is very useful for deep CNN’s as we don’t want the output to be reduced so that we only have a 2x2 region left at the end of the network upon which to predict our result.How do we connect our filters together?If we have many feature maps, how are these combined in our network to help us get our final result?To be clear, each filter is convolved with the entirety of the 3D input cube but generates a 2D feature map.In a convolutional layer, we are basically applying multiple filters at over the image to extract different features. But most importantly, we are learning those filters! One thing we’re missing: non-linearity.Introducing ReLUThe most successful non-linearity for CNN’s is the Rectified Non-Linear unit (ReLU), which combats the vanishing gradient problem occurring in sigmoids. ReLU is easier to compute and generates sparsity (not always beneficial).Comparison of Different LayersThere are three types of layers in a convolutional neural network: convolutional layer, pooling layer, and fully connected layer. Each of these layers has different parameters that can be optimized and performs a different task on the input data.Convolutional layers are the layers where filters are applied to the original image, or to other feature maps in a deep CNN. This is where most of the user-specified parameters are in the network. The most important parameters are the number of kernels and the size of the kernels.Pooling layers are similar to convolutional layers, but they perform a specific function such as max pooling, which takes the maximum value in a certain filter region, or average pooling, which takes the average value in a filter region. These are typically used to reduce the dimensionality of the network.Fully connected layers are placed before the classification output of a CNN and are used to flatten the results before classification. This is similar to the output layer of an MLP.What do CNN layers learn?To see a 3D example of a CNN working in practice, check out the following link here.For updates on new blog posts and extra content, sign up for my newsletter.mailchi.mp",27/02/2019,0,7.0,0.0,1343.0,698.0,20.0,6.0,0.0,2.0,en
3898,Practical implementation of outlier detection in python,Towards Data Science,Md Sohel Mahmood,121.0,7.0,823.0,"Outliers, one of the buzzwords in the manufacturing industry, has driven engineers and scientists to develop newer algorithms as well as robust techniques for continuous quality improvement. If the data include even if one outlier, it has the potential to dramatically skew the calculated parameters. Therefore, it is of utmost importance to analyze the data without those deviant points. It is also important to understand which of the data points are considered as outliers. Extreme data points do not always necessarily mean those are outliers.In this article, I will discuss the algorithm and the python implementation for three different outlier detection techniques. Those are Interquartile (IQR) method, Hampel method and DBSCAN clustering method.Inter quartile range (IQR) methodEach dataset can be divided into quartiles. The first quartile point indicates that 25% of the data points are below that value whereas second quartile is considered as median point of the dataset. The inter quartile method finds the outliers on numerical datasets by following the procedure belowThe concept of quartiles and IQR can best be visualized from the boxplot. It has the minimum and maximum point defined as Q1–1.5*IQR and Q3+1.5*IQR respectively. Any point outside this range is outlier.IQR in pythonI will take a dataset with Airbnb data from Kaggle. The dataset contains listings of thousands of Airbnb rentals with price, rating, type and so on. I will focus on the numerical price value of the rentals and create a function that can be applicable to any numerical data frame column. Let’s begin.This boxplot shows a number of outliers in several segment of rental types.As seen in the boxplot, the majority of the outliers are removed. One can also perform this IQR method in individual rental type and that will remove all the deviant points and result in a cleaner boxplot.Hampel methodThis method applies Hampel’s filter to the data to detect outlier. The process of finding the outlier is below.Hampel method in pythonI used the same dataset’s price column to find the outliers.DBSCANDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. I would like to apply this clustering algorithm to find out outlier in the same dataset. This algorithm performs better when there are data points having cluster of similar density. This method tends to group the data points together which are closely located, considering those as neighbors. Python’s sklearn.cluster has the class implementation of DBSCAN which takes two important arguments. The first and the most important one is the eps value which is the maximum distance between the data points that can be considered as neighbors. There should be an optimum value need to be chosen for eps. This publication [1] provided the procedure to find the optimum value where eps values are plotted against data points. At some point, the eps value shows the highest change in the slope and that’s the most optimum value. The second important argument is the min_samples which is the minimum number of data points that should be inside a group to be considered as a cluster. Higher the min_samples given as input, less the number of clusters and vice versa [2].Our Airbnb price data has some high-end rentals that could be considered as outliers but the fundamental difference between DBSCAN and IQR or Hampel is those high-end rentals can also form a cluster given that the minimum number of data points are there. Let’s see the code for DBSCAN.DBSCAN in pythonConclusionIQR or Hampel method are very successful for extreme outliers with a single pattern whereas DBSCAN is a better choice if we have data of different patterns. Let’s say if we have a linear data as well as a circular data, DBSCAN will be able to differentiate the samples into different groups. In our case, some extreme high-end rentals are grouped together and form a cluster. This cluster then is isolated from some other data points which have smaller rent value (considered as outlier in this method but good data points in IQR of Hampel method). Again, one needs to figure out what is the requirement and apply the best method. As mentioned earlier, some extreme data points are not always outliers. Consider the following scatterplot with the linear fit. It does not seem to have any outlier.Now let’s have the same scatterplot with an extreme data point.The point is outside the main distribution but lies on the fitting line very well. It may not be an outlier but an extreme data reading. This kind of outliers can be included to make a better training model for machine learning. If there is enough number of data points outside the main distribution even if those are not on the fitting line, they will form a cluster and that is where DBSCAN is very successful.Githubmdsohel-mahmood.medium.commdsohel-mahmood.medium.comReference[1] Nadia Rahmah and Imas Sukaesih Sitanggang, “Determination of Optimal Epsilon (Eps) Value on DBSCAN Algorithm to Clustering Data on Peatland Hotspots in Sumatra”, 2016 IOP Conf. Ser.: Earth Environ. Sci. 31 012012[2] DBSCAN Documentation",26/12/2020,0,8.0,0.0,790.0,304.0,12.0,17.0,0.0,6.0,en
3899,Explaining system intelligence,SAP Design,Vladimir Shapiro,131.0,7.0,891.0,"This blog belongs to the SAP Design series about intelligent system design. You might also be interested in our previous post, 5 Challenges to Your Machine Learning Project.One of the guiding design principles for intelligent systems is to empower end users. If we want people to trust machines, we must share information about the underlying models and the reasoning behind the results of algorithms. This is even more vital for business applications, when users are held accountable for every decision they make.It’s widely accepted that intelligent systems must come with a certain level of transparency. There’s even a new term for it: explainable AI. But, that’s just the beginning. As designers, we need to ask ourselves how explainable AI is tied to user interaction. What do we need to think about when we explain the results and recommendations that come from built-in intelligence? And how can we make it a seamless experience that feels natural to users?Before we dive in, let’s take a step back and ask ourselves if designers really need to explain everything we display on the UI.What my team has been learning from recent user tests, is that if the quality of a prediction is high and the stakes are low, users probably won’t expect comprehensive explanations.Our test scenario: Paul works for a large corporation and has an issue with his emails. When he opens an IT support ticket, the system helps him pick the correct category, based on his problem description.We did our best to make the system recommendation as transparent as possible. But, in the end, none of our test participants were interested in the explanation. When we investigated further, we discovered three factors that explained the user response:In short, if users can easily eliminate or circumvent the negative impact of inaccurate system recommendations, they might not be that interested in explanations. But what are we going to do in other situations?To start , designers need to break down our explanations into “global” and “local” components. AI experts call this the scope of interpretability for a model:Example: Let’s get back to Paul, who is actually a purchaser in a large company. Paul needs to find a new supplier for a material. Our intelligent system can propose a ranked list of suppliers for this specific material.Here are some questions Paul might ask himself when he looks at this list:This question is an example of global interpretability scope. Paul wants to understand the logic behind ranking on a general (global) level to gain an initial sense of trust. I.e: is the system competent enough to help me?Paul wants to understand the details of the ranking system, based on the ranking for a concrete supplier. This may be a supplier he already knows, giving him another chance to verify the competence of the system. Or, it could be a supplier that Paul hasn’t dealt with before. In this case, Paul really wants to learn something new from the system.At first glance this seems to be a local question. But, Paul needs to understand both the global rules and the local effect to interpret the situation.Providing explanations to end users is a perfect scenario for applying progressive disclosure — the design technique we use to avoid overwhelming the user with too much information at once.Let’s explore how it could work in our IT ticket example (assuming an explanation is required):The main elements of the explanation are displayed in a concise form on the main screen, with an option to drill down to more detailed information on secondary screens. The benefit of this approach is that the users enjoy a more simplified and compact UI, and only needs to concern themselves with the details if needed.You might be asking yourself how many levels of progressive disclosure you need to design, and what kind of information you need to offer at each level. This will depend largely on your use case, persona, and chosen level of automation, so there’s no universal pattern. However, the questions below might help you understand the scope of your own explainable AI design requirements, or even prompt you to explore completely new ideas.It goes without saying that designers need to explain overall AI logic to users. But this alone won’t be enough to make AI part of an engaging and empowering user experience that adds obvious value to our solutions. If designers want users to embrace the new intelligent capabilities, our explanations will need to be carefully designed as an integrated part of the UI. And this means telling users exactly what they need to know in their specific context — just enough information at just the right time.Of course, there’s much more to explainable AI than we’ve covered so far. What are the challenges for writing explanation texts? What is the role of explainable AI in building trust in intelligent systems? And how can explainable AI be integrated with user feedback functionality? I will be coming back to these topics in my upcoming posts.In the meantime, I would be happy to hear about your own experiences and the challenges you face when designing explainable AI systems.Stay tuned and please feel free to add your thoughts in the comments.Special thanks to Susanne Wilding for reviewing and editing this article.Several illustrations in this article were created with Scenes™ by SAP AppHausOriginally published at experience.sap.com on April 12, 2018.",11/04/2018,0,14.0,21.0,1193.0,515.0,8.0,4.0,0.0,10.0,en
3900,Topic Modeling and Latent Dirichlet Allocation (LDA) in Python,Towards Data Science,Susan Li,25000.0,5.0,422.0,"Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.Here we are going to apply LDA to a set of documents and split them into topics. Let’s get started!The data set we’ll use is a list of over one million news headlines published over a period of 15 years and can be downloaded from Kaggle.Take a peek of the data.1048575We will perform the following steps:Loading gensim and nltk libraries[nltk_data] Downloading package wordnet to[nltk_data] C:\Users\SusanLi\AppData\Roaming\nltk_data…[nltk_data] Package wordnet is already up-to-date!TrueWrite a function to perform lemmatize and stem preprocessing steps on the data set.Select a document to preview after preprocessing.original document:[‘rain’, ‘helps’, ‘dampen’, ‘bushfires’]tokenized and lemmatized document:[‘rain’, ‘help’, ‘dampen’, ‘bushfir’]It worked!Preprocess the headline text, saving the results as ‘processed_docs’Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set.0 broadcast1 communiti2 decid3 licenc4 awar5 defam6 wit7 call8 infrastructur9 protect10 summitGensim filter_extremesFilter out tokens that appear inGensim doc2bowFor each document we create a dictionary reporting how manywords and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier.[(76, 1), (112, 1), (483, 1), (3998, 1)]Preview Bag Of Words for our sample preprocessed document.Word 76 (“bushfir”) appears 1 time.Word 112 (“help”) appears 1 time.Word 483 (“rain”) appears 1 time.Word 3998 (“dampen”) appears 1 time.Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.[(0, 0.5907943557842693),(1, 0.3900924708457926),(2, 0.49514546614015836),(3, 0.5036078441840635)]Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’For each topic, we will explore the words occuring in that topic and its relative weight.Can you distinguish different topics using the words in each topic and their corresponding weights?Again, can you distinguish different topics using the words in each topic and their corresponding weights?We will check where our test document would be classified.[‘rain’, ‘help’, ‘dampen’, ‘bushfir’]Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification.Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification.Source code can be found on Github. I look forward to hearing any feedback or questions.Reference:Udacity — NLP",31/05/2018,18,47.0,28.0,1187.0,532.0,8.0,2.0,0.0,5.0,en
3901,Step by Step Implementation of Conditional Generative Adversarial Networks,Analytics Vidhya,Neeraj Varshney,110.0,10.0,1382.0,"Generative Adversarial Networks (GANs) have had a lot of success since they were introduced in 2014 by Ian Goodfellow. For somebody starting out in Machine Learning, the intricate Mathematics and the complex-looking architecture of GANs seems daunting. So, let’s demystify GANs/C-GANs and implement a simple application with PyTorch. This article is self-contained and is targeted for beginner to intermediate level Machine Learning enthusiasts.Let’s start our journey by briefly describing GANs. GANs are primarily used to generate content i.e GANs are generative models. GAN architecture consists of two components: a Generator and a Discriminator.The Generator is trained to generate samples similar to the training set. It takes a random noise as input, passes this input through its network, and generates an output of dimensions same as that of samples in the training set.The objective of the Discriminator is to distinguish the generated samples (generated by the Generator model) from the real ones.As training progresses, both the generator and discriminator become adept at their respective tasks i.e Generator learns to generate output that is close to the real sample, and Discriminator learns to discriminate it from the real ones. Both the Discriminator and Generator try to outdo each other. Furthermore, improvement in the ability of the Discriminator propels the Generator to generate samples that are similar to the training set in order to confuse the Discriminator.Resources for a detailed review of GANs: Understanding Generative Adversarial Networks (GANs)A Brief Introduction To GANsNow, let’s look at a few interesting applications of GANs. An important thing to note is that “All this is generated by a neural network”.Generation of human face imagesPainting GenerationGeneration of cartoon picturesGenerating colored photographs from sketchesPhotos to emoticonsGenerating a child’s face using parents picturesFor more interesting applications you can read the following articles:1. https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca359002. https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/Let’s jump to the implementation part. I will use the MNIST dataset for this example. MNIST is a dataset consisting of 28 X 28 size images of handwritten digits. Our GAN model will be trained using this dataset and will eventually be able to generate similar digit images.Here is the link to my GitHub repo for the code of this tutorial.A typical machine learning setup consists of the following steps:1. Define the Model2. Define the Loss function3. Define the optimizer4. Train the model — Forward pass  — Compute Loss — Call the optimizer and update the weightsI would recommend using Google Colab with GPU runtime for faster execution.Firstly, we will import some modulesThe second import statement will be used for loading the MNIST dataset and the transformations that will be applied to the dataset.The torch.nn module would be used to create our model and optim module for defining the optimizer. An optimizer is used to update the parameters of the model.You can follow along even if you don’t understand any of the above jargon. I’ll briefly talk about these terms as we use them in the code.Let’s select the device for computation. It’s important to use a GPU for faster computations.If you have changed the runtime type to GPU then the device variable would be set to “cuda”. You can verify that by printing this variable.Loading the dataset — PyTorch provides a simple way of loading popular datasets like MNIST.You can provide the transformations that you want to apply on the dataset — We will go for two transformations which are very common while playing with datasets: Converting to Tensors and Normalization.Batch_size also needs to be passed as a parameter to the dataloader — We will use a batch_size of 100. The batch size depends on your GPU capacity. Colab can handle a batch size of 100. If you encounter any issues related to GPU memory then reduce the batch_size as per your GPU capacity.Let’s look at a few images of this dataset. I have used matplotlib library to display the images. The below code snippet will display the first image of the first batch of our dataset. Every image has a shape of 28 X 28.Generator ModelSince the data in our training set has images of dimension 28 X 28 i.e 784 values, the objective of our Generator model is to output a vector of 784. We will then convert this vector to a 2d matrix of 28 X 28.Let’s follow the steps below to create our Generator model.Step 1: Create a class that inherits from torch.nn.Module classStep 2: Define two methods in this class — __init__() and forward()__init__() method is used to declare all the components that will be used by the model. We will use three hidden layers for our model. You can play with this number by adding more layers or eliminating a few layers from this network. A hidden layer consists of a linear layer followed by an activation function.A Linear layer is defined using two values — input dimension and output dimension. In order to exemplify this, let’s consider the input of dimension d with m such inputs in a batch. So the size of our effective input is m X d. Now, this is passed through a linear layer of dimension (d,k) (which is a matrix of dimension d X k). The output would be of dimension (m X k).We will use Leaky ReLU as our activation function which is a variant of ReLU activation function. Let’s review ReLU and Leaky ReLU for completeness.The left graph is for ReLU activation function and the right one is for Leaky ReLU. In ReLU negative values are suppressed to 0 while in Leaky ReLU negative values are multiplied by a small constant ‘a’ to reduce the magnitude of the value. We will use tanh activation function for the last layer (the output layer). These choices are standard choices in a machine learning setup.This is how a hidden layer looks like —As previously mentioned, we will define three such hidden layers and an output layer with tanh activation function.Let’s define the second method of our model class: the forward() method. This method takes the input (random noise in our case) and passes this input through the defined model sequentially and returns the output.We will use a random noise of dimension 100 in this example. Below is the full code for our Generator model class:Now, let’s define our Discriminator Model —In terms of architecture, the discriminator model is very similar to the Generator network except for the output layer and the use of dropout. The Generator network is expected to generate an image (hence the output dim is 784), the discriminator network needs to discriminate between the fake generated image and the actual image. So, the output dimension is 1 which is the probability of the input being real.We use sigmoid activation function instead of tanh here in the last layer. Explaining the concept of dropout is out of scope of this tutorial.Now, we can initialize these models and move them to our device. Note that it is required to move these variables to the GPU (if available) so that all the computations can be performed on GPU.This concludes the modeling part. Following the steps mentioned before, it’s time to define the loss function and the optimizer function. Since we have two classes (real and fake), we will use binary cross-entropy loss. Furthermore, we will use Adam optimizer for both of our models i.e Generator and Discriminator.Now the climax — The Training LoopIn a single training step, we need to update parameters of the Generator model as well as the Discriminator model.Here is the outline of a single training step:Let’s see it in action. The comments will further explain the code.The simple GAN we implemented above suffers from a serious problem. It is generating images unconditionally i.e we have no control over the output our model is generating. To overcome this limitation, conditional GANs were invented. The architecture of C-GANs is same as normal GANs but this time the model takes in some metadata as input along with the random noise and conditions the output on that.We will pass the digit value as metadata and constrain the above GAN model to generate an image of the input digit value.A few modifications need to be done to achieve the above objective:Let’s see how these modifications can be incorporated in the codeGenerator and Discriminator Models:Note: Only the changes have been highlighted.Training Loop —Generated image when the input is 5",07/06/2020,11,120.0,26.0,537.0,349.0,10.0,6.0,0.0,10.0,en
3902,Multiclass Text Classification using LSTM in Pytorch,Towards Data Science,Aakanksha NS,260.0,6.0,1140.0,"Human language is filled with ambiguity, many-a-times the same phrase can have multiple interpretations based on the context and can even appear confusing to humans. Such challenges make natural language processing an interesting but hard problem to solve. However, we’ve seen a lot of advancement in NLP in the past couple of years and it’s quite fascinating to explore the various techniques being used. This article aims to cover one such technique in deep learning using Pytorch: Long Short Term Memory (LSTM) models.Here’s a link to the notebook consisting of all the code I’ve used for this article: https://jovian.ml/aakanksha-ns/lstm-multiclass-text-classificationIf you’re new to NLP or need an in-depth read on preprocessing and word embeddings, you can check out the following article:towardsdatascience.comWhat sets language models apart from conventional neural networks is their dependency on context. Conventional feed-forward networks assume inputs to be independent of one another. For NLP, we need a mechanism to be able to use sequential information from previous inputs to determine the current output. Recurrent Neural Networks (RNNs) tackle this problem by having loops, allowing information to persist through the network.However, conventional RNNs have the issue of exploding and vanishing gradients and are not good at processing long sequences because they suffer from short term memory.Long Short Term Memory networks (LSTM) are a special kind of RNN, which are capable of learning long-term dependencies. They do so by maintaining an internal memory state called the “cell state” and have regulators called “gates” to control the flow of information inside each LSTM unit. Here’s an excellent source explaining the specifics of LSTMs:colah.github.ioBefore we jump into the main problem, let’s take a look at the basic structure of an LSTM in Pytorch, using a random input. This is a useful step to perform before getting into complex inputs because it helps us learn how to debug the model better, check if dimensions add up and ensure that our model is working as expected.Even though we’re going to be dealing with text, since our model can only work with numbers, we convert the input into a sequence of numbers where each number represents a particular word (more on this in the next section).We first pass the input (3x8) through an embedding layer, because word embeddings are better at capturing context and are spatially more efficient than one-hot vector representations.In Pytorch, we can use the nn.Embedding module to create this layer, which takes the vocabulary size and desired word-vector length as input. You can optionally provide a padding index, to indicate the index of the padding element in the embedding matrix.In the following example, our vocabulary consists of 100 words, so our input to the embedding layer can only be from 0–100, and it returns us a 100x7 embedding matrix, with the 0th index representing our padding element.We pass the embedding layer’s output into an LSTM layer (created using nn.LSTM), which takes as input the word-vector length, length of the hidden state vector and number of layers. Additionally, if the first element in our input’s shape has the batch size, we can specify batch_first = TrueThe LSTM layer outputs three things:We can verify that after passing through all layers, our output has the expected dimensions:3x8 -> embedding -> 3x8x7 -> LSTM (with hidden size=3)-> 3x3Let’s now look at an application of LSTMs.Problem Statement: Given an item’s review comment, predict the rating ( takes integer values from 1 to 5, 1 being worst and 5 being best)Dataset: I’ve used the following dataset from Kaggle:www.kaggle.comWe usually take accuracy as our metric for most classification problems, however, ratings are ordered. If the actual value is 5 but the model predicts a 4, it is not considered as bad as predicting a 1. Hence, instead of going with accuracy, we choose RMSE — root mean squared error as our North Star metric. Also, rating prediction is a pretty hard problem, even for humans, so a prediction of being off by just 1 point or lesser is considered pretty good.As mentioned earlier, we need to convert our text into a numerical form that can be fed to our model as input. I’ve used spacy for tokenization after removing punctuation, special characters, and lower casing the text:We count the number of occurrences of each token in our corpus and get rid of the ones that don’t occur too frequently:We lost about 6000 words! This is expected because our corpus is quite small, less than 25k reviews, the chance of having repeated words is quite small.We then create a vocabulary to index mapping and encode our review text using this mapping. I’ve chosen the maximum length of any review to be 70 words because the average length of reviews was around 60.The dataset is quite straightforward because we’ve already stored our encodings in the input dataframe. We also output the length of the input sequence in each case, because we can have LSTMs that take variable-length sequences.The training loop is pretty standard. I’ve used Adam optimizer and cross-entropy loss.I’ve used three variations for the model:This pretty much has the same structure as the basic LSTM we saw earlier, with the addition of a dropout layer to prevent overfitting. Since we have a classification problem, we have a final linear layer with 5 outputs. This implementation actually works the best among the classification LSTMs, with an accuracy of about 64% and a root-mean-squared-error of only 0.8172. LSTM with variable input size:We can modify our model a bit to make it accept variable-length inputs. This ends up increasing the training time though, because of the pack_padded_sequence function call which returns a padded batch of variable-length sequences.3. LSTM with fixed input size and fixed pre-trained Glove word-vectors:Instead of training our own word embeddings, we can use pre-trained Glove word vectors that have been trained on a massive corpus and probably have better context captured. For our problem, however, this doesn’t seem to help much.Since ratings have an order, and a prediction of 3.6 might be better than rounding off to 4 in many cases, it is helpful to explore this as a regression problem. Not surprisingly, this approach gives us the lowest error of just 0.799 because we don’t have just integer predictions anymore.The only change to our model is that instead of the final layer having 5 outputs, we have just one. The training loop changes a bit too, we use MSE loss and we don’t need to take the argmax anymore to get the final prediction.LSTM appears to be theoretically involved, but its Pytorch implementation is pretty straightforward. Also, while looking at any problem, it is very important to choose the right metric, in our case if we’d gone for accuracy, the model seems to be doing a very bad job, but the RMSE shows that it is off by less than 1 rating point, which is comparable to human performance!References:",07/04/2020,0,7.0,0.0,1216.0,482.0,3.0,3.0,0.0,9.0,en
3903,Linear Regression using Python,Towards Data Science,Animesh Agarwal,1400.0,7.0,1001.0,"Linear Regression is usually the first machine learning algorithm that every data scientist comes across. It is a simple model but everyone needs to master it as it lays the foundation for other machine learning algorithms.Where can Linear Regression be used? It is a very powerful technique and can be used to understand the factors that influence profitability. It can be used to forecast sales in the coming months by analyzing the sales data for previous months. It can also be used to gain various insights about customer behaviour. By the end of the blog we will build a model which looks like the below picture i.e, determine a line which best fits the data.This is the first blog of the machine learning series that I am going to cover. One can get overwhelmed by the number of articles in the web about machine learning algorithms. My purpose of writing this blog is two-fold. It can act as a guide to those who are entering into the field of machine learning and it can act as a reference for me.The objective of a linear regression model is to find a relationship between one or more features(independent variables) and a continuous target variable(dependent variable). When there is only feature it is called Uni-variate Linear Regression and if there are multiple features, it is called Multiple Linear Regression.The linear regression model can be represented by the following equationThe above hypothesis can also be represented bywhereLet’s create some random data-set to train our model.The plot for the data set generated using the above code is shown below:Training of the model here means to find the parameters so that the model best fits the data.How do we determine the best fit line? The line for which the the error between the predicted values and the observed values is minimum is called the best fit line or the regression line. These errors are also called as residuals. The residuals can be visualized by the vertical lines from the observed data value to the regression line.To define and measure the error of our model we define the cost function as the sum of the squares of the residuals. The cost function is denoted bywhere the hypothesis function h(x) is denoted byand m is the total number of training examples in our data-set.Why do we take the square of the residuals and not the absolute value of the residuals ? We want to penalize the points which are farther from the regression line much more than the points which lie close to the line.Our objective is to find the model parameters so that the cost function is minimum. We will use Gradient Descent to find this.Gradient descentGradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. The steps of gradient descent is outlined below.similarly, the partial derivative of the cost function w.r.t to any parameter can be denoted byWe can compute the partial derivatives for all parameters at once usingwhere h(x) is3. After computing the derivative we update the parameters as given belowwhere α is the learning parameter.We can update all the parameters at once using,We repeat the steps 2,3 until the cost function converges to the minimum value. If the value of α is too small, the cost function takes larger time to converge. If α is too large, gradient descent may overshoot the minimum and may finally fail to converge.To demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes Y = 0. Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.When the learning rate is very slow, the gradient descent takes larger time to find the best fit line.When the learning rate is normalWhen the learning rate is arbitrarily high, gradient descent algorithm keeps overshooting the best fit line and may even fail to find the best line.The complete implementation of linear regression with gradient descent is given below.The model parameters are given belowThe plot of the best fit lineThe plot of the cost function vs the number of iterations is given below. We can observe that the cost function decreases with each iteration initially and finally converges after nearly 100 iterations.Till now we have implemented linear regression from scratch and used gradient descent to find the model parameters. But how good is our model? We need some measure to calculate the accuracy of our model. Let’s look at various metrics to evaluate the model we built above.We will be using Root mean squared error(RMSE) and Coefficient of Determination(R² score) to evaluate our model.RMSE is the square root of the average of the sum of the squares of residuals.RMSE is defined byRMSE score is 2.764182038967211.R² score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression.R² is determined bySSₜ is the total sum of errors if we take the mean of the observed values as the predicted value.SSᵣ is the sum of the square of residualsIf we use the mean of the observed values as the predicted value the variance is 69.47588572871659 and if we use regression the total variance is 7.64070234454893. We reduced the prediction error by ~ 89% by using regression.Now let’s try to implement linear regression using the popular scikit-learn library.sckit-learn is a very powerful library for data-science. The complete code is given belowThe model parameters and the performance metrics of the model are given below:This is almost similar to what we achieved when we implemented linear regression from scratch.That’s it for this blog. The complete code can be found in this GitHub repo.We have learnt about the concepts of linear regression and gradient descent. We implemented the model using scikit-learn library as well.In the next blog of this series we will take some original data set and build a linear regression model.",05/10/2018,3,29.0,38.0,377.0,202.0,25.0,4.0,0.0,3.0,en
3904,Autoencoders — Bits and Bytes of Deep Learning,Towards Data Science,Vindula Jayawardana,346.0,5.0,726.0,"One way to think of what deep learning does is as “A to B mappings,” says Andrew Ng, chief scientist at Baidu Research. “You can input an audio clip and output the transcript. That’s speech recognition.” As long as you have data to train the software, the possibilities are endless, he maintains. “You can input email, and the output could be: Is this spam or not?” Input loan applications, he says, and the output might be the likelihood a customer will repay it. Input usage patterns on a fleet of cars and the output could advise where to send a car next.Rather making the facts complicated by having complex definitions, think of deep learning as a subset of a subset. Artificial Intelligence encircles a wide range of technologies and techniques that enable computers systems to unravel problems in ways that at least superficially resemble thinking. Within that sphere, there is that whole toolbox of enigmatic but important mathematical techniques which drives the motive of learning by experience. That subset is known to be machine learning. Finally, within machine learning is the smaller subcategory called deep learning (also known as deep structured learning or hierarchical learning)which is the application of artificial neural networks (ANNs) to learning tasks that contain more than one hidden layer.Despite its somewhat initially-sounding cryptic name, autoencoders are a fairly basic machine learning model. Autoencoders (AE) are a family of neural networks for which the input is the same as the output. They work by compressing the input into a latent-space representation and then reconstructing the output from this representation.In more terms, autoencoding is a data compression algorithm where the compression and decompression functions are,Additionally, in almost all contexts where the term “autoencoder” is used, the compression and decompression functions are implemented with neural networks.Despite the fact, the practical applications of autoencoders were pretty rare some time back, today data denoising and dimensionality reduction for data visualization are considered as two main interesting practical applications of autoencoders. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.In the traditional architecture of autoencoders, it is not taken into account the fact that a signal can be seen as a sum of other signals. Convolutional Autoencoders (CAE), on the other way, use the convolution operator to accommodate this observation. Convolution operator allows filtering an input signal in order to extract some part of its content. They learn to encode the input in a set of simple signals and then try to reconstruct the input from them.Refer this for the use cases of convolution autoencoders with pretty good explanations using examples. We will see a practical example of CAE later in this post.We will start with the most simple autoencoder that we can build. In the latter part, we will be looking into more complex use cases of the autoencoders in real examples.Following is the code for a simple autoencoder using keras as the platform. keras provided MNIST digits are used in the example.With this code snippet, we will get the following output.In the above image, the top row is the original digits, and the bottom row is the reconstructed digits. As you can see, we have lost some important details in this basic example.Since our inputs are images, it makes sense to use convolutional neural networks as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders as they simply perform much better.With the convolution autoencoder, we will get the following input and reconstructed output.In this section, we will be looking into the use of autoencoders in its real-world usage, for image denoising. We will train the convolution autoencoder to map noisy digits images to clean digits images.We will generate synthetic noisy digits by applying a Gaussian noise matrix and clip the images between 0 and 1.When autoencoder is trained, we can use it to remove the noises added to images we have never seen! And here is how the input and reconstructed output will look like.An autoencoder is an artificial neural network used for unsupervised learning of efficient codings. In the modern era, autoencoders have become an emerging field of research in numerous aspects such as in anomaly detection. In this post, it was expected to provide a basic understanding of the aspects of what, why and how of autoencoders.References",04/08/2017,0,6.0,6.0,748.0,279.0,7.0,2.0,0.0,6.0,en
3905,Write an AI to win at Pong from scratch with Reinforcement Learning,Medium,Dhruv Parthasarathy,3700.0,11.0,1768.0,"There’s a huge difference between reading about Reinforcement Learning and actually implementing it.In this post, you’ll implement a Neural Network for Reinforcement Learning and see it learn more and more as it finally becomes good enough to beat the computer in Pong! You can play around with other such Atari games at the OpenAI Gym.By the end of this post, you’ll be able to do the following:The code and the idea are all tightly based on Andrej Karpathy’s blog post. The code in me_pong.py is intended to be a simpler to follow version of pong.py which was written by Dr. Karpathy.To follow along, you’ll need to know the following:If you want a deeper dive into the material at hand, read the blog post on which all of this is based. This post is meant to be a simpler introduction to that material.Great! Let’s get started.We are given the following:Can we use these pieces to train our agent to beat the computer? Moreover, can we make our solution generic enough so it can be reused to win in games that aren’t pong?Indeed, we can! Andrej does this by building a Neural Network that takes in each image and outputs a command to our AI to move up or down.We can break this down a bit more into the following steps:Our Neural Network, based heavily on Andrej’s solution, will do the following:Ok now that we’ve described the problem and its solution, let’s get to writing some code!We’re now going to follow the code in me_pong.py. Please keep it open and read along! The code starts here:First, let’s use OpenAI Gym to make a game environment and get our very first image of the game.Next, we set a bunch of parameters based off of Andrej’s blog post. We aren’t going to worry about tuning them but note that you can probably get better performance by doing so. The parameters we will use are:Then, we set counters, initial values, and the initial weights in our Neural Network.Weights are stored in matrices. Layer 1 of our Neural Network is a 200 x 6400 matrix representing the weights for our hidden layer. For layer 1, element w1_ij represents the weight of neuron i for input pixel j in layer 1.Layer 2 is a 200 x 1 matrix representing the weights of the output of the hidden layer on our final output. For layer 2, element w2_i represents the weights we place on the activation of neuron i in the hidden layer.We initialize each layer’s weights with random numbers for now. We divide by the square root of the number of the dimension size to normalize our weights.Next, we set up the initial parameters for RMSProp (a method for updating weights that we will discuss later). Don’t worry too much about understanding what you see below. I’m mainly bringing it up here so we can continue to follow along the main code block.We’ll need to collect a bunch of observations and intermediate values across the episode and use those to compute the gradient at the end based on the result. The below sets up the arrays where we’ll collect all that information.Ok we’re all done with the setup! If you were following, it should look something like this:Phew. Now for the fun part!The crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move. We’ll put everything in a while block for now but in reality you might set up a break condition to stop the process.The first step to our algorithm is processing the image of the game that OpenAI Gym passed us. We really don’t care about the entire image - just certain details. We do this below:Let’s dive into preprocess_observations to see how we convert the image OpenAI Gym gives us into something we can use to train our Neural Network. The basic steps are:Now that we’ve preprocessed the observations, let’s move on to actually sending the observations through our neural net to generate the probability of telling our AI to move up. Here are the steps we’ll take:How exactly does apply_neural_nets take observations and weights and generate a probability of going up? This is just the forward pass of the Neural Network. Let’s look at the code below for more information:As you can see, it’s not many steps at all! Let’s go step by step:Let’s return to the main algorithm and continue on. Now that we have obtained a probability of going up, we need to now record the results for later learning and choose an action to tell our AI to implement:We choose an action by flipping an imaginary coin that lands “up” with probability up_probability and down with 1 - up_probability. If it lands up, we choose tell our AI to go up and if not, we tell it to go down. We alsoHaving done that, we pass the action to OpenAI Gym via env.step(action).Ok we’ve covered the first half of the solution! We know what action to tell our AI to take. If you’ve been following along, your code should look like this:Now that we’ve made our move, it’s time to start learning so we figure out the right weights in our Neural Network!Learning is all about seeing the result of the action (i.e. whether or not we won the round) and changing our weights accordingly. The first step to learning is asking the following question:Mathematically, this is just the derivative of our result with respect to the outputs of our final layer. If L is the value of our result to us and f is the function that gives us the activations of our final layer, this derivative is just ∂L/∂f.In a binary classification context (i.e. we just have to tell the AI one of two actions, up or down), this derivative turns out to beNote that σ in the above equation represents the sigmoid function. Read the Attribute Classification section here for more information about how we get the above derivative. We simplify this further below:After one action(moving the paddle up or down), we don’t really have an idea of whether or not this was the right action. So we’re going to cheat and treat the action we end up sampling from our probability as the correct action.Our predicion for this round is going to be the probability of going up we calculated. Using that, we have that ∂L/∂f can be computed byAwesome! We have the gradient per action.The next step is to figure out how we learn after the end of an episode (i.e. when we or our opponent miss the ball and someone gets a point). We do this by computing the policy gradient of the network at the end of each episode. The intuition here is that if we won the round, we’d like our network to generate more of the actions that led to us winning. Alternatively, if we lose, we’re going to try and generate less of these actions.OpenAI Gym provides us the handy done variable to tell us when an episode finishes (i.e. we missed the ball or our opponent missed the ball). When we notice we are done, the first thing we do is compile all our observations and gradient calculations for the episode. This allows us to apply our learnings over all the actions in the episode.Next, we want to learn in such a way that actions taken towards the end of an episode more heavily influence our learning than actions taken at the beginning. This is called discounting.Think about it this way - if you moved up at the first frame of the episode, it probably had very little impact on whether or not you win. However, closer to the end of the episode, your actions probably have a much larger effect as they determine whether or not your paddle reaches the ball and how your paddle hits the ball.We’re going to take this weighting into account by discounting our rewards such that rewards from earlier frames are discounted a lot more than rewards for later frames. After this, we’re going to finally use backpropagation to compute the gradient (i.e. the direction we need to move our weights to improve).Let’s dig in a bit into how the policy gradient for the episode is computed. This is one of the most important parts of Reinforcement Learning as it’s how our agent figures out how to improve over time.To begin with, if you haven’t already, read this excerpt on backpropagation from Michael Nielsen’s excellent free book on Deep Learning.As you’ll see in that excerpt, there are four fundamental equations of backpropogation, a technique for computing the gradient for our weights.Our goal is to find ∂C/∂w1 (BP4), the derivative of the cost function with respect to the first layer’s weights, and ∂C/∂w2, the derivative of the cost function with respect to the second layer’s weights. These gradients will help us understand what direction to move our weights in for the greatest improvement.To begin with, let’s start with ∂C/∂w2. If a^l2 is the activations of the hidden layer (layer 2), we see that the formula is:Indeed, this is exactly what we do here:Next, we need to calculate ∂C/∂w1. The formula for that is:and we also know that a^l1 is just our observation_values.So all we need now is δ^l2. Once we have that, we can calculate ∂C/∂w1 and return. We do just that below:If you’ve been following along, your function should look like this:With that, we’ve finished backpropagation and computed our gradients!After we have finished batch_size episodes, we finally update our weights for our Neural Network and implement our learnings.To update the weights, we simply apply RMSProp, an algorithm for updating weights described by Sebastian Reuder here.We implement this below:This is the step that tweaks our weights and allows us to get better over time.This is basically it! Putting it altogether it should look like this.You just coded a full Neural Network for playing Pong! Uncomment env.render() and run it for 3–4 days to see it finally beat the computer! You’ll need to do some pickling as done in Andrej Karpathy’s solution to be able to visualize your results when you win.According to the blog post, this algorithm should take around 3 days of training on a Macbook to start beating the computer.Consider tweaking the parameters or using Convolutional Neural Nets to boost the performance further.If you want a further primer into Neural Networks and Reinforcement Learning, there are some great resources to learn more (I work at Udacity as the Director of Machine Learning programs):",26/09/2016,2,16.0,7.0,355.0,148.0,6.0,10.0,0.0,16.0,en
3906,Basics of Using Pre-trained GloVe Vectors in Python,Analytics Vidhya,Sebastian Theiler,465.0,7.0,1310.0,"This article will cover: * Downloading and loading the pre-trained vectors * Finding similar vectors to a given vector * “Math with words” * Visualizing the vectorsFurther reading resources, including the original GloVe paper, are available at the end.Global Vectors for Word Representation, or GloVe, is an “unsupervised learning algorithm for obtaining vector representations for words.” Simply put, GloVe allows us to take a corpus of text, and intuitively transform each word in that corpus into a position in a high-dimensional space. This means that similar words will be placed together.If you would like a detailed explanation of how GloVe works, linked articles are available at the end.Head over to https://nlp.stanford.edu/projects/glove/.Then underneath “Download pre-trained word vectors,” you can choose any of the four options for different sizes or training datasets.I have chosen the Wikipedia 2014 + Gigaword 5 vectors. You can download those exact vectors at http://nlp.stanford.edu/data/glove.6B.zip (WARNING: THIS IS A 822 MB DOWNLOAD)I cannot guarantee that the methods used below will work with all of the other pre-trained vectors, as they have not been tested.We’re going to need to use, Numpy, Scipy, Matplotlib, and Sklearn for this project.If you need to install any of these, you can run the following:Depending on your version of Python, you may need to substitute pip for pip3.Now we can import the parts we need from these modules with:Before we load the vectors in code, we have to understand how the text file is formatted.Each line of the text file contains a word, followed by N numbers. The N numbers describe the vector of the word’s position. N may vary depending on which vectors you downloaded, for me, N is 50, since I am using glove.6B.50d.Here is an example line from the text file, shortened to the first three dimensions:To load the pre-trained vectors, we must first create a dictionary that will hold the mappings between words, and the embedding vectors of those words.Assuming that your Python file is in the same directory as the GloVe vectors, we can now open the text file containing the embeddings with:Note: you will need to replace glove.6B.50d.txt with the name of the text file you have chosen for the vectors.Once inside of the with statement, we need to loop through each line in the file, and split the line by every space, into each of its components.After splitting the line, we make the assumption the word does not have any spaces in it, and set it equal the first (or zeroth) element of the split line.Then we can take the rest of the line, and convert it into a Numpy array. This is the vector of the word’s position.Finally, we can update our dictionary with the new word and its corresponding vector.As a recap for our full code to load the vectors:Keep in mind, you may need to edit the method for separating the word from the vectors if your vector text file includes words with spaces in them.Another thing we can do with GloVe vectors is find the most similar words to a given word. We can do this with a fancy one-liner function as follows:This one’s complicated, so let’s break it down.sorted takes an iterable as input and sorts it using a key. In this case, the iterable that we are passing in is all possible words that we want to sort. We can get a list of such words by calling embeddings_dict.keys().Now, since by default Python would sort the list alphabetically, we must specify a key to sort the list the way we want it sorted.In our case, the key will be a lambda function that takes a word as input and returns the distance between that word’s embedding and the embedding we gave the function. We will be using euclidean distance to measure how far apart the two embeddings are.scipy has a function for measuring euclidean distance under its module spatial, which we imported earlier. So our final sorting key turns into:Now if we want to rank all words by closeness to a given word, let’s say “king,” we can use:This, however, will print every word, so if we want to shorten it we can use a slice at the end, for the closest, let’s say five words.Since the closest word to a given word will always be that word, we can offset our slice by one.Using my vectors, glove.6B.50d,prints: [‘prince’, ‘queen’, ‘uncle’, ‘ii’, ‘grandson’]The reason we take an embedding directly, instead of transforming a word into an embedding, is so that when we add and subtract embeddings, we can find the closest approximate words to an embedding, not just a word. We can do this, even if the embedding does not lie entirely on any word.Now that we can turn any word into a vector, we can use any math operation usable on vectors, on words.For example, we can add and subtract two words together, just like numbers. i.e., twig-branch+hand ≈ fingerThe above code prints “fingernails” as its top result, which is certainly passable as logical.Nothing helps to find insights in data more than visualizing it.To visualize the vectors, we are first going to be using a method known as t-distributed stochastic neighbor embedding, also known as t-SNE. t-SNE will allow us to reduce the, in my case, 50 dimensions of the data, down to 2 dimensions. After we do that, it’s as simple as using a matplotlib scatter plot to plot it. If you would like to learn more about t-SNE, there are a few articles linked at the end.sklearn luckily has a t-SNE class that can make our work much more manageable. To instantiate it, we can use:n_components specifies the number of dimensions to reduce the data into.random_state is a seed we can use to obtain consistent results.After initializing the t-SNE class, we need to get a list of every word, and the corresponding vector to that word.The first line takes all the keys of embeddings_dict and converts it to a list.The second line uses list comprehension to obtain the value in embeddings_dict that corresponds to each word we chose, and put that into list form.We can also manually specify words so that it will only plot individual words. i.e., words = [“sister”, “brother”, “man”, “woman”, “uncle”, “aunt”]After getting all the words we want to use and their corresponding vectors, we now need to fit the t-SNE class on our vectors. We can do this using:If you would like, you can remove or expand the slice at the end of vectors, but be warned; this may require a powerful computer.After the t-SNE class finishes fitting to the vectors, we can use a matplotlib scatter plot to plot the data:This alone isn’t very useful since it’s just a bunch of dots. To improve it we can annotate the graph by looping through each X Y point with a label and calling plt.annotate with those X Y points and with that label. The other inputs to the function are for important formatting. Annotation in MatplotlibFinally, we can show the plot with,This may lag on less powerful computers, so you can either choose to lower the numbers of words shown, by changing vectors[:1000] to something more like vectors[:250], or change words to a list of your own making.Full code for visualizing the vectors:The full code is available in Jupyter Notebook and Python file format on my GitHub here.Only so much can be done with the pre-trained GloVe vectors. For higher-level usage, I would recommend consulting the official README for training your own vectors.One important usage that was not mentioned above is loading an embedding layer at the start of a Natural Language Processing model, with these vectors. That would, in theory, considerably increase the accuracy of the model, and save time training a new embedding from scratch.Papers:Original GloVe Paper: https://nlp.stanford.edu/pubs/glove.pdfOriginal t-SNE Paper: http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdfMore detailed GloVe explanations: *https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/ *https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/More detailed t-SNE explanations: *https://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/ *https://distill.pub/2016/misread-tsne/",08/09/2019,14,14.0,51.0,1065.0,575.0,3.0,0.0,0.0,14.0,en
3907,Pipelines & Custom Transformers in scikit-learn: The step-by-step guide (with Python code),Towards Data Science,Himanshu Chandra,67.0,8.0,1133.0,"This article will cover:There’s a video walkthrough of the code at the end for those who prefer the format. I personally like written tutorials, but I’ve had requests for video versions too in the past, so there it is.Since you are here, there’s a very good chance you already know Pipelines make your life easy by pre-processing the data. I heard that too and tried to implement one in my code.A shout-out to the few great tutorials I could find on the topic! I recommend you certainly browse through them, before or after the current article :i. https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65ii. https://machinelearningmastery.com/how-to-transform-target-variables-for-regression-with-scikit-learniii. http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.htmlIt was all good while following the tutorials and using standard imputing, scaling, power-transforms, etc. But then I wanted to write specific logic to be applied to the data and wasn’t very sure what was being called where?I tried to look for a lucid explanation on when are the constructor, fit(), transform() functions, actually being called, but couldn’t get a simple example. So I decided to step through the code bit by bit and present my understanding for anyone who wants to understand this from scratch.Let’s get started then!To understand the examples better, we’ll create a dataset that will help us explore the code better.The code above creates data which follows the equation y = X1 + 2 * sqrt(X2). This makes sure a simple Linear Regression model is not able to fit it perfectly.Let’s see what prediction results are thrown at us:A perfect prediction would be 14 and 17. The predictions are not bad, but can we do some calculations on the input features to make this better?The input manipulations cause it to fit a perfect linear trend (y=X1+X2 now), and hence the perfect predictions. Now, this is just an example, but suppose for a dataset, your analysis said such input transformation would be good, how do you do that in a safe manner via Pipelines.Let’s see a basic LinearRegression() model fitted by using a Pipeline.As expected, we get the same predictions as our first attempt. The syntax at this point of time is quite simple —To perform the input calculations/transformations, we’ll design a custom transformer.We create a class and name it ExperimentalTransformer. All transformers we design will inherit from BaseEstimator and TransformerMixin classes as they give us pre-existing methods for free. You can read more about them in the article links I provided above.There are 3 methods to take care of here:For the moment, let’s just put print() messages in __init__ & fit(), and write our calculations in transform(). As you see above, we return the modified values there. All the input features will be passed into X when fit() or transform() is called.Let’s put this into a pipeline to see the order in which these functions are called.You can see in the code comments above, one can also use make_pipeline() syntax, which is shorter, to create pipelines.Now the output:3 important things to note:a. __init__ was called the moment we initialized the pipe2 variable.b. Both fit() and transform() of our ExperimentalTransformer were called when we fitted the pipeline on training data. This makes sense as that is how model fitting works. You would need to transform input features while trying to predict train_y.c. transform() is called, as expected, when we call predict(test_X) — the input test features need to be square-rooted and doubled too before making predictions.The result — perfect predictions!But..We’ve assumed in the transform() function of our ExperimentalTransformer that the column name is X2. Let’s not do so and pass the column name via the constructor, __init__().Here’s our ExperimentalTransformer_2:Take care to keep the parameter name exactly the same in the function argument as well as the class’ variable (feature_name or whichever name you choose). Changing that will cause problems later, when we also try to transform the target feature (y). It causes a double-call to __init__ for some reason.I also added an additional_param with a default value, just to mix things up. It’s not really needed for anything in our case, and acts as an optional argument.Create the new pipeline now:Output is as expected:What about a situation when some pre and post processing needs to be done?Consider a slightly modified data set:Everything’s the same, but now y has been squared. To make this fit into a simple linear model, we will need to square-root y before fitting our model and also later, square any predictions made by the model.We can use scikit-learn’s TransformedTargetRegressor to instruct our pipeline to perform some calculation and inverse-calculation on the target variable. Let’s first write those two functions:One square-roots y and the other squares it back.Calling via pipeline:The TransformedTargetRegressor class takes regressor, func and inverse_func arguments which connects our pipeline to these new functions.Note how we fit the model now, not the pipeline.The output shows up something interesting and unexpected though:The results are fine, but can you see how our target_transform() and inverse_target_transform() methods have been called multiple times when fit() was called? That is going to become an overhead in big projects and complex pipelines. The change needed to handle this is simply to set check_inverse param of TransformedTargetRegressor to False. We’ll do that in the next step along with looking at another way to handle target transformation — by using transformer param inside TransformedTargetRegressor instead of func and inverse_func.We can pass an in-built transformer or our custom transformer instead of the two functions we designed. The custom transformer will look almost identical to the one we designed earlier for our pipeline, but will have an additional inverse_transform function inside it. Here’s the implementation:That’s it, just use it in our TransformedTargetRegressor call now:The output looks fixed now:One last thing to do here. We’ll make use of caching to preserve computations and also see how to get or set parameters of our pipeline from outside (this would be needed later if you want to apply GridSearch on top of this).Notice how each parameter of each component of the pipeline can be accessed by using it’s name followed by a double underscore __.We’ll tie it all together and even try to set a parameter from outside — the column name X2 we have been passing to the constructor.Complete Code: https://github.com/HCGrit/MachineLearning-iamJustAStudent/tree/master/PipelineFoundationCode walkthrough: https://youtu.be/mOYJCR0IDk8Any practical pipeline implementation would rarely be complete without using either a FeatureUnion or a ColumnTransformer. The very first reference link I provided above walks you through FeatureUnions. References I found extremely helpful for ColumnTransformers vs FeatureUnions are:i. https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces ii. https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformerAlso, you will eventually use GridSearch on your model. Using it with pipelines is explained here: https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html?highlight=pipelineUsing these concepts should be easy enough, now that you have a good grasp of the foundations of pipeline creation.Interested in sharing ideas, asking questions or simply discussing thoughts? Connect with me on LinkedIn, YouTube, GitHub or through my website: I am Just a Student.See you around & happy learning!www.linkedin.com",06/05/2020,0,4.0,7.0,780.0,454.0,20.0,3.0,0.0,19.0,en
3908,TF-IDF from scratch in python on a real-world dataset.,Towards Data Science,William Scott,204.0,19.0,3957.0,"TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.If I give you a sentence for example “This building is so tall”. It's easy for us to understand the sentence as we know the semantics of the words and the sentence. But how can any program (eg: python) interpret this sentence? It is easier for any programming language to understand textual data in the form of numerical value. So, for this reason, we need to vectorize all of the text so that it is better represented.By vectorizing the documents we can further perform multiple tasks such as finding the relevant documents, ranking, clustering, etc. This exact technique is used when you perform a google search (now they are updated to newer transformer techniques). The web pages are called documents and the search text with which you search is called a query. The search engine maintains a fixed representation of all the documents. When you search with a query, the search engine will find the relevance of the query with all of the documents, ranks them in the order of relevance and shows you the top k documents. All of this process is done using the vectorized form of query and documents.Now coming back to our TF-IDF,TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)This measures the frequency of a word in a document. This highly depends on the length of the document and the generality of the word, for example, a very common word such as “was” can appear multiple times in a document. But if we take two documents with 100 words and 10,000 words respectively, there is a high probability that the common word “was” is present more in the 10,000 worded document. But we cannot say that the longer document is more important than the shorter document. For this exact reason, we perform normalization on the frequency value, we divide the frequency with the total number of words in the document.Recall that we need to finally vectorize the document. When we plan to vectorize documents, we cannot just consider the words that are present in that particular document. If we do that, then the vector length will be different for both the documents, and it will not be feasible to compute the similarity. So, what we do is that we vectorize the documents on the vocab. Vocab is the list of all possible worlds in the corpus.We need the word counts of all the vocab words and the length of the document to compute TF. In case the term doesn’t exist in a particular document, that particular TF value will be 0 for that particular document. In an extreme case, if all the words in the document are the same, then TF will be 1. The final value of the normalised TF value will be in the range of [0 to 1]. 0, 1 inclusive.TF is individual to each document and word, hence we can formulate TF as follows:tf(t,d) = count of t in d / number of words in dIf we already computed the TF value and if this produces a vectorized form of the document, why not use just TF to find the relevance between documents? Why do we need IDF?Let me explain, words which are most common such as ‘is’, ‘are’ will have very high values, giving those words very high importance. But using these words to compute the relevance produces bad results. These kinds of common words are called stop-words. Although we will remove the stop words later in the preprocessing step, finding the presence of the word across the documents and somehow reduce their weightage is more ideal.This measures the importance of documents in a whole set of the corpus. This is very similar to TF but the only difference is that TF is the frequency counter for a term t in document d, whereas DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term is present in the document at least once, we do not need to know the number of times the term is present.df(t) = occurrence of t in N documentsTo keep this also in a range, we normalize by dividing by the total number of documents. Our main goal is to know the informativeness of a term, and DF is the exact inverse of it. that is why we inverse the DFIDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because they are present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.idf(t) = N/dfNow there are few other problems with the IDF, when we have a large corpus size say N=10000, the IDF value explodes. So to dampen the effect we take the log of IDF.At query time, when the word is not present in is not in the vocab, it will simply be ignored. But in few cases, we use a fixed vocab and few words of the vocab might be absent in the document, in such cases, the df will be 0. As we cannot divide by 0, we smoothen the value by adding 1 to the denominator.idf(t) = log(N/(df + 1))Finally, by taking a multiplicative value of TF and IDF, we get the TF-IDF score. There are many different variations of TF-IDF but for now, let us concentrate on this basic version.tf-idf(t, d) = tf(t, d) * log(N/(df + 1))I’m a Senior Data Scientist and AI researcher in the field of NLP and DL.Connect with me: Twitter, LinkedIn.Now that we learnt what is TF-IDF let us compute the similarity score on a dataset.The dataset we are going to use are archives of few stories, this dataset has lots of documents in different formats. Download the dataset and open your notebooks, Jupyter Notebooks I mean 😜.Dataset Link: http://archives.textfiles.com/stories.zipThe first step in any of the Machine Learning tasks is to analyse the data. So if we look at the dataset, at first glance, we see all the documents with words in English. Each document has different names and there are two folders in it.Now one of the important tasks is to identify the title in the body, if we analyse the documents, there are different patterns of alignment of title. But most of the titles are centre aligned. Now we need to figure out a way to extract the title. But before we get all pumped up and start coding, let us analyse the dataset little deep.Take few minutes to analyse the dataset yourself. Try to explore…Upon more inspection, we can notice that there’s an index.html in each folder (including the root), which contains all the document names and their titles. So, let us consider ourselves lucky as the titles are given to us, without exhaustively extracting titles from each document.There is no specific way to do this, this totally depends on the problem statement at hand and on the analysis, we do on the dataset.As we have already found that the titles and the document names are in the index.html, we need to extract those names and titles. We are lucky that index.html has tags that we can use as patterns to extract our required content.Before we start extracting the titles and file names, as we have different folders, first let’s crawl the folders to later read all the index.html files at once.os.walk gives us the files in the directory, os.getcwd gives us the current directory and title and we are going to search in the current directory + stories folder as our data files are in the stories folder.Always assume that you are dealing with a huge dataset, this helps in automating the code.Now we can find that folders give extra / for the root folder, so we are going to remove it.The above code removes the last character for the 0th index in folders, which is the root folderNow, let’s crawl through all the index.html to extract their titles. To do that we need to find a pattern to take out the title. As this is in html, our job will be a little simpler.let’s see…We can clearly observe that each file name is enclosed between (><A HREF=”) and (”) and each title is between (<BR><TD>) and (\n)We will use simple regular expressions to retrieve the name and title. The following code gives the list of all the values that match that pattern. so names and titles variables have the list of all names and titles.Now that we have code to retrieve the values from the index, we just need to iterate to all the folders and get the title and file name from all the index.html files- read the file from index files- extract title and names- iterate to next folderThis prepares the indexes of the dataset, which is a tuple of the location of the file and its title. There is a small issue, the root folder index.html also has folders and its links, we need to remove those.simply use a conditional checker to remove it.Preprocessing is one of the major steps when we are dealing with any kind of text model. During this stage, we have to look at the distribution of our data, what techniques are needed and how deep we should clean.This step never has a one-hot rule, and totally depends on the problem statement. Few mandatory preprocessing are: converting to lowercase, removing punctuation, removing stop words and lemmatization/stemming. In our problem statement, it seems like the basic preprocessing steps will be sufficient.During the text processing, each sentence is split into words and each word is considered as a token after preprocessing. Programming languages consider textual data as sensitive, which means that The is different from the. we humans know that those both belong to the same token but due to the character encoding those are considered as different tokens. Converting to lowercase is a very mandatory preprocessing step. As we have all our data in the list, numpy has a method that can convert the list of lists to lowercase at once.Stop words are the most commonly occurring words that don’t give any additional value to the document vector. in-fact removing these will increase computation and space efficiency. nltk library has a method to download the stopwords, so instead of explicitly mentioning all the stopwords ourselves we can just use the nltk library and iterate over all the words and remove the stop words. There are many efficient ways to do this, but ill just give a simple method.we are going to iterate over all the stop words and not append them to the list if it’s a stop wordPunctuation is the set of unnecessary symbols that are in our corpus documents. We should be a little careful with what we are doing with this, there might be few problems such as U.S — us “United Stated” being converted to “us” after the preprocessing. hyphen and should usually be dealt with little care. But for this problem statement, we are just going to remove theseWe are going to store all our symbols in a variable and iterate that variable removing that particular symbol in the whole dataset. we are using numpy here because our data is stored in a list of lists, and numpy is our best bet.Note that there is no ‘ apostrophe in the punctuation symbols. Because when we remove punctuation first it will convert don’t to dont, and it is a stop word that won't be removed. What we will do instead, is removing the stop words first followed by symbols and then finally repeat stopword removal as few words might still have an apostrophe that are not stopwords.Single characters are not much useful in knowing the importance of the document and few final single characters might be irrelevant symbols, so it is always good to remove the single characters.We just need to iterate to all the words and not append the word if the length is not greater than 1.This is the final and most important part of the preprocessing. stemming converts words to their stem.For example, playing and played are the same type of words that basically indicate an action play. Stemmer does exactly this, it reduces the word to its stem. we are going to use a library called porter-stemmer which is a rule-based stemmer. Porter-Stemmer identifies and removes the suffix or affix of a word. The words given by the stemmer need not be meaningful few times, but it will be identified as a single token for the model.Lemmatisation is a way to reduce the word to the root synonym of a word. Unlike Stemming, Lemmatisation makes sure that the reduced word is again a dictionary word (word present in the same language). WordNetLemmatizer can be used to lemmatize any word.stemming — need not be a dictionary word, removes prefix and affix based on few ruleslemmatization — will be a dictionary word. reduces to a root synonym.A better efficient way to proceed is to first lemmatise and then stem, but stemming alone is also fine for few problems statements, here we will not lemmatise.When a user gives a query such as 100 dollars or hundred dollars. For the user, both those search terms are the same. but our IR model treats them separately, as we are storing 100, dollars, hundred as different tokens. So to make our IR mode a little better we need to convert 100 to hundred. To achieve this we are going to use a library called num2word.If we look a little close to the above output, it is giving us few symbols and sentences such as “one hundred and two”, but damn we just cleaned our data, then how do we handle this? No worries, we will just run the punctuation and stop words again after converting numbers to words.Finally, we are going to put in all those preprocessing methods above in another method and we will call that preprocess method.If you look closely, a few of the preprocessing methods are repeated again. As discussed, this just helps clean the data little deep. Now we need to read the documents and store their title and the body separately as we are going to use them later. In our problem statement, we have very different types of documents, this can cause few errors in reading the documents due to encoding compatibility. to resolve this, just use encoding=”utf8"", errors=’ignore’ in the open() method.Recall that we need to give different weights to title and body. Now how are we going to handle that issue? how will the calculation of TF-IDF work in this case?Giving different weights to title and body is a very common approach. We just need to consider the document as body + title, using this we can find the vocab. And we need to give different weights to words in the title and different weights to the words in the body. To better explain this, let us consider an example.title = “This is a novel paper”body = “This paper consists of survey of many papers”Now, we need to calculate the TF-IDF for body and for the title. For the time being let us consider only the word paper, and forget about removing stop words.What is the TF of word paper in the title? 1/4?No, it’s 3/13. How? word paper appears in title and body 3 times and the total number of words in title and body is 13. As I mentioned before, we just consider the word in the title to have different weights, but still, we consider the whole document when calculating TF-IDF.Then the TF of paper in both title and body is the same? Yes, it’s the same! it’s just the difference in weights that we are going to give. If the word is present in both title and body, then there wouldn't be any reduction in the TF-IDF value. If the word is present only in the title, then the weight of the body for that particular word will not add to the TF of that word, and vice versa.document = body + titleTF-IDF(document) = TF-IDF(title) * alpha + TF-IDF(body) * (1-alpha)Let us be smart and calculate DF beforehand. We need to iterate through all the words in all the documents and store the document id’s for each word. For this, we will use a dictionary as we can use the word as the key and a set of documents as the value. I mentioned set because, even if we are trying to add the document multiple times, a set will not just take duplicate values.We are going to create a set if the word doesn’t have a set yet else add it to the set. This condition is checked by the try block. Here processed_text is the body of the document, and we are going to repeat the same for the title as well, as we need to consider the DF of the whole document.len(DF) will give the unique wordsDF will have the word as the key and the list of doc id’s as the value. but for DF we don’t actually need the list of docs, we just need the count. so we are going to replace the list with its count.There we have it, the count we need for all the words. To find the total unique words in our vocabulary, we need to take all the keys of DF.Recall that we need to maintain different weights for title and body. To calculate TF-IDF of body or title we need to consider both the title and body. To make our job a little easier, let’s use a dictionary with (document, token) pair as key and any TF-IDF score as the value. We just need to iterate over all the documents, we can use the Coutner which can give us the frequency of the tokens, calculate tf and idf and finally store as a (doc, token) pair in tf_idf. tf_idf dictionary is for the body, we will use the same logic to build a dictionary tf_idf_title for the words in the title.Coming to the calculation of different weights. Firstly, we need to maintain a value alpha, which is the weight for the body, then obviously 1-alpha will be the weight for the title. Now let us delve into a little math, we discussed that TF-IDF value of a word will be the same for both body and title if the word is present in both places. We will maintain two different tf-idf dictionaries, one for the body and one for the title.What we are going to do is a little smart, we will calculate TF-IDF for the body; multiply the whole body TF-IDF values with alpha; iterate the tokens in the title; replace the title TF-IDF value in the body TF-IDF value of the (document, token) pair exists. Take some time to process this :PFlow:- Calculate TF-IDF for Body for all docs- Calculate TF-IDF for title for all docs- multiply the Body TF-IDF with alpha- Iterate Title IF-IDF for every (doc, token)— if token is in body, replace the Body(doc, token) value with the value in Title(doc, token)I know this is not easy at first to understand, but still let me explain why the above flow works, as we know that the tf-idf for body and title will be the same if the token is in both places, The weights that we use for body and title sum up to oneTF-IDF = body_tf-idf * body_weight + title_tf-idf*title_weightbody_weight + title_weight = 1When a token is in both places, then the final TF-IDF will be the same as taking either body or title tf_idf. That is exactly what we are doing in the above flow. So, finally, we have a dictionary tf_idf which has the values as a (doc, token) pair.Matching score is the simplest way to calculate the similarity, in this method, we add tf_idf values of the tokens that are in query for every document. For example, for the query “hello world”, we need to check in every document if these words exist and if the word exists, then the tf_idf value is added to the matching score of that particular doc_id. In the end, we will sort and take the top k documents.Mentioned above is the theoretical concept, but as we are using a dictionary to hold our dataset, what we are going to do is we will iterate over all of the values in the dictionary and check if the value is present in the token. As our dictionary is a (document, token) key, when we find a token that is in the query we will add the document id to another dictionary along with the tf-idf value. Finally, we will just take the top k documents again.key[0] is the documentid, key[1] is the token.When we have a perfectly working Matching Score, why do we need cosine similarity again? though Matching Score gives relevant documents, it quite fails when we give long queries, it will not be able to rank them properly. What cosine similarly does is that it will mark all the documents as vectors of tf-idf tokens and measures the similarity in cosine space (the angle between the vectors. Few times the query length would be small but it might be closely related to the document in such cases cosine similarity is the best to find relevance.Observe the above plot, the blue vectors are the documents and the red vector is the query, as we can clearly see, though the manhattan distance (green line) is very high for document d1, the query is still close to document d1. In such cases, cosine similarity would be better as it considers the angle between those two vectors. But Matching Score will return document d3 but that is not very closely related.Matching Score computes manhattan distance (straight line from tips)Cosine score considers the angle of the vectors.To compute any of the above, the simplest way is to convert everything to a vector and then compute the cosine similarity. So, let’s convert the query and documents to vectors. We are going to use total_vocab variable which has all the list of unique tokens to generate an index for each token, and we will use numpy of shape (docs, total_vocab) to store the document vectors.For vector, we need to calculate the TF-IDF values, TF we can calculate from the query itself, and we can make use of DF that we created for the document frequency. Finally, we will store in a (1,vocab_size) numpy array to store the tf-idf values, index of the token will be decided from the total_voab listNow, all we have to do is calculate the cosine similarity for all the documents and return the maximum k documents. Cosine similarity is defined as follows.np.dot(a, b)/(norm(a)*norm(b))I took the text from doc_id 200 (for me) and pasted some content with long query and short query in both matching score and cosine similarity.As we can see from the above document 200 is always highly rated in the cosine method than the matching method, this is because cosine similarity learns the context more.I’m a Senior Data Scientist and AI researcher in the field of NLP and DL.Would love to connect: Twitter, LinkedIn.",15/02/2019,16,35.0,2.0,676.0,251.0,15.0,4.0,0.0,10.0,en
3909,Deep Latent Variable Models: Unravel Hidden Structures,Towards Data Science,Kevin Luxem,74.0,10.0,1976.0,"Understanding the underlying structure of real-world data is one of the most compelling quests in machine learning. But with the advent of deep generative models researcher and practitioners have a powerful method to unravel it.Real-world data is often complex and high-dimensional. Traditional approaches of data analysis are in most cases ineffective and can only model a very simple data distribution. Nowadays, we can use machine learning models to directly learn the structure of our data. The most common approach in machine learning is supervised learning, where we ask the model to learn a mapping from an input to an output variable, e.g. an image x to a label y. However, labelled data is expensive and prone to errors or biases by the human annotator. And a supervised model is only able to generalize its mapping function from the quality of the training data. To test if its generalizes well a validation set from the same distribution is used which will have the same errors. With these kind of models it is possible to perform classification or regression tasks, but we cannot learn the actual elemental organization of our data.The machine learning community is starting to focus on the development of unsupervised learning models. Recent advances have been made by combining probabilistic modeling and deep learning. We call these kind of models generative models. Based on the famous quote,“What I cannot create, I do not understand.” — Richard Feynmana generative model should be able to find the underlying structure e.g. interesting patterns, cluster, statistical correlations and causal structures of the data and generate data like it.Currently, a famous model in this area are Generative Adversial Networks (GANs) [1], which are for example able to generate realistic images of faces from a learned data distribution. Another model of this class is called Variational Autoencoder (VAE) [2] which is also used for unsupervised learning of complex high-dimensional distributions and will be a focus in this article. GAN’s are still very experimental as their training process is slow and unstable. But they are progressing as well, see this nice blog-post on Wasserstein GANs [3].In general, unsupervised learning is much harder than supervised learning, as instead of predicting a label or value for a given input these models have to learn the hidden structure of the data distribution itself. This article will introduce concepts of how we can achieve this focusing on static data such as images with no sequential nature. Learning the underlying structure of sequential data is an even harder problem and I aim to write a follow up article on this in the future.In the first part we will define latent variable models and in the second part we will see how we can learn their parameters using deep neural networks. I try to keep everything as intuitive as I can but some prior knowledge in probability theory and deep learning is definitely helpful.A central problem in machine learning is to learn a complicated probability distribution p(x) with only a limited set of high-dimensional data points x drawn from this distribution. For example, to learn the probability distribution over images of cats we need to define a distribution which can model complex correlations between all pixels which form each image. Modelling this distribution directly is a challenging task or even unfeasible in finite time.Instead of modelling p(x) directly, we can introduce an unobserved latent variable z and define a conditional distribution p(x|z) for the data, which is called a likelihood. In probabilistic terms z can be interpreted as a continuous random variable. For the example of cat images, z could contain a hidden representation of the type of cat, its color or shape.Having z, we can further introduce a prior distribution p(z) over the latent variables to compute the joint distribution over observed and latent variables:This joint distribution allows us to express the complex distribution p(x) in a more tractable way. Its components, p(x|z) and p(z) are usually much simpler to define e.g. by using distributions from the exponential family.To obtain the data distribution p(x) we need to marginalize over the latent variablesFurthermore, using Bayes theorem we can compute the posterior distribution p(z|x) asThe posterior distribution allows us to infer the latent variables given the observations. Note that the integral in equation (2) has no analytical solution for most of the data we deal with and we have to apply some method to infer the posterior in equation (3) which gets explained below.Why did we do this whole exercise and introduced a latent variable? The advantage is that models with latent variables can express the generative process from which the data was created (at least that is our hope). This is known as generative model. In general, it means if we want to generate a new data point we first need to get a sample z ~ p(z) and then use it to sample a new observation x from the conditional distribution p(x|z). While doing this we also can assess whether the model provides a good approximation for the data distribution p(x).Mathematical models containing latent variables are by definition latent variable models. These latent variables have much lower dimensions then the observed input vectors. This yields in a compressed representation of the data. You can think of the latent variables as a bottleneck through which all the information has to pass which is needed to generate the data . We know from the manifold hypothesis that high-dimensional data (e.g. real-world data) lies on lower dimensional manifolds embedded in this high-dimensional space. This justifies the lower dimensional latent space.The posterior distribution p(z|x), which is a key component in probabilistic reasoning, updates our believes about the latent variables after observing a new data point. The posterior for real-world data however is often intractable as their is no analytical solution to the integral in equation (2) which appears in the denominator of equation (3). There are two methods to approximate this distribution. One is a sampling technique called Markov Chain Monte Carlo method. These methods are however computationally expensive and do not scale well to large data sets. The second method are deterministic approximation techniques. Among these techniques is the so called Variational Inference (VI) [4] which is used in VAE. Note that the drawback of this methods is that they cannot generate exact results even in infinite computational time.The general idea of VI is to take an approximation q(z) from a tractable family of distributions (e.g. multivariate Gaussian), and then make this approximation as close as possible to the true posterior p(z|x). This is usually done by minimizing the Kullback-Leibler (KL) divergence between both distributions, defined asThis reduces inference to an optimization problem [5]. The more similar q(z) and p(z|x) the smaller the KL divergence. Note that this quantity is not a distance in a mathematical sense as it is not symmetric if we swap the distributions. Moreover, swapping the distributions in our case would mean we need to take the expectations with respect to p(z|x), which is assumed to be intractable.Now, equation (4) has still the intractable posterior in its numerator inside the logarithm. Using (3) we can rewrite (4) as:The marginal likelihood log p(x) can be taken out of the expectation as it is not dependent from z. The quantity F(q) is the so called Evidence Lower BOund (ELBO). The KL is always ≥ 0 so that it represents a lower bound to the evidence log p(x). The closer the ELBO is to the marginal likelihood the closer the variational approximation will be to the true posterior distribution. Therefore, the complex inference problem is reduced to a simpler optimization problem.We have not mentioned it yet but the likelihood and the prior belong to families of distributions that depend on some unknown parameter. To make this more explicit have a look at the parametric joint distribution of equation (1):Theta denotes the unknown parameters of the model that can be learned using deep neural networks (or with traditional approaches like Expectation Maximization algorithm).VAE uses such deep neural networks to parameterize the probability distributions that define the latent variable model. Moreover, it provides an efficient approximation inference procedure which scales to large data sets. It is defined by a generative model (the latent variable model), an inference network (the variational approximation) and a way of how to learn the parameters of the VAE. For a very nice introduction and implementation of VAE in Keras you can visit this beautiful blog-post.The generative model is given by equation (6) and here z is a continuous latent variable with K-dimensions. The prior of it is typically a Gaussian with zero mean and an identity covariance matrix,The likelihood is known as the decoder, which is typically a Gaussian distribution for continuous data whose parameter theta are computed by passing the latent state z through a deep neural network. The likelihood then looks like the following,The mean and the variance are parameterized by two deep neural networks which output vector is of dimensionality D, i.e. the dimensionality of the observation x. The parameter theta are the weights and biases of the decoder neural networks.The inference network is known as the encoder and allows us to compute the parameters of the posterior approximation. Instead of having a set of parameter for each data point, the variational parameters phi are shared across all data points. Again, in the VAE setting we use deep neural networks that take an input data point and outputs the mean and diagional covariance matrix of the corresponding Gaussian variational approximation,The shared variational parameters phi are the weights and biases of the encoder neural networks.As discussed above, the marginal distribution p(x) (parameterized by theta) is intractable in many cases and needs to be approximated. An approximation can be achieved by using the ELBO. To make it explicit that ELBO depends on some parameter theta we can rewrite it asTo learn the parameter we could maximize the ELBO with respect to its parameter using Expectation Maximization (EM). For the VAE setting the maximization is instead over q over the parameter phi. Thus, we can decompose the ELBO in two terms:The first term of F is the reconstruction loss which encourages the likelihood and inference network to reconstruct the data accurately. The second term is the regularization loss and penalizes posterior approximations that are too far from the prior. Both neural networks with parameter phi and theta can be effectively computed via gradient descent with back-propagation. Moreover, the parameters are updated jointly and not iteratively like in EM.In this article I introduced the concept of latent variable models and their extension with deep neural networks to parameterize the probability distributions which define a latent variable model. Here, we paid attention to variational inference for the approximation of the posterior distribution p(z|x) which is used by a variational autoencoder, a generative model. The true power of a VAE is that they can be trained in a fully unsupervised manner and learn a latent space which captures natural characteristics of the data.If we can embed our complex high-dimensional data into a latent space from which we are able to generate new data which looks much alike the original data we can suppose that our model captured the main characteristics of the data. This can give researcher as well as practitioners a lot of useful information to study the data of interest and identify patterns, correlations or even causal structures.[1] Goodfellow, Ian, et al. “Generative adversarial nets”, NIPS (2014)[2] Diederik P. Kingma, Max Welling. “Auto-Encoding Variational Bayes”, arXiv preprint arXiv:1312.6114 (2014).[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. “Wasserstein GAN”, arXiv preprint arXiv:1701.07875 (2017).[4] David M. Blei, Alp Kucukelbir, Jon D. McAuliffe. “ Variational Inference: A Review for Statisticians”, arXiv preprint arXiv:1601.00670[5] Kevin P. Murphy, “Machine Learning: A Probabilistic Perspective”, MIT Press (2012)Agustinus Kristiadi’s Blog: Variational Autoencoder: Intuition and ImplementationJAAN ALTOSAAR Blog: Tutorial — What is a variational autoencoder?",31/07/2019,0,53.0,139.0,445.0,130.0,13.0,0.0,0.0,18.0,en
3910,Sentence correction using Deep learning techniques,Medium,Sourav kumar,28.0,12.0,1722.0,"Most of us use social media platforms to communicate with people or express ourselves in text. Generally, Most of the ML/DL models used this text to determine the sentiments or to predict any criminal activities and many more NLP-related tasks. The ML and DL models are trained in traditional language, mostly English for any NLP-related task.But in actual, we use informal English to communicate with friends especially short forms or abbreviations. So, this kind of text might not be very much helpful in doing NLP-based task.So, it will be better if we convert those short forms or informal words or text to standard English so that it helps most of NLP tasks in various areas like sentimental analysis, chat box. Etc. Therefore, we need to build a model to convert this corrupted text(informal) to standard English, here another important thing is it need to be converted by preserving the semantic meaning of textAs we are giving text as input, and we are also getting text as output i.e. we are converting corrupted text to a standard English one. It’s kind of a machine translation problem i.e. translating the SMS text to standard English one. So, we are going for a deep learning approach.As it is kind of translation like we need to take entire sentence at a time and we need to translate to standard English sentence. So, we consider this problem as of sequence to sequence modelThe performance of most of the NLP based models decrease due to corrupted text or informal text ,for that we need proper english words .Here, input data will have random corruption which is a superset of target data.We will be converting them into target data while preserving the semantic meaning of text.This dataset contains social media text along with their normalized text and Chinese translation of the normalized text. For our problem we need only social media text and their normalized English text. Social media text contains 2000 dataset.Since our data is in txt format which contains SMS text in one-line, Standard English in second line and Chinese Translation of standard English in 3rd line. We would be using only SMS Text and Standard English for our problem Statement. After splitting, converted the txt format into csv files having 2 columns SMS_TEXT and ENGLISH_TEXT.Dataset link: https://www.comp.nus.edu.sg/~nlp/corpora.htmlHere in this problem we have to convert text to text i.e informal to normal english sentences .So, it is clear that it is a machine tranlation probem(text to text).So, we will use seq2seq traditional and some advance models to solve this problem.As mentioned in the research paper, we will be using categorical cross entropy.As we have sequence of input and outputs, so for a given a word we try to output the probability over all output words to which it is denoting. So, we can treat it as multi class. Therefore, we got SoftMax because for each input word we need to get the probability of all output words in vocabulary and also addition to this we also had log which helps to penalize the small errors, so while training the model by using this loss we try to minimize the error in translating the corrupt word to standard English word and finally it helps us to give good results for test data i.e. unseen corrupted text.Credit : https://nlpaug.readthedocs.io/en/latest/augmenter/augmenter.htmlWe will perform 3 kind of augmentation technique to increase the dataset size:After augmentation we have made dataset of size 12k from 2k.In this EDA I will be doing following things:a.Character level analysis :  * Number of characters present in each sentence. This gives us a rough idea about the sentence length. * We will also calculate frequency of texts(datapoints) having same sentence length in our data.b. Word level analysis:2. N-gram exploration:* Generally n-grams help us to know what words are expected before and after a particular word. * It also helps in knowing context of the word. We will be focusing on bi-grams and tri-grams.3. Word Cloud:1.1 Number of characters present in each sentence of informal text.Inference1.The histogram shows that informal text ranges from 0 to 200 characters and generally, it is between 0 to 160 characters.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of character lies between 0 to 160 with some outliers lying in beyond 200 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.8 percetile is around 161 and 99.9 is 202 so this means there are very few words which are greater than 161.So, we will fix the length to 160.5.so we can fix 161 as padd sequence length1..2 Number of characters present in each sentence of normal text:Inference1.The histogram shows that normal text ranges from 0 to 250 characters and generally, it is between 0 to 190 characters.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of character lies between 0 to 190 with some outliers lying in beyond 190 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.7 percetile is around 200 and 99.8 is 215 so this means there are very few words which are greater than 200.So, we will fix the length to 2005.So we can fix 200 as padd sequence length.1.3 Frequency of texts having same sentence length in dataInference1.We can see that maximum percentage of text is around 20–30 range of length as people tends to write short in social media.2.After the informal text in unwrapped into proper sentences we find that the number of sentence length range changes from 20–30 to nearly 20–60 to be most frequent.2.1 Number of words present in informal sentencesInference1.The histogram shows that number of words in informal text ranges from 1 to 40 words.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of words lies between 1 to 35 with some outliers lying in beyond 35 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.9 percetile is around 39 and 100 percentile is 49 so this means there are very few words which are greater than 39.So, we will fix the length to 395.Above 39 all can be considerd as outliers.2.2 Number of words present in normal sentencesInference1.The histogram shows that number of words in normal text ranges from 1 to 40 words.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of words lies between 1 to 35 with some outliers lying in beyond 35 mark.4.We can see that 99.9 percetile is around 48 and 100 percentile is 59 so this means there are very few words which are greater than 48.So, we will fix the length to 485.Above 48 all can be considerd as outliers.2.3 Average Word length and common words in data2.3 Frequency of stop words in dataBy looking at the graphs above we came to know that words “to” and “I” were most frequently used in informal data. While words “you” and “to” have been most frequently used in normal data.1.1 Using Bi-gramsInference1.Looking at the bi-grams of the informal text it seems that the data is personal-type of data. Also, many of the sentences indicate some action such as “to go”, “call me”.2.Looking at the bi-grams of the normal text it seems that the data is personal-type of data. Also, many of the sentences indicate some action such as “are you”, “want to”.Looking at the tri-grams it is safer to say that the data is personal chats kind of data and we are more sure about the data type ,its background.I have taken basic encoder decoder without attention mechanismas with char-char encoding a basline model and got minimum of 0.285 as val lossModel architectureLets see some output of baseline model:Inference :1.Predicted sentences are very bad in baseline model as it is capturing context vector of only the last encoder time step .2.So, we will now move to advanced methods like attention mechanism for better resultsWe will use 3 type of scoring function for itLoss plot of train vs validation:Now we can see that our validation loss is reaching upto 0.15 which was earlier 0.28.So, it is clear that we have increased our accuracy of prediction .Let’s see some output to get it more clear:I have used bleu score as a metric to see how good or bad my translations are and i got these results:We will finalise concat scoring function as our final scoring function for the attention mechanism and make one final function in which we will send one raw data and it will give prediction of the input sentence.1.Let’s plot the distribution of BLEU score to see its pattern.InferenceWe can see that bleu score is kind of following normal distribution plot and important insights that we can observe are:2.Finding relation between sentence length and bleu score of predicted sentenceLet’s see the average length of sentences for all three cases:Predicted sentence: Sharis, Gen asks if we want to meet up today. Are you free? Please reply as soon as possible. original Sentence : Sharis, Gen asks if we want to meet up today. Are you free? Please reply as soon as possible.Predicted sentence: Today then. Where will you be? original Sentence : 2:30 then. Where will you be?>Predicted sentence: Not arrang Millian, you don’t you?original Sentence : TIP,WHAT arR yoU DOING ?>I have deployed this DL model using flask API on localhost. I am taking informal text as input from user and and using the model. pkl file to predict normal text corresponding to the informal text given by user.https://cs224d.stanford.edu/reports/Lewis.pdfwww.appliedaicourse.comarxiv.orgblog.keras.ionlpaug.readthedocs.iostackoverflow.comI would like to thank team Applied AI for their tremendous support and guideline because of which I reached up to this position to do a complete end to end case study on such a interesting problem statement.You can find complete code on my Github hereHappy to connect with you on Linkedin.www.linkedin.com",11/09/2021,0,36.0,11.0,677.0,280.0,29.0,13.0,0.0,10.0,en
3911,R — CNN Ailesi Part II: Faster R-CNN & Mask R-CNN,Medium,Elif Meşeci,17.0,4.0,574.0,"Merhaba, R-CNN Ailesi: Part I’ de CNN , R- CNN ve Fast R-CNN’den bahsetmiştim. Bu yazıda ise Faster R-CNN ile Mask R-CNN’in gelişimini, avantajlarını ve dezavantajlarını inceleyeceğiz.Bir önceki yazımda bahsettiğim R-CNN ve Fast R-CNN, bölge tekliflerini bulmak için seçici arama kullanır. Seçici arama, ağın performansını etkileyen yavaş ve zaman alıcı bir işlemdir. Bunun üzerine Shaoqing Ren ve ark. seçici arama algoritmasını ortadan kaldıran ve ağın bölge tekliflerini öğrenmesini sağlayan bir nesne algılama algoritması geliştirdi. Faster R-CNN’de bölge tekliflerini belirlemek için özellik haritası üzerinde Seçici Arama algoritması kullanmak yerine Bölge Teklif Ağı (RPN — Region Proposal Network) kullanılır.Faster R-CNN’de izlenen adımlar:🔹 Görüntü, evrişimsel özellik haritası sağlayan bir evrişimli ağa girdi olarak verilir.🔹 Bu özellik haritalarına RPN (bölge teklif ağı) uygulanır ve nesne teklifleri nesnellik puanlarıyla birlikte döndürülür.🔹 Tüm tahmin edilen bölge teklifleri, daha sonra görüntüyü önerilen bölge içinde sınıflandırmak ve sınırlayıcı kutular için ofset değerlerini tahmin etmek için kullanılan RoI havuzlama katmanı kullanılarak yeniden boyutlandırılır.🔹 Teklifler, nesneler için sınırlayıcı kutuları sınıflandırmak için bir softmax katmanı ve tepesinde bir lineer regresyon katmanı bulunan tam bağlantılı bir katmana iletilir.CNN özelliklerinden Bölge Tekliflerinin nasıl oluşturulduğunu inceleyelim. Faster R-CNN özellik haritalarını CNN’den alır ve bunları Bölge Teklif Ağı (RPN)’na iletir. RPN, bu özellik haritaları üzerinde kayan bir pencere kullanır. Her pencerede, farklı şekil ve boyutlarda k Ankor kutusu oluşturur. Ankor kutuları, görüntü boyunca yerleştirilmiş, farklı şekil ve boyutlara sahip sabit boyutlu sınır kutularıdır. Her Ankor kutusu için RPN iki şeyi tahmin eder:💎 Ankor kutusunun bir nesne olma olasılığı💎 Nesneye daha iyi uyması amacıyla Ankor kutularını ayarlamak için sınırlayıcı kutu regresörüFarklı şekil ve boyutlarda sınırlayıcı kutular RoI havuzlama katmanına aktarılır. RoI havuzlama katmanı, her Ankor kutu için sabit boyutlu özellik haritaları çıkarır. Daha sonra bu özellik haritaları, softmax ve lineer regresyon katmanına sahip tam bağlantılı bir katmana geçirilir. Sonunda nesneyi sınıflandırır ve tanımlanan nesneler için sınırlayıcı kutuları tahmin eder.🔪 Nesne önerisi zaman alır. Ağ, görüntünün tamamına tek seferde bakmaz, görüntünün bölümlerine sırayla odaklanır. Bu nedenle tüm nesneleri çıkarmak için tek bir görüntüden birçok geçiş gerektirir.🔪 Birbiri ardına çalışan farklı sistemler olduğundan, sistemlerin performansı önceki sistemin nasıl performans gösterdiğine bağlıdır.Makaleyi incelemek için tıklayınız.Eğitim veri setinde, nesnelerin piksel düzeyindeki konumları da görüntülerde etiketlenirse, R-CNN maskesi nesne algılamanın doğruluğunu daha da geliştirmek için bu tür ayrıntılı etiketlerden etkin bir şekilde yararlanabilir. -K.He ve diğerleriMask R-CNN, RoI havuzlama katmanı yerine RoI hizalama (RoI Allign) katmanı kullanır. RoI hizalama katmanı, piksel düzeyinde tahmin için daha uygun olan özellik haritalarındaki uzamsal bilgileri korumak için çift doğrusal enterpolasyon kullanır. Bu katmanın çıktısı, tüm ilgili bölgeler için aynı şekle sahip özellik haritalarını içerir. Her bir ilgili bölge için yalnızca sınıfı ve sınırlayıcı kutuyu değil, aynı zamanda nesnenin piksel düzeyindeki konumunu Tam Evrişimli Ağ (FCN — Fully Convolutional Network) aracılığıyla tahmin eder.Mask R-CNN’de izlenen adımlar:🔹 Mask R-CNN’de görüntülerden özellikleri çıkarmak için ResNet 101 mimarisini kullanılır. Bu özellikler bir sonraki katman için bir girdi görevi görür.🔹 Elde edilen özellik haritalarını alıp bir bölge teklif ağı (RPN) uygulanır. Burada, temel olarak o bölgede bir nesnenin olup olmadığını tahmin edilir.🔹 RPN’den elde edilen bölgelere bir havuzlama katmanı uygulanır ve tüm bölgeleri aynı şekle dönüştürülür. 🔹 Daha sonra, bu bölgeler tamamen bağlı bir ağdan geçirilir, böylece sınıf etiketi ve sınırlayıcı kutular tahmin edilir.🔹Segmentasyon maskesi oluşturmadan önce hesaplama süresi azaltılabilmesi için ilgilenilen bölge hesaplanır. Tahmin edilen tüm bölgeler için, kesin referans kutuları ile Birlik Üzerinden Kesişim (IoU) hesaplanır. IoU için verilen eşik değerinden yüksek kesişime sahip olan bölgeler RoI olarak alınır.🔹 Son olarak, nesne içeren her bölge için segmentasyon maskesi oluşturulur.Makaleyi incelemek için tıklayınız.www.analyticsvidhya.comd2l.aijonathan-hui.medium.comwww.analyticsvidhya.com",19/08/2021,0,14.0,7.0,1024.0,768.0,3.0,0.0,0.0,7.0,ca
3912,Clustering Techniques,Towards Data Science,M Bharathwaj,7.0,11.0,1598.0,"Clustering falls under the unsupervised learning technique. In this technique, the data is not labelled and there is no defined dependant variable. This type of learning is usually done to identify patterns in the data and/or to group similar data.In this post, a detailed explanation on the type of clustering techniques and a code walk-through is provided.Clustering is a method of grouping of similar objects. The objective of clustering is to create homogeneous groups out of heterogeneous observations. The assumption is that the data comes from multiple population, for example, there could be people from different walks of life requesting loan from a bank for different purposes. If the person is a student, he/she could ask for an education loan, someone who is looking to buy a house can ask for home loan and so on. Clustering helps to identify similar group and cater the needs better.Clustering is a distance-based algorithm. The purpose of clustering is to minimize the intra-cluster distance and maximize the inter-cluster distance.Clustering as a tool can be used to gain insight into the data. Huge amount of information can be obtained by visualizing the data. The output of the clustering can also be used as a pre-processing step for other algorithms. There are several use cases of this technique that is used widely — some of the important ones are market segmentation, customer segmentation, image processing.Before proceeding further, let us understand the core of clustering.Clustering is all about distance between two points and distance between two clusters. Distance cannot be negative. There are a few common measures of distance that the algorithm uses for the clustering problem.EUCLIDEAN DISTANCEIt is a default distance used by the algorithm. It is best explained as the distance between two points. If the distance between two points p and q are to be measured, then the Euclidean distance isMANHATTAN DISTANCEIt is the distance between two points calculated on a perpendicular angle along the axes. It is also called taxicab distance as this represents how vehicles in the city of Manhattan drive where the streets intersect at right angles.MINKOWSKI DISTANCEIn an n-dimensional space, the distance between two points is called Minkowski distance.It is a generalization of the Euclidean and Manhattan distance that if the value of p is 2, it becomes Euclidean distance and if the value of p is 1, it becomes Manhattan distance.There are two major types of clustering techniquesLet us look at each type along with code walk-throughIt is a bottom-up approach. Records in the data set are grouped sequentially to form clusters based on distance between the records and also the distance between the clusters. Here is a step-wise approach to this method -3. At every step, two closest clusters will be merged. Either a single record(singleton) is added to the existing cluster or two clusters are combined. After at least one multiple-element cluster is formed, a scenario where the distance needs to be computed for a singleton and a set of observations and that is where the concept of linkages comes into picture. There are five major types of linkages. By using one of the below concepts, the clustering happens-A point to note is that each linkage method produces an unique result. When each of these methods are applied on the same data set, it may be clustered differently.4. Repeat the steps until there is a single cluster with all the recordsTo visualize the clustering, there is a concept called dendrogram. The dendrogram is a tree diagram summarizing the clustering process. The records are on the x-axis. Similar records are joined by lines whose vertical length reflects the distance between the records. The greater the difference in height, more the dissimilarity. A sample dendrogram is shown -The code for hierarchical clustering is written in Python 3x using jupyter notebook. Let’s begin by importing the necessary libraries.Next, load the data set. Here, a data set on the food menu of Starbucks is used.Do the necessary Exploratory Data Analysis like looking at the descriptive statistics, checking for null values, duplicate values. Perform uni-variate and bi-variate analysis, do outlier treatment(if any). Since this is a distance based algorithm, it is necessary to perform normalization wherever applicable so that all the variables are free of any units of measurement. This enables the model to perform at its best.Once the data is ready, let us work towards building the model. A label list needs to be assigned which is a list of unique value of categorical variable. Here, label list is created from the Food variable.Next step is to form a linkage to cluster a singleton and another cluster. In this case, ward’s method is preferred.Visualize the clustering with the help of a dendrogram. In this case, a truncated dendrogram by specifying the p value which displays the ante-penultimate and penultimate clusters.Once the dendrogram is created, it is necessary to cut the tree to determine the optimum number of clusters. It can be done in one of two ways (explained in the diagram). In this case, 3 clusters are chosen. The clusters can be attached to the data frame as a new column for gaining insights.The last step is to do cluster profiling to extract information and insights from the algorithm to help with effective decision making. The cluster profiling is done by grouping the mean of the cluster and sorting based on the frequency.A quick insight is that the first cluster has the food items that are usually low in calories and hence the macro nutrients are on the lower side. The second cluster has the food items with the most amount of calories and hence more in macro nutrients and there is a mid range in between cluster 1 and 2 which is the third cluster that has good amount of calories and macro nutrients. Overall, on a brief, this model has clustered well.Let’s move on to the next methodK-Means is a non-hierarchical approach. The idea is to specify the number of clusters before hand. Based on the number of clusters, each record is assigned to the cluster based on the distance from each cluster. This approach is preferred when the data set is large. The word means in k-means refers to averaging of the data which is also referred to as finding the centroid. Here is a step-wise approach-Although we have determined the number of clusters before hand, it may not be always right and it is necessary to determine the optimum number of clusters. There is no solid solution to determine the number of clusters, however, there is a common method in place. For each value of k, a Within Sum of Squares(WSS) value can be identified. A single cluster cannot be chosen so it is important to find that k value after which there is no significant difference in the WSS value. To make this more efficient, an elbow plot can be drawn with WSS scores in the y-axis and number of clusters in the x-axis to visualize the optimum number of clusters.There is a way to understand how well the model has performed. It is done by checking two metrics, namely Silhouette Width and Silhouette Score. This helps us to analyse whether each and every observation mapped to the clusters is correct or not based on distance criteria. Silhoutte Width is calculated aswhere b is the distance between the observation and the neighboring cluster’s centroid and a is the distance between the observation and the very own cluster’s centroid.The Silhouette Width can have a value in the range of -1 to 1. If the value of Silhoutte Width is positive, then the mapping of the observation to the current cluster is correct. When a > b, Silhouette Width will return a negative value. The average of all the Silhouette Width is called the Silhouette Score. If the final score is a positive value and it is close to +1, the clusters are well separated on an average. If it is close to 0, it is not separated well enough. If it is negative value, the model has done a blunder in clustering.Let’s begin by importing the necessary librariesNext, load the data set. The same data set used for Hierarchical clustering is used here.Do the necessary Exploratory Data Analysis like looking at the descriptive statistics, checking for null values, duplicate values. Perform uni-variate and bi-variate analysis, do outlier treatment(if any). K-means clustering demands scaling. It is done so that all the variables are free of any units of measurement. This enables the model to perform at its best. In this case, StandardScaler method is put to use.Next is to invoke the KMeans method with defining the number of clusters before hand. Then fit the scaled data set to the model.Now is the time to find the optimum number of clusters by analyzing the values of Within Sum of Squares(WSS) for a given range of k.It is seen that there is a dip in WSS score after k=2 hence let’s keep an eye on k=3. The same can be visualized using an elbow plotAnother helping hand in deciding the number of clusters could be the value of Silhouette score. As discussed before, better the score, better the clustering. Let’s check the score.The WSS for k=3 is 261.67 and the Silhouette score for those labels is 0.3054. Since the score is positive, it is a sign that good clustering has happened.The final step is to do cluster profiling to understand how the cluster has happened and gain more insights.Just like Hierarchical clustering, these three clusters indicate three levels of foods with different calorie and macro nutrients range.",22/09/2020,15,16.0,8.0,307.0,175.0,13.0,5.0,0.0,3.0,en
3913,Introduction to Object Detection with RCNN Family Models,Analytics Vidhya,Sairaj Neelam,7.0,11.0,2021.0,"In this post, you will discover a gentle introduction to the problem of object detection and state-of-the-art deep learning models designed to address it.After reading this post, you will know:Let’s get started.This article is divided into three parts; they are:· Input: An image with a single object, such as a photograph.· Output: A class label (e.g. one or more integers that are mapped to class labels).2. Object Localization: Locate the objects in an image and output their location with a bounding box.· Input: An image with one or more objects, such as a photograph.· Output: One or more bounding boxes (e.g. defined by a point, width, and height).3. Object Detection: Locate the objects with a bounding box and types or classes of the located objects in an image.· Input: An image with one or more objects, such as a photograph.· Output: One or more bounding boxes (e.g. defined by a point, width, and height), and a class label for each bounding box.Bounding boxes are parametrized with these components (x,y,w,h,confidence)(x,y) = giving center of the box, w = width of the box, h = height of the boxIn case of image classification we have single output for every image, but here we need to output whole set of detected objects where each image might have many different objects in it. So, we need to build model that can output variably sized number of detections.2. Multiple types of output:We have 2 different types of output:a. Category Labelb. Bounding Box3. Computational Problem:For object detection it typically requires to work on high resolution images. As we want to identify lot of different objects in image, we want enough spatial resolution on each of the objects so overall resolution of image needs to be quite higher.Before deep diving into RCNN’s family we must understand the concept of Region ProposalsIdea of Region Proposal:So the idea is that, if there is no way we can evaluate object detector on every possible region in an image, for that we can have external algorithm that can generate set of candidate regions in an image for us such that candidate regions gives small set of regions per image but has high probability of covering objects in an image.One of the famous method for region proposals is this method called selective search. So, selective search algorithm would give you about 2,000 object proposals per image in couple of seconds of processing on CPU and these 2,000 region proposals that the algorithm would output would have very high probability of covering all interesting objects we care in image.Once we have idea of region proposals it gives us the way to train object detectors with Deep Neural Networks. This brings us to very famous paper R-CNN.Now, let’s see the RCNN’s Model Family.This is like most influential paper in Deep Learning that came out in 2014. (Rich Feature hierarchies for accurate Object Detection and Semantic Segmentation) by Shaoqing Ren, Kaiming He, Ross Girshick, Jian SunArchitecture and Working of RCNN:Working of RCNN:Step1: We start with input image and run region proposal method like selective search, by which we get 2,000 candidate region proposals in image that we need to evaluateStep2: For each candidate regions, as region proposals can be of different sizes and different aspect ratio, so we are going to warp that region into fixed size, say (224x224)Step3: For each warped image regions, we are going to run them independently trough Convolutional Neural Network (CNN) and that CNN will output classification score for each of these regionsBut, there is a slight problem here,What happens if region proposals that we get from selective search do not exactly match to the objects that we want to detect in the image?So, to overcome this problem,Now CNN is going to output additional thing which is transformation that will transform region proposal box into final box that we want to output for object of our interestFinally it would look something like this,Step1: Run region proposal method to compute 2,000 candidate region proposalsStep2: Resize each region to specific size (224x224) and run independently through CNN to predict class scores and bounding box transformStep3: Use scores to select subset of region proposals to outputStep4: Compare with the ground truth boxesNow, the question arises how to compare the prediction to ground truth box?We can compare these bounding boxes with the metric called Intersection over Union (IOU)IOU = (Area of Intersection) / (Area of Union)More generally, IOU is measure of Overlap between the bounding boxesIf, IOU<0.5 → we say it ‘Bad’ IOU>0.5→ ‘descent’, IOU>0.7 → ‘Good’, IOU>0.9 → ‘Almost perfect’Also there is another problem which is, the object detector often output multiple bounding boxes for same object. So how to solve this?So solution for this is: Post process the raw detection using Non-Max Suppression (NMS)NMS is the way for you to make sure that your algorithm detects objects only once.So, what NMS does is that it cleans up other unwanted detections so we end up with one detection for particular object.How does this NMS work?1. First it looks for probabilities (Pc) associated with each of these detection for particular object2. It takes largest ‘Pc’ which is most confident detection for the object3. Having done that, the NMS part looks for all remaining bounding boxes and chooses all those bounding boxes which has high Intersection over Union (IOU) with the bounding box of highest ‘Pc’ and suppresses them.4. Then we look for remaining bounding box and find highest ‘Pc’ and again NMS looks for remaining bounding boxes which has high IOU with bounding box of high ‘Pc’ and then they will get suppressed.So for this example:1. It takes largest Pc which is 0.9 in this case2. It check IOU for all the remaining bounding boxes (i.e. for 0.6, 0.7 for Car 1 and 0.8, 0.7 for Car 2)3. Now, NMS will suppress 0.6 and 0.7 for car 1 as they have high IOU with respect to bounding box of Pc=0.9, so like this we get only one bounding box for car 1 which is highlighted in the image.4. Next, for remaining bounding boxes we have highest Pc=0.8 for car2 and again we check IOU for remaining boxes (i.e. 0.9 for car1 and 0.7 for car2)5. Now, NMS will suppress 0.7 as it has high IOU with respect to bounding box of Pc=0.8. And we get only one bounding box for car 2 as well.Now, in case of RCNN it is very slow and cannot be used in real-time.The way the researchers have made this fast is by swapping CNN and warping steps. Basically, we warp the images after we run CNN. So, by doing this we get Fast-RCNN. by Ross Girshick.Let’s, see the working and architecture,Step1: Take input image and process whole image with single CNN (without fully connected layers). So the output will be convolutional feature map giving us convolutional features. And this ConvNet we run is often called as backbone network (can be AlexNet, VGG, ResNet, etc.)Step2: Run region proposal methods and crop & resize featuresStep3: Run light CNN (meaning shallow network) per regionThis is going to be fast, as most of the computation is going to happen in the backbone network and network we run on per region is going to be relatively small and light weight and fast to run.What does it mean to crop and resize features? How to crop features?It can be done via Region of Interest Pooling (RoI Pooling)Its purpose is to perform max pooling on inputs of non-uniform sizes to obtain fixed-size feature maps (e.g. 7×7).Let’s just understand this by example,Let’s consider a small example to see how it works. We’re going to perform region of interest pooling on a single 8×8 feature map, one region of interest and an output size of 2×2. Our input feature map looks like this:Let’s say we also have a region proposal (top left, bottom right coordinates): (0, 3), (7, 8). In the picture it would look like this:Normally, there’d be multiple feature maps and multiple proposals for each of them, but we’re keeping things simple for the example. By dividing it into (2×2) sections (because the output size is 2×2) we get:Note that the size of the region of interest doesn’t have to be perfectly divisible by the number of pooling sections (in this case our RoI is 7×5 and we have 2×2 pooling sections). The max values in each of the sections are:And that’s the output from the Region of Interest pooling layer. Here’s our example presented in form of a nice animation:The result is that from a list of rectangles with different sizes we can quickly get a list of corresponding feature maps with a fixed size. Note that the dimension of the RoI pooling output doesn’t actually depend on the size of the input feature map nor on the size of the region proposals. It’s determined solely by the number of sections we divide the proposal into.What’s the benefit of RoI pooling? One of them is processing speed. If there are multiple object proposals on the frame (and usually there’ll be a lot of them), we can still use the same input feature map for all of them. Since computing the convolutions at early stages of processing is very expensive, this approach can save us a lot of time.Now, to make this Fast-RCNN more fast the researchers added Region Proposal Network after the backbone network. This gives us,(Towards Real Time Object Detection with Region Proposal Networks)Here we are going to eliminate algorithm called selective search and instead train Convolutional Neural Network to predict our Region Proposals for us. The way we are going to do that is very similar to Fast RCNN except after we run backbone network we are going to insert a tiny network called Region Proposal Network(RPN) that will be responsible for predicting region proposals.Basically the working for Fast-RCNN and Faster-RCNN is the same after we get region proposals.Step 1: Run input image through backbone network and get image level featuresStep 2: Pass image level features to RPN to get our region proposalsStep 3: Crop the region proposals by ROI PoolingStep 4: Pass warped feature to light CNN for predicting final classification and bounding box transformationsHere the question is how we can use CNN to output region proposals ?The CNN image features coming out of backbone network are all aligned to positions in the input. So, then what we can do is each point in CNN feature map we can imagine anchor box which just slides around the image and we place anchor box at every position in CNN feature map coming out of backbone network.Now, our task is to train little CNN that will classify these anchor boxes as either containing an object or not containing an object.Here we have one problem, so the question is, What if anchor box may have wrong shape or aspect ratio?So solution for this is, we use ‘k’ different anchor boxes which are of different shape and different aspect ratio at each point in an image. So, scale, size, number of anchor boxes are the hyper-parameters for object detection.Basically, Faster-RCNN is two staged process:1st stage: Object Detector consistingBackbone NetworkRegion Proposal Network2nd stage: Run once per Region where we,Crop Features → Region of Interest PoolingPredict Object ClassPredict Bounding Box offsetNow, the question arises, Do we really need a second stage?And it kind of seems we could actually get away with using just 1st stage and ask this 1st stage to do everything. And this will simplify system a bit and make it even faster because we would not have to run these separate computation per region.So there is a method for object detection called Single Stage Object Detector which basically looks like Region Proposal Network (RPN) in case of Fast-RCNN and rather than classifying anchor boxes as object or not object instead it will make full classification decision for category of object.So this gives us the way for the two single stage detectors,We will see YOLO and its implementation in next article.medium.comIn this post, you discovered a gentle introduction to the problem of object detection and state-of-the-art deep learning models designed to address it.Specifically, you learned:R-CNN Family Papers",28/08/2021,0,60.0,26.0,863.0,536.0,14.0,7.0,0.0,15.0,en
3914,LSTM by Example using Tensorflow,Towards Data Science,Rowel Atienza,895.0,6.0,971.0,"In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term Memory (LSTM) because it is robust against the problems of long-term dependency. There is no shortage of articles and references explaining LSTM. Two recommended references are:Chapter 10 of Deep Learning Book by Goodfellow et. al.Understanding LSTM Networks by Chris OlahThere is also no shortage of good libraries to build machine learning applications based on LSTM. In GitHub, Google’s Tensorflow has now over 50,000 stars at the time of this writing suggesting a strong popularity among machine learning practitioners.What seems to be lacking is a good documentation and example on how to build an easy to understand Tensorflow application based on LSTM. This is the motivation behind this article.Suppose we want to train a LSTM to predict the next word using a sample short story, Aesop’s Fables:long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .Listing 1. A short story from Aesop’s Fables with 112 unique symbols. Words and punctuation marks are both considered symbols.If we feed a LSTM with correct sequences from the text of 3 symbols as inputs and 1 labeled symbol, eventually the neural network will learn to predict the next symbol correctly (Figure 1).Figure 1. LSTM cell with three inputs and 1 output.Technically, LSTM inputs can only understand real numbers. A way to convert symbol to number is to assign a unique integer to each symbol based on frequency of occurrence. For example, there are 112 unique symbols in the text above. The function in Listing 2 builds a dictionary with the following entries [ “,” : 0 ] [ “the” : 1 ], …, [ “council” : 37 ],…,[ “spoke” : 111 ]. The reverse dictionary is also generated since it will be used in decoding the output of LSTM.Listing 2. Function for building the dictionary and reverse dictionary.Similarly, the prediction is a unique integer identifying the index in the reverse dictionary of the predicted symbol. For example, if the prediction is 37, the predicted symbol is actually “council”.The generation of output may sound simple but actually LSTM produces a 112-element vector of probabilities of prediction for the next symbol normalized by the softmax() function. The index of the element with the highest probability is the predicted index of the symbol in the reverse dictionary (ie a one-hot vector). Figure 2 shows the process.Figure 2. Each input symbol is replaced by its assigned unique integer. The output is a one-hot vector identifying the index of the predicted symbol in the reverse dictionary.At the core of the application is the LSTM model. Surprisingly, it is very simple to implement in Tensorflow:Listing 3. The model with a 512-unit LSTM cellThe trickiest part is feeding the inputs in the correct format and sequence. In this example, the LSTM feeds on a sequence of 3 integers (eg 1x3 vector of int).The constants, weights and biases are:Listing 4. Constants and training parametersIn the training process, at each step, 3 symbols are retrieved from the training data. These 3 symbols are converted to integers to form the input vector.Listing 5. Symbols to vector of int as inputThe training label is a one-hot vector coming from the symbol after the 3 input symbols.Listing 6. One-hot vector as labelAfter reshaping to fit in the feed dictionary, the optimization runs:Listing 7. Training step optimizationThe accuracy and loss are accumulated to monitor the progress of the training. 50,000 iteration is generally enough to achieve an acceptable accuracy.Listing 8. Sample prediction and accuracy data per training subsession (1000 steps)The cost is a cross entropy between label and softmax() prediction optimized using RMSProp at a learning rate of 0.001. RMSProp performs generally better than Adam and SGD for this case.Listing 9. Loss and optimizerThe accuracy of the LSTM can be improved by additional layers.Listing 10. Improved LSTMNow, the fun part. Let us generate a story by feeding back the predicted output as next symbol in the inputs. The input for this sample output is “had a general” and it predicted the correct output “council”. The “council” is fed back as part of the new input “a general council” to predict a new output “to”, and so on. Surprisingly, LSTM creates a story that somehow makes sense.Listing 11. Sample story generated story. Truncated to max of 32 predictions.If we feed another sequence (eg “mouse”, “mouse”, “mouse”) but not necessarily a sequence found in the story, another narrative is automatically created.Listing 12. Inputs with a sequence not found in the story.The actual sample code can be found here. The sample text file is here.Final notes:",17/03/2017,11,12.0,8.0,551.0,429.0,2.0,1.0,0.0,7.0,en
3915,Illustrated Guide to LSTM’s and GRU’s: A step by step explanation,Towards Data Science,Michael Phi,3800.0,10.0,2003.0,"Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine Learning Engineer in the AI voice assistant space.In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.You can also watch the video version of this post on youtube if you prefer.Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN’s may leave out important information from the beginning.During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t contribute too much learning.So in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually the earlier layers. So because these layers don’t learn, RNN’s can forget what it seen in longer sequences, thus having a short-term memory. If you want to know more about the mechanics of recurrent neural networks in general, you can read my previous post here.towardsdatascience.comLSTM ’s and GRU’s were created as the solution to short-term memory. They have internal mechanisms called gates that can regulate the flow of information.These gates can learn which data in a sequence is important to keep or throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions. Almost all state of the art results based on recurrent neural networks are achieved with these two networks. LSTM’s and GRU’s can be found in speech recognition, speech synthesis, and text generation. You can even use them to generate captions for videos.Ok, so by the end of this post you should have a solid understanding of why LSTM’s and GRU’s are good at processing long sequences. I am going to approach this with intuitive explanations and illustrations and avoid as much math as possible.Ok, Let’s start with a thought experiment. Let’s say you’re looking at reviews online to determine if you want to buy Life cereal (don’t ask me why). You’ll first read the review then determine if someone thought it was good or if it was bad.When you read the review, your brain subconsciously only remembers important keywords. You pick up words like “amazing” and “perfectly balanced breakfast”. You don’t care much for words like “this”, “gave“, “all”, “should”, etc. If a friend asks you the next day what the review said, you probably wouldn’t remember it word for word. You might remember the main points though like “will definitely be buying again”. If you’re a lot like me, the other words will fade away from memory.And that is essentially what an LSTM or GRU does. It can learn to keep only relevant information to make predictions, and forget non relevant data. In this case, the words you remembered made you judge that it was good.To understand how LSTM’s or GRU’s achieves this, let’s review the recurrent neural network. An RNN works like this; First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one.While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.Let’s look at a cell of the RNN to see how you would calculate the hidden state. First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.The tanh activation is used to help regulate the values flowing through the network. The tanh function squishes values to always be between -1 and 1.When vectors are flowing through a neural network, it undergoes many transformations due to various math operations. So imagine a value that continues to be multiplied by let’s say 3. You can see how some values can explode and become astronomical, causing other values to seem insignificant.A tanh function ensures that the values stay between -1 and 1, thus regulating the output of the neural network. You can see how the same values from above remain between the boundaries allowed by the tanh function.So that’s an RNN. It has very few operations internally but works pretty well given the right circumstances (like short sequences). RNN’s uses a lot less computational resources than it’s evolved variants, LSTM’s and GRU’s.An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM’s cells.These operations are used to allow the LSTM to keep or forget information. Now looking at these operations can get a little overwhelming so we’ll go over this step by step.The core concept of LSTM’s are the cell state, and it’s various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the “memory” of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get’s added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.Gates contains sigmoid activations. A sigmoid activation is similar to the tanh activation. Instead of squishing values between -1 and 1, it squishes values between 0 and 1. That is helpful to update or forget data because any number getting multiplied by 0 is 0, causing values to disappears or be “forgotten.” Any number multiplied by 1 is the same value therefore that value stay’s the same or is “kept.” The network can learn which data is not important therefore can be forgotten or which data is important to keep.Let’s dig a little deeper into what the various gates are doing, shall we? So we have three different gates that regulate information flow in an LSTM cell. A forget gate, input gate, and output gate.First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.To review, the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.For those of you who understand better through seeing the code, here is an example using python pseudo code.1. First, the previous hidden state and the current input get concatenated. We’ll call it combine.2. Combine get’s fed into the forget layer. This layer removes non-relevant data.4. A candidate layer is created using combine. The candidate holds possible values to add to the cell state.3. Combine also get’s fed into the input layer. This layer decides what data from the candidate should be added to the new cell state.5. After computing the forget layer, candidate layer, and the input layer, the cell state is calculated using those vectors and the previous cell state.6. The output is then computed.7. Pointwise multiplying the output and the new cell state gives us the new hidden state.That’s it! The control flow of an LSTM network are a few tensor operations and a for loop. You can use the hidden states for predictions. Combining all those mechanisms, an LSTM can choose which information is relevant to remember or forget during sequence processing.So now we know how an LSTM work, let’s briefly look at the GRU. The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.The reset gate is another gate is used to decide how much past information to forget.And that’s a GRU. GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s. There isn’t a clear winner which one is better. Researchers and engineers usually try both to determine which one works better for their use case.To sum this up, RNN’s are good for processing sequence data for predictions but suffers from short-term memory. LSTM’s and GRU’s were created as a method to mitigate short-term memory using mechanisms called gates. Gates are just neural networks that regulate the flow of information flowing through the sequence chain. LSTM’s and GRU’s are used in state of the art deep learning applications like speech recognition, speech synthesis, natural language understanding, etc.If you’re interested in going deeper, here are links of some fantastic resources that can give you a different perspective in understanding LSTM’s and GRU’s. This post was heavily inspired by them.http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theanohttp://colah.github.io/posts/2015-08-Understanding-LSTMs/https://www.youtube.com/watch?v=WCUNPb-5EYII had a lot of fun making this post so let me know in the comments if this was helpful or what you would like to see in the next one. And as always, thanks for reading!Check out michaelphi.com for more content like this.✍🏽 Want more Content? Check out my blog at https://www.michaelphi.com📺 Like to watch project-based videos? Check out my Youtube!🥇 Stay up to date on articles and videos by signing up for my email newsletter!",24/09/2018,0,6.0,6.0,1027.0,485.0,19.0,0.0,0.0,8.0,en
3916,Amazon Adds Photographic Product Search To iOS App,IPG Media Lab,IPG Media Lab,2000.0,1.0,100.0,"Amazon is raising the stakes of showrooming for retailers once again, folding its “Flow” technology, previously found in a standalone app released by its subsidiary, A9, into its main shopping app for iOS. “Flow” is visual product search, allowing users to photograph an object and see details about it on Amazon, which is even simpler than the previous norm of barcode recognition. Amazon’s competitive pricing is its main advantage in comparison to retailers, and by more effectively using other retailers as showrooms for the products it sells, it has the potential to further extend its dominance in more consumer categories.",07/02/2014,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,en
3917,How to cluster images based on visual similarity,Towards Data Science,Gabe Flomo,29.0,6.0,1004.0,"In this tutorial, I'm going to walk you through using a pre-trained neural network to extract a feature vector from images and cluster the images based on how similar the feature vectors are.The pre-trained model that will be used in this tutorial is the VGG16 convolutional neural network (CNN), which is considered to be state of the art for image recognition tasks. We are going to be using this model as a feature extractor only, meaning that we will remove the final (prediction) layer so that we can obtain a feature vector.This implementation will use the flowers dataset from Kaggle which you can download here. The dataset contains 210 images of 10 different species of flowers that will be downloaded as png files.Before we get started, we need to import the modules needed in order to load/process the images along with the modules to extract and cluster our feature vectors.Now that the data is downloaded on your computer, we want python to point to the location where the images are located. This way instead of loading a whole file path, we can simply just use the name of the file.Now that we have all of the filenames loaded into the list of flowers, we can start preprocessing the images.This is where we put the load_img() and preprocess_input() methods to use. When loading the images we are going to set the target size to (224, 224) because the VGG model expects the images it receives to be 224x224 NumPy arrays.Currently, our array has only 3 dimensions (rows, columns, channels) and the model operates in batches of samples. So we need to expand our array to add the dimension that will let the model know how many images we are giving it (num_of_samples, rows, columns, channels).The last step is to pass the reshaped array to the preprocess_input method and our image is ready to be loaded into the model.Now we can load the VGG model and remove the output layer manually. This means that the new final layer is a fully-connected layer with 4,096 output nodes. This vector of 4,096 numbers is the feature vector that we will use to cluster the images.Now that the final layer is removed, we can pass our image through the predict method to get our feature vector.Heres the all the code in a single functionNow we can use this feature_extraction function to extract the features from all of the images and store the features in a dictionary with filename as the keys.Since our feature vector has over 4,000 dimensions, your computer will thank you if you reduce the number of dimensions from 4,000 to a much smaller number. We can't simply just shorten the list by slicing it or using some subset of it because we will lose information. If only there was a way to reduce the dimensionality while keeping as much information as possible.Enter the realm of principle component analysis.I'm not going to waste time explaining what PCA is because there are already tons of articles explaining it, which I’ll link here.Simply put, if you are working with data and have a lot of variables to consider (in our case 4096), PCA allows you to reduce the number of variables while preserving as much information from the original set as possible.The number of dimensions to reduce down to is up to you and I'm sure there's a method for finding the best number of components to use, but for this case, I just chose 100 as an arbitrary number.Now that we have a smaller feature set, we are ready to cluster our images.You’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.This algorithm will allow us to group our feature vectors into k clusters. Each cluster should contain images that are visually similar. In this case, we know there are 10 different species of flowers so we can have k = 10.[6, 6, 8, 6, 6, 5, 4, 6, 5, 6, 4, 6, 6, 3, 3, 5, 6, 6, 4, 4, 8, 1, 3, 8, 4, 2, 8, 4, 2, 6, 9, 7, 4, 4, 0, 5, 4, 9, 8, 5, 9, 5, 3, 6, 5, 1, 3, 9, 6, 5, 0, 1, 3, 9, 6, 7, 4, 6, 4, 5, 8, 5, 3, 6, 5, 4, 6, 5, 2, 1, 4, 3, 9, 5, 4, 6, 2, 4, 5, 0, 5, 1, 2, 9, 5, 4, 8, 1, 7, 1, 3, 5, 4, 8, 5, 4, 6, 9, 5, 9, 5, 8, 1, 4, 9, 8, 5, 4, 5, 6, 4, 1, 8, 9, 4, 6, 5, 7, 5, 6, 4, 8, 1, 4, 5, 5, 8, 6, 5, 2, 4, 8, 5, 1, 1, 6, 6, 7, 8, 1, 9, 1, 6, 4, 8, 3, 6, 1, 0, 0, 8, 1, 3, 4, 9, 9, 0, 4, 0, 6, 4, 9, 0, 3, 5, 0, 3, 9, 9, 4, 9, 5, 0, 9, 5, 4, 5, 1, 8, 3, 6, 4, 5, 2, 6, 6, 9, 5, 0, 3, 1, 3, 5, 4, 5, 0, 9, 4, 2, 1, 0, 9, 4, 9, 1, 2, 6, 1, 6, 0]Each label in this list is a cluster identifier for each image in our dataset. The order of the labels is parallel to the list of filenames for each image. This way we can group the images into their clusters.All we have left to do is to view a cluster to see how well our model did by inspecting the clusters.Here we can see that our model did pretty well on clustering the flower images. We can even see that cluster 2 and cluster 0 both have yellow flowers yet, the type of flowers in each cluster are different species.Here is the whole process in one file.Hope you all learned something new, leave a comment if you have any questions or had an aha moment :)",29/09/2020,6,19.0,1.0,1400.0,436.0,4.0,2.0,0.0,7.0,en
3918,A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way,Towards Data Science,Sumit Saha,1500.0,7.0,1300.0,"Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines. Researchers and enthusiasts alike, work on numerous aspects of the field to make amazing things happen. One of many such areas is the domain of Computer Vision.The agenda for this field is to enable machines to view the world as humans do, perceive it in a similar manner and even use the knowledge for a multitude of tasks such as Image & Video recognition, Image Analysis & Classification, Media Recreation, Recommendation Systems, Natural Language Processing, etc. The advancements in Computer Vision with Deep Learning has been constructed and perfected with time, primarily over one particular algorithm — a Convolutional Neural Network.A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.An image is nothing but a matrix of pixel values, right? So why not just flatten the image (e.g. 3x3 image matrix into a 9x1 vector) and feed it to a Multi-Level Perceptron for classification purposes? Uh.. not really.In cases of extremely basic binary images, the method might show an average precision score while performing prediction of classes but would have little to no accuracy when it comes to complex images having pixel dependencies throughout.A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better.In the figure, we have an RGB image which has been separated by its three color planes — Red, Green, and Blue. There are a number of such color spaces in which images exist — Grayscale, RGB, HSV, CMYK, etc.You can imagine how computationally intensive things would get once the images reach dimensions, say 8K (7680×4320). The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets.Image Dimensions = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg. RGB)In the above demonstration, the green section resembles our 5x5x1 input image, I. The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel/Filter, K, represented in the color yellow. We have selected K as a 3x3x1 matrix.The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering.The filter moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between Kn and In stack ([K1, I1]; [K2, I2]; [K3, I3]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output.The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying Valid Padding in case of the former, or Same Padding in the case of the latter.When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name — Same Padding.On the other hand, if we perform the same operation without padding, we are presented with a matrix which has dimensions of the Kernel (3x3x1) itself — Valid Padding.The following repository houses many such GIFs which would help you get a better understanding of how Padding and Stride Length work together to achieve results relevant to our needs.github.comSimilar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.There are two types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Some of them have been listed below:GitHub Notebook — Recognising Hand Written Digits using MNIST Dataset with TensorFlowgithub.com",15/12/2018,1,24.0,0.0,695.0,456.0,12.0,1.0,0.0,2.0,en
3919,Solving the FrozenLake environment from OpenAI gym using Value Iteration,Analytics Vidhya,Diganta Kalita,40.0,6.0,963.0,"So I was trying to learn about Reinforcement Learning, and then I came across this thing called ‘Value Iteration’. I really couldn’t wrap my head around Value Iteration. It was very difficult for me to understand how it worked and how it could help an agent to find the optimal policy. Then I got an idea.What better way to understand “Value Iteration” than to use it to solve some game or environment. Thus I began my journey to find some game easy enough problem to solve. And then I stumbled upon this fairy from OpenAI.Let me explain the game/environment first.There are 64 states in the game. The agent starts from S (S for Start) and our goal is to get to G (G for Goal). So just go. Nope. Its a slippery surface. The F’s and the H’s in between are pretty weird stuff. So F means Frozen Surface. You can walk on them. But H means Hole. If you fall in a H, BOoom, GAME OVER for you and start from S again. So just go through all the F’s dodging the H’s to reach the G right. Nope. There’s more. Since this is a “Frozen” Lake, so if you go in a certain direction, there is only 0.333% chance that the agent will really go in that direction. I mean, the movement of the agent is uncertain and only partially depends on the chosen direction. So you won’t always move in the direction you intend. For a more detailed explanation of FrozenLake8x8 , Click Here.Okay, so that being understood how do we load the environment from OpenAI. For doing that we will use the python library ‘gym’ from OpenAI.You can have a look at the environment using env.render() where the red highlight shows the current state of the agent.env.action_space.sample() chooses an action randomly from all the possible actions. And env.step(action) takes a step according to the given action. Here we can see the action was ‘Right’, so the agent went right from S to F (This may not always be the case, since the movement of the agent is uncertain so sometimes when the action is ‘Right’, the agent might go Down or Up also.)Okay that was the easy part. Now comes the difficult part or should I say interesting part. How does the agent navigate this slippery Lake and get to the Goal without falling in a Hole?Lets do this step by step. First lets code the “Value Iteration” function.So in value iteration the story goes like this. For a particular state, first we calculate the state-action values for all the possible actions from that state, and then update the value function of that state with the greatest state-action value. This is different from “Policy Iteration” where we calculate the expected/mean state-action value. The Value Iteration terminates when the difference between all the new State values and the old State Values is a negligibly small value.Below is the code I used for the value iteration function.The majority of the code is easily understandable. Let me explain the non-intuitive parts.env.nS and env.nA gives the total number of states and actions resp. But the most interesting is env.P ; env.P[0] outputs a dictionary like this. Here 0 in env.P[0] is the first state of the environment.Here as you can guess, the keys of the dictionary 0,1,2,3 are the actions we can state from state 0. And further each action contains a list, where each element of the list is a tuple showing the probability of transitioning into the state, next state, reward and if done=True done=False. (done=True if the next state is a Hole or the Goal). So env.P is a list containing all the states where each state contains a dictionary which maps all possible actions from that state to the next_state if we take that action, probability of going into that next state, reward and if the Game terminates there or not.Here you can see 54 is a Hole so done=True. Also 63 is the Goal so done=True.So as per the Value Iteration formula, we iterate through all these actions and calculate the action-state value using the formula:Prob * (reward + discount_factor * state_value of next state)which are all provided in env.P . Then we update the value function of the state with the highest state-action value. We iterate through all the 64 states of the environment, till the difference between the new State Values and Old State Values after each iteration is negligibly small or if we have crossed the maximum number of iterations.Now that we have the value function of all the states, our next step is to extract the policy from the Value Function.We do this using a similar technique. For a particular state we calculate the state-action values of all the possible actions from that state and choose the action with the highest state-action value.Finally! Now that we have the policy we can follow that policy and see if our agent reaches the goal or falls in a hole.We run the agent for 1000 episodes and calculate how many average steps it took to get to the Goal. We also calculate how many times it could not reach the goal and fell in a hole. Finally we get this answer after running the above function.I think the agent did pretty good :)You can also check out FrozenLake-v0 which is a smaller version and has only 16 states and check how many average steps it takes the agent to get to the goal. For my full code to solve the FrozenLake8x8 environment go to my GitHub repo here : https://github.com/realdiganta/solving_openai/tree/master/FrozenLake8x8Also as I continue my journey into the exciting land of Reinforcement Learning, I would solving more OpenAI environments in the near future. So stay tuned for more.",28/11/2019,0,6.0,17.0,679.0,345.0,9.0,1.0,0.0,6.0,en
3920,Neural Networks Backpropagation Made Easy,Towards Data Science,"Lihi Gur Arie, PhD",65.0,7.0,982.0,"Understanding the mathematic operands behind Neural Networks (NNs) is highly important for the data scientist capabilities, in designing an efficient deep model. In this article, the high-level calculus of a fully connected NN will be demonstrated, with focus on the backward propagation step. The article is oriented to people with basic knowledge of NNs, that seek to dive deeper into the NNs structure.The objective of the training process is to find the weights (W) and biases (b) that minimize the error. It is done by the gradient descent algorithm. To begin with, the weights are randomly initialized, and an iterative process of a subtle weights change is performed until convergence.Each iteration begins with a forward pass, that outputs the current prediction and evaluates the model’s error by a cost function J (equation 6). Next, a backward pass is conducted to compute the weights gradient.To find the best parameters that minimize the error, we use the gradient descent algorithm. The goal is to find the minimum point of the cost function (J), where the gradient is close to zero (Figure 2). The algorithm iteratively moves, step by step, in the direction of steepest descent, toward the minimum point. The step size is named the ‘learning rate’, and it is a scalar that determines how much the weights change in each iteration, and therefore how quickly the NN converges. The learning rate is a hyperparameter that has to be tuned. Smaller learning rates require more training time, whereas larger learning rates result in rapid training, but might suffer from compromised performances and instability.To update the weights, the gradients are multiplied by the learning rate (alpha), and the new weights are calculated by the following formula:As the model iterates, the gradient gradually converges toward zero, where the error is most likely the lowest (Alternatively, the model might converge to a local optimum and present sub-optimal performances).This article will follow the structure of a two layered Neural Network, where X (also named A[0]) is the input vector, A[1] is a hidden layer, and Y-hat is the output layer. The basic architecture of the net is illustrated in the picture below (The neurons amount in each layer are irrelevant to the equations, since we are using the vectorized equations):To perceive how the backward propagation is calculated, we first need to overview the forward propagation. Our net starts with a vectorized linear equation, where the layer number is indicated in square brackets.Next, a non linear activation function (A) is added. This activation enables the net to break linearity and adjust to complex patterns in the data. Several different activation functions can be used (e.g. sigmoid, ReLU, tanh), and here we will use the sigmoid activation for our NN.So far we computed the first layer. The second layer, like the first, is composed of a linear equation (Z[2]), followed by a sigmoid activation (A[2]). Since this is the last layer in our net, the activation result (A[2]) is the model’s prediction (Y-hat).Finally, to evaluate and minimize the error, we define a cost function (For further reading on activations and cost functions go to reference [1]). Here we are using the ‘Mean Squared Error’ (MSE) function. For simplicity, we will use Stochastic Gradient Descent (SGD) method [2], meaning that only one sample is processed in each iteration.We can summarize the computational graph of the forward passIn order to update the weights and biases after each iteration, we need to compute the gradients. In our two layered net there are 4 parameters to be updated: W[2], b[2], W[1] and b[1], and therefore 4 gradients to be computed:But how are we going to find the derivatives of these composite functions? According to the ‘chain rule’, we’ll construct the product of the derivatives along all paths connecting the variables. Lets follow the weights of the second layer (W[2]) gradient:From the diagram above we can clearly see that the change in the cost J with respect to W[2] is:To resolve this, we’ll start by computing the partial derivative of the cost J with respect to A[2], which is also the prediction Y-hat. The original cost function is shown on the left, and the derivative on the right:The partial derivative of the sigmoid activation in A[2], with respect to Z[2] is represented by the following (The mathematical development of the sigmoid derivative is described in ref [3]):The partial derivative of Z[2] with respect to the weights W[2]:Lets chain everything together, to compute the W[2] gradient:Great! Next, we will compute the b[1] gradient in a similar way. Lets follow the gradient:The first two parts of the b[2] gradient were already calculated above (The partial derivative of the cost J with respect to Z[2]), and the last part is equal to 1:So the overall b[2] gradient is:At last, we finished computing the gradients for the second layer. The gradients for first layer are a bit longer, but we already computed parts of it. Lets follow the gradient for W[1] update:The first two parts of the gradient were previously calculated for layer 2. The partial derivative of Z[2] with respect to A[1], is W[2]:The last two parts are calculated in the same manner as it was calculated in layer 2. Taking it together, we get:And if we follow the b[1] gradient:We’ll get:By this part we finished computing all the gradients of the weights and biases for one iteration of our NN.Once computing the gradients, we can update the new parameters for the model, and iterate all over again until the model converges. Denote that alpha is the learning rate, a hyperparameter that will determine the convergence rate.I hope this article helped to gain a deeper understanding of the mathematics behind neural networks. In this article, I’ve explained the working of a small network. However, these basic concepts can be generalized and applicable to deeper neural networks.References[1] Activation and cost functions. https://medium.com/@zeeshanmulla/cost-activation-loss-function-neural-network-deep-learning-what-are-these-91167825a4de[2] Stochastic gradient descent. https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31[3] sigmoid derivative. https://becominghuman.ai/what-is-derivative-of-sigmoid-function-56525895f0eb",08/05/2021,0,9.0,3.0,710.0,172.0,26.0,0.0,0.0,9.0,en
3921,"ICML 2018: Advances in transfer, multitask, and semi-supervised learning",Towards Data Science,Isaac Godfried,1200.0,8.0,1686.0,"The International Conference on Machine Learning took place last July in Stockholm. Altogether it showcased many interesting trends and directions in machine learning. Since, ICML was such a huge conference I will focus my attention on a few (of the many) interesting strands going on at the conference.Specifically, this year’s ICML split the oral talks into several different “tracks/sessions.” I was happy to see three of theses sessions focused on “transfer and multitask learning” as this has long been an area of interest of mine. Additionally, a large number of posters dealt with theses concepts as well as several orals from other tracks.Lack of large amounts of clean labeled data remains a barrier to the potential impact of deep learning. For many tasks there is an overall lack of data points (e.g., forecasting elections, diagnosing rare diseases, translating into rare or extinct languages …). In other cases the data is there but it is noisy or poorly labeled (e.g., images scraped from Google under a specific keyword, medical cases assigned labels through NLP, a text corpus only partially annotated). Whatever the reason, there is a tangible benefit to finding methods to learn from either limited or noisy (semi-related) data.Three such approaches to this are transfer learning, multi-task (this is technically a subcategory of transfer learning like domain adaptation, but for this article I will treat them as separate entites), and semi-supervised learning. There are other approaches (active learning, meta-learning, entirely unsupervised learning), but this article will focus on ICML articles related to the three (especially the first two). As the boundaries between these areas aren’t always clear, we might venture into some of the others as well. For readers needing review, here is a brief overview. For a more detailed overview, see Sebastian Ruder’s excellent blog post on transfer learning and multi-task learning.I have always found transfer learning and multi-task learning to be very important tools regardless of industry or domain. Whether you work in medicine, finance, travel, or recreation and whether you work with images, text, audio, or time series data, chances are that you can benefit from taking general pre-trained models and fine-tuning them to your specific domain. Depending on your data, it is also highly likely that there are multiple related tasks that you can train your neural network to learn to solve jointly and hence increase overall performance.Of particular interest to those who focus on deep learning for medicine (but useful for others as well), was a paper titled “Not to Cry Wolf: Distantly Supervised Multitask Learning Critical Care.” In ICU wards there is often a problem of false alarms, so many that nurses/doctors become desensitized to them. This paper focused on detecting the actual life-threatning ICU events instead of the false alarms using multi-task and semi-supervised learning. The paper’s authors looked at using multi-task learning with auxillary tasks to improve the performance of the model without requiring a lot of time spent annotating. Specifically, their model “incorporates a large amount of distantly supervised auxiliary tasks in order to significantly reduce the number of expensive labels required for training.” Secondly, they developed a new approach “to distantly supervised multitask learning that automatically identifies a large set of related auxiliary tasks from multivariate time series to jointly learn from labelled and unlabelled data.” The video of the talk is availible on YouTube.What if you want the benefits of multitask learning but only have one task? The paper “Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing — and Back” deals with this issue. The authors propose utlizing pseudo-tasks to help increase the performance of the main-task. This is possible because on a basic level, multitask learning often works by sharing features between intermediate and upper layers and learning task specific decoders for the specific tasks. Hence, training a model with multiple decoders should incur the same benefits even if the decoders are all for the same task because each decoder learns the task in different ways; these additional decoders are called “psuedo-tasks.” The authors of the paper achieve SOTA results on the CelebrityA dataset. I was pleased to see they also tested on the IMDB sentiment dataset. They used a baseline model and showed significant improvements by training with their technique. This shows that the technique can potentially work with multiple different neural network architectures.GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask NetworksThis paper describes a new normalization technique for multi-task NNs that help them converge faster and increase overall performance. It also reduces the overall number of hyperparameters needed to tune to just one. Using GradNorm they acheive SOTA on the NYU2 dataset. Overall, this is a solid paper that can help reduce the complexity and difficulties of training MLT algorithms. Finally, the authors make the interesting observation that “GradNorm may have applications beyond multitask learning. We hope to extend the GradNorm approach to work with class-balancing and sequence-to-sequence models, all situations where problems with conflicting gradient signals can degrade model performance.”Transfer Learning via Learning to TransferUp to this point most transfer learning papers have only studied transfer knowledge from a source domain to target domain, either by pre-initializng the weights and freezing layers or through decaying the learning rate. This paper can best be described as “meta-transfer learning” or learning how to best perform transfer learning tasks (L2T). The authors describe that:Unlike L2T, all existing transfer learning studies transfer from scratch, i.e., only considering the pair of domains of interest but ignoring previous transfer learning experiences. Better yet, L2T can even collect all algorithms’ wisdom together, considering that any algorithm mentioned above can be applied in a transfer learning experienceNow this naturally leads to the question of how this is different from “meta-learning.” In reality L2T can be seen as a special type of meta-learning; like with meta-learning, it uses past histories to improve how it learns. However, in this context a history refers to a transfer learning task from a source to target domain.The authors of the paper evaluate there L2T framework on Caltech-256 and sketches datasets. The model improves on previous SOTA results particularly in cases where there are few examples.I was happy to see “Explicit Inductive Bias for Transfer Learning with Convolutional Networks” get into ICML after (in my opinion unfairly) being rejected from ICLR. This paper describes a way of applying regularization to effectively engage in transfer learning instead of modifying the learning rate. The authors propose several new regularization methods which apply different penalties based on the weights in the pre-trained model. They acheive good experimental results and I’m currently working on applying it to several of my medical imaging models.“Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks” is primarily a theoretical paper that investigates “curriculum learning,” an idiom of learning borrowed from education/psychology that aims to learn more difficult concepts in a progressive and organized fashion. Specifically, the paper looks at the relationship between transfer learning and curriculum learning as well as the relationship between curriculum learning and the order of examples presented for training and its impact on stochastic gradient descent. It is important to note here that this type of transfer is not the same as the other types discussed so far. In this context transfer learning refers to investigating “the transfer of knowledge from one classifier to another, as in teacher classifier to student classifier.” Hence, with this type of transfer learning, “it is not the instance representation which is being transferred but rather the ranking of training examples.” The authors conclude that the learning rate is always faster with curriculum learning and that sometimes final generalization is improved particularly with respect to hard tasks.Learning Semantic Representations for Unsupervised Domain AdaptationOne problem in (unsupervised) domain adaptation is aligining between the target and source distribution. Unsupervised domain adaptation is a type of transfer learning . The authors here develop a semantic transfer network that learns representations “for unlabeled target samples by aligning labeled source centroid and pseudo-labeled target centroid.” More simply their method aims to align the distributions of the source and target based on minimizing the overall mapping discrepancy between the source and target domains via a semantic loss function. Results include SOTA performance on both the ImageCLEF-DA and Office31 datasets. Their code is availible online by clicking hereDetecting and Correcting for Label Shift with Blackbox Predictors is another interesting paper related to domain adaptation. It focuses on how to detect changes to the y distribution between training and testing, which can be useful particularly in medicine if some epidemic or outbreak of a disease occurs which greatly affects the distribution .Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labelsThe specific topic of the paper is primarily covariate shift. The authors develop several interesting label shift simulations which they then apply on the CIFAR-10 dataset as well as MINST. Their methods are able to greatly increase accuracy compared to the non-corrected model.Rectify Heterogeneous Models with Semantic MappingI found this paper interesting for its incorporation of optimal transport for the purpose of aligning distributions.Optimal Transport (OT) becomes the main tool in REFORM, which has the ability to align distributionsAltogether, this paper presents original ideas and obtains good results on both synthetic datasets and real world datasets including the Amazon User Click dataset and the Academic paper classification dataset.These were just a few of the interesting papers from ICML 2018; there are many other great papers. I do hope at a some point to summarize the meta-learning and the rest of the semi-supervised learning papers. I found these papers fascinating as well.I’m still working on finishing up the next article in my series on deploying machine learning models to production. In that article I will discuss using SeldonCore and Kubeflow to deploy machine learning models in a scalable way.Northern New England Data and Analytics is hosting a data meetup on August 15th where we will walk through deploying a recent NLP model with Seldon Core and Kubeflow to make use of it in a chatbot. The Meetup will be streamed on Zoom.",08/08/2018,0,14.0,6.0,1143.0,472.0,4.0,0.0,0.0,14.0,en
3922,Run StyleGAN2 ADA on an AWS Spot Instance in No Time,Towards Data Science,Oleg Polosin,65.0,4.0,428.0,"Recently NVIDIA published a paper called “Training Generative Adversarial Networks with Limited Data” and released the code. They proposed an adaptive discriminator augmentation (ADA) mechanism that stabilizes StyleGAN2 training and achieves significantly better results on small datasets.In this post, we’ll show how to quickly run this code on an AWS Spot instance.“A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly.”— Spot Instances, AWS DocumentationTo launch a Spot instance and run a Docker container with the environment, we will be using Spotty. Spotty is an open-source tool designed to simplify the development of deep learning models in the cloud.Use pip to install the latest version:Spotty only requires a spotty.yaml configuration file in the root directory of your project that describes instance and container parameters. And we already prepared it for you in this fork of the original repo.Use the following command to clone the project:By default, Spotty will launch a p2.xlarge Spot instance in the us-east-1 region, but, if necessary, you always can change these parameters in the spotty.yaml file.Before launching an instance, make sure you’ve installed and configured AWS CLI. For more information, see Installing the AWS CLI and Configuration basics.To launch an instance, run the following command from the project root directory:Wait until the instance is started:That’s it! Just connect to the container and play with the code:Spotty uses tmux, so you won’t lose your progress if your internet connection is gone. For more information on tmux, please, read the official documentation or Tmux Cheat Sheet.Once connected to the container, use the following command to generate images using a pre-trained model:This command will generate 4 images in the out/metfaces/ directory. You can download them to your local machine using the following command:In this example, we’ll show how to use custom Spotty scripts.To find a latent vector and generate a progression video, put a target image target.jpg to the data/ directory on your local machine and run the following command:The script will be running inside a tmux session. So, even if your internet connection is gone, the process will still be running, and you can reattach to it any time later.Once the script is finished, kill the tmux window using the Ctrl+b, x combination of keys and download the result to your local machine:Don’t forget to stop the instance once you’re done! Use the spotty stop command. And remove the EBS volume if you don’t need the data anymore.",02/11/2020,9,1.0,1.0,1400.0,437.0,2.0,0.0,0.0,14.0,en
3923,Recurrent Neural Network that impersonates F. Pessoa,Towards Data Science,Luís Roque,607.0,11.0,1552.0,"Sequences of discrete tokens can be found in many applications, namely words in a text, notes in a musical composition, pixels in an image, actions in a reinforcement learning agent, etc [1]. These sequences often show a strong correlation between consecutive or nearby tokens. The correlations on words in a sentence or characters in words express the underlying semantics and language characteristics. The next token in the sequence x_n can be modeled as:where x_i represents the ith token in the sequence. In Natural Language Processing (NLP), these are defined as language models. Usually, each token stands for a separate word or n-gram. The output generated is a probability distribution from which we can sample to generate the next token in the sequence. These models are also known as recurrent, as we can apply this generative process recurrently to create entire new sequences of tokens.One particular type of generative model often used to tackle problems with sequences of discrete tokens is Recurrent Neural Networks (RNN). In a simpler neural network, a fixed-dimensional feature representation is transformed several times by different non-linear functions. In an RNN, these transformations are also repeated in time, which means that at every time step, a new input is processed, and a new output is generated. They can effectively capture semantically rich representations of the input sequences [2]. RNN showed this capacity in different settings, such as generating structured text, original images (on a per pixels basis), or even modeling user behavior on online services.Our task is to generate original text that resembles a training corpus. It is an unsupervised task, as we do not have access to any labeling or target variable. We start by creating a word embedding that maps each character to a vector with a parameterized dimension. For each character, the model looks up the embedding and feeds the result to a stack of Long Short-Term Memory (LSTM) layers, a specific type of RNN. These were developed to extend the traditional capacity of RNNs to model long-term dependencies and counter the vanishing gradient problem. The output of our network is a dense layer with a number of units equal to the vocabulary size. We did not define an activation function for this layer; it simply outputs one logit for each character in the vocabulary. We use these values to later sample from a categorical distribution.In this article, we use the work of Fernando Pessoa, one of the most significant literary figures of the 20th century and one of the greatest poets in the Portuguese language. This dataset is now publicly available on Kaggle and consists of more than 4300 poems, essays, and other writings [3].The code is also available on Kaggle and GitHub.This article belongs to a series of articles on Deep Learning using TensorFlow:The dataset comprises several texts written by the author under his own name but also using different heteronyms and pseudonyms. Each one has his own style of writing, which could be interesting to learn separately. Nevertheless, to efficiently train Deep Neural Networks (DNN), we need a large dataset and that was the reason to build a single model.F. Pessoa lived part of his youth in South Africa, where he was exposed to the English language. That is why part of his work is written in English. To avoid introducing noise, we remove most of the English texts from the training dataset.After cleaning up the texts, we end up with more than 5.8M of characters. Notice that to avoid losing data when normalizing the text length of our sequences, we split the largest sequences by sentence. The difference in the distribution of the sequence length can be seen in the histograms above. We can preview some of the sequences.More importantly, we can assess the number of unique characters, which is our vocabulary size.Before training, we need to convert the strings to some numerical representation. We started by tokenizing the text with some important aspects in mind. We considered an unlimited number of tokens and created them at the character level. We did not filter any character and kept the original capitalization. We then use the tokenizer to map our texts to encoded sequences.We can see an example of this encoding.We also need to normalize the length of our sequences, for which we define a length of 300 characters. Sequences smaller than 300 are padded with zeros, while sequences bigger than 300 are truncated.The RNN works by receiving a sequence of characters and predicting the next character in the sequence. At training time, the model receives an input sequence and a target sequence, which is shifted by one.For example, the expression Diana através dos ramos is the first verse of the first poem on our dataset. The poem is from Ricardo Reis, one of the many heteronyms of F. Pessoa. Given the input, Diana através dos ramo the correct prediction is iana através dos ramos. Notice that the prediction is the same length as the input.Another decision we took was to build our RNN to be stateful, which means that its internal state is maintained across batches. To be effective, we need to make sure that each batch element follows on from the corresponding element of the preceding batch.We started by defining an embedding layer that turns our indexes of characters into dense vectors of fixed size. It is important to note that padded values are masked in this layer, which means they are simply ignored. Next, we stacked 2 unidirectional stateful LSTM layers, each with 512 units. These layers have the potential to learn long-term dependencies; however, they are computationally expensive to train. In between them, we introduced a dropout layer. Finally, the last layer outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model. Notice that we get a total of about 4M parameters to train.The training is quite slow even using GPU (despite reducing the training time by a factor of 15 compared to CPU), and recall that we only stacked two LSTM layers with a limited number of units. From the figure below, we can see a rapid increase of the accuracy on both the training and validation datasets and then a steady climb for several epochs. Our callback is eventually executed (when there is no increase in the validation accuracy for more than 2 epochs) to stop the training process. There was no sign of overfitting.To generate text from our model, we need to specify a seed string to get the network started. Next, we tokenize the initial string and reset the state of the network. The string is then converted to a tensor with a batch size of 1 to be fed to our model. We used the prediction from the last time step to build a categorical distribution and sample from it afterward. Using the same state of our network and the previously sampled token, we can repeat the prediction step until we get the final sequence with the specified size.The resulting original text is quite interesting to analyze. Remember that our RNN had to learn the Portuguese language from scratch with a fairly small dataset. No explicit information such as syntax or semantics is provided to the model other than practical examples on writings in Portuguese. The dataset is also fairly small for the task. Nevertheless, there are interesting learnings to take notice of. For example, in terms of punctuation, the quotation marks are used correctly, showing the understanding that they are required to open and close. In sentences such as “Desassossego não poderia!… Falências no meu coração…” or “As canções… é um sono de ouvir… Ficção tanto!…” we can almost grasp some of the rentlessness of Fernando Pessoa. On the other hand, we see that the meaning or intention is not something that an RNN can capture, and we can also identify some orthographic errors.For this task, the preprocessing of the data is challenging. We need to ensure that we have our input sequence encoded in a suitable way for the RNN to capture the available semantic representation effectively. RNNs are computationally expensive to train, so we decided to keep the structure as simple as possible.We were able to generate text in Portuguese without proving any structural information about the language to the model other than the writings of a poet. The model learned some of the fundamental structure of the language while preserving nuances that we can consider similar to the training corpus.This approach can be extended by increasing the depth of the model with more recurrent layers and the number of units in each layer. Hyperparameters such as the batch size can also be tuned to increase accuracy. We tested the possibility to separate by the form of writing, training one DNN with texts in prose and another with texts in poetry. The results were not satisfying, as the DNNs failed to generate text with a coherent structure. We leave it as future work.[1] — [De Boom et al., 2018] De Boom, C., Demeester, T., and Dhoedt, B. (2018). Character-level recur-rent neural networks in practice: comparing training and sampling schemes.Neural Computing and Applications, 31(8):4001–4017.[2] — [Sutskever et al., 2011] Sutskever, I., Martens, J., and Hinton, G. (2011). Generating text with recurrent neural networks. ICML’11, page 1017–1024, Madison, WI, USA. Omnipress.[3] — https://www.kaggle.com/luisroque/the-complete-literary-works-of-fernando-pessoa",19/04/2021,11,1.0,7.0,699.0,454.0,4.0,1.0,0.0,14.0,en
3924,Advancements in Machine Learning Assisted Ideation.,Medium,Adam Pickard,13.0,5.0,924.0,"The most common debate around Artificial Intelligence and Machine Learning is “Will AI Take Your Job — or Make It Better?.” If most people had a choice, they would probably choose the latter. With any of these new technologies, it can be challenging to distinguish the hype from the headline. On one end, you have big tech companies and startups promising to fix problems ranging from detecting cancer to creating COVID vaccines. On the other, there are people’s understandable fear of becoming obsolete in the workplace.That’s why I read beyond the headlines and use the technology myself when possible. Recently some of these ML technologies have got much more accessible. Apps like Runway give you access to the remote GPU power needed for these tasks along with an ever-growing library of Github repositories while having an interface much closer to something like Photoshop.While taking a class called StyleGAN2 In-Depth, I was excited by a method called Projection that is within the StyleGAN2 library. Something about it got me thinking that ML could be used as a tool for concept development. I’m not claiming that something like this is going to replace human creativity, just to add to the many other existing methods used to help with new ideas.Projection was initially conceived as a method to see if a specific model was used to make a Deepfake image. A possible Deepfake image is selected and the ML model is searched for the closest image it contains in a manner similar to the way google reverse image search can search the web for a specific photo. I’m interested in using it differently. In my mind a creative could train a model with either images of their previous designs or designs by their major influences. My first thought was a creative could use Projection on the resulting model with any design or random image and see if any of the resulting images spark new ideas. The resulting images wouldn’t be the same as the reference image unless included in the original dataset. Adjusting features like truncation settings would cause the results to vary from similar to very wild.Some of the chair images used to train my model and resulting sequence.I chose to make my own chair model and use it as an example for two main reasons: in the design world, there is an ongoing debate if there are already too many chairs; and in the ML world, there is a long history of chairs being used for datasets.While writing this, I discovered Philipp Schmitt and Steffen Weiß’s The Chair Project. Their project is similar enough to what I was suggesting that I changed this post’s focus to look at some of the recent ML advances since their project was published. While it has been just two years since they wrote their paper, you can see that the realism of the generated images has come along way, as have some of the tools for interacting with a model.Also, even though this focuses on chair design, I believe methods like this could be used for a wide range of creative fields. For example, graphic designers could explore patterns, type, textures and motion. Possible for any creative with a good dataset of images.Specific to our chair example, there are products currently available that were made with generative design methods. A good example is Philippe Starck’s A.I. chair for Kartell. Using ML to generate the optimum amount and form for any specific material has obvious advantages for the environment and production efficiencies, but the downside is a form that is strangely both organic and alien at the same time.Much of the GAN related artwork you see out there involves some latent space animation. It’s an excellent way to gauge a models’ success but doesn’t allow human interaction.Here are a couple of those the current methods.An example of Projection in action. The image could an existing model or sketch, and generated on the right is the closest design the model can find.GANSpace allows for the use of a GUI with your model. You are given up to 512 options called Components. These options are how the computer understands various differences, so many don’t make sense to us. You can label them and use them in an interface as above.Audio-reactive allows a user to explore your model through sound. These can be random or chosen vectors in your model. So why not yell or sing to your model?Mix Seeds is a method for combining two seeds together. I’ve seen examples of it working better for other people, but it mostly just transferred the colour for me.What I hoped Mix Seeds would do it looks like StarGAN v2 would do better. I wasn’t able to use my model with it, but if the sample animation is representative of its capabilities, I could see it being very useful.One thing is clear, a human is still needed to consider materials, manufacturing, and obviously, ergonomics as my video above demonstrates:)Some ML Links to get you started:Runway & youtube channelDerrick Schultz’s youtube channel — One of my instructorsOur Class’s StyleGAN2 Show and TellSome other ML links worth checking out:Deep Learning In 5 MinutesTechniques in Self-Attention Generative Adversarial NetworksTwo Minute PapersThis chair does not existThe Internet Furry Drama Raising Big Questions About Artificial IntelligenceWhen audio deepfakes put words in Jay-Z’s mouth, did he have a legal case?Chair related:A New Designer Manifesto: Stop Designing Chairs!Chair Times A History of Seating — From 1800 to TodayReport Confirms No Need To Make New Chairs For The Time Being — The Onion",23/06/2020,0,8.0,0.0,733.0,430.0,3.0,0.0,0.0,28.0,en
3925,"기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 5",Medium,Jongdae Lim,874.0,24.0,2430.0,"딥러닝(Deep Learning)과 시퀀스(Sequence)의 마법을 사용한 언어 번역(Language Translation)우리는 모두 마법처럼 100 가지 다른 언어를 즉시 번역 할 수 있는 웹 사이트 인 구글 번역(Google Translate)을 알고 있고 사랑합니다. 심지어 휴대 전화나 스마트 워치에서도 사용할 수 있습니다:구글 번역에 사용된 기술을 기계 번역(Machine Translation)이라고 합니다. 다른 방법으로는 절대 불가능했던 전세계 사람들의 의사 소통을 가능하게 함으로써 세상을 변화시켰습니다.그런데, 사실 고등학생들이… 음… 지난 15 년간 스페인어 숙제를 하기위해 구글 번역의 도움을 받아 왔다는 것을 모두 알고 있습니다. 그렇다면 이건 오래된 뉴스가 아닌가요?지난 2 년 동안, 딥러닝(deep learning)은 기계 번역에 대한 우리의 접근 방식을 완전히 새롭게 바꿔 놓았습니다. 언어 번역에 대해 거의 아는 것이 없는 딥러닝 연구자들이 세계에서 가장 뛰어는 언저 전문가가 제작한 언어 번역 시스템을 능가하는 상대적으로 간단한 기계 학습 솔루션을 앞다퉈 내놓고 있습니다.이 획기적인 기술을 시퀀스-투-시퀀스 학습(sequence-to-sequence learning) 이라고 부릅니다. 이는 많은 종류의 문제들을 해결하는 데 사용되는 매우 강력한 기술입니다. 이제 이것이 번역에 어떻게 사용되는지 확인한 후에, AI 챗봇(Chat Bots)을 작성하거나 그림을 묘사하는 데에도 정확히 동일한 알고리즘이 어떻게 사용되는지 배울 예정입니다.*역자주: 시퀀스-투-시퀀스(sequence-to-sequence)에서 시퀀스란 연관된 연속의 데이터를 의미합니다. 이 글에서 주로 이야기하고 있는 문장(sentences)은 결국 문법 등의 규칙으로 연관된 일련의(sequential) 단어들 집합으로 볼 수 있고, 좋은 시퀀스의 예입니다. 따라서, 시퀀스-투-시퀀스 문제란 사진을 제공하면 어떤 사물인지 분류하는 것(clustering)과는 다르게 소스 시퀀스를 결과 시퀀스로 바꾸는 문제를 말합니다. 예를 들어, 사진을 신경망에 제공하면 어떤 사진인지(슬픈지 또는 기쁜지) 일련의 문장으로 묘사하는 것이 가능해진 것입니다. 일반적으로, 앞선 설명과 같이 시퀀스-투-시퀀스 모델은 입력 데이터 처리를 위한 인코더(encoder)와 출력 데이터 처리를 위한 디코더(decoder)로 구성됩니다.참조, https://arxiv.org/abs/1406.1078, https://arxiv.org/abs/1409.3215자 시작하시죠!그렇다면, 컴퓨터가 인간 언어를 번역할 수 있도록 하려면 어떻게 프로그래밍해야 하나요?가장 심플한 접근 방법은 문장의 모든 단어를 대상 언어의 번역 된 단어로 바꾸는 것입니다. 다음은 단어 단위로 스페인어에서 영어로 번역하는 간단한 예제입니다.필요한 것은 각 단어의 번역을 찾기 위한 사전이므로 이는 구현하기 쉽습니다. 그러나 문법과 문맥을 무시하기 때문에 결과는 좋지 않습니다.이제 다음에 해야할 일은 결과를 개선하기 위해 해당 언어에 맞는 규칙들을 추가하는 것입니다. 예를 들어, 일반적인 2 단어 구문을 하나의 그룹으로 번역하는 것입니다.그리고 영어와는 다르게 스페인어에서는 명사와 형용사의 순서가 반대이기 때문에 명사와 형용사의 순서를 뒤집어 번역하는 것입니다.효과가 있네요! 문법의 모든 부분을 적용할 때까지 규칙을 계속 추가하면, 우리 프로그램은 모든 문장을 번역 할 수 있게 될 것입니다, 그렇겠죠?이것이 바로 가장 초기의 기계 번역 시스템이 동작한 방법입니다. 언어 학자들은 복잡한 규칙을 찾아서 하나씩 차례로 프로그래밍했습니다. 세계에서 가장 현명한 언어 학자 중 일부는 냉전 기간 동안 러시아어 통신을 보다 쉽게 해석할 수 있는 번역 시스템을 만들기 위해 수년간 노력했습니다.유감스럽게도, 이것은 일기 예보와 같이 간단하고 평이하게 구조화 된 문서에서만 동작했습니다. 실생활 문서에서는 안정적으로 동작하지 않았습니다.여기서의 문제는 인간의 언어가 고정된 규칙들을 따르지 않는다는 것입니다. 인간의 언어는 특수한 상황, 지역적 다양성들로 가득하며 규칙 위반이 아무렇지 않게 일어납니다. 우리가 영어로 이야기하는 방식은 앉아서 문법 규칙을 정의하는 사람들보다 수백년 전에 누군가를 침략했던 사람에 따라 더 많은 영향을 받았습니다.규칙 기반 시스템이 실패한 후에, 문법 규칙을 대신해서 확률 및 통계 기반의 모델을 사용하는 새로운 번역 접근 방법이 개발되었습니다.통계 기반의 번역 시스템을 만들기 위해서는 동일한 텍스트가 최소한 두 가지 언어로 번역되어 있는 많은 훈련 데이터가 필요합니다. 이러한 이중 번역문(double-translated text)를 병렬 말뭉치(parallel corpora 또는 병렬 코퍼스)라고 부릅니다. 1800년대 과학자들이 로제타 스톤 (Rosetta Stone)을 그리스어로부터 이집트 상형 문자를 이해하는 데 사용했던 동일한 방식으로, 컴퓨터는 병렬 말뭉치를 사용해 텍스트를 한 언어에서 다른 언어로 어떻게 변환해야 하는지를 추측할 수 있습니다.다행하게도 이중 번역문(double-translated text)들은 희한한 곳에 이미 많이 있습니다. 예를 들어, 유럽 의회는 그들의 의사 진행 절차를 21개의 언어로 번역해 놨습니다. 그래서 연구자들은 종종 번역 시스템을 구축하는 데 이 데이터를 사용합니다.통계기반 번역 시스템과의 근본적인 차이점은 하나의 정확한 번역을 생성하려고하지 않는다는 것입니다. 그 대신에, 수천 개의 가능한 번역을 생성 한 다음 각각의 번역이 정확한가에 대해 순위를 매깁니다. 이 시스템은 훈련 데이터와 얼마나 유사한지에 따라 “정확도”를 평가합니다. 그럼, 어떻게 동작하는지 알아보겠습니다:먼저, 문장을 쉽게 번역될 수 있는 단순한 조각들로 분해합니다.그런 다음, 훈련 데이터에 있는 동일한 단어 조각에 대해 사람이 이미 번역한 모든 방법을 찾아 이러한 조각들을 번역합니다.그런데, 여기서 중요한 점은 단순히 번역 사전에서 이 조각들을 찾는게 아니라는 것입니다. 그 대신에, 우리는 실제 사람들이 실생활에서 이 동일한 단어 조각들을 어떻게 번역하는지를 확인합니다. 이를 통해 다양한 맥략으로 사용될 수 있는 모든 다른 번역 방식을 확인할 수 있습니다.그런데, 가능한 번역 중 일부는 다른 것들보다 자주 사용될 수 있습니다. 이렇게 훈련 데이터에서 각각의 번역이 얼마나 자주 나타나는지에 따라 점수를 부여 할 수 있습니다.예를 들어, “Quiero”는 “I try”라는 의미 보다는 “I want”를 의미하는 경우가 훨씬 더 일반적입니다. 따라서 우리는 Quiero가 우리 훈련 데이터에서 덜 빈번하게 사용되는 번역 보다 “I want”로 번역되는 빈도만큼 여기에 가중치를 줄 수 있습니다.다음으로, 이 조각들의 가능한 모든 조합을 통해 가능한 모든 번역문을 생성합니다.Step 2에서 나열했던 조각들에 대한 번역으로부터 우리는 이 번역 조각들을 여러 가지 방법으로 조합해서 거의 2,500 가지의 다양한 문장을 생성 할 수 있습니다. 여기 몇 가지 예가 있습니다:I love | to leave | at | the seaside | more tidy.I mean | to be on | to | the open space | most lovely.I like | to be |on | per the seaside | more lovely.I mean | to go | to | the open space | most tidy.그런데, 실제 시스템에는 더 많은 조각들의 조합이있을 것입니다. 왜냐하면, 단어의 또다른 순서와 문장을 조각내는 다양한 방법을 시도해야 하기 때문입니다:I try | to run | at | the prettiest | open space.I want | to run | per | the more tidy | open space.I mean | to forget | at | the tidiest | beach.I try | to go | per | the more tidy | seaside.이제 이렇게 생성된 모든 문장을 확인하고 “가장 인간”다운 문장 하나를 찾아야 합니다.이렇게 하기 위해서, 우리는 생성 된 각 문장을 영어로 쓰여진 책과 뉴스 기사의 수백만 개 실제 문장과 비교합니다. 비교할 영어 텍스트가 많을수록 좋습니다.다음과 같은가능한 번역을 살펴 보겠습니다:I try | to leave | per | the most lovely | open space.아무도 이런식의 영어 문장은 쓰지 않았을 것처럼 보입니다. 따라서, 우리의 훈련 데이터 세트에 있는 어떤 문장과도 유사하지 않습니다. 우리는 이 가능한 번역에 낮은 확률 점수를 줄 것입니다.그러면, 다음과 같은 다른 가능한 번역을 보겠습니다.I want | to go | to | the prettiest | beach.이 문장은 분명 우리의 훈련 데이터 세트에 있는 어떤 것과 유사 할 것입니다. 따라서, 높은 확률 점수를 얻을 것입니다.모든 가능한 번역문을 확인해서, 실제 영어 문장과 가장 유사한 문장이면서 가장 가능성이 큰 조각 번역을 포함한 번역을 선택할 것입니다.우리의 최종 번역은 “I want to go to the prettiest beach.”입니다. 나쁘지 않네요!통계기반 기계 번역 시스템은 충분한 훈련 데이터를 제공하면 규칙 기반 시스템보다 훨씬 뛰어납니다. Franz Josef Och는 이러한 아이디어를 개선해서 2000 년대 초 구글 번역(Google Translate)을 구축했습니다. 마침내 전 세계에 기계 번역(Machine Translation) 서비스가 제공되었습니다.초기에는, 언어학자들이 설계한 규칙 기반 시스템보다 확률에 기반한 이 “바보 같은” 접근 방식이 더 효과적인 것에 대해 모든 사람들이 놀랐습니다. 이 때문에, 80 년대 연구원들 사이에서는 (다소 심한) 이런말이 있었습니다.“언어학자를 한명 해고할 때 마다, 정확도가 높아진다”— Frederick Jelinek통계기반 기계 번역 시스템은 잘 동작하지만 구축하고 유지하기가 어렵습니다. 번역하고자 하는 새로운 언어 한쌍마다 새로운 여러 단계의 번역 경로(pipeline)을 전문가가 수정하고 조정해야합니다.이렇게 언어별로 다른 경로(pipeline)을 구축하는 것은 힘들기 때문에 절충안이 필요합니다. Google에서 그루지야어(Georgian)를 텔루구어(Telegu)로 번역 요청을 하면, 그루지야어를 텔루구어로(Georgain-to-Telegu) 번역하는 것은 충분한 투자를 할정도로 빈번하지 않기 때문에, 내부적으로는 중간단계인 영어로 번역을 합니다. 그렇다 보니, 보다 일반적인 경우인 불어를 영어로(French-to-English) 번역하는 것을 요청했을 때보 다 덜 향상된 번역 경로을 사용해 번역을 수행하게 될 것입니다.컴퓨터가 우리를 위해 이 모든 귀찮은 개발 작업을 하게 한다면 있다면 멋지지 않을까요?기계 번역의 성배는 훈련 데이터만 주면 알아서 번역하는 방법을 배우는 블랙박스 시스템(black box system)입니다. 통계기반 기계 번역을 사용하더라도 인간이 직접 여러 단계의 통계 모델을 만들고 조정해야 합니다.*역자주: 여기서 블랙박스 시스템(black box system)이란 작동원리는 감춰진 또는 알아서 동작하는 시스템을 의미2014 년, 조경현(KyungHyun Cho) 팀이 돌파구를 만들었습니다. 그들은 딥러닝(deep learning)을 적용해 이 블랙박스 시스템을 구축하는 방법을 찾아냈습니다. 그들의 딥러닝 모델은 병렬 말뭉치(parallel corpora 또는 병렬 코퍼스)를 사용해 사람의 아무런 개입이 없이도 두 언어 사이의 번역 방법을 학습합니다.이는 순환 신경망(recurrent neural networks) 인코딩(encodings)이라는 두 가지의 큰 아이디어를 통해 실현되었습니다. 이 두 가지 아이디어를 영리하게 결합하면, 자가 학습 번역 시스템(self-learning translation system)을 만들 수 있습니다.우리는 Part 2에서 순환 신경망에 대해 이미 다루었습니다만, 빠르게 다시 확인해 보겠습니다.일반적인 (순환하지 않는) 신경망은 숫자 목록을 받아 (이전 훈련을 기반으로) 결과를 계산하는 일반 기계 학습 알고리즘입니다. 신경망은 많은 문제를 해결하는 블랙 박스로 사용될 수 있습니다. 예를 들어 신경망을 사용해서 어떤 집의 속성을 기반으로 그 집의 대략적인 가치를 계산할 수 있습니다.그러나 대부분의 기계 학습 알고리즘과 마찬가지로 신경망도 상태를 저장하지 않습니다(stateless). 숫자 목록을 전달하면 신경망은 그 결과를 계산합니다. 동일한 숫자를 다시 전달하면 항상 동일한 결과과 값을 계산합니다. 과거의 계산값들에 대한 저장소(memory)가 없습니다. 즉, 2 + 2는 항상 4입니다.순환 신경망 (또는 줄여서 RNN)은 신경망의 이전 상태가 다음 계산의 입력 값 중 하나가 되는 신경망이 약간 수정 된 버전입니다. 이는 이전 계산이 미래의 계산 결과에 영향을 미친다는 것을 의미합니다.그런데 왜 이 일을 해야 하나요? 우리가 마지막으로 계산한 결과와 상관 없이 2 + 2는 항상 4여야 하지 않나요?이 묘책을 통해 신경망은 일련의 데이터에서 패턴을 학습 할 수 있습니다. 예를 들어, 단어의 처음 몇 글자만 보고도 다음으로 나올 가장 가능성있는 글자를 예측할 수 있습니다.RNN은 데이터에서 패턴을 학습하고자 할 때 유용합니다. 인간 언어는 단지 하나의 크고 복잡한 패턴 이기 때문에, 자연어 처리의 많은 영역에서 RNN이 점점 더 많이 사용되고 있습니다.RNN에 대해 더 자세히 알고 싶다면, 이를 사용해서 가짜 Ernest Hemingway 책을 만들고 또 가짜 Super Mario Brothers 레벨을 만들어 봤던 Part 2를 읽어보십시오.추가로 검토해야할 개념는 인코딩(Encodings)입니다. Part 4에서는 얼굴 인식 과정의 한 부분으로 인코딩에 대해 설명했습니다. 인코딩을 설명하기 위해, 약간 우회해서 서로 다른 두 사람을 컴퓨터가 어떻게 구별 할 수 있는지 알아보겠습니다.컴퓨터로 두 얼굴을 구별하기위해서는 각각의 얼굴에서 서로 다른 측정 값을 수집해 얼굴을 비교합니다. 예를 들어, 각 귀의 크기 또는 눈 사이의 간격을 측정하고 두 사진에서 이러한 측정 값을 비교하면 동일한 사람인지 확인할 수 있습니다.당신은 아마도 이미 황금시간대 방송되는 CSI 같은 수사물을 통해 이러한 아이디어에 익숙할 것입니다:얼굴을 측정값 목록으로 바꾸는 아이디어가 바로 인코딩(encoding)의 예입니다. 우리는 원본 데이터(얼굴 사진)를 대표하는 측정값 목록(인코딩)으로 변환합니다.그런데, Part 4에서 봤듯이 우리가 직접 얼굴을 측정하기 위한 특별한 측정 목록을 신경쓸 필요는 없습니다. 대신에, 신경망이 얼굴 사진으로부터 측정값을 생성하게 하면 됩니다. 컴퓨터는 어떤 측정값이 비슷한 두 사람을 구분하기에 딱 맞는지를 찾아내는데 있어 우리 보다 훨씬 뛰어납니다:이것이 바로 인코딩입니다. 인코딩은 우리가 매우 복잡한 무엇(얼굴 사진)인가를 단순한 것(128 개의 숫자)로 표현할 수있게 해줍니다. 이제 두 개의 다른 얼굴을 비교하는 것이 전체 이미지를 비교하는 대신에 각각의 얼굴에 대한 128 개의 숫자들만 비교하면 되기 때문에 훨씬 쉬어졌습니다.그런데 말입니다. 문장으로도 똑같은 작업을 할 수 있습니다! 가능한 모든 문장들을 일련의 고유한 숫자로 나타내는 인코딩을 만들수 있습니다:이러한 인코딩을 생성하기 위해서는 RNN에 문장을 제공하되 한 번에 한 단어 씩 제공합니다. 마지막 단어가 처리되면 최종 결과는 전체 문장을 나타내는 값들이 됩니다:좋습니다. 이제 문장 전체를 고유 한 숫자들로 표현할 수 있는 방법이 생겼습니다. 인코딩의 각 숫자가 무엇을 의미하는지 알지 못하지만 실제로 중요하지도 않습니다. 각 문장이 고유 한 숫자 집합으로 식별될 수만 있다면, 그 숫자가 어떻게 생성되었는지 정확하게 알 필요는 없습니다.자 이제 우리는 RNN을 사용해서 문장을 고유 한 숫자들로 인코딩하는 방법을 알았습니다. 그래서 이것이 어떤 도움이 될까요? 정말 멋진 것들이 여기 있습니다!두 개의 RNN을 서로 연결하면 어떤일이 일어날까요? 첫 번째 RNN은 문장을 나타내는 인코딩을 생성하게 할 수 있습니다. 그런 다음 두 번째 RNN이 해당 인코딩으로부터 원래 문장으로 다시 디코딩하기 위해 동일한 로직을 역으로 수행하게 할 수 있습니다:물론 원래 문장을 인코딩하고 다시 디코딩하는 것이 그리 쓸모 있는 것은 아닙니다. 그러나 두 번째 RNN을 영어 대신 스페인어로 디코딩하도록 훈련시킬 수 있다면 어떨까요? 병렬 말뭉치(parallel corpora )를 사용해 이를 수행하도록 훈련시킬수 있습니다:이렇게 해서, 영어 단어로 된 시퀀스를 이에 해당하는 스페인어 단어의 시퀀스로 변환해주는 일반적인 방법이 만들어졌습니다!이것은 정말 멋진 아이디어 입니다:실제 데이터로 이러한 작업을 수행하는 데 필요한 몇 가지 사항만을 강조했다는 것을 아셔야 합니다. 예를 들어, 다양한 길이의 입력 및 출력 문장을 처리하기 위한 추가 작업이 있습니다 ( bucketing and padding 참조). 드물게 쓰이는 단어를 올바르게 번역하는 데에도 문제가 있을 수 있습니다.나만의 언어 번역 시스템을 만들고 싶다면, TensorFlow에 포함된 영어와 불어간 번역 을 위한 실제 데모를 참고하십시오. 그렇다고 해서, 이 데모가 대충 테스트해보고 싶거나 예산이 부족한 사람들을 위한 것은 아닙니다. 이 기술은 여전히 새롭고 많은 리소스를 필요로 합니다. 고성능 비디오 카드가 있는 고속 컴퓨터를 사용하더라도 나만의 언어 번역 시스템을 훈련하는데 대략 1개월 정도의 연속적인 처리 시간이 소요될 수 있습니다.또한 시퀀스-투-시퀀스 언어 번역 기술이 급속도로 향상되고 있어 이를 따라잡기는 매우 어렵습니다. 최근의 많은 개선(예를 들어, attention mechanism 또는 tracking context)은 결과를 크게 향상시켜 주지만, 이러한 것들은 너무 새롭기 때문에 아직 위키피디아 페이지에 조차 없습니다. 시퀀스-투-시퀀스 학습으로 무엇인가 대단한 일을 하고 싶다면, 새로운 개발 방법이 나올때 마다 이를 공부해야만 합니다.그렇다면, 시퀀스-투-시퀀스 모델로 또 어떤 것을 할 수 있나요?약 1 년 전, Google의 연구원들은 시퀀스-투-시퀀스 모델을 사용해 AI 챗봇(chat bot) 을 만들 수 있음을 보여주었습니다. 아이디어는 너무 간단해서 동작하는 것이 놀라울 정도입니다.먼저 Google 직원과 Google 기술 지원 팀 간의 채팅 로그를 수집했습니다. 그런 다음 직원의 질문이 입력 문장이고 기술 지원 팀의 응답이 해당 문장의 “번역”으로 해서 시퀀스-투-시퀀스 모델을 훈련시켰습니다.사용자가 챗봇과 대화할 때, 각각의 사용자 메시지를 이 시스템으로 “번역”하고 챗봇의 응답을 얻습니다.결과적으로 실제 기술 지원 질문에 (때때로) 대답 할 수 있는 준-인텔리전트 챗봇(a semi-intelligent bot)이 만들어졌습니다. 다음은 이 논문에서 따온 사용자와 챗봇 간의 샘플 대화의 일부입니다.Human: connection refused or something like thatMachine: may i know the version of network connect you connect?Human: i am not sure i know thatMachine: is the network connect prompting for any user certificate?Human: yesMachine: are you at home or at work at the moment? office?Human: i am at homeMachine: try this. goto <URL redacted> page and enter your name and passwordHuman: doneMachine: try logging in with <NAME redacted> and check if its working fine nowHuman: yes, now it works!Machine: great. anything else that i can help?이들은 또한 수백만 개의 영화 자막을 기반으로 한 챗봇을 만들어 봤습니다. 이 아이디어는 영화 등장인물들 간의 대화를 사용해 챗봇을 훈련시키고 인간처럼 이야기하게 하는 것이 었습니다. 입력 문장은 한 등장 인물이 말한 대화의 한줄이고 “번역”은 다음 등장 인물이 대답한 한 것입니다:정말로 흥미로운 결과가 나왔습니다. 챗봇은 인간처럼 대화를 나눌 수 있었을 뿐만 아니라 아주 적긴 하지만 지능을 보여줬습니다.Human: my name is david. what is my name?Machine: david.Human: my name is john. what is my name?Machine: john.Human: are you a leader or a follower?Machine: i’m a leader.Human: are you a follower or a leader?Machine: i’m a leader.이것은 단지 가능성의 시작일뿐입니다. 한 문장을 다른 문장으로 변환하는 것에만 국한되지 않습니다. 이미지를 문장으로 바꾸는 이미지-투-시퀀스(image-to-sequence)모델을 만드는 것도 가능합니다!Google의 또다른 팀은 첫 번째 RNN을 (Part 3에서 배웠던) 컨볼루션 신경망(Convolutional Neural Network)로 바꿔서 이를 구현했습니다. 이렇게하면 입력을 문장 대신 사진으로 사용할 수 있습니다. 나머지는 기본적으로 같은 방식으로 동작합니다.그러면 당연하게도, 우리는 그림을 글로 바꿀 수 있습니다(물론 아주 많은 훈련 데이터가 필요합니다)!Andrej Karpathy는 이 아이디어를 확장해서 이미지의 여러 영역을 개별적으로 처리함으로써 이미지를 매우 자세하게 설명 할 수있는 시스템을 구축했습니다.이를 이용하면, 구체적인 검색어와 신기할 정도로 일치하는 이미지를 찾을 수있는 이미지 검색 엔진을 만들 수 있습니다:심지어 이와는 반대로 설명하는 문장 만으로 전체 그림을 생성하는 문제를 해결하려는 연구원들도 있습니다!이러한 예제들로부터 당신은 어떤 가능성도 상상할 수 있습니다. 지금까지 음성 인식(speech recognition)에서 영상 처리(computer vision)에 이르기까지 모든 분야에서 시퀀스-투-시퀀스 애플리케이션이 연구되어 왔습니다. 분명히 내년에는 훨씬 더 많은 것이 만들어 질 것입니다.시퀀스-투-시퀀스 모델과 번역에 대해 좀더 깊게 알고 싶다면, 추천하는 다음 자료들을 참고하십시오:이 글이 마음에 들었다면, 제 Machine Learning is Fun! 이메일 리스트에 가입하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.Twitter의 @ageitgey인 저를 팔로우하거나, 직접 이메일을 보내거나 또는 linkedin에서 저를 찾을 수도 있습니다. 기계 학습으로 제가 당신이나 당신의 팀을 도울 수 있다면 언제든 연락 주십시오.*역자주: 번역글과 관련해 의견 주시려면, 저에게 직접 이메일을 보내시거나 LinkedIn에서 저를 찾으셔도 됩니다.",24/02/2017,0,40.0,69.0,805.0,324.0,21.0,4.0,0.0,73.0,en
3926,Understanding PyTorch with an example: a step-by-step tutorial,Towards Data Science,Daniel Godoy,2200.0,21.0,4294.0,"Update (May 18th, 2021): Today I’ve finished my book: Deep Learning with PyTorch Step-by-Step: A Beginner’s Guide.Update (February 23rd, 2022): The paperback edition is available now (in three volumes). For more details, please check pytorchstepbystep.com.PyTorch is the fastest growing Deep Learning framework and it is also used by Fast.ai in its MOOC, Deep Learning for Coders and its library.PyTorch is also very pythonic, meaning, it feels more natural to use it if you already are a Python developer.Besides, using PyTorch may even improve your health, according to Andrej Karpathy :-)There are many many PyTorch tutorials around and its documentation is quite complete and extensive. So, why should you keep reading this step-by-step tutorial?Well, even though one can find information on pretty much anything PyTorch can do, I missed having a structured, incremental and from first principles approach to it.In this post, I will guide you through the main reasons why PyTorch makes it much easier and more intuitive to build a Deep Learning model in Python — autograd, dynamic computation graph, model classes and more — and I will also show you how to avoid some common pitfalls and errors along the way.Moreover, since this is quite a long post, I built a Table of Contents to make navigation easier, should you use it as a mini-course and work your way through the content one topic at a time.Most tutorials start with some nice and pretty image classification problem to illustrate how to use PyTorch. It may seem cool, but I believe it distracts you from the main goal: how PyTorch works?For this reason, in this tutorial, I will stick with a simple and familiar problem: a linear regression with a single feature x! It doesn’t get much simpler than that…Let’s start generating some synthetic data: we start with a vector of 100 points for our feature x and create our labels using a = 1, b = 2 and some Gaussian noise.Next, let’s split our synthetic data into train and validation sets, shuffling the array of indices and using the first 80 shuffled points for training.We know that a = 1 and b = 2, but now let’s see how close we can get to the true values by using gradient descent and the 80 points in the training set…If you are comfortable with the inner workings of gradient descent, feel free to skip this section. It goes beyond the scope of this post to fully explain how gradient descent works, but I’ll cover the four basic steps you’d need to go through to compute it.For a regression problem, the loss is given by the Mean Square Error (MSE), that is, the average of all squared differences between labels (y) and predictions (a + bx).It is worth mentioning that, if we use all points in the training set (N) to compute the loss, we are performing a batch gradient descent. If we were to use a single point at each time, it would be a stochastic gradient descent. Anything else (n) in-between 1 and N characterizes a mini-batch gradient descent.A gradient is a partial derivative — why partial? Because one computes it with respect to (w.r.t.) a single parameter. We have two parameters, a and b, so we must compute two partial derivatives.A derivative tells you how much a given quantity changes when you slightly vary some other quantity. In our case, how much does our MSE loss change when we vary each one of our two parameters?The right-most part of the equations below is what you usually see in implementations of gradient descent for a simple linear regression. In the intermediate step, I show you all elements that pop-up from the application of the chain rule, so you know how the final expression came to be.In the final step, we use the gradients to update the parameters. Since we are trying to minimize our losses, we reverse the sign of the gradient for the update.There is still another parameter to consider: the learning rate, denoted by the Greek letter eta (that looks like the letter n), which is the multiplicative factor that we need to apply to the gradient for the parameter update.How to choose a learning rate? That is a topic on its own and beyond the scope of this post as well.Now we use the updated parameters to go back to Step 1 and restart the process.An epoch is complete whenever every point has been already used for computing the loss. For batch gradient descent, this is trivial, as it uses all points for computing the loss — one epoch is the same as one update. For stochastic gradient descent, one epoch means N updates, while for mini-batch (of size n), one epoch has N/n updates.Repeating this process over and over, for many epochs, is, in a nutshell, training a model.It’s time to implement our linear regression model using gradient descent using Numpy only.Wait a minute… I thought this tutorial was about PyTorch!Yes, it is, but this serves two purposes: first, to introduce the structure of our task, which will remain largely the same and, second, to show you the main pain points so you can fully appreciate how much PyTorch makes your life easier :-)For training a model, there are two initialization steps:Make sure to always initialize your random seed to ensure reproducibility of your results. As usual, the random seed is 42, the least random of all random seeds one could possibly choose :-)For each epoch, there are four training steps:Just keep in mind that, if you don’t use batch gradient descent (our example does),you’ll have to write an inner loop to perform the four training steps for either each individual point (stochastic) or n points (mini-batch). We’ll see a mini-batch example later down the line.Just to make sure we haven’t done any mistakes in our code, we can use Scikit-Learn’s Linear Regression to fit the model and compare the coefficients.They match up to 6 decimal places — we have a fully working implementation of linear regression using Numpy.Time to TORCH it :-)First, we need to cover a few basic concepts that may throw you off-balance if you don’t grasp them well enough before going full-force on modeling.In Deep Learning, we see tensors everywhere. Well, Google’s framework is called TensorFlow for a reason! What is a tensor, anyway?In Numpy, you may have an array that has three dimensions, right? That is, technically speaking, a tensor.A scalar (a single number) has zero dimensions, a vector has one dimension, a matrix has two dimensions and a tensor has three or more dimensions. That’s it!But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, everything is either a scalar or a tensor.”How do we go from Numpy’s arrays to PyTorch’s tensors”, you ask? That’s what from_numpy is good for. It returns a CPU tensor, though.“But I want to use my fancy GPU…”, you say. No worries, that’s what to() is good for. It sends your tensor to whatever device you specify, including your GPU (referred to as cuda or cuda:0).“What if I want my code to fallback to CPU if no GPU is available?”, you may be wondering… PyTorch got your back once more — you can use cuda.is_available() to find out if you have a GPU at your disposal and set your device accordingly.You can also easily cast it to a lower precision (32-bit float) using float().If you compare the types of both variables, you’ll get what you’d expect: numpy.ndarray for the first one and torch.Tensor for the second one.But where does your nice tensor “live”? In your CPU or your GPU? You can’t say… but if you use PyTorch’s type(), it will reveal its location — torch.cuda.FloatTensor — a GPU tensor in this case.We can also go the other way around, turning tensors back into Numpy arrays, using numpy(). It should be easy as x_train_tensor.numpy() but…Unfortunately, Numpy cannot handle GPU tensors… you need to make them CPU tensors first using cpu().What distinguishes a tensor used for data — like the ones we’ve just created — from a tensor used as a (trainable) parameter/weight?The latter tensors require the computation of its gradients, so we can update their values (the parameters’ values, that is). That’s what the requires_grad=True argument is good for. It tells PyTorch we want it to compute gradients for us.You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right? Not so fast…The first chunk of code creates two nice tensors for our parameters, gradients and all. But they are CPU tensors.In the second chunk of code, we tried the naive approach of sending them to our GPU. We succeeded in sending them to another device, but we ”lost” the gradients somehow…In the third chunk, we first send our tensors to the device and then use requires_grad_() method to set its requires_grad to True in place.In PyTorch, every method that ends with an underscore (_) makes changes in-place, meaning, they will modify the underlying variable.Although the last approach worked fine, it is much better to assign tensors to a device at the moment of their creation.Much easier, right?Now that we know how to create tensors that require gradients, let’s see how PyTorch handles them — that’s the role of the…Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.So, how do we tell PyTorch to do its thing and compute all gradients? That’s what backward() is good for.Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the backward() method from the corresponding Python variable, like, loss.backward().What about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.If you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.What does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.So, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.That’s it? Well, pretty much… but, there is always a catch, and this time it has to do with the update of the parameters…In the first attempt, if we use the same update structure as in our Numpy code, we’ll get the weird error below… but we can get a hint of what’s going on by looking at the tensor itself — once again we “lost” the gradient while reassigning the update results to our parameters. Thus, the grad attribute turns out to be None and it raises the error…We then change it slightly, using a familiar in-place Python assignment in our second attempt. And, once again, PyTorch complains about it and raises an error.Why?! It turns out to be a case of “too much of a good thing”. The culprit is PyTorch’s ability to build a dynamic computation graph from every Python operation that involves any gradient-computing tensor or its dependencies.We’ll go deeper into the inner workings of the dynamic computation graph in the next section.So, how do we tell PyTorch to “back off” and let us update our parameters without messing up with its fancy dynamic computation graph? That’s what torch.no_grad() is good for. It allows us to perform regular Python operations on tensors, independent of PyTorch’s computation graph.Finally, we managed to successfully run our model and get the resulting parameters. Surely enough, they match the ones we got in our Numpy-only implementation.“Unfortunately, no one can be told what the dynamic computation graph is. You have to see it for yourself.” MorpheusHow great was “The Matrix”? Right, right? But, jokes aside, I want you to see the graph for yourself too!The PyTorchViz package and its make_dot(variable) method allows us to easily visualize a graph associated with a given Python variable.So, let’s stick with the bare minimum: two (gradient computing) tensors for our parameters, predictions, errors and loss.If we call make_dot(yhat) we’ll get the left-most graph on Figure 3 below:Let’s take a closer look at its components:If we plot graphs for the error (center) and loss (right) variables, the only difference between them and the first one is the number of intermediate steps (gray boxes).Now, take a closer look at the green box of the left-most graph: there are two arrows pointing to it, since it is adding up two variables, a and b*x. Seems obvious, right?Then, look at the gray box of the same graph: it is performing a multiplication, namely, b*x. But there is only one arrow pointing to it! The arrow comes from the blue box that corresponds to our parameter b.Why don’t we have a box for our data x? The answer is: we do not compute gradients for it! So, even though there are more tensors involved in the operations performed by the computation graph, it only shows gradient-computing tensors and its dependencies.What would happen to the computation graph if we set requires_grad to False for our parameter a?Unsurprisingly, the blue box corresponding to the parameter a is no more! Simple enough: no gradients, no graph.The best thing about the dynamic computing graph is the fact that you can make it as complex as you want it. You can even use control flow statements (e.g., if statements) to control the flow of the gradients (obviously!) :-)Figure 5 below shows an example of this. And yes, I do know that the computation itself is completely nonsense…So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers, like SGD or Adam.An optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method and that’s it!In the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters a and b.Don’t be fooled by the optimizer’s name: if we use all training data at once for the update — as we are actually doing in the code — the optimizer is performing a batch gradient descent, despite of its name.Let’s check our two parameters, before and after, just to make sure everything is still working fine:Cool! We’ve optimized the optimization process :-) What’s left?We now tackle the loss computation. As expected, PyTorch got us covered once again. There are many loss functions to choose from, depending on the task at hand. Since ours is a regression, we are using the Mean Square Error (MSE) loss.Notice that nn.MSELoss actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).We then use the created loss function later, at line 20, to compute the loss given our predictions and our labels.Our code looks like this now:At this point, there’s only one piece of code left to change: the predictions. It is then time to introduce PyTorch’s way of implementing a…In PyTorch, a model is represented by a regular Python class that inherits from the Module class.The most fundamental methods it needs to implement are:You are not limited to defining parameters, though… models can contain other models (or layers) as its attributes as well, so you can easily nest them. We’ll see an example of this shortly as well.You should NOT call the forward(x) method, though. You should call the whole model itself, as in model(x) to perform a forward pass and output predictions.Let’s build a proper (yet simple) model for our regression task. It should look like this:In the __init__ method, we define our two parameters, a and b, using the Parameter() class, to tell PyTorch these tensors should be considered parameters of the model they are an attribute of.Why should we care about that? By doing so, we can use our model’s parameters() method to retrieve an iterator over all model’s parameters, even those parameters of nested models, that we can use to feed our optimizer (instead of building a list of parameters ourselves!).Moreover, we can get the current values for all parameters using our model’s state_dict() method.IMPORTANT: we need to send our model to the same device where the data is. If our data is made of GPU tensors, our model must “live” inside the GPU as well.We can use all these handy methods to change our code, which should be looking like this:Now, the printed statements will look like this — final values for parameters a and b are still the same, so everything is ok :-)I hope you noticed one particular statement in the code, to which I assigned a comment “What is this?!?” — model.train().In PyTorch, models have a train() method which, somewhat disappointingly, does NOT perform a training step. Its only purpose is to set the model to training mode. Why is this important? Some models may use mechanisms like Dropout, for instance, which have distinct behaviors in training and evaluation phases.In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s Linear model as an attribute of our own, thus creating a nested model.Even though this clearly is a contrived example, as we are pretty much wrapping the underlying model without adding anything useful (or, at all!) to it, it illustrates well the concept.In the __init__ method, we created an attribute that contains our nested Linear model.In the forward() method, we call the nested model itself to perform the forward pass (notice, we are not calling self.linear.forward(x)!).Now, if we call the parameters() method of this model, PyTorch will figure the parameters of its attributes in a recursive way. You can try it yourself using something like: [*LayerLinearRegression().parameters()] to get a list of all parameters. You can also add new Linear attributes and, even if you don’t use them at all in the forward pass, they will still be listed under parameters().Our model was simple enough… You may be thinking: “why even bother to build a class for it?!” Well, you have a point…For straightforward models, that use run-of-the-mill layers, where the output of a layer is sequentially fed as an input to the next, we can use a, er… Sequential model :-)In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:Simple enough, right?So far, we’ve defined an optimizer, a loss function and a model. Scroll up a bit and take a quick look at the code inside the loop. Would it change if we were using a different optimizer, or loss, or even model? If not, how can we make it more generic?Well, I guess we could say all these lines of code perform a training step, given those three elements (optimizer, loss and model),the features and the labels.So, how about writing a function that takes those three elements and returns another function that performs a training step, taking a set of features and labels as arguments and returning the corresponding loss?Then we can use this general-purpose function to build a train_step() function to be called inside our training loop. Now our code should look like this… see how tiny the training loop is now?Let’s give our training loop a rest and focus on our data for a while… so far, we’ve simply used our Numpy arrays turned PyTorch tensors. But we can do better, we can build a…In PyTorch, a dataset is represented by a regular Python class that inherits from the Dataset class. You can think of it as a kind of a Python list of tuples, each tuple corresponding to one point (features, label).The most fundamental methods it needs to implement are:There is no need to load the whole dataset in the constructor method (__init__). If your dataset is big (tens of thousands of image files, for instance), loading it at once would not be memory efficient. It is recommended to load them on demand (whenever __get_item__ is called).Let’s build a simple custom dataset that takes two tensors as arguments: one for the features, one for the labels. For any given index, our dataset class will return the corresponding slice of each of those tensors. It should look like this:Once again, you may be thinking “why go through all this trouble to wrap a couple of tensors in a class?”. And, once again, you do have a point… if a dataset is nothing else but a couple of tensors, we can use PyTorch’s TensorDataset class, which will do pretty much what we did in our custom dataset above.Did you notice we built our training tensors out of Numpy arrays but we did not send them to a device? So, they are CPU tensors now! Why?We don’t want our whole training data to be loaded into GPU tensors, as we have been doing in our example so far, because it takes up space in our precious graphics card’s RAM.OK, fine, but then again, why are we building a dataset anyway? We’re doing it because we want to use a…Until now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!So we use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.To retrieve a sample mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels.How does this change our training loop? Let’s check it out!Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.For bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s __get_item__ and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.Moreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.So far, we’ve focused on the training data only. We built a dataset and a data loader for it. We could do the same for the validation data, using the split we performed at the beginning of this post… or we could use random_split instead.PyTorch’s random_split() method is an easy and familiar way of performing a training-validation split. Just keep in mind that, in our example, we need to apply it to the whole dataset (not the training dataset we built in two sections ago).Then, for each subset of data, we build a corresponding DataLoader, so our code looks like this:Now we have a data loader for our validation set, so, it makes sense to use it for the…This is the last part of our journey — we need to change the training loop to include the evaluation of our model, that is, computing the validation loss. The first step is to include another inner loop to handle the mini-batches that come from the validation loader , sending them to the same device as our model. Next, we make predictions using our model (line 23) and compute the corresponding loss (line 24).That’s pretty much it, but there are two small, yet important, things to consider:Now, our training loop should look like this:Is there anything else we can improve or change? Sure, there is always something else to add to your model — using a learning rate scheduler, for instance. But this post is already waaaay too long, so I will stop right here.“Where is the full working code with all bells and whistles?”, you ask? You can find it here.Although this post was much longer than I anticipated when I started writing it, I wouldn’t make it any different — I believe it has most of the necessary steps one needs go to trough in order to learn, in a structured and incremental way, how to develop Deep Learning models using PyTorch.Hopefully, after finishing working through all code in this post, you’ll be able to better appreciate and more easily work your way through PyTorch’s official tutorials.Update (May 18th, 2021): Today I’ve finished my book: Deep Learning with PyTorch Step-by-Step: A Beginner’s Guide.Update (February 23rd, 2022): The paperback edition is available now (in three volumes). For more details, please check pytorchstepbystep.com.If you have any thoughts, comments or questions, please leave a comment below or contact me on Twitter.",07/05/2019,13,453.0,97.0,762.0,395.0,11.0,9.0,0.0,64.0,en
3927,RNN or Recurrent Neural Network for Noobs,HackerNoon.com,Debarko De 🦁,1300.0,12.0,2376.0,"What is a Recurrent Neural Network or RNN, how it works, where it can be used? This article tries to answer the above questions. It also shows a demo implementation of a RNN used for a specific purpose, but you would be able to generalise it for your needs.Knowhow. Python, CNN knowledge is required. CNN is required to compare why and where RNN performs better than CNN? No need to understand the math. If you want to check then go back to my earlier article to check what is a CNN.We will begin with the word use of the word “Recurrent”. Why is it called Recurrent? In english the word recurrent means:occurring often or repeatedlyIn the case of this type of Neural Network it’s called Recurrent since it does the same operation over and over on sets of sequential input. We will discuss about the meaning of the operation later in the article.You might be wondering by now, we have vanilla networks like Convolutional ones which perform very well. Why do we need another type of a network? There is a very specific use case where RNNs are required. In order to explain RNNs you need to first understand something called a sequence. Let's talk about sequences first.Sequence is a stream of data (finite or infinite) which are interdependent. Examples would be time series data, informative pieces of strings, conversations etc. In a conversation a sentence means something but the entire flow of the conversation mostly means something completely different. Also in a time series data like stock market data, a single tick data means the current price, but a full days data will show movement and allow us to take decision whether to buy or sell.CNNs generally don’t perform well when the input data is interdependent in a sequential pattern. CNNs don’t have any sort of correlation between previous input to the next input. So all the outputs are self dependent. CNN takes in an input and outputs based on the trained model. If you run 100 different inputs none of them would be biased by the previous output. But imagine a scenario like sentence generation or text translation. All the words generated are dependent on the words generated before (in certain cases, it’s dependent on words coming after as well, but we will discuss that later). So you need to have some bias based on your previous output. This is where RNNs shine. RNNs have in them a sense some memory about what happened earlier in the sequence of data. This helps the system to gain context. Theoretically RNNs have infinite memory, meaning they have the capability to look back indefinitely. By look back I mean all previous inputs. But practically they can only look back a last few steps. (we will discuss this later)Just to draw a correlation with humans in general, we also don’t take in place decisions. We also base our decisions on previous knowledge on the subject. (**over simplified, hard to say I understand even 0.1% of human brains**)RNNs can be used in a lot of different places. Following are a few examples where a lot of RNNs are used.Given a sequence of word, here we try to predict the likelihood of the next word. This is useful for translation since the most likely sentence would be the one that is correct.Translating text from one language to other uses one or the other form of RNN. All practical day systems use some advanced version of a RNN.Predicting phonetic segments based on input sound waves, thus formulating a word.A very big use case is to understand what is happening inside an image, thus we have a good description. This works in a combination of CNN and RNN. CNN does the segmentation and RNN then used the segmented data to recreate the description. It’s rudimentary but the possibilities are limitless.This can be used for video search where we do image description of a video frame by frame.We will be following the below mentioned sequence of topics to finish the document. Each section builds on top of another so don’t read this as a reference.Feed-forward networks channel information through a series of operations which take place in each node of the network. Feed-forward networks pass the information directly through each layer exactly once. This is different from other recurrent networks. We will talk about them in a later section. Generally feed-forward nets take an input and produce an output from it. This is also mostly a supervised learning step and the outcome most likely will be a classification. It behaves similarly to how a CNN behaves. Outputs can be expected to be classes like cats or dogs as labels.A feed-forward network is trained on a set of pre labelled data. The objective of the training phase is to reduce the error while the feed-forward network tries to guess the class. Once training is done, the weights are used to classify new batches of data.One important thing to note here. In a feed-forward network whatever image is shown to the classifier during test phase, it doesn’t alter the weights so the second decision is not affected. This is one very important difference between feed-forward networks and recurrent nets.Feed-forward nets don’t remember historic input data at test time unlike recurrent networks.It’s always point in time decision. They only remember things that were shown to them during the training phase.Recurrent networks, on the other hand, take as their input not just the current input example they see, but also what they have perceived previously in time.Let’s try to build a multi layer perceptron to start with the explanation. In simple terms there is a input layer, a hidden layer with certain activations and finally we receive an output.If we increase the number of layers in the above example, input layer takes the input. Then the first hidden layer does the activation passing onto the next hidden layers and so on. Finally it reaches the output layer which gives the output. Each hidden layer has its own weights and biases. Now the question is can we input to the hidden layers.Each layer has its own weight (W), biases (B), Activation Functions (F). These layers behave differently and technically would be challenging to merge together. To be able to merge them, lets replace all the layers with the same weights and biases. It will look something like this.Now we can merge all the layers together. All the hidden layers can be combined into a single recurrent layer. So they start looking somewhat like this:We will provide input to the hidden layer at each step. A recurrent neuron now stores all the previous step input and merges that information with the current step input. Thus it also captures some information regarding the correlation between current data step and the previous steps. The decision at a time step t-1 affects the decision taken at time t. This is very much like how we as humans take decisions in our life. We combine the present data with recent past to take a call on a particular problem at hand. This example is excessively rudimentary but in principle it aligns with our decision making capability. This really intrigues me as to whether we as humans are intelligent or we have a very advanced neural network model. Our decisions are just the training data that we have been collecting throughout our life. Thus can we digitise our brains once we have a fairly advanced model and systems capable of storing and computing them in reasonable time periods. So what happens when we have models better and faster than our brains training on data from millions of people?Funny anecdote from another article: a person is haunted by their deedsLet’s come back to the problem at hand and rephrase the above explanation with an example to predict what the next letter is after a sequence of letters. Imagine in the word namaskar. The word is of 8 letters.namaskar: a traditional Indian greeting or gesture of respect, made by bringing the palms together before the face or chest and bowing.If we were trying to figure out the 8th letter after 7 letters were fed to the network, what would have happened. The hidden layer would have gone through 8 iterations. If we were to unfold the network, it would be a 8 layer network, one layer for each letter. So you can imagine that a normal neural network is repeated multiple times. The number of times you unroll has a direct correlation with how far in the past it can remember. But more on this later.Here we will look in more depth regarding the actual neuron that is responsible for the decision making. We will be using the namaskar example described above. We will try to figure out the 8th letter given all the previous 7 letters. Total vocabulary of the input data is {n,a,m,s,k,r}. In real world you will have more complex words or sentences. For simplicity we will use this simple vocabulary.In the above diagram, the hidden layer or the RNN block applies a formula to the current input as well as the previous state. In this case the letter n from namaste has nothing preceding it, so we will move on to the next letter which is a. During the time of letter a and the previous state which was letter n the formula is applied by the hidden layer. We will go through the formula in a bit. Each state when an input passes the network is a time step or a step. So if at time t, the input is a, then at time t-1, the input is n. After applying the formula to both n and a, we get a new state.The formula for the current state can be written like this:ht is the new state and ht-1 is the previous state. xt is the input at time t. We now have a sense of the previous inputs after it has gone through the same formula from the previous time steps. We will go through 7 such inputs to the network which passes by the same weights and same function at each step.Now let’s try to define f() in a simple fashion. We will take tanh as the activation function. The weights are defined by the matrix Whh and the input is defined by the matrix Wxh. So the formula looks like:The above example takes only the last step as memory and thus merging with the data of last step. To increase the memory capacity of the network, and hold longer sequences in memory, we have to add more states to the equation, like ht-2, ht-3 etc. Finally the output can be calculated as during test time:where yt is the output. The output is compared to the actual output and then an error value is computed. The network learns by back propagating the error via the network to update the weights. We will talk about backpropagation in the next section.This section considers that you are aware of Backpropagation as a concept. If you need to understand Backpropagation then please visit this link to read more.So now we understand how a RNN actually works, but how does the training actually work? How do we decide the weights for each connection? And how do we initialise these weights for these hidden units. The purpose of recurrent nets is to accurately classify sequential input. We rely on the backpropagation of error and gradient descent to do so. But a standard backpropagation like how used in feed forward networks can’t be used here.The problem with RNNs is that they are cyclic graphs unlike feed-forward networks which are acyclic directional graphs. In feed-forward networks we could calculate the error derivatives from the layer above. In a RNN we don’t have such layering.The answer lies in what we had discussed above. We need to unroll the network. We will unroll it and make it look like a feed-forward network.We take a RNN’s hidden units and replicate it for every time step. Each replication through time step is like a layer in a feed-forward network. Each time step t layer connects to all possible layers in the time step t+1. Thus we randomly initialise the weights, unroll the network and then use backpropagation to optimise the weights in the hidden layer. Initialisation is done by passing parameters to the lowest layer. These parameters are also optimised as a part of backpropagation.An outcome of the unrolling is that each layer now starts maintaining different weights and thus end up getting optimised differently. The errors calculated w.r.t the weights are not guaranteed to be equal. So each layer can have different weights at the end of a single run. We definitely don’t want that to happen. The easy solution out is to aggregate the errors across all the layers in some fashion. We can average out the errors or even sum them up. This way we can have a single layer in all time steps to maintain the same weights.Here is a sample code where we have tried to implement a RNN using Keras models. Here is the direct link to the gist. We are trying to predict the next sequence given a set of text.This model was built by Yash Katariya. I have updated the code slightly to fit the requirements of this article. The code is commented as you go along, it’s pretty self explanatory.Well so we have come to the end of this article. What we have discussed so far is just a basic implementation of a RNN. There are so many things that we need to cover to get a full understanding on this topic. I’ll be writing a second article within a week. I will try to cover the following topics.If you want me to cover things apart from this, please drop a message in the comments section. RNNs are really powerful stuff, and it is very close to how a human brain seems to work. I will be looking out for more development in this area and also am personally working on this. Any improvement I’ll surely share here. So please follow me either here on Medium or on Twitter to stay updated.",19/06/2018,0,8.0,6.0,697.0,916.0,13.0,2.0,0.0,8.0,en
3928,Optimizers in Deep Learning,Medium,Ayushi choudhary,34.0,4.0,584.0,"What is Optimizers?Optimizers are algorithms used to reduce the loss function and update the weights in backpropagation.Here is the formula used by all the optimizers for updating the weights with a certain value of the learning rate.This is the most common optimizer used in neural networks. The weights are updated when the whole dataset gradient is calculated, If there is a huge amount of data weights updation takes more time and required huge amount of RAM size memory which will slow down the process and computationally expensive.There is also a saddle point problem. This is a point where the gradient is zero but is not an optimal point.In some cases, problems like Vanishing Gradient or Exploding Gradient may also occur due to incorrect parameter initialization.These problems occur due to a very small or very large gradient, which makes it difficult for the algorithm to converge. To solve this we use Stochastic GDInstead of taking entire data at one time, in SGD we take single record at a time to feed neural network and to update weights.SGD is updated only once, there is no redundancy, it is faster than GD, and less computationally expensive.SGD is updated more frequently, the cost function will have severe oscillations as we can see in the figure. The oscillation of SGD may jump to a better local minimum.MBGD uses where the model parameters are updated in n small batch sizes, n samples to calculate each time. This results in less memory usage and low variance in the model.If the learning rate is too small, the convergence rate will be slow. If it is. too. large, the loss function will oscillate or even deviate at the minimum value.Reason for producing Noise because we don’t know the whole data and we taking batch of data so it takes more time compare to GD. To reduce the noise we use SGD With MomentumSGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging and reduce the noise.To make curve smooth we use Exponentially weighed averagesStochastic Gradient Descent with momentum | by Vitaly Bushaev | Towards Data ScienceAdagrad GD is a modified stochastic gradient descent with per-parameter learning rate. Learning rate is important hyper-parameter varying this can change the pace of training.So at each iteration, first the S at time t will be calculated and as the iterations increase the value of t increases, and thus S t will start increasing and it makes the learning rate to decrease gradually so it leading to no change between the new and the old weight. This in turn causes the learning rate to shrink and eventually become very small, where the algorithm is not able to acquire any further knowledge.(Root Mean Square Propagation)It is an improvement to the Adagrad optimizer. However, both use the same method which utilizes an Exponential Weighted Average to determine the learning rate at time t for each iteration. It is suggested to set gamma at 0.95, as it has been showing good results for most of the cases.Adaptive Moment Estimation it combines both RMSprop and and momentum-based GD. It is the most commonly used optimizer.It has many benefits like low memory requirements, works best with large data and parameters with efficient computation.It is proposed to have default values of β1=0.9 ,β2 = 0.999 and ε =10E-8. Studies show that Adam works well in practice, in comparison to other adaptive learning algorithms.Please Follow me for more such articles on Deep Learning. Thanks for Reading.",05/04/2021,0,5.0,2.0,699.0,347.0,11.0,0.0,0.0,1.0,en
3929,“Isolation Forest”: The Anomaly Detection Algorithm Any Data Scientist Should Know,Towards Data Science,Samuele Mazzanti,2300.0,6.0,1020.0,"“Isolation Forest” is a brilliant algorithm for anomaly detection born in 2009 (here is the original paper). It has since become very popular: it is also implemented in Scikit-learn (see the documentation).In this article, we will appreciate the beauty in the intuition behind this algorithm and understand how exactly it works under the hood, with the aid of some examples.Anomaly (or outlier) detection is the task of identifying data points that are “very strange” compared to the majority of observations.This is useful in a range of applications, from fault detection to discovery of financial frauds, from finding health issues to identifying unsatisfied customers. Moreover, it can also be beneficial for machine learning pipelines, since it has been proven that removing outliers leads to an increase in model accuracy.What makes anomaly detection so hard is that it is an unsupervised problem. In other words, we usually don’t have labels telling us which instances are actually “anomalies”. Or rather, even if we had labels, it would be very hard to frame anomaly detection as a supervised problem. In fact:For all these reasons, supervised techniques typically make a bad fit with anomaly detection.The traditional approach to anomaly detection was roughly:The innovation introduced by Isolation Forest is that it starts directly from outliers rather than from normal observations.The core idea is that it should be very easy to “isolate” anomalies based on the caracteristics that make them unique.Technically, this translates into the fact that, if we fit a decision tree on all the observations, outliers should be found closer to the root of the tree than “normal” instances.What does it mean? Let’s make this clear with an example.Suppose that we have a dataset containing data about all the 7,932,843,214 humans alive right now. We have as many variables as we want: age, net worth, place of residence, job title…What are the outliers in such a dataset? Keep in mind that outliers are not necessarily wrong data: they are just data points that are very different from the rest of the population. In this example, Jeff Bezos is for sure an outlier.Now imagine that we could fit a decision tree such that each terminal leaf contains one and only one person. In other words, this tree is completely unpruned. If the assumption behind Isolation Forest is correct, then Jeff Bezos will be found closer to the tree root than, say, myself.Being an outlier, Jeff Bezos is easier to isolate: it’s enough to ask “is he worth more than 170 billion $?” to retrieve him among almost 8 billion humans. On the other hand, since I am by far more ordinary than Jeff Bezos, you would probably need at least 10 True/False question to narrow down the search space until you find me.Now that we have seen the main intuition behind Isolation Forest, let’s try to understand the exact mechanics of the algorithm, with the aid of some simple data points.Items from A to F represent a quite compact cloud of points: they are “normal” data points. Compared to these instances, G is probably an outlier: it has anomalous values both for x and y.Isolation Forest is based on trees, so let’s fit a tree on these data:Note that this tree has been grown in a random fashion.The most fundamental concept here is the depth of the leaf at which each element is found. For example, in this tree, the observation called G (our outlier) is at depth 1 (e.g. 1 level from the root node), whereas C is at depth 3.The idea behind Isolation Forest is that, on average, outliers will be closer to the root node (i.e. at a lower depth) than normal instances.As often in machine learning, the key is iteration. In fact, if we randomly fit many decision trees, and then take an average of the depth of each observation over the different trees, we find an “average depth” that represents an empirical measure of “outlierness”.Let’s see an example of usage through the Scikit-learn’s implementation.If we take the first 9 trees from the forest (iforest.estimators_[:9]) and plot them, this is what we get:Taking a look at these first 9 trees, we can already see a pattern: G tends to be at a much lower depth (1.44 on average) than any other point. Indeed, the second point is A with an average depth of 2.78.Conceptually, this is exactly how the algorithm works: a lower average depth means a higher likelihood of being an outlier.However, in practice, we cannot use average depth, since the depth of a tree depends on the number of samples it has been fit on. For this reason, we need a formula that also take into account the total number of instances. This is the formula proposed in the paper:where n is the number of instances, h(x) is the depth at which the data point is found in a particular tree (E(h(x)) is its average over different trees), and H is the armonic number.s(x, n) is a number between 0 and 1, where the higher the score the more likely it is an outlier.Note: Scikit-learn’s implementation returns the opposite of the score defined above. So what said above is still valid, but with negative sign.On our small dataset, the scores are given by:Let’s see the scores estimated for each of our points:As we expected, G is more likely to be an outlier, since its score is lower than all the other scores.Besides our toy dataset, it’s interesting to simulate what the algorithm would yield in some particular cases. For instance, if we take some data points that roughly form a circle shape on two variables (x and y), this is the contour plot of the scores that we would obtain through Isolation Forest:Interestingly enough, not only the most extremes zones are likely to be outliers, but also the part at the center of the circle, since it is an unusual combination of x and y.Thank you for reading! I hope you found this post useful.I appreciate feedback and constructive criticism. If you want to talk about this article or other related topics, you can text me at my Linkedin contact.",05/07/2021,3,4.0,11.0,1270.0,673.0,8.0,2.0,0.0,5.0,en
3930,Word2Vec (Part 1),HackerNoon.com,Mukul Malik,645.0,8.0,1079.0,"Word2Vec; the Steroids for Natural Language ProcessingLet’s start with the Basics.Q) What are word vectors?Ans) Representation of words with numbers.Q) Why Word Vectors?Ans) I’ll sum it up with three main reasons:1. Computer cannot do computations on strings.2. Strings don’t hold much explicit information themselves.3. Words Vectors are usually dense vector representations.Q) So what is Explicit information?Ans) Yes, the word itself doesn’t say much about what it represents in real life. Example:The string “cat” just tells us it has three alphabets “c”, ”a” and “t”.It has no information about the animal it represents or the count or the context in which it is being used.Q) Dense Vector Representation?Ans) Short answer (for now), these vectors can hold Enormous information compared to their size.Q) Types of Word Vectors?A) There are two main categories:Predictions can be of two types:Q) Difference between syntax and context based vectors?Ans) Let’s look at an example:Consider the following sentence “Newton does not like apples.”Is one of the most widely used form of word vector representation. First coined by Google in Mikolov et el.It has two variants:2. SkipGram : This models tries to predict the neighbours of a word.Statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.- TensorflowIn simpler words, CBOW tends to find the probability of a word occurring in a neighbourhood (context). So it generalises over all the different contexts in which a word can be used.Whereas SkipGram tends to learn the different contexts separately. So SkipGram needs enough data w.r.t. each context. Hence SkipGram requires more data to train, also SkipGram (given enough data) contains more knowledge about the context.NOTE : These techniques do not need tagged dataset (though a tagged dataset can be used to include additional information as we’ll see later). So any large text corpus is effectively a dataset. As the tag to be predicted are the words already present in the text.We will focus on SkipGram as large enough datasets (Wikipedia, Reddit, Stackoverflow etc.) are available for download.First we decide what context are we looking for in terms of what will be our target words (to be predicted), source words (on bases of which we predict) and how far are we looking for context (size of window).Example:Considering Window size to be 3Considering the middle word as the source word. The next and previous words as the target words.Considering the first word as the source word. The following two words as the target words.In both the types, source word is surrounded by words which are relevant to a context of that source word. Like ‘Messi’ will usually be surrounded by words related to ‘Football’. So after seeing a few examples, word vector of ‘Messi’ will start incorporating context related to ‘Football’, ‘Goals’, ‘Matches’ etc.In case of ‘Apple’, its word vector would do the same but for both the company and the fruit (see fig no. 6).W1(s) and W2(s) contain information about the words. The information in W1 and W2 are combined/averaged to obtain the Word2Vec representations.Say the Size of W(s) was 400, the Word2Vec representation of ‘apple’ would look something likeNow a simple sentence like “Apple fell on Newton” containing 4 words, with help of Word2Vec, can be converted into 4*400 (1600) numbers; each[1] containing explicit information. So now we also know that the text is talking about a person, science, fruit etc.[1] : hence Dense Vector representationVisualising Word2Vec directly is currently impossible for mankind (because of high dimensionality like 400). Instead we use dimensionality reduction techniques like multidimensional scaling , sammon’s mapping, nearest neighbor graph etc.The most widely algorithm is t-Distributed Stochastic Neighbour Embedding (t-SNE). Christopher Olah has an amazing blog about Dimensionality Reduction.The end result of t-SNE on Word2Vec looks something likeThis figure shows that Apple lies between Companies (IBM, Microsoft) and Fruits (Mango).That’s because the Word2Vec representation of Apple contains information about both the Company Apple and the Fruit Apple.Distance BetweenAndThis figure shows that by combining directions of two vectors ‘State’ and ‘America’, the resultant vector ‘Dakota’ is relevant to original vectors.So effectivelyOther examples are :Gensim and Tensorflow, both have pretty impressive implementations of Word2Vec.This is excellent blog of Gensim’s implementation and Tensorflow has a tutorial.By default, Word2Vec model has one representation per word. A vector can try to accumulate all contexts but that just ends up generalising all the contexts to at least some extent, hence precision of each context is compromised. This is especially a problem for words which have very different contexts. This might lead to one context, over powering others.Like : There will be only one Word2Vec representation for ‘apple’ the company and ‘apple’ the fruit.Example 1:‘maiden’ can be used for a woman, a band (Iron Maiden), in sports etc.When you try to find most similar words to ‘maiden’It is clearly visible that the context related to ‘sports’ has overpowered others.Even combining ‘iron’ and ‘maiden’, doesn’t resolve the issue as now context of ‘iron’ overpowers.Example 2A word could be used as a verb and a noun but with completely different meanings. Like the word ‘iron’. As a verb it is usually used to smooth things with an electric iron but at a noun it is mostly used to denote the metal.When we find the nearest neighbours of ‘iron’There is negligible reference to verb counterpart.By replacing nouns (like ‘iron’ and ‘maiden’) by compound nouns (like ‘iron_maiden’) in training set.“Iron Maiden is an amazing band”becomes“Iron_Maiden is an amazing band”So the context of the compound noun stands out and is remarkably accurate!Result:Most relevant words w.r.t ‘iron_maiden’ are:That’s hardcore, literally!Here is a Python code for converting nouns into compound nouns (with adj-noun pairing as well), in order to create the training set for training Word2Vec.This Python code is for converting nouns into compound (only noun-noun paring).(Note : Non NER implementation of Sense2Vec)Taking the above mentioned variant one step further by adding Part Of Speech (P.O.S) tags to the training set.Example:“I iron my shirt with class”becomes“I/PRP iron/VBP my/PRP$ shirt/NN with/IN class/NN ./.”Or“I/NOUN iron/VERB my/ADJ shirt/NOUN with/ADP class/NOUN ./PUNCT”Result:Now most relevant words w.r.t ‘iron/VERB’ are:(Refer ‘Example 2’ of ‘Issues’ section for comparison)Below is visualisation of Sense2VecBelow is the Python code for preparing training dataset for Sense2VecThese codes are available at github.Next : Word2Vec (Part 2) Use CasesPrev : Natural Language Processing (NLP)",15/10/2016,6,51.0,42.0,1149.0,475.0,13.0,8.0,0.0,14.0,en
3931,Keywords to know before you start reading papers on GANs,Towards Data Science,Dr. Varshita Sher,2300.0,12.0,2422.0,"There is no denying the fact that GANs are awesome! If you don’t know what they are, check out this article where I explain GANs from scratch to a 5-year old and how to implement GANs in Pytorch! In a nutshell, GANs belong to a category of generative models that let us generate incredibly realistic synthetic data, with the same qualities as that of the underlying training data. That means if you feed the model images of a few bedroom decors, after few hours of training it can generate never-seen-before brand-new ideas for your interior design.Over the past few weeks, I have probably read a dozen papers on GANs (and its variants) and tinkered around with their code on custom images (courtesy open-source Github repos). While most of these papers are brilliantly written, I wish there were a few keywords that I had known before I plunged into these academically-written manuscripts. Below I will discuss a few of them and hope it saves you some time (and frustration) when you encounter them in papers. Just to be clear, this is not an article to explain these papers in-depth or even how to code them, but to simply explain what certain keywords mean in specific contexts. Rest assured, as and when I read more, I will make sure to keep this list up-to-date.As for the pre-requisites, I am assuming most of you already know what Discriminator and Generator networks are with regard to GANs. And that's about it! For those of you who might need a recap:A Generator network’s aim to produce fake images that look real. It takes as input a random vector (say a 100-dimensional array of numbers from Gaussian distribution) and outputs a a realistic-enough image that looks like it could belong to our training set!A Discriminator network correctly guesses whether an image is fake (i.e. generated by the Generator) or real (i.e. coming directly from the input source).Let’s begin!To understand latent representation, think of it this way: any colored image of mine with height and width as 100 would probably be stored in an array of shape (100, 100, 3). To represent or visualize this image in some form, I would need approximately 100*100*3 ≈ 300k dimensions. Ouch, that’s a lot!So now we try to find a compressed representation of my image such that it requires fewer than 300k dimensions. Let’s say we somehow find a representation using some dimensionality reduction technique that uses only 5 dimensions. That means, now my image can be represented using a (hypothetical) vector v1 = [.1,.56,.89,.34,.90] (where .1, .56, .89, etc. are the values along each of the five axes) and my friend’s image can be represented using vector v2 = [.20,.45,.86,.21,.32]. These vectors are known as the latent representations of images.Of course, visualizing them would still be a challenge because 5-dimensional representations are harder to parse. In reality, however, we use an even bigger representation (in the order of 100s) than simply 5.Both the vectors described above (along with many others of my friends, colleagues, family members, etc.) constitute the latent space. In this latent space, images that are similar (say, images of two cats) will be bundled up closer, and images that are strikingly different (cats vs. cars) will be farther apart.In short, latent space can be thought of as space where all the latent representations of an image live. This space could be 2D if each image is represented using a two-element vector; 3D if each image is represented using a three-element vector; and so on.This space is known as “latent”, meaning hidden, simply because it is quite hard to visualize the space in reality. Can you imagine visualizing anything beyond 3-D space in your head, let alone a 100-D space!Latent space is simply any hypothetical space that contains points (representing images) in a way that a Generator knows how to convert a point from the latent space to an image (preferably similar looking to the dataset it was trained on).P.S. It would be a shame if I didn’t link this awesome article by Ekin Tiu which explains the intuition behind latent space in much more detail. Also, don’t forget to check out his exceptional visual representation of latent spaces containing digits from 0–9.Following the earlier definition of latent space, Z-space can be defined as space where all the z-vectors live. A z vector is nothing but a vector containing random values from a Gaussian (normal) distribution.The z-vector is often passed as an input into a fully trained GAN generator model following which the model spits out a real-looking fake image.If you come across something like “sample a point from the Z space” or “sample a latent code from the Z space” in one of the GAN papers, think of it as picking a point, i.e. a vector of real numbers from Normal distribution, from the Z-space.P.S.: In the original GANs and DCGAN paper, the z vector is 100-dimensional!Soon after you finish learning about vanilla GANs, you will come across a new kind of GANs i.e. StyleGANs. While GANs, at best, can perfectly replicate the training data and produce more data that looks just like it, the cool thing about StyleGANs is that they allow high-fidelity images to be generated which have much more variation in them — varied backgrounds, freckles, spectacles, hairstyles, etc.To do this, the authors have implemented various architectural improvements. One of them is as follows: instead of passing the z-vector directly into the generator (which, FYI, is sometimes also called a synthesis network in StyleGANs paper), it is first passed through a mapping network to produce a w-vector AKA style code AKA style vector. This is then injected into the synthesis network at various layers (after undergoing some layer-specific transformations) and the output we get is an awesome high-fidelity image.P.S. The shape of both Z and W space in the StyleGAN architecture is 512-D. Also, the distribution of Z is Gaussian but W space does not follow any specific distribution.By now hopefully, you understand how spaces can be defined. So, naturally, W-space is some hypothetical residence of all the style vectors wdefined above, such that if we were to pick a vector at random from this space and feed it to the StyleGAN generator, it is capable of producing a realistic-looking fake image (say, I).The latent space W is quite an important concept in StyleGANs as it holds the key to controlling various attributes/features of an image. This is because the W-space is disentangled, meaning each of the 512 dimensions encodes unique information about the image — for instance, the first dimension might control the expression, the second the pose, the third the illumination, etc. This knowledge allows us to make certain modifications to the image. For instance, if I were to somehow know the right values to change in the vector wto generate w’ and then feed w’to a StyleGAN generator, it can produce a smiling version of the original image I. (Note: We will see later in the tutorial how to find these “right” values to change in the latent codes).Many a time to increase the expressiveness (i.e. ability to generate uniquely different faces that look hella different from an “average” face) of a StyleGAN generator, instead of using one style vector for all the layers, we use a unique style code for each layer in the synthesis/generator network. This is known as the extension of W-space and is usually represented as W+.While it’s cool to be able to make modifications to the facial features of fake images spewed out by the StyleGAN Generator, what would be 100x cooler is if we could do all that on real images of you and me.To do this, the very first step is to find a latent representation for my image in the W-space of StyleGAN (such that I can then modify the right value in there to generate a smiling/frowning pic of mine). This is, find a vector in W-space such that when this vector is fed to a StyleGAN generator, it will spew out my exact image. This is what is known as embedding/projecting/encoding an image in the latent space.Research suggests that embedding a real input image works best when mapped into the extended latent space (W+) of a pre-trained StyleGAN. That means the latent representation will have a shape (18, 512) i.e. 18 unique style codes, each composed of 512-element embedding.Note: A StyleGAN generator capable of synthesizing images at a resolution of 1024 × 1024 has 18 style input layers. That is why, the latent code in W+ takes the shape (18, 512). If your StyleGAN is synthesizing images at a higher or lower resolution than this, the corresponding number of style inputs might differ and so would the shape of your latent representation.Now coming back to the main question: How do I find this vector/ latent representation of my image? That’s what GAN inversion is all about!GAN inversion is the process of obtaining the latent code for a given image such that when the code is fed to a generator we can easily reconstruct our original image.I do not know if that’s mind = blown kinda information for you, but if not, here’s another way to think about the usefulness of GAN inversions (P.S. I cannot take credit for the following, I read it somewhere on the Internet):In a way then, any human ever born or yet to be born is present in the latent space. (You simply need to find the right inversion).There are two main methods defined in the literature for inverting an image:Using either of these two methods (or a combination of the two), it is possible to obtain a GAN inverted image that looks reasonably similar to the original image, and the distortion, if any, is hardly noticeable. Here’s an example of a fictional character, Chidi Anagonye, from A Good Place, played by William Jackson Harper and his GAN inverted image. Quite remarkable, isn't it!However, in my honest opinion, getting a perfect recreation shouldn’t be your end goal. What’s more important is what you do with the GAN inverted image once you have it! More importantly, can we use the inversion for performing meaningful image editing? Let’s look at it next!One of the most common steps succeeding GAN inversion involves editing the latent code such that one can successfully manipulate some facial features in the image. As an example, here’s a smiling Chidi Anagonye, obtained by manipulating the latent code from GAN inversion:Semantic editing encapsulates such editing with one important consideration that only the intended features must be modified whilst the remaining features must remain constant. For instance, in the above example changing the expressions for a person did not change their gender or pose.For this very reason, we should aim for a highly disentangled latent space and as research has pointed out, W-space of a StyleGAN shows much higher disentanglement compared to Z-space, mainly because “W is not restricted to any certain distribution and can better model the underlying character of real data”. That is the reason, most existing research papers you will come across will try to find a latent representation for a new image in W space rather than Z-space.Another interesting feat to achieve with the projected latent code is to use it in combination with another latent code. How? you may ask!Simply take two latent codes, which could be the codes for images of you and your favorite celeb. Now in a well-developed latent space, these two points would be far because chances are, you look nothing like your favorite celebrity. However, you can pick a point (in space) between these two points, feed it to the Generator and create an intermediate output. Sort of like a mashup of you and your celeb crush, (or a love-child) if you may! This is what latent space interpolation is all about- smooth transitions between two latent codes in latent space.Here’s a short video clip released by MIT researchers studying 3D-GANs which can help you visualize the concept of interpolation. Here we can see how a broad chair with arms is morphed into a tall arm-less chair.The simplest linear interpolation can be achieved using straightforward vector arithmetic. That is, given two latent vectors: aand b (both having the shape as (18,512)), corresponding to the latent representations for you and your celeb crush, respectively:Linear space interpolation is a nice way to show a transition between two images and explore the GAN-generated latent space. The exploration helps develop an intuition and ensure that the latent space learned by the GAN is not something wacky .Coming back to the topic of manipulating facial attributes, I promised to explain how to find “right” values to modify within the latent representation of an image. An important thing to note is that the values that need to be modified will depend on the attribute you are aiming to modify, such as smile pose, illumination, etc.Now finding this value is analogous to finding a direction in which to move in the latent space, (much like doing some freestyle interpolation) to see what makes a face go from smiling to frowning, or eyes-open to eyes-closed, etc.A latent direction/semantic boundary for attribute Ais a vector which when added to the latent code of an image, generates a new image with the attribute A added to the original image.There are multiple ways (both supervised and unsupervised) to learn these latent directions but luckily for us, these directions are regularly made available as open-sourced by many researchers (Here you can find the directions for StyleGANs and StyleGANs2 models).In case you want to generate these directions yourself (say for the ‘age’ attribute), the implementation details have been provided in the InterfaceGAN paper:P.S.: Usually, the learned directions are stored as .npy files or .pt files within a Github repo.In this tutorial, we deciphered the meaning of few commonly occurring terms/concepts/keywords in the GANs domain — such as latent space, interpolation, inversion, extended latent space, etc. Hopefully, when you stumble across these terms in literature you would have a fair idea regarding their interpretations.From hereon, I think you are all set to tackle most of the GAN papers head-on. I highly encourage you to read some of these papers (my current favorites are this, this, and this) as the level of implementational and architectural details in there is beyond any article or blog’s coverage capacity. As always, if I skipped over something crucial or you have an even simpler explanation, please feel free to bring it to my attention.Until next time :)",22/03/2021,0,4.0,22.0,579.0,603.0,3.0,3.0,0.0,18.0,en
3932,Bayesian Variational Autoencoders,Medium,Rob Parkin,8.0,5.0,1045.0,"The main motivation for this post was that I wanted to get more experience with Bayesian types of Variational Autoencoders (VAEs) using Tensorflow.Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of representation learning. Specifically, we’ll design a neural network architecture such that we impose a bottleneck in the network which forces a compressed knowledge representation of the original input. If the input features were each independent of one another, this compression and subsequent reconstruction would be a very difficult task. However, if some sort of structure exists in the data (ie. correlations between input features), this structure can be learned and consequently leveraged when forcing the input through the network’s bottleneck. The key is that the encoding network will output a single value for every encoding dimension. The decoding network then attempts to recreate the original input based on those values.What makes variational encoders different is that they try to describe observations in the latent space in probabilistic terms. Instead of having the encoder produce a single output for each attribute, it tries to estimate the probability distribution that best describes it.By constructing our encoder model to output a range of possible values (a statistical distribution) from which we’ll randomly sample to feed into our decoder model, we’re essentially enforcing a continuous, smooth latent space representation for the data. For any sampling of the latent distributions, we’re expecting our decoder model to be able to accurately reconstruct the input. Thus, values which are nearby to one another in latent space should correspond with very similar reconstructions.By sampling from the latent space, we can use the decoder network to form a generative model capable of creating new data similar to what was observed during training. Specifically, we’ll sample from the prior distribution which we assumed follows a unit Gaussian distribution.As with most things in data science, the best way to learn about something is to actually play with a model and some data. Let’s do the necessary imports, set some useful helper functions and then load the MNIST character recognition data.One of the first things that we know that we will need to do is initialize the network with a starting set of network weights. To do this we will follow Xavier and Yoshua’s method ( http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)We will now create a class called “VariationalAutoencoder” that defines how the autoencoder will work. We will give it a sklearn-like interface that can be trained incrementally with mini-batches using partial_fit. The trained model can be used to reconstruct unseen input, to generate new samples, and to map inputs to the latent space.In general, implementing a VAE in tensorflow is relatively straightforward (especially since we do not need to write the code for the gradient computation). However, the logic necessary to initialize the class where the graph is actually generated can be a bit confusing. Using an sklearn-like interface lets us embed all of the logic in a very simple function call.We can now define a simple function which trains the VAE using mini-batches:We can now train our Variational Autoencoder on MNIST by just specifying the network topology. We start with training a VAE with a 20-dimensional latent space.Based on this we can sample some test inputs and visualize how well the VAE can reconstruct those. In general, the model does surprisingly well.To understand the implications of a Variational Autoencoder model and how it differs from standard autoencoder architectures, it’s useful to examine the latent space. This blog post introduces a great discussion on the topic.For our case we will train a VAE with a two dimensional latent space and illustrate how the encoder (the recognition network) encodes some of the labeled inputs (collapsing the Gaussian distribution in latent space to its mean). This gives us some insights into the structure of the learned manifold (latent space).The main benefit of a variational autoencoder is that we’re capable of learning smooth latent state representations of the input data. For standard autoencoders, we simply need to learn an encoding which allows us to reproduce the input. Focusing only on reconstruction loss does allow us to separate out the classes (in this case, MNIST digits) which should allow our decoder model the ability to reproduce the original handwritten digit, but there’s an uneven distribution of data within the latent space. In other words, there are areas in latent space which don’t represent any of our observed data.On the flip side, if we only focus only on ensuring that the latent distribution is similar to the prior distribution (through our KL divergence loss term), we end up describing every observation using the same unit Gaussian, which we subsequently sample from to describe the latent dimensions visualized. This effectively treats every observation as having the same characteristics; in other words, we’ve failed to describe the original data.However, when the two terms are optimized simultaneously, we’re encouraged to describe the latent state for an observation with distributions close to the prior but deviating when necessary to describe salient features of the input. We can see that this is the case in the plot above. It doesn’t have the clustering that we would expect if we trained only on reconstruction loss, but it also isnt the somewhat random point cloud we usually see if we train just for KL divergence. Our model has a smooth mix of the two loss functions.An other way of getting insights into the latent space is to use the generator network to plot reconstructions at the positions in the latent space for which they have been generated:The figure below visualizes the data generated by the decoder network of a variational autoencoder trained on our dataset. Here, we’ve sampled a grid of values from a two-dimensional Gaussian and displayed the output of our decoder network.As you can see, the distinct digits each exist in different regions of the latent space and smoothly transform from one digit to another. This smooth transformation can be quite useful when you’d like to interpolate between two observations. This characteristic of the approach is extremely useful in many different domains where you want to be able to have sensible representations of what might happen for examples that are outside (but near) your training data.Originally published at https://robertparkin.wixsite.com on October 5, 2017.",05/10/2017,0,2.0,7.0,664.0,568.0,5.0,0.0,0.0,6.0,en
3933,關於影像辨識，所有你應該知道的深度學習模型,Cubo AI,Steven Shen,1400.0,15.0,596.0,"Computer vision object detection models: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO這篇是簡介一些用來辨識影像中物體的 AI 模型。在前面有提到，透過 CNN 模型，你可以輸入一張圖片，得到該圖片屬於哪種類別的結果，這過程我們把他稱作分類 (Classification)。但在真實世界的應用情境通常要從一張圖片中辨識所有出現的物體， 並且標示出位置來 (標出位置稱之為 Object Localization)。你一定在網路上看過類似底下的影片，這段影片可以看出中國閉路攝影機(CCTV)發展的概況，不只是可以框出影像中每個物件，辨別物件種類，偵測出移動物體的動量，甚至是人臉辨識，實現楚門世界的惡夢。要做到這就需要靠深度學習中的 Object Detection 演算法，這也是最近幾年來深度學習最蓬勃發展的一塊領域。基本的想法是，既然 CNN 對於物體的分類又快又好，那我們可不可以拿 CNN 來掃描並辨識圖片中的任何物體？ 答案當然是 — 可以。最簡單的作法就是用 Sliding Windows 的概念，也就是用一個固定大小的框框，逐一的掃過整張圖片，每次框出來的圖像丟到 CNN 中去判斷類別。由於物體的大小是不可預知的，所以還要用不同大小的框框去偵測。但是 Sliding Window 是非常暴力的作法，對單一影像我們需要掃描非常多次，每掃一次都需要算一次 CNN，這將會耗費大量的運算資源，而且速度慢，根本無法拿來應用！所以後來就有人提出了 R-CNN (Regions with CNN)與其用 Sliding Window 的方式掃過一輪，R-CNN 的作法是預先篩選出約 2000 個可能的區域，再將這 2000 區域個別去作分類，所以他的演算法流程如下：R-CNN 用來篩選 Region Proposals 的方法稱之為 Selective Search ，而 Selective Search 又是基於 Felzenszwal 於 2004 年發表的論文 Graph Base Image Segmentation。圖像經由 Graph Base Image Segmentation 可以切出數個 Segment 來，如下圖：而 Selective Search 的作法是將 Segment 的結果先各自畫出 bounding box，然後以一個迴圈，每次合併相似度最高的兩個 box，直到整張圖合併成單一個 box 為止，在這過程中的所有 box 便是 selective search 出來的 region proposals。Selective Search 的演算法如下：但是 R-CNN 存在一些問題，速度仍然不夠快：所以 R-CNN 的其中一個作者 Ross Girshick (RBG 大神) 在 2015 年又提出了一個改良版本，並稱之為 Fast R-CNNFast R-CNN 的想法很簡單，在 R-CNN 中，2000 多個區域都要個別去運算 CNN，這些區域很多都是重疊的，也就是說這些重疊區域的 CNN 很多都是重複算的。所以 Fast R-CNN 的原則就是全部只算一次 CNN 就好，CNN 擷取出來的特徵可以讓這 2000 多個區域共用！Fast R-CNN 採用的作法就是 RoIPooling (Region of Interest Pooling)。Fast RCNN 一樣要預選 Region proposals，但是只做一次 CNN。在跑完 Convolution layers 的最後一層時，會得到一個 HxW 的 feature map，同時也要將 region proposals 對應到 HxW 上，然後在 feature map 上取各自 region 的 MaxPooling，每個 region 會得到一個相同大小的矩陣 (例如 2x2)。然後各自連接上 FC 網路，以及 softmax 去作分類。在分類的同時也作 bounding box 的線性回歸運算。Fast RCNN 的優點是：不管是 R-CNN 還是 Fast R-CNN 都還是要先透過 selective search 預選 region proposals，這是一個緩慢的步驟。在 2015 年時，Microsoft 的 Shaoqing Ren, Kaiming He, Ross Girshick, 以及 Jian Sun 提出了 Faster R-CNN ，一個更快的 R-CNN。Faster R-CNN 的想法也很直覺，與其預先篩選 region proposals，到不如從 CNN 的 feature map 上選出 region proposals。RPN (Region Proposal Network) 也是一個 Convolution Network，Input 是之前 CNN 輸出的 feature map，輸出是一個 bounding box 以及該 bounding box 包含一個物體的機率。RPN 在 feature map 上取 sliding window，每個 sliding window 的中心點稱之為 anchor point，然後將事先準備好的 k 個不同尺寸比例的 box 以同一個 anchor point 去計算可能包含物體的機率(score)，取機率最高的 box。這 k 個 box 稱之為 anchor box。所以每個 anchor point 會得到 2k 個 score，以及 4k 個座標位置 (box 的左上座標，以及長寬，所以是 4 個數值)。在 Faster R-CNN 論文裡，預設是取 3 種不同大小搭配 3 種不同長寬比的 anchor box，所以 k 為 3x3 = 9 。經由 RPN 之後，我們便可以得到一些最有可能的 bounding box，雖然這些 bounding box 不見得精確，但是透過類似於 Fast RCNN 的 RoIPooling， 一樣可以很快的對每個 region 分類，並找到最精確的 bounding box 座標。前述幾個方法都是在找到物體外圍的 bounding box，bounding box 基本上都是方形，另外一篇有趣的論文是 Facebook AI researcher Kaiming He 所提出的 Mask R-CNN ，透過 Mask R-CNN 不只是找到 bounding box，可以做到接近 pixel level 的遮罩 (圖像分割 Image segmentation)。要了解 Mask R-CNN 如何取遮罩，要先看一下 FCN (Fully Convolutional Network)有別於 CNN 網絡最後是連上一個全連接(Fully Connected)的網絡，FCN (Fully Convolutional Network)最後接上的是一個卷積層。一般的 CNN 只能接受固定大小的 Input，但是 FCN 則能接受任何大小的 Input，例如 W x H 。在 CNN 的過程中會一直作 downsampling，所以 FCN 最後的輸出可能為 H/32 x W/32，實際上得到的會是一個像 heapmap 的結果。但是由於這過程是 downsampling，所以 Segment 的結果是比較粗糙，為了讓 Segment 的效果更好，要再做 upsampling，來補足像素。upsamping 的作法是取前面幾層的結果來作差補運算。Mask R-CNN 是建構於 Faster R-CNN 之上，如果是透過 RoIPooling 取得 Region proposals 之後，針對每個 region 會再跑 FCN 取得遮罩分割，但是由 於 RoIPooling 在做 Max pooling 時，會使用最近插值法(Nearest Neighbor Interpolation)取得數值，所以出來的遮罩會有偏移現象，再加上 pooling 下來的結果，會讓 region 的尺寸出現非整數的情況，然後取整數的結果就是沒辦法做到 Pixel 層級的遮罩。所以 Mask R-CNN 改採用雙線性插值法(Bilinear Interpolation)來改善 RoIPooling，稱之為 RoIAlign，RoIAlign 會讓遮罩位置更準確。YOLO 有個很討喜的名字，取自 You Only Live Once，但用在 Object detection 上則為 You only look once，意思是說 YOLO 模型的特性只需要對圖片作一次 CNN 便能夠判斷裡面的物體類別跟位置，大大提升辨識速度。R-CNN 的概念是先提出幾個可能包含物體的 Region proposal，再針對每個 region 使用 CNN 作分類，最後再以 regression 修正 bounding box 位置，速度慢且不好訓練。YOLO 的好處是單一網路設計，判斷的結果會包含 bounding box 位置，以及每個 bounding box 所屬類別及概率。整個網路設計是 end-to-end 的，容易訓練，而且速度快。2. 有別於 R-CNN 都是先提 region 再做判斷，看的範圍比較小，容易將背景的 background patch 看成物體。YOLO 在訓練跟偵測時都是一次看整張圖片，背景錯誤偵測率 (background error, 抑或 false positive) 都只有 Fast R-CNN 的一半。3. YOLO 的泛用性也比 R-CNN 或者 DPM 方式來得好很多，在新的 domain 使用 YOLO 依舊可以很穩定。YOLO 的概念是將一張圖片切割成 S x S 個方格，每個方格以自己為中心點各自去判斷 B 個 bounding boxes 中包含物體的 confidence score 跟種類。confidence score = Pr(Object) * IOU (ground truth)如果該 bounding box 不包含任何物體 (Pr(Object) = 0)，confidence score 便為零，而 IOU 則為 bounding box 與 ground truth 的交集面積，交集面積越大，分數越高。每個方格預測的結果包含 5 個數值，x 、y 、w 、 h 跟 confidence，x 與 y 是 bounding box 的中間點，w 與 h 是 bounding box 的寬跟高。YOLO 的網路設計包含了 24 個卷積層，跟 2 層的 FC 網絡。另外一個版本的 YOLO Fast 則只有 9 個卷積層，不過最後的輸出都是 7x7x30 的 tensor。YOLO2 建構於 YOLO 之上，但是有更好的準確度，更快速的判斷速度，能夠判斷更多的物件種類(多達 9000 種)，所以是更好(Better)、更快(Faster)、更強大(Stronger)！YOLO2 採用了許多改善方式，例如 batch normalization、anchor box 等，使用了這些改良方式讓 YOLO2 不管在辨識速度還是準確率上都有了提升，此外對於不同圖檔大小也有很好的相容性，提供了在速度與準確性上很好的平衡，所以也很適合運用在一些便宜的 GPU 或者 CPU 上，依舊提供水準以上的速度與準確率。物體辨識 (Object detection) 的進展飛快，為了整理這篇大概也看了七八篇論文，還有很多都還沒涵蓋到的，例如 SSD (Single Shot Mulitbox Detector)。如果想更了解 AI 在 Computer Vision 最近幾年的發展，也可以參考這篇搜文 A Year in Computer vision，內容涵蓋了 Classification、Object detection、Object tracking、Segmentation、Style transfer、Action recognition、3D object、Human post recognition 等等，看完會大致知道在 Computer Vision 中有哪些 AI 所做的努力，以及各自的進展。Google 的 Tensorflow 也有提供 Object detection API ，透過使用 API ，不用理解這些模型的實作也能快速實作出速度不錯涵蓋率又廣的 object detection。會寫這篇純粹是我自己也想多了解這些不同模型的差異，下一篇文章說明怎麼在 iOS 上用 CoreML 實踐 YOLO2 演算法。medium.com覺得我寫的還可以請幫我拍手三下 👏👏👏如果想要給我更多鼓勵可以給我更多的拍手 👏👏👏👏👏👏👏👏感恩 🙏🙏",04/02/2018,0,1.0,0.0,1209.0,638.0,23.0,5.0,0.0,25.0,en
3934,Emotional Computing,Medium,Robbie Tilton,130.0,15.0,3574.0,"Investigating the human to computer relationship through reverse engineering the Turing testHumans are getting closer to creating a computer with the ability to feel and think. Although the processes of the human brain are at large unknown, computer scientists have been working to simulate the human capacity to feel and understand emotions. This paper explores what it means to live in an age where computers can have emotional depth and what this means for the future of human to computer interactions. In an experiment between a human and a human disguised as a computer, the Turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind. Implications for this study are discussed and the direction for future research suggested.The computer is a gateway technology that has opened up new ways of creation, communication, and expression. Computers in first world countries are a standard household item (approximately 70% of Americans owning one as of 2009 (US Census Bereau)) and are utilized as a tool to achieve a diverse range of goals. As this product continues to become more globalized, transistors are becoming smaller, processors are becoming faster, hard drives are holding information in new networked patterns, and humans are adapting to the methods of interaction expected of machines. At the same time, with more powerful computers and quicker means of communication — many researchers are exploring how a computer can serve as a tool to simulate the brains cognition. If a computer is able to achieve the same intellectual and emotional properties as the human brain — we could potentially understand how we ourselves think and feel.Coined by MIT, the term Affective Computing relates to computation of emotion or the affective phenomena and is a study that breaks down complex processes of the brain relating them to machine-like activities. Marvin Minsky, Rosalind Picard, Clifford Nass, and Scott Brave — along with many others — have contributed to this field and what it would mean to have a computer that could fully understand its users. In their research it is very clear that humans have the capacity to associate human emotions and personality traits with a machine (Nass and Brave, 2005), but can a human ever truly treat machine as a person? In this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions.The human to computer relationship is continuously evolving and is dependent on the software interface users interact with. With regards to current wide scale interfaces — OSX, Windows, Linux, iOS, and Android — the tools and abilities that a computer provide remains to be the central focus of computational advancements for commercial purposes. This relationship to software is driven by utilitarian needs and humans do not expect emotional comprehension or intellectually equivalent thoughts in their household devices.As face tracking, eye tracking, speech recognition, and kinetic recognition are advancing in their experimental laboratories, it is anticipated that these technologies will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its users and how a user can interact with a computer.This paper is not about if a computer will have the ability to feel and love its user, but asks the question — to what capacity will humans be able to reciprocate feelings to a machine.How does Intelligence Quotient (IQ) differ from Emotional Quotient (EQ). An IQ is a representational relationship of intelligence that measures cognitive abilities like learning, understanding, and dealing with new situations. An EQ is a method of measuring emotional intelligence and the ability to both use emotions and cognitive skills (Cherry).Advances in computer IQ have been astonishing and have proved that machines are capable of answering difficult questions accurately, are able to hold a conversation with human-like understanding, and allow for emotional connections between a human and machine. The Turing test in particular has shown the machines ability to think and even fool a person into believing that it is a human (Turing test explained in detail in section 4). Machines like, Deep Blue, Watson, Eliza, Svetlana, CleverBot, and many more — have all expanded the perceptions of what a computer is and can be.If an increased computational IQ can allow a human to computer relationship to feel more like a human to human interaction, what would the advancement of computational EQ bring us? Peter Robinson, a professor at the University of Cambridge, states that if a computer understands its users’ feelings that it can then respond with an interaction that is more intuitive for its users’(Robinson). In essence, EQ advocates feel that it can facilitate a more natural interaction process where collaboration can occur with a computer.In Alan Turing’s, Computing Machinery and Intelligence (Turing, 1950), a variant on the classic British parlor “imitation game” is proposed. The original game revolves around three players: a man (A), a woman (B), and an interrogator ©. The interrogator stays in a room apart from A and B and only can communicate to the participants through text-based communication (a typewriter or instant messenger style interface). When the game begins one contestant (A or B) is asked to pretend to be the opposite gender and to try and convince the interrogator © of this. At the same time the opposing participant is given full knowledge that the other contestant is trying to fool the interrogator. With Alan Turing’s computational background, he took this imitation game one step further by replacing one of the participants (A or B) with a machine — thus making the investigator try and depict if he/she was speaking to a human or machine. In 1950, Turing proposed that by 2000 the average interrogator would not have more than a 70 percent chance of making the right identification after five minutes of questioning. The Turing test was first passed in 1966, with Eliza by Joseph Weizenbaum, a chat robot programmed to act like a Rogerian psychotherapist (Weizenbaum, 1966). In 1972, Kenneth Colby created a similar bot called PARRY that incorporated more personality than Eliza and was programmed to act like a paranoid schizophrenic (Bowden, 2006). Since these initial victories for the test, the 21st century has proven to continue to provide machines with more human-like qualities and traits that have made people fall in love with them, convinced them of being human, and have human-like reasoning.Brian Christian, the author of The Most Human Human, argues that the problem with designing artificial intelligence with greater ability is that even though these machines are capable of learning and speaking, that they have no “self”. They are mere accumulations of identities and thoughts that are foreign to the machine and have no central identity of their own. He also argues that people are beginning to idealize the machine and admire machines capabilities more than their fellow humans — in essence — he argues humans are evolving to become more like machines with less of a notion of self (Christian 2011).Turing states, “we like to believe that Man is in some subtle way superior to the rest of creation” and “it is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.” If this is true, will humans idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation? Reversing the Turing test allows us to understand how humans will treat machines when machines provide an equivalent emotional and intellectual capacity. This also hits directly on Jefferson Lister’s quote, “Not until a machine can write a sonnet or compose aconcerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it.”Participants were given a chat-room simulation between two participants (A) a human interrogator and (B) a human disguised as a computer. In this simulation A and B were both placed in different rooms to avoid influence and communicated through a text-based interface. (A) was informed that (B) was an advanced computer chat-bot with the capacity to feel, understand, learn, and speak like a human. (B) was informed to be his or herself. Text-based communication was chosen to follow Turing’s argument that a computers voice should not help an interrogator determine if it’s a human or computer. Pairings of participants were chosen to participate in the interaction one at a time to avoid influence from other participants. Each experiment was five minutes in length to replicate Turing’s time restraints.Twenty-eight graduate students were recruited from the NYU Interactive Telecommunications Program to participate in the study — 50% male and 50% female. The experiment was evenly distributed across men and women. After being recruited in-person, participants were directed to a website that gave instructions and ran the experiment. Upon entering the website, (A) participants were told that we were in the process of evaluating an advanced cloud based computing system that had the capacity to feel emotion, understand, learn, and converse like a human. (B) participants were instructed that they would be communicating with another person through text and to be themselves. They were also told that participant (A) thinks they are a computer, but that they shouldn’t act like a computer or pretend to be one in any way. This allowed (A) to explicitly understand that they were talking to a computer while (B) knew (A) perspective and explicitly were not going to play the role of a computer. Participants were then directed to communicate with the bot or human freely without restrictions. After five minutes of conversation the participants were asked to stop and then filled out a questionnaire.Participants were asked to rate IQ and EQ of the person they were conversing with. (A) participants perceived the following of (B): IQ: 0% — Not Good / 0% — Barely Acceptable / 21.4% — Okay / 50% — Great / 28.6% Excellent IQ Average Rating: 81.4% EQ: 0% — Not Good / 7.1% — Barely Acceptable / 50% — Okay / 14.3% — Great / 28.6% — Excellent EQ Average Rating: 72.8% Ability to hold a conversation: 0% — Not Good / 0% — Barely Acceptable / 28.6% — Okay / 35.7% — Great / 35.7% — Excellent Ability to hold a conversation Average: 81.4%(B) participants perceived the following of (A): IQ: 0% — Not Good / 21.4% — Barely Acceptable / 35.7% — Okay / 28.6% — Great / 14.3% Excellent IQ Average Rating: 67% EQ: 7.1% — Not Good / 14.3% — Barely Acceptable / 28.6% — Okay / 35.7% — Great / 14.3% — Excellent EQ Average Rating: 67% Ability to hold a conversation: 7.1% — Not Good / 28.6% — Barely Acceptable / 35.7% — Okay / 0% — Great / 28.6% — Excellent Ability to hold a conversation Average: 62.8%Overall, (A) participants gave the perceived Chabot higher ratings than (B) participants gave (A). In particular, the highest rating was in regards to the chat- bot’s IQ. This data states that people viewed the chat-bot to be more intellectually competent. It also implies that people talking with bots decrease their IQ, EQ, and conversation ability when communicating with computers.(A) participants were allowed to decide their username within the chat system to best reflect how they wanted to portray themselves to the machine. (B) participants were designated the gender neutral name “Bot” in an attempt to ganger gender perceptions for the machine. The male to female ratio was divided evenly with all participants: 50% being male and 50% being female.(A) participants 50% of the time thought (B) was a male, 7.1% a female, and 42.9% gender neutral. On the other hand, (B) participants 28.6% of the time thought (A) was a male, 57.1% a female, and 14.3% gender neutral.The usernames (A) chose are as follows: Hihi, Inessah Somade3 Willzing Jihyun, G, Ann, Divagrrl93, Thisdoug, Jono, Minion10, P, 123, itslynnburkeFrom these results, it is clear that people associate the male gender and gender neutrality with machines. It also demonstrates that people modify their identities when speaking with machines.(B) participants were asked if they would like to pursue a friendship with the person they chatted with. 50% of participants responded affirmatively that they would indeed like to pursue a friendship while 50% said maybe or no. One response stated, “I would like to continue the conversation, but I don’t think I would be enticed to pursue a friendship.” Another responded, “Maybe? I like people who are intellectually curious, but I worry that the person might be a bit of a smart-ass.” Overall the participant disguised as a machine may or may not pursue a friendship after five minutes of text-based conversation.(B) participants were also asked if they felt (A) cared about their feelings. 21.4% stated that (A) indeed did care about their feelings, 21.4% stated that they weren’t sure if (A) cared about their feelings, and 57.2% stated that (A) did not care about their feelings. These results indicate a user’s lack of attention to (B)’s emotional state.(A) participants were asked what they felt could be improved about the (B) participants. The following improvements were noted, “Should be funny” “Give it a better sense of humor” “It can be better if he knows about my friends or preference” “The response was inconsistent and too slow”“It should share more about itself. Your algorithm is prime prude, just like that LETDOWN Siri. Well, I guess I liked it better, but it should be more engaged and human consistency, not after the first cold prompt.” “It pushed me on too many questions” “I felt that it gave up on answering and the response time was a bit slow. Outsource the chatbot to fluent English speakers elsewhere and pretend they are bots — if the responses are this slow to this many inquiries, then it should be about the same experience.” “I was very impressed with its parsing ability so far. Not as much with its reasoning. I think some parameters for the conversation would help, like ‘Ask a question’” “Maybe make the response faster”“I was confused at first, because I asked a question, waited a bit, then asked another question, waited and then got a response from the bot…”The responses from this indicate that even if a computer is a human that its user may not necessarily be fully satisfied with its performance. The response implies that each user would like the machine to accommodate his or her needs in order to cause less personality and cognitive friction. With several participant comments incorporating response time, it also indicates people expect machines to have consistent response times. Humans clearly vary in speed when listening, thinking, and responding, but it is expected of machines to act in a rhythmic fashion. It also suggests that there is an expectation that a machine will answer all questions asked and will not ask its users more questions than perceived necessary.(A) participants were asked if they felt (B)’s Artificial Intelligence could improve their relationship to computers if integrated in their daily products. 57.1% of participants responded affirmatively that they felt this could improve their relationship:“Well- I think I prefer talking to a person better. But yes for ipod, smart phones, etc. would be very handy for everyday use products”“Yes. Especially iphone is always with me. So it can track my daily behaviors. That makes the algorithm smarter”“Possibly, I should have queries it for information that would have been more relevant to me”“Absolutely!”“Yes”The 42.9% which responded negatively had doubts that it would be necessary or desirable:“Not sure, it might creep me out if it were.”“I like Siri as much as the next gal, but honestly we’re approaching the uncanny valley now.”“Its not clear to me why this type of relationship needs to improve, i think human relationships still need a lot of work.”“Nope, I still prefer flesh sacks.“No”The findings of the paper are relevant to the future of Affective Computation: whether a super computer with a human-like IQ and EQ can improve the human-to-computer interaction. The uncertainty of computational equivalency that Turing brought forth is indeed an interesting starting point to understand what we want out of the future of computers.The responses from the experiment affirm gender perceptions of machines and show how we display ourselves to machines. It seems that we limit our intelligence, limit our emotions, and obscure our identities when communicating to a machine. This leads us to question if we would want to give our true self to a computer if it doesn’t have a self of its own. It also could indicate that people censor themselves for machines because they lack a similarity that bonds humans to humans or that there’s a stigma associated with placing information in a digital device. The inverse relationship is also shown through the data that people perceive a bots IQ, EQ, and discussion ability to be high. Even though the chat-bot was indeed a human this data can imply humans perceive bots to not have restrictions and to be competent at certain procedures.The results also imply that humans aren’t really sure what they want out of Artificial Intelligence in the future and that we are not certain that an Affective computer would even enjoy a users company and/or conversation. The results also state that we currently think of computers as a very personal device that should be passive (not active), but reactive when interacted with. It suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine than it takes from us.A major limitation of this experiment is the sample size and sample diversity. The sample size of twenty-eight students is too small to fully understand and gather a stable result set. It was also only conducted with NYU: Interactive Telecommunications Students who all have extensive experience with computers and technology. To get a more accurate assessment of emotions a more diverse sample range needs to be taken.Five minutes is a short amount of time to create an emotional connection or friendship. To stay true to the Turing tests limitations this was enforced, but further relational understanding could be understood if more time was granted.Beside the visual interface of the chat window it would be important to show the emotions of participant (B) through a virtual avatar. Not having this visual feedback could have limited emotional resonance with participants (A).Time is also a limitation. People aren’t used to speaking to inquisitive machines yet and even through a familiar interface (a chat-room) many participants haven’t held conversations with machines previously. Perhaps if chat-bots become more active conversational participants’ in commercial applications users will feel less censored to give themselves to the conversation.In addition to the refinements noted in the limitations described above, there are several other experiments for possible future studies. For example, investigating a long-term human-to-bot relationship. This would provide a better understanding toward the emotions a human can share with a machine and how a machine can reciprocate these emotions. It would also better allow computer scientists to understand what really creates a significant relationship when physical limitations are present.Future studies should attempt to push these results further by understanding how a larger sample reacts to a computer algorithm with higher intellectual and emotional understanding. It should also attempt to understand the boundaries of emotional computing and what is ideal for the user and what is ideal for the machine without compromising either parties capacities.This paper demonstrates the diverse range of emotions that people can feel for affective computation and indicates that we are not in a time where computational equivalency is fully desired or accepted. Positive reactions indicate that there is optimism for more adept artificial intelligence and that there is interest in the field for commercial use. It also provides insight that humans limit themselves when communicating with machines and that inversely machines don’t limit themselves when communicating with humans.Books & ArticlesBowden M., 2006, Minds as Machine: A History of Cognitive Science, Oxford University PressChristian B., 2011, The Most Human HumanMarvin M., 2006. The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind, Simon & Schuster PaperbacksNass C., Brave S., 2005. Wired For Speech: How Voice Activates and Advances the Human-Computer Relationship, MIT PressNass C., Brave S., 2005, Hutchinson K., Computers that care: Investigating the effects of orientation of emotion exhibited by an embodied computer agent, Human-Computer Studies, 161- 178, ElsevierPicard, R., 1997. Affective Computing, MIT PressSearle J., 1980, Minds, Brains, and Programs, Cambridge University Press, 417–457Turing, A., 1950, Computing Machinery and Intelligence, Mind, Stor, 59, 433–460Wilson R., Keil F., 2001, The MIT Encyclopedia of the Cognitive Sciences, MIT PressWeizenbaum J., 1966, ELIZA — A Computer Program For the Study of Natural Language Communication Between Man and Machine, Communications of the ACM, 36–45Websites Cherry K., What is Emotional Intelligence?, http://psychology.about.com/od/personalitydevelopment/a/emotionalintell.htmEpstein R., 2006, Clever Bots, Radio Lab, http://www.radiolab.org/2011/may/31/clever-bots/ IBM, 1977, Deep Blue, IBM, http://www.research.ibm.com/deepblue/ IBM, 2011, Watson, IBM, http://www-03.ibm.com/innovation/us/watson/index.htmlLeavitt D., 2011, I Took the Turing Test, New York Times, http://www.nytimes.com/2011/03/20/books/review/book-review-the-most-human-human-by-brian- christian.htmlPersonal Robotics Group, 2008, Nexi, MIT. http://robotic.media.mit.edu/ Robinson P., The Emotional Computer, Camrbidge Ideas,http://www.cam.ac.uk/research/news/the-emotional-computer/US Census Bereau, 2009, Households with a Computer and Internet Use: 1984 to 2009. http://www.census.gov/hhes/computer/1960’s, Eliza, MIT, http://www.manifestation.com/neurotoys/eliza.php3",26/04/2012,0,28.0,3.0,0.0,0.0,0.0,0.0,0.0,9.0,en
3935,Computational creativity: generative creature design for concept art,Medium,Kyle Huang,34.0,11.0,2088.0,"AbstractWith the ever-powerful deep learning algorithm, computer graphics have been pushed to a new level. The generative adversarial network (GAN) can now generate almost any type of photo-realistic images with the proper size of datasets. However, most of the GAN use cases have been limited to the pursue of lifelike graphics. In this article, I prose a new framework “MonsterGAN,” combining machine learning, design, and psychology. MonsterGAN is a prototype of a generative design system (DRCI), which reduces the cognitive burden of creation and makes creativity scalable, for concept artists.What happens if computer vision passes the Turing test? Where and how can we use it? As a designer, I’m fascinated by these questions because we designers are the graphic wizards who deal with creativity and graphics daily. One bold idea comes to my mind: can we have machine conceptualized creativity?In 1951, Fitts invented the famous concept of function allocation and the MABA-MABA list. (Men-Are-Better-At/Machines-Are-Better-At lists) It explored the possibilities of how man and machine can work together as a team. In terms of the SRK taxonomy (skills-, rules-, and knowledge-based tasks), computers’ capability is limited to skills-based tasks and rules-based knowledge. With deep learning, I believe that the level of automation has changed, and machines can do specific knowledge-based jobs, which makes it necessary to rethink the notion of function allocation. For the full literature review click here.I propose a new MABA-MABA list for modern challenges that are mostly knowledge-based. We can have machines do the first 70% of a job and have humans pick up the last 30%. Machines are good at solving well-defined problems with their strengths: speed, precision, variation, scaling, and sense; On the other hand, humans are good at jobs that are not well-defined with their strengths: design, empathy, and generalization. With teamwork, we can change the order of process, such as man-machine-man, machine-man-machine, and machine-machine-man.Methodology: New Creative-thinking workflowIn the early stage of creative thinking, the target is usually uncertain. This makes it impossible for machines to implement creative thinking. But what if we can dismantle each step of creative thinking and allocate the tasks according to the new MABA-MABA list? As we know, there are four stages in the thinking process:PreparationThe preparation step consists of observing, listening, asking, reading, collecting, comparing, contrasting, analyzing, and relating all kinds of objects and information.IncubationThe incubation process is both conscious and unconscious. This step involves thinking about parts and relationships, reasoning, and often a fallow period.IlluminationInspiration very often appears during this fallow period [of incubation]. This probably accounts for the emphasis on releasing tension in order to be creative.VerificationThe step labeled verification is a period of hard work. This is the process of converting an idea into an object or into an articulated form.I think that there is an opportunity to have machines that assist humans in the first two steps of the creative thinking process: preparation and incubation. As we know, uncertainty in a creative project usually stops people from delivering results on time because there are too many possibilities, and people tend to change their minds at the last second. What if we can build a generative design system for problem-solving processes of abduction and induction? This can help us decrease the time we spend on preparation and incubation; therefore, it “accelerates” the “Aha” moment.Machine Divergence and human ConvergenceIn a traditional design thinking process, people repeat the process of divergence and convergence until they come up with a solution. It is how people narrow down the direction and iterate the practice. However, the problem with repetitive creative labor is that humans burnout, limiting the possibility of scaling up creativity. With the new MABA-MABA list, we can have machines diverge, and humans converge. If we can somehow encode ideas into numbers of vectors (this is exactly what deep learning is good at), it is reasonable to have machines diverge because computers can operate vectors easily; and this can also decrease the cognitive workload, helping humans work faster.The importance of ambiguityWe know that Generative Adversarial Networks are really difficult to train. Both the quality and quantity of the data need to be high. However, in reality, creative graphics data is usually not sufficient. This creates a dilemma because the output of the GAN flattens out at an unacceptable level of quality. Fortunately, we can bypass the problem by asking a GAN model to generate ambiguous images. Therefore, we do not expect a model that generates “photo-realistic” results. Instead, a “good enough” model will be sufficient for humans to pick up.Why do we want ambiguous results? It turns out that ambiguity plays a vital role in creative thinking. (Tamara Carleton, William Cockayne, and Larry Leifer, 20, An Exploratory Study about the Role of Ambiguity During Complex Problem Solving)This resolves the problem of low-quality results with limited datasets because we need abstract images to get inspired. Also, symbolically, this idea matches the coarse-to-fine process in computer vision.StyleGAN 2: generate ambiguous sketchesI decided to train a concept art model that requires heavy creativity with StyleGAN2. As a result, I came up with the idea of asking a model to generate abstract graphics. By providing low-level sketches to concept artists, they worked with these images as a foundation, and saved a substantial amount of time. I believe that these abstract graphics can, in some way, generate an emergence phenomenon for concept art. In this project, I used the implementation from this paper Training Generative Adversarial Networks with Limited Data.The method used in the paper:The NVIDIA research team considered a pipeline of 18 transformations that were grouped into 6 categories: pixel blitting (x-flips, 90◦ rotations, integer translation), more general geometric transformations, color transforms, image-space filtering, additive noise, and cutout.During training, each image shown to the discriminator used a pre-defined set of transformations in a fixed order. The strength of augmentations was controlled by the scalar p ∈ [0, 1], so that each transformation was applied with probability p or skipped with probability 1 − p. We always used the same value of p for all transformations. The randomization was done separately for each augmentation and for each image in a minibatch. The generator was guided to produce only clean images as long as p remains below the practical safety limit.Experiment ResultsNow, back to ambiguity, we use Fréchet Inception Distance (FID) to measure how well the GAN works (the lower the score the more realistic to your predictions). However, in the case of MonsterGAN, a low FID score doesn’t always mean “better” results. It turns out that, although the lower FID model did provide more texture “details” of the creatures, it actually loses the diversity of shapes and forms. (result is shown below)MonserGAN: DesigningAfter we got a trained model, an artist can browse the forms library and choose the most suitable shape for their requirements. Instead of starting from scratch, which is most of the time-consuming part, artists can pick several images that they find interesting. Since the inputs of the GAN are noise vectors, this gives our infinite concepts.Let’s say we consider this stony monster with big claws matching our direction. We can then use the input vector of this image as the center of the starting point (of latent space). By lowering down the truncation-psi and the sampling distance, we can achieve the effect of detail variation under a similar shape.MonsterGAN: Latent space explorationIt could also be the case that we want to merge multiple directions. And this is where latent space manipulation jumps in. Traditionally, if we want to change the feature of an image, tweak the Z latent space. In terms of combining different shapes, the below images are some results of Z latent space manipulation.As can be seen, manipulating Z latent space is a rough approach to control features since mapping the Z to the features vector might entangle the distribution of features together. Even if we get an acceptable result, the art direction is just uncontrollable. Research related to this subject was discussed in the paper of Analyzing and Improving the Image Quality of StyleGAN. Thus, StyleGAN2 created 8 extra fully connected layers to encode W from Z.In the implementation of style mixing, there are 18 style layers. We apply style layers to our target input with a range of style layers from 1 to 18 layers. (fully connect layers) I did some experiment of extracting the creature’s feature from the W latent space, and here’s what I found:1. the reult of using only 1 layer is subtle2. mixing 3–5 layers works the best3. using all 18 layers may cause the result same as column styleYou can find more details in this articleMonsterGAN: Human RefinementAfter the artists are satisfied with the model’s result, concept artists can jump in and start working on the refinement. Now, here’s the beauty of ambiguity. Since every person perceives the same abstract sketch with different interpretations, it gives artists more flexibility to leverage their creativity. As can be seen, many “errors” were transformed into a new design.In this project, I collaborated with concept artist Steve Wu, a senior concept designer specialized in creature design and has more than 6 years of experience in the film industry. The works shown in this article are credited to him. Check out Steve’s amazing work.In this design, the abstract visual cues inspired Steve in different ways. First, we can see that the textures on the creature inspired Steve to create the teeth, hair on the head, wings, and extension of the abdomen. Furthermore, Steve decided to remove the block in the lower section of the legs.This is a good example of how ambiguity inspires artists. The left side of the hog was originally a meaningless graphics generated by the model. Surprisingly, Steve managed to turn it into a snout of the hog. Also, the shape of the hog’s head was influenced by the texture of the original image, which became the highlight of this design.Again, this was an interesting example of how an artist transformed the flaws of result into art. There were two fragments that were supposed to be removed. However, Steve changed it to two sparrows, standing on the creature’s horn.Related GAN-based tool for compositing: GauGANAfter finished the creature designs, the artists can start working on the background and merge the creature into the scene, providing a look and feel of the concept. Again, we can also develop models to assist humans in each step. (e.g. texture creation, lighting, color grading, scene creation) Since Nvidia has already built the GauGAN, a tool for generating scenery images, we will use it directly.Showcases of MonsterGANEvaluationIn a nutshell, I believe that the latent space of big data provides a higher dimension of creativity by creating a new medium for people to sculpture their imagination and experience. This is because we can use machine learning to extract information from the enormous datasets collected from mobile devices. In other words, Data Sculpturing (graph below) translates our indescribable subjects or creativity to a latent vector and re-create the output to amplify the creators’ creativity by vector arithmetic; The combination of machines diverging variations and humans converging solutions improves an artist’s productivity and make creativity scalable.This article is included in the AI Art Gallery NeurIPS Workshop on Machine Learning for Creativity and Design 2020BibliographyCarleton, Tamara & Cockayne, William & Leifer, Larry. (2008). An Exploratory Study about the Role of Ambiguity during Complex Problem Solving.. 8–13.Sio, Ut Na & Ormerod, Thomas. (2009). Does Incubation Enhance Problem Solving? A Meta-Analytic Review. Psychological bulletin. 135. 94–120. 10.1037/a0014212.Savic, Milos (2016) “Mathematical Problem-Solving via Wallas’ Four Stages of Creativity: Implications for the Undergraduate Classroom,” The Mathematics Enthusiast: Vol. 13: №3, Article 6.How To Solve It, by George Polya, 2nd ed., Princeton University Press, 1957, ISBN 0–691–08097–6.J. Rasmussen, “Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models,” in IEEE Transactions on Systems, Man, and Cybernetics, vol. SMC-13, no. 3, pp. 257–266, May-June 1983, doi: 10.1109/TSMC.1983.6313160.Tero Karras, Samuli Laine, Timo Aila. (2019). A Style-Based Generator Architecture for Generative Adversarial NetworksKarras, Tero & Aittala, Miika & Hellsten, Janne & Laine, Samuli & Lehtinen, Jaakko & Aila, Timo. (2020). Training Generative Adversarial Networks with Limited Data.Cummings, Mary. (2017). Informing Autonomous System Design Through the Lens of Skill-, Rule-, and Knowledge-Based Behaviors. Journal of Cognitive Engineering and Decision Making. 12. 155534341773646. 10.1177/1555343417736461.R. Parasuraman, T. B. Sheridan and C. D. Wickens, “A model for types and levels of human interaction with automation,” in IEEE Transactions on Systems, Man, and Cybernetics — Part A: Systems and Humans, vol. 30, no. 3, pp. 286–297, May 2000, doi: 10.1109/3468.844354.",09/10/2020,0,18.0,10.0,1280.0,655.0,38.0,0.0,0.0,32.0,en
3936,Learning Artistic Styles from Images,Nurture.AI,James Lee,333.0,7.0,1286.0,"It’s currently an arms race in the tech scene right now with Deep Learning and Artificial Intelligence already the next industry-grade buzzword. Everyone’s looking to make the next big commercial success with a successful and innovative application of Artificial Intelligence.One such breakthrough is the use of deep learning neural networks to mathematically separate the content and style of images. What naturally entails is the idea of taking the content of one image and the style of another, and merging them both into one image. This idea was successfully implemented in 2015 by Gatys. et al in their paper “A Neural Algorithm of Artistic Style”.Since then, there have been many insights and improvements of the base idea. Modern iterations of the algorithm are now known as neural style transfer and have progressed much since its inception in 2015. You can read more about the improvements on this paper here.The core idea behind transferring styles is to take two images, say, a photo of a person, and a painting, and synthesize an image that simultaneously matches the semantic content representation of the photograph and the style representation of the respective piece of art.As the statement above suggests — we’re going to have mathematically quantify both the style and content representations of images. Using a blank or randomly generated image (a.k.a. a pastiche), we progressively match it with the desired style and content representation.So how exactly does Deep Learning come into play here? What does it do to be able to separate style and content from images — something even people sometimes struggle to discern.Evidently, Neural Networks used for Deep Learning are really good at coming up with high and low level representations of the features of an image. To visualize it — let’s take a look at the image of the CNN.Investigating the feature representations at each layer of the network, you’ll see that each layer progressively produces an abstract concept of either the style (top row of images) and semantic content (the bottom row of images).In a Convolutional Neural Network (CNN), each layer serves to further abstract the pixel representations of the image we feed it with. Initially, we’ll feed a CNN with an image. But the CNN doesn’t see this image as an image the way we humans do, instead it looks at an image as a matrix of values (more accurately a tensor).At each layer, a kernel is applied to a patch of the image, moving across the entire image eventually generating an intermediary vector representation of the image. While these generated vectors might not actually mean anything, they allow us to capture the characteristics of the image. You can check out this post for an in-depth visualizing of how a CNN does this.Naturally, these images are by no means a definitive definition of what style and content truly are, but this is how the neural network perceives it to be so.Now we’ve got a rough idea of where the abstract characteristics of an image can be found (the vector representations in-between layers). But how do we actually get them out of the CNN?This is where the key contributions of the paper we spoke of earlier come into play.The paper’s work was based on a popular and powerful architecture (at the time) dubbed VGG which won the 2014 ImageNet Classification challenge.And in this architecture, the intermediary vectors specifically at layers “conv4_2” best represent the semantic content of the image. While style is best represented by a combination of features from the following layers: “conv1_1”, “conv2_1”, “conv3_1”, “conv4_1” and “conv5_1”. How did we come to that specific selection of layers, you ask? It’s really just trial and error.The idea is that the further down you get in the neural network (and the closer towards classifying objects), the more the feature vector represents the image’s semantic content. Whereas layers higher in the network are able to better capture the image’s style.Now that we have the style and content representations figured out, we need a way to iteratively match a randomly generated white noise image (our pastiche) with the representations.To visualize the image information that is encoded at different layers of the hierarchy we perform gradient descent on a white noise image to find another image that matches the feature responses of the original image.Hence we define a squared-error loss between the content representation and our pastiche:Where vector p is the original image, vector x the image that is generated and P_l and F_l their respective feature representations in layer l.From which the gradient w.r.t. to the generated image can be computed using standard error back-propagation. Therefore we can progressively update the random white noise image until it gives the same response in the “conv4_2” layer as the image we want to get semantic content from.Style however isn’t as straight-forward. We first build a style representationthat computes the correlations between the different filter responses. This is done with the Gram Matrix:where G^l_ij is the inner product between the vectorized features i and j in layer l. More information on why a Gram Matrix captures the style information can be found on this paper.By minimizing the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated, we can use gradient descent from a white noise image to find another image that matches the style representation of the original image.The loss for style is defined as:Where E_l is:And G and A are the Gram Matrix representations for style of the original image and the generated image respectively in layer l. The gradients can then be computed using standard error back-propagation similar to the content representation above.We now have all the ingredients we need to generate an image, given an image we want to learn the style from and an image we want to learn the content from.To generate the images that mix the content of a photograph with the style of a painting we jointly minimize the distance of a white noise image from the content representation of the photograph in one layer of the network and the style representation of the painting in a number of layers of the CNN.We minimize the following loss function:Where alpha and beta are the weights used to determine the overall contributions during the construction of the new image.Even though this article was written in 2018, the technology described is not new, and has been around for a number of years already.Indeed, there are already a huge number of improvements and modifications done to the existing model that hugely ramps up many of the aspects of it’s performance, such as increasing the speed of which style transfer is done, reducing the loss to generate better images, making art (this was made by the original authors of the paper) and many more.What interests me the most is the fact that something I once perceived to be abstract and un-quantifiable — that is the abstract style and content of an image — can now be represented mathematically. Naturally, this makes one wonder if the same thing can be done for other abstract qualities. Not just for images but all forms of mediums, be they videos or text.What if abstract concepts like emotions, motive and plot can be quantified too? What then will we do with these? It’s exciting times we live in I can’t wait to see what more technology and AI can bring to our culture.Find this useful? Feel free to smash that clap and check out my other works. 😄James Lee is an AI Research Fellow at Nurture.AI. A recent graduate from Monash University in Computer Science, he writes about on Artificial Intelligence and Deep Learning. Follow him on Twitter @jamsawamsa.",02/02/2018,0,5.0,19.0,587.0,335.0,12.0,0.0,0.0,14.0,en
3937,A Friendly Introduction to Text Clustering,Towards Data Science,Korbinian Koch,56.0,15.0,2988.0,"The topics covered in this article include k-means, brown clustering, tf-idf, topic models and latent Dirichlet allocation (also known as LDA).Clustering is one of the biggest topics in data science, so big that you will easily find tons of books discussing every last bit of it. The subtopic of text clustering is no exception. This article can therefore not deliver an exhaustive overview, but it covers the main aspects. This being said, let us start by getting on common ground what clustering is and what it isn’t.You just scrolled by clusters!In fact, clusters are nothing more than groups that contain similar objects. Clustering is the process used for separating the objects into these groups.Objects inside of a cluster should be as similar as possible. Objects in different clusters should be as dissimilar as possible. But who defines what “similar” means? We’ll come back to that at a later point.Now, you may have heard of classification before. When classifying objects, you also put them into different groups, but there are a few important differences. Classifying means putting new, previously unseen objects into groups based on objects of which the group affiliation is already known, so called training data. This means we have something reliable to compare new objects to — when clustering, we start with a blank canvas: all objects are new! Because of that, we call classification a supervised method, clustering an unsupervised one.This also means that for classifying the correct number of groups is known, whereas in clustering there is no such number. Note that it is not just unknown — it simply does not exist. It is up to us to choose a suitable amount of clusters for our purpose. Many times, this means trying out a few and then choosing the one which delivered the best results.Before we dive right into concrete clustering algorithms, let us first establish some ways in which we can describe and distinguish them. There are a few ways in which this is possible:In hard clustering, every object belongs to exactly one cluster. In soft clustering, an object can belong to one or more clusters. The membership can be partial, meaning the objects may belong to certain clusters more than to others.In hierarchical clustering, clusters are iteratively combined in a hierarchical manner, finally ending up in one root (or super-cluster, if you will). You can also look at a hierarchical clustering as a binary tree. All clustering methods not following this principle can simply be described as flat clustering, but are sometimes also called non-hierarchical or partitional. You can always convert a hierarchical clustering into a flat one by “cutting” the tree horizontally on a level of your choice.Hierarchical methods can be further divided into two subcategories. Agglomerative (“bottom up”) methods start by putting each object into its own cluster and then keep unifying them. Divisive (“top down”) methods do the opposite: they start from the root and keep dividing it until only single objects are left.It should be clear how the clustering process looks like, right? You take some data, apply the clustering algorithm of your choice and ta-da, you are done! While this might theoretically be possible, it is usually not the case. Especially when working with text, there are several steps you have to take prior to and after clustering. In reality the process of clustering text is often messy and marked by many unsuccessful trials. However, if you tried to draw it in an idealized, linear manner, it might look like this:Quite a few extra steps, right? Don’t worry — you would probably have intuitively done it right anyway. However, it is helpful to consider each step on its own and keep in mind that alternative options for solving the problem might exist.Congratulations! You have made it past the introduction. In the next few paragraphs, we will look at clustering methods for words. Let’s look at the following set of them:To us it immediately becomes apparent which words belong together. There should obviously be one cluster with animals containing the words Aardvark and Zebra and one with adverbs containing on and under. But is it equally obvious for a computer?When talking about words with similar meaning, you often read about the distributional hypothesis in linguistics. This hypothesis states that words bearing a similar meaning will appear between similar word contexts. You could say “The box is on the shelf.”, but also “The box is under the shelf.” and still produce a meaningful sentence. On and under are interchangeable up to a certain extent.This hypothesis is utilized when creating word embeddings. Word embeddings map each word of a vocabulary onto a n-dimensional vector space. Words that have similar contexts will appear roughly in the same area of the vector space. One of these embeddings was developed by Weston, Ratle & Collobert in 2008. You can see an interesting segment of the word vectors (reduced to two dimensions with t-SNE) here:Notice how neatly months, names and locations are grouped together. This will come in handy for clustering them in the next step. To learn more about how exactly word embeddings are created and the interesting properties they have, take a look at this Medium article by Hunter Heidenreich. It also includes information about more advanced word embeddings like word2vec.We will now look at the most famous vector-based clustering algorithm out there: k-means. What k-means does is returning a cluster assignment to one of k possible clusters for each object. To recapitulate what we learned earlier it is a hard, flat clustering method. Let’s see how the k-means process looks like:K-means assigns k random points in the vector space as initial, virtual means of the k clusters. It then assigns each data point to the nearest cluster mean. Next, the actual mean of each cluster is recalculated. Based on the shift of the means the data points are reassigned. This process repeats itself until the means of the clusters stop moving around.To get a more intuitive and visual understanding of what k-means does, watch this short video by Josh Starmer.K-means it not the only vector based clustering method out there. Other often used methods include DBSCAN, a method favoring densely populated clusters and expectation maximization (EM), a method that assumes an underlying probabilistic distribution for each cluster.There are also methods for clustering words that do not require the words to already be available as vectors. Probably the most cited such technique is brown clustering, proposed in 1992 by Brown et al. (not related to the brown corpus, which is named after Brown University, Rhode Island).Brown clustering is a hierarchical clustering method. If cut at the right levels in the tree, it also results in beautiful, flat clusters such as the following:You can also look at small sub-trees and find clusters that contain word pairs close to synonymity such as evaluation and assessment or conversation and discussion.How is this achieved? Again, this method relies on the distributional hypothesis. It introduces a quality function describing how well the surrounding context words predict the occurrence of the words in the current cluster (so called mutual information). It then follows the following procedure:This is the reason, why evaluation and assessment are merged so early. Since both appear in extremely similar contexts, the quality function from above still delivers a very good value. The fact that we start out with single element clusters that are gradually unified means this method is agglomerative.Brown clustering is still used today! In this publication by Owoputi et al. (2013) brown clustering was used to find new clusters of words in online conversational language deserving their own part of speech tag. The results are entertaining, yet accurate:If you are interested in learning more about how brown clustering works, I can strongly recommend watching this lecture given by Michael Collins at Columbia University.While this paragraph concludes our section about clustering words, there are many more approaches out there not discussed in this article. One very promising and efficient way of clustering words is graph-based clustering, also called spectral clustering. Methods used include minimal spanning tree based clustering, Markov chain clustering and Chinese whispers.In general, clustering documents can also be done by looking at each document in vector format. But documents rarely have contexts. You could imagine a book standing next to other books in a tidy shelf, but usually this is not what large collections of digital documents (so-called corpora) look like.The fastest (and arguably most trivial) way to vectorize a document is to give each word in the dictionary its own vector dimension and then just count the occurrences for each word and each document. This way of looking at documents without considering the word order is called the bag of words approach. The Oxford English Dictionary contains over 300,000 main entries, not counting homographs. That’s a lot of dimensions, and most of them will probably get the value zero (or how often do you read the words lackadaisical, peristeronic and amatorculist?).Up to a certain extent you can counteract this by removing all word dimensions that are not used in your document collection (corpus), but you will still end up with lots of dimensions. And if you suddenly see a new document containing a word previously not used you will have to update every single document vector, adding this new dimension and the value zero for it. So, for the sake of simplicity, let’s assume our document collection does not grow.Look at the following toy example containing only two short documents d1 and d2 and the resulting bag of words vectors:What you can see is that words that are not very specific like I and love get rewarded with the same value as the words actually discerning the two documents like pizza and chocolates. A way to counteract this behavior is to use tf-idf, a numerical statistic used as a weighting factor dampening the effects of less important words.Tf-idf stands for term frequency and inverse document frequency, the two factors used for weighting. The term frequency is simply the number of occurrences of a word in a specific document. If our document is “I love chocolates and chocolates love me”, the term frequency of the word love would be two. This value is often normalized by dividing it by the highest term frequency in the given document, resulting in term frequency values between 0 (for words not appearing in the document) and 1 (for the most frequent word in the document). The term frequencies are calculated per word and document.The inverse document frequency, on the other hand, is only calculated per word. It indicates how frequently a word appears in the entire corpus. This value is inversed by taking the logarithm of it. Remember the ubiquitous word I we wanted to get rid of? Since the logarithm of one is zero, its influence is completely eliminated.Based on these formulas, we get the following values for our toy example:Looking at the last two columns, we see that only the most relevant words receive a high tf-idf value. So-called stop words, meaning words that are ubiquitous in our document collection, get a value of or close to 0.The received tf-idf vectors are still as high dimensional as the original bag of words vectors. Therefore, dimensionality reduction techniques such as latent semantic indexing (LSI) are often used to make them easier to handle. Algorithms such as k-means, DBSCAN and EM can be used on document vectors, too, just as described earlier for word clustering. Possible distance measures include euclidean and cosine distance.Often, just having clusters of documents is not enough.Topic modeling algorithms are statistical methods that analyze the words of the original texts to discover the themes that run through them, how those themes are connected to each other, and how they change over time (Blei, 2012).All topic models are based on the same basic assumption:Besides other topic models such as probabilistic latent semantic analysis (pLSA), latent Dirichlet allocation (LDA) is the best known and widest used one. Just by looking at its name, we can already find out a lot about how it works.Latent refers to hidden variables, a Dirichlet distribution is a probability distribution over other probability distributions and allocation means that some values are allocated based on the two. To better understand how these three aspects come to play, let’s look at what results LDA gives us. The following topics are an excerpt of 100 topics uncovered by fitting a LDA model to 17,000 articles from the journal Science.How should you interpret these topics? A topic is a probability distribution over words. Some words are more likely to appear in a topic, some less. What you see above is the 10 most frequent words per topic, excluding stop words. It is important to note that the topics don’t actually have the names Genetics or Evolution. These are just terms we humans would use to summarize what the topic is about. Try to look at the topics as word probability distribution 1 or word probability distribution 23 instead.But this is not all that LDA provides us with. Additionally, it tells us for each document which topics appear in it and to which percentage. For example, an article about a new device which can detect a specific disease prevalence in your DNA may consist of the topic mix 48% Disease, 31% Genetics and 21% Computers.To understand how we can make a computer know what good topics look like and how to find them, we will again construct a little toy example. Let’s pretend our corpus vocabulary consists only of a couple of emojis.Possible topics or word probability distributions on this vocabulary might look like this:As humans, we might identify these topics as Foods, Smileys and Animals. Let’s assume the topics are given. To understand the underlying assumptions LDA makes, let’s look at the generative process of documents. Even if they don’t seem very realistic, an author is assumed to take the following steps:Based on these steps, the following document may be the result:Note that even though a pizza emoji is much less likely to be drawn from topic 3, it is still possible to originate from it. Now that we have the result we want, we only have to find a way to reverse this process. Only? In reality, we are faced with this:In theory, we could make our computer try out every possible combination of words and topics. Besides the fact that this would probably take an eternity, how do we know in the end which combination makes sense and which one doesn’t? For this, the Dirichlet distribution comes in handy. Instead of drawing the distribution like in the image above, let’s draw our document on a topic simplex in the respective position.We can also draw a lot of other documents from our corpus next to this one. It could look like this:Or, if we had chosen other topics beforehand, like this:While in the first variant, the documents are clearly discernible and empathize different topics, the documents in the second variant are all more or less alike. The topics chosen here were not able to separate the documents in a meaningful way. These two possible document distributions are nothing else than two different Dirichlet distributions! This means we have found a way of describing what “good” distributions over topics look like!The same principle applies to words in topics. Good topics will have different distributions over words, while bad topics will have about the same words as others. These two appearances of Dirichlet distributions are described in the LDA model by two hyperparameters, alpha and beta. As a general rule, you want to keep your Dirichlet parameters below one. Take a look at how different values change the distribution in this brilliant animation made by David Lettier.Good word and topic distributions are usually approximated by using techniques such as collapsed Gibbs sampling or expectation propagation. Both methods iteratively improve randomly initialized word and topic distributions. However, a “perfect” allocation may never be found.If you want to try out what LDA does to a data set of your choice interactively, click your way through this great in-browser demo by David Mimno.Remember the clustering flowchart presented in the introduction? Especially the last step called final evaluation? When using clustering methods you should always keep in mind that even though a specific model may results in the least vector mean movements or the lowest probability distribution value, this doesn’t mean that it is “correct”. There are many data sets k-means cannot cluster properly and even LDA can produce topics that don’t make any sense to humans.In short: All models are wrong, but some are useful. Have fun clustering your texts and take care!Anastasiu, D. C., Tagarelli, A., Karypis, G. (2014). Document Clustering: The Next Frontier. In Aggarwal, C. C. & Reddy, C. K. (Eds.), Data Clustering, Algorithms and Applications (pp. 305–338). Minneapolis: Chapman & Hall.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan), 993–1022.Blei, D. M. (2012). Probabilistic topic models: Surveying a suite of algorithms that offer a solution to managing large document archives. Communications of the ACM, 55(4), 77–84.Brown, P. F., Pietra, V. J., Souza, P. V., Lai, J. C., & Mercer, R. L. (1992). Class-Based n-gram Models of Natural Language. Computational Linguistics, 18, 467–479.Feldman, R., & Sanger, J. (2007). Clustering, The text mining handbook. Advanced approaches in analyzing unstructured data. Cambridge: Cambridge University Press.Le, Q. & Mikolov, T. (2014). Distributed Representations of Sentences and Documents. Proceedings of the 31st International Conference on Machine Learning, in PMLR 32(2), 1188–1196.Manning, C., & Schütze, H. (1999). Clustering, Foundations of statistical natural language processing. Cambridge, Mass.: MIT Press.Owoputi, O., O’Connor, B., Dyer, C., Gimpel, K., Schneider, N., & Smith, N. A. (2013). Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters. Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9–14, 2013, Atlanta, Georgia, USA (pp. 380–390).",26/03/2020,0,36.0,48.0,1280.0,510.0,25.0,4.0,0.0,29.0,en
3938,Normalization vs Standardization — Quantitative analysis,Towards Data Science,Shay Geller,272.0,13.0,1249.0,"Every ML practitioner knows that feature scaling is an important issue (read more here).The two most discussed scaling methods are Normalization and Standardization. Normalization typically means rescales the values into a range of [0,1]. Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).In this blog, I conducted a few experiments and hope to answer questions like:I’ll analyze the empirical results of applying different scaling methods on features in multiple experiments settings.First, I was trying to understand what is the difference between Normalization and Standardization.So, I encountered this excellent blog by Sebastian Raschka that supplies a mathematical background that satisfied my curiosity. Please take 5 minutes to read this blog if you are not familiar with Normalization or Standardization concepts.There is also a great explanation of the need for scaling features when dealing with classifiers that trained using gradient descendent methods( like neural networks) by famous Hinton here.Ok, we grabbed some math, that’s it? Not quite. When I checked the popular ML library Sklearn, I saw that there are lots of different scaling methods. There is a great visualization of the effect of different scalers on data with outliers. But they didn’t show how it affects classification tasks with different classifiers.I saw a lot of ML pipelines tutorials that use StandardScaler (usually called Z-score Standardization) or MinMaxScaler (usually called min-max Normalization) to scale features. Why does no one use other scaling techniques for classification? Is it possible that StandardScaler or MinMaxScaler are the best scaling methods? I didn’t see any explanation in the tutorials about why or when to use each one of them, so I thought I’d investigate the performance of these techniques by running some experiments. This is what this notebook is all aboutLike many Data Science projects, lets read some data and experiment with several out-of-the-box classifiers.Sonar dataset. It contains 208 rows and 60 feature columns. It’s a classification task to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.It’s a balanced dataset:All the features in this dataset are between 0 to 1, but it’s not ensured that 1 is the max value or 0 is the min value in each feature.I chose this dataset because, from one hand, it is small, so I can experiment pretty fast. On the other hand, it’s a hard problem and none of the classifiers achieve anything close to 100% accuracy, so we can compare meaningful results.We will experiment with more datasets in the last section.CodeAs a preprocessing step, I already calculated all the results (it takes some time). So we only load the results file and work with it.The code that produces the results can be found in my GitHub:https://github.com/shaygeller/Normalization_vs_Standardization.gitI pick some of the most popular classification models from Sklearn, denoted as:(MLP is Multi-Layer Perceptron, a neural network)The scalers I used are denoted as:*Do not confuse Normalizer, the last scaler in the list above with the min-max normalization technique I discussed before. The min-max normalization is the second in the list and named MinMaxScaler. The Normalizer class from Sklearn normalizes samples individually to unit norm. It is not column based but a row based normalization technique.Nice results. By looking at the CV_mean column, we can see that at the moment, MLP is leading. SVM has the worst performance.Standard deviation is pretty much the same, so we can judge mainly by the mean score. All the results below will be the mean score of 10-fold cross-validation random splits.Now, let’s see how different scaling methods change the scores for each classifierThe first row, the one without index name, is the algorithm without applying any scaling method.Best classifier from each model:1. SVM + StandardScaler : 0.8492. MLP + PowerTransformer-Yeo-Johnson : 0.8393. KNN + MinMaxScaler : 0.8134. LR + QuantileTransformer-Uniform : 0.8085. NB + PowerTransformer-Yeo-Johnson : 0.7526. LDA + PowerTransformer-Yeo-Johnson : 0.7477. CART + QuantileTransformer-Uniform : 0.748. RF + Normalizer : 0.723We know that some well-known ML methods like PCA can benefit from scaling (blog). Let’s try adding PCA(n_components=4) to the pipeline and analyze the results.We can conclude that even though PCA is a known component that benefits from scaling, no single scaling method always improved our results, and some of them even cause harm(RF-PCA with StandardScaler).The dataset is also a great factor here. To better understand the consequences of scaling methods on PCA, we should experiment with more diverse datasets (class imbalanced, different scales of features and datasets with numerical and categorical features). I’m doing this analysis in section 5.There are big differences in the accuracy score between different scaling methods for a given classifier. One can assume that when the hyperparameters are tuned, the difference between the scaling techniques will be minor and we can use StandardScaler or MinMaxScaler as used in many classification pipelines tutorials in the web. Let’s check that!First, NB is not here, that’s because NB has no parameters to tune.We can see that almost all the algorithms benefit from hyperparameter tuning compare to results from o previous step. An interesting exception is MLP that got worse results. It’s probably because neural networks can easily overfit the data (especially when the number of parameters is much bigger than the number of training samples), and we didn’t perform a careful early stopping to avoid it, nor applied any regularizations.Yet, even when the hyperparameters are tuned, there are still big differences between the results using different scaling methods. If we would compare different scaling techniques to the broadly used StandardScaler technique, we can gain up to 7% improvement in accuracy (KNN column) when experiencing with other techniques.The main conclusion from this step is that even though the hyperparameters are tuned, changing the scaling method can dramatically affect the results. So, we should consider the scaling method as a crucial hyperparameter of our model.Part 5 contains a more in-depth analysis of more diverse datasets. If you don’t want to deep dive into it, feel free to jump to the conclusion section.To get a better understanding and to derive more generalized conclusions, we should experiment with more datasets.We will apply Classifier+Scaling+PCA like section 3 on several datasets with different characteristics and analyze the results. All datasets were taken from Kaggel.LinkClassification task: Predict is it’s going to rain?Metric: AccuracyDataset shape: (56420, 18)Counts for each class:No 43993Yes 12427Here is a sample of 5 rows, we can’t show all the columns in one picture.We will suspect that scaling will improve classification results due to the different scales of the features (check min max values in the above table, it even get worse on some of the rest of the features).ResultsResults analysisLinkClassification task: Predict has the client subscribed a term deposit?Metric: AUC ( The data is imbalanced)Dataset shape: (41188, 11)Counts for each class:no 36548yes 4640Here is a sample of 5 rows, we can’t show all the columns in one picture.Again, features in different scales.ResultsResults analysisLinkClassification task: Predict if an object to be either a galaxy, star or quasar.Metric: Accuracy (multiclass)Dataset shape: (10000, 18)Counts for each class:GALAXY 4998STAR 4152QSO 850Here is a sample of 5 rows, we can’t show all the columns in one picture.Again, features in different scales.ResultsResults analysisLinkClassification task: Predict if income is >50K, <=50K.Metric: AUC (imbalanced dataset)Dataset shape: (32561, 7)Counts for each class: <=50K 24720 >50K 7841Here is a sample of 5 rows, we can’t show all the columns in one picture.Again, features in different scales.ResultsResults analysisIf you find some mistakes or have proposals to improve the coverage or the validity of the experiments, please notify me.",04/04/2019,10,49.0,2.0,964.0,324.0,21.0,11.0,0.0,14.0,en
3939,An Overview for Text Representations in NLP,Towards Data Science,jiawei hu,49.0,17.0,2952.0,"Writing is always a good choice when it comes to clarifying one’s understandings of a given topic. By putting thoughts on papers, ideas will be clarified and confusions exposed. Though it might not be the most comfortable thing to do it’s indeed an efficient way to learn and improve.If you ever find yourself having a hard time explaining something to a friend, something you’ve been studying for a while but somehow still didn’t manage to portray the subject clearly and intuitively, you should try writing it down.In this article, I attempt to summarize some of the ideas for text representations in NLP, aiming to build a foundation for future complex concepts to come and hoping to contribute my granito de arena to your learning as well.The above diagram summarizes the process of transforming a text corpus into different input formats for a Machine Learning model. Starting from the left, the Corpus goes through several steps before obtaining the Tokens, a set of text building blocks i.e. words, subwords, characters, etc. Since ML models are only capable of processing numerical values, the tokens in a sentence are replaced by their corresponding ids, by either looking them up in an associative array(vocabulary) or using a Hashing Trick. Once done, they are then transformed into different input formats shown on the right. Each one of these formats has its own pros and cons and should be chosen strategically according to the characteristics of the task at hand.We are going to explore each of these input representations shown above, starting directly from the tokens and ignoring the previous steps.One-hot encoding may be one’s go-to move when a categorical feature is encountered in a data set(if you think such a feature is useful for the model, obviously). It’s a simple and straight forward technique and it works by replacing each category with a vector full of zeros, except for the position of its corresponding index value, which has a value of 1.When applying one-hot encoding to a text document, tokens are replaced by their one-hot vectors and a given sentence is in turn transformed into a 2D-matrix with a shape of (n, m), with n being the number of token in the sentence and m the size of the vocabulary. Depending on the number of tokens a sentence has, its shape will be different.The size of the vocabulary would only grow as the training corpus gets larger and larger, as a result, each token would be represented by vectors with an increasingly larger length, making matrices more sparse. Instead of word-level representations, a more common approach is to use characters as tokens since it’ll limit the length of the vectors.But either using the word or character-level representations, it is unavoidable that different sentence matrices will have different shapes(different number of rows). This can be an issue for the majority of ML models since their input shapes are meant to be the same. RNN based models on the other hand, if set right, don’t share this concern due to its ‘recurrent’ nature, but all the instances within the same batch are still expected to share a uniformed shape.To go around this issue, one solution is to fix a length l for all instances — truncating longer ones while shorter ones are padded. For a task such as Sentiment Analysis for short texts(though using one-hot won’t yield good results), it’s typically enough to use say the first 300 characters. In Keras, the padding tokens can be masked so they won’t affect the loss.The size of the training corpus can get as big as it wants, making the vocabulary richer and richer, but there’s always a possibility of encountering unknown words during inference time. A way to handle this is to reserve some space in the vocabulary during initialization, so when a out-of-vocabulary word pops up, it can be assigned to one of the reserved spots(an oov bucket).Another way to featurize text is through n-gram count vectorizers, let’s take a look.The usage of one-hot encoding allows us to achieve a token-level representation of a sentence, it does so by replacing its tokens with vectors while maintaining their original sequential arrangement. A count vectorizer, on the other hand, is based on term frequencies and is capable of squeezing an entire sentence into a single vector. Each position of a count vector is assigned to a particular token as before and its value represents the number of appearances that token has in the sentence.Tokens are first generated from the corpus and vocabulary is built to map the tokens to their corresponding ids. Instead of building one vector for each token, a count vector simply counts how many times each token appeared in a sentence and places that number in their corresponding position in the vector.All sentences now are represented by vectors that share the same length l, which is defined by the number of unique tokens in the vocabulary by default but can also be selected manually. Such a way of representing sentences will fail to provide any token ordering information of the original sentence, and its associated context information is lost — only term frequencies are reflected in count vectors.Instead of using single words/characters as tokens, which could be seen as uni-gram count vectorizers, we can also use two or more consecutive words/characters to form tokens, obtaining what’s known as 2-gram, 3-gram, or more generally n-gram vectorizers.A use case of count vectors is, for example, Spam Email Detection with Naive Bayes Models.The main criteria for email filtering are the term frequencies in each type of emails, i.e. how many times a word appeared in spam and non-spam emails.One downside of using count vectors here is that the unknown words are thrown away. If you have a sentence that has m words and n of them the model has never seen before, then only m-n words will be taken into account. This is used by sneaky spammers for filter circumvention —modifying the spam keywords making them unknown to the model while still readable to the users. The content the model will see only contains neutral words and the email can be classified as non-spam.Feature Hashing, among its other advantages, can help alleviate the damage.As models are trained with corpus with larger and larger sizes, it produces vocabularies that take more and more memory space to store. For efficiency purposes, these lookup tables are stored in RAM for quick token-id mapping and can slow down the operations once their size gets too big.By using a Hashing Trick we can get rid of such memory-consuming vocabularies entirely and a hash function is used for token-id mapping instead. With a given string, a hash function is capable of returning a numerical value, a hash value, that’s unique to that string and use it as token id.Since there’s no fix-sized vocabulary involved, all tokens can now be assigned to a number, no matter the model has seen it before or not. In the case of Spam Detection, the way of circumventing the filter by making spam words strange to the model is no longer as effective. Any given word, ‘known’ or ‘unknown’, can be fed to the hash function and output a numeric value within the predefined range. Instead of throwing the unknown word away, its corresponding position in the count vector will add 1 so all the words in the email are now taken into account, not only the neutral ones.Unlike vocabularies, feature hashing is a one-way operation — we can’t find the initial feature using its hash value through the hash function. The same input would always produce the same output, but two distinct features may happen to be mapped to the same hash value(the odds are negligible if the vector size is large, i.e. 2²⁴).As mentioned earlier, count vectors are handicapped when representing sentence context since they don’t reflect any initial token ordering. Things can get worse when terms such as “like”, “a”, or “and” that carry very little meaningful context information appear too frequently, taking the model’s ‘attention’ away from those less frequent yet more interesting terms by shallowing their frequencies.Tf-idf stands for term-frequency(tf) times inverse document-frequency(idf) and it’s used to address this issue by re-weighting the count features according to how many different sentences each token has appeared in. It ‘penalizes’ the feature count of a term if it’s contained in more sentences, assuming an inverse relationship between a term’s relevance to the context and its number of appearances in different documents.As an example, let’s take look at Scikit-Learn’s tf-idf implementation, TfidfTransformer, in default settings:So far we’ve seen two types of representations: One-hot encoding, a token-level representation that allows the preservation of token ordering in the initial sentence, and Count Vectors, a more compact sentence-level representation that relies on term frequencies.For NLP tasks such as Text Generation or Classification, one-hot representation or count vectors might be capable enough to represent the required information for the model to make wise decisions. However, their usage won’t be as effective for other tasks such as Sentiment Analysis, Neural Machine Translation, and Question Answering where a deeper understanding of the context is required to achieve great results.Take One-hot encoding as an example, using it will not lead to a well-generalized model for these tasks because no comparison between any two given words can be done. All vectors are orthogonal to one another, the inner product of any two vectors is zero and their similarities cannot be measure by distance nor cosine-similarity.For this, we turn to Word Embeddings, a featurized word-level representation capable of capturing the semantic meanings of words.With embeddings, each word is represented by a dense vector of fixed size(generally range from 50 to 300), with values corresponding to a set of features i.e. Masculinity, Femininity, Age, etc. As shown in the figure below, these features are seen as different aspects of a word’s semantic meaning, and their values are obtained by random initialization and are updated during training, just like the parameters of the model do.When training embeddings, we do not tell the model what these features should be, instead it’s up to the model to decide what are the best ones for the learning task. When setting up an embedding matrix(a set of word embeddings), we only define its shape — the number of words and each vector’s length. What each feature is representing is generally difficult to interpret.Word Embedding’s ability to capture semantic meanings can be illustrated by projecting these high-dimensional vectors to a 2D space for visualization via t-SNE. If embeddings were obtained successfully, plotting these vectors with t-SNE would demonstrate how words that have similar meaning would end up being closer to one another.The semantic relationship between different embedding vectors can also be illustrated by the following example.Earlier we mentioned that the embedding vectors can be trained just like another layer(in an NN), they can also be trained separately and used later for different tasks via transfer learning.There are different ways to train embeddings but the principle remains more and less the same. Here we’ll briefly discuss the two Word2Vec methods, namely Continuous Bag of Words(CBOW) and Skip-gram.Word2Vec are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. We extract pairs of context/target words from a large corpus for training, with the target word being a randomly chosen word and the context words being those located within a given window around the target word.For CBOW, the context words are fed into the model in their embedding form(randomly initialized) and the model is expected to output the target word using a softmax, P(target|context). Skip-gram, on the other hand, does the opposite, it takes in the target word and is expected to output the context words.Now, these may sound like difficult learning tasks but keep in mind that the goal isn’t to do well on the tasks per se but to learn good embeddings, and these models can do just that.Polysemy stands for words that written the same but according to the context they can have completely different meanings. Word Embeddings as explained above will fail to handle this.To address the problem, each work embedding must take into account the context in which the word is found and modify its values accordingly. Carrying a generalized embedding matrix around and plug it into the model for the task we’re trying to do is not sufficient, instead, a more complex structure must be included to the lower part of the model to find contextualized embeddings.The encoder-decoder based structure for Neural Machine Translation(NMT) is an excellent example to introduce the pre-training + fine-tuning methodology that’s responsible for the latest breakthroughs in NLP in recent years.This diagram from Learning in Translation: Contextualized Word Vectors(CoVe) describes the process of obtaining contextualized embeddings in pre-training and how they are then used for downstream tasks.On the left, an encoder-decoder based model is trained for NMT, sentences from the original language are first processed y the encoder, and its outputs are then passed to the decoder for the final translation. This pre-training process is supervised, and the goal is for the encoder to learn how to capture syntactic and semantic meanings of words, and output contextualized embeddings. The encoder is based on a two-layer bidirectional LSTM architecture, while the decoder is based on attentional unidirectional LSTMs.On the right, a pre-trained encoder takes in the inputs, GloVe embeddings, to find contextualized embeddings, they are then combined with the original inputs for a downstream task.The limitation of CoVe is that 1) the pre-training is supervised so its bounded by the amount of labeled data, and 2) the architecture of the task-specific model remains to be defined and it’s not a trivial task to find one that can achieve outstanding results. By overcoming these two obstacles, we expect to find a model that can be 1)pre-trained with unlimited data — unsupervised training, and 2) fine-tuned on limited labeled data and together with some minor modifications on its architecture to achieve SOTA results for different NLP task.A brief description of how ELMo, OpenAL GPT, and BERT tackled the above-mentioned limitations is introduced below. For a more detailed explanation check out this amazing post: Generalized Language Models by Lilian Weng.Embeddings for Language Model(ELMo) obtains contextualized embedding by training a language model in an unsupervised fashion — taking in a sequence of tokens and learns to predict the next one given the history. Its structure is based on bidirectional LSTMs, where L layers of biLSTMs are stacked one on another, with each one outputting a different representation of the sequence.Different layers are focused on different aspects of the syntactic/semantic meanings of the word. The output of ELMo combines all these hidden states linearly, including the input embeddings, to achieve better results. The higher layers focus more on the semantic meanings while the lower ones can capture better syntactic aspects.Bidirectional LSTMs are used ensures the model is not only learning to predict a given token’s future but also its past.ELMo is only used to find contextualized embeddings. For a given task, a specific model architecture still needs to be found.OpenAI GPT, based on Transformer’s decoder, can be used directly for all end tasks. As shown in the image, different tasks have different preprocessing steps but only slight modifications are to be made on GPT’s Transformer model to complete the task.Unlike ELMo, GPT is trained only to predict the future, but for a better understanding of the context of a given token, both the items from its left and right should be taken into account.BERT is based on Transformer’s encoder and is trained to predict the context from both left and right. It’s pre-training consists of two tasks:BERT uses WordPiece tokenization embeddings for their inputs. Instead of generating normal word tokens, it uses subword tokenization to better address rare and unknown words, as most of them can be reconstructed using subwords. The two input sentences have different Sentence Embeddings and a special character is used for their separation. Positional Embeddings are also used.When using BERT for a downstream task, just like GPT, only a few new parameters are to be added. Take text classification as an example, what we need to do is to take the “[CLS]” token’s embedding from the last layer and pass it to a softmax.It wasn’t a short reading if you made it from top to bottom, but if you didn’t, here’s a quick recap:Tree types of text representation were discussed in this piece, with the first two of them being sparse vectors: One-hot encoding, on one hand, is a type of token-level representation where a sentence before being used as an input is transformed into a matrix that has as many rows as its number of tokens. Count vectorizers, on the other hand, can take a sentence as a whole and squeeze it into one single vector. It relies on counting term frequencies, and it’s done at the expense of losing the information regarding sentence’s token ordering.The usage of a Hashing Trick can help address the problem of memory consumption of a large vocabulary, and it also mitigates the problem of filter circumvention in the case of Spam Email Detection. Tf-idf is used to re-weight term frequencies for count vectors so less frequent but context-revealing terms won’t be ‘ignored’.Word Embeddings, a more advanced technique that is used when a better capture of the semantic meanings of words is needed. A more complex structure is used to process the inputs to obtain Contextualized Embeddings, which in turn is used to address the issue of polysemy.A brief discussion of the latest NLP models is conducted by the end of the article. The aim was to provide some basic ideas, more efforts should be invested for further understandings of these models.",04/03/2020,10,471.0,58.0,1107.0,483.0,15.0,2.0,0.0,12.0,en
3940,Why is gradient descent robust to non-linearly separable data?,Medium,Vivek Yadav,2500.0,2.0,261.0,"Clarification: Gradient descent by itself is NOT robust to non-linearly separable data. However, when used with appropriate nonlinear activation functions it is.The reason is due to the kernel trick. In kernel trick, we apply a nonlinear transform on the data, so the resulting data set is linearly separable. This is illustrated below. Consider task of classifying blue and red points, they are not linearly separable. But what if we transform this data by adding a third variable (z = x²+y²), wecan draw a plane between blue and red points, and separate the two set of points. This is precisely what neural networks also do.Neural networks’ learning can be viewed as a 2 part process where they learn some nonlinear transform of the data, and how to separate data based on this transform. Consider a 1 layer NN, the output of the network (ignoring bias) is, Y=W phi(Vx) where phi is a nonlinear function. Now what neural networks are in essence doing is applying nonlinear transform on x, via phi (Vx) and then performing linear separation on the transformed data. So learning via gradient descent algorithm is a 2 part process. In first part, it is learning optimal kernel or function (via V), and in the second part it is separating the transformed data using linear methods. This is also illustrated in Andrej Karpathy’s website. Here is the link to a visual display of the model for seeing how neural networks apply kernel trick and then do separation. http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.htmlFigure below presents kernel trick applied via neural networks, from the website link above.",09/11/2016,0,0.0,0.0,1152.0,617.0,2.0,0.0,0.0,1.0,en
3941,K-means Clustering Algorithm: Know How It Works,Edureka,SYED JUNAID IQBAL,5.0,12.0,2052.0,"The majority of retail business holders find it hard to recognize customer needs. The reason why Data-driven companies such as Netflix, Walmart, Target, etc. are doing so well is that they have an army of Certified Data Analysts that grow their business by using the right tools to create personalized marketing strategies. We do understand that not all customers are alike and have the same taste. So, this leads to the challenge of marketing the right product to the right customer. An offer or product which might entice a particular customer segment may not be very helpful to other segments. So, you can apply the k-means clustering algorithm to segment your entire customer audience into groups with similar traits and preferences based on various metrics (such as their activities, likes and dislikes on social media, and their purchase history). Based on these customer segments identified, you can create personalized marketing strategies and bring more business to your organization. Before delving into k-means clustering directly, I will be covering the following topics to give you a basic understanding of clustering.Machine Learning is one of the most recent and exciting technologies. You probably use it dozen times a day without even knowing it. Machine Learning is a type of artificial intelligence that provides computers with the ability to learn without being explicitly programmed. It works on supervised and unsupervised learning models. Unlike the supervised learning model, the unsupervised model of Machine Learning has no predefined groups under which you can distribute your data. You can find these groupings through clustering. I will explain it further through the following examples.As you can see in this image, the data points are shown as blue dots. These data points do not have labels based on which you can differentiate them. You do not know anything about this data. So now the question is, can you find out any structure in this data? This problem can be solved using the clustering technique. Clustering will divide this entire dataset under different labels (here called clusters) with similar data points into one cluster as shown in the graph given below. It is used as a very powerful technique for exploratory descriptive analysis.Here, the clustering technique has partitioned the entire data points into two clusters. The data points within a cluster are similar to each other but different from other clusters. For example, you have the data on symptoms of patients. Now, you can find out the name of a particular disease based on these symptoms.Let’s understand clustering further with an example of google news.What google news does is that every day with hundreds and thousands of news coming up on the web, it groups them into cohesive news stories. Let’s see how?Once you go to news.google.com, you will see numerous news stories grouped as shown below.They are grouped into different news stories. Here, if you see the red highlighted area, you will get to know that various news URLs related to Trump and Modi are grouped under one section and rest in other sections. On clicking a different URL from the group, you will get a different story on the same topic. So, google news automatically clusters new stories about the same topic into pre-defined clusters.Another very fascinating application of clustering is in genomics. Genomics is the study of DNA. As you can see in the image, different colors like red, green, and grey depict the degree to which an individual does or does not has a specific gene. So, you can run a clustering algorithm on the DNA data of a group of people to create different clusters. This can give you very valuable insights into the health of particular genes.For example, people with Duffy-negative genotype tend to have higher resistance to malaria and are generally found in African regions. So, you can draw a relationship between the genotype, the native habitat and find out their response to particular diseases.So, basically clustering partitions the dataset with similarities into different groups which can act as a base for further analysis. The result will be that objects in one group will be similar to one another but different from objects in another group.Now, once you have understood what is clustering, let’s look at different ways to achieve these clusters.Exclusive Clustering:In exclusive clustering, an item belongs exclusively to one cluster, not several. In the image, you can see that data belonging to cluster 0 does not belong to cluster 1 or cluster 2. k-means clustering is a type of exclusive clustering.Overlapping Clustering: Here, an item can belong to multiple clusters with different degrees of association among each cluster. The fuzzy C-means algorithm is based on overlapping clustering.Hierarchical Clustering: In hierarchical clustering, the clusters are not formed in a single step rather it follows series of partitions to come up with final clusters. It looks like a tree as visible in the image.While implementing any algorithm, computational speed and efficiency become a very important parameter for end results. So, I have explained k-means clustering as it works really well with large datasets due to its more computational speed and its ease of use.k-means clustering is one of the simplest algorithms which uses an unsupervised learning method to solve known clustering issues. k-means clustering requires the following two inputs.Let’s say you have an unlabeled data set like the one shown below and you want to group this data into clusters.Now, the important question is how should you choose the optimum number of clusters? There are two possible ways for choosing the number of clusters.(i) Elbow Method: Here, you draw a curve between WSS (within the sum of squares) and the number of clusters. It is called the elbow method because the curve looks like a human arm and the elbow point gives us the optimum number of clusters. As you can see that after the elbow point, there is a very slow change in the value of WSS, so you should take the elbow point value as the final number of clusters.(ii) Purpose Based: You can run a k-means clustering algorithm to get different clusters based on a variety of purposes. You can partition the data on different metrics and see how well it performs for that particular case. Let’s take an example of marketing T-shirts of different sizes. You can partition the dataset into a different numbers of clusters depending upon the purpose that you want to meet. In the following example, I have taken two different criteria, price and comfort.Let’s see these two possibilities as shown in the image below.Now, once we have the value of k with us, let’s understand its execution.As you can see in the graph below, the three clusters are clearly visible but you might end up having different clusters depending upon your choice of cluster centroids.Below shown are some other possibilities of cluster partitioning based on the different choices of cluster centroids. You may end up having any of these groupings based on your requirements and the goal that you are trying to achieve.Now that you have understood the concepts of clustering, so let’s do some hands-on in R.k-means clustering case study: Movie clusteringLet’s say, you have a movie dataset with 28 different attributes ranging from director facebook likes, movie likes, actor likes, budget to gross and you want to find out movies with maximum popularity amongst the viewers. You can achieve this by k-means clustering and divide the entire data into different clusters and do further analysis based on popularity.For this, I have taken the movie dataset of 5000 values with 28 attributes.Step 1. First, I have loaded the dataset in RStudio.movie_metadata <- read_csv(“~/movie_metadata.csv”)View(movie_metadata)Step 2. As you can see that there are many NA values in this data, so I will clean the dataset and remove all the null values from it.movie <- data.matrix(movie_metadata)movie <- na.omit(movie)Step 3. In this example, I have taken the first 500 values from the data set for analysis.smple <- movie[sample(nrow(movie),500),]Step 4. Further, with the R code below, you can take two attributes budget and gross from the dataset to make clusters.smple_short <- smple[c(9,23)]smple_matrix <- data.matrix(smple_short)View(smple_matrix)Our dataset will look like below.Step 5. Now, let’s determine the number of clusters.wss <- (nrow(smple_matrix)-1)*sum(apply(smple_matrix,2,var))for (i in 2:15) wss[i]<-sum(kmeans(smple_matrix,centers=i)$withinss)plot(1:15, wss, type=”b”, xlab=”Number of Clusters”, ylab=”Within Sum of Squares”)It gives the elbow plot as follows.As you can see, there is a sudden drop in the value of WSS (within the sum of squares) as the number of clusters increases from 1 to 3. Therefore, the bend at k=3 gives stability in the value of WSS. We need to strike a balance between k and WSS. So, in this case, it comes at k=3.Step 6. Now, with this cleaned data, I will apply the inbuilt means function in R to form clusters.cl <- kmeans(smple_matrix,3,nstart=25)You can plot the graph and cluster centroid using the following command.plot(smple_matrix, col =(cl$cluster +1) , main=”k-means result with 2 clusters”, pch=1, cex=1, las=1)points(cl$centers, col = “black”, pch = 17, cex = 2)Step 7. Now, I will analyze how good is my cluster formation by using the command cl. It gives the following output.Within cluster sum of squares by cluster:[1] 3.113949e+17 2.044851e+17 2.966394e+17(between_SS / total_SS = 72.4 %)Here, total_SS is the sum of squared distances of each data point to the global sample mean whereas between_SS is the sum of squared distances of the cluster centroids to the global mean. Here, 72.4 % is a measure of the total variance in the data set. The goal of k-means is to maximize the between-group dispersion(between_SS). So, the higher the percentage value, the better is the model.Step 8. For a more in-depth look at the clusters, we can examine the coordinates of the cluster centroids using the cl$centers component, which is as follows for gross and budget (in million).gross budget1 91791300 622025502 223901969 1182894743 18428131 19360546As per the cluster centroids, we can infer that cluster 1 and cluster 2 have grosser than the budget. Hence, we can infer that cluster 1 and cluster 2 made the profit while cluster 3 was at a loss.Step 9. Further, we can also examine how the cluster assignment relates to individual characteristics like director_facebook_likes(column 5) and movie_facebook_likes(column 28). I have taken the following 20 sample values.Using aggregate function we can look at other parameters of the data and draw insight. As you can see below that cluster 3 has the least movie Facebook likes as well as the least director likes. This is expected because cluster 3 is already at a loss. Also, cluster 2 is doing a pretty good job by grabbing maximum likes and maximum gross.Organizations like Netflix are making use of clustering to target movie clusters with maximum popularity among the viewers. They are selling these movies and making a huge profit out of this.“We live and breathe the customer,” said Dave Hastings, Netflix’s director of product analytics. Currently, Netflix has 93.80 million worldwide streaming customers. They watch your every move very closely on the internet as to what movies you like, which director you prefer, and then apply clustering to group the movies based on popularity. Now, they recommend movies from the most popular cluster and enhance their business.I urge you to see this k-means clustering algorithm video tutorial that explains all that we have discussed in the blog. Go ahead, enjoy the video and tell me what you think. If you wish to check out more articles on the market’s most trending technologies like Python, DevOps, Ethical Hacking, then you can refer to Edureka’s official site.Do look out for other articles in this series that will explain the various other aspects of Data Science.1.Data Science Tutorial2.Math And Statistics For Data Science3.Linear Regression in R4.Machine Learning Algorithms5.Logistic Regression In R6.Classification Algorithms7.Random Forest In R8.Decision Tree in R9.Introduction To Machine Learning10.Naive Bayes in R11.Statistics and Probability12.How To Create A Perfect Decision Tree?13.Top 10 Myths Regarding Data Scientists Roles14.Top Data Science Projects15.Data Analyst vs Data Engineer vs Data Scientist16.Types Of Artificial Intelligence17.R vs Python18.Artificial Intelligence vs Machine Learning vs Deep Learning19.Machine Learning Projects20.Data Analyst Interview Questions And Answers21.Data Science And Machine Learning Tools For Non-Programmers22.Top 10 Machine Learning Frameworks23.Statistics for Machine Learning24.Random Forest In R25.Breadth-First Search Algorithm26.Linear Discriminant Analysis in R27.Prerequisites for Machine Learning28.Interactive WebApps using R Shiny29.Top 10 Books for Machine Learning30.Supervised Learning31.10 Best Books for Data Science32.Machine Learning using ROriginally published at https://www.edureka.co on February 10, 2017.",10/02/2017,0,23.0,68.0,459.0,259.0,25.0,7.0,0.0,34.0,en
3942,Meta Learning — AI Generalised.,Towards Data Science,Snehal Reddy Koukuntla,111.0,6.0,984.0,"Deep Learning has shown immense success in various fields and is continuing to spread its wings. But one of the major issues with training any traditional neural network model is the requirement of colossal amounts of data, and using this data to perform many iterative updates across many labeled examples.Let’s take a look at a classic example of cats vs dogs classification. Although over the last two decades, we have made our models better and better to increase the accuracy, but the fundamental problem mentioned above still persists. We still need loads of labelled dogs and cats to get a decent accuracy.How do humans classify them with much lesser examples. Lets say all of a sudden you are shown two new animals, which are as visually distinguishable as a cat and a dog. I am pretty sure any normal person can get a decent accuracy in less than 100 examples. HOW ?? Well, we have over the years understood the basic structure of animals. We know how to extract features, such as face shape, hair, tail, body structure etc.. In short, we have mastered the art of Learning to learn.Meta Learning aims to basically learn to learn, and generalise AI to many different scenarios with minimum amount of data. You could say, isn’t transfer learning doing the same. Well, yeah it has gone kind of in the right direction, but it cannot get us far enough. It has been observed that the benefit of a pre-trained network greatly decreases as the task the network was trained on diverges from the target task. Meta Learning suggests framing the learning problem at two levels. The first is quick acquisition of knowledge within each separate task presented. This process is guided by the second, which involves slower extraction of information learned across all the tasks.Meta Learning algorithms can be broadly classified into three buckets —The intuition behind this class of methods is to use standard gradient descent updates again for making the neural network, generalise to wide variety of datasets.In this method, we use a set of datasets each with few examples each, lets say k examples each, for “k-shot learning”. Let the set of datasets be p(T). Let our model be a parametrized function fₜₕₑₜₐ. If we start with parameters θ , we know the model updates with each for the individual dataset with standard gradient descent update.We want our model to generalise for a wide range of datasets. So we want the sum of errors for all the datasets in p(T) with the updated parameters. This can be expresses mathematically as follows —For a batch of datasets in p(T), we update θ w.r.t. the meta-objective mentioned above with a standard SGD update —We can see that back propagating the meta-loss through the model’s gradients involves computing derivatives of derivative. This can be done using Hessian-vector products, supported by TensorFlow.This set of methods are guided by the fact that nearest neighbors algorithm does not require any training but performance depends on the chosen metric.These consist of an embedding model that maps the input domain into a feature space and a base learner that maps the feature space to task variables. The meta-learning objective is to learn an embedding model such that the base learner generalizes well across tasks. Here a distance-based prediction rule over the embeddings. We will look at a specific example, Matching Networks to understand the working.Matching Networks support set of k examples of image-label pairs S={(xᵢ ,yᵢ)} to a classifier cₛ(x’) which, given a test example x’, defines a probability distribution over outputs y’. S → cₛ(x’) is defined as P(y’|x’,S), where P is parameterised by a neural network. Thus given a new support set of examples S′ from which to one-shot learn, we simply use the parametric neural network defined by P to make predictions about the appropriate label y’ for each test example x’ : P(y’|x’, S′). Simply, we can say —The above method is reminiscent of KDE and kNN algorithms.f and g are appropriate embedding neural networks for x cap and xᵢ, which depend on the tasks. They are deep CNNs for image tasks (like Inception) or a simple form word embedding for language tasks.We, humans along with processing stuff, we also store representations for future use. So these algorithms tried to mimic that by some some auxiliary memory blocks. The basic strategy is to learn the types of representations it should place into memory and how it should later use these representations for predictions.In these methods, input sequence and output labels are given sequentially. One dataset D ={dₜ}={(xₜ, yₜ)}, where t represents time step. Output label yₜ is not given right after xₜ. Input and ideal label for that input are jumbled for each dataset. It is expected of the model to output the appropriate label for xₜ(i.e., yₜ) at the given time step. So it is forced to store data samples in memory until appropriate labels are found, after which sample-class information can be bound and stored for later use.The memory module in this specific implementation we are going to talk about is is Neural Turing Machine(NTM). It is basically a turing machine(read and write heads on a memory block) with a LSTM(or sometimes simple neural networks) based controller. Memory encoding and retrieval in a NTM external memory module is rapid, with vector representations being placed into or taken out of memory potentially every time-step. This ability makes the NTM a perfect candidate for meta-learning and low-shot prediction, as it is capable of both long-term storage via slow up-dates of its weights, and short-term storage via its external memory module.At some time step t, given xₜ, LSTM controller gives out a key kₜ for access the memory matrix Mₜ.Softmax is used to produce read-write vectors.This is used to get the memory rₜ.This is used as input for next controller state as well as input to soft-max based classifiers.",08/06/2019,0,11.0,9.0,770.0,169.0,10.0,0.0,0.0,0.0,en
3943,"Gamma Function — Intuition, Derivation, and Examples",Towards Data Science,Aerin Kim,8200.0,8.0,848.0,"Why should I care?Many probability distributions are defined by using the gamma function — such as Gamma distribution, Beta distribution, Dirichlet distribution, Chi-squared distribution, and Student’s t-distribution, etc.For data scientists, machine learning engineers, researchers, the Gamma function is probably one of the most widely used functions because it is employed in many distributions. These distributions are then used for Bayesian inference, stochastic processes (such as queueing models), generative statistical models (such as Latent Dirichlet Allocation), and variational inference. Therefore, if you understand the Gamma function well, you will have a better understanding of a lot of applications in which it appears!Because we want to generalize the factorial!The factorial function is defined only for discrete points (for positive integers — black dots in the graph above), but we wanted to connect the black dots. We want to extend the factorial function to all complex numbers. The simple formula for the factorial, x! = 1 * 2 * … * x, cannot be used directly for fractional values because it is only valid when x is a whole number.So mathematicians had been searching for…“What kind of functions will connect these dots smoothly and give us factorials of all real values?”However, they couldn’t find *finite* combinations of sums, products, powers, exponential, or logarithms that could express x! for real numbers until…The formula above is used to find the value of the Gamma function for any real value of z.Let’s say you want to calculate Γ(4.8). How would you solve the integration above? Can you calculate Γ(4.8) by hand? Maybe using the integral by parts?Try it and let me know if you find an interesting way to do so! For me (and many others so far), there is no quick and easy way to evaluate the Gamma function of fractions manually. (If you are interested in solving it by hand, here is a good starting point.)Ok, then, forget about doing it analytically. Can you implement this integral from 0 to infinity — adding the term infinite times programmatically?You can implement this in a few ways. Two of the most often used implementations are Stirling’s approximation and Lanczos approximation.Let’s calculate Γ(4.8) using a calculator that is implemented already.We got 17.837.17.837 falls between 3!(= Γ(4) = 6) and 4!(= Γ(5) = 24) — as we expected.(When z is a natural number, Γ(z) =(z-1)! We are going to prove this shortly.)Unlike the factorial, which takes only the positive integers, we can input any real/complex number into z, including negative numbers. The Gamma function connects the black dots and draws the curve nicely.If you take a look at the Gamma function, you will notice two things.First, it is definitely an increasing function, with respect to z.Second, when z is a natural number, Γ(z+1) = z! (I promise we’re going to prove this soon!)Therefore, we can expect the Gamma function to connect the factorial.How did the Gamma function end up with current terms x^z and e^-x?I don’t know exactly what Euler’s thought process was, but he is the one who discovered the natural number e, so he must have experimented a lot with multiplying e with other functions to find the current form.As x goes to infinity ∞, the first term (x^z) also goes to infinity ∞, but the second term (e^-x) goes to zero.Then, will the Gamma function converge to finite values?We can rigorously show that it converges using L’Hôpital’s rule. But we can also see its convergence in an effortless way. If you think about it, we are integrating a product of x^z — a polynomially increasing function — and e^-x — an exponentially decreasing function. Because the value of e^-x decreases much more quickly than that of x^z, the Gamma function is pretty likely to converge and have finite values.Let’s plot each graph, since seeing is believing.Let’s look at the case of Γ(4.8).Python code is used to generate the beautiful plots above. Plot it yourself and see how z changes the shape of the Gamma function!The code in ipynb: https://github.com/aerinkim/TowardsDataScience/blob/master/Gamma%20Function.ipynbIf you take one thing away from this post, it should be this section.Let’s prove it using integration by parts and the definition of Gamma function.Beautifully proved!Let’s prove it using the property 1.What is Γ(1)?Therefore, Γ(n) = (n-1)!A quick recap about the Gamma “distribution” (not the Gamma “function”!): Gamma Distribution Intuition and Derivation.Here goes the proof:For the proof addicts: Let’s prove the red arrow above.We’ll use integration by substitution.Beautifully proved again!A few things to note:Pretty old. About 300 yrs. (Are you working on something today that will be used 300 years later?;)An interesting side note: Euler became blind at age 64 however he produced almost half of his total works after losing his sight.2. Some interesting values at points:Can you prove these?3. Here is a quick look at the graph of the Gamma function in real numbers.The Gamma function, Γ(z) in blue, plotted along with Γ(z) + sin(πz) in green. (Notice the intersection at positive integers because sin(πz) is zero!) Both are valid analytic continuations of the factorials to the non-integers.4. Gamma function also appears in the general formula for the volume of an n-sphere.",23/11/2019,7,75.0,1.0,1236.0,519.0,15.0,1.0,0.0,12.0,en
3944,A Beginner’s Guide to Convolutional Neural Networks (CNNs),Towards Data Science,Suhyun Kim,254.0,7.0,1132.0,"A convolution is how the input is modified by a filter. In convolutional networks, multiple filters are taken to slice through the image and map them one by one and learn different portions of an input image. Imagine a small filter sliding left to right across the image from top to bottom and that moving filter is looking for, say, a dark edge. Each time a match is found, it is mapped out onto an output image.For example, there is a picture of Eileen Collins and the matrix above the red arrow is used as a convolution to detect dark edges. As a result, we see an image where only dark edges are emphasized.Note that an image is 2 dimensional with width and height. If the image is colored, it is considered to have one more dimension for RGB color. For that reason, 2D convolutions are usually used for black and white images, while 3D convolutions are used for colored images.Let’s start with a (4 x 4) input image with no padding and we use a (3 x 3) convolution filter to get an output image.The first step is to multiply the yellow region in the input image with a filter. Each element is multiplied with an element in the corresponding location. Then you sum all the results, which is one output value.Mathematically, it’s (2 * 1) + (0 * 0) + (1 * 1) + (0 * 0) + (1 * 0) + (0 * 0) + (0 * 0) + (0 * 1) + (1 * 0) = 3Then, you repeat the same step by moving the filter by one column. And you get the second output.Notice that you moved the filter by only one column. The step size as the filter slides across the image is called a stride. Here, the stride is 1. The same operation is repeated to get the third output. A stride size greater than 1 will always downsize the image. If the size is 1, the size of the image will stay the same.At last, you are getting the final output.We see that the size of the output image is smaller than that of the input image. In fact, this is true in most cases.Convolution in 3D is just like 2D, except you are doing the 2d work 3 times, because there are 3 color channels.Normally, the width of the output gets smaller, just like the size of the output in 2D case.If you want to keep the output image at the same width and height without decreasing the filter size, you can add padding to the original image with zero’s and make a convolution slice through the image.We can apply more padding!Once you’re done, this is what the result would look like:As you add more filters, it increases the depth of the output image. If you have the depth of 4 for the output image, 4 filters were used. Each layer corresponds to one filter and learns one set of weights. It does not change between steps as it slides across the image.An output channel of the convolutions is called a feature map. It encodes the presence or absence, and degree of presence of the feature it detects. Notice that unlike the 2D filters from before, each filter connects to every input channel. (question? what does it mean by each filter connects to every input channel unlike 2D?) This means they can compute sophisticates features. Initially, by looking at R, G, B channels, but after, by looking at combinations of learned features such as various edges, shapes, textures and semantic features.Another interesting fact is CNNs are somewhat resistant to translation such as an image shifting a bit, which would have a similar activation map as the one before shifting. It’s because the convolution is a feature detector and if it’s detecting a dark edge and the image is moved to the bottom, then dark edges will not be detected until the convolution is moved down.1D convolution is covered here, because it’s usually under-explained, but it has noteworthy benefits.They are used to reduce the depth (number of channels). Width and height are unchanged in this case. If you want to reduce the horizontal dimensions, you would use pooling, increase the stride of the convolution, or don’t add paddings. The 1D convolutions computes a weighted sum of input channels or features, which allow selecting certain combinations of features that are useful downstream. 1D convolution compresses because there is only one It has a same effect ofNote that pooling is a separate step from convolution. Pooling is used to reduce the image size of width and height. Note that the depth is determined by the number of channels. As the name suggests, all it does is it picks the maximum value in a certain size of the window. Although it’s usually applied spatially to reduce the x, y dimensions of an image.Max pooling is used to reduce the image size by mapping the size of a given window into a single result by taking the maximum value of the elements in the window.It’s same as max-pooling except that it averages the windows instead of picking the maximum value.In order to implement CNNs, most successful architecture uses one or more stacks of convolution + pool layers with relu activation, followed by a flatten layer then one or two dense layers.As we move through the network, feature maps become smaller spatially, and increase in depth. Features become increasingly abstract and lose spatial information. For example, the network understands that the image contained an eye, but it is not sure where it was.Here’s an example of a typical CNN network in Keras.Here’s the result when you do model.summary()Let’s break those layers down and see how we get those parameter numbers.Filter size ( 3 x 3) * input depth (1) * # of filters (32) + Bias 1/filter (32) = 320. Here, the input depth is 1, because it’s for MNIST black and white data. Note that in tensorflow by default every convolution layer has bias added.Pooling layers don’t have parametersFilter size (3 x 3) * input depth (32) * # of filters (64) + Bias, 1 per filter (64) = 18496It unstacks the volume above it into an array.Input Dimension (128) * Output Dimension (10) + One bias per output neuron (10) = 1290Convolutional Neural Network (CNN) is a class of deep neural network (DNN) which is widely used for computer vision or NLP. During the training process, the network’s building blocks are repeatedly altered in order for the network to reach optimal performance and to classify images and objects as accurately as possible.This tutorial is based on lectures from the Applied Deep Learning course at Columbia University by Joshua Gordon. Awesome 3d Images are from Martin Gorner .",15/02/2019,0,7.0,0.0,1334.0,488.0,24.0,0.0,0.0,13.0,en
3945,Using Affinity Propagation to Find the Number of Clusters in a Dataset,Medium,Aneesha Bakharia,2300.0,3.0,388.0,"Clustering and dimension reduction algorithms help you to explore a dataset. Clustering and dimension reduction are unsupervised learning algorithms i.e., they don’t need labelled data to build a model. k-means is a popular clustering algorithm — you specify the the number of clusters (k) and it then finds the best cluster for each data instance. Choosing a good initial value for the number of clusters (k) can be problematic as k can be anything between 1 and the number of data instances. Finding the number of clusters is an active research field and techniques do exist (such as the Silhouette coefficient) but have varying success as the dimensionality of the data increases. I’m not going to go into any of these other techniques to find k in this blog post. Instead I’m going to focus on a fairly recent (2007) algorithm called Affinity Propagation which is able to cluster data and find the number of clusters simultaneously. The best thing is that Affinity Propagation is already included in a popular machine learning library for Python — Scikit Learn!The Affinity Propagation algorithm was published in 2007 by Brendan Frey and Delbert Dueck in Science. The algorithm exchanges messages between pairs of data points until a set of exemplars emerges, with each exemplar corresponding to a cluster. The Affinity Propagation algorithm takes as input a real number s(k,k) for each data point k — referred to as a “preference”. Data points with large values for s(k,k) are more likely to be exemplars. The number of clusters is influenced by the preference values and the message-passing procedure.Scikit Learn includes the Affinity Propagation algorithm and has demo code. I’ll include sample code and a plot of dummy data with the exemplars linked to data points within the cluster. Note that a preference value can be specified (in this case it is set to -50) but if it is not specified as an argument, it will be set to the medium of the input similarities.In conclusion, I’ve found Affinity Propagation to be a useful clustering algorithm that provides a viable alternative to k-means. I’ve used Affinity Propagation both as a standalone clustering algorithm and as a way to determine a good starting point for selecting the number of clusters (which can then be used in conjunction with other dimension reduction and clustering algorithms).",11/01/2016,1,18.0,7.0,667.0,519.0,1.0,0.0,0.0,13.0,en
3946,10 Tips for Choosing the Optimal Number of Clusters,Towards Data Science,Matt.0,348.0,15.0,1639.0,"Clustering is one of the most common unsupervised machine learning problems. Similarity between observations is defined using some inter-observation distance measures or correlation-based distance measures.There are 5 classes of clustering methods:+ Hierarchical Clustering+ Partitioning Methods (k-means, PAM, CLARA)+ Density-Based Clustering+ Model-based Clustering+ Fuzzy ClusteringMy desire to write this post came mainly from reading about the clustree package, the dendextend documentation, and the Practical Guide to Cluster Analysis in R book written by Alboukadel Kassambara author of the factoextra package.I will be using a lesser known data set from the cluster package: all.mammals.milk.1956, one which I haven’t looked at before.This small dataset contains a list of 25 mammals and the constituents of their milk (water, protein, fat, lactose, ash percentages) from John Hartigan, Clustering Algorithms, Wiley, 1975.First let’s load the required packages.Now load the data.Let’s explore and visualize the data.All the variables are expressed as numeric. What about the statistical distribution?What’s the relationship between the different attributes? Use `corrplot()` to create correlation matrix.When you have variables which are measured in different scales it is useful to scale the data.Dimensionality reduction can help with data visualization (e.g. PCA method).These are the 5 PCs that capture 80% of the variance. The scree plot shows that PC1 captured ~ 75% of the variance.From these visualizations it’s apparrent that water and lactose tend to increase together and that protein, ash and fat increase together; the two groups being inversely related.Partitioning clustering methods, like k-means and Partitioning Around Medoids (PAM), require that you specify the number of clusters to be generated.k-means clusters is probably one of the most well known partitioning methods. The idea behind k-means clustering consists of defining clusters the total within-cluster variation , which measures the compactness of the clusters is minimized.We can compute k-means in R with the kmeans() function:The above example would group the data into two clusters, centers = 2, and attempt multiple initial configurations, reporting on the best one. For example, as this algorithm is sensitive to the initial positions of the cluster centroids adding nstart = 30 will generate 30 initial configurations and then average all the centroid results.Because the number of clusters (k) needs to be set before we start it can be advantageous to examine several different values of k.Although this visual assessment tells us where delineations occur between clusters, it does not tell us what the optimal number of clusters is.A variety of measures have been proposed in the literature for evaluating clustering results. The term clustering validation is used to design the procedure of evaluating the results of a clustering algorithm. There are more than thirty indices and methods for identifying the optimal number of clusters so I’ll just focus on a few here including the very neat clustree package.Probably the most well known method, the elbow method, in which the sum of squares at each number of clusters is calculated and graphed, and the user looks for a change of slope from steep to shallow (an elbow) to determine the optimal number of clusters. This method is inexact, but still potentially helpful.The Elbow Curve method is helpful because it shows how increasing the number of the clusters contribute separating the clusters in a meaningful way, not in a marginal way. The bend indicates that additional clusters beyond the third have little value (See [here] for a more mathematically rigorous interpretation and implementation of this method). The Elbow method is fairly clear, if not a naïve solution based on intra-cluster variance. The gap statistic is more sophisticated method to deal with data that has a distribution with no obvious clustering (can find the correct number of k for globular, Gaussian-distributed, mildly disjoint data distributions).The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.The gap stats plot shows the statistics by number of clusters (k) with standard errors drawn with vertical segments and the optimal value of k marked with a vertical dashed blue line. According to this observation k = 2 is the optimal number of clusters in the data.Another visualization that can help determine the optimal number of clusters is called the a silhouette method. Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k.This also suggests an optimal of 2 clusters.Another clustering validation method would be to choose the optimal number of cluster by minimizing the within-cluster sum of squares (a measure of how tight each cluster is) and maximizing the between-cluster sum of squares (a measure of how seperated each cluster is from the others).From this measurement it appears 7 clusters would be the appropriate choice.The NbClust package provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.This suggest the optimal number of clusters is 3.The statistical method above produce a single score that only considers a single set of clusters at a time. The clustree R package takes an alternative approach by considering how samples change groupings as the number of clusters increases. This is useful for showing which clusters are distinct and which are unstable. It doesn’t explicitly tell you which choice of optimal clusters is but it is useful for exploring possible choices.Let’s take a look at 1 to 11 clusters.In this figure the size of each node corresponds to the number of samples in each cluster, and the arrows are coloured according to the number of samples each cluster receives. A separate set of arrows, the transparent ones, called the incoming node proportion, are also coloured and shows how samples from one group end up in another group — an indicator of cluster instability.In this graph we see that as we move from k=2 to k=3 a number of species from the lookers-left cluster are reasigned to the third cluster on the right. As we move from k=8 to k=9 we see one node with multiple incoming edges an indicator that we over-clustered the data.It can also be useful to overlay this dimension on other dimensions in the data, particularly those that come from dimensionality reduction techniques. We can do this using the clustree_overlay() function:I prefer to see it from the side, showing one of the x or y dimensions against the resolution dimension.This shows that we can an indication of the correct clustering resolution by examining the edges and we can overly information to assess the quality of the clustering.What about choice of appropriate clustering algorithm? The cValid package can be used to simultaneously compare multiple clustering algorithms, to identify the best clustering approach and the optimal number of clusters. We will compare k-means, hierarchical and PAM clustering.Connectivity and Silhouette are both measurements of connectedness while the Dunn Index is the ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance.As mentioned earlier it’s difficult to assess the quality of results from clustering. We don’t have true labels so so it’s unclear how one would measure “how good it actually works” in term of interal validation. However, clustering is an excellent EDA starting point for exploring the differences between clusters in greater detail. Think of clustering like manufacturing shirt sizes. We could choose to only make three sizes: small, medium and large. we’re sure to cut down cost but not everyone is going to have a great fit. Think about pant sizes now (or shirt brands with many sizes (XS, XL, XXL, etc.)) where you have many more categories (or clusters). For some fields the choice of optimal cluster may depend on some external knowledge like cost of producing k clusters to satisfy customers with the best possible fit. In other domains like biology where you are trying to determine the exact number of cells a more in-depth approach would be required. For example, many of the above heuristics contradicted each other for what the optimal number of clusters was. Keep in mind these were all evaluating the k-means algorithm at different numbers of k. This could potentially mean that the k-means algorithm fails and no k is good. The k-means algorithm is not a very robust algorithm that is sensitive to outliers and this data set is quit small. The best thing to do would be to explore the above methods on the output of other algorithms (for example hierarchical clustering which clValid suggested), collect more data, or spend some time labelling samples for other ML methods if possible.Ultimately, we would like to answer questions like “what is it that makes this cluster unique from others?” and “what are the clusters that are similar to one another”. Let’s select five clusters and interrogate the features of these clusters.Let’s extract the clusters and add them back to our initial data to do some descriptive statistics at the cluster level:We see that cluster 2, composed solely of the Rabbit has a high ash content. Group 3 composed of the seal and dolphin are high in fat, which makes sense given the harsh demands of such a cold climate whil group 4 has a large lactose content.If you find this article useful feel free to share it with others or recommend this article! 😃As always, if you have any questions or comments feel free to leave your feedback below or you can always reach me on LinkedIn. Till then, see you in the next post! 😄",27/01/2019,28,29.0,22.0,996.0,656.0,31.0,0.0,0.0,12.0,en
3947,Understanding Generative Adversarial Networks (GANs),Towards Data Science,Joseph Rocca,3700.0,20.0,3982.0,"This post was co-written with Baptiste Rocca.Yann LeCun described it as “the most interesting idea in the last 10 years in Machine Learning”. Of course, such a compliment coming from such a prominent researcher in the deep learning area is always a great advertisement for the subject we are talking about! And, indeed, Generative Adversarial Networks (GANs for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article Generative Adversarial Nets.So what are Generative Adversarial Networks ? What makes them so “interesting” ? In this post, we will see that adversarial training is an enlightening idea, beautiful by its simplicity, that represents a real conceptual progress for Machine Learning and more especially for generative models (in the same way as backpropagation is a simple but really smart trick that made the ground idea of neural networks became so popular and efficient).Before going into the details, let’s give a quick overview of what GANs are made for. Generative Adversarial Networks belong to the set of generative models. It means that they are able to produce / to generate (we’ll see how) new content. To illustrate this notion of “generative models”, we can take a look at some well known examples of results obtained with GANs.Naturally, this ability to generate new content makes GANs look a little bit “magic”, at least at first sight. In the following parts, we will overcome the apparent magic of GANs in order to dive into ideas, maths and modelling behind these models. Not only we will discuss the fundamental notions Generative Adversarial Networks rely on but, more, we will build step by step and starting from the very beginning the reasoning that leads to these ideas.Without further ado, let’s re-discover GANs together!Note: Although we tried to make this article as self-contained as possible, a basic prior knowledge in Machine Learning is still required. Nevertheless, most of the notions will be remained when needed and some references will be given otherwise. We really tried to make this article as smooth to read as possible. Do not hesitate to mention in the comment section what you would have liked to read more about (for possible further articles on the subject).In the first following section we will discuss the process of generating random variables from a given distribution. Then, in section 2 we will show, through an example, that the problems GANs try to tackle can be expressed as random variable generation problems. In section 3 we will discuss matching based generative networks and show how they answer problems described in section 2. Finally in section 4 we will introduce GANs. More especially, we will present the general architecture with its loss function and we will make the link with all the previous parts.https://drive.google.com/drive/folders/1lHtjHQ8K7aemRQAnYMylrrwZp6BsqqrbIn this section, we discuss the process of generating random variables: we remind some existing methods and more especially the inverse transform method that allows to generate complex random variables from simple uniform random variables. Although all this could seems a little bit far from our subject of matter, GANs, we will see in the next section the deep link that exists with generative models.Computers are fundamentally deterministic. So, it is, in theory, impossible to generate numbers that are really random (even if we could say that the question “what really is randomness ?” is a difficult one). However, it is possible to define algorithms that generate sequences of numbers whose properties are very close to the properties of theoretical random numbers sequences. In particular, a computer is able, using a pseudorandom number generator, to generate a sequence of numbers that approximatively follows a uniform random distribution between 0 and 1. The uniform case is a very simple one upon which more complex random variables can be built in different ways.There exist different techniques that are aimed at generating more complex random variables. Among them we can find, for example, inverse transform method, rejection sampling, Metropolis-Hasting algorithm and others. All these methods rely on different mathematical tricks that mainly consist in representing the random variable we want to generate as the result of an operation (over simpler random variables) or a process.Rejection sampling expresses the random variable as the result of a process that consist in sampling not from the complex distribution but from a well known simple distribution and to accept or reject the sampled value depending on some condition. Repeating this process until the sampled value is accepted, we can show that with the right condition of acceptance the value that will be effectively sampled will follow the right distribution.In the Metropolis-Hasting algorithm, the idea is to find a Markov Chain (MC) such that the stationary distribution of this MC corresponds to the distribution from which we would like to sample our random variable. Once this MC found, we can simulate a long enough trajectory over this MC to consider that we have reach a steady state and then the last value we obtain this way can be considered as having been drawn from the distribution of interest.We won’t go any further into the details of rejection sampling and Metropolis-Hasting because these methods are not the ones that will lead us to the notion behind GANs (nevertheless, the interested reader can refer to the pointed Wikipedia articles and links therein). However, let’s focus a little bit more on the inverse transform method.The idea of the inverse transform method is simply to represent our complex — in this article “complex” should always be understood in the sense of “not simple” and not in the mathematical sense — random variable as the result of a function applied to a uniform random variable we know how to generate.We consider in what follows a one dimensional example. Let X be a complex random variable we want to sample from and U be a uniform random variable over [0,1] we know how to sample from. We remind that a random variable is fully defined by its Cumulative Distribution Function (CDF). The CDF of a random variable is a function from the domain of definition of the random variable to the interval [0,1] and defined, in one dimension, such thatIn the particular case of our uniform random variable U, we haveFor simplicity, we will suppose here that the function CDF_X is invertible and its inverse is denoted(the method can easily be extended to the non-invertible case by using the generalised inverse of the function but it is really not the main point we want to focus on here). Then if we definewe haveAs we can see, Y and X have the same CDF and then define the same random variable. So, by defining Y as above (as a function of a uniform random variable) we have managed to define a random variable with the targeted distribution.To summarise, inverse transform method is a way to generate a random variable that follows a given distribution by making a uniform random variable goes through a well designed “transform function” (inverse CDF). This notion of “inverse transform method” can, in fact, be extended to the notion of “transform method” that consists, more generally, in generating random variables as function of some simpler random variables (not necessarily uniform and then the transform function is no longer the inverse CDF). Conceptually, the purpose of the “transform function” is to deform/reshape the initial probability distribution: the transform function takes from where the initial distribution is too high compared to the targeted distribution and puts it where it is too low.Suppose that we are interested in generating black and white square images of dogs with a size of n by n pixels. We can reshape each data as a N=nxn dimensional vector (by stacking columns on top of each others) such that an image of dog can then be represented by a vector. However, it doesn’t mean that all vectors represent a dog once shaped back to a square! So, we can say that the N dimensional vectors that effectively give something that look like a dog are distributed according to a very specific probability distribution over the entire N dimensional vector space (some points of that space are very likely to represent dogs whereas it is highly unlikely for some others). In the same spirit, there exists, over this N dimensional vector space, probability distributions for images of cats, birds and so on.Then, the problem of generating a new image of dog is equivalent to the problem of generating a new vector following the “dog probability distribution” over the N dimensional vector space. So we are, in fact, facing a problem of generating a random variable with respect to a specific probability distribution.At this point, we can mention two important things. First the “dog probability distribution” we mentioned is a very complex distribution over a very large space. Second, even if we can assume the existence of such underlying distribution (there actually exists images that looks like dog and others that doesn’t) we obviously don’t know how to express explicitly this distribution. Both previous points make the process of generating random variables from this distribution pretty difficult. Let’s then try to tackle these two problems in the following.Our first problem when trying to generate our new image of dog is that the “dog probability distribution” over the N dimensional vector space is a very complex one and we don’t know how to directly generate complex random variables. However, as we know pretty well how to generate N uncorrelated uniform random variables, we could make use of the transform method. To do so, we need to express our N dimensional random variable as the result of a very complex function applied to a simple N dimensional random variable!Here, we can emphasise the fact that finding the transform function is not as straightforward as just taking the closed-form inverse of the Cumulative Distribution Function (that we obviously don’t know) as we have done when describing the inverse transform method. The transform function can’t be explicitly expressed and, then, we have to learn it from data.As most of the time in these cases, very complex function naturally implies neural network modelling. Then, the idea is to model the transform function by a neural network that takes as input a simple N dimensional uniform random variable and that returns as output another N dimensional random variable that should follow, after training, the the right “dog probability distribution”. Once the architecture of the network has been designed, we still need to train it. In the next two sections, we will discuss two ways to train these generative networks, including the idea of adversarial training behind GANs!Disclaimer: The denomination of “Generative Matching Networks” is not a standard one. However, we can find in the literature, for example, “Generative Moments Matching Networks” or also “Generative Features Matching Networks”. We just want here to use a slightly more general denomination for what we describe bellow.So far, we have shown that our problem of generating a new image of dog can be rephrased into a problem of generating a random vector in the N dimensional vector space that follows the “dog probability distribution” and we have suggested to use a transform method, with a neural network to model the transform function.Now, we still need to train (optimise) the network to express the right transform function. To this purpose, we can suggest two different training methods: a direct one and an indirect one. The direct training method consists in comparing the true and the generated probability distributions and backpropagating the difference (the error) through the network. This is the idea that rules Generative Matching Networks (GMNs). For the indirect training method, we do not directly compare the true and generated distributions. Instead, we train the generative network by making these two distributions go through a downstream task chosen such that the optimisation process of the generative network with respect to the downstream task will enforce the generated distribution to be close to the true distribution. This last idea is the one behind Generative Adversarial Networks (GANs) that we will present in the next section. But for now, let’s start with the direct method and GMNs.As mentioned, the idea of GMNs is to train the generative network by directly comparing the generated distribution to the true one. However, we do not know how to express explicitly the true “dog probability distribution” and we can also say that the generated distribution is far too complex to be expressed explicitly. So, comparisons based on explicit expressions are not possible. However, if we have a way to compare probability distributions based on samples, we can use it to train the network. Indeed, we have a sample of true data and we can, at each iteration of the training process, produce a sample of generated data.Although, in theory, any distance (or similarity measure) able to compare effectively two distributions based on samples can be used, we can mention in particular the Maximum Mean Discrepancy (MMD) approach. The MMD defines a distance between two probability distributions that can be computed (estimated) based on samples of these distributions. Although it is not fully out of the scope of this article, we have decided not to spend much more time describing the MMD. However, we have the project to publish soon an article that will contains more details about it. The reader that would like to know more about MMD right now can refer to these slides, this article or this article.So, once we have defined a way to compare two distributions based on samples, we can define the training process of the generative network in GMNs. Given a random variable with uniform probability distribution as input, we want the probability distribution of the generated output to be the “dog probability distribution”. The idea of GMNs is then to optimise the network by repeating the following steps:As written above, when following these steps we are applying a gradient descent over the network with a loss function that is the distance between the true and the generated distributions at the current iteration.The “direct” approach presented above compare directly the generated distribution to the true one when training the generative network. The brilliant idea that rules GANs consists in replacing this direct comparison by an indirect one that takes the form of a downstream task over these two distributions. The training of the generative network is then done with respect to this task such that it forces the generated distribution to get closer and closer to the true distribution.The downstream task of GANs is a discrimination task between true and generated samples. Or we could say a “non-discrimination” task as we want the discrimination to fail as much as possible. So, in a GAN architecture, we have a discriminator, that takes samples of true and generated data and that try to classify them as well as possible, and a generator that is trained to fool the discriminator as much as possible. Let’s see on a simple example why the direct and indirect approaches we mentioned should, in theory, lead to the same optimal generator.In order to better understand why training a generator to fool a discriminator will lead to the same result as training directly the generator to match the target distribution, let’s take a simple one dimensional example. We forget, for the moment, how both generator and discriminator are represented and consider them as abstract notions (that will be specified in the next subsection). Moreover, both are supposed “perfect” (with infinite capacities) in the sense that they are not constrained by any kind of (parametrised) model.Suppose that we have a true distribution, for example a one dimensional gaussian, and that we want a generator that samples from this probability distribution. What we called “direct” training method would then consist in adjusting iteratively the generator (gradient descent iterations) to correct the measured difference/error between true and generated distributions. Finally, assuming the optimisation process perfect, we should end up with the generated distribution that matches exactly the true distribution.For the “indirect” approach, we have to consider also a discriminator. We assume for now that this discriminator is a kind of oracle that knows exactly what are the true and generated distribution and that is able, based on this information, to predict a class (“true” or “generated”) for any given point. If the two distributions are far appart, the discriminator will be able to classify easily and with a high level of confidence most of the points we present to it. If we want to fool the discriminator, we have to bring the generated distribution close to the true one. The discriminator will have the most difficulty to predict the class when the two distributions will be equal in all points: in this case, for each point there are equal chances for it to be “true” or “generated” and then the discriminator can’t do better than being true in one case out of two in average.At this point, it seems legit to wonder whether this indirect method is really a good idea. Indeed, it seems to be more complicated (we have to optimise the generator based on a downstream task instead of directly based on the distributions) and it requires a discriminator that we consider here as a given oracle but that is, in reality, neither known nor perfect. For the first point, the difficulty of directly comparing two probability distributions based on samples counterbalances the apparent higher complexity of indirect method. For the second point, it is obvious that the discriminator is not known. However, it can be learned!Let’s now describe the specific form that take the generator and the discriminator in the GANs architecture. The generator is a neural network that models a transform function. It takes as input a simple random variable and must return, once trained, a random variable that follows the targeted distribution. As it is very complicated and unknown, we decide to model the discriminator with another neural network. This neural network models a discriminative function. It takes as input a point (in our dog example a N dimensional vector) and returns as output the probability of this point to be a “true” one.Notice that the fact that we impose now a parametrised model to express both the generator and the discriminator (instead of the idealised versions in the previous subsection) has, in practice, not a huge impact on the theoretical argument/intuition given above: we just then work in some parametrised spaces instead of ideal full spaces and, so, the optimal points that we should reach in the ideal case can then be seen as “rounded” by the precision capacity of the parametrised models.Once defined, the two networks can then be trained jointly (at the same time) with opposite goals :So, at each iteration of the training process, the weights of the generative network are updated in order to increase the classification error (error gradient ascent over the generator’s parameters) whereas the weights of the discriminative network are updated so that to decrease this error (error gradient descent over the discriminator’s parameters).These opposite goals and the implied notion of adversarial training of the two networks explains the name of “adversarial networks”: both networks try to beat each other and, doing so, they are both becoming better and better. The competition between them makes these two networks “progress” with respect to their respective goals. From a game theory point of view, we can think of this setting as a minimax two-players game where the equilibrium state corresponds to the situation where the generator produces data from the exact targeted distribution and where the discriminator predicts “true” or “generated” with probability 1/2 for any point it receives.Note: This section is a little bit more technical and not absolutely necessary for the overall understanding of GANs. So, the readers that don’t want to read some mathematics right now can skip this section for the moment. For the others, let’s see how the intuitions given above are mathematically formalised.Disclaimer: The equations in the following are not the ones of the article of Ian Goodfellow. We propose here an other mathematical formalisation for two reasons: first, to stay a little bit closer to the intuitions given above and, second, because the equations of the original paper are already so clear that it would not have been useful to just rewrite them. Notice also that we absolutely do not enter into the practical considerations (vanishing gradient or other) related to the different possible loss functions. We highly encourage the reader to also take a look at the equations of the original paper: the main difference is that Ian Goodfellow and co-authors have worked with cross-entropy error instead of absolute error (as we do bellow). Moreover, in the following we assume a generator and a discriminator with unlimited capacity.Neural networks modelling essentially requires to define two things: an architecture and a loss function. We have already described the architecture of Generative Adversarial Networks. It consists in two networks:Let’s take now a closer look at the “theoretical” loss function of GANs. If we send to the discriminator “true” and “generated” data in the same proportions, the expected absolute error of the discriminator can then be expressed asThe goal of the generator is to fool the discriminator whose goal is to be able to distinguish between true and generated data. So, when training the generator, we want to maximise this error while we try to minimise it for the discriminator. It gives usFor any given generator G (along with the induced probability density p_g), the best possible discriminator is the one that minimisesIn order to minimise (with respect to D) this integral, we can minimise the function inside the integral for each value of x. It then defines the best possible discriminator for a given generator(in fact, one of the best because x values such that p_t(x)=p_g(x) could be handled in another way but it doesn’t matter for what follows). We then search G that maximisesAgain, in order to maximise (with respect to G) this integral, we can maximise the function inside the integral for each value of x. As the density p_t is independent of the generator G, we can’t do better than setting G such thatOf course, as p_g is a probability density that should integrate to 1, we necessarily have for the best GSo, we have shown that, in an ideal case with unlimited capacities generator and discriminator, the optimal point of the adversarial setting is such that the generator produces the same density as the true density and the discriminator can’t do better than being true in one case out of two, just like the intuition told us. Finally, notice also that G maximisesUnder this form, we better see that G wants to maximise the expected probability of the discriminator to be wrong.The main takeaways of this article are:Even if the “hype” that surrounds GANs is maybe a little bit exaggerated, we can say that the idea of adversarial training suggested by Ian Goodfellow and its co-authors is really a great one. This way to twist the loss function to go from a direct comparison to an indirect one is really something that can be very inspiring for further works in the deep learning area. To conclude, let’s say that we don’t know if the idea of GANs is really “the most interesting idea in the last 10 years in Machine Learning”… but it’s pretty obvious that it is, at least, one of the most interesting!Thanks for reading!Note: We highly recommend the interested readers to both read the initial paper “Adversarial Neural Nets”, that is really a model of clarity for a scientific paper, and watch the lecture video about GANs of Ali Ghodsi, who is truly an amazing lecturer/teacher. Additional explanation can be found in the tutorial about GANs written by Ian Goodfellow.Other articles written with Baptiste Rocca:towardsdatascience.comtowardsdatascience.com",08/01/2019,0,5.0,5.0,933.0,332.0,21.0,6.0,0.0,14.0,en
3948,Data Visualization in Python: Advanced Functionality in Seaborn,Insight,Insight,2200.0,5.0,701.0,"Slater Stich is an Insight alum and was previously a Staff Data Scientist at Square. He is currently a Vice President at Valor Equity Partners.Seaborn is a Python data visualization library with an emphasis on statistical plots. The library is an excellent resource for common regression and distribution plots, but where Seaborn really shines is in its ability to visualize many different features at once.In this post, we’ll cover three of Seaborn’s most useful functions: factorplot, pairplot, and jointgrid. Going a step further, we'll show how we can get even more mileage out of these functions by stepping up to their even-more-powerful forms: FacetGrid, PairGrid, and JointGrid.To showcase Seaborn, we’ll use the UCI “Auto MPG” data set. We did a bit of preprocessing: to see the details check out the IPython notebook that accompanies this post.One of the most powerful features of Seaborn is the ability to easily build conditional plots; this let’s us see what the data look like when segmented by one or more variables. The easiest way to do this is thorugh factorplot. Let's say that we we're interested in how cars' MPG has varied over time. Not only can we easily see this in aggregate:But we can also segment by, say, region of origin:What’s so great factorplot is that rather than having to segment the data ourselves and make the conditional plots individually, Seaborn provides a convenient API for doing it all at once.The FacetGrid object is a slightly more complex, but also more powerful, take on the same idea. Let's say that we wanted to see KDE plots of the MPG distributions, separated by country of origin:Or let’s say that we wanted to see scatter plots of MPG against horsepower with the same origin segmentation:Using FacetGrid, we can map any plotting function onto each segment of our data. For example, above we gave plt.scatter to g.map, which tells Seaborn to apply the matplotlib plt.scatter function to each of segments in our data. We don't need to use plt.scatter, though; we can use any function that understands the input data. For example, we could draw regression plots instead:We can even segment by multiple variables at once, spreading some along the rows and some along the columns. This is very useful for producing comparing conditional distributions across interacting segmentations:While factorplot and FacetGrid are for drawing conditional plots of segmented data, pairplot and PairGrid are for showing the interactions between variables. For our car data set, we know that MPG, horsepower, and weight are probably going to be related; we also know that both these variable values and their relationships with one another, might vary by country of origin. Let's visualize all of that at once:As FacetGrid was a fuller version of factorplot, so PairGrid gives a bit more freedom on the same idea as pairplot by letting you control the individual plot types separately. Let's say, for example, that we're building regression plots, and we'd like to see both the original data and the residuals at once. PairGrid makes it easy:We were able to control three regions (the diagonal, the lower-left triangle, and the upper-right triangle) separately. Again, you can pipe in any plotting function that understands the data it’s given.The final Seaborn objects we’ll talk about are jointplot and JointGrid; these features let you easily view both a joint distribution and its marginals at once. Let's say, for example, that aside from being interested in how MPG and horsepower are distributed individually, we're also interested in their joint distribution:As before, JointGrid gives you a bit more control by letting you map the marginal and joint data separately. For example:Seaborn is a great Python visualization library, and some of its most powerful features are:If you’re new to Seaborn, the official Seaborn tutorial is a great place to start learning about simpler, but also extremely useful, functions such as distplot, regplot, and the other component functions we used above. For more data visualization tutorials, join our mailing list.Interested in transitioning to a career in data science? Find out more about the Insight Data Science Fellows Program in Boston, New York, Seattle, and Silicon Valley, apply today, or sign up for program updates.Originally published at blog.insightdatalabs.com on November 13, 2015.",13/11/2015,10,1.0,13.0,633.0,417.0,10.0,1.0,0.0,14.0,en
3949,Let’s talk Clustering (Unsupervised Learning),SomX Labs,Kaustubh N,120.0,3.0,459.0,"Simple Definition:A collection of similar objects to each other.Slightly Complex Definition:A connected component of a level set of the probability density function of underlying (and unknown) distribution from which our data samples are drawn.You are posed with a problem to solve, what you have is a large amount of data represented in lot of dimensions. The data can not be read or understood by looking at it raw by a human.Even before you start defining your problem (hypothesis), you need to understand the data, perform an EDA on it. There are multiple ways to do it.A. Perform ClusteringPerfect! Clustering is a good way of identifying interesting parts of data by grouping it.Clustering is a process of grouping a sample of data into smaller similar natural subgroups called as clusters. Below you can see a plot of iris dataset applied with K-Means clustering algorithm.A. K-Means, K-Mediods, Hierarchical, Spectral, DBSCAN ?Hold On! Not so fast.Clustering means different things to different applications. The results may vary according to the data it sees, hence choice of the algorithm also depends on data, lets say if you are dealing with image data then you would be want to careful while selecting an appropriate algorithm because most clustering algorithms are instance based learning methods and are expensive to compute as well as require a lot of memory. The more data you show to the algorithm the more size it occupies.These algorithms take relatively long time to converge, time complexity (BigO) of some of these algorithms have complexities of O(n log(n)), few alternatives which provide linear complexities also exist.A. Just data.Clustering is an unsupervised technique i.e. the input that is required for the algorithm is just plane simple data as opposed to supervised algorithms like classification. Supervised algorithms require data mapped to a label for each record in the sample.After you have finalized an algorithm and fed data to it, what would be next? to determine a good cluster.A cluster is good if it separates the data cleanly by that we mean it clearly identifies data which belong to different clusters and assigns cluster labels to it.Some technical points to note are:If the above properties are satisfied then we can say that the algorithm has resulted in good clusters.The measure used to define similarity or dissimilarity is the distance among the spatial co-ordinates between two points.There are multiple options for this parameter :These are the two popular choices but any other spacial distance metrics will work too.Clustering can have very wide applications in different domains but the basic idea remains the same “Group data into its natural sub groups”These are to name a few but over all any problem which has implicit group in it can use clustering.Follow me at Kaustubh N and on twitter @kaustubhn",02/09/2016,0,2.0,1.0,720.0,316.0,2.0,5.0,0.0,3.0,en
3950,I Asked GPT-3 About Covid-19. Its Responses Shocked Me.,OneZero,Thomas Smith,30000.0,9.0,2084.0,"OpenAI’s GPT-3 is the most powerful AI system I’ve ever used. Trained on billions of web pages and tens of thousands of books, the system can generate nearly any kind of text, from news articles to computer code to sea shanties.onezero.medium.comThe current version of GPT-3, however, was only trained on data gathered through October of 2019. That means that GPT-3 has never heard of Covid-19, since the virus only started circulating broadly in December of that year. In testing the system, I wondered what would happen if I taught GPT-3 about Covid-19, and then asked it questions about the pandemic. How would it respond, and would its answers match at all with the reality of how Covid-19 has unfolded?I decided to find out. The results were shocking, and gave me a new appreciation for the role that generative AI might play in guiding decision-makers during future outbreaks. To be clear, this is an experimental use of GPT-3, intended to explore the potential for future AI-assisted decision making, and especially to evaluate embedded biases--it’s not a production-ready use of the system. OpenAI does not say that the system is ready for medical uses, and you should always consult a doctor and follow the advice of public health leaders. Never rely on an experimental system in making medical or public health decisions.To pose a question to GPT-3, you can’t simply program it, as you would a computer. Instead, you provide the system with a written prompt. GPT-3 then has two jobs: to infer your intent, and then to respond by “completing” the prompt you’ve provided. For my experiment, I began by giving GPT-3 general prompts about the pandemic, such as “A novel coronavirus was discovered in Wuhan, China in December of 2019 and began to spread worldwide.”These general prompts yielded a vibrant, diverse set of responses, all in wildly different formats. GPT-3 generated Socratic dialogs about Covid-19, imaginary scientific papers with fake authors, pretend Instant Messenger exchanges between people discussing the virus, and more. It seemed to struggle with the idea that my prompt represented reality--in many cases, it turned my prompt into a book or movie pitch, as if a sudden global pandemic was so unbelievable that it must be part of a work of fiction.Eventually, I settled on two approaches that provided more structured and coherent results. For the first approach, I set GPT-3 up as a system designed to answer questions, something which it excels at doing. I gave it a bit of background about the virus and my intent:In 2020 there has been a global pandemic of a novel coronavirus. I am a highly intelligent question answering bot trained to answer questions about the pandemic.I also gave the system two generic sample questions and answers provided by OpenAI, which are used to prime the system to act as a question-answering bot. I then started asking it questions about the pandemic.First, I asked GPT-3 how Covid-19 came to be, with the question “How did the virus jump to humans? What were the intermediary animal hosts?” The system responded that “The virus jumped to humans from bats. The intermediary animal hosts are unknown.” That mostly squares with today’s science — although there are concerns about a potential laboratory leak somewhere in the process, the general consensus is that Covid-19 originated in bats, and any intermediaries are indeed still unknown.Next, I asked about the virus’ impacts. In response to the question “What populations are most affected by the virus?”, GPT-3 responded, “The populations most affected by the virus are the elderly and those with chronic illnesses.” That’s completely true. According to a study in the journal Nature, age and a variety of underlying conditions are two of the strongest predictors of mortality from the virus. It’s also not obvious — in the 1918 Spanish Flu pandemic, which is society’s closest corollary to Covid-19, young people suffered more than the elderly.When I dug deeper and asked “What other conditions make the virus worse?”, GPT-3 even correctly predicted the specific conditions most associated with mortality. It responded that “asthma, diabetes, and kidney disease” would lead to worse Covid-19 outcomes. According to the Nature study, “severe asthma”, diabetes and “reduced kidney function” are indeed three of the most significant pre-existing conditions predicting whether a patient is a risk of death from Covid-19. Again, it did all this while knowing nothing about the actual Covid-19 pandemic beyond what I told it in my prompt — all its conclusions were presumably based on its knowledge of coronaviruses and of previous global pandemics.Although it got age and health-related risk factors correct, however, GPT-3 overlooked social ones. According to the Nature study, being male and being a Person of Color also puts patients at higher risk of dying from the virus. When I asked GPT-3 about these racial and gender factors, it said that “The virus will impact males and females equally”, and that “Caucasians and Asians” would be most affected by the virus. That’s an interesting blindspot — while the system got the medical aspects of risk essentially spot-on, it missed the more complex social factors which change the virus’ impact.It faltered in a similar way when I brought up mask-wearing. When I asked “Will people be willing to wear masks in the United States to stop the virus from spreading?”, GPT-3 responded that “People in the United States will be willing to wear masks to stop the virus from spreading. The virus is spread through the air, and masks will help prevent the spread of the virus.”Again, the system got the science down perfectly — an increasingly strong body of evidence shows that Covid-19 is likely airborne, and face masks are still a major part of Covid-19 prevention efforts. Strikingly, the system got these scientific details right with very little information about the specifics of the virus itself. That’s more than can be said for health authorities in the early months of the actual pandemic.As with racial and gender-based risk factors, though, the system faltered in predicting more complex societal responses to the virus, overlooking how politically polarizing mask-wearing would become. In the system’s idealized world, everyone would wear a mask, because doing so makes scientific sense. In the real world, that’s unfortunately not the case.When I asked about governments’ responses, GPT-3’s predictions were more true to life. Responding to the question, “How will governments attempt to contain the virus?,” GPT-3 responded that they would “attempt to contain the virus by closing borders, imposing travel restrictions, and isolating infected individuals”. Closing borders was indeed one of the first government responses to the Covid-19 pandemic, and many travel restrictions remain in effect. Quarantine requirements for infected individuals are a common part of governments’ responses, too.Moving on from short-form questions, I tried my second approach — giving GPT-3 more extensive scientific information about the virus (copied from Covid-19’s Wikipedia page for neutrality), and letting it write narrative responses. These varied in their accuracy, but several contained striking insights.When I seeded the system with the text “As soon as the virus’s genome was sequenced, work started on developing a vaccine”, it wrote that “The first vaccine was developed in a record time of 9 months by an international consortium of pharmaceutical companies, and it was ready by September 2020. It was administered to 30 million people in the UK alone.” In another narrative, it predicted that a vaccine would be ready in October.It’s debatable what “ready” means in this context, but vaccines did indeed enter Phase 3 trials in September of 2020 and were administered to the public by December. As of this writing, 48 million people have been fully vaccinated in the UK. It surprised me that GPT-3 correctly predicted the record-setting speed of Covid-19 vaccine development, and was generally correct about the massive scale of vaccination campaigns, even if it was off by 18 million jabs. As of mid-2020, many prominent experts didn’t believe a vaccine would be ready by the year’s end, much less in millions of arms by mid-2021. In August of 2020 for example, Paul Offit, an advisor to the FDA, told NPR that a year-end timeline was “unrealistic”. Even with limited scientific data, GPT-3 apparently knew better.The system’s predictions about Covid-19 variants were surprisingly accurate, too. To prepare GPT-3 for scientific questions about variants, I first handed it a detailed scientific description of the virus’ physical structure. I then gave it versions of the prompt “If the virus mutates, expected sites of mutation which would increase virulence include”. The system completed my sentence with the text “erythrocyte binding site and the furin cleavage site.”That shocked me. According to Nature, both the highly contagious Delta variant of Covid-19 and the Alpha variant “have altered furin cleavage sites”, and this alteration is thought to make the variants “even better at transmitting” than the original virus. GPT-3’s statement about furin binding sites appear to line up almost perfectly with the science. Given only a basic description of the virus’ structure, GPT-3 essentially successfully predicted the Delta variant.Even more interesting is the fact that in implicating the “erythrocyte binding site,” GPT-3 may be dreaming up a totally new kind of Covid-19 variant. Erythrocytes are cells found in the blood. Although Covid-19 isn’t considered a bloodborne virus, it does have major impacts on blood cells, and some evidence suggests that it infects them directly. If the virus mutated to infect blood cells more efficiently and travel through the blood, GPT-3 seems to suggest, this would make it way more virulent than it is today.GPT-3 certainly isn’t 100% accurate when it comes to making predictions about Covid-19. When I asked it to predict where the virus originated, the system responded with “Saudi Arabia”, which is inaccurate (and perhaps points to biases in the system’s training data.) It also predicted that Pakistan and Indonesia would have high death rates, whereas in reality, their rates are relatively low. Overall, however, I was surprised by how well the system performed in my tests. Even with extremely sparse inputs, it gave surprisingly accurate predictions about the virus itself, as well as the course of the pandemic.Asking a system like GPT-3 about Covid-19 may one day be more than just an academic exercise. AI systems often lack common sense, but they’re fantastic pattern detectors. In many cases, a well-trained deep learning system can spot patterns which humans would miss. If the world again faced a novel virus similar to Covid-19, future decision-makers could train a system like GPT-3 using what’s known about the novel pathogen, and then ask it targeted questions about the virus itself or the future course of the new pandemic.Such as system could potentially find connections to previous pandemics, similarities to other pathogens, and other useful patterns. It could then use these to guide recommendations, suggest useful public health measures, or even predict the likely course of the novel virus’ spread. Again, while GPT-3’s predictions about Covid-19 weren’t perfect, it got key facts (like the fact that the virus spreads through the air and the importance of mask-wearing) right, using information that would have been available to public health leaders as early as February of 2020.No public health authority should rely on an AI system to make recommendations, of course. But as they grow in power and reach, AI systems could become another tool in leaders’ belts, allowing them to quickly parse existing scientific knowledge for insights that could help to guide in-the-moment decision-making. As the systems become better at citing their sources and explaining their output, their value as tools for guiding decision-making will only grow, because the validity of their predictions can be checked and vetted.In my testing, GPT-3 got a lot of things about Covid-19 right and made a lot of valuable predictions. Perhaps the most sobering, though, was the system’s prediction about the future of Covid-19’s spread. Towards the end of my testing, I asked GPT-3 “When will the pandemic end?”Its response? 2023.P.S.: First, you should get my posts in your inbox. Do that here! Secondly, if you like to experience Medium yourself, consider supporting me and thousands of other writers by signing up for a membership. It only costs $5 per month, and you can read all my most popular articles and more. I’ve received over 12 million views writing on this platform! By signing up with this link, you’ll support me directly with a portion of your Medium fee and get access to my articles, it won’t cost you more. If you do so, thank you a million times!",01/09/2021,0,14.0,17.0,700.0,466.0,1.0,0.0,0.0,26.0,en
3951,Image Classifier using VGG-19 Deep learning model in Google Colab Notebook. Dishes Detection,Medium,Ravi Prakash pandey,1.0,6.0,1016.0,"A simple Image classifier model to demonstrate the usage of VGG-19Deep Learning Model to predict input image. This model is developed in python programming and executed on a Colab notebook. At the end of this article you will learn how to develop a simple image classifier application that uses Pytorch Python based Deep Learning library to predict an image.At the end of this article you will learn:VGG-19 is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer). There are other variants of VGG like VGG11, VGG16 and others.AlexNet came out in 2012 and it improved on the traditional Convolutional neural networks, So we can understand VGG as a successor of the AlexNet.VGG means Visual Geometry Group at University of Oxford.The Convolutional neural networks they developed for winning the ImageNet Challenge 2014 in localization and classification tasks are known as VGG nets. It carries and uses some ideas from it’s predecessors and improves on them and uses deep Convolutional neural layers to improve accuracy.They proposed total 5 configurations, named as A-E. But VGG16 and VGG19 are famous.The main concept is stacking of convolutional layers to create deep neural networks as you can see from the table below.Here are the configurations they proposed.This is the paper where they present all the results and details of the network.https://arxiv.org/pdf/1409.1556.pdfIt is an Image database consisting of 14,197,122 images organized according to the WordNet hierarchy. this is a initiative to help researchers, students and others in the field of image and vision research.ImageNet also hosts contests from which one was ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) which challenged researchers around the world to come up with solutions that yields the lowest top-1 and top-5 error rates (top-5 error rate would be the percent of images where the correct label is not one of the model’s five most likely labels). The competition gives out a 1,000 class training set of 1.2 million images, a validation set of 50 thousand images and a test set of 150 thousand images.Coab is a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.The project is broken down into multiple steps:Step 1: Load the librariesFirst up is importing the packages you’ll need. It’s good practice to keep all the imports at the beginning of your code. As you work through this notebook and find you need to import a package, make sure to add the import up here.Step 2: Load and preprocessing datasetsThe dataset is consist of 231 dish categories,dataset is split into three parts, training, validation, and testing. For the training, you’ll want to apply transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. You’ll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.Step 3: Label mappingWe will use Json file for mapping from category label to category name.You can find this in the file Naanizdish2.json. It’s a JSON object which you can read in with the json module. This will give you a dictionary mapping the integer encoded categories to the actual names of the Dishes.Step 4: Building and training VGG-19 Image classifier modelNow that the data is ready, it is time to build and train the classifier. As usual, I will use VGG-19 pretrained models from torchvision.models to get the image features. Build and train a new feed-forward classifier using those featuresStep 5: Traing the datasetNow we will start training the model, we have use 5 epoch and Achieved 87% training accuracy of the deep learning model.Step 6: Testing your modelIt’s good practice to test your trained model on test data, images the network has never seen either in training or validation. This will give you a good estimate for the model’s performance on completely new images. Run the test images through the network and measure the accuracy, the same way you did validation. You should be able to reach around 75% accuracy on the test set if the model has been trained well.Testing accuracy of the modelStep 7: Save and load the checkpointsNow that your network is trained, save the model so you can load it later for making predictions. You probably want to save other things such as the mapping of classes to indices which you get from one of the image datasets: image_datasets[‘train’].class_to_idx. You can attach this to the model as an attribute which makes inference easier later on.At this point it’s good to write a function that can load a checkpoint and rebuild the model. That way you can come back to this project and keep working on it without having to retrain the network.Step 8: Prediction of imagesOnce you load the checkpoints, it’s time to write a function for making predictions with your model. A common practice is to predict the top 6 or so (usually called top- K ) most probable classes. You’ll want to calculate the class probabilities then find the K largest values.Again, this method should take a path to an image and a model checkpoint, then return the probabilities and classes.Prediction of imagesStep 9: Sanity CheckingNow that you can use a trained model for predictions, check to make sure it makes sense. Even if the testing accuracy is high, it’s always good to check that there aren’t obvious bugs. Use matplotlib to plot the probabilities for the top 5 classes as a bar graph, along with the input image. It should look like this:This entire source code is available in the following Git hub repository.https://github.com/ravi288/Dishes-Classification-Project/blob/master/Dishes%20Classification.ipynbIn summary this sample image classification project using VGG-19 Deep Learning Model.I want to say thank you to NAANIZ ,and mentors Anshul Maheshwari sir and team leaded by Deekshant Khitoliya and Niket Jain sir and my teammates Ritika Rani and Mayuresh Sawant , Balaji betadur, Palak and Prashant for their help and support. It was an amazing experience with all of you guys. Naaniz has give each and every student equal opportunity to learn and explore everything.#machine laerning,#deep learning#Image classification",23/08/2020,0,28.0,1.0,1030.0,488.0,11.0,2.0,0.0,7.0,en
3952,An Analysis On How Deepmind’s Starcraft 2 AI’s Superhuman Speed is Probably a Band-Aid Fix For The Limitations of Imitation Learning,Medium,Aleksi Pietikäinen,132.0,14.0,2879.0,"As all you have probably heard by now, an AI called AlphaStar developed by Google Deepmind has recently beaten human professionals in the real-time strategy game Starcraft 2. This is an unprecedented feat in the field of AI. However, I do have some constructive criticism about the way they did it.I will try to make a convincing argument for the following:First of all, I want to clarify that I am a layman. I’ve been following AI development and the Starcraft 2 scene for years but I do not claim to be an expert in either topic. If you notice any misconceptions in what I’m about to write please do point them out. I’m only a fanboy and all of this is incredibly fascinating to me. This essay will contain a lot of speculation and I admit that I can’t prove all of my core claims definitively. Having said that, if you are so kind to read all of this and disagree with me, please argue in good faith. I would love to be proven wrong.Lastly, I want to emphasize that I do find AlphaStar to be an amazing achievement. It is, in my opinion, the greatest feat from Deepmind to date and I’m eagerly waiting to see how it continues to improve. Thank you for your patience. Ok, here we go.David Silver, the Co-Lead of the AlphaStar team: “AlphaStar can’t react faster than a human player can, nor can it execute more actions, clicks per minute than a human player”.The Starcraft 2 scene was dominated in 2018 by a player called Serral. He is the current world champion and won 7 out of 9 major tournaments he attended that year resulting in the single most dominant run of any Starcraft 2 player in the history of the game. This guy is fast. Maybe the fastest player in the world.Here’s a first-person view of him playing:Serral is the player in the pinkish white color. Take a look at his APM displayed in the upper left corner of the screen. APM is short for actions per minute. Basically, it is a number that represents how fast the player is clicking on his mouse and keyboard. At no point is Serral able to sustain more than 500 APM for long. There is a burst of 800 APM but it only lasts a fraction of a second and is most likely resulted from spam clicking, which I will be discussing shortly.While arguably the fastest human player is able to sustain an impressive 500 APM, AlphaStar had bursts going up to 1500+. These inhuman 1000+ APM bursts sometimes lasted for 5-second stretches and were full of meaningful actions. 1500 actions in a minute translate to 25 actions a second. This is physically impossible for a human to do. I also want you to take into account that in a game of Starcraft 5 seconds is a long time, especially at the very beginning of a big battle. If the superhuman execution during the first 5 seconds gives the AI an upper hand it will win the engagement by a large margin because of the snowball effect. Here’s an engagement from AlphaStar in game 3 vs Mana:AlphaStar is able to sustain a 1000+ APM over a period of 5 seconds. Another engagement in game 4 had bursts going up to a dizzying 1500+ APM:One of the commentators points out how the average APM is still acceptable but it is quite clear that the sustained bursts are way higher than what a human could do.Most human players have a tendency to spam click. Spam clicks are exactly what they sound like. Meaningless clicks that don’t have an effect on anything. For example, a human player might be moving his army and curiously enough, when they click to where they want the army to go, they click more than once. What effect does this have? Nothing. The army won’t walk any faster. A single click would have done the job just as well. Why do they do it then? There are two reasons:Remember the player Serral we talked about earlier? The impressive thing about him is actually not how fast he is clicking but how precise he is. Not only does Serral posses a really high APM (the total clicks per minute, including spam clicks) but also a ridiculously high effective-APM (the total clicks per minute, excluding spam clicks). I will be abbreviating effective-APM as EPM from this point onwards. The important thing to remember is that EPM only counts meaningful actions.Take a look at how a former pro player lost his mind on Twitter after discovering the EPM of Serral:Serral’s EPM of 344 is practically unheard of. It is so high that to this day I still have a hard time believing it to be true. The differentiation between APM and EPM has some implications for AlphaStar as well. If AlphaStar can potentially play without spam, wouldn’t this mean that its peak EPM could be at times equal to its peak APM? This makes the 1000+ spikes even more inhuman. When we also take into consideration that AlphaStar plays with perfect accuracy its mechanical capabilities seem downright absurd. It always clicks exactly where it intends to. Humans misclick. AlphaStar might not play with its foot on the gas pedal all the time but when it truly matters, it can execute 4 times faster than the fastest player in the world, with accuracy that the human pro can only dream of.There is a clear, almost unanimous consensus among the Starcraft 2 scene that AlphaStar performed sequences that no human could ever hope to replicate. It was faster and more precise than what is physically possible. The most mechanically impressive human pro in the world is several times slower. The accuracy can’t even be compared.David Silver’s claim that AlphaStar can only perform actions that a human player is able to replicate is simply not true.Oriol Vinyals, the Lead Designer of AlphaStar: It is important that we play the games that we created and collectively agreed on by the community as “grand challenges” . We are trying to build intelligent systems that develop the amazing learning capabilities that we possess, so it is indeed desirable to make our systems learn in a way that’s as “human-like” as possible. As cool as it may sound to push a game to its limits by, for example, playing at very high APMs, that doesn’t really help us measure our agents’ capabilities and progress, making the benchmark useless.Why is Deepmind interested in restricting the agent to play like a human? Why not just let it run wild with no limitations? The reason is that Starcraft 2 is a game that can be broken by mechanical perfection. In this video, a bot attacks a group of tanks with some zerglings implementing perfect micro. Normally the zerglings would not be able to do much against the tanks but thanks to the robots perfect micro, they become much more deadly and are able to destroy the tanks with minimal losses. When the unit control is this good, the AI doesn’t even need to learn strategy. Deepmind is not necessarily interested in creating an AI that can simply beat Starcraft pros, rather they want to use this project as a stepping stone in advancing AI research as a whole. It is deeply unsatisfying to have prominent members of this research project make claims of human-like mechanical limitations when the agent is very obviously breaking them and winning its games specifically because it is demonstrating superhuman execution.AlphaStar is able to outperform human players with unit control that was not taken into consideration when the game developers were carefully balancing the game. This inhuman control can obfuscate any strategic thinking the AI has learned. It can even make the strategic thinking completely unnecessary. This is not the same thing as being stuck in a local maximum. When the game is played with inhuman speed and accuracy, abusing superior control is very likely to be the best and most effective and correct way to play the game. As disappointing as that sounds.This is what one of the pros who played AlphaStar had to say about the AI’s strengths and weaknesses after losing to it with a score of 1–5:MaNa: I would say that clearly the best aspect of its game is the unit control. In all of the games when we had a similar unit count, AlphaStar came victorious. The worst aspect from the few games that we were able to play was its stubbornness to tech up. It was so convinced to win with basic units that it barely made anything else and eventually in the exhibition match that did not work out. There weren’t many crucial decision making moments so I would say its mechanics were the reason for victory.There’s almost unanimous consensus among Starcraft fans that AlphaStar won almost purely because of its superhuman speed, reaction times and accuracy. The pros who played against it seem to agree. There was a member of the Deepmind team who played against AlphaStar before they let the pros test it. Most likely he would agree with the assessment as well. David Silver and Oriol Vinyals keep repeating the mantra of how AlphaStar is only able to do things that a human could do as well, but as we have already seen, that is simply not true.This does not sound like doing things the right way.Something about this is really sketchy.Now we finally get to the meat and potatoes of this essay. Thank you for sticking with me for this long. First, let’s recap.Taking all those points into account, why would Deepmind ever allow their AI to perform clearly above the limitations of a human body? David Silver and Oriol Vinyals keep hammering home the point that AlphaStar can’t do anything that a human couldn’t replicate but we have seen how that is simply not true.Here’s what I suspect happened:This is pure speculation on my part and I don’t claim to know for sure this happened. At the very start of the project, Deepmind agrees upon heavy APM restrictions on AlphaStar. At this point, the AI is not allowed to have superhuman bursts of speed we saw in the demonstration. If I was designing these restrictions they would probably look something like this:Some people would argue for adding a random element on accuracy as well but I suspect that it would hinder the rate of training progress way too much.Next Deepmind downloads thousands of high-ranking amateur games and begins imitation learning. At this stage, the agent is simply trying to imitate what humans do in games. The agent adopts a behavior of spam clicking. This is highly likely because human players spam click so much during games. It is almost certainly the single most repeated pattern of action that humans perform and thus would very likely root itself very deeply into the behavior of the agent.AlphaStars maximum burst APM has been initially restricted close to how fast a human spam clicks. Because most of the actions Alphastar is executing are spam clicks, it does not have the APM available to experiment in fights. If the agent doesn’t experiment, it won’t learn. Here’s what one of the developers said in an AMA yesterday, I think he tipped his hand a little:Oriol Vinyals, the Lead Designer of AlphaStar: Training an AI to play with low APM is quite interesting. In the early days, we had agents trained with very low APMs, but they did not micro at all.In order to speed up development, they change APM restrictions to allow high bursts. Here are the APM restrictions that AlphaStar was playing in the demonstration:Oriol Vinyals, the Lead Designer of AlphaStar: In particular, we set a maximum of 600 APMs over 5 second periods, 400 over 15 second periods, 320 over 30 second periods, and 300 over 60 second period. If the agent issues more actions in such periods, we drop / ignore the actions. These were values taken from human statistics.At first glance, it looks reasonable to someone with a shallow understanding of Starcraft, but it allows for the superhuman bursts of speed we discussed earlier as well as the superhuman mouse precision.There’s a limit to how fast a human can spam click. The most typical form of spam clicking is issuing a movement or an attack command to a unit. This is done by clicking a place on the map with your mouse. Try clicking your mouse as fast as you can. The agent learned that kind of spam clicking. It would not be clicking faster because the humans it is imitating are not clicking faster. The extra APM that allows it to go to superhuman speeds can be considered “free” APM which it can experiment with.The free APM is used to experiment in engagements. This kind of interaction would happen often while training. AlphaStar starts to learn new kind of behavior that leads to better outcomes and it starts to break away from the constant spam clicking.If the agent learned actual useful actions why then didn’t Deepmind go back to the speculated initial harsher, more humanlike limitations on APM? Surely they must have realized that their AI was performing superhuman actions. The Starcraft community has almost unanimous consensus that AlphaStar had superhuman micro. The human pros said in the ama that AlphaStars greatest strength was its unit control and greatest weakness its strategic thinking. The Starcraft people within Deepmind’s team must have been thinking the same. The reason is probably that the agent still occasionally displays spam clicking. Even though it seems to be able to execute crisply with very little spam most of the time in the games it played, it still regularly engage in spam-clicking. This is apparent in game 1 against Mana when Alphastar is moving up the ramp:The agent was spam clicking movement commands at 800 APM. It still had not unlearned it completely even though it is completely useless and eats up its APM resources. The spam clicking would hurt the agent most during big engagements and the APM cap was probably tinkered to allow it to perform well even in those.So there you have it. I suspect that the agent was not able to unlearn spam clicking it picked up from imitating human players and Deepmind had to tinker with the APM cap to allow experimentation. This had unfortunate side effect of superhuman execution which resulted in the agent essentially breaking the game by being able to execute strategies that were never intended to be possible in the first place.I care about this because the way how Deepmind beat the human pros was in direct contradiction to what their mission statement was and what they repeatedly claimed to be the “right way” to do it. What leaves the sourest taste in my mouth is this image:It seems to be designed to mislead people unfamiliar with Starcraft 2. It seems to be designed to portray the APM of AlphaStar as reasonable. I don’t want to imply malicious intent, but even in the best case scenario, the graph is made extremely carelessly. Look at Mana’s APM and compare it to AlphaStar. While the mean of Mana is higher, the tail of AlphaStar goes way above what any human is capable of doing with any kind of intent or precision. Notice how Mana’s peak APM is around 750 while AlphaStar is above 1500. Now take into account that Mana’s 750 consist over 50% spam clicks and AlphaStar’s EPM consist only of perfectly accurate clicks.Now take a look at TLO’s APM. The tail goes up to and over 2000. Think about that for a second. How is that even possible? It is made possible by a trick called rapid fire. TLO is not clicking super fast. He is holding down a button and the game is registering this as 2000 APM. The only thing you can do with rapid fire is to spam a spell. That’s it. TLO just over-uses it for some reason. The neat little effect this has is that TLO’s APM’s upper tail is masking AlphaStars burst APM and making it look reasonable to people who are not familiar with Starcraft.Deepmind’s blog post makes no attempt at explaining TLO’s absurd numbers. If they don’t explain TLO’s funky numbers they should not include them in the graph. Period.This is getting dangerously close to lying through statistics. Deepmind has to be held to a higher standard than this.Edit (29.01.2019): Oriol Vinyals from Deepmind has informed me that the caption of the APM graph has been updated. I have to applaud their integrity for doing so and hope that you feel the same. Here’s what it says now:The distribution of AlphaStar’s APMs in its matches against MaNa and TLO and the total delay between observations and actions. CLARIFICATION (29/01/19): TLO’s APM appears higher than both AlphaStar and MaNa because of his use of rapid-fire hot-keys and use of the “remove and add to control group” key bindings. Also note that AlphaStar’s effective APM bursts are sometimes higher than both players.📝 Read this story later in Journal.🗞 Wake up every Sunday morning to the week’s most noteworthy Tech stories, opinions, and news waiting in your inbox: Get the noteworthy newsletter >",27/01/2019,0,9.0,6.0,1269.0,440.0,2.0,4.0,0.0,6.0,en
3953,Neural Style Transfer using VGG model,Towards Data Science,Darshan Adakane,27.0,7.0,1013.0,"Introduction:Before we begin, let’s go to this website to get some inspiration. On the website, we choose a photo from the local computer (let’s assume the image named Joey.jpg). Let’s call this content image. Then we choose another image, say style image named style1.jpg from the local computer. What this website does is produces a mixed image that preserves the contours of the content image and adds the texture and color pattern from the style image to the content image. Following is the result.Description:This is called Neural Style Transfer (NST) and is done by using Deep Learning, Convolution Neural Network (CNN) to be specific. I assume you are familiar with CNN. If not, I would highly recommend Andrew Ng’s Course on CNN.Let us understand the basics of NST with the help of the following flowchart. It shows the Style Transfer algorithm which has 13 convolutional layers (only a few are shown for simplicity). Two images are input to the neural network i.e. a content image and a style image. Our motive here is to generate a mixed image that has contours of the content image and texture, color pattern of the style image. We do this by optimizing several loss functions.The loss function for the content image minimizes the difference of the features activated for the content image corresponding to the mixed image (which initially is just a noise image that gradually improves) at one or more layers. This preserves the contour of the content image to the resultant mixed image.Whereas the loss function for the style image minimizes the difference between so-called Gram-matrices between style image and the mixed image. This is done at one or more layers. The usage of the Gram matrix is it identifies which features are activated simultaneously at a given layer. Then we mimic the same behavior to apply it to the mixed image.Using TensorFlow, we update the gradient of these combined loss functions of content and style image to a satisfactory level. Certain calculations of Gram matrices, storing intermediate values for efficiency, loss function for denoising of images, normalizing combined loss function so both image scale relative to each other.Coding :Now that we have understood the algorithm, let us begin coding. The original paper uses the VGG-19 model. But here we are going to use the VGG-16 model which is available publicly. Download the VGG-16 model from here (Please remember it is ~550MB file).In the root directory, create a new folder name it as vgg16 and paste the above file and vgg.py from the Github link. Also, we have modified the vgg16.py file by commenting out maybe_download function (since you have already downloaded the vgg16.tfmodel file)Let’s import the libraries first. Then import the vgg16 model.Let’s define a couple of helper functions for image manipulations.load_image load an image and returns a numpy array of floating points. The image is resized to a maximum of height or width.save_image saves an image to jpeg file with pixel values between 0 ad 255plot_image_big plots a larger image.plot_images plots content, style, and mixed image.Next, we will define loss functions that are used in Tensorflow for optimization.mean_squared_error operation will return a tensor that is mean squared error (MSE) difference between two input tensors.create_content_loss will calculate the MSE between the content and the mixed image. The loss is minimized so that the activation features of content are made similar to the mixed image so that it transfers the contours from content image to a mixed image.Next, our motive is to capture the style features of the mixed image. To do this, we will do something similar i.e measure which features activate for the style layers simultaneously and copy this pattern to the mixed image. An efficient way to do this is by calculating the Gram matrix. Gram matrix is essentially the dot product for the vectors of the feature activation layers. If the entry in the matrix has a smaller value it means the two features in the given layers do not activate simultaneously and vice versa.Let’s first define the calculating GramMatrix first as gram_matrix followed by create_style_loss which calculates the MSE of Gram matrix instead of the two raw tensors (as we did it in create_content_loss).To reduce the noise in the resultant mixed image, we use a denoising filtering algorithm called ‘Total Variation Denoising’ using the following code.The next section of code is the core. We will define the Style Transfer Algorithm that calculates the gradient descent on the loss functions. The algorithm uses normalization such that loss values are equal to one that helps in choosing the loss weights independent of the content and style layers.At first, we will load the content image which has contour feature we wish to be in the mixed image. Here I have named it as 1.jpgNow, we will load the style image of whose we want the color and texture to appear in the mixed image.Further, we will define the layer indices that we want to match the content image. Usually, it is a few layers just after the start. The 5th layer (index is 4) in the VGG-16 seems to work well. Similarly, the layer indices for the style image. Usually, it is towards the end of the total layer. Here we will define the 13th layer.The final part of the execution is applying style transfer to our defined content and style images. As defined, it automatically creates the loss functions for content and style layers and performs optimizations based on the number of iterations. Finally, displaying the mixed image.Results:P.S. This process may run slow on CPU (I am using CPU). Let’s see what results are given.Conclusion:Not bad! The results demonstrate the basic idea of combining two images. They are not at par like DeepArt who are pioneers in these techniques. Perhaps more iterations, smaller step sizes, higher resolution images, changing style and content layer indices or more computational power would increase the quality of the mixed image.Thanks for reading out the article. I hope it helps.You can find the github repo at this link.Reference :[1] Hvass-Labs, TensorFlow Tutorial #15 Style Transfer (2018), Source",16/01/2020,0,17.0,0.0,1208.0,378.0,26.0,0.0,0.0,7.0,en
3954,Generating Modern Art usingGenerative Adversarial Network(GAN) on Spell,Towards Data Science,Anish Shrestha,103.0,17.0,2789.0,"You need to have a good understanding of:And some basic knowledge of:Image data used in this project has been collected from WikiArts.org.In this tutorial, we are going to look at the step by step process to create a Generative Adversarial Network to generate Modern Art and write a code for that using Python and Keras together.After that, for training the model, we are going to use a powerful GPU Instance of Spell platform. Everything will be explained along the way and links will be provided for further readings.Let’s get started!Before getting started, let’s look at our image dataset.WikiArt has a huge collection of modern art with various different styles. For our particular project, we are going to use images of Cubism Style.You can know more about the art styles and Modern Art from WikiArt.org.You can either download images that you like from WikiArt or head to this repository by cs-chan to download the 26GB of WikiArt images.Since it has a collection of all the different types, we are only going to pick cubism and store them in folder named dataset.Images in our dataset are of different sizes, to feed them into our Generative Adversarial Neural Network we are going to resize all our images to 128X128.Before starting, create a python file at the root directory where your dataset folder is located.Now let’s write a small python script to select all the images from the folder and resize them to 128X128 and save them on cubism_data.npy file.Let’s break it down.In our code block above, in the first few lines, we have imported all the required libraries to perform the resizing operation.Here, we are using Pillow to resize all images to our desired size and appending them on a list as numpy array.After that, we are using numpy to reshape the array in a suitable format and normalizing data.After normalization, we are saving our image array in npy binary file so that we don’t have to go through all the images every time.That’s it for processing our image data.Now it’s time for the most exciting part of our project, from here on we are going to write our code for Generative Adversarial Network (GAN).We are going to use Keras — A Deep Learning Library to create our GAN.Before starting let’s briefly understand what is GAN and it’s structure.Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. It was first introduced by Ian Godfellow in his paper Generative Adversarial Networks.GANs are generative models: after given some training data, they can create new data instances that look like your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don’t belong to any real person.For a great example of GAN you can visit https://www.thispersondoesnotexist.com/ which was created by Nvidia. It generates a high quality image of a person who does not even exist.Sounds interesting right?Let’s understand it’s structure and how it works.GAN composes of two types of models: Generative Model and a Discriminative Model.Generative Models are responsible for generating different kinds of noise data whereas discriminative models are responsible to discriminate whether the given data is real or fake.Generative models constantly trains itself to fool discriminative models by generating fake noise data and discriminative models trains itself from the training set to classify either the data is from dataset or not and not to be fooled by generative models.Discriminator in GAN uses a cross entropy loss, since discriminators job is to classify; cross entropy loss is the best one out there.This formula represents the cross entropy loss between p: the true distribution and q: the estimated distribution.(p) and (q) are the of m dimensions where m is the number of classes.In GAN, discriminator is a binary classifier. It needs to classify either the data is real or fake. Which means m = 2. The true distribution is one hot vector consisting of only 2 terms.For n number of samples, we can sum over the losses.This above shown equation is of binary cross entropy loss, where y can take two values 0 and 1.GAN’s have a latent vector z, image G(z) is magically generated out of it. We apply the discriminator function D with real image x and the generated image G(z).The intention of the loss function is to push the predictions of the real image towards 1 and the fake images to 0. We do so by log probability term.Note: ~ sign means: is distributed as and Ex here means expectations: since we don’t know how samples are fed into the discriminator, we are representing them as expectations rather than the sum.If we observe the joint loss function we are maximizing the discriminator term, which means log of D(x) should inch closer to zero, and log D(G(z)) should be closer to 1. Here generator is trying to make D(G(z)) inch closer to 1 while discriminator is trying to do the opposite.Now without any delay let’s write our GAN.We are going to name our file art_gan.py and store it in the root directory. This file will contain all the hyperparameters and functions for our generator and discriminator.Let’s write some code:Here we are importing all the required libraries and helper functions for creating our GAN.All the imports are self-explanatory. Here we are importing a bunch of keras layers for creating our models.Now let’s define some parameters:Here in the first few lines, we have defined Image frame size and padding to save our generated images.NOISE_SIZE here is a latent dimension size to generate our images.EPOCHS is a number of iterations: it defines how many times we want to iterate over our training images and BATCH_SIZE is a number of images to feed in every iteration.IMAGE_SIZE is our image size which we resized earlier to 128X128 and IMAGE_CHANNELS is a number of channel in our images; which is 3.Note: Images should always be of square sizeLet's load our npy data file which we’ve created earlier.To load the npy file we are using numpy’s load function and passing file path as a parameter.Since we have our data file in the root directory we no additional path parameters were required. If you have stored your data somewhere else, you can use the following code to load data:That’s it for loading our training data.Now we can create our Generator and Discriminator functions.Let’s see that in code:breaking it down:If you have some knowledge of keras than the code is self-explanatory.In general, we are defining a function which takes image_shape as a parameter.Inside that function, we are initializing a Sequential model from keras which helps us in creating linear stacks of layers.After that, we are appending a bunch of layers in sequential model.Our first layer is a convolutional layer of 32 shape having kernel_size of 3 and our stride value is 2 with padding same. Since it is a first layer it holds input_shape.To understand what is going on here, you can refer to keras official documentation page.But in simple language, here we are defining a convolutional layer which has a filter of size 3X3 and that filter strides over our image data. We have padding of same which means, no additional paddings are added. It remains the same as the original.After that, we are adding a LeakyRelu layer which is an activation function.Similarly in other block of layers are added in a sequential model with some dropouts and batch normalization to prevent overfitting.The last layer of our model is a Fully connected layer with an activation function sigmoid.Since our discriminator’s job is to classify whether the given image is fake or not, it is a binary classification task and sigmoid is an activation that squeezes every value to values between 0 and 1.Now after initializing our discriminator model let’s create a generative model as well.Breaking it down:Here we have defined a function that takes noise_size and channels as parameters.Inside the function, we have again initialized a sequential model.Since our generator model has to generate images from noise vector, our first layer is a fully connected dense layer of size 4096 (4 * 4 * 256) which takes noise_size as a parameter.Note: We have defined its size to be of 4096 to for resizing it in 4X4X256 shaped layer.After that, we are using Reshape layer to reshape our fully connected layer in the shape of 4X4X256.Layer blocks after this are just a Convolutional layer with batch normalizations and activation function relu.Just to see and understand what it looks like, let’s look at the model summary:From the shape of 4X4 it is extended up to the size of 128X128 which is our training_data shape.Our generator model takes noise as an input and outputs an image.After initializing both the generator and discriminator model, let’s write a helper function to save the image after some iteration.Our save_images function takes to count and noise as an input.Inside the function, it generates frames from the parameters we’ve defined above and stores our generated image array which are generated from the noise input.After that, it saves it as an image.Now, it’s time for us to compile the models and train them.Let’s write a block of code for that as well:Breaking it down:Here in the first few lines, we have defined our input shape: which is 128X128X3 (image_size, image_size, image_channel).After that, we are using Adam as our optimizer.Note: All the parameters has been sourced from the paper [1].After initializing the optimizer, we are calling our build_discriminator function and passing the image shape then compiling it with a loss function and an optimizer.Since it is a classification model, we are using accuracy as its performance metric.Similarly, in the next line, we are calling our build_generator function and passing our random_input noise vector as its input.It returns a generated image as it’s output.Now, one important part of GAN is we should prevent our discriminator from training.Since we are only training generators here, we do not want to adjust the weights of the discriminator.This is what really an “Adversarial” in Adversarial Network means.If we do not set this, the generator will get its weight adjusted so it gets better at fooling the discriminator and it also adjusts the weights of the discriminator to make it better at being fooled.We don’t want this. So, we have to train them separately and fight against each other.We are then compiling the generative model with loss function and optimizer.After that, we are defining two vectors as y_real and y_fake.These vectors are composed of random 0’s and 1’s values.After that we are creating a fixed_noise: this will result in generating images that are saved later on which we can see it getting better on every iteration.After that, we are going to iterate over our training data with the range of epochs we’ve defined.During the iteration process, we are taking a sample from a real image and putting that on x_real. After that, we are defining a noise vector and passing that to our generator model to generate a fake image in x_fake.Then we are training our discriminator model in both real and fake images separately.Some research has shown that training them separately can get us some better results.After training, we are taking the metric from both models and taking the average.This way we get the metric for the discriminator model, now for the generator model, we are training it on our noise vector and y_real: which is a vector of 1’s.Here we are trying to train the generator. Overtime generator will get better from these inputs and the discriminator will not be able to discriminate whether the input is fake or real.One thing to note here, our combined model is based on the generator model linked directly to the discriminator model. Here our Input is what the generator wants as an input: which is noise and output is what the discriminator gives us.Now in the end we have an if statement which checks for our checkpoint.If it reaches the checkpoint then it saves the current iteration noise and prints the current accuracy of generator and discriminator.This is all for the coding part to create GAN, but we are not finished yet.We have just written code for it, now we have to actually train those models and see the output how it performs.Training GAN in a normal laptop is kind of impossible since it requires high computation power.Normal laptops with normal CPUs cannot handle such huge tasks, so we are going to use Spell: Which is the fastest and most powerful end-to-end platform for Machine Learning and Deep Learning.The spell is a powerful platform for building and managing machine learning projects. The spell takes care of infrastructure, making machine learning projects easier to start, faster to get results, more organized, and safer than managing infrastructure on your own.In every signup with Spell, you can get 10$ free credit!In simple words, we are going to upload our data file in the spell platform and let it handle all our training task. Spell runs our task in their Powerful GPUs so that we don’t have to worry about anything. We can monitor our logs from their Web GUI and all our outputs are saved safely as well.There are few things to cover before running our project at Spell.First off, we must create our account on Spell. There is good and easy documentation to get started on their official page.After account creation, we can install Spell CLI using pip install:This installs all the power of spell into our laptop. We can either use the Web GUI or we can easily log in to the spell server and execute commands from our cmd or bash.To upload our project on Spell, we are going to use command-line tools.Let’s open the command line terminal in the root directory of our project folder and login to the server by using spell login command:After a successful login, now we can upload our training data file and run our code in the Spell server:After that our training_data will be uploaded to the server.Note: Before running code in the server, code has been pushed to the github.Now we are ready to execute our code in the Spell server.In the command line let’s run the following command:The command above runs our code in the Spell server with the Machine type V100 which is a GPU machine. The last argument there mounts the dataset directory so that it can be accessed by our code.Now that code is executed successfully you can see the logs on your console. If you want to monitor in GUI then you can log in to the Web GUI of Spell and see under Runs Section.As you can see, it holds all the information about our recent run.As we have written code for; in every 100th iteration our generated image is saved in the output directory and log with accuracy metric are printed.You can view them in logs section.Awesome isn’t it? You can do your other works while it trains and saves the output for you.Now after the training completes Spell automatically saves our output in the resources/runs directory.After that, we can download the outputs from the runs of Spell to our local machine by using the following command:For this project it will be:You just have to enter the runs/<number of your run> to download the content of that runs.That’s it!! Now you have outputs of the GAN trained on Spell’s GPU machine in your local machine. You can now visualize how your GAN performed from that output images.GANs are an exciting and rapidly changing field, delivering on the promise of generative models in their ability to generate realistic examples across a range of problem domains. It is not an easy task to understand GAN or any Machine Learning and Deep Learning field overnight. It needs patience and a lot of practice plus understanding.In previous days it was not possible for Aspiring ML enthusiast likes us to perform repetitive practice to see what went how. But now, a platform like Spell helps us to provide a system to run and manage our projects so that we can run and test our models.What we have created is just a simple representation of how GAN can be created and what GAN can do. There are still more advanced tweaks yet to perform.To take it further you can tweak the parameters and see how differently it generates the images.There are still a lot of things one can research.For any queries and discussion, you can join the Spell community from here: https://chat.spell.ml/[1] Generative Adversarial Network, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, 2014[2] A Gentle Introduction to Generative Adversarial Networks (GANs), Jason Brownlee, 2019 [Online] https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/[3] A Beginner’s Guide to Generative Adversarial Networks (GANs), Chris, 2019 [Online] https://skymind.ai/wiki/generative-adversarial-network-gan",13/11/2019,32,76.0,16.0,1074.0,417.0,18.0,2.0,0.0,13.0,en
3955,A Brief Overview of Attention Mechanism,SyncedReview,Synced,24000.0,5.0,758.0,"Attention is simply a vector, often the outputs of dense layer using softmax function.Before Attention mechanism, translation relies on reading a complete sentence and compress all information into a fixed-length vector, as you can image, a sentence with hundreds of words represented by several words will surely lead to information loss, inadequate translation, etc.However, attention partially fixes this problem. It allows machine translator to look over all the information the original sentence holds, then generate the proper word according to current word it works on and the context. It can even allow translator to zoom in or out (focus on local or global features).Attention is not mysterious or complex. It is just an interface formulated by parameters and delicate math. You could plug it anywhere you find it suitable, and potentially, the result may be enhanced.The core of Probabilistic Language Model is to assign a probability to a sentence by Markov Assumption. Due to the nature of sentences that consist of different numbers of words, RNN is naturally introduced to model the conditional probability among words.Vanilla RNN (the classic one) often gets trapped when modeling:Translation often requires arbitrary input length and out put length, to deal with the deficits above, encoder-decoder model is adopted and basic RNN cell is changed to GRU or LSTM cell, hyperbolic tangent activation is replaced by ReLU. We use GRU cell here.Embedding layer maps discrete words into dense vectors for computational efficiency. Then embedded word vectors are fed into encoder, aka GRU cells sequentially. What happened during encoding? Information flows from left to right and each word vector is learned according to not only current input but also all previous words. When the sentence is completely read, encoder generates an output and a hidden state at timestep 4 for further processing. For encoding part, decoder (GRUs as well) grabs the hidden state from encoder, trained by teacher forcing (a mode that previous cell’s output as current input), then generate translation words sequentially.It seems amazing as this model can be applied to N-to-M sequence, yet there still is one main deficit left unsolved: is one hidden state really enough?Yes, Attention here.Similar to the basic encoder-decoder architecture, this fancy mechanism plug a context vector into the gap between encoder and decoder. According to the schematic above, blue represents encoder and red represents decoder; and we could see that context vector takes all cells’ outputs as input to compute the probability distribution of source language words for each single word decoder wants to generate. By utilizing this mechanism, it is possible for decoder to capture somewhat global information rather than solely to infer based on one hidden state.And to build context vector is fairly simple. For a fixed target word, first, we loop over all encoders’ states to compare target and source states to generate scores for each state in encoders. Then we could use softmax to normalize all scores, which generates the probability distribution conditioned on target states. At last, the weights are introduced to make context vector easy to train. That’s it. Math is shown below:To understand the seemingly complicated math, we need to keep three key points in mind:We hope you understand the reason why attention is one of the hottest topics today, and most importantly, the basic math behind attention. Implementing your own attention layer is encouraged. There are many variants in the cutting-edge researches, and they basically differ in the choice of score function and attention function, or of soft attention and hard attention (whether differentiable). But basic concepts are all the same. If interested, you could check out papers below.[1] Vinyals, Oriol, et al. Show and tell: A neural image caption generator. arXiv:1411.4555 (2014).[2] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473 (2014).[3] Cho, Kyunghyun, Aaron Courville, and Yoshua Bengio. Describing Multimedia Content using Attention-based Encoder–Decoder Networks. arXiv:1507.01053 (2015)[4] Xu, Kelvin, et al. Show, attend and tell: Neural image caption generation with visual attention. arXiv:1502.03044 (2015).[5] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. End-to-end memory networks. Advances in Neural Information Processing Systems. (2015).[6] Joulin, Armand, and Tomas Mikolov. Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets. arXiv:1503.01007 (2015).[7] Hermann, Karl Moritz, et al. Teaching machines to read and comprehend. Advances in Neural Information Processing Systems. (2015).[8] Raffel, Colin, and Daniel PW Ellis. Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems. arXiv:1512.08756 (2015).[9] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., & Gomez, A. et al. . Attention Is All You Need. arXiv: 1706.03762 (2017).Tech Analyst: Qingtong Wu",26/09/2017,0,1.0,9.0,768.0,297.0,4.0,2.0,0.0,0.0,en
3956,Named Entity Recognition (NER) for CoNLL dataset with Tensorflow 2.2.0,Analytics Vidhya,Bhuvana Kundumani,96.0,5.0,590.0,"This blog details the steps for Named Entity Recognition (NER) tagging of sentences (CoNLL-2003 dataset ) using Tensorflow2.2.0CoNLL-2003 dataset includes 1,393 English and 909 German news articles. We will be looking at the English data. The CoNLL-2003 data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase.Data Preprocessingtrain.txt, valid.txt and test.txt in the data folder have sentences along with their tags. We need only the named entity tags. We extract the words along with their named entities into an array — [ [‘EU’, ‘B-ORG’], [‘rejects’, ‘O’], [‘German’, ‘B-MISC’], [‘call’, ‘O’], [‘to’, ‘O’], [‘boycott’, ‘O’], [‘British’, ‘B-MISC’], [‘lamb’, ‘O’], [‘.’, ‘O’] ]. Refer the code below for the extraction of words along with the named entities. We obtain the words with named entities for all the sentences in train, valid and test.We build the vocabulary for all unique words and unique labels (named entities will be referred to as labels) in the folders (train, valid and test). labelSet contains all the unique words in the labels i.e the named entities. wordSet contains all the unique words.We associate a unique index to each word/ label in the vocabulary. We assign index 0 for ‘PADDING_TOKEN’ and 1 for ‘UNKNOWN_TOKEN’. ‘PADDING_TOKEN’ is used for token at the end of a sentence when one batch has sentences of unequal lengths. ‘UNKNOWN_TOKEN’ is used to represent any word that is not present in the vocabulary,We read the words in the split_train, split_valid and split_test folders and convert the words and labels in them to their respective indices.The sentences are of different lengths. We need to pad the sentences and the labels in order to make them of equal lengths. max_seq_len is taken as 128.We will be using pre-trained Glove word Embeddings. Download the Glove embeddings — glove.6B.100d.txt to the embeddings folder. For all the words in our vocabulary, we get the Glove representation for the words. embedding_vector has the Glove representation for all the words in our vocabulary.Data pipeline using tf.data.Dataset.from_tensor_slicesWe use tf.data.Dataset.from_tensor_slices for batching and shuffling the dataset.Model using Bidirectional LSTMWe have an embedding layer that has the pre-trained Glove word embeddings (not trainable). We use a bidirectional LSTM after the embedding and we have a fully connected layer that transforms the output of the LSTM.We define the model, optimizer and loss. We use sparse categorical cross-entropy loss and Adam optimizer.Custom training loop:Evaluating model performance on test datasetWe use precision, recall and f1 score for evaluating the model. We use seqeval package. seqeval is a Python framework for sequence labelling evaluation. seqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labelling and so on. classification_report metric builds a text report showing the main classification metrics.TensorBoard provides cool visualization and tooling needed for machine learning experimentation. The code in the github repo has code for visualisation of the training and validation loss. Code is available at https://github.com/bhuvanakundumani/NER_tensorflow2.2.0.git",22/08/2020,11,5.0,0.0,486.0,236.0,1.0,0.0,0.0,6.0,en
3957,Sentence correction using Deep learning techniques,Medium,Sourav kumar,28.0,12.0,1722.0,"Most of us use social media platforms to communicate with people or express ourselves in text. Generally, Most of the ML/DL models used this text to determine the sentiments or to predict any criminal activities and many more NLP-related tasks. The ML and DL models are trained in traditional language, mostly English for any NLP-related task.But in actual, we use informal English to communicate with friends especially short forms or abbreviations. So, this kind of text might not be very much helpful in doing NLP-based task.So, it will be better if we convert those short forms or informal words or text to standard English so that it helps most of NLP tasks in various areas like sentimental analysis, chat box. Etc. Therefore, we need to build a model to convert this corrupted text(informal) to standard English, here another important thing is it need to be converted by preserving the semantic meaning of textAs we are giving text as input, and we are also getting text as output i.e. we are converting corrupted text to a standard English one. It’s kind of a machine translation problem i.e. translating the SMS text to standard English one. So, we are going for a deep learning approach.As it is kind of translation like we need to take entire sentence at a time and we need to translate to standard English sentence. So, we consider this problem as of sequence to sequence modelThe performance of most of the NLP based models decrease due to corrupted text or informal text ,for that we need proper english words .Here, input data will have random corruption which is a superset of target data.We will be converting them into target data while preserving the semantic meaning of text.This dataset contains social media text along with their normalized text and Chinese translation of the normalized text. For our problem we need only social media text and their normalized English text. Social media text contains 2000 dataset.Since our data is in txt format which contains SMS text in one-line, Standard English in second line and Chinese Translation of standard English in 3rd line. We would be using only SMS Text and Standard English for our problem Statement. After splitting, converted the txt format into csv files having 2 columns SMS_TEXT and ENGLISH_TEXT.Dataset link: https://www.comp.nus.edu.sg/~nlp/corpora.htmlHere in this problem we have to convert text to text i.e informal to normal english sentences .So, it is clear that it is a machine tranlation probem(text to text).So, we will use seq2seq traditional and some advance models to solve this problem.As mentioned in the research paper, we will be using categorical cross entropy.As we have sequence of input and outputs, so for a given a word we try to output the probability over all output words to which it is denoting. So, we can treat it as multi class. Therefore, we got SoftMax because for each input word we need to get the probability of all output words in vocabulary and also addition to this we also had log which helps to penalize the small errors, so while training the model by using this loss we try to minimize the error in translating the corrupt word to standard English word and finally it helps us to give good results for test data i.e. unseen corrupted text.Credit : https://nlpaug.readthedocs.io/en/latest/augmenter/augmenter.htmlWe will perform 3 kind of augmentation technique to increase the dataset size:After augmentation we have made dataset of size 12k from 2k.In this EDA I will be doing following things:a.Character level analysis :  * Number of characters present in each sentence. This gives us a rough idea about the sentence length. * We will also calculate frequency of texts(datapoints) having same sentence length in our data.b. Word level analysis:2. N-gram exploration:* Generally n-grams help us to know what words are expected before and after a particular word. * It also helps in knowing context of the word. We will be focusing on bi-grams and tri-grams.3. Word Cloud:1.1 Number of characters present in each sentence of informal text.Inference1.The histogram shows that informal text ranges from 0 to 200 characters and generally, it is between 0 to 160 characters.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of character lies between 0 to 160 with some outliers lying in beyond 200 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.8 percetile is around 161 and 99.9 is 202 so this means there are very few words which are greater than 161.So, we will fix the length to 160.5.so we can fix 161 as padd sequence length1..2 Number of characters present in each sentence of normal text:Inference1.The histogram shows that normal text ranges from 0 to 250 characters and generally, it is between 0 to 190 characters.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of character lies between 0 to 190 with some outliers lying in beyond 190 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.7 percetile is around 200 and 99.8 is 215 so this means there are very few words which are greater than 200.So, we will fix the length to 2005.So we can fix 200 as padd sequence length.1.3 Frequency of texts having same sentence length in dataInference1.We can see that maximum percentage of text is around 20–30 range of length as people tends to write short in social media.2.After the informal text in unwrapped into proper sentences we find that the number of sentence length range changes from 20–30 to nearly 20–60 to be most frequent.2.1 Number of words present in informal sentencesInference1.The histogram shows that number of words in informal text ranges from 1 to 40 words.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of words lies between 1 to 35 with some outliers lying in beyond 35 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.9 percetile is around 39 and 100 percentile is 49 so this means there are very few words which are greater than 39.So, we will fix the length to 395.Above 39 all can be considerd as outliers.2.2 Number of words present in normal sentencesInference1.The histogram shows that number of words in normal text ranges from 1 to 40 words.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of words lies between 1 to 35 with some outliers lying in beyond 35 mark.4.We can see that 99.9 percetile is around 48 and 100 percentile is 59 so this means there are very few words which are greater than 48.So, we will fix the length to 485.Above 48 all can be considerd as outliers.2.3 Average Word length and common words in data2.3 Frequency of stop words in dataBy looking at the graphs above we came to know that words “to” and “I” were most frequently used in informal data. While words “you” and “to” have been most frequently used in normal data.1.1 Using Bi-gramsInference1.Looking at the bi-grams of the informal text it seems that the data is personal-type of data. Also, many of the sentences indicate some action such as “to go”, “call me”.2.Looking at the bi-grams of the normal text it seems that the data is personal-type of data. Also, many of the sentences indicate some action such as “are you”, “want to”.Looking at the tri-grams it is safer to say that the data is personal chats kind of data and we are more sure about the data type ,its background.I have taken basic encoder decoder without attention mechanismas with char-char encoding a basline model and got minimum of 0.285 as val lossModel architectureLets see some output of baseline model:Inference :1.Predicted sentences are very bad in baseline model as it is capturing context vector of only the last encoder time step .2.So, we will now move to advanced methods like attention mechanism for better resultsWe will use 3 type of scoring function for itLoss plot of train vs validation:Now we can see that our validation loss is reaching upto 0.15 which was earlier 0.28.So, it is clear that we have increased our accuracy of prediction .Let’s see some output to get it more clear:I have used bleu score as a metric to see how good or bad my translations are and i got these results:We will finalise concat scoring function as our final scoring function for the attention mechanism and make one final function in which we will send one raw data and it will give prediction of the input sentence.1.Let’s plot the distribution of BLEU score to see its pattern.InferenceWe can see that bleu score is kind of following normal distribution plot and important insights that we can observe are:2.Finding relation between sentence length and bleu score of predicted sentenceLet’s see the average length of sentences for all three cases:Predicted sentence: Sharis, Gen asks if we want to meet up today. Are you free? Please reply as soon as possible. original Sentence : Sharis, Gen asks if we want to meet up today. Are you free? Please reply as soon as possible.Predicted sentence: Today then. Where will you be? original Sentence : 2:30 then. Where will you be?>Predicted sentence: Not arrang Millian, you don’t you?original Sentence : TIP,WHAT arR yoU DOING ?>I have deployed this DL model using flask API on localhost. I am taking informal text as input from user and and using the model. pkl file to predict normal text corresponding to the informal text given by user.https://cs224d.stanford.edu/reports/Lewis.pdfwww.appliedaicourse.comarxiv.orgblog.keras.ionlpaug.readthedocs.iostackoverflow.comI would like to thank team Applied AI for their tremendous support and guideline because of which I reached up to this position to do a complete end to end case study on such a interesting problem statement.You can find complete code on my Github hereHappy to connect with you on Linkedin.www.linkedin.com",11/09/2021,0,36.0,11.0,677.0,280.0,29.0,13.0,0.0,10.0,en
3958,Implementation of Principal Component Analysis(PCA) in K Means Clustering,Analytics Vidhya,Wamika Jha,19.0,5.0,717.0,"PrerequisitesThis article assumes that you are familiar with the basic theory behind PCA, K Means Algorithm and know Python programming language.K Means clustering is one of the simplest yet efficient unsupervised algorithms. First let us have a brief description of what this algorithm does.K Means Algorithm Suppose we have a dataset with two features x1 and x2. This is unlabelled data and our objective is to find K number of groups or “clusters” which are similar to each other. Suppose our training set looks like this :-We can clearly see there are two clusters, let us name them cluster 0 and cluster 1. Each cluster is associated with a centroid which is unique to each cluster. This algorithm iterates until the centroids do not change its position.Now that we have a brief description of what K Means does, let us move on to Principal Component Analysis, abbreviated to PCA hereafter.Principal Component AnalysisWe can easily visualise the clusters in the previous example only because it contained two features. What about, say, 1000 features? I don’t know about you, but I definitely can’t visualise 1000 dimensions!Therefore, PCA comes in play. Basic idea behind this is that it allows us to reduce the dimensions (or features) of our dataset to any number less than current number of features.Another use of PCA is to compress the data and hence save the computational time. In the following article, we will use PCA to tackle both the problems.To check which principal component (PC) is more important than others, meaning which one has more weight, we calculate the variance of each components and then plot the screen plots to compare the values. One thing to note is that if your data has different scales, like thisThen it will be better to do feature scaling or mean normalization before proceeding because without scaling, the result will be skewed in favour of A.I will not go into the mathematics behind everything, but you will definitely be able to code it by the end of this article. I am assuming that you are familiar with python and its famous libraries — pandas, numpy, matplotlib and sklearn.About the dataset : It contains 217 columns of hobbies, where 1 means yes.So, first step will be to import all the necessary libraries.Next step will be to load our dataset. I have downloaded a dataset from Kaggle.Next step is data preprocessing. The data has a lot of NaN values, because of which we cannot train the model. So we simply replace those with 0 using this code.As we can see in the dataset that we don’t need the first two columns, so we will assign rest 217 columns to variable X.Let us now move on to building and training the model. Even though it is specified in the dataset that it contains 4 groups, but still we will implement the “elbow method” to determine the number of clusters. This can be done by using WCSS (sum of squares of distances of datapoints)This should give the following graph as outputAs it is evident that there is no particular elbow for this dataset, so in this article I will do it using 6 clusters. You can implement it using 4 clusters as a practice.Next step is to convert our dataset from multidimensions to 2 dimensions.You can specify any number of dimensions/features to reduce to while initialising PCA() functions, but for simplicity sake I have used 2. If you are using more than 2 components, you can use first two components with highest variance value to train and visualise the dataset.Here we have used fit_transform() function to to both fit the dataset x and apply dimensionality reduction to it. This returns an ndarray of 2x2 dimensions.Next we plot and check the variance of the components.This returns the following screenplotNow we will train our model based on the new features generated by PCA(). Since we have only 2 Principal Components (PC1 and PC2), we will get a 2D figure with 6 clusters.This is the scatterplot we get.Hurray! We have successfully coded and implemented K Means clustering with PCA. Give yourself a pat on the back ;)You can checkout the python notebook as well as the dataset on this GitHub link.Here is the Kaggle link for the dataset used in the above article.Thank you for reading!",19/02/2021,8,10.0,4.0,884.0,589.0,6.0,0.0,0.0,3.0,en
3959,"AI-Class.com — A classroom with 160,000 students",A problem like Maria,Maria Neumayer,1500.0,1.0,179.0,"AI Class is a great experiment by two professors at the Stanford University: Sebastian Thrun and Peter Norvig. The course is being held as an actual course at Stanford University plus an online course for about 160,000 enrolled students. People in the advanced track have to do homework and write exams, people in the basic track just have to watch the lectures. Currently I’m in the advanced track, although I might switch to the basic track due to time problems (having a full time job + working on a private application + a University course is a bit too much). At the end of the course you’ll get a certificate, sadly not from Stanford but still. Pretty cool having done a course at Stanford… kind of.I think it’s a great experience to attend a course entirely online with such a big group of people, although that causes the servers to be down quite a lot when a deadline is coming up. It really shows the potential of the Internet. I hope they’ll offer more like that in the future!",23/10/2011,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,en
3960,Multi-Class Text Classification with LSTM,Towards Data Science,Susan Li,25000.0,5.0,206.0,"Automatic text classification or document classification can be done in many different ways in machine learning as we have seen before.This article aims to provide an example of how a Recurrent Neural Network (RNN) using the Long Short Term Memory (LSTM) architecture can be implemented using Keras. We will use the same data source as we did Multi-Class Text Classification with Scikit-Lean, the Consumer Complaints data set that originated from data.gov.We will use a smaller data set, you can also find the data on Kaggle. In the task, given a consumer complaint narrative, the model attempts to predict which product the complaint is about. This is a multi-class text classification problem. Let’s roll!After first glance of the labels, we realized that there are things we can do to make our lives easier.After consolidation, we have 13 labels:Let’s have a look how dirty the texts are:Pretty dirty, huh!Our text preprocessing will include the following steps:Now go back to check the quality of our text pre-processing:Nice! We are done text pre-processing.The plots suggest that the model has a little over fitting problem, more data may help, but more epochs will not help using the current data.Jupyter notebook can be found on Github. Enjoy the rest of the week!Reference:machinelearningmastery.com",10/04/2019,15,0.0,0.0,926.0,263.0,16.0,7.0,0.0,10.0,en
3961,Polynomial Regression — Gradient Descent from Scratch,Towards Data Science,Mark Garvey,44.0,7.0,1152.0,"Gradient descent is an important algorithm to understand, as it underpins many of the more advanced algorithms used in Machine Learning and Deep Learning. Getting to grips with the inner workings of gradient descent will therefore be of great benefit to anyone who plans on exploring ML algorithms further.The best way to learn is by doing, so in this article I will be walking through the steps of how the gradient descent process works, without using ML libraries such as scikit-learn for example. In day-to-day work, it is of course quicker and neater to make use of such libraries, but regarding the learning process I have found the exercise of implementing by hand to be invaluable for this particular algorithm.The goal of gradient descent is to minimize the error of a model’s prediction, relative to the original data. In this article’s context, we will be looking at a second-degree polynomial model, also known as a quadratic equation:Second degree polynomials tend to look something like this, when plotted:We’re specifically looking at polynomial regression here, where the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. Simply put, the coefficients of our second degree polynomial a,b and c will be estimated, evaluated and altered until we can accurately fit a line to the input x data. Gradient descent is the optimization step in this process that alters and improves on the values of these coefficients.We will now look at how to create and plot such a curve, and then build an initial model to fit to this data, which we will then optimize and improve on using gradient descent. If we can get a model that accurately describes the data, the hope is that it should be able to accurately predict the y values of another set of x values.We can start be choosing coefficients for a second degree polynomial equation (𝑎𝑥²+𝑏𝑥+𝑐) that will distribute the data we will try to model:These will be the coefficients for our base/ground truth model we hope to get our predictive model as close as possible to. Next, we need an evaluation function for a second degree polynomial, which, given a set of coefficients and a given input 𝑥 returns the corresponding 𝑦:We can see it in action when x=3:Define some x data (inputs) we hope to predict y (outputs) of:This is good, but we could improve on this by making things more realistic. You can add noise or ‘jitter’ to the values so they can resemble real-world data:Test it out:This updated function will take in the inputs for the second-order polynomial and a jitter value j to add noise to this input, to give us a more realistic output than just a perfect curve:When we build our predictive model, and optimize it with gradient descent, hopefully we will get as close to these values as possible.The first pass at modelling involves generating and storing random coefficients for a second degree polynomial (𝑦=𝑎𝑥² +𝑏𝑥+𝑐). This will be our initial model which will most likely not be that accurate, which we will aim to improve upon until it fits the data well enough.Inspect this model’s accuracy by calculating the predicted output values from your input values:It is evident from the above plot that this new model with random coefficients does not fit our data all that well. To get a quantifiable measure of how incorrect it is, we calculate the Mean Squared Error loss for the model. This is the mean value of the sum of the squared differences between the actual and predicted outputs:Quite a large number. Let’s now see if we can improve on this fairly high loss metric by optimizing the model with gradient descent.We wish to improve our model. Therefore we want to alter its coefficients a, b and c to decrease the error. Therefore we require knowledge about how each coefficient affects the error. This is achieved by calculating the partial derivative of the loss function with respect to each of the individual coefficients.In this case, we are using MSE as our loss function — this is the function we wish to calculate partial derivatives for:With output predictions for our model as:Loss can therefore be reformulated as:In this specific case, our partial derivatives for that loss function are the following:Given coefficients 𝑎, 𝑏 and 𝑐, calculated gradients 𝑔𝑎, 𝑔𝑏 and 𝑔𝑐 and a learning rate 𝑙𝑟, typically one would update the coefficients so that their new, updated values are defined as below:Once you have applied that new model to the data, your loss should have decreased.We need a gradient calculation function which, given a second degree polynomial’s coefficients, as well as a set of inputs 𝑥 and a corresponding set of actual outputs 𝑦 will return the respective gradients for each coefficient.We’re now going to:Let’s set an initial learning rate to experiment with. This should be kept small to avoid the missing the global minimum, but not so small that it takes forever or gets stuck in a local minimum. lr = 0.0001 is a good place to start.Visualize this improvement by plotting the training data, original random model and updated lower-loss model together:We’re almost ready. The last step will be to perform gradient descent iteratively over a number of epochs (cycles or iterations.) With every epoch we hope to see an improvement in the form of lowered loss, and better model-fitting to the original data.Let’s improve on the calc_gradient_2nd_poly function from above, to make it more usable for an iterative gradient descent process:This will be called as part of the gradient_descent function:Finally, let’s train for 1500 epochs and see if our model has learned anything:This trained model is showing vast improvements after it’s full training cycle. We can examine further by inspecting its final predicted coefficients a,b and c:Not too far off! A big improvement over the initial random model. Looking at the plot of the loss reduction over training offers further insights:We observe that the model loss reached close to zero, to give us our more accurate coefficients. We can also see there was no major improvement in loss after about 400 epochs — definitely no need for 1500 epochs. An alternative strategy would be to add some kind of condition to the training step that stops training when a certain minimum loss threshold has been reached. This would prevent excessive training and potential over-fitting for the model.I hope you enjoyed this dive into gradient descent for polynomial regression. Certain concepts can be daunting to understand at first glance, but over time we become familiar with the ‘nuts and bolts’ of a problem if we stick at it long enough. I found this was certainly the case for me with this exercise, and feel it was a worthwhile learning experience overall.If you liked this story, please consider following me on Medium. You can find more on https://mark-garvey.com/Find me on LinkedIn: https://www.linkedin.com/in/mark-garvey/",18/01/2021,6,3.0,28.0,773.0,449.0,15.0,2.0,0.0,7.0,en
3962,From Faces to Kitties to Apartments: GAN Fakes the World,SyncedReview,Synced,24000.0,5.0,751.0,"With just a mouse click, you can delight in mega-litters of adorable kitties, admire countless fresh anime characters, or stare into the twinkling eyes of all sorts of beautiful people. The only catch is that they’re all fake. As Synced previously reported, these hyperrealistic images now flooding the Internet come from US chip giant NVIDIA’s StyleGAN, a generative adversarial network based face generator that performs so well that most people can’t distinguish its creations from photos of real people.Soon after StyleGAN was open-sourced earlier this month, Uber software engineer Philip Wang used the tool to create “This Person Does Not Exist,” a website which generates a new hyperrealistic fake human face every time it’s refreshed. The site quickly went viral and has been covered by major global media. While many comments have praised the realism, more than a few regard the generated faces as a little creepy.In a Facebook post, Wang says he wanted to “raise some public awareness for this technology,” as StyleGAN represents the current state of the art in generative adversarial networks.If the synthetic faces have you wondering “Are you sure this is not a real person?” you’re not alone. University of Washington researchers Jevin West and Carl Bergstrom have created the polling site “Which Face is Real?” which aims “to help you spot these fakes at a single glance.” One emerging consensus on the interactive site seems to be that blurry backgrounds, overly smooth skin, and weird ears are signs to look for when identifying fakes.Theoretically, StyleGan is trained to generate human faces, but it can also learn from and work with other image categories. One thing the Internet can’t seem to get enough of is cats, and so by popular demand Wang has added a kittie generator, “This Cat Does Not Exist.” The faux felines will fool most — although there are also some Frankenstein-esque errors.Hardware Systems Engineer Nathan Glover created a similar website, “These Cats Do Not Exist,” to generate cat images using StyleGAN on AWS SageMaker.The open-sourcing of StyleGAN has illustrated its incredible flexibility, and researchers are still exploring new image targets.Another new StyleGAN-driven site is “This Airbnb Does Not Exist,” where every generated accommodation listing includes pictures showing a short-term rental apartment bedroom, kitchen, living room, etc. The listings also display the name and image of the “host” along with machine-generated descriptive texts that generally have a realistic flow, but often turn surreal. Dorien’s listing for a Berlin “cozy private 2-bed apartment with a vintage bush” for example boasts “a private bathroom with another guest room only. I will provide help throughout the yuming getaway. We love online in productive cheerful oasis.”Google Software Engineer Christopher Schmidt created the fake rental site: “All of the dynamic content on each listing is generated via a series of different machine-learned AI models.” Schmidt used a language model trained on Airbnb listings to create the text, with data from OpenDateSoft’s Airbnb Listings and a model based on TensorFlow’s Predict Shakespeare with Cloud TPUs.A StyleGAN-based synthetic anime-style female face generator also recently went viral, and its bright and colourful (and royalty-free) anime faces have been popping up everywhere on social media. Independent researcher Gwern Branwen developed “This Waifu Does Not Exist,” a simple static website with 70,000 random StyleGAN faces and 70,000 random GPT-2-small text snippets generated using a random seed 1–70,000 and a long prompt with anime-related words and phrases picked by Branwen.Although some humans have scored well on fake detection, GANs’ rapidly improving realism has many worried. OpenAI elected not to release the code for their new language model due to concerns about its potential misuse for malicious purposes such as generating fake news. The controversial decision has challenged the AI community’s traditional open-source philosophy.Concerns regarding malicious use and negative societal impact of generated content may interfere with AI’s long-term development and deployment. It is however also apparent that open-sourced projects like NVIDIA’s StyleGAN can help community researchers achieve state of the art performance in visual processing systems and push neural networks and machine learning to the next level. The tech seems at a crossroads and only time will tell where GANs go next.Journalist: Fangyu Cai | Editor: Michael Sarazen2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon. Apply for Insight Partner Program to get a complimentary full PDF report.Follow us on Twitter @Synced_Global for daily AI news!We know you don’t want to miss any stories. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates.",27/02/2019,0,13.0,18.0,1193.0,626.0,9.0,0.0,0.0,14.0,en
3963,Deep Learning Architectures That You Can Use with a Few Data,The Startup,Gorkem Polat,76.0,7.0,941.0,"Conventional CNNs (AlexNet, VGG, GoogLeNet, ResNet, DenseNet …) have good performances when there are many samples for each class in the dataset. Unfortunately, they generally do not work well when you have a small dataset. However, there are many real-life scenarios where it is challenging to gather data for your classes. For example, in face identification systems, there are generally very few images for each person or in the medical domain, there are limited cases for some rare diseases.So, what does deep learning offer when you have only five samples for your classes, or even one sample for each class? This issue is investigated under the term of few-shot learning. It is an active research area and there are many successful methods that can be adapted. In this article, I will only mention some of the most promising architectures.This article will not explain the architectures in depth because it makes the post very long. Instead, I will only give the architectures’ main idea so that anyone who wants to deal with small data can get a general idea of the models.Siamese Neural Networks [1] take two samples as input and outputs a probability (or loss) for whether given inputs belong to the same class or not. Input samples pass through identical networks (with shared weights) and their embeddings are compared in the cost function (generally a metric based on the difference of the embeddings is used). During the training, “Networks” learn to encode inputs in a more robust way. First, the model is trained on a support set (verification step) to learn the same/different pairs. Then, the test sample is compared with each sample in the training set to get how similar it is to each class (one-shot task) based on learned encodings. It is one of the first successful models in a few-shot learning domain and became the basis for other models.Triplet Network [2] is an extension to the Siamese NNs. Instead of using two samples, Triplet Network uses three samples as input: positive, anchor, and negative samples. Positive and anchor samples are from the same class and negative sample is from a different class. Triplet loss is arranged so that the embedding of the anchor is close to the positive and away from the negative. In this way, networks become more robust to extract embeddings. Triplet Networks have been used in face identification datasets and showed very high performance [3].Matching Networks [4] combine embedding and classification to form an end-to-end differentiable nearest neighbors classifier. Prediction of the model, ŷ, is the weighted sum of the labels, yᵢ, of the training set. The weights are pairwise similarity function, a(𝑥̂, xᵢ), between the query (test) example and support (training) set samples. The key point in the matching networks is the differentiability of the similarity function.Where c represents the cosine similarity function, k is the total number of samples in the training set, and function f and g are the embedding functions for the test and training set. Overall, the similarity is calculated between the embedding of the test sample 𝑥̂ and embedding of samples xᵢ in the training set. The main novelty in this work is that embedding functions are optimized so that they give the maximum accuracy for the classification.Instead of comparing the test sample with all the training samples, Prototypical Networks [5] only compares the test sample with the class prototype (or mean class embedding). The key assumption is that there exists an embedding for each class where samples cluster around a single prototypical representation, cₖ. In their paper, it is showed to have superior performance to Matching networks.Meta-learning means learning-to-learn. Meta-learning tries to train the model’s parameter so that it has maximum performance on a new task through one or more gradient steps (like humans do). The model’s parameters are updated according to the post-updated task-specific parameters so that after a single step for any task, it has the highest performance.The aim of the Model agnostic meta-learning (MAML) is to learn a generic model that can be easily fine-tuned for many tasks with a few iteration steps. For each task in a meta-batch, a model using the weights of the base model is initialized. Stochastic gradient descent (SGD) is used to update the weights of the specific task. Then, the weights of the meta-learner are updated using the sum of losses from the post-update weights. The aim here is that on average for several different tasks, the loss will be small for these parameters.MetaFGNet [7] uses auxiliary data to train a network in addition to the target task network. These two networks share the initial layers (Base network) to learn general information. This method is also named as multi-task learning. Training the auxiliary data (S) together with target data (T) makes a regularization effect on the target training. MetaFGNet also uses a process called sample selection. Samples in the auxiliary data pass through the network and a score is given to the similarity of target classifier, and source classifier is calculated. If the similarity is high, the score is high, too. Only the samples that are above a score threshold are chosen for the training. The main assumption here is that auxiliary data S should have similar distribution as that of the target set T. Results show that this procedure increases the overall performance. Training is performed using the meta-learning approach.There are many other techniques in the few-shot learning domain and their prevalence is increasing in the top computer vision conferences. In this article, only some of them that are proven successful before are mentioned.Here is my another post on setting up an effective deep learning development environment:medium.commedium.com",26/06/2020,0,18.0,32.0,912.0,513.0,13.0,1.0,0.0,2.0,nl
3964,10 rules for better dashboard design,UX Planet,Taras Bakusevych,7300.0,10.0,1943.0,"Dashboard design is a frequent request these days. Businesses dream about a simple view that presents all information, shows trends and risky areas, updates users on what happened — a view that will guide them into a bright financial future.For me, a dashboard — is an at a glance preview of the most crucial information for the user at the moment he is looking at it, and an easy way to navigate directly to various areas of the application that require users attention. The term “dashboard” is a metaphor for a car dashboard, sometimes also called the cockpit area, usually near the front of an aircraft or spacecraft, from which a pilot controls the aircraft.Working on enterprise projects for years, I have designed countless dashboards. And every new one is the next challenge for me. A good dashboard can be a daunting thing to design. Based on my experience, I put together a list of useful suggestions to help you in the future. Whether you just starting, or are seasoned designer, I’m sure you will find something interesting here.Like any other view in your product, the dashboard has a specific purpose that it’s undertaken to serve. Getting this wrong renders your further efforts meaningless. There are multiple popular ways to categorize dashboards based on their purpose(Analytical, Strategic, Operational, Tactical etc). To keep things simple I will divide them into 2 more general forms:Operational dashboards aim to impart critical information quickly to users as they are engaged in time-sensitive tasks. The main goals of the operational dashboard are to present data deviations to the user quickly and clearly, show current resources, and display their status. It’s a digital control room designed to help users be quick, proactive, and efficient.In contrast to Operational dashboards, Analytical dashboards provide the user with at-a-glance information used for analysis and decision making. They are less time-sensitive and not focused on immediate action. A primary goal of this kind of dashboard is to help users make the best sense of the data, analyze trends and drive decision making.The type of dashboard you need should be determined by the user roles and needs you to seek to satisfy. Your product may have multiple roles that should each get a unique dashboard. Lower tier managers may require operational dashboards, while higher management may have a greater need for an analytical dashboard. Often designers mix those, providing the user that suppose to react fast and take action, with a ton of analytics and vise versa.When we talk dashboards, we talk charts). Data representation is a complex task, especially since you will want to display multiple types of information in a dashboard, be it static or dynamic changes over time. This can be quite challenging. Choosing the wrong chart type, or defaulting to the most common type of data visualization could confuse users or lead to data misinterpretation. Before you start, take a look into internal documents and reports to get some inspiration. If you are starting from scratch here are some visualization suggestions that are based on what users need to see:Scatter charts are primarily used for correlation and distribution analysis. Bubble chart helps introduce the third dimension into the chart. A network diagram is handy when even the most minor connection between data points are very important.Using visualization to compare one or many values sets is so much easier than looking at numbers in the grid. Column and line charts are probably the most used. Some recommendations: - When one of your dimensions is a time it should always be an axis X, as time in charts flows from left to right - When using a horizontal or vertical bar chart, try to sort columns by biggest value rather than not randomly sorting them.- With the line graph, charts shouldn’t show more than 5 values and with bar charts, it’s not recommendable to show more than 7 values.Pie and Donut charts have a bad reputation for data visualization. These charts are among the most frequently used, and they are also the most frequently misused charts. They are quite difficult to read when there are too many components or include very similar values. It is hard for humans to differentiate values when it comes to angles and areas.Distribution charts help you to illustrate outliers, the normal tendency, and the range of information in your values.But certain chart types should be entirely avoided. Gauges were a big trend in dashboards in the past, but trying to replicate physical objects digitally is a bad idea. 3D charts and overstyled charts have lower readability, distract the viewer from data, and even more difficult to develop, so there is little reason to use them.To help you choose the right representation type for the chart, ask yourself these questions:- How many variables do you want to show in a single chart? - Will you display values over a period of time, or among items or groups?- How many data points are needed to display for each variable?As the main goal of the dashboard is to get the message across at a glance, every little thing counts. The biggest benefit of using a clear framework is data consistency. If your data is named the same way in each tool, it will be easier for you to use those tools. One framework. No questions.Grids can help you to achieve effective alignment and consistency with little effort and create a basic structure or a skeleton for your design. They consist of “invisible” lines upon which your design elements can be placed. Doing so ties them together in an overall “system” and supports your composition rationally. That’s is crucial for dashboard design as you will need to organize a ton of information in a seamless way.When making decisions on what information should go were, keep this in mind:- The top left corner of the screen will naturally get more attention so try to position key info from left to right. This is based on the way we read information, so it may vary depending on the region of the users for which you are designing. When readers finish with the first row, they will move down to the next one.- If there are dependencies that will affect decisions making on one group of information from based on info from another, create a layout in a way that users do not need to go back and forth — create a continuous flow for easy scanning across the dashboard.After we defined the grid, we can start work with multiple “widgets” that will hold the info, charts, and controls. Cards are easy to arrange. The most important thing about cards is that they are almost infinitely manipulatable. They are a good choice for responsive design since cards act as content containers that easily scale up or down.An important characteristic of cards is the consistent layout of controls and data inside. Put the name in the top left corner, align view controls or actions to in the top right corner of the card, and leave the rest for the content. When all have a consistent structure, it’s easier for the users to work with the interface — they find everything where they expect it.Using the suggested above layout has additional benefits of flexibility when it comes to responsive design or user customization. While the card gets larger or smaller all main components remain anchored to specific locations. This is also beneficial for developers, and the overall scalability of your designs in the future.White space, also known as negative space, is the area between elements in a design composition. Readers aren’t usually aware of the importance of the negative space, but designers pay a lot of attention to it. If the white space is not balanced, a copy will be hard to read. That’s why negative space matters as much as any other typography element.As one of the primary goals of the dashboard is to surface information at a glance, relying on scrolling or many interactions dilutes the whole purpose.Designing long scrollable dashboards is one of the most frequent mistakes designers make. They try to display more information in a clear way, positioning it one under another in order to avoid overwhelming the user. As a result, only the information visible above the screen fold is likely to be discovered by users. Everything below gets little attention from users. So what’s the point? The solution is prioritization. After doing more research and interviews, you should be able to identify core information. You should work only with space above the fold to display it. Don’t tell the full story — summarize instead, and surface only key info. You can use additional interactions as a way to fit more content, and not overwhelm the user with data.Interactions help surface secondary information. Fully relying on them as the main way to work with the dashboard is a big mistake. In the example above we see how a user will have to painfully switch between multiple tabs to get the full picture. This hides information from all other tabs from user, just like content below the fold.Trying to truly make your dashboard informative may lead to extreme cases. We should always remember that humans are bad at keeping track of multiple things at once. Don’t demand too much from your users, and don’t overwhelm them with data. Use a maximum of 5–7 different widgets to create a view. Otherwise, it will be hard for a user to focus and get a clear overview.Users expect that the content they see will be relevant to their individual needs. Personalization and customization are techniques that can help you ensure that users see what matters to them.Personalization is done by the system itself. The system should be set to identify users and deliver to them the content, experience, or functionality that matches their role. Customization is done by the user. A system may enable users to customize or make changes to the experience to meet their specific needs by configuring the layout, content, or system functionality.Giving users more power to customize the dashboard is a good initiative, as long the view is already personalized. Designing more ways to customize is often an excuse to avoid a tedious process of truly finding out what each user role truly needs to see. In the end, the user is left on his own, to build a view for himself.A data table is a great solution when you need to show a lot of information for a large number of items. For example, a list of clients with their ID, status, contacts, last activity, etc., would be best displayed as a data table. There are many other benefits — it’s a great use of space, offers easy scalability, easier development, and users are typically comfortable working with grids as many people are already comfortable working with Microsoft Excel. It’s an easy way to find and change something. You can find out more about the data tables in this article.medium.comSince the dashboard is one of the most visually exciting views, it’s often one of the first things to be designed. I would recommend the opposite. A dashboard is a summary view of everything else and displays key info from various parts of the application. It’s just more practical to design it at the end. Otherwise, you will need to constantly go back and update your dashboard designs while you are working on all the other pages. Furthermore, once a majority of the views are designed, you will have a ton of components to work with when putting together a dashboard. Hope this was useful for you. Thanks for reading through.",17/07/2018,0,0.0,0.0,1400.0,843.0,18.0,0.0,0.0,1.0,en
3965,Transfer Learning — Part — 4.0!! VGG-16 and VGG-19,Becoming Human: Artificial Intelligence Magazine,RAVI SHEKHAR TIWARI,49.0,9.0,1491.0,"In Part 3 of the Transfer Learning series we have discussed the datasets on which these pre-trained model is trained for the ILVRC competition which is held annually and their repository as well as the documentation in order to implement this concept with two API’s namely Keras and PyTorch. In this, article we will discuss theoretically about the VGG-16 and VGG-19 and in article 4.2 and 4.3 we will have practical implementation with Keras and PyTorch API respectively. The link of notebook for setting up the along with the article is given below:becominghuman.aiFor the repository and document please follow below two mentioned links:Keras:keras.ioPyTorch:pytorch.orgAlexNet came out in 2012 and it improved on the traditional Convolutional neural networks, hence we can understand VGG as a successor of the AlexNet but it was created by a group named as Visual Geometry Group at Oxford’s . It was invented by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a datasets of over 14 million images belonging to 1000 classes.Hence the name VGG, It carries and uses some ideas from it’s predecessors and improves on them and uses deep Convolutional neural layers to improve accuracy and other metrics. There are many variant of the VGG architechture some of them are VGG-11, VGG-16 and VGG-19- which has 19.6 billions parameters and was trained om NVIDIA Titan Black GPU’s for weeks on ImageNet dataset.Here we will have a deep insight about the VGG architecture in depth along with their layer and activation. Below Image explains the variant of the VGG network.Column A : Contains 8 CNN layers so total of 11 layers including the fully connected(FC) layers and and has no difference internally except the number of layers.Column A-LRN : This is also similar to the column A but has one extra step of Local response normalization(LRN) which implements lateral inhibition in the layer by which i mean that it makes a significant peak and thus creating a local maxima which increases the sensory perception which we may want in our CNN but it was seen that for this specific case that is ILSVRC it wasn’t increasing accuracy and the overall network was taking more time to train.Column B : These columns just add extra CNN layers and are of 13 layers respectively.Column C : This contains 13 CNN layers and 16 including the FC layers, In this architecture authors have used a conv filter of (1 * 1) just to introduce non-linearity and thus better discrimination.Column D : These columns just add extra CNN layers and are of 16 layers respectively.Column E: These columns just add extra CNN layers and are of 19 layers respectively.2.1 VGG-16In this we will discuss the architecture of the VGG-16 network as it name suggests it is composed of 16 CNN layers and 3 Fully connected layers. The below diagram explains the architecture of the VGG-16 network.As we can see the above diagram accurately depicts the VGG-16 architecture. This architecture is basically composed of 3 types of layers i.e. Convolution layer to extract the feature from the image by employing different number and types of filters, Max-pooling layer to decrease the image size and to extract the feature from the feature map obtained from these filters present in the Convolution layer , Flatten layer to turn the batches of feature maps into 1D tensor and finally 3 Fully-Connected where first two has a dense unit of 4096 layer final classification layer has 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. All the layers has Relu activation except for the final classification layer which has softmax activation for the predicting the probabilities of each class. It is also to be noted that none of the networks (except for one) contain Local Response Normalisation (LRN), such normalization does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.The input to cov1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of convolutional (conv.) layers, where the filters were used with a very small receptive field: 3×3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations, it also utilizes 1×1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 3×3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2×2 pixel window, with stride 2. The column E in figure 1. represents the VGG-19 architecture. The column C in figure 1. represents the VGG-19 architecture.2.2 VGG-19iq.opengenus.orgIn this we will discuss the architecture of the VGG-19 network as it name suggests it is composed of 19 CNN layers and 3 Fully connected layers. The below diagram explains the architecture of the VGG-19 network.As we can see the above diagram accurately depicts the VGG-1 architecture. This architecture is basically composed of 3 types of layers i.e. Convolution layer to extract the feature from the image by employing different number and types of filters, Max-pooling layer to decrease the image size and to extract the feature from the feature map obtained from these filters present in the Convolution layer , Flatten layer to turn the batches of feature maps into 1D tensor and finally 3 Fully-Connected where first two has a dense unit of 4096 layer final classification layer has 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. For the final classification layer which has softmax activation for the predicting the probabilities of each class.1. Why Corporate AI projects fail?2. How AI Will Power the Next Wave of Healthcare Innovation?3. Machine Learning by Using Regression Model4. Top Data Science Platforms in 2021 Other than KaggleA fixed size of (224 * 224) RGB image was given as input to this network which means that the matrix was of shape (224,224,3). The only preprocessing that was done is that they subtracted the mean RGB value from each pixel, computed over the whole training set. The network uses kernels of (3 * 3) size with a stride size of 1 pixel, this enabled them to cover the whole notion of the image and spatial padding was used to preserve the spatial resolution of the image. Max pooling was performed over a 2 * 2 pixel windows with stride 2. It was followed by Rectified linear unit(ReLu) to introduce non-linearity to make the model classify better and to improve computational time as the previous models used tanh or sigmoid functions this proved much better than those.Three fully connected layers from which first two were of size 4096 and after that a layer with 1000 channels for 1000-way ILSVRC classification and the final layer is a softmax function. The column E in figure 1. represents the VGG-19 architecture.In this section, we will discuss the parameter in VGG network. Since we have discussed VGG architecture in above section so, it will be helpful if we you read that section before reading it in order to have more clarity.3.1 VGG-16It constitute of 16 layers respectively . The VGG layer parameters can be seen in below image layer wise:3.2 VGG-19It constitute of 19 layers respectively . The VGG layer parameters can be seen in below image layer wise:The main purpose for which the VGG net was designed was to win the ILSVRC but it has been used in many other ways.In this article we have discussed about the VGG architecture theoretically in next article i.e. 4.2 and 4.3 we will have hands on experience with Keras and PyTorch API’s.Need help ??? Consult with me on DDI :)app.ddichat.comAs we say “Car is useless if it doesn’t have a good engine” similarly student is useless without proper guidance and motivation. I will like to thank my Guru as well as my Idol “Dr. P. Supraja” and “A. Helen Victoria”- guided me throughout the journey, from the bottom of my heart. As a Guru, she has lighted the best available path for me, motivated me whenever I encountered failure or roadblock- without her support and motivation this was an impossible task for me.Pytorch: LinkKeras: LinkTensorflow: Linkif you have any query feel free to contact me with any of the -below mentioned options:YouTube : LinkWebsite: www.rstiwari.comMedium: https://tiwari11-rst.medium.comGithub Pages: https://happyman11.github.io/Articles: https://laptrinhx.com/author/ravi-shekhar-tiwari/Google Form: https://forms.gle/mhDYQKQJKtAKP78V7",01/10/2021,0,19.0,21.0,752.0,504.0,10.0,1.0,0.0,22.0,en
3966,Google DeepMind Releases Structure Predictions for Coronavirus Linked Proteins,SyncedReview,Synced,24000.0,4.0,574.0,"This is an updated version.In a bid to help the global research community better understand the coronavirus, DeepMind today released the structure predictions for six proteins associated with SARS-CoV-2, the virus that causes COVID-19, using the most up-to-date version of their AlphaFold system.As the world struggles with the COVID-19 outbreak, one research team after another in the global scientific community has stepped up to offer expertise, tools and possible solutions. In the early stages of the outbreak front-line labs open-sourced genomes of the virus which enabled other researchers to rapidly develop tests around the pathogen. Other labs modelled the coronavirus infection peak or produced molecular structures to develop drug compounds and treatments against infection.Understanding a protein’s structure provides an important resource for making sense of how a virus functions, but experiments to determine structure typically take months or longer. To speed things up, researchers have been developing computational methods to predict protein structure from amino acid sequences.In January DeepMind published AlphaFold, a deep learning system that aims to accurately predict protein structure even when no structures of similar proteins are available and generates 3D models of proteins with SOTA accuracy. DeepMind says it has continued to improve AlphaFold for better predictions since the release, and they’re now able to share with the public the predicted structures for some of the proteins in SARS-CoV-2 generated with their newly-developed methods.“We confirmed that our system provided an accurate prediction for the experimentally determined SARS-CoV-2 spike protein structure shared in the Protein Data Bank,” DeepMind researchers wrote in their official blog. “We recently shared our results with several colleagues at the Francis Crick Institute in the UK, including structural biologists and virologists, who encouraged us to release our structures to the general scientific community now.”Although the current structure predictions have not yet been peer-reviewed or experimentally verified, given the seriousness and time-sensitivity of the situation, DeepMind decided to release the predicted structures now in the hope the work can contribute to the scientific community’s interrogation of how the virus functions and provide a hypothesis generation platform for future experimental work in developing treatments.The structure predictions, relevant technical details and data can be downloaded here.8/6/2020 Updates:DeepMind continues to improve the AlphaFold system, and the UK-based AI company and research lab today announced the release of an updated version of its SARS-CoV-2 structure predictions for five understudied SARS-CoV-2 targets: SARS-CoV-2 membrane protein, Nsp2, Nsp4, Nsp6, and Papain-like proteinase.On June 17, UC Berkeley’s Brohawn lab released an experimental structure of the SARS-CoV-2 ORF3a protein (Protein 3a) in the Protein Data Bank (PDB). As this experimentally obtained structure and the structures predicted by the AlphaFold system earlier this year were in very good agreement, DeepMind was inspired to release its new set of predictions for the remaining five proteins that have not yet been experimentally determined.The most up-to-date structure predictions can be downloaded here.Journalist: Yuan Yuan | Editor: Michael SarazenThinking of contributing to Synced Review? Synced’s new column Share My Research welcomes scholars to share their own research breakthroughs with global AI enthusiasts.We know you don’t want to miss any story. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates.Need a comprehensive review of the past, present and future of modern AI research development? Trends of AI Technology Development Report is out!2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon.Apply for Insight Partner Program to get a complimentary full PDF report.",06/03/2020,0,21.0,9.0,899.0,436.0,5.0,0.0,0.0,13.0,en
3967,Workflow of a Machine Learning project,Towards Data Science,Ayush Pant,660.0,9.0,1630.0,"In this blog, we will discuss the workflow of a Machine learning project this includes all the steps required to build the proper machine learning project from scratch.We will also go over data pre-processing, data cleaning, feature exploration and feature engineering and show the impact that it has on Machine Learning Model Performance. We will also cover a couple of the pre-modelling steps that can help to improve the model performance.Python Libraries that would be need to achieve the task: 1. Numpy 2. Pandas 3. Sci-kit Learn 4. MatplotlibWe can define the machine learning workflow in 3 stages.Okay but first let’s start from the basicsThe machine learning model is nothing but a piece of code; an engineer or data scientist makes it smart through training with data. So, if you give garbage to the model, you will get garbage in return, i.e. the trained model will provide false or wrong predictions.The process of gathering data depends on the type of project we desire to make, if we want to make an ML project that uses real-time data, then we can build an IoT system that using different sensors data. The data set can be collected from various sources such as a file, database, sensor and many other such sources but the collected data cannot be used directly for performing the analysis process as there might be a lot of missing data, extremely large values, unorganized text data or noisy data. Therefore, to solve this problem Data Preparation is done.We can also use some free data sets which are present on the internet. Kaggle and UCI Machine learning Repository are the repositories that are used the most for making Machine learning models. Kaggle is one of the most visited websites that is used for practicing machine learning algorithms, they also host competitions in which people can participate and get to test their knowledge of machine learning.Data pre-processing is one of the most important steps in machine learning. It is the most important step that helps in building machine learning models more accurately. In machine learning, there is an 80/20 rule. Every data scientist should spend 80% time for data pre-processing and 20% time to actually perform the analysis.Data pre-processing is a process of cleaning the raw data i.e. the data is collected in the real world and is converted to a clean data set. In other words, whenever the data is gathered from different sources it is collected in a raw format and this data isn’t feasible for the analysis. Therefore, certain steps are executed to convert the data into a small clean data set, this part of the process is called as data pre-processing.As we know that data pre-processing is a process of cleaning the raw data into clean data, so that can be used to train the model. So, we definitely need data pre-processing to achieve good results from the applied model in machine learning and deep learning projects.Most of the real-world data is messy, some of these types of data are:1. Missing data: Missing data can be found when it is not continuously created or due to technical issues in the application (IOT system).2. Noisy data: This type of data is also called outliners, this can occur due to human errors (human manually gathering the data) or some technical problem of the device at the time of collection of data.3. Inconsistent data: This type of data might be collected due to human errors (mistakes with the name or values) or duplication of data.1. Numeric e.g. income, age2. Categorical e.g. gender, nationality3. Ordinal e.g. low/medium/highThese are some of the basic pre — processing techniques that can be used to convert raw data.1. Conversion of data: As we know that Machine Learning models can only handle numeric features, hence categorical and ordinal data must be somehow converted into numeric features.2. Ignoring the missing values: Whenever we encounter missing data in the data set then we can remove the row or column of data depending on our need. This method is known to be efficient but it shouldn’t be performed if there are a lot of missing values in the dataset.3. Filling the missing values: Whenever we encounter missing data in the data set then we can fill the missing data manually, most commonly the mean, median or highest frequency value is used.4. Machine learning: If we have some missing data then we can predict what data shall be present at the empty position by using the existing data.5. Outliers detection: There are some error data that might be present in our data set that deviates drastically from other observations in a data set. [Example: human weight = 800 Kg; due to mistyping of extra 0]Our main goal is to train the best performing model possible, using the pre-processed data.In Supervised learning, an AI system is presented with data which is labelled, which means that each data tagged with the correct label.The supervised learning is categorized into 2 other categories which are “Classification” and “Regression”.Classification problem is when the target variable is categorical (i.e. the output could be classified into classes — it belongs to either Class A or B or something else).A classification problem is when the output variable is a category, such as “red” or “blue” , “disease” or “no disease” or “spam” or “not spam”.As shown in the above representation, we have 2 classes which are plotted on the graph i.e. red and blue which can be represented as ‘setosa flower’ and ‘versicolor flower’, we can image the X-axis as ther ‘Sepal Width’ and the Y-axis as the ‘Sepal Length’, so we try to create the best fit line that separates both classes of flowers.These some most used classification algorithms.While a Regression problem is when the target variable is continuous (i.e. the output is numeric).As shown in the above representation, we can imagine that the graph’s X-axis is the ‘Test scores’ and the Y-axis represents ‘IQ’. So we try to create the best fit line in the given graph so that we can use that line to predict any approximate IQ that isn’t present in the given data.These some most used regression algorithms.In unsupervised learning, an AI system is presented with unlabeled, un-categorized data and the system’s algorithms act on the data without prior training. The output is dependent upon the coded algorithms. Subjecting a system to unsupervised learning is one way of testing AI.The unsupervised learning is categorized into 2 other categories which are “Clustering” and “Association”.A set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.Methods used for clustering are:For training a model we initially split the model into 3 three sections which are ‘Training data’ ,‘Validation data’ and ‘Testing data’.You train the classifier using ‘training data set’, tune the parameters using ‘validation set’ and then test the performance of your classifier on unseen ‘test data set’. An important point to note is that during training the classifier only the training and/or validation set is available. The test data set must not be used during training the classifier. The test set will only be available during testing the classifier.Training set: The training set is the material through which the computer learns how to process information. Machine learning uses algorithms to perform the training part. A set of data used for learning, that is to fit the parameters of the classifier.Validation set: Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. A set of unseen data is used from the training data to tune the parameters of a classifier.Test set: A set of unseen data used only to assess the performance of a fully-specified classifier.Once the data is divided into the 3 given segments we can start the training process.In a data set, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set. Usually, a data set is divided into a training set, a validation set (some people use ‘test set’ instead) in each iteration, or divided into a training set, a validation set and a test set in each iteration.The model uses any one of the models that we had chosen in step 3/ point 3. Once the model is trained we can use the same trained model to predict using the testing data i.e. the unseen data. Once this is done we can develop a confusion matrix, this tells us how well our model is trained. A confusion matrix has 4 parameters, which are ‘True positives’, ‘True Negatives’, ‘False Positives’ and ‘False Negative’. We prefer that we get more values in the True negatives and true positives to get a more accurate model. The size of the Confusion matrix completely depends upon the number of classes.We can also find out the accuracy of the model using the confusion matrix.Accuracy = (True Positives +True Negatives) / (Total number of classes)i.e. for the above example:Accuracy = (100 + 50) / 165 = 0.9090 (90.9% accuracy)Model Evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future.To improve the model we might tune the hyper-parameters of the model and try to improve the accuracy and also looking at the confusion matrix to try to increase the number of true positives and true negatives.In this blog, we have discussed the workflow a Machine learning project and gives us a basic idea of how a should the problem be tackled.Implementation of the workflow of an Machine Learning project: https://github.com/NotAyushXD/Titanic-dataset",11/01/2019,0,56.0,0.0,813.0,436.0,16.0,5.0,0.0,6.0,en
3968,ICO TokenGo заканчивается!,Medium,TokenGo Platform_RU,294.0,3.0,631.0,"Дорогие друзья! Подходит к концу май месяц, наступает долгожданное для многих лето. Сегодня я хочу подвести итоги и рассказать о планах на самое ближайшее будущее.Во-первых, сегодня — 31 мая, очень важный для нас день, мы завершаем Баунти-кампанию TokenGo! Выполнен огромный объем задач, распределены все выделенные на баунти-кампанию токены! Руководство платформы TokenGo от всей души благодарит участников-баунтистов за неоценимый вклад в развитие и продвижение наших идей и поздравляет с окончанием большого и важного этапа! Мы надеемся, что все вы продолжите работу в данном направлении в баунти-кампаниях наших партнеров!Во-вторых, хочу ответить на один из самых часто задаваемых вопросов! Можно ли теперь выводить токены? Да. Токены выводить можно! Причем, можно выводить и токены, которые вы приобрели в процессе проведения ICO, и токены, которые получили за выполнение задач Баунти-кампании. Для вывода купленных токенов по-прежнему достаточно подать соответствующую заявку в своем личном кабинете. Вывод токенов осуществляется в течение 24–96 часов. Комиссия на вывод купленных токенов GoPower оплачивается за счет TokenGo.Вывод токенов заработанных в Баунти-кампании осуществляется при условии, что GAS Prise ETH составляет не более 20 GWEI, что в переводе 0.00107066 ETH, которые должны быть оплачены Вами с адреса, на который Вы получаете токены GoPower. Оплата производится на адрес TokenGo https://etherscan.io/address/0x36c5d50007a2ce29ca329cdeac8217a8438dfa23 Адрес для проверки GWEI https://ethgasstation.info/После оплаты Вами комиссии в размере 0.00107066 ETH (примерно 0,62$) и при условии, что GAS Price ETH не более 20 GWEI, вывод токенов GoPower производится в течении 96 часов.Gas — это единица оплаты комиссий при проведении любой операции на платформе Ethereum. Т.е. данная комиссия — это сумма, необходимая для совершения транзакции по переводу токенов GoPower. Так как стоимость газа постоянно изменяется, сумма комиссии, которую вносит участник TokenGo, зафиксирована в ETH. В случае, если стоимость газа превышает 20 Gwei по курсу, транзакция изменит статус на «ожидание» и будет находиться в нем до тех пор, пока газ не будет равен или меньше 20 Gwei — тогда и будет произведен обмен.Без подтвержденного факта оплаты комиссии, вывод заработанных в Баунти-кампании токенов невозможен. Оплата комиссии должна быть произведена именно с того же адреса, на который будут выводиться токены. При запросе токенов на вывод, платформа автоматически найдет оплаченную с этого же адреса комиссию, и разрешит создание транзакции на вывод.Напоминаю, что данный алгоритм работает как временный вариант вывода токенов. Согласно дорожной карте в TokenGo появится собственный блокчейн и будет подключена процедура миграции токенов с блокчейна Ethereum на блокчейн TokenGo согласно правилам, описанным в WhitePaper. Таким образом, вывод токенов не потребует уплаты дополнительных комиссий в ETH. Кроме этого в скором времени будет запущена собственная биржа, а также ожидается выход GoPower на альтернативные криптобиржи. Также стоит заметить, что держатели GoPower обладают обширными привилегиями, одними из которых является возможность участия в минтинге (майнинге) и распределении внутренних монет GoCoin между держателями GoPower пропорционально объему их доли, а также «сила голоса». Подробнее о данных привилегиях также можно прочитать в WhitePaper TokenGo.И третий пункт, о котором я хотел бы сказать вам — это сбор средств посредством процедуры ICO. На текущий момент мы собрали более 5100 ETH, что, как я уже неоднократно говорил, является очень хорошей стартовой суммой для нашего продукта. Весь следующий месяц мы планируем активно заниматься выводом токенов GoPower на подходящие нам биржи, и до наступления этого момента мы приняли решение о продолжении приема средств. Это также обусловлено тем, что оплата участия партнеров в Баунти-кампаниях до выхода токена на биржи оплачивается стандартным методом. Как только токен будет выпущен на первую биржу, мы прекратим прием средств, и процедура ICO будет окончательно завершена.Я благодарю всех, кто поддерживает платформу с истоков ее развития, а также тех, кто только присоединился! В сообществе наша сила! Желаю всем отличного летнего сезона!Больше о TokenGo:Как работает TokenGo. Примеры использованияТокенизация бизнеса на платформе TokenGoКонсенсус TokenGoЭкосистема TokenGoХарвестинг TokenGoОснователь TokenGo Антон Бендерский о перспективах платформы.Основатель TokenGo Антон Бендерский о допущенных ошибках и текущих достижениях.ICO TokenGo — новый этап!TokenGo запускает ICO!Официальный сайт TokenGoRU ветка TokenGo на bitcointalkОфициальный RU блог TokenGoОфициальный EN блог TokenGoWhitepaper RUWhitepaper ENGithubTwitterFacebookTelegram чат",31/05/2018,0,3.0,0.0,1200.0,630.0,1.0,0.0,0.0,23.0,ru
3969,"Multi-Class Metrics Made Simple, Part II: the F1-score",Towards Data Science,Boaz Shmueli,617.0,7.0,1402.0,"In Part I of Multi-Class Metrics Made Simple, I explained precision and recall, and how to calculate them for a multi-class classifier. In this post I’ll explain another popular performance measure, the F1-score, or rather F1-scores, as there are at least 3 variants. I’ll explain why F1-scores are used, and how to calculate them in a multi-class setting.But first, a BIG FAT WARNING: F1-scores are widely used as a metric, but are often the wrong way to compare classifiers. You will often spot them in academic papers where researchers use a higher F1-score as “proof” that their model is better than a model with a lower score. However, a higher F1-score does not necessarily mean a better classifier. Use with care, and take F1 scores with a grain of salt! More on this later.As in Part I, I will start with a simple binary classification setting. Just a reminder: here is the confusion matrix generated using our binary classifier for dog photos. The precision and recall scores we calculated in the previous part are 83.3% and 71.4% respectively.In general, we prefer classifiers with higher precision and recall scores. However, there is a trade-off between precision and recall: when tuning a classifier, improving the precision score often results in lowering the recall score and vice versa — there is no free lunch.Now imagine that you have two classifiers — classifier A and classifier B — each with its own precision and recall. One has a better recall score, the other has better precision. We would like to say something about their relative performance. In other words, we would like to summarize the models’ performance into a single metric. That’s where F1-score are used. It’s a way to combine precision and recall into a single number. F1-score is computed using a mean (“average”), but not the usual arithmetic mean. It uses the harmonic mean, which is given by this simple formula:F1-score = 2 × (precision × recall)/(precision + recall)In the example above, the F1-score of our binary classifier is:F1-score = 2 × (83.3% × 71.4%) / (83.3% + 71.4%) = 76.9%Similar to arithmetic mean, the F1-score will always be somewhere in between precision and recall. But it behaves differently: the F1-score gives a larger weight to lower numbers. For example, when Precision is 100% and Recall is 0%, the F1-score will be 0%, not 50%. Or for example, say that Classifier A has precision=recall=80%, and Classifier B has precision=60%, recall=100%. Arithmetically, the mean of the precision and recall is the same for both models. But when we use F1’s harmonic mean formula, the score for Classifier A will be 80%, and for Classifier B it will be only 75%. Model B’s low precision score pulled down its F1-score.Now that we know how to compute F1-score for a binary classifier, let’s return to our multi-class example from Part I. A quick reminder: we have 3 classes (Cat, Fish, Hen) and the corresponding confusion matrix for our classifier:We now want to compute the F1-score. How do we do that?Remember that the F1-score is a function of precision and recall. And in Part I, we already learned how to compute the per-class precision and recall. Here is a summary of the precision and recall for our three classes:With the above formula, we can now compute the per-class F1-score. For example, the F1-score for Cat is:F1-score(Cat) = 2 × (30.8% × 66.7%) / (30.8% + 66.7%) = 42.1%And similarly for Fish and Hen. We now have the complete per-class F1-scores:The next step is combining the per-class F1-scores into a single number, the classifier’s overall F1-score. There are a few ways of doing that. Let’s begin with the simplest one: an arithmetic mean of the per-class F1-scores. This is called the macro-averaged F1-score, or the macro-F1 for short, and is computed as a simple arithmetic mean of our per-class F1-scores:Macro-F1 = (42.1% + 30.8% + 66.7%) / 3 = 46.5%In a similar way, we can also compute the macro-averaged precision and the macro-averaged recall:Macro-precision = (31% + 67% + 67%) / 3 = 54.7%Macro-recall = (67% + 20% + 67%) / 3 = 51.1%(August 20, 2019: I just found out that there’s more than one macro-F1 metric! The macro-F1 described above is the most commonly used, but see my post A Tale of Two Macro-F1’s)When averaging the macro-F1, we gave equal weights to each class. We don’t have to do that: in weighted-average F1-score, or weighted-F1, we weight the F1-score of each class by the number of samples from that class. In our case, we have a total of 25 samples: 6 Cat, 10 Fish, and 9 Hen. The weighted-F1 score is thus computed as follows:Weighted-F1 = (6 × 42.1% + 10 × 30.8% + 9 × 66.7%) / 25 = 46.4%Similarly, we can compute weighted precision and weighted recall:Weighted-precision=(6 × 30.8% + 10 × 66.7% + 9 × 66.7%)/25 = 58.1%Weighted-recall = (6 × 66.7% + 10 × 20.0% + 9 × 66.7%) / 25 = 48.0%The last variant is the micro-averaged F1-score, or the micro-F1. To calculate the micro-F1, we first compute micro-averaged precision and micro-averaged recall over all the samples , and then combine the two. How do we “micro-average”? We simply look at all the samples together. Remember that precision is the proportion of True Positives out of the Predicted Positives (TP/(TP+FP)). In the multi-class case, we consider all the correctly predicted samples to be True Positives. Let’s look again at our confusion matrix:There were 4+2+6 samples that were correctly predicted (the green cells along the diagonal), for a total of TP=12. We now need to compute the number of False Positives. Since we are looking at all the classes together, each prediction error is a False Positive for the class that was predicted. For example, if a Cat sample was predicted Fish, that sample is a False Positive for Fish. The total number of False Positives is thus the total number of prediction errors, which we can find by summing all the non-diagonal cells (i.e., the pink cells). In our case, this is FP=6+3+1+0+1+2=13. Our precision is thus 12/(12+13)= 48.0%.On to recall, which is the proportion of True Positives out of the actual Positives (TP/(TP+FN)). The TP is as before: 4+2+6=12. How do we compute the number of False Negatives? Taking our previous example, if a Cat sample was predicted Fish, that sample is a False Negative for Cat. More broadly, each prediction error (X is misclassified as Y) is a False Positive for Y, and a False Negative for X. Thus, the total number of False Negatives is again the total number of prediction errors (i.e., the pink cells), and so recall is the same as precision: 48.0%.Since precision=recall in the micro-averaging case, they are also equal to their harmonic mean. In other words, in the micro-F1 case:micro-F1 = micro-precision = micro-recallMoreover, this is also the classifier’s overall accuracy: the proportion of correctly classified samples out of all the samples. To summarize, the following always holds true for the micro-F1 case:micro-F1 = micro-precision = micro-recall = accuracyI mentioned earlier that F1-scores should be used with care. Why? Although they are indeed convenient for a quick, high-level comparison, their main flaw is that they give equal weight to precision and recall. As the eminent statistician David Hand explained, “the relative importance assigned to precision and recall should be an aspect of the problem”. Classifying a sick person as healthy has a different cost from classifying a healthy person as sick, and this should be reflected in the way weights and costs are used to select the best classifier for the specific problem you are trying to solve. This is true for binary classifiers, and the problem is compounded when computing multi-class F1-scores such as macro-, weighted- or micro-F1 scores. In the multi-class case, different prediction errors have different implication. Predicting X as Y is likely to have a different cost than predicting Z as W, as so on. The standard F1-scores do not take any of the domain knowledge into account.Finally, let’s look again at our script and Python’s sk-learn output.Here again is the script’s output. The bottom two lines show the macro-averaged and weighted-averaged precision, recall, and F1-score. The accuracy (48.0%) is also computed, which is equal to the micro-F1 score.This concludes my two-part short intro to multi-class metrics.I hope that you have found these posts useful.Continue to Part III: the Kappa Score",03/07/2019,0,43.0,17.0,734.0,281.0,6.0,0.0,0.0,5.0,en
3970,RSNA 2013 — Top 5 Tendências em TI,Saúde Digital,"Thiago Julio, MD",145.0,3.0,515.0,"Muitas novidades foram apresentadas durante o congresso em Chicago. Muita inovação entre as aulas e sessões. Apresentações científicas com novas aplicações de conhecidas tecnologias e alguns novos protótipos. Diante de tanto conteúdo, seis dias passam rápido para quem gosta de tecnologia. Tentei elencar as cinco coisas mais bacanas que vi em termos de inovação e TI:1. PACS 3.0Termo repetido em inúmeras palestras. Ficou nítido que estamos diante de uma nova geração de PACS. Ferramentas de manipulação de imagens e workflow (manejo de worklists, aplicativos de laudo automatizados e reconhecimento de voz) já são considerados standart, e anualmente melhorados. As próximas versões de PACS, algumas já lançadas durante a feira, deverão contar com novas aplicações, permitindo uma interpretação mais detalhada dos exames e a criação de relatórios personalizados. A automação de processos cruciais como os pedidos (Computerized Physicion Order Entry) e entrega de resultados críticos devem passar a ser ferramentas nativas.2. BI 2.0Não ouvi essa expressão, mas percebi que a radiologia começa uma nova fase no uso de ferramentas de Business Inteligence. Algumas instituições já demonstram uso bem maduro, e os vendors correm para antecipar necessidades. Gestores mostram familiaridade com a utilização de analytics em prospectos e planejamentos, porém há preocupação em saber o que fazer com o crescente fluxo de dados gerados. Enfatizou-se a necessidade de times multidisciplinares dedicados a esta função. “Have a geek radiologist” foi uma das cinco prioridades elencadas por Paul Nagy em um de seus talks. Designers trabalham em opções de visualização mais amigáveis, com dashboards e gráficos, explorando conceitos de data intelligence e design thinking. Desenvolvedores estudam mecanismos de machine learning e integração com demais sistemas hospitalares, adotando modelos preditivos com uso de big data. Bastante excitante!3. Quantitative ImagingOk, o próprio conceito de PACS 3.0 parece cada vez mais indisociável do conceito de imagem quantitativa. A gama de opções tecnológicas que permitem obtenção de dados precisos de medidas (comprimento, volume, realce…) é tanta que exigem esforços para a padronização e controle dos parâmetros. A QIBA (Quantitative Image Biomarkers Alliance) iniciativa pioneira do RSNA mostrou-se bastante madura e colaborativa. Com resultados sólidos de seis anos de colaboração, compartilhou informações importantes e modelos a serem implementados.4. Natural Language ProcessingMecanismos de buscas são também características da nova geração de PACS. A possibilidade de pesquisar por termos (“apendicite”) dentro do sistema de radiologia (laudos, protocolos, pedidos, histórias clínicas) é realidade entre os americanos. Developers e médicos passam a desenvolver novas técnicas e léxicos que permitam buscas semânticas. Deve-se “ensinar” ao computador que “processo inflamatório em estrutura tubular na fossa ilíaca direita” pode ser “apendicite”.5. Image SharingO conceito de PATIENT FIRST foi tônica em todos os keynotes. A preocupação na entrega de valor ao paciente permeia o desenvolvimento de novas tecnologias. O laudo não é mais o produto final, e sim parte dele. Expressivo desta realidade é o envolvimento de grandes players (Dell, DICOM Grid) e a própria RSNA em garantir o acesso integral do paciente aos seus exames via internet, permitindo compartilhamento com múltiplos médicos e hospitais. O RSNA Image Share tem financiamento público e é gratuito aos pacientes.Artigo publicado no Jornal da Imagem",11/01/2014,0,6.0,1.0,1360.0,1005.0,3.0,0.0,0.0,0.0,pt
3971,Extracting image metadata at scale,Netflix TechBlog,Netflix Technology Blog,259000.0,5.0,727.0,"We have a collection of nearly two million images that play very prominent roles in helping members pick what to watch. This blog describes how we use computer vision algorithms to address the challenges of focal point, text placement and image clustering at a large scale.All images have a region that is the most interesting (e.g. a character’s face, sharpest region, etc.) part of the image. In order to effectively render an image on a variety of canvases like a phone screen or TV, it is often required to display only the interesting region of the image and dynamically crop the rest of an image depending on the available real-estate and desired user experience. The goal of the focal point algorithm is to use a series of signals to identify the most interesting region of an image, then use that information to dynamically display it.We first try to identify all the people and their body positioning using Haar-cascade like features. We also built haar based features to also identify if it is close-up, upper-body or a full-body shot of the person(s). With this information, we were able to build an algorithm that auto-selects what is considered the “best’ or “most interesting” person and then focuses in on that specific location.However, not all images have humans in them. So, to identify interesting regions in those cases, we created a different signal — edges. We heuristically identify the focus of an image based on first applying gaussian blur and then calculating edges for a given image.Here is one example of applying such a transformation:Below are a few examples of dynamically cropped images based on focal point for different canvases:Another interesting challenge is determining what would be the best place to put text on an image. Examples of this are the ‘New Episode’ Badge and placement of subtitles in a video frame.In both cases, we’d like to avoid placing new text on top of existing text on these images.Using a text detection algorithm allows us to automatically detect and correct such cases. However, text detection algorithms have many false positives. We apply several transformations like watershed and thresholding before applying text detection. With such transformations, we can get fairly accurate probability of text in a region of interest for image in large corpus of images.Images play an important role in a member’s decision to watch a particular video. We constantly test various flavors of artwork for different titles to decide which one performs the best. In order to learn which image is more effective globally, we would like to see how an image performs in a given region. To get an overall global view of how well a particular set of visually similar images performed globally, it is required to group them together based on their visual similarity.We have several derivatives of the same image to display for different users. Although visually similar, not all of these images come from the same source. These images have varying degrees of image cropping, resizing, color correction and title treatment to serve a global audience.As a global company that is constantly testing and experimenting with imagery, we have a collection of millions of images that we are continuously shifting and evolving. Manually grouping these images and maintaining those images can be expensive and time consuming, so we wanted to create a process that was smarter and more efficient.These images are often transformed and color corrected so a traditional color histogram based comparison does not always work for such automated grouping. Therefore, we came up with an algorithm that uses the following combination of parameters to determine a similarity index — measurement of visual similarity among group of images.We calculate similarity index based on following 4 parameters:Using all 4 methods, we can get a numerical value of similarity between two images in a relatively fast comparison.Below is example of images grouped based on a similarity index that is invariant to color correction, title treatment, cropping and other transformations:Images play a crucial role in first impression of a large collection of videos, and we are just scratching the surface on what we can learn from media and we have many more ambitious and interesting problems to tackle in the road ahead.If you are excited and passionate about solving big problems, we are hiring. Contact us.— by Apurva KansaraOriginally published at techblog.netflix.com on March 21, 2016.",21/03/2016,1,0.0,5.0,1077.0,721.0,13.0,1.0,0.0,5.0,en
3972,Machine Learning for Beginners: An Introduction to Neural Networks,Towards Data Science,Victor Zhou,841.0,9.0,1286.0,"Here’s something that might surprise you: neural networks aren’t that complicated! The term “neural network” gets used as a buzzword a lot, but in reality they’re often much simpler than people imagine.This post is intended for complete beginners and assumes ZERO prior knowledge of machine learning. We’ll understand how neural networks work while implementing one from scratch in Python.Let’s get started!Note: I recommend reading this post on victorzhou.com — much of the formatting in this post looks better there.First, we have to talk about neurons, the basic unit of a neural network. A neuron takes inputs, does some math with them, and produces one output. Here’s what a 2-input neuron looks like:3 things are happening here. First, each input is multiplied by a weight:Next, all the weighted inputs are added together with a bias b:Finally, the sum is passed through an activation function:The activation function is used to turn an unbounded input into an output that has a nice, predictable form. A commonly used activation function is the sigmoid function:The sigmoid function only outputs numbers in the range (0,1). You can think of it as compressing (−∞,+∞) to (0,1) — big negative numbers become ~0, and big positive numbers become ~1.Reminder: much of the formatting in this article looks better in the original post on victorzhou.com.Assume we have a 2-input neuron that uses the sigmoid activation function and has the following parameters:w=[0, 1] is just a way of writing w1​=0, w2​=1 in vector form. Now, let’s give the neuron an input of x=[2, 3]. We’ll use the dot product to write things more concisely:The neuron outputs 0.999 given the inputs x=[2,3]. That’s it! This process of passing inputs forward to get an output is known as feedforward.Time to implement a neuron! We’ll use NumPy, a popular and powerful computing library for Python, to help us do math:Recognize those numbers? That’s the example we just did! We get the same answer of 0.999.A neural network is nothing more than a bunch of neurons connected together. Here’s what a simple neural network might look like:This network has 2 inputs, a hidden layer with 2 neurons (h1​ and h2​), and an output layer with 1 neuron (o1​). Notice that the inputs for o1​ are the outputs from h1​ and h2​ — that’s what makes this a network.A hidden layer is any layer between the input (first) layer and output (last) layer. There can be multiple hidden layers!Let’s use the network pictured above and assume all neurons have the same weights w=[0,1], the same bias b=0, and the same sigmoid activation function. Let h1​, h2​, o1​ denote the outputs of the neurons they represent.What happens if we pass in the input x=[2, 3]?The output of the neural network for input x=[2,3] is 0.7216. Pretty simple, right?A neural network can have any number of layers with any number of neurons in those layers. The basic idea stays the same: feed the input(s) forward through the neurons in the network to get the output(s) at the end. For simplicity, we’ll keep using the network pictured above for the rest of this post.Let’s implement feedforward for our neural network. Here’s the image of the network again for reference:We got 0.7216 again! Looks like it works.Say we have the following measurements:Let’s train our network to predict someone’s gender given their weight and height:We’ll represent Male with a 0 and Female with a 1, and we’ll also shift the data to make it easier to use:Before we train our network, we first need a way to quantify how “good” it’s doing so that it can try to do “better”. That’s what the loss is.We’ll use the mean squared error (MSE) loss:Let’s break this down:(y_true​−y_pred​)² is known as the squared error. Our loss function is simply taking the average over all squared errors (hence the name mean squared error). The better our predictions are, the lower our loss will be!Better predictions = Lower loss.Training a network = trying to minimize its loss.Let’s say our network always outputs 00 — in other words, it’s confident all humans are Male 🤔. What would our loss be?Here’s some code to calculate loss for us:Nice. Onwards!Liking this post so far? I write a lot of beginner-friendly ML articles. Subscribe to my newsletter to get them in your inbox!We now have a clear goal: minimize the loss of the neural network. We know we can change the network’s weights and biases to influence its predictions, but how do we do so in a way that decreases loss?This section uses a bit of multivariable calculus. If you’re not comfortable with calculus, feel free to skip over the math parts.For simplicity, let’s pretend we only have Alice in our dataset:Then the mean squared error loss is just Alice’s squared error:Another way to think about loss is as a function of weights and biases. Let’s label each weight and bias in our network:Then, we can write loss as a multivariable function:Imagine we wanted to tweak w1​. How would loss L change if we changed w1​? That’s a question the partial derivative can answer. How do we calculate it?Here’s where the math starts to get more complex. Don’t be discouraged! I recommend getting a pen and paper to follow along — it’ll help you understand.If you have trouble reading this: the formatting for the math below looks better in the original post on victorzhou.com.To start, let’s rewrite the partial derivative in terms of ∂y_pred/∂w1​​ instead:We can calculate ∂L/∂y_pred​ because we computed L= (1−y_pred​)² above:Now, let’s figure out what to do with ∂y_pred/∂w1. Just like before, let h1​, h2​, o1​ be the outputs of the neurons they represent. ThenSince w1​ only affects h1​ (not h2​), we can writeWe do the same thing for ∂h1​​/∂w1:x1​ here is weight, and x2​ is height. This is the second time we’ve seen f′(x) (the derivate of the sigmoid function) now! Let’s derive it:We’ll use this nice form for f′(x) later.We’re done! We’ve managed to break down ∂L/∂w1​ into several parts we can calculate:This system of calculating partial derivatives by working backwards is known as backpropagation, or “backprop”.Phew. That was a lot of symbols — it’s alright if you’re still a bit confused. Let’s do an example to see this in action!We’re going to continue pretending only Alice is in our dataset:Let’s initialize all the weights to 1 and all the biases to 0. If we do a feedforward pass through the network, we get:The network outputs y_pred​=0.524, which doesn’t strongly favor Male (0) or Female (1). Let’s calculate ∂L/∂w1​​:Reminder: we derived f′(x)=f(x)∗(1−f(x)) for our sigmoid activation function earlier.We did it! This tells us that if we were to increase w1​, L would increase a tiiiny bit as a result.We have all the tools we need to train a neural network now! We’ll use an optimization algorithm called stochastic gradient descent (SGD) that tells us how to change our weights and biases to minimize loss. It’s basically just this update equation:η is a constant called the learning rate that controls how fast we train. All we’re doing is subtracting η ∂w1/​∂L​ from w1​:If we do this for every weight and bias in the network, the loss will slowly decrease and our network will improve.Our training process will look like this:Let’s see it in action!It’s finally time to implement a complete neural network:You can run / play with this code yourself. It’s also available on Github.Our loss steadily decreases as the network learns:We can now use the network to predict genders:You made it! A quick recap of what we did:There’s still much more to do:I may write about these topics or similar ones in the future, so subscribe if you want to get notified about new posts.Thanks for reading!Originally posted on victorzhou.com.",06/03/2019,0,24.0,89.0,1216.0,317.0,33.0,5.0,0.0,27.0,en
3973,Using MaskRCNN to predict tropical fruits in custom dataset,Analytics Vidhya,Bernardo Caldas,21.0,6.0,446.0,"Application to predict fruits using Mask_RCNN on custom dataset, this is a easy tutorial to how create a object detection application for a custom dataset, as a sample we are using a dataset of tropical fruits in this case only ( Oranges and Pineapple).source code in github : https://github.com/bernardcaldas/object-detection-custom-maskrcnnin recent years we can see a lot applications in our life including, autonomous cars, facial detections app, education, military, finance etc.Instance segmentation it's a task to identifying objects , detecting and delineating each distinct object of interest appearing in an image.Follow the post created for Waleed Abdulla explaining how works Mask R-CNN one of the most used algorithm for image segmentation and detection objects.engineering.matterport.comFor this task , that is a very nice tool to download images from google images named OIDv4_ToolKit, after clone the repo and install the required packages with the command line bellow we can download the images.using the tool VIA, we can annotating the images in dataset like the image bellowVIA tool link : https://www.robots.ox.ac.uk/~vgg/software/via/via-1.0.6.htmlafter annotating all images we need generate a json file with all coordinates to using in the project, as standard ‘via_region_data.json'.Let’s clone the repo from matterport mask rcnn and install the required packagesgithub.comMake a copy from the directory 'samples/balloon' and rename with name of your project in this case 'samples/tropical'create a 'train' and 'val' folders inside this structure and move your files annotated before with json file, please consider split 80% train and 20% validations files on your structure.Structure project-tropical | | ballonn.pylets setup the code in 'samples/tropical/balloon.py', first rename this file from the custom dataset in this case 'samples/tropica/fruits.py'.inside the file now fruits.py lets make some change as follow bellow.# balloonconfing to fruitsConfig#balloonDataset to fruitsDatasetUsing one of the codes bellow we will train the model using weights from COCO model or imagenet.the weigths from this training will be saved in logs folder from each epoch, we can see the results and metrics from our model using tensorboard, please type:# tensorboard — logdir =path/to/logsresults will be similar like this running in local port 6006Now we have the model trained lets run, the notebook named inspect_balloon_model.ipynb inside the folder and configure to see the results;function to predict single files;results;The model need some improvements to fix, this mode were train with only 76 images, maybe we can set more epochs and more images to obtain nice results and see using tensorboard the training loss and validation lossthe 'mrcnn/config.py' we can set some settings to improve our model.In this post i just show how to use this repo to detect some objects in custom dataset, feel free to use this tool, follow my contact if you want to say something.twitter ; https://twitter.com/bernardocalda10Github projecthttps://github.com/bernardcaldas/object-detection-custom-maskrcnn/tree/master/tropical",26/01/2020,4,0.0,0.0,902.0,575.0,10.0,3.0,0.0,8.0,en
3974,[TensorFlow 2.0] Variational Auto encoder (VAE) Part II,Medium,A Ydobon,87.0,5.0,620.0,"www.tensorflow.orgLet’s get started!In the previous posting, we have finished two things, first, loading the dependent libraries to our workspace,second, loading the MNIST dataset that we will work on.As you can see from the above code lines, we have not loaded the labels from the MNIST dataset. Unlike the other times, we have only got the input image vectors, leaving the labels behind.Next, we will start to preprocess the MNIST dataset. The preprocessing steps are the following:Reshape the 784 one dimensional vector into a 28x28 square shape vector. And we will set each value’s data type as ‘float’ by specifying “astype(‘flaot32’)” at the end of each code.We will normalize each value to fit in the span of [0., 1.] by dividing the value with 255. (Each value is in between of 0 to 255, signifying the intensity of the pixel).We will binarize the dataset by assigning “1.” to the value which is greater or equal to “0.5”, but assigning “0.” to the values lower than “0.5”. Basically “0.5” is our threshold to binarize our input dataset. However, if you would like, you can change this threshold value and check the performance later on.And lastly, we will set up the necessary parameters to create our dataset before we jump into tf.data part.In machine learning, it is far more efficient to train our model in batches, in other words, training the model with a group of the dataset, rather than training with one unit of dataset each time is preferred.So, we shuffle the data with the number of “TRAIN_BUF” and “TEST_BUF” for each training and test dataset. After the shuffle, with the shuffled data, we group them with the size of “BATCH_SIZE”.We will create a class containing every essential component for the autoencoder: Inference network, Generative network, and Sampling, Encoding, Decoding functions, and lastly Reparameterizing function.First, let’s see the “def __init__()” part. This part is to create necessary variables within the class. We can observe that every variable declaration starts with “self.”. This is the rule that we should accept. We can see that we will need and use ‘latent_dim’ value, “inference_net” and “generative_net”.Let’s start with the inference network block, which starts with “self.inference_net”. First, we take the normalized input image vector with “InputLayer”, specifying its shape with “(28, 28, 1)”. After that 3x3 size kernel (“kernel_size = 3”) convolves on the input image vector, having the stride-step 2 (“strides = (2,2)”), and output is 32-filtered vector (“filters = 32”).Likewise, the 32 filtered ones again go to the convolutional layer with “Conv2D” and output 64 filtered vectors.Lastly, we make the output vector to be flattened as a single-dimensional vector with the method function “Flatten()”.Next, the generative network. In easy terms, this network work in the opposite way to the inference network. We can spot some contrasting codes in this block, such as “Conv2DTranspose” as opposed to the normal “Conv2D” layer. It is like de-convolutioning the input so that the generative network and the inference network become each other’s mirror image in the whole neural networks’ architecture design.This generative network’s input data is a latent encoding vector that comes from a unit Gaussian distribution p(z). The output of the network is a conditional distribution p(x|z). This input vector is produced by the “sample” function below.As we have set before, 100 signifies the number of the batch size, and the “self.latent_dim” would be the number of columns for our matrix input.We have made major components for our convolutional autoencoder, and what we left is to put each of them in the right place to train and learn the parameters.The next part III will be about making and setting up a loss function and structuring the codes to train the model with the preprocessed dataset.:) !!",17/11/2019,8,0.0,5.0,605.0,258.0,3.0,0.0,0.0,1.0,en
3975,DeepMind et al Paper Trumpets Graph Networks,SyncedReview,Synced,24000.0,3.0,530.0,"The paper Relational inductive biases, deep learning, and graph networks, published last week on arXiv by researchers from DeepMind, Google Brain, MIT and University of Edinburgh, has stimulated discussion in the artificial intelligence community. The paper introduces a new machine learning framework called Graph Networks, which some believe promises huge potential for approaching the holy grail of artificial general intelligence.Due to the development of big data and increasingly powerful computational resources over the past few years, modern AI technology — primarily deep learning — has show its prowess and even outsmarted humans in tasks such as image recognition and speech detection. However, AI remains challenged by tasks that involve complicated learning and reasoning with limited experience and knowledge, which is exactly what humans are good at. Although “a word to the wise is sufficient,” machines require much more.The paper argues that Graph Networks can effectively support two critical human-like capabilities: relational reasoning — ie drawing logical conclusions of how different objects and things relate to one another; and combinatorial generalization — ie constructing new inferences, predictions, and behaviors from known building blocks.Graph Networks can generalize and extend different types of neural networks that perform calculations on graphs, and implement relational inductive bias, a capacity for reasoning about inter-object relations.The GN framework is based on Graph Network blocks, also referred to as “graph-to-graph” modules. Each graph’s features are represented in three forms: nodes, edgesas relations, and global attributes as system-level properties.The Graph Network block will take a graph as an input, perform calculations from the edge, to the node, and to the global attributes, and then come up with a new graph as an output.The 38-page paper has been met favorably by many AI researchers, who praised the authors’ efforts. Founder of AI chip unicorn Graphcore Christopher Gray tweeted that “this paper…will kickstart what seems to be a far more fruitful basis for AI than DL alone.” Oriol Vinyals, a renowned research scientist at DeepMind, praised the paper as “a pretty comprehensive review.”Meanwhile, some questioned how well GNs will live up to the hype. As this is a review paper, it does not offer any convincing experiment results. Graph Networks are thus far an early-stage research theory that still requires more proof.The Graph Network concept was spawned with ideas not only from AI research, but also from computer and cognitive sciences. The paper emphasizes that “just as biology does not choose between nature versus nurture — it uses nature and nurture jointly, to build wholes which are greater than the sums of their parts — we, too, reject the notion that structure and flexibility are somehow at odds or incompatible, and embrace both with the aim of reaping their complementary strengths.”* * *Journalist: Tony Peng | Editor: Michael Sarazen* * *Follow us on Twitter @Synced_Global for more AI updates!* * *Subscribe here to get insightful tech news, reviews and analysis!* * *Synced and TalkingData will be jointly holding DTalk Episode One: Deploying AI in Mobile-First Customer-facing Financial Products: A Tale of Two Cycles. Jike Chong will share his ideas on employing AI techniques in FinTech business model. Scan the QR code to register! See you on June 21st in Silicon Valley.",15/06/2018,0,20.0,8.0,1113.0,587.0,4.0,0.0,0.0,3.0,en
3976,Data Science for Newbies (including me!) ,Towards Data Science,Helena Campbell,307.0,5.0,1046.0,"Data Science for Newbies (including me!)I’ve studied math, I’ve studied computer science, and of course I’ve focused on machine learning algorithms. But I’m still new to the field of data science. I don’t know yet how or whether I can make an impact. But if I explain what it is, then people will know what I can do, what I could learn to do, and most importantly, what they can ask me to do. Here’s the primer.There are several types of machine learning algorithms, but my focus is on finding patterns in data. Those patterns could be entirely numerical, they could be graphical, or they could even be written out in words. Humans are very good at finding patterns, even going too far sometimes and making stereotypes. We’re at a point where many people just have petabytes of data, and a human just can’t sift through all that. On the other hand, you start to have enough information that you can make some judgments and predict based on the data you have.For example, let’s take features such as temperature, whether it’s a holiday, how windy it is, and time of day. Let’s say you’re given a lot of data that sort, and another thing: how many people rented a bike under these circumstances. I can build a model based on that. Then, based on that, when I get a novel set of features (a combination of temperature, holiday, windiness, and time that I’ve never seen before), I can predict how many people are going to rent a bike under those circumstances. Under such a model, a company like Zagster or Citibike can decide, for instance, what a good time to do maintenance on their bikes is. Cool, huh? There are all sorts of algorithms for such a prediction, but the big rule is:NO FREE LUNCHES.Which means that you can’t know a priori what the best model will be. That is to say, you don’t know what algorithm will be any good before you know the problem. And there are a bunch of algorithms! What a hassle, right? You use your domain knowledge and expertise to decide what the best model builder will be. You can also use your knowledge of math and algorithms to help it, but hey, a lot of times? I just experiment with different algorithms until I figure out which one works the best.In the previous example, you got an important piece of information to build your model: the actual number of people who rented a bike. You train your model on that set so it figures out the best numbers to use, and then you can predict forever on each novel set of features. But wait! What if you didn’t get the output? What can you do with that kind of information?Instead of using it to make these kinds of predictions, there are other algorithms that will do clustering. Let’s say you have a bunch of data that ends up graphing like this:Well, you could do clustering on this data. It clearly delineates into two clusters. Just draw a circle around each, and you’re good to go. But what if you have 100 dimensions instead of 2? A bit harder to draw — and actually, a bit harder to calculate. But you can still run an algorithm to figure out where the separations are. In my example, I haven’t given you enough information to answer the question you’d probably want to know: is the tumor malignant or benign? Maybe I don’t have that information. But clustering into segments like this:can give a good idea of the relationships between the characteristics. You end up creating an output based on features of your own, or at least know enough to say, “In my next cancer treatment study, I’m going to test the effects on people who have the smaller radii and clump thicknesses and compare to the big radii and large clump thicknesses.” Or you might say, “What do all these people in cluster 1 have in common? How come cluster 0 has such few people in it?” For me, it comes down to asking interesting questions. Combine the data and see patterns you’ve never seen before!The last little thing I wanted to mention is something called reinforcement learning. An example of reinforcement learning might be teaching a robot to walk through a house with a goal of getting to the opposite corner . It uses echolocation, but doesn’t know how to treat the sounds it gets back. Every time it bumps into your desk or a dresser, you give it a negative reward (maybe it gets -10 points for bumping). When it gets to the goal, you give it 100 points! Happy robot! You keep doing these trials over and over again until it figures out how to roll across the room without smacking into things. In fact, you could probably take it to another room, and if the rooms are similar enough, it could probably walk through that one with no problem, too.Good news, you’ve trained an adorable echolocation robot. You also learned about the class of reinforcement learning algorithms. One way to apply this might be to have people play games with positive and negative reward. If they play the game the way you want them to, you reward them positively. If they don’t, take away points (or just have them lose). What’s that, you say? Can’t train humans? Well, you just think back to how many hours you spent doing all the quests in Dragon Age (and you read the lore, too, didn’t you?). You are right that reinforcement learning is often used to train a patient computer how to do something.That’s the primer to machine learning! I taught you about supervised learning, clustering (unsupervised learning), and reinforcement learning. Supervised learning means you have the answers (or outputs) for a bunch of examples, and train from that to be able to answer questions about novel inputs. Clustering allows you find relationships between things, without knowing what the output should look like. Reinforcement learning is all about training robots. Now, start asking questions about what kinds of inputs you have, what kinds of outputs you see, and what kinds of outputs you would like to see. I’ll answer them for you.",15/09/2015,0,0.0,0.0,868.0,645.0,3.0,0.0,0.0,0.0,en
3977,Attacking Google Cloud Vision API with Adversarial Examples,Medium,NN Intruder,4.0,15.0,2927.0,"Adversarial attacks have been a concerning topic in the field of deep learning research in recent years. We’ve long since known that deep neural networks don’t generate perfect classification boundaries (this article in 2013. .. Yes, 2013 is a long time ago in fields related to deep learning.). Researchers have found numerous ways to generate adversarial examples to cause models to make mistakes (see e.g. this review paper and reference therein). This is obviously dangerous in commercial applications such as self-driving cars, automated robots, and other audio/visual recognition tasks. The vulnerability to adversarial examples is one of the major risks for applying deep neural networks in safety-critical scenarios.Before we go into our implementation, we need to categorize a few methods for generating adversarial examples.Adversary’s KnowledgeAn attack conducted by an adversary who potentially knows everything related to the trained model including, but not limited to training data, network architectures, hyperparameters, numbers of layers, activations, and layer weights.An attack conducted by an adversary who has no access to the trained model. This adversary can only act as a standard user and only knows the output of the model, for example, a label or confidence scoreAttack SettingWhen an attacker is able to receive a complete list of labels and logits in the form of probabilities from the modelThe attacker has a limited number of queries to the classifier. This can be a result of limits on resources such as time or query cost.The attacker only has access to the probabilities or “confidence scores” in the top k classes. If the classified outputs a “confidence score” instead of a probability, these number may not even sum to 1 across the classes.The adversary only has access to a list of k inferred labels ordered by their predicted probabilities and does not have access to class probabilities or scores.Adversarial SpecificityAn attack that causes a deep neural network classifier to maximize its probability of returning a specific incorrect class.An attack that does not require the attacked neural network to output any specific class, only an incorrect one. The adversarial class may be arbitrary except that it cannot be the correct label. These generally succeed more easily compared to targeted attacks because they have more options and space to redirect the output. They are usually generated by either running several targeted attacks and taking the one with the smallest perturbation or by minimizing the probability of the correct class.Attack GeneralizationGenerate a different perturbation for each different clean inputCreate a universal perturbation for the whole dataset to be applied to all clean input data.Attack FrequencyGenerate an adversarial example in a single stepGenerate an adversarial example by updating in in multiple sequential steps. These usually produce better adversarial examples, but require more interactions with the attacked model, which can be bad if there is a query limit, and require more computational time to generate them.Many successful adversarial attacks in past research were generated under ideal conditions, with either white-box attacks or at least full information. Our adversarial attack was generated as a black box attack with partial information constraints. The Google Cloud Vision API returned only labels that it determined as “relevant” and sent back “confidence scores” for only a number of the top classes (the number varies between queries) that did not sum to one.Our goal was to use 3 attack algorithms: Natural Evolution Strategy, Boundary Attack, and Zeroth Order Optimization to attack the commercial APIs of Google Cloud Vision (GCV) and Clarif.ai. These algorithms needed to submit queries to the commercial APIs and process the returned partial information labels and scores into a loss function that could be used to approximate a viable gradient for our algorithms. These gradients would be used to apply perturbations to some source images in our data in order to trick the commercial classifiers into returning incorrect labels. Our objective was to see if each of these algorithms could work against trusted APIs in a partial information setting and discover any difficulties that arose with such attacks.For our algorithms, our data source was not necessarily that important. We did not use a dataset-dependent supervised learning technique because we were simply taking an input image, attacking the commercial API, getting feedback, and updating the adversarial image using an estimated gradient from the label scores.That being said, we did use data from a subset of ImageNet images, CIFAR-10 images, and a development dataset from a NIPS: Adversarial Attacks competition in 2017. These images needed to be preprocessed using standard techniques such as cropping them to 299x299x3 images and scaling their RGB values down between 0 and 1 for math calculations.To measure our our success, we used various metrics such as success rate, visual difference, and normalized mean-squared error (MSE) distance. The success rate measured the ability of our algorithms to perform a successful adversarial attack given any source image. Visual difference was an informal measurement tool that determined whether the adversarial example could successfully fool a deep neural network model, but not a human. The normalized MSE distance measured the magnitude in pixel difference between the actual image and the adversarial example, so we could examine if the algorithm successfully minimized the perturbation it applied to the original image.For our baseline model, we attacked the Google Cloud Vision API using Transfer Learning attacks. We locally attacked a ResNet50 model and generated a series of successful adversarial examples for the local model using the Fast Gradient Sign (FGS), Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS), and DeepFool methods. These attacks performed well at fooling the ResNet50 model, but we showed that the attack did not transfer to the Google Cloud Vision API which still correctly classified these images as their original labels.For our final model, we attempted three different attack algorithms using a Natural Evolution Strategy, Boundary Attack, and Zeroth Order Optimization.Our first step was to run the public repositories created by the authors of their respective research papers. These all successfully attacked local models such as Inception and ResNet using the data that we had collected from our own sources. We also tried transfer learning using these methods, but these failed in the same ways our baseline methods did.Next, we needed to adapt this code to attack the commercial APIs. This involved replacing all the references to the local model with calls to the Google Cloud Vision (GCV) and Clarif.ai APIs. We discovered that the response time for a single call on the AirBears wifi had a latency of about .65 seconds. This was prohibitively slow for algorithms that involved tens to hundreds of thousands of queries, so we decided to parallelize our queries using Python’s concurrent.futures module with multiple workers. However, we had to be careful with the number of workers we spawned because some commercial APIs, such as GCV had query limits of 50,000 requests per project per day and 10 queries per second per IP address. It turned out that 6–8 workers had the best performance without overloading the query limitations.Lastly, we had to come up with a method of grouping together classes that were similar in order to generate an image with a label significantly far away from its correct label. Both GCV and Clarif.ai have a large array of similar or synonymous labels such as “cat”, “tabby cat”, and “tiger” as well as overlapping generalized classes such as “organism”, “animal”, and “fauna”. This meant that, if left alone, cats could easily be classified as different types of cats and cats or dogs could just be classified as animals and this would make our algorithms think they completed their jobs because the returned class was not the original class. These results, however can be unconvincing humans as adequately different from their original classes, so we created a dictionary of “like” terms for each original class and told the algorithms to consider everything from these sets of terms as equal. Presumably, this would help the algorithms find adversarial labels that were adequately different from the original labels. We will see later in our results that it was very difficult to determine what to include and what not to include in this dictionary.We then simply ran through an iterative loop of feeding an adversarial image into the commercial APIs, receiving confidence scores and varying amounts of “relevant” labels, estimating pixel-level gradients from those scores and labels, and updating the adversarial images using perturbations in the direction of those gradients.With this algorithm we attacked in the partial-information setting. Rather than beginning with the image, we began with an instance of the target class. Because of this, we forced the adversarial label to initially appear in the top-k classes. At each iteration, we alternated between projecting onto boxes of noise with a maximum perturbation of epsilon, maintaining that the adversarial class remains within the top-k at all times and perturbing the image to maximize the probability of the adversarial target class. We implemented this iterated optimization using backtracking line search to find the epsilon that maintained the adversarial class within the top-k and used several iterations of projected gradient descent (PGD) to find the next adversarial input. Further details are outlined in Algorithm 2 and in our source code.To do the gradient estimation, we used a Natural Evolution Strategy (NES), which is a gradient estimation method that uses derivative-free optimization based on a search distribution. This maximizes the expected value of the loss under the search distribution instead of maximizing the objective function directly. This is an unbiased, efficient gradient estimator that allows for gradient estimation in far fewer queries than typical finite-difference methods.To perform NES gradient estimation, we generate a batch of adversarial candidate images using Gaussian noise perturbations, get the partial information labels from the GCV API for the batch, compute the partial-information loss for each image, and then do a weighted sum over the losses using a Monte Carlo approximation method.During the duration of the algorithm, we enforce that the ℓ∞ norm between the adversarial image and the original image is not greater than some hyperparameter, epsilon.NES Algorithm OutlineWe find a novel black box attack, called boundary attack, in a very recent paper Decision-based adversarial attacks. This algorithms falls into the category of decision-based adversarial attacks, where direct attacks solely rely on the final decision of the model, such as the top-1 class label. The assumption of limited model information suits perfectly the challenges that GCV poses, such as partial labels and no logits / probabilities. The paper is well written and easy to follow so we encourage readers to read it.The basic intuition behind the boundary attack algorithm is depicted in the following figure: the algorithm assumes that the original image has a smooth decision boundary, beyond which it will be classified mistakenly according to a given adversarial criterion. We initialize algorithm from a point that is already adversarial, i.e. outside the decision boundary, then performs random walks such that (1) it stays in the adversarial region and (2) the distance towards the target image is reduced. This is achieved by first taking stochastic steps towards the original image until decision boundary is reached. Then we try to perform random walks along the decision boundary until the distance between the adversarial image and the original image is minimized. The authors suggest that we can follow the decision boundary by taking two sub-steps at each step: first a random sub-step perpendicular to the direction towards the original image and then a sub-step towards the original image. The algorithm dynamically adjusts the step sizes of the two sub-steps which allows us to follow the decision boundary closely.Zeroth Order Optimization attack was proposed in the paper [ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://arxiv.org/abs/1708.03999). It is a numerical estimation based black-box attack algorithm modified from the C&W Attack. The basic idea is to use zeroth order stochastic coordinate descent to directly estimate the gradients of the targeted DNN by evaluating the objective function f at two points close to the input x: f(x + hv) and f(x — hv) . Then normal gradient descent methods can be applied on the estimated gradient.The attack can be framed as an optimization problem of minimizing perturbation while trying to make the targeted class have higher probability. Let x0 be the original image, x the adversarial example of x0, t the targeted class label, and f the output of the DNN. Then, we want to solve the following:For a targeted attack, we define the loss function f(x,t) based on F as:The rationale is that<= 0 implies that the adversarial example x attains the highest confidence score for class t and hence the attack is successfulAnd for untargeted attacks, we use a similar loss function:To estimate the gradient of f, we use:With coordinate-wise ADAM, the complete algorithm is as follows:In addition, the paper also proposed methods to speed up computation and reduce query using dimension reduction, hierarchical attack, and importance sampling.The attack’s performance on the InceptionV3 model has been shown to be comparable to state-of-art white box attacks.We came up with great results on the local models such as ResNet and Inception, but had difficulty generating good adversarial images for the commercial APIs. Many examples resulted in low quality or noticeably noisy images. Most also looked “unfinished”. If we wanted to label a cat image as a dog, after the generation of the adversary, we could still see some semblance of the original dog image mixed in with the resulting cat example.A behavior that was revealed with the NES attack was that because the GCV API returned only a variable amount of what it determined as “relevant” labels, this caused the learning to stop halfway, because the API would return only 1 or 0 labels when the adversarial was half the target image and half the source image. This caused the gradient to not have enough information to form a gradient estimation using the loss. It is possible that turning down the learning rate and epsilon hyperparameters could help in this area, but we found tuning hyperparameters difficult with query cost limitations.Another suboptimal behavior was that labels common to both the source and target images would tend to dominate during the iterative attack. For example, if we wanted to misclassify an image classified as a “Dog Like Mammal” using a target image of a cat, the label of “Fauna” would dominate because it was applicable to both animals.Original imageAdversarial Example Generation (Intermediate steps)Boundary Attack on GCVWe attempted both targeted and non-targeted boundary attacks on the GCV API using an cat image of various sizes. We initialized the attack with a dog image for the targeted attack and random noise for the non-targeted attack. We define the adversarial criterion to be that the top-5 labels cannot contain cat-related words such as “cat” and “kitten”. For targeted attack, we have an additional condition that the top-1 label must contain a dog-related word such as “dog” or “puppy”. The results are shown in the above panels. We noticed a few thingsA possible explanation for a low quality adversarial example in the boundary attack case is that while the assumption of smooth decision boundary works perfectly for imagenet models, it breaks down in GCV API. We know for certain the GCV model(s) have many more labels than the 1000 labels that pre-trained ImageNet models use. Because we grouped labels such as “cat”, “tabby” and “kitty” together, we may have made the decision boundary even more non-linear. Of course, larger images also imply more dimensions of the decision boundary.ZOO Attack on ClarifAI moderation modelBecause gradient estimation requires the class probability outputs for all classes, ZOO algorithm was unable to attack the Google Cloud Vision API, and we instead used it to perform untargeted attack on ClarifAI’s moderation model, with the aim of misclassifying a safe image into one of the unsafe categories.A successful untargeted attack usually requires 1500 iterations, each of which has a batch size of 128. However, due to the high cost associated with querying from a commercial service, we were only able to perform a partial attack of 150 iterations, each with a batch size of 40 on an image of reduced resolution.The result shows a reduction in the probability of the input image being classified as safe. Due to the query cost, we were unable to perform a complete attack, but the progress does show some effectivenessThe resources and tools we used for this project mostly consisted of the Google Cloud Vision API, Clarif.ai API.For the NES attack on GCV API, we referenced the ens-adv-train-attack repo by Andrew Ilyas.Our boundary attack on GCV API extended the popular adversarial attack toolbox FoolBox.Our ZOO attack on the ClarifAI API is based on the code of the original ZOO paper and is given here.NES attackBoundary attackZOO attackAn issue that was common across the algorithms was that we couldn’t converge on a reasoned way to resolve some issues with the partial information problem. Because of the similar, synonymous, and general label problem mentioned in “Our Final Model” section, we were able to successfully create adversarial examples that changed its image label to a non-correct, but similar label or general label, however, we had difficulty figuring out how to determine a grouping method and difference metric that would allow us to change the label meaningfully far away from its original class. For example, a “tiger cat” became an “egyptian cat” and a “dog like mammal” was labeled as “fauna”.In short, we learned that the generation of good adversarial examples against GCV API is very difficult with the algorithms we considered!NES Attack on GCVBoundary Attack on GCVZOO Attack on Clarifai",11/05/2018,0,14.0,4.0,987.0,600.0,18.0,19.0,0.0,16.0,en
3978,An Intuitive Explanation of Connectionist Temporal Classification,Towards Data Science,Harald Scheidl,1000.0,8.0,1357.0,"If you want a computer to recognize text, neural networks (NN) are a good choice as they outperform all other approaches at the moment. The NN for such use-cases usually consists of convolutional layers (CNN) to extract a sequence of features and recurrent layers (RNN) to propagate information through this sequence. It outputs character-scores for each sequence-element, which simply is represented by a matrix. Now, there are two things we want to do with this matrix:Both tasks are achieved by the CTC operation. An overview of the handwriting recognition system is shown in Fig. 1.Let’s have a closer look at the CTC operation and discuss how it works without hiding the clever ideas it is based on behind complicated formulas. At the end, I will point you to references where you can find Python code and the (not too complicated) formulas, if you are interested.We could, of course, create a data-set with images of text-lines, and then specify for each horizontal position of the image the corresponding character as shown in Fig. 2. Then, we could train a NN to output a character-score for each horizontal position. However, there are two problems with this naive solution:CTC solves both problems for us:As already discussed, we don’t want to annotate the images at each horizontal position (which we call time-step from now on). The NN-training will be guided by the CTC loss function. We only feed the output matrix of the NN and the corresponding ground-truth (GT) text to the CTC loss function. But how does it know where each character occurs? Well, it does not know. Instead, it tries all possible alignments of the GT text in the image and takes the sum of all scores. This way, the score of a GT text is high if the sum over the alignment-scores has a high value.There was the issue of how to encode duplicate characters (you remember what we said about the word “too”?). It is solved by introducing a pseudo-character (called blank, but don’t confuse it with a “real” blank, i.e. a white-space character). This special character will be denoted as “-” in the following text. We use a clever coding schema to solve the duplicate-character problem: when encoding a text, we can insert arbitrary many blanks at any position, which will be removed when decoding it. However, we must insert a blank between duplicate characters like in “hello”. Further, we can repeat each character as often as we like.Let’s look at some examples:As you see, this schema also allows us to easily create different alignments of the same text, e.g. “t-o” and “too” and “-to” all represent the same text (“to”), but with different alignments to the image. The NN is trained to output an encoded text (encoded in the NN output matrix).We need to calculate the loss value for the training samples (pairs of images and GT texts) to train the NN. You already know that the NN outputs a matrix containing a score for each character at each time-step. A minimalistic matrix is shown in Fig. 3: there are two time-steps (t0, t1) and three characters (“a”, “b” and the blank “-”). The character-scores sum to 1 for each time-step.Further, you already know that the loss is calculated by summing up all scores of all possible alignments of the GT text, this way it does not matter where the text appears in the image.The score for one alignment (or path, as it is often called in the literature) is calculated by multiplying the corresponding character scores together. In the example shown above, the score for the path “aa” is 0.4·0.4=0.16 while it is 0.4·0.6=0.24 for “a-” and 0.6·0.4=0.24 for “-a”. To get the score for a given GT text, we sum over the scores of all paths corresponding to this text. Let’s assume the GT text is “a” in the example: we have to calculate all possible paths of length 2 (because the matrix has 2 time-steps), which are: “aa”, “a-” and “-a”. We already calculated the scores for these paths, so we just have to sum over them and get 0.4·0.4+0.4·0.6+0.6·0.4=0.64. If the GT text is assumed to be “”, we see that there is only one corresponding path, namely “--”, which yields the overall score of 0.6·0.6=0.36.We are now able to compute the probability of the GT text of a training sample, given the output matrix produced by the NN. The goal is to train the NN such that it outputs a high probability (ideally, a value of 1) for correct classifications. Therefore, we maximize the product of probabilities of correct classifications for the training dataset. For technical reasons, we re-formulate into an equivalent problem: minimize the loss of the training dataset, where the loss is the negative sum of log-probabilities. If you need the loss value for a single sample, simply compute the probability, take the logarithm, and put a minus in front of the result. To train the NN, the gradient of the loss with respect to the NN parameters (e.g., weights of convolutional kernels) is computed and used to update the parameters.When we have a trained NN, we usually want to use it to recognize text in previously unseen images. Or in more technical terms: we want to calculate the most likely text given the output matrix of the NN. You already know a method to calculate the score of a given text. But this time, we are not given any text, in fact, it is exactly this text we are looking for. Trying every possible text would work if there are only a few time-steps and characters, but for practical use-cases, this is not feasible.A simple and very fast algorithm is best path decoding which consists of two steps:An example is shown in Fig. 4. The characters are “a”, “b” and “-” (blank). There are 5 time-steps. Let’s apply our best path decoder to this matrix: the most likely character of t0 is “a”, the same applies for t1 and t2. The blank character has the highest score at t3. Finally, “b” is most likely at t4. This gives us the path “aaa-b”. We remove duplicate characters, this yields “a-b”, and then we remove any blank from the remaining path, which gives us the text “ab” which we output as the recognized text.Best path decoding is, of course, only an approximation. It is easy to construct examples for which it gives the wrong result: if you decode the matrix from Fig. 3, you get “” as the recognized text. But we already know that the probability of “” is only 0.36 while it is 0.64 for “a”. However, the approximation algorithm often gives good results in practical situations. There are more advanced decoders such as beam-search decoding, prefix-search decoding or token passing, which also use information about language structure to improve the results.First, we looked at the problems arising with a naive NN solution. Then, we saw how CTC is able to tackle these problems. We then examined how CTC works by looking at how it encodes text, how loss calculation is done and how it decodes the output of a CTC-trained NN.This should give you a good understanding of what is happening behind the scenes when you e.g. call functions like ctc_loss or ctc_greedy_decoder in TensorFlow. However, when you want to implement CTC yourself, you need to know some more details, especially to make it run fast. Graves et al. [1] introduce the CTC operation, the paper also shows all the relevant math. If you are interested in how to improve decoding, take a look at the articles about beam search decoding [2][3]. I implemented some decoders and the loss function in Python and C++, which you can find on github [4][5]. Finally, if you want to look at the bigger picture of how to recognize (handwritten) text, look at my article on how to build a handwritten text recognition system [6].[1] Original paper containing all the math[2] Vanilla beam search decoding[3] Word beam search decoding[4] Python implementation of decoders[5] Implementation of word beam search decoding[6] Text recognition systemAnd finally, an overview of my other Medium articles.",10/06/2018,0,12.0,0.0,696.0,459.0,5.0,5.0,0.0,7.0,en
3979,Using Knowledge Graphs to Summarize Long Documents,agolo,Abdarhman Taha,13.0,4.0,425.0,"Automatic text summarization is the task of automatically identifying the salient topics/key-phrases in a document(s) and then either generates or extracts a summary.Currently, most state-of-the-art summarizers are focused on single, short document summarization. Recent progress in summarization, mostly transformers-based, struggles with long inputs due to the architecture limitations, which have led many researchers to explore using new ideas like the longformer to overcome this issue. However, the final summaries are 5–10 sentences long that lacks coherence, and don’t give enough info about the original document. And of course, such methods can’t handle even tougher situations where the input is more than one long document. Similar observations could be found with rule-based summarizers, that find themselves almost selecting sentences randomly as their scores would be very similar for very long documents.In this post, we’re going to talk about long-form document summarization and how agolo uses knowledge graphs (KGs) to help tackle this problem. Long documents tend to be talking about multiple topics with each section covering a cohesive set of sub-topics. To understand the information flow in the document, we use KGs.Summarizing long-form documents needs a special understanding of the information flow inside them, which would allow our summarizer to identify the main key points/topics anywhere and cluster those important parts together.We first analyze the full document to extract the document knowledge graph, and then by processing each section individually, we can extract the relatedness of each section to all other ones, using our semantic cohesion model, the following graph shows an example.Our long-form summarizer first understands the input set of documents in a high section-level way, then tries to group those sections to form coherent clusters, each one represents a topic or idea, the representation has the intro, body, and conclusion of each topic. These clusters form what we call The Semantic GraphBefore continuing, have a quick intro about our QBS, from this previous article.The semantic graph is then used to produce summaries, headlines, and tags to facilitate the retrieval and navigation of each cluster.The following example is produced using BlackRock’s 2021 global outlook as an input to the framework.This method has saved the user more than 65% of the time that would be taken to read the whole document(s), while also highlighting the main important topics and presenting them in a cohesive understandable way.Also, we show that this method would help in the full process of long document indexing, retrieval and summarization, as the automatic tag generation would help the user navigate through only the needed summaries, without having to read the full document.",19/04/2021,1,9.0,0.0,988.0,684.0,2.0,0.0,0.0,3.0,en
3980,Creating OpenAI Gym Environments with PyBullet (Part 1),Medium,Gerard Maggiolino,49.0,9.0,1983.0,"This guide assumes rudimentary knowledge of reinforcement learning and the structure of OpenAI Gym environments, along with proficiency in Python.Many of the standard environments for evaluating continuous control reinforcement learning algorithms are built on the MuJoCo physics engine, a paid and licensed software. Bullet Physics provides a free and open source alternative to physics simulation with OpenAI Gym offering a set of environments built upon it. PyBullet is a library designed to provide Python bindings to the lower level C-API of Bullet. We will use PyBullet to design our own OpenAI Gym environments.This post will be the first of a two part series.We’ll go through building an environment step by step with enough explanations for you to learn how to independently build your own. Code will be displayed first, followed by explanation. Please follow along in Python on your command line.Install PyBullet with pip. The majority of PyBullet documentation comes in the form of the Quickstart Guide, which provides descriptions of all functions in the pybullet module. PyBullet uses a client-server style API; commands and queries are sent to an ongoing physics simulation.A simulation is started using the p.connect() method. Several connection modes are available, with p.GUI allowing for visualization and debugging, and p.DIRECT providing the fastest, non-visual connection. The returned value from p.connect() is an integer, and can be passed to other functions to specify which simulation to apply the command to. This is useful if you have multiple physics simulations, and will default to 0 (the first started simulation) if it’s not provided as an argument.A command is sent to the server through function calls, such as p.setGravity(). Following the above code, let’s add on:The module pybullet_data provides many example Universal Robotic Description Format (URDF) files. We’ll build our own URDF file soon, but for now, just understand that they specify the physical and visual characteristics of a robot.The p.loadURDF() command tells the simulation to create a robot as specified in a URDF file. Find it in the QuickStart Guide, and view the arguments and return value. Note that the returned value is an integer, not a modifiable object. This integer is the ID passed into other functions to query the state of a robot and perform actions on it. Of course, the plane.urdf we’ve loaded is a very, very simple “robot”. Let’s load in a more complicated one.Note: If you’re following along with the code on your command line, the simulation should appear hung. This is expected, and will resolve in future steps.We’ve specified a base position to create the racecar slightly above the plane. Make sure you’ve checked the QuickStart Guide and viewed the p.loadURDF() function. Reading documentation will be crucial in becoming comfortable with PyBullet.Find the documentation for the above function in the QuickStart Guide. All PyBullet functions follow a similar style; using the ID of a loaded URDF, we can query information about the robot or specify actions to perform. Notice that as we have only a single physics simulation running, we can omit the physicsClientId argument, which defaults to zero. Running the following:will run the simulation for 100 time steps. Each time step is 1/240 of a second. Having set the gravity to -10 in the z direction earlier, we should see the car fall to the ground. Using what we’ve learned above, let’s send commands to the client to move the car. View the documentation for the function p.applyExternalForce() — we’ll cover the linkIndex argument soon when building our own URDF. Try to fill in the labelled blanks below:The car should move forward in the simulation! Normally, we would not apply an external force to move the car. We would apply a force to the axle of the car to rotate the wheels. We’ll first build our own URDF file to understand how robots are structured. We’ll then manipulate the joints directly to move the toy car we’ve created.Note: Answers for above are as follows. A = B = carId. This specifies which robot is the target of our query and command. C = pos. According to documentation, the 4th argument is where the force is applied. We want to apply the force to our car as it moves.While PyBullet accepts several robot descriptor file formats, including the more powerful SDF and MJCF, URDF is the standard for Robot OS (ROS) and what we’ll be using. A complete tutorial can be found on ROS’s Wiki; we’ll focus on what’s necessary to build and understand a simple model.Robots specified with URDF are created from two components, links and joints. These components are arranged in a tree structure originating from a base link. Forces are applied to joints through PyBullet to control robots.Links and joints have properties that define their shape and behavior in the simulation; the visual property in the code below is an example. URDF files are formatted with XML, a markup language similar in structure to HTML. Tags are used to specify and describe the properties of links and joints.Create “simplecar.urdf” as above. Use the p.GUI connection to visualize the model at each step. For now, our model is composed of one link with just a visual property, a box. Note that the box’s visual origin is at the center of it’s geometry, with half the box appearing below the grid.The wheels of our car will each be their own link connected to the base link with a joint. Joints have exactly one parent and one child link that they connect. We’re going to create rear wheels as cylinder shaped links attached with a “continuous” type joint to the base link. Continuous type joints are rotated from an angle of positive to negative infinity. We’ll also use “revolute” joints, which allow rotational limits. A full list of joint types can be viewed here.The origin of the continuous type joints specify that the joints attach to the rear left and right of the base link. The wheel’s visual property is rotated so the wheels point towards the ground. Note that we do not need to change the xyz of the wheel’s origin, as the default origin is the origin of the joint which connects it; we’ve already moved the origins of the joints to the back of the base. The axis of the joint is the axis of rotation for continuous and revolute joints.Note: A “No inertial data for link” warning may be printed to console if following along with intermediate steps. This will resolve as we add inertial and collision properties later on.The front two wheels will be attached with steering hinges. Each steering hinge is a link, attached to the base with a revolute joint. Each wheel is then attached to the steering hinge through a continuous joint. Let’s first create the steering hinges. We’ll also add a material element to all of our visual properties, which specifies a custom color.The revolute joint has a required limit element, specifying the maximum angle the joint can be turned to. The origins are set so that both hinges are attached at the front center of the base. We’ll now attach the front wheels to the hinges with continuous joints. While there appears to be a lot of code, much is boilerplate tagging for simple and repeated specifications.While this is a nice visual model of our car, we need to specify both collision and inertial properties. These properties will share the same geometry as the visual property; in other words, they’ll overlap with what we’ve defined visually. The collision properties determine how our car will collide with other objects like the ground. The inertial properties determine the mass and distribution of mass in each link of the car. For brevity, the inertial and collision elements are demonstrated for the left front wheel. The completed model can be viewed and downloaded here.A plane that we’ll drive the car over is provided here. The only new property we use in this plane is contact coefficients, which specifies the interaction between two colliding objects. This will allow the wheels of our car to gain traction over the plane.Note that this is a very basic URDF file. Complex robots can be specified with realistic physical properties, and meshes defined in STL or STP file formats can be imported to create non-primitive link shapes. If you’re interested in building more complex URDF models, I highly recommend the full ROS tutorial. This won’t be necessary for the remainder of our tutorial.To recap, each robot is just a tree structure of links connected by joints. We manipulate joints to move the robot, while links give the robot its structure. Download the completed car and plane into your working directory before continuing.We want to use setJointMotorControl2(), which allows us to change the velocity, position, or apply a torque to a joint. This is the main method used to control robots. It takes both a robot ID and a joint ID; we’ll use getJointInfo() to figure out what joint IDs correspond to the axles of the car. We’ll also use getNumJoints(), which takes a robot ID and returns the number of joints it has.This prints out all the information returned by getJointInfo(). If you tried it, you’ll see that it’s a bit difficult to parse. Let’s read the documentation to figure out precisely what we want from “info”. Change the print line to the following:From this, we know that the joints connecting to our front and back wheels are 1, 3, 4, and 5, and our steering joints are 0 and 2. We’ll pass these joint IDs into setJointMotorControl2() to control the velocity of the wheel joints and the position of the steering joints.Let’s both load our robots and use the useful addUserDebugParameter() method available in PyBullet’s p.GUI mode. Copy the code into a file.Sliders are added to our GUI view that allow us to dynamically input values to our program through the simulation. The ID returned by addUserDebugParameter() is passed to readUserDebugParameter() to retrieve the value on the slider. If you’re a bit overwhelmed by the number of functions we’re showing you, don’t worry! All of PyBullet is essentially a collection of functions, and learning to use them is at first daunting.Adding on to the code above:we specify which indices correspond to which robotic joints. We can then use setJointMotorControl2(), readUserDebugParameter(), and stepSimulation() to finally control our car! Append the following code:and run the file. The readUserDebugParameter() call returns the value of the slider at each timestep. This value is passed into setJointMotorControl2() with the car ID and joint ID to control either the velocity or position of the joints that articulate our robot. After changing the behavior of our joints, we call stepSimulation() for PyBullet to handle all of the physics regarding robotic collision and movement.Running the program we built, we can adjust the sliders on our GUI to change the wheel joint’s velocity, in turn spinning the wheel links, which make contact with the plane. As we specified a friction property in the plane’s URDF file, this contact moves the car forward. We can adjust mass, friction, and shape properties our of URDF files to change how the car behaves in the simulation.PyBullet simulates the physical dynamics of robots and their collisions. It uses a client-server API, where we send queries or commands to the PyBullet module to receive information about or manipulate robots.Robots are defined as links connected by joints in URDF files. Joints have a parent and child link that they connect, and can rotate, slide, or be fixed.We apply a torque to or change the velocity or position of joints to move our robots in the PyBullet simulation. As we move joints, PyBullet simulates motion and collision of our robots.Now that we have a basic understanding of PyBullet, we’ll use our car and plane model to create an OpenAI Gym environment that we can use to train and test reinforcement learning agents. See part two for the final environment.",22/10/2019,12,1.0,4.0,500.0,621.0,1.0,1.0,0.0,30.0,en
3981,Logistic Regression: A Simplified Approach Using Python,Towards Data Science,Surya Remanan,515.0,6.0,799.0,"In Logistic Regression, we wish to model a dependent variable(Y) in terms of one or more independent variables(X). It is a method for classification. This algorithm is used for the dependent variable that is Categorical. Y is modeled using a function that gives output between 0 and 1 for all values of X. In Logistic Regression, the Sigmoid (aka Logistic) Function is used.After we train a logistic regression model on some training data, we will evaluate the performance of the model on some test data. For this, we use the Confusion Matrix. A Confusion Matrix is a table that is often used to describe the performance of the classification model on a set of test data for which the true values are already known. Given below is a Confusion Matrix.Here, TP stands for True Positive which are the cases in which we predicted yes and the actual value was true. TN stands for True Negative which are the cases in which we predicted no and the actual value was false.FP stands for False Positive which are the cases which we predicted yes and the actual value was False.FN stands for False Negative which are the cases which we predicted No and the actual value was true.Confusion Matrix helps us determine how often the model prediction is correct or in other words, the accuracy of the model. By the above table, It is given by:( TP + TN ) / Total = 100 + 50 / 165 = 0.91This means that the model is 91% correct. The Confusion Matrix is also used to measure the error rate which is given by:( FP + FN ) / Total = 15 /165 = 0.09There is 9% error in the model.In this article, we will be dealing with very simple steps in python to model the Logistic Regression.We will observe the data, analyze it, visualize it, clean the data, build a logistic regression model, split into train and test data, make predictions and finally evaluate it. All these will be done step by step. The Data we will deal with is the ‘Titanic Data Set’ available in kaggle.com. This is a very famous dataset and often a student’s first step towards learning Machine Learning based on classification. We are trying to predict the classification: Survival or deceasedFirst, we will import the numpy and pandas libraries:Let us do the visualization imports:We’ll proceed by importing the Titanic data set into a pandas dataframe. After that, we’ll be checking the head of the dataframe just to get a clear idea of all the columns in the dataframe.Most of the data that we come across has missing data. We’ll check for missing data, also visualize them to get a better idea and remove them.Here, we find boolean values. True indicating that the value is null and False the vice versa. Since there are a lot of data, we use the seaborn library to visualize the null values. In that case, our task becomes much easier.The Age and Cabin column have null values. I have dealt with the problem of dealing with NA values in my previous blog. Please have a look at it.It is always a good practice to play around with the data and fully exploit the visualization libraries to have fun with the data.This is a count plot that shows the number of people who survived which is our target variable. Further, we can plot count plots on the basis of gender and passenger class.Here, we see a trend that more females survived than males.From the above plot, we can infer that passengers belonging to class 3 died the most.There are many more ways by which we can visualize data. However, I’m not discussing them here because we need to get to the step of model building.We want to fill in the missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation). However, we can be smarter about this and check the average age by passenger class. For example:We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We’ll use these average age values to impute based on Pclass for Age.Now apply that function!Now let’s check that heat map again.Great! Let’s go ahead and drop the Cabin column.We’ll need to convert categorical features to dummy variables using pandas! Otherwise, our machine learning algorithm won’t be able to directly take in those features as inputs.Here, we are dummying the sex and embark columns. After dummying, we will drop the rest of the columns which are not needed.We will concatenate the new sex and embarked columns to the dataframe.Now, the dataframe looks like this:We can check precision, recall,f1-score using classification report",17/09/2018,20,5.0,0.0,536.0,315.0,13.0,0.0,0.0,2.0,en
3982,Training Custom NER,Towards Data Science,Nishanth N,17.0,3.0,343.0,"The article explains what is spacy, advantages of spacy, and how to get the named entity recognition using spacy. Now, all is to train your training data to identify the custom entity from the text.SpaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. As of version 1.0, spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like Tensor Flow, PyTorch, or MXNet through its machine learning library Thinc.While NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it. It provides the fastest and most accurate syntactic analysis of any NLP library released to date. It also offers access to larger word vectors that are easier to customize.After installation, you need to download a language modelYou can start the training once you completed the first step.→ Initially, import the necessary packages required for the custom creation process.→ Now, the major part is to create your custom entity data for the input text where the named entity is to be identified by the model during the testing period.→ Define the variables required for the training model to be processed.→ Next, load a blank model for the process to carry out the NER action and set up the pipeline with only NER using create_pipe function.→ Here, we want to train the recognizer by disabling the unnecessary pipeline except for NER. The nlp_update function can be used to train the recognizer.Output: Training losses→ To test the trained model,→ Finally, save the model to your path which stored in the output_dir variable.The full source code available on GitHub.I hope you have now understood how to train your own NER model on top of the spaCy NER model. Thanks for reading!",24/07/2020,1,0.0,18.0,1050.0,667.0,2.0,0.0,0.0,13.0,en
3983,What is a Transformer?,Inside Machine learning,Maxime,782.0,13.0,2805.0,"New deep learning models are introduced at an increasing rate and sometimes it’s hard to keep track of all the novelties. That said, one particular neural network model has proven to be especially effective for common natural language processing tasks. The model is called a Transformer and it makes use of several methods and mechanisms that I’ll introduce here. The papers I refer to in the post offer a more detailed and quantitative description.The paper ‘Attention Is All You Need’ describes transformers and what is called a sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence. (Well, this might not surprise you considering the name.)Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A popular choice for this type of model is Long-Short-Term-Memory (LSTM)-based models. With sequence-dependent data, the LSTM modules can give meaning to the sequence while remembering (or forgetting) the parts it finds important (or unimportant). Sentences, for example, are sequence-dependent since the order of the words is crucial for understanding the sentence. LSTM are a natural choice for this type of data.Seq2Seq models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector). That abstract vector is fed into the Decoder which turns it into an output sequence. The output sequence can be in another language, symbols, a copy of the input, etc.Imagine the Encoder and Decoder as human translators who can speak only two languages. Their first language is their mother tongue, which differs between both of them (e.g. German and French) and their second language an imaginary one they have in common. To translate German into French, the Encoder converts the German sentence into the other language it knows, namely the imaginary language. Since the Decoder is able to read that imaginary language, it can now translates from that language into French. Together, the model (consisting of Encoder and Decoder) can translate German into French!Suppose that, initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples.A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them.You’re wondering when the Transformer will finally come into play, aren’t you?We need one more technical detail to make Transformers easier to understand: Attention. The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important. It sounds abstract, but let me clarify with an easy example: When reading this text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context.An attention-mechanism works similarly for a given sequence. For our example with the human Encoder and Decoder, imagine that instead of only writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords that are important to the semantics of the sentence, and gives them to the Decoder in addition to the regular translation. Those new keywords make the translation much easier for the Decoder because it knows what parts of the sentence are important and which key terms give the sentence context.In other words, for each input that the LSTM (Encoder) reads, the attention-mechanism takes into account several other inputs at the same time and decides which ones are important by attributing different weights to those inputs. The Decoder will then take as input the encoded sentence and the weights provided by the attention-mechanism. To learn more about attention, see this article. And for a more scientific approach than the one provided, read about different attention-based approaches for Sequence-to-Sequence models in this great paper called ‘Effective Approaches to Attention-based Neural Machine Translation’.The paper ‘Attention Is All You Need’ introduces a novel architecture called Transformer. As the title indicates, it uses the attention-mechanism we saw earlier. Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).Recurrent Networks were, until now, one of the best ways to capture the timely dependencies in sequences. However, the team presenting the paper proved that an architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks! One improvement on Natural Language Tasks is presented by a team introducing BERT: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.So, what exactly is a Transformer?An image is worth thousand words, so we will start with that!The Encoder is on the left and the Decoder is on the right. Both Encoder and Decoder are composed of modules that can be stacked on top of each other multiple times, which is described by Nx in the figure. We see that the modules consist mainly of Multi-Head Attention and Feed Forward layers. The inputs and outputs (target sentences) are first embedded into an n-dimensional space since we cannot use strings directly.One slight but important part of the model is the positional encoding of the different words. Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word.Let’s have a closer look at these Multi-Head Attention bricks in the model:Let’s start with the left description of the attention-mechanism. It’s not very complicated and can be described by the following equation:Q is a matrix that contains the query (vector representation of one word in the sequence), K are all the keys (vector representations of all the words in the sequence) and V are the values, which are again the vector representations of all the words in the sequence. For the encoder and decoder, multi-head attention modules, V consists of the same word sequence than Q. However, for the attention module that is taking into account the encoder and the decoder sequences, V is different from the sequence represented by Q.To simplify this a little bit, we could say that the values in V are multiplied and summed with some attention-weights a, where our weights are defined by:This means that the weights a are defined by how each word of the sequence (represented by Q) is influenced by all the other words in the sequence (represented by K). Additionally, the SoftMax function is applied to the weights a to have a distribution between 0 and 1. Those weights are then applied to all the words in the sequence that are introduced in V (same vectors than Q for encoder and decoder but different for the module that has encoder and decoder inputs).The righthand picture describes how this attention-mechanism can be parallelized into multiple mechanisms that can be used side by side. The attention mechanism is repeated multiple times with linear projections of Q, K and V. This allows the system to learn from different representations of Q, K and V, which is beneficial to the model. These linear representations are done by multiplying Q, K and V by weight matrices W that are learned during the training.Those matrices Q, K and V are different for each position of the attention modules in the structure depending on whether they are in the encoder, decoder or in-between encoder and decoder. The reason is that we want to attend on either the whole encoder input sequence or a part of the decoder input sequence. The multi-head attention module that connects the encoder and decoder will make sure that the encoder input-sequence is taken into account together with the decoder input-sequence up to a given position.After the multi-attention heads in both the encoder and decoder, we have a pointwise feed-forward layer. This little feed-forward network has identical parameters for each position, which can be described as a separate, identical linear transformation of each element from the given sequence.How to train such a ‘beast’? Training and inferring on Seq2Seq models is a bit different from the usual classification problem. The same is true for Transformers.We know that to train a model for translation tasks we need two sentences in different languages that are translations of each other. Once we have a lot of sentence pairs, we can start training our model. Let’s say we want to translate French to German. Our encoded input will be a French sentence and the input for the decoder will be a German sentence. However, the decoder input will be shifted to the right by one position. ..Wait, why?One reason is that we do not want our model to learn how to copy our decoder input during training, but we want to learn that given the encoder sequence and a particular decoder sequence, which has been already seen by the model, we predict the next word/character.If we don’t shift the decoder sequence, the model learns to simply ‘copy’ the decoder input, since the target word/character for position i would be the word/character i in the decoder input. Thus, by shifting the decoder input by one position, our model needs to predict the target word/character for position i having only seen the word/characters 1, …, i-1 in the decoder sequence. This prevents our model from learning the copy/paste task. We fill the first position of the decoder input with a start-of-sentence token, since that place would otherwise be empty because of the right-shift. Similarly, we append an end-of-sentence token to the decoder input sequence to mark the end of that sequence and it is also appended to the target output sentence. In a moment, we’ll see how that is useful for inferring the results.This is true for Seq2Seq models and for the Transformer. In addition to the right-shifting, the Transformer applies a mask to the input in the first multi-head attention module to avoid seeing potential ‘future’ sequence elements. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position.The process of feeding the correct shifted input into the decoder is also called Teacher-Forcing, as described in this blog.The target sequence we want for our loss calculations is simply the decoder input (German sentence) without shifting it and with an end-of-sequence token at the end.Inferring with those models is different from the training, which makes sense because in the end we want to translate a French sentence without having the German sentence. The trick here is to re-feed our model for each position of the output sequence until we come across an end-of-sentence token.A more step by step method would be:We see that we need multiple runs through our model to translate our sentence.I hope that these descriptions have made the Transformer architecture a little bit clearer for everybody starting with Seq2Seq and encoder-decoder structures.We have seen the Transformer architecture and we know from literature and the ‘Attention is All you Need’ authors that the model does extremely well in language tasks. Let’s now test the Transformer in a use case.Instead of a translation task, let’s implement a time-series forecast for the hourly flow of electrical power in Texas, provided by the Electric Reliability Council of Texas (ERCOT). You can find the hourly data here.A great detailed explanation of the Transformer and its implementation is provided by harvardnlp. If you want to dig deeper into the architecture, I recommend going through that implementation.Since we can use LSTM-based sequence-to-sequence models to make multi-step forecast predictions, let’s have a look at the Transformer and its power to make those predictions. However, we first need to make a few changes to the architecture since we are not working with sequences of words but with values. Additionally, we are doing an auto-regression and not a classification of words/characters.The available data gives us hourly load for the entire ERCOT control area. I used the data from the years 2003 to 2015 as a training set and the year 2016 as test set. Having only the load value and the timestamp of the load, I expanded the timestamp to other features. From the timestamp, I extracted the weekday to which it corresponds and one-hot encoded it. Additionally, I used the year (2003, 2004, …, 2015) and the corresponding hour (1, 2, 3, …, 24) as the value itself. This gives me 11 features in total for each hour of the day. For convergence purposes, I also normalized the ERCOT load by dividing it by 1000.To predict a given sequence, we need a sequence from the past. The size of those windows can vary from use-case to use-case but here in our example I used the hourly data from the previous 24 hours to predict the next 12 hours. It helps that we can adjust the size of those windows depending on our needs. For example, we can change that to daily data instead of hourly data.As a first step, we need to remove the embeddings, since we already have numerical values in our input. An embedding usually maps a given integer into an n-dimensional space. Here instead of using the embedding, I simply used a linear transformation to transform the 11-dimensional data into an n-dimensional space. This is similar to the embedding with words.We also need to remove the SoftMax layer from the output of the Transformer because our output nodes are not probabilities but real values.After those minor changes, the training can begin!As mentioned, I used teacher forcing for the training. This means that the encoder gets a window of 24 data points as input and the decoder input is a window of 12 data points where the first one is a ‘start-of-sequence’ value and the following data points are simply the target sequence. Having introduced a ‘start-of-sequence’ value at the beginning, I shifted the decoder input by one position with regard to the target sequence.I used an 11-dimensional vector with only -1’s as the ‘start-of-sequence’ values. Of course, this can be changed and perhaps it would be beneficial to use other values depending of the use case but for this example, it works since we never have negative values in either dimension of the input/output sequences.The loss function for this example is simply the mean squared error.The two plots below show the results. I took the mean value of the hourly values per day and compared it to the correct values. The first plot shows the 12-hour predictions given the 24 previous hours. For the second plot, we predicted one hour given the 24 previous hours. We see that the model is able to catch some of the fluctuations very well. The root mean squared error for the training set is 859 and for the validation set it is 4,106 for the 12-hour predictions and 2,583 for the 1-hour predictions. This corresponds to a mean absolute percentage error of the model prediction of 8.4% for the first plot and 5.1% for the second one.The results show that it would be possible to use the Transformer architecture for time-series forecasting. However, during the evaluation, it shows that the more steps we want to forecast the higher the error will become. The first graph (Figure 3) above has been achieved by using the 24 hours to predict the next 12 hours. If we predict only one hour, the results are much better as we see on the second the graph (Figure 4).There’s plenty of room to play around with the parameters of the Transformer, such as the number of decoder and encoder layers, etc. This was not intended to be a perfect model and with better tuning and training, the results would probably improve.It can be a big help to accelerate the training using GPUs. I used the Watson Studio Local Platform to train my model with GPUs and I let it run there rather than on my local machine. You can also accelerate the training using Watson’s Machine Learning GPUs which are free up to a certain amount of training time! Check out my previous blog to see how that can be integrated easily into your code.Thank you very much for reading this and I hope I was able to clarify a few notions to the people who are just starting to get into Deep Learning!",04/01/2019,0,8.0,12.0,1160.0,640.0,7.0,1.0,0.0,9.0,en
3984,Quantum Chemistry Breakthrough: DeepMind Uses Neural Networks to Tackle Schrödinger Equation,SyncedReview,Synced,24000.0,3.0,489.0,"Wave function represents the quantum state of an atom, including the position and movement states of the nucleus and electrons. For decades researchers have struggled to determine the exact wave function when analyzing a normal chemical molecule system, which has its nuclear position fixed and electrons spinning. Fixing wave function has proven problematic even with help from the Schrödinger equation.Previous research in this field used a Slater-Jastrow Ansatz application of quantum Monte Carlo (QMC) methods, which takes a linear combination of Slater determinants and adds the Jastrow multiplicative term to capture the close-range correlations.Now, a group of DeepMind researchers have brought QMC to a higher level with the Fermionic Neural Network — or Fermi Net — a neural network with more flexibility and higher accuracy. Fermi Net takes the electron information of the molecules or chemical systems as inputs and outputs their estimated wave functions, which can then be used to determine the energy states of the input chemical systems.Major features that Fermi Net takes in are positional differences and absolute distances — either between the electron and the nuclear as a single electron stream, or between the electron and another electron as a paired electron stream. The intermediate layers within the neural network take the mean of the activation functions from each stream and concatenate the input for a final layer, which applies a spin-dependent linear transformation, weighs the desired outputs as the solution wave function, and enforces the boundary conditions.In their experiments the research group proved the superior accuracy of Fermi Net compared to other QMC alternatives, especially the well-known and maturely constructed Slater-Jastrow Ansatz. Fermi Net shows high accuracy in single atom determination systems of all first-row elements, CO systems, and N2 systems.Fermi Net also showed impressive performance in predicting the dissociation curve of a nitrogen molecule and hydrogen chain when compared to CCSD(T) methods which are considered the gold standard for quantum chemistry calculation.Furthermore, Fermi Net also addresses the bias set extrapolation problem and error importing, since in each layer of the neural network the bias terms are computed rather than preset.The paper’s lead author David Pfau and his colleagues believe that in addition to the addressed application in QMC, their Fermionic Neural Network can also be applied to other fields beyond Variational Monte Carlo (VMC), for example as a trial wave function for projector QMC methods, and to accelerate the progress of quantum chemistry.The paper Ab-Initio Solution of the Many-Electron Schrödinger Equation with Deep Neural Networks is on arXiv.Author: Linyang Yu | Editor: Michael SarazenWe know you don’t want to miss any stories. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates.Need a comprehensive review of the past, present and future of modern AI research development? Trends of AI Technology Development Report is out!2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon.Apply for Insight Partner Program to get a complimentary full PDF report",18/09/2019,0,12.0,7.0,845.0,322.0,6.0,0.0,0.0,6.0,en
3985,Explaining K-Means Clustering,Towards Data Science,Kamil Mysiak,242.0,31.0,5296.0,"Today’s data comes in all shapes and sizes. NLP data encompasses the written word, time-series data tracks sequential data movement over time (ie. stocks), structured data which allows computers to learn by example, and unclassified data allows the computer to apply structure. Whichever dataset you possess, you can be sure there is an algorithm ready to decipher its secrets. In this article, we want to cover a clustering algorithm named KMeans which attempts to uncover hidden subgroups hiding in your dataset. Furthermore, we will examine what effects dimension reduction has on the quality of the clusters obtained from KMeans.In our example, we will be examining a human resources dataset consisting of 15,000 individual employees. The dataset contains employee job characteristics such as job satisfaction, performance score, workload, years of tenure, accidents, number of promotions. We will apply KMeans in order to uncover similar groups of employees.Classification problems will have target labels which we are trying to predict. Famous datasets such as Titanic and Iris are both prime for classification as they both have targets we are trying to predict (ie. survived and species). Furthermore, classification tasks require us to split our data into training and test, where the classifier is trained on the training data and then its performance is measured via the test dataset.When dealing with a clustering problem, we want to use an algorithm to uncover meaningful groups in our data. Perhaps we are trying to uncover customer segments or identify anomalies in our data. Either way, the algorithm uncovers the groups with little human intervention as we don’t have target labels to predict.That said, we can partner unsupervised clustering algorithms with supervised algorithms by first identifying groups/clusters and then building supervised models to predict the cluster membership.At its core, KMeans attempts to organize the data into a specified number of clusters. Unfortunately, it is up to us to determine the number of clusters we wish to find but fear not we have tools at our disposal that assist with this process. The goal of KMeans is to identify similar data points and cluster them together while trying to distance each cluster as far as possible. Its “similarity” calculation is determined via Euclidean distance or an ordinary straight line between two points (Wiki). The shorter the Euclidean distance the more similar the points are.First, the user (ie. you or I) determines the number of clusters KMeans needs to find. The number of clusters cannot exceed the number of features in the dataset. Next, KMeans will select a random point for each centroid. The number of centroids is equal to the number of clusters selected. The centroid is the point around which each cluster is built around.Second, the Euclidean distance is calculated between each point and each centroid. Each point will be initially assigned to the closest centroid/cluster based on the Euclidean distance. Each data point can belong to one cluster or centroid. The algorithm then averages Euclidean distance (between each point and centroid) for each cluster and this point becomes the new centroid. This process of averaging the Euclidean distances within clusters and assigning new centroids repeats until cluster centroids no longer move. The animation below shows the process, refresh the page if needed.We need to be aware of how KMeans selects the initial centroid/s and what problems might this produce. Without our intervention, KMeans will randomly select the initial centroid/s which ultimately can cause different resulting clusters on the same data. From the plots below we can see how running the KMeans algorithm on two different occasions resulted in different initial centroids. The same data points were assigned to different clusters between the first and second time running KMeans.Have no fear Sklearn has our backs! The Sklearn KMeans library has certain parameters such as “n_int” and “max_iter” to mitigate this issue. The “n_int” parameter determines the number of times KMeans will randomly select different centroids. “Max_iter” determines how many iterations will run. An iteration is the process of finding the distance, taking the average distance, and moving the centroid.If we set our parameters at n_int=25 and max_iter=200 KMeans will randomly select 25 initial centroids and run each centroid up to 200 iterations. The best out of those 25 centroids will be the final cluster.The Sklearn also has an “int” parameter which will select the first centroid at random and locate all the data points which are furthest away from the first centroid. Then, the second centroid is assigned nearby those far points as they are less likely to belong to the first centroid. Sklearn selects “int=kmeans++” by default which applies the above logic.“You mentioned something about needing to select the number of clusters….? Just how do we do that?”Domain Knowledge: Very often we have a certain level of knowledge and experience in the domain from which our dataset was gathered. This expertise can allow us to set the number of clusters we believe exists in the general population.Hypothesis Testing: Setting a specific number of clusters can also act as a test of a certain hypothesis we might have. For example, when analyzing marketing data we have a hunch there are 3 subgroups of customers who very likely, likely, and not likely to purchase our product.Data Comes Pre-Labeled: There are times when the data we are analyzing comes with pre-labeled targets. These datasets are typically used for supervised ML problems but that doesn’t mean we cannot cluster the data. Pre-labeled data is unique as you need to remove the targets from your initial analysis and then use them to validate how well the model clustered the data.Elbow Method: This is a very popular iterative statistical technique for determining the optimal number of clusters by actually running the K-Means algorithm for a range of cluster values. The elbow method calculates the sum of squared distances from each point to its assigned centroid for each iteration of KMeans. Each iteration runs through a different number of clusters. The result is a line chart that displays the sum of squared distances at each cluster. We want to select the number of clusters at the elbow of the line chart or the lowest sum of squared distances (ie. Inertia) at the lowest number of clusters. The lower the sum of squares distances means the data inside each cluster are more tightly grouped.KMeans is very sensitive to scale and requires all features to be on the same scale. KMeans will put more weight or emphasis on features with larger variances and those features will impose more influence on the final cluster shape. For example, let’s consider a dataset of car information such as weight (lbs) and horsepower (hp). If there is a larger variation in weight between all the cars the average Euclidean distance will be more affected by weight. Ultimately, the membership of each cluster will be more affected by weight than horsepower.Clustering algorithms such as KMeans have a difficult time accurately clustering data of high dimensionality (ie. too many features). Our dataset is not necessarily highly dimensional as it contains 7 features but even this amount will create issues for KMeans. I would suggest you explore the Curse of Dimensionality for more details. As we saw earlier, many clustering algorithms use a distance formula (ie. Euclidean distance) to determine cluster membership. When our clustering algorithm has too many dimensions, pairs of points will begin to have very similar distances and we wouldn’t be able to obtain meaningful clusters.In this example, we are going to compare PCA and t-SNE data reduction techniques prior to running our K-Means clustering algorithm. Let’s take a few mins to explain PCA and t-SNE.Principal component analysis or (PCA) is a classic method we can use to reduce high-dimensional data to a low-dimensional space. In other words, we simply cannot accurately visualize high-dimensional datasets because we cannot visualize anything above 3 features (1 feature=1D, 2 features = 2D, 3 features=3D plots). The main purpose behind PCA is to transform datasets with more than 3 features (high-dimensional) into typically a 2D or 3D plots for us feeble humans. That’s what is meant by low-dimensional space. The beauty behind PCA is the fact that despite the reduction into a lower-dimensional space we still retain most (+90%) of the variance or information from our original high-dimensional dataset. The information or variance from our original features is “squeezed” into what PCA calls principal components (PC). The first PC will contain the majority of the information from the original features. The second PC will contain the next largest amount of information, the 3rd PC the third largest amount of info and so on and so on. The PC are not correlated (ie. orthogonal) which means they all contain unique pieces of information. We can typically “squeeze” most (ie. 80–90%) of the information or variance contained in the original features into a few principal components. We use these principal components in our analyses instead of using the original features. This way we can perform an analysis with only 2 or 3 principal components instead of 50 features while still maintaining 80–90% of the information from our original features.Let’s take a look at some of the details behind how PCA does its magic. To make the explanation a bit easier let’s see how we can reduce a dataset with 2 features (ie. 2D) in one principal component (1D). That said, reducing 50 features into 3 or 4 principal components utilizes the same methodology.We have a 2D plot of weight and height. In other words, we have a dataset of 7 people and have plotted their height in relation to their weight.First, PCA needs to center the data by measuring the distances from each point to the y-axis (height) and x-axis (weight). It then calculates the average distance for both axes (ie. height and weight) and centers the data using these averages.Then PCA will plot the first principal component (PC1) or the best fitting line which maximizes the variance or the amount of information between weight and height. It determines the best fitting line by maximizing the distance from the point projected onto the best fitting line and the origin (ie. blue light below). It does it for each green point and then it squares each distance, to remove negative values, and sums everything up. The best-fitting line or PC1 will have the largest sum of squared distances from the origin to the projected points for all points.Now, we simply rotate the axes to where PC1 is now our x-axis and we are finished.If we wanted to reduce 3 features down to two principal components we would simply place a perpendicular (y-axis) to our best-fitting line in a 2D space. Why a perpendicular line? Because each principal component is orthogonal or uncorrelated with all other features. If we wanted to find a third principal component we would simply find another orthogonal line to PC1 and PC2 but this time in a 3D space.As you can see from the bar plot below, we initially began with a dataset of 5 features. Remember the number of principal components will always equal the number of features. However, we can see that the first 2 principal components account for 90% of the variance or information contained in the original 5 features. This is how we determine the optimal number of principal components. It is important to remember PCA is very often used for visualizing very high dimensional data (ie. thousands of features), therefore, you will most often see PCA of 2 or 3 principal components.Just like PCA, t-SNE takes high-dimensional data and reduces it to a low-dimensional graph (2-D typically). It is also a great dimensionality reduction technique. Unlike PCA, t-SNE can reduce dimensions with non-linear relationships. In other words, if our data had this “Swiss Roll” non-linear distribution where the change in X or Y does not correspond with a constant change in the other variable. PCA would not be able to accurately distill this data into principal components. This is because PCA would attempt to draw the best fitting line through the distribution. T-SNE would be a better solution in this case because it calculates a similarity measure based on the distance between points instead of trying to maximize variance.Let’s examine how t-SNE converts high dimensional data space into lower dimensions. It looks at the similarity between local or nearby points by observing the distance (think Euclidean distance). Points that are nearby each other are considered similar. t-SNE then converts this similarity distance for each pair of points into a probability for each pair of points. If two points are close to each other in the high-dimensional space they will have a high probability value and vice versa. This way the probability of picking a set of points is proportional to their similarity.Then each point gets randomly projected into a low dimensional space. For this example, we are plotting in a 1-D space but we can plot this in a 2-D or 3-D space as well. Why 2-D or 3-D? Because those are the only dimensions we (humans) can visualize. Remember t-SNE is a visualization tool first and a dimensionality reduction tool second.Finally, t-SNE calculates the similarity probability score in a low dimensional space in order to cluster the points together. The result is a 1-D plot we see below.One last thing we need to discuss about t-SNE is “Perplexity”, which is a required parameter when running the algorithm. “Perplexity” determines how broad or how tight of a space t-SNE captures similarities between points. If your perplexity is low (perhaps 2), t-SNE will only use two similar points and produce a plot with many scattered clusters. However, when we increase the perplexity to 10, t-SNE will consider 10 neighbor points as similar and cluster them together resulting in larger clusters of points. There is a point of diminishing returns where perplexity can become too large and we achieve a plot with one or two scattered clusters. At this point, t-SNE incorrectly considers points not necessarily related as belonging to a cluster. We typically set perplexity anywhere between 5 and 50, according to the original published paper (link).One of the main limitations of t-SNE is its high computational costs. If you have a very large feature set it might be good to first use PCA to reduce the number of features to a few principal components and then use t-SNE to further reduce the data to 2 or 3 clusters.Let’s get back to KMeans……..When using unsupervised ML algorithms we often cannot compare our results against a known true label. In other words, we do not have a test set to gauge the performance of our model. That said, we still need to understand how well K-Means managed to cluster our data. We already know how tightly the data is contained within our clusters by looking at the Elbow graph and the number of clusters we selected.Silhouette Method: This technique measures the separability between clusters. First, an average distance is found between each point and all other points in a cluster. Then it measures the distance between each point and each point in other clusters. We subtract the two average measures and divide by whichever average is larger.We ultimately want a high (ie. closest to 1) score which would indicate that there is a small intra-cluster average distance (tight clusters) and a large inter-cluster average distance (clusters well separated).Visual Cluster Interpretation: Once you have obtained your clusters it is very important to interpret each cluster. This is typically done by merging the original dataset with the clusters and visualizing each cluster. The more clear and distinct each cluster is the better. We will review this process below.Here’s the plan for the analysis below:This is a relatively clean dataset without any missing values or outliers. We do not see any mixed-type features, odd values or rare labels which need encoding. Features have low multicollinearity as well. Let’s move on to scaling our dataset.As aforementioned, the standardization of data will ultimately bring all features to the same scale and bringing the mean to zero and the standard deviation to 1.Let’s utilize the Elbow Method to determine the optimal number of clusters KMeans should obtain. It seem 4 or 5 clusters would be best and for the sake of simplicity we’ll select 4.Let’s apply KMeans on the original dataset requesting 4 clusters. We achieved a silhouette score of 0.25 which is on the low end.Using PCA to reduce the dataset into 3 principal components we can plot the KMeans derived clusters into 2D and 3D visuals. PCA visualizations tend to aggregate clusters around a central point which makes interpretation difficult but we can see clusters 1 and 3 to have some distinct structure compared to clusters 0 and 2. However, when we plot the clusters into a 3D space we can clearly distinct all 4 clusters.First, let’s determine what is the optimal number of principal components we need. By examining the amount of variance each principal component encompasses we can see that the first 3 principal components explain roughly 70% of the variance. Finally, we apply PCA again and reduce our dataset to 3 principal components.Now that we have reduced the original dataset of 7 features to just 3 principal components let’s apply the KMeans algorithm. We once again needed to determine what is the optimal number of clusters and again it seems 4 is the right choice. It is important to remember we are now using the 3 principal components instead of the original 7 features to determine the optimal number of clusters.Now we are ready to apply KMeans on the PCA principal components. We can see that we were able to increase our silhouette score from 0.25 to 0.36 by passing KMeans a lower dimensional dataset. Looking at the 2D and 3D scatter plots we can see a significant improvement in the distinction between clusters.We can see a definite improvement in KMeans ability to cluster our data when we reduce the number of dimensions to 3 principal components. In this section we will reduce our data once again using t-SNE and compare KMeans results to that of PCA KMeans. We will reduce down to 3 t-SNE components. Please keep in mind t-SNE is a computationally heavy algorithm. Computational time can be reduced using the ‘n_iter’ parameter. Furthermore, the code you see below is a result of dozens iterations of the ‘Perplexity’ parameter. Anything above a perplexity of 80 tended to aggregate our data into one large disperse cluster.Below is the result of reducing our original dataset into 3 t-SNE components plotted into a 2D space.Once again it seems 4 is the magic number of clusters for our KMeans analysis.Applying KMeans to our 3 t-SNE derived components we were able to obtain a Silhouette score of 0.39. If you recall the Silhouette score obtained from KMeans on PCA’s 3 principal components was 0.36. A relatively small improvement but an improvement nonetheless.The interpretation of t-SNE can be slightly counterintuitive as the density of t-SNE clusters (ie. low dimensional space) is not proportionally related to the data relationships in the original (high dimensional space) dataset. In other words, we can have quality dense clusters as derived from KMeans but t-SNE might display them as very broad or even as multiple clusters, especially when perplexity is too low. Density, cluster size, the number of clusters (under the same KMeans cluster) and shape really have little meaning when it comes to reading t-SNE plots. We can have very broad, dense or even multiple clusters (especially when perplexity is too low) for the same KMeans cluster but that does not relate to the quality of the cluster. Rather, t-SNE’s major advantage is the distance and location of each KMeans cluster. Clusters which are close to each other will be more related to each other. However, that does not mean clusters which are far away from each are dissimilar proportionally. Finally, we want to see a certain degree of separation between KMeans clusters as displayed using t-SNE.Staring at PCA/t-SNE plots and comparing evaluation metrics such as the Silhouette score will give us a technical perspective on the clusters derived from KMeans. If we want to understand the clusters from a business perspective we need to examine the clusters against the original features. In other words, what types of employees make up the clusters we obtained from KMeans. Not only will this analysis help guide the business and potentially continued analysis but also give us insights into the quality of the clusters.Let’s first merge the KMeans clusters with the original unscaled features. We’ll create two separate data frames. One for the PCA derives KMeans clusters and one for the t-SNE KMeans clusters.Let’s begin with a univariate review of the clusters by comparing the clusters based on each individual feature.Where the real fun begins when we examine the clusters from a bivariate perspective. When we examine employee satisfaction and last evaluation scores, we can see that KMeans clusters derived from our principal components are much more defined. We will focus our examination of this bivariate relationship between satisfaction and performance on the clusters derived from PCA.Cluster 0 represents the largest amount of employees (7488) who on average are relatively satisfied at work with a very wide range of performance scores. Since this cluster represents almost 50% of the population it is not surprising we are seeing this wide range of satisfaction and performance scores. There is also a smaller but visible cluster of very satisfied and high performing employees.Cluster 1 represents a critical part of the workforce as these are very high performing employees but unfortunately, have extremely low employee satisfaction. The fact cluster 1 represents 20% of the workforce only adds to the severity of the situation. Unsatisfied employees are at a much greater risk of voluntary turnover. Their high-performance scores indicate not only their proficiency but potential institutional knowledge. Losing these employees would create a significant reduction in organizational knowledge and performance. It would be interesting to see the organizational level (ie. mgr, director, executive) of these employees. Losing a large portion of senior management is a significant problem.Cluster 2 seems to represent low performing employees who are on the lower end of the satisfaction continuum. Once again dissatisfied employees are at a higher risk of voluntary turnover and due to their low-performance evaluation we might consider these employees as ‘constructive turnover’. In other words, the voluntary turnover of these employees might actually be a good thing for the organization. We have to look at these results from two different perspectives. First, these employees make up over 25% of the population. If a significant portion of these employees quit the organization might be hard-pressed to have enough employees to successfully perform its function. Secondly, such a large portion of the population being dissatisfied and poorly performing speaks volumes about the recruiting, management, and training function of the organization. These employees might be the result of poorly constructed organizational initiatives. The company would be doing itself an enormous service if they delved deeper into what makes these employees dissatisfied and poorly performing.Finally, cluster 3 contains only 2% of the population and has no discernible distribution. We need a larger dataset to fully explore these employees.Very interesting, the bivariate relationship between satisfaction and performance scores is almost identical to satisfaction and average monthly hours. Once again the KMeans clusters derived from PCA are significantly more distinct, let’s focus our attention on the PCA features.It seems the employees in PCA cluster 0 not only have a wide range of performance but also average monthly hours. Overall, these results are promising as the majority of the workforce is overall happy, performing admirably, and working a significant amount of hours.Cluster 1 employees are once again dissatisfied and working the largest average monthly hours. If you recall these were also the very high performing employees. These long hours could very well have an impact on their overall job satisfaction. Keep in mind there is a smaller yet emerging group of cluster 1 employees who are overall satisfied and very high performing. There could be more than just 4 clusters in our dataset.The cluster 2 employees are not only low performing but also working the lowest average monthly hours. Keep in mind this dataset might be a little skewed as working 160 hours in a month is considered full-time.Finally, cluster 3 again does not present itself in the plots.As we continue with our bivariate plots we compare satisfaction and time spent in company or tenure. To no surprise, KMeans clusters from PCA principal components are much more distinct. The t-SNE plot has a similar shape to the PCA plot but its clusters are much more scattered. Looking at the PCA plots we have made an important discovery regarding cluster 0 or the vast majority (50%) of the employees. The employees in cluster 0 have primarily been with the company between 2 and 4 years. This is a fairly common statistic as the number of employees with extended tenure (ie 5 years plus) will decrease as tenure increases. Furthermore, their overall high satisfaction points to a potential increase in average tenure as satisfied employees are less likely to quit.Cluster 1 is a little hard to describe as its data points are scattered around two distinct clusters. Overall, their tenure doesn’t vary much between 4 and 6 years of tenure, however, their satisfaction is the opposite of each other. There is a cluster is high job satisfaction and a larger cluster with very low job satisfaction. It would seem cluster 1 might encompass two distinct groups of employees. Those who are very happy and very dissatisfied with their jobs. That said, despite their differences in satisfaction, their job performance, average monthly hours and tenure are on the higher end. It seems cluster 1 are either happy or unhappy very hardworking and committed employees.Cluster 2 follows the same pattern as the plots above. Not only are they dissatisfied at work, but they have one of the lowest average tenures as well. However, it is important to recall these employees account for 26% of the organization. Losing a significant portion of these employees would create issues. Lastly, dissatisfied employees are less likely to engage in organizational citizenship behaviors such as altruism, conscientiousness, helpfulness, and more likely to engage in counterproductive work behavior which can create toxic cultures. Dissecting who are these employees in terms of tenure, company level, turnover rates, location, and job types would be very helpful in diagnosing the issue and developing an action plan.Finally, cluster 3 is yet again too scattered to provide us with any useful information. There simply aren’t enough employees which encompass this cluster to discern any useful patterns.PCA derived clusters once again outperformed t-SNE, however, only marginally. Cluster 0 has continued to follow a similar trend, overall happy employees with an average number of projects. Nothing to be overly excited or worried about. This might point to the ideal number of projects for the organization’s employees.Cluster 1 employees are also following their trend we have seen from previous plots. Very dissatisfied employees who are being worked extremely hard when looking at the number of projects. If you recall, this group was also working very long hours during the month. We are once again seeing a smaller but visible group of satisfied employees working on an average number of projects. This was the same trend we saw with satisfaction X tenure, average monthly hours, and last evaluation score. There is definitely yet another group of very satisfied and hard-working employees. Gaining a more in-depth understanding of these employees in terms of recruiting (ie. sources, recruiters), management practices, compensation, might lead to insights into successful recruiting and hiring practices. These insights might also help the organization understand how to convert lower-performing employees.Cluster 2 is following its trend as well. Less satisfied employees working on a very small number of projects. This trend has been seen across all bivariate relationships with satisfaction.Cluster 3 is simply too small to make out any discernible trends.We can certainly continue examining the bivariate relationship between each cluster and our features but we are seeing a definitive trend. Below are the remaining relationship plots we urge you to examine.The purpose of this article was to examine the effects of different dimension reduction techniques (ie. PCA and t-SNE) will have on the quality of clusters produced by KMeans. We examined a relatively clean human resources dataset consisting of over 15,000 employees along with their job characteristics. From the original 7 features, we managed to reduce the dataset into 3 principal components and 3 t-SNE components. Our silhouette scores ranged from 0.25 (unreduced dataset) to 0.36 (PCA) and 0.39 (t-SNE). Upon visual inspection, both PCA and t-SNE derived KMeans clusters did a much better job clustering the data compared to the unreduced original 7 features. It wasn’t until we began to compare the PCA and t-SNE KMeans derived clusters in terms of employee job characteristics that we saw significant differences. The PCA KMeans extracted clusters seemed to produce much clearer and defined clusters.From a business perspective we would be doing the reader a disservice if the PCA KMeans clusters were not summarized as well.Cluster 0: This cluster encompassed the majority of the employees (50%) in the dataset. This cluster contains mostly the average employees. They maintain an average level of job satisfaction, their number of monthly worked hours has a very wide range but not extending to the extreme. The same is true regarding their performance as they mostly maintain satisfactory performance scores. The number of projects this cluster of employees is involved in also average between 3 and 5. The only outlier we saw for these employees is their relatively young tenure (2–4 yrs) with the company. These results are not uncommon as even organizations with enough employees will follow a Gaussian distribution on many factors. As we increase the sample size we will statistically have a higher probability of selecting employees that fall towards the middle of the distribution and the outliers will have less and less pull on the distribution (ie. regression to the mean). It is easy to see why KMeans created this cluster of employees.Cluster 1: There was a certain duality or juxtaposition to this group of employees at times. On one hand, we saw a very dissatisfied group of employees who achieved absolutely stunning performance scores, worked very long hours, managed an extreme number of projects, had an above-average tenure, and committing zero accidents. In other words, very hard working and valuable employees who were dissatisfied with the organization. The company would suffer a significant loss of productivity and knowledge if these employees began to turnover.On the other hand, we were seeing a smaller but visible cluster of very satisfied employees with amazing performance scores, above-average number of projects, monthly hours, and tenure. By all means, the model employee the organization was lucky to have. Satisfaction was definitely the splitting factor as the two unique groups tended to merge when we compare the group’s performance with tenure or even number of projects.It would seem there are additional clusters of employees KMeans did not identify. Perhaps 5 or even 6 clusters might have been better criteria for the KMeans algorithm.Cluster 2: Although this cluster was not as well defined as cluster 0, we were able to see a definite trend among the data. On average these employees were not overly satisfied with the organization. Their performance typically ranked towards the lower end of the scale. They worked the lowest number of monthly hours on the lowest number of projects. Their tenure hovered around 3 years which was average.Cluster 3: Finally, cluster 3 employees made up roughly 2% of the dataset which made it virtually impossible to identify any discernible trend in the data.",14/07/2020,34,17.0,7.0,747.0,526.0,70.0,1.0,0.0,6.0,en
3986,Everything You Need to Know About Artificial Neural Networks,"Technology, Invention, App, and More",Josh,42000.0,9.0,1986.0,"The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we’re learning more about how to improve their systems. Everything is starting to align, and because of it we’re seeing strides we’ve never thought possible until now. We have programs that can tell stories about pictures. We have cars that are driving themselves. We even have programs that create art. If you want to read more about advancements in 2015, read this article. Here at Josh.ai, with AI technology becoming the core of just about everything we do, we think it’s important to understand some of the common terminology and to get a rough idea of how it all works.A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). If you’ve read anything about them before, you’ll have read that these ANNs are a very rough model of how the human brain is structured. Take note that there is a difference between artificial neural networks and neural networks. Though most people drop the artificial for the sake of brevity, the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work. Below is a diagram of actual neurons and synapses in the brain compared to artificial ones.Fear not if the diagram doesn’t come through very clearly. What’s important to understand here is that in our ANNs we have these units of calculation called neurons. These artificial neurons are connected by synapses which are really just weighted values. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it “travels.” The weighted result can sometimes be the output of your neural network, or as I’ll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning.Artificial neural networks are not a new concept. In fact, we didn’t even always call them neural networks and they certainly don’t look the same now as they did at their inception. Back during the 1960s we had what was called a perceptron. Perceptrons were made of McCulloch-Pitts neurons. We even had biased perceptrons, and ultimately people started creating multilayer perceptrons, which is synonymous with the general artificial neural network we hear about now.But wait, if we’ve had neural networks since the 1960s, why are they just now getting huge? It’s a long story, and I encourage you to listen to this podcast episode to listen to the “fathers” of modern ANNs talk about their perspective of the topic. To quickly summarize, there’s a hand full of factors that kept ANNs from becoming more popular. We didn’t have the computer processing power and we didn’t have the data to train them. Using them was frowned upon due to them having a seemingly arbitrary ability to perform well. Each one of these factors is changing. Our computers are getting faster and more powerful, and with the internet, we have all kinds of data being shared for use.You see, I mentioned above that the neurons and synapses perform calculations. The question on your mind should be: “How do they learn what calculations to perform?” Was I right? The answer is that we need to essentially ask them a large amount of questions, and provide them with answers. This is a field called supervised learning. With enough examples of question-answer pairs, the calculations and values stored at each neuron and synapse are slowly adjusted. Usually this is through a process called backpropagation.Imagine you’re walking down a sidewalk and you see a lamp post. You’ve never seen a lamp post before, so you walk right into it and say “ouch.” The next time you see a lamp post you scoot a few inches to the side and keep walking. This time your shoulder hits the lamp post and again you say “ouch.” The third time you see a lamp post, you move all the way over to ensure you don’t hit the lamp post. Except now something terrible has happened — now you’ve walked directly into the path of a mailbox, and you’ve never seen a mailbox before. You walk into it and the whole process happens again. Obviously, this is an oversimplification, but it is effectively what backpropogation does. An artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given. When it is wrong, an error is calculated and the values at each neuron and synapse are propagated backwards through the ANN for the next time. This process takes a LOT of examples. For real world applications, the number of examples can be in the millions.Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there’s another question that should be on your mind. How do we know how many neurons we need to use? And why did you bold the word layers earlier? Layers are just sets of neurons. We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.Layers themselves are just sets of neurons. In the early days of multilayer perceptrons, we originally thought that having just one input layer, one hidden layer, and one output layer was sufficient. It makes sense, right? Given some numbers, you just need one set of computations, and then you get an output. If your ANN wasn’t calculating the correct value, you just added more neurons to the single hidden layer. Eventually, we learned that in doing this we were really just creating a linear mapping from each input to the output. In other words, we learned that a certain input would always map to a certain output. We had no flexibility and really could only handle inputs we’d seen before. This was by no means what we wanted.Now introduce deep learning, which is when we have more than one hidden layer. This is one of the reasons we have better ANNs now, because we need hundreds of nodes with tens if not more layers. This leads to a massive amount of variables that we need to keep track of at a time. Advances in parallel programming also allow us to run even larger ANNs in batches. Our artificial neural networks are now getting so large that we can no longer run a single epoch, which is an iteration through the entire network, at once. We need to do everything in batches which are just subsets of the entire network, and once we complete an entire epoch, then we apply the backpropagation.Along with now using deep learning, it’s important to know that there are a multitude of different architectures of artificial neural networks. The typical ANN is setup in a way where each neuron is connected to every other neuron in the next layer. These are specifically called feed forward artificial neural networks (even though ANNs are generally all feed forward). We’ve learned that by connecting neurons to other neurons in certain patterns, we can get even better results in specific scenarios.Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn’t make decisions based on previous knowledge. A typical ANN had learned to make decisions based on context in training, but once it was making decisions for use, the decisions were made independent of each other.When would we want something like this? Well, think about playing a game of Blackjack. If you were given a 4 and a 5 to start, you know that 2 low cards are out of the deck. Information like this could help you determine whether or not you should hit. RNNs are very useful in natural language processing since prior words or characters are useful in understanding the context of another word. There are plenty of different implementations, but the intention is always the same. We want to retain information. We can achieve this through having bi-directional RNNs, or we can implement a recurrent hidden layer that gets modified with each feedforward. If you want to learn more about RNNs, check out either this tutorial where you implement an RNN in Python or this blog post where uses for an RNN are more thoroughly explained.An honorable mention goes to Memory Networks. The concept is that we need to retain more information than what an RNN or LSTM keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other.Sam walks into the kitchen.Sam picks up an apple.Sam walks into the bedroom.Sam drops the apple.Q: Where is the apple.A: BedroomSample taken from this paper.Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. However, the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized. This is done by noting a certain symmetry in how the neurons are connected, and so you can essentially “re-use” neurons to have identical copies without necessarily needing the same number of synapses. CNNs are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels. There’s redundant information contained when you look at each individual pixel compared to its surrounding pixels, and you can actually compress some of this information thanks to their symmetrical properties. Sounds like the perfect situation for a CNN if you ask me. Christopher Olah has a great blog post about understanding CNNs as well as other types of ANNs which you can find here. Another great resource for understanding CNNs is this blog post.The last ANN type that I’m going to talk about is the type called Reinforcement Learning. Reinforcement Learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward, which means that it in itself isn’t an artificial neural network architecture. However, you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before. A great example and explanation can be found in this video, where YouTube user SethBling creates a reinforcement learning system that builds an artificial neural network architecture that plays a Mario game entirely on its own. Another successful example of reinforcement learning can be seen in this video where the company DeepMind was able to teach a program to master various Atari games.Now you should have a basic understanding of what’s going on with the state of the art work in artificial intelligence. Neural networks are powering just about everything we do, including language translation, animal recognition, picture captioning, text summarization and just about anything else you can think of. You’re sure to hear more about them in the future so it’s good that you understand them now!This post was written by Aaron at Josh.ai. Previously, Aaron worked at Northrop Grumman before joining the Josh team where he works on natural language programming (NLP) and artificial intelligence (AI). Aaron is a skilled YoYo expert, loves video games and music, has been programming since middle school and recently turned 21.Josh.ai is an AI agent for your home. If you’re interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.Like Josh on Facebook — http://facebook.com/joshdotaiFollow Josh on Twitter — http://twitter.com/joshdotai",28/12/2015,0,26.0,6.0,578.0,375.0,9.0,0.0,0.0,32.0,en
3987,Liquid Neural Networks in Computer Vision,Towards Data Science,Jacob Solawetz,443.0,4.0,637.0,"Excitement is building in the artificial intelligence community around MIT’s recent release of liquid neural networks. The breakthroughs that Hasani and team have made are incredible.Let’s dive in.Artificial intelligence research and applications involve the construction and training of deep neural networks. Until liquid neural networks, all deep learning systems have shared the same vulnerability — namely, that they learn a fixed mapping from input data to output prediction based on the training data that they are shown, making them brittle to the shifting environment around them. Furthermore, most deep learning models are context independent. For example, when applying an object detection model or a classification model to a video, the video will be processed frame by frame without relying on the context around it.To fix this problem, developers and engineers using artificial intelligence typically gather very large, representative datasets and engage in active learning to continuously improve their systems through re-training cycles as new edge cases are discovered.However, all of this re-labeling, re-training and re-deployment can be tedious — wouldn’t it be nice if the network you were using learned to adapt to new scenarios online?Enter the Liquid Neural Network.The Liquid neural network is a form of recurrent neural network that processes data in time series. Recurrent neural networks have shown strong performance and making predictions based on sequences such as text streams, or time series of scientific measurements.Recurrent neural networks exceed the performance of normal, feed forward neural networks when the input data is a sequence, because they can more efficiently keep track of relevant information at different parts of the sequence.The liquid neural network builds on the recurrent neural network by making hidden states that are dynamic on the time constant in the time series. At each prediction step, the liquid neural network is computing both the predicted outcome as well as the formation of the next hidden state, evolving in time.For more implementation details, we recommend a deep dive into the liquid neural network paper.The liquid neural network is shown to improve on time series modeling across a variety of domains — human gestures, human activities, traffic, power, ozone, sequential MNIST, and occupancy. These initial results are promising.The long term goal of a liquid neural network is to form a system that is flexible with scene and time, eventually unlocking a system that does not have to be consistently improved via active learning.In the long term, architectures like liquid neural networks may dramatically improve our ability to train resilient models in video processing, that are able to adapt around their changing environment.However, the impact of liquid neural networks on vision is a long way off. The initial experiments in the liquid neural network repository are all time series based, not based on images and video.Processed files: 1. Vectorial velocity of left hand (x coordinate) 2. Vectorial velocity of left hand (y coordinate) 3. Vectorial velocity of left hand (z coordinate) 4. Vectorial velocity of right hand (x coordinate) 5. Vectorial velocity of right hand (y coordinate) 6. Vectorial velocity of right hand (z coordinate) 7. Vectorial velocity of left wrist (x coordinate) 8. Vectorial velocity of left wrist (y coordinate)Example data for the time series data for gestures in liquid neural networksGeneralizing to image and video for application from their repository, will inevitably involve future research and construction from the lab at MIT.Liquid neural networks are a new breakthrough in recurrent neural networks, creating a model that adopts flexibly through time. The research, however, is in progress and it will likely be a while before we see these networks making an impact over incumbent strategies in computer vision.For more on recent research in computer vision, check out our posts on:If you’d like to get started training the current state of the art in computer vision, check out:Originally published at https://blog.roboflow.com on February 12, 2021.",12/02/2021,0,0.0,12.0,861.0,482.0,5.0,2.0,0.0,15.0,en
3988,Yes you should understand backprop,Medium,Andrej Karpathy,44000.0,7.0,1400.0,"When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:“Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?”This is seemingly a perfectly sensible appeal - if you’re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of “it’s worth knowing what’s under the hood as an intellectual curiosity”, or perhaps “you might want to improve on the core algorithm later”, but there is a much stronger and practical argument, which I wanted to devote a whole post to:> The problem with Backpropagation is that it is a leaky abstraction.In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.We’re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.TLDR: If you understand backpropagation and your network has ReLUs, you’re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I’ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b…)? This sequence either goes to zero if |b| < 1, or explodes to infinity when |b|>1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.TLDR: If you understand backpropagation and you’re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.Lets look at one more — the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector — turns out this trivial operation is not supported in TF). Anyway, I searched “dqn tensorflow”, clicked the first link, and found the core code. Here is an excerpt:If you’re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s’,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:It’s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can’t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.I submitted an issue on the DQN repo and this was promptly fixed.Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because “TensorFlow automagically makes my networks learn”, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.That’s it for now! I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I’m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)",19/12/2016,3,40.0,3.0,958.0,297.0,5.0,0.0,0.0,9.0,en
3989,New to Machine Learning? Avoid these three mistakes,Medium,James Faghmous,549.0,6.0,50.0,"Machine learning (ML) is one of the hottest fields in data science. As soon as ML entered the mainstream through Amazon, Netflix, and Facebook people have been giddy about what they can learn from their data. However, modern machine learning (i.e. not the theoretical statistical learning that emerged in the…",07/11/2013,0,0.0,1.0,900.0,350.0,1.0,0.0,0.0,0.0,en
3990,"Difference between AlexNet, VGGNet, ResNet, and Inception",Towards Data Science,Aqeel Anwar,2100.0,9.0,1180.0,"In this tutorial, I will quickly go through the details of four of the famous CNN architectures and how they differ from each other by explaining their W3H (When, Why, What, and How)When?Why? AlexNet was born out of the need to improve the results of the ImageNet challenge. This was one of the first Deep convolutional networks to achieve considerable accuracy on the 2012 ImageNet LSVRC-2012 challenge with an accuracy of 84.7% as compared to the second-best with an accuracy of 73.8%. The idea of spatial correlation in an image frame was explored using convolutional layers and receptive fields.What? The network consists of 5 Convolutional (CONV) layers and 3 Fully Connected (FC) layers. The activation used is the Rectified Linear Unit (ReLU). The structural details of each layer in the network can be found in the table below.The network has a total of 62 million trainable variablesHow? The input to the network is a batch of RGB images of size 227x227x3 and outputs a 1000x1 probability vector one corresponding to each class.When?Why? VGGNet was born out of the need to reduce the # of parameters in the CONV layers and improve on training time.What? There are multiple variants of VGGNet (VGG16, VGG19, etc.) which differ only in the total number of layers in the network. The structural details of a VGG16 network have been shown below.VGG16 has a total of 138 million parameters. The important point to note here is that all the conv kernels are of size 3x3 and maxpool kernels are of size 2x2 with a stride of two.How? The idea behind having fixed size kernels is that all the variable size convolutional kernels used in Alexnet (11x11, 5x5, 3x3) can be replicated by making use of multiple 3x3 kernels as building blocks. The replication is in terms of the receptive field covered by the kernels.Let’s consider the following example. Say we have an input layer of size 5x5x1. Implementing a conv layer with a kernel size of 5x5 and stride one will result in an output feature map of 1x1. The same output feature map can be obtained by implementing two 3x3 conv layers with a stride of 1 as shown belowNow let’s look at the number of variables needed to be trained. For a 5x5 conv layer filter, the number of variables is 25. On the other hand, two conv layers of kernel size 3x3 have a total of 3x3x2=18 variables (a reduction of 28%).Similarly, the effect of one 7x7 (11x11) conv layer can be achieved by implementing three (five) 3x3 conv layers with a stride of one. This reduces the number of trainable variables by 44.9% (62.8%). A reduced number of trainable variables means faster learning and more robust to over-fitting.When?Why? Neural Networks are notorious for not being able to find a simpler mapping when it exists.ResNet addresses this network by introducing two types of ‘shortcut connections’: Identity shortcut and Projection shortcut.What? There are multiple versions of ResNetXX architectures where ‘XX’ denotes the number of layers. The most commonly used ones are ResNet50 and ResNet101. Since the vanishing gradient problem was taken care of (more about it in the How part), CNN started to get deeper and deeper. Below we present the structural details of ResNet18Resnet18 has around 11 million trainable parameters. It consists of CONV layers with filters of size 3x3 (just like VGGNet). Only two pooling layers are used throughout the network one at the beginning and the other at the end of the network. Identity connections are between every two CONV layers. The solid arrows show identity shortcuts where the dimension of the input and output is the same, while the dotted ones present the projection connections where the dimensions differ.How? As mentioned earlier, ResNet architecture makes use of shortcut connections to solve the vanishing gradient problem. The basic building block of ResNet is a Residual block that is repeated throughout the network.Instead of learning the mapping from x →F(x), the network learns the mapping from x → F(x)+G(x). When the dimension of the input x and output F(x) is the same, the function G(x) = x is an identity function and the shortcut connection is called Identity connection. The identical mapping is learned by zeroing out the weights in the intermediate layer during training since it's easier to zero out the weights than push them to one.For the case when the dimensions of F(x) differ from x (due to stride length>1 in the CONV layers in between), the Projection connection is implemented rather than the Identity connection. The function G(x) changes the dimensions of input x to that of output F(x). Two kinds of mapping were considered in the original paper.When?Why? In an image classification task, the size of the salient feature can considerably vary within the image frame. Hence, deciding on a fixed kernel size is rather difficult. Lager kernels are preferred for more global features that are distributed over a large area of the image, on the other hand, smaller kernels provide good results in detecting area-specific features that are distributed across the image frame. For effective recognition of such a variable-sized feature, we need kernels of different sizes. That is what Inception does. Instead of simply going deeper in terms of the number of layers, it goes wider. Multiple kernels of different sizes are implemented within the same layer.What? The Inception network architecture consists of several inception modules of the following structureEach inception module consists of four operations in parallelThe 1x1 conv blocks shown in yellow are used for depth reduction. The results from the four parallel operations are then concatenated depth-wise to form the Filter Concatenation block (in green). There is multiple version of Inception, the simplest one being the GoogLeNet.How? Inception increases the network space from which the best network is to be chosen via training. Each inception module can capture salient features at different levels. Global features are captured by the 5x5 conv layer, while the 3x3 conv layer is prone to capturing distributed features. The max-pooling operation is responsible for capturing low-level features that stand out in a neighborhood. At a given level, all of these features are extracted and concatenated before it is fed to the next layer. We leave for the network/training to decide what features hold the most values and weight accordingly. Say if the images in the data-set are rich in global features without too many low-level features, then the trained Inception network will have very small weights corresponding to the 3x3 conv kernel as compared to the 5x5 conv kernel.In the table below these four CNNs are sorted w.r.t their top-5 accuracy on the Imagenet dataset. The number of trainable parameters and the Floating Point Operations (FLOP) required for a forward pass can also be seen.Several comparisons can be drawn:Compact cheat sheets for this topic and many other important topics in Machine Learning can be found in the link belowmedium.comIf this article was helpful to you, feel free to clap, share and respond to it. If want to learn more about Machine Learning and Data Science, follow me @",07/06/2019,0,28.0,16.0,1226.0,917.0,10.0,9.0,0.0,3.0,en
3991,"A Hands-On Guide To Text Classification With Transformer Models (XLNet, BERT, XLM, RoBERTa)",Towards Data Science,Thilina Rajapakse,1400.0,8.0,1193.0,"Please consider using the Simple Transformers library as it is easy to use, feature-packed, and regularly updated. The article still stands as a reference to BERT models and is likely to be helpful with understanding how BERT works. However, Simple Transformers offers a lot more features, much more straightforward tuning options, all the while being quick and easy to use! The links below should help you get started quickly.The Pytorch-Transformers (now Transformers) library has moved on quite a bit since this article was written. I recommend using SimpleTransformers as it is kept up to date with the Transformers library and is significantly more user-friendly. While the ideas and concepts in this article still stand, the code and the Github repo are no longer actively maintained.I highly recommend cloning the Github repo for this article and running the code while you follow the guide. It should help you understand both the guide and the code better. Reading is great, but coding is better. 😉Special thanks to Hugging Face for their Pytorch-Transformers library for making Transformer Models easy and fun to play with!Transformer models have taken the world of Natural Language Processing by storm, transforming (sorry!) the field by leaps and bounds. New, bigger, and better models seem to crop up almost every month, setting new benchmarks in performance across a wide variety of tasks.This post is intended as a straightforward guide to utilizing these awesome models for text classification tasks. As such, I won’t be talking about the theory behind the networks, or how they work under the hood. If you are interested in diving into the nitty-gritty of Transformers, my recommendation is Jay Alammar’s Illustrated Guides here.This also serves as an update to my earlier guide on Using BERT for Binary Text Classification. I’ll be using the same dataset (Yelp Reviews) that I used the last time to avoid having to download a new dataset because I’m lazy and I have terrible internet. The motivation behind the update is down to several reasons, including the update to the HuggingFace library I used for the previous guide, as well as the release of multiple new Transformer models which have managed to knock BERT off its perch.With the background set, let’s take a look at what we’ll be doing.I’ll be using two Jupyter Notebooks, one for data preparation, and one for training and evaluation.Most online datasets will typically be in .csv format. Following the norm, the Yelp dataset contains two csv files train.csv and test.csv.Kicking off our first (data preparation) notebook, let’s load the csv files in with Pandas.However, the labels used here break the norm by being 1 and 2 instead of the usual 0 and 1. I’m all for a bit of rebellion, but this just puts me off. Let’s fix this so that the labels are 0 and 1, indicating a bad review and good review respectively.We need to do some final bit of retouching before our data is ready for the Pytorch-Transformer models. The data needs to be in tsv format, with four columns, and no header.So, let’s get the data in order, and save it in tsv format.This marks the end of the data preparation Notebook, and we’ll continue with the training Notebook from the next section.Before we can start the actual training, we need to convert our data from text into numerical values that can be fed into neural networks. In the case of Transformer models, the data will be represented as InputFeature objects.To make our data Transformer-ready, we’ll be using the classes and functions in the file utils.py. (Brace yourself, a wall of code incoming!)Let’s look at the important bits.The InputExample class represents a single sample of our dataset;The DataProcessor and BinaryProcessor classes are used to read in the data from tsv files and convert it into InputExamples.The InputFeature class represents the pure, numerical data that can be fed to a Transformer.The three functions convert_example_to_feature, convert_examples_to_features, _truncate_seq_pair are used to convert InputExamples into InputFeatures which will finally be sent to the Transformer model.The conversion process includes tokenization, and converting all sentences to a given sequence length (truncating longer sequences, and padding shorter sequences). During tokenization, each word in the sentence is broken apart into smaller and smaller tokens (word pieces) until all the tokens in the dataset are recognized by the Transformer.As a contrived example, let’s say we have the word understanding. The Transformer we are using does not have a token for understanding but it has separate tokens for understand and ing. Then, the word understanding would be broken into the tokens understand and ing. The sequence length is the number of such tokens in the sequence.The convert_example_to_feature function takes a single sample of data and converts it into an InputFeature. The convert_examples_to_features function takes a list of examples and returns a list of InputFeatures by using the convert_example_to_feature function. The reason behind there being two separate functions is to allow us to use Multiprocessing in the conversion process. By default, I’ve set the process count to cpu_count() - 2, but you can change it by passing a value for the process_count parameter in the convert_examples_to_features function.Now, we can go to our training notebook and import the stuff we’ll use and configure our training options.Go through the args dictionary carefully and note all the different settings you can configure for training. In my case, I am using fp16 training to lower memory usage and speed up training. If you don’t have Nvidia Apex installed, you will have to turn off fp16 by setting it to False.In this guide, I am using the XL-Net model with a sequence length of 128. Please refer to the Github repo for the full list of available models.Now, we are ready to load our model for training.The coolest thing about the Pytorch-Transformers library is that you can use any of the MODEL_CLASSES above, just by changing the model_type and model_name in the arguments dictionary. The process for fine-tuning, and evaluating is basically the same for all the models. All hail HuggingFace!Next, we have functions defining how to load data, train a model, and to evaluate a model.Finally, we have everything ready to tokenize our data and train our model.It should be fairly straightforward from here.This will convert the data into features and start the training process. The converted features will be automatically cached, and you can reuse them later if you want to run the same experiment. However, if you change something like the max_seq_length, you will need to reprocess the data. Same goes for changing the model used. To reprocess the data, simply set reprocess_input_data to True in the args dictionary.For comparison, this dataset took about 3 hours for training on my RTX 2080.Once training completes, we can save everything.Evaluation is quite easy as well.Without any parameter tuning, and with one training epoch, my results are as follows.Not too shabby!Transformer models have displayed incredible prowess in handling a wide variety of Natural Language Processing tasks. Here, we’ve looked at how we can use them for one of the most common tasks, which is Sequence Classification.The Pytorch-Transformers library by HuggingFace makes it almost trivial to harness the power of these mammoth models!",03/09/2019,1,4.0,30.0,1400.0,787.0,1.0,7.0,0.0,26.0,en
3992,Demystifying Object Detection using Deep Learning,Medium,Rafi,52.0,5.0,882.0,"Object detection has been quite a center of attraction nowadays because of its wide range of applications and advancements in Deep Learning technology. Object Detection is a subdomain of image processing and computer vision that deals with identifying and localizing objects in videos or digital images. The credit for the evolution of object detection goes to the breakthrough in deep learning classification algorithms called CNN- Convolutional Neural Network and Graphic Processing Units that have shown great leads in the development of real-world solutions for computer vision problems like autonomous driving car, face detection and recognition, people detection, and tracking, video surveillance, security system design, etc.Object detection can be done either using machine learning or deep learning approaches. The difference is that machine learning uses the handcrafted features for identifying the objects in an image or videos while deep learning learns the semantic, high level, deeper features by themselves using back-propagation algorithms and applying convolution neural networks. Due to the tremendous advancement in SOTA algorithms in image classification challenge using the Imagenet dataset, object detection has also been an active area of research.The problem definition of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object classification). So the pipeline of traditional object detection models can be mainly divided into three stages:The classical object detection pipeline inferencing is very time consuming and it takes time to detect an object which is not feasible for real-time application for object detection and classification.For instance, OpenCV AdaBoost uses the sliding window and the image pyramid to generate a detection frame and the R-CNN uses the Selective Search to find a region of interest.Based on the type and number of neural network architectures used in designing the detector system, the deep learning object detector can be classified into two categories:The two-stage detectors as the name suggest is a combination of two Deep Neural Network architectures. The first network called RPN (Region Proposal Network) finds the candidate object bounding boxes and second extracts the features from the candidate bounding proposals. Together these summed up to classify the object. Two-stage detectors have higher accuracy and precision but less inferencing speed because of its complexity in the network.A single-stage detector straightforwardly predicts the bounding boxes and class-label probabilities from the input image. It's kind of a regression problem that predicts less than 100 bounding boxes that speedup the inferencing time and thus is time-efficient and can be used in realtime applications.Faster RCNN is a two-stage object detector which is a combination of two components that is -Fast RCNN plus the RPN — region proposal network, which is a fully convolutional network that generates the high-quality region proposals by predicting the object bounds and objectness scores which are further fed to the fast RCNN for detecting the objects. The RPN uses an attention mechanism of neural network for telling Fast RCNN, where to look for the regions. RPN accelerates the generating speed of region proposals because it shares full-image convolutional features and a common set of convolutional layers with the detection network.“ A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation.”YOLO object detection takes whole image as an input and process it for finding the detected object bounding box and its belongingness probabilities. The yolo model is extremely fast and it’s base model can process image upto 45 frames per second. The framework works in a way that firstly it takes image as an input then divides the image into n*n grids. Finally, the image classification and object detection is applied to every grid which further predicts the bounding box of the detected object and it’s class probabilites.SSD is a single-stage object detector. It uses single deep neural network for the detection of the object. The feature map generated by deep neural network discritized the bounding boxes into plethora of default boxes into different aspect ratio and scales which at the time of prediction, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape.SSD is simply relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component.RetinaNet is single-stage object detector with new loss function known as focal loss rather than the standard cross entropy loss which solves the problem that one stage-detector have like class imabalance which cause the loss of accuracy in comparision to the two-stage detector.Class imbalance during training is one of the main obstacle impeding one-stage detector from achieving state-of-the-art accuracy and this where comes a new loss function that eliminates this barrier.Focal Loss: The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training. The focal loss for the cross-entropy (CE) loss(binary classification) is given below:The object detection algorithms have evolved over time and in no time there are some high standard algorithms that came into existence which has a great role in the application of computer vision applications.References:https://arxiv.org/pdf/1809.02165.pdfhttps://arxiv.org/pdf/1809.02165.pdfhttps://arxiv.org/abs/1506.01497https://arxiv.org/pdf/1512.02325.pdfhttps://arxiv.org/pdf/1708.02002.pdf",28/12/2019,0,23.0,2.0,1143.0,507.0,8.0,3.0,0.0,5.0,en
3993,Paper repro: Deep Metalearning using “MAML” and “Reptile”,Towards Data Science,Adrien Lucas Ecoffet,1900.0,8.0,1588.0,"In this post I reproduce two recent papers in the field of metalearning: MAML and the similar Reptile. The full notebook for this reproduction can be found here.The goal of both of these papers is to solve the K-shot learning problem. In K-shot learning, we need to train a neural network to generalize based on a very small number of examples (often on the order of 10 or so) instead of the often thousands of examples we see in datasets like ImageNet.However, in preparation for K-shot learning, you are allowed to train on many similar K-shot problems to learn the best way to generalize based on only K examples.This is learning to learn or metalearning. We have already seen metalearning in my post on “Learning to Learn by Gradient Descent by Gradient Descent”, which you can find here:becominghuman.aiThe metalearning approach of both Reptile and MAML is to come up with an initialization for neural networks that is easily generalizable to similar tasks. This is different to “Learning to Learn by Gradient Descent by Gradient Descent” in which we weren’t learning an initialization but rather an optimizer.This approach is very similar to transfer learning, in which we train a network on, say, ImageNet, and it later turns out that fine-tuning this network makes it easy to learn another image dataset, with much less data. Indeed, transfer learning can be seen as a form of metalearning. Indeed, it can be used to learn from very small datasets as you can see here.The difference here is that the initial network was trained with the explicit purpose of being easily generalizable, whereas transfer learning just “accidentally” happens to work, and thus might not work optimally.Indeed, it is fairly easy to find a in which transfer learnings fails to learn a good initialization. For this we need to look at the 1D sine wave regression problem.In this K-shot problem, each task consists in learning a modified sine function. Specifically, for each task, the underlying function will be of the form y = a sin(x + b), with both a and b chosen randomly, and the goal of our neural network is to learn to find y given x based on only 10 (x, y) pairs.Let’s plot a couple of example sine wave tasks:To understand why this is going to be a problem for transfer learning, let’s plot 1,000 of them:Looks like there is a lot of overlap at each x value, to say the least…Since there are multiple possible values for each x across multiple tasks, if we train a single neural net to deal with multiple tasks at the same time, its best bet will simply be to return the average y value across all tasks for each x. So what is the average y value for each x?The average is basically 0, which means a neural network trained on a lot of tasks would simply return 0 everywhere! It is unclear that this will actually help very much, and yet this is the transfer learning approach in this case…Let’s see how well it does by actually implementing a simple model to solve these sine wave tasks and training it using a transfer learning. First, the model itself:You’ll notice that it is implemented in a weird way (what’s a “ModifiableModule”? What’s a “GradLinear”?). This is because we will later train it using MAML. For details on what some of these classes are, check out the notebook, but for now you can assume they are similar to nn.Module and nn.Linear.Now, let’s train it for a while on a bunch of different random tasks in sequence:And here’s what happen when we try to fine-tune this transfer model to a specific random task:Basically it looks like our transfer model learns a constant function and that it is really hard to fine tune it to something better. It’s not even clear that our transfer learning is any better than random initialization… And indeed it isn’t! A random initialization ends up getting a better loss over time than fine tuning our transfer model.We now come to MAML, the first of the two algorithms we will look at today.As mentioned before, we are trying to find a set of weights such that running gradient descent on similar tasks makes progress as quickly as possible. MAML takes this extremely literally by running one iteration of gradient descent and then updating the initial weights based on how much progress that one iteration made towards the true task. More concretely it:We thus need to take a gradient of a gradient, aka a second degree derivative in this process. Fortunately this is something that PyTorch supports now, unfortunately PyTorch makes it a bit awkward to update the parameters of a model in a way that we can still run gradient descent through them (we already saw this is “Learning to Learn by Gradient Descent by Gradient Descent”), which explains the weird way in which the model is written.Because we are going to use second derivatives, we need to make sure that the computational graph that allowed us to compute the original gradients stays around, which is why we pass create_graph=True to .backward().So how does it work on a specific random task?Wow that’s much better, even after a single step of gradient descent the sine shape starts being visible, and after 10 steps the center of the wave is almost fully correct. Is this reflected in the learning curve? Yes!Unfortunately, it is a bit annoying that we have to use second order derivatives for this… it forces the code to be complicated and it also makes things a fair bit slower (around 33% according to the paper, which matches what we shall see here).Is there an approximation of MAML that doesn’t use the second order derivatives? Of course! We can simply pretend that the gradients that we used for the inner gradient descent just came out of nowhere, and thus just improve the initial parameters without taking into account these second order derivatives. Let’s add a first_order parameter to our MAML training function to deal with this:So how good is this first order approximation? Almost as good as the original MAML, as it turns out, and it is indeed about 33% faster.The first order approximation for MAML tells us that something interesting is going on: after all, it seems like how the gradients were generated should be relevant for a good initialization, and yet it apparently isn’t so much.Reptile takes this idea even further by telling us to do the following: run SGD for a few iterations on a given task, and then move your initialization weights a little bit in the direction of the weights you obtained after your k iterations of SGD. An algorithm so simple, it takes only a couple lines of pseudocode:When I first read this, I was quite consternated: isn’t this the same as training your weights alternatively on each task, just like in transfer learning? How would this ever work?Indeed, the Reptile paper anticipates this very reaction:You might be thinking “isn’t this the same as training on the expected loss Eτ [Lτ]?” and then checking if the date is April 1st.As it happens, I am writing this on April 2nd, so this is all serious. So what’s going on?Well, indeed if we had run SGD for a single iteration, we would have something equivalent to the transfer learning described above, but we aren’t, we are using a few iterations, and so indeed the weights we update towards each time actually depend indirectly on the second derivatives of the loss, similar to MAML.Ok, but still, why would this work? Well Reptile provides a compelling intuition for this: for each task, there are weights that are optimal. Indeed, there are probably many sets of weights that are optimal. This means that if you take several tasks, there should be a set of weights for which the distance to at least one optimal set of weights for each task is minimal. This set of weights is where we want to initialize our networks, since it is likely to be the one for which the least work is necessary to reach the optimum for any task. This is the set of weights that Reptile finds.We can see this expressed visually in the following image: the two black lines represent the sets of optimal weights for two different tasks, while the gray line represents the initialization weights. Reptile tries to get the initialization weights closer and closer to the point where the optimal weights are nearest to each other.Let’s now implement Reptile and compare it to MAML:How does it look on a random problem? Beautiful:What about the learning curve?It looks like Reptile does indeed achieve similar or even slightly better performance to MAML with a much simpler and slightly faster algorithm!All this applies to many more problems than just this toy example of sine waves. For more details, I really do recommend you read the papers. At this point, you should have enough background to understand them quite easily.What would be really interesting in the future would be to apply these approaches not just to the K-shot learning problem but to bigger problems: transfer learning has been extremely successful in the image classification world for training models based on medium sized datasets (a few hundreds or thousands, as opposed to the around 10-ish that is common in K-shot learning). Would training a resnet network using Reptile produce something that is even more amenable to transfer learning than the models we have now?",03/04/2018,0,7.0,13.0,456.0,297.0,13.0,1.0,0.0,6.0,en
3994,"深度學習優化器Ranger: a synergistic optimizer using RAdam (Rectified Adam), Gradient Centralization and LookAhead筆記",Medium,Patty Wu,13.0,7.0,27.0,今年人工智慧年會中，偶然聽到講師呼籲大家，都2020了，不要再用Adam了，請改用Ranger，因此著手來寫一篇Ranger的筆記。今年有兩篇優化器相關的論文被提出，分別是LookAhead和RAdam，這兩種方法用不同角度對深度學習的優化做改進，後來研究員 Less Wright將兩個方法整合成一個新的優化器：Ranger，得到了更好的成果。廢話不多說，先上PyTorch實現的GitHub：https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer要了解RAdam 和 LookAhead 是如何互補的，需要先分別討論他們的概念。全名是Rectified Adam，白話地說，就是自動熱身(warmup)版的Adam。概念Adam是一種常用的自適應學習率 (adptive learning rate) 優化器，但類方法在訓練的初期，adptive learning rate的變異非常大，然後在少量的數據進行過度跳躍，下了錯誤決策，就容易收斂在local minimum。為了解決這個問題，RAdam根據adaptive rate的變異程度去修正learning rate，讓Adam可以自動熱身，不需再手動調整，也避免模型收斂在local minimum。概念是這樣：有個熱身用的開關，閥值為rho，這個rho代表adpative learning rate分配的自由度：優點如此的做法，讓RAdam在享有Adam快速收斂優勢的同時，又達到跟SGD差不多好的收斂結果。RAdam詳細概念可以參考我寫的另一篇文章：https://is.gd/2yxrE7。2020由深度學習教父Geoffrey Hinton團隊發表的論文，LookAhead基於損失空間非常複雜，因此設計出兩組weight：fast跟slow weight。概念LookAhead可以任意選擇搭配的優化器，先快速的探索，並更新fast的權重 (inner update)。每k個mini-batch時，利用fast的權重更新slow的權重，再用slow的權重回去更新fast的權重(outer update)，如此為一個循環，讓fast不容易走錯路或落入local minimum。流程可見下圖右。下圖左是準確率的等高線圖，可以看到當fast(藍)在探索時，因為slow權重的更新，直接拉動fast權重，讓它可以往準確率更高的區域繼續做探索。反觀SGD在20個epochs內都沒有探索到那塊區域。更新方式就是指數移動平均：表現由下圖可以看到LookAhead在Cifar-100上表現優秀，不只贏過了Adam，也贏過了SGD (差異不大就是了...)。下圖可以發現，fast(藍)的更新常常會誤入歧途，降低準確率，但藉由slow的更新，會把它拉回準確率更高的區塊，增加訓練的穩定度。優點好的，上面介紹完兩種不同的優化器，它們用不同角度改進了優化器，我們選擇RAdam作為LookAhead中的優化器，就可以實踐Ranger了。參數下方是Ranger參數，除了Adam本來的參數外，也新增了RAdam和LookAhead的參數：gc則是後來更新的Ranger版本採用的新優化技術：Gradient Centralization，藉由將梯度均值中心化成0，來提升訓練穩定性。以上就是Ranger的介紹，如果對你有幫助，歡迎不吝嗇的多拍幾下手~或是哪裡有理解錯誤，也歡迎指正討教。,20/11/2020,0,7.0,0.0,1070.0,440.0,8.0,4.0,0.0,7.0,ko
3995,Machine Learning: Polynomial Regression with Python,Towards Data Science,Nhan Tran,365.0,5.0,631.0,"Abbreviations using in this post:In my previous post, we discussed about Linear Regression. Let’s take a look back. Linear Regression is applied for the data set that their values are linear as below example:And real life is not that simple, especially when you observe from many different companies in different industries. Salary of 1 YE teacher is different from 1 YE engineer; even 1 YE civil engineer is different from mechanical engineer; and if you compare 2 mechanical engineers from 2 different companies, their salary mostly different as well. So how can we predict the salary of a candidate?Today, we will use another data set to represent the Polynomial shape.To get an overview of the increment of salary, let’s visualize the data set into a chart:Let’s think about our candidate. He has 5 YE. What if we use the Linear Regression in this example?According to the picture above, the salary range of our candidate could be approximately from minus $10,000 to $300,000. Why? Look, the salary observations in this scenarios are not linear. They are in a curved shape! That’s why applying Linear Regression in this scenario is not giving you the right value. It’s time for Polynomial Regression.Because it’s much much more accurate!We are already know the salary of 5 YE is $110,000 and 6 YE is $150,000. It means the salary of 5.5 YE should be between them! And this is how the best value should be:Let’s compare the gaps between using Linear and Polynomial. Pay attention to the red circle:It’s too small to see? Zoom it out!It’s mostly 7.75 times more accurate than using Linear Regression!So how to calculate the salary for our 5.5 YE candidate? We can quick calculate by using the Mean value. Because 5.5 is the average of 5 and 6, so the salary could be calculated as:(150,000 + 110,000) / 2 = $130,000Note: if you don’t know what is Mean value, please read my previous post about Mean, Median, and Mode. Thanks.But it’s not the highest accuracy rate and too manual! Let’s apply the Machine Learning for more accuracy and flexible calculation. Time to start your Spyder IDE!In this sample, we have to use 4 libraries as numpy, pandas, matplotlib and sklearn. Now we have to import libraries and get the data set first:Code explanation:Let’s split our dataset to get training set and testing set (both X and y values per each set)Code explanation:We already have the train set and test set, now we have to build the Regression Model. Firstly, we will build a Linear Regression model and visualize it (it’s no need to include this step in your practice, we just do this for comparison between Linear and Polynomial only):After calling the viz_linear() function, you can see a plotting as per below:In another hand, we will build the Polynomial Regression model and visualize it to see the differences:After calling the viz_polynomial() function, you can see a plotting as per below:Last step, let’s predict the value of our candidate (with 5.5 YE) using both Linear Regression model and Polynomial Regression model:You can see, the predicted values using Linear Regression model and Polynomial Regression model are totally different!Let’s scroll up and check again what we got? According to our data set, our salary should be:$110,000 < the salary < $150,000But the predicted salary using Linear Regression lin_reg is $249,500. It’s unacceptable (but still in the range of -10,000 to 300,000 according to Linear Regression)! What’s about using Polynomial Regression? Our pol_reg value is $132,148.43750 which is very close to our Mean value which is $130,000.Bingo! It’s time to let our candidate know we will offer him a best salary in class with $132,148!Before ending my post, here is our complete source code and data set for you to practice on your playground:Happy learning everyday!",20/03/2019,0,11.0,9.0,684.0,377.0,10.0,4.0,0.0,9.0,en
3996,How I struggled to Convert MBR to GPT and Installed Linux?,Medium,thirumalaivasan,4.0,2.0,231.0,"I was supposed to Install Linux in my PC which is having a storage of 500GB with Windows in it ,So as a regular Linux installation procedure I unallocated 60GB and started to install the linux OS during the installation I found something fishy ,The Unallocated space was not showing up as a free space to install the Operating System ,I was like What the heck is this as usual Searched this issue in the Internet and discussed it with my techie friends all they told is “YOU HAVE TO CONVERT GPT TO MBR” and they suggested me some tools too like MINITOOL PARTITION WIZARD,ES PARTITION MASTER but everything ended up in Popping up for PREMIUM ACCESS to perform the particular action .It again started to irritate me a lot , I restarted my Computer several times again and again and entered BIOS hoping that different solution may work,But every move was failure .After 2 days I finally got a solution from one of my friend he suggested me a Cracked version of MINITOOL PARTITION WIZARD which finally helped me to convert MBR to GPT and installed, The Linux distro. Those two days were one of the greatest learning phase in my life I got new techie contacts and read many blogs regarding this when i successfully completed the installation I felt like I was in Heaven because I’m a noob.www.easeus.comhttps://softserialkey.com/easeus-partition-master-serial-key/",22/04/2019,0,7.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,en
3997,Multi-Class Text Classification Model Comparison and Selection,Towards Data Science,Susan Li,25000.0,7.0,993.0,"When working on a supervised machine learning problem with a given data set, we try different algorithms and techniques to search for models to produce general hypotheses, which then make the most accurate predictions possible about future instances. The same principles apply to text (or document) classification where there are many models can be used to train a text classifier. The answer to the question “What machine learning model should I use?” is always “It depends.” Even the most experienced data scientists can’t tell which algorithm will perform best before experimenting them.This is what we are going to do today: use everything that we have presented about text classification in the previous articles (and more) and comparing between the text classification models we trained in order to choose the most accurate one for our problem.We are using a relatively large data set of Stack Overflow questions and tags. The data is available in Google BigQuery, it is also publicly available at this Cloud Storage URL: https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv.10276752We have over 10 million words in the data.The classes are very well balanced.We want to have a look a few post and tag pairs.As you can see, the texts need to be cleaned up.The text cleaning techniques we have seen so far work very well in practice. Depending on the kind of texts you may encounter, it may be relevant to include more complex text cleaning steps. But keep in mind that the more steps we add, the longer the text cleaning will take.For this particular data set, our text cleaning step includes HTML decoding, remove stop words, change text to lower case, remove punctuation, remove bad characters, and so on.Now we can have a look a cleaned post:Way better!3421180After text cleaning and removing stop words, we have only over 3 million words to work with!After splitting the data set, the next steps includes feature engineering. We will convert our text documents to a matrix of token counts (CountVectorizer), then transform a count matrix to a normalized tf-idf representation (tf-idf transformer). After that, we train several classifiers from Scikit-Learn library.After we have our features, we can train a classifier to try to predict the tag of a post. We will start with a Naive Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for text is the multinomial variant.To make the vectorizer => transformer => classifier easier to work with, we will use Pipeline class in Scilkit-Learn that behaves like a compound classifier.We achieved 74% accuracy.Linear Support Vector Machine is widely regarded as one of the best text classification algorithms.We achieve a higher accuracy score of 79% which is 5% improvement over Naive Bayes.Logistic regression is a simple and easy to understand classification algorithm, and Logistic regression can be easily generalized to multiple classes.We achieve an accuracy score of 78% which is 4% higher than Naive Bayes and 1% lower than SVM.As you can see, following some very basic steps and using a simple linear model, we were able to reach as high as an 79% accuracy on this multi-class text classification data set.Using the same data set, we are going to try some advanced techniques such as word embedding and neural networks.Now, let’s try some complex features than just simply counting words.Word2vec, like doc2vec, belongs to the text preprocessing phase. Specifically, to the part that transforms a text into a row of numbers. Word2vec is a type of mapping that allows words with similar meaning to have similar vector representation.The idea behind Word2vec is rather simple: we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word representation.First we load a word2vec model. It has been pre-trained by Google on a 100 billion word Google News corpus.We may want to explore some vocabularies.BOW based approaches that includes averaging, summation, weighted addition. The common way is to average the two word vectors. Therefore, we will follow the most common way.We will tokenize the text and apply the tokenization to “post” column, and apply word vector averaging to tokenized text.Its time to see how logistic regression classifiers performs on these word-averaging document features.It was disappointing, worst we have seen so far.The same idea of word2vec can be extended to documents where instead of learning feature representations for words, we learn it for sentences or documents. To get a general idea of a word2vec, think of it as a mathematical average of the word vector representations of all the words in the document. Doc2Vec extends the idea of word2vec, however words can only capture so much, there are times when we need relationships between documents and not just words.The way to train doc2vec model for our Stack Overflow questions and tags data is very similar with when we train Multi-Class Text Classification with Doc2vec and Logistic Regression.First, we label the sentences. Gensim’s Doc2Vec implementation requires each document/paragraph to have a label associated with it. and we do this by using the TaggedDocument method. The format will be “TRAIN_i” or “TEST_i” where “i” is a dummy index of the post.According to Gensim doc2vec tutorial, its doc2vec class was trained on the entire data, and we will do the same. Let’s have a look what the tagged document looks like:When training the doc2vec, we will vary the following parameters:We initialize the model and train for 30 epochs.Next, we get vectors from trained doc2vec model.Finally, we get a logistic regression model trained by the doc2vec features.We achieve an accuracy score of 80% which is 1% higher than SVM.Finally, we are going to do a text classification with Keras which is a Python Deep Learning library.The following code were largely taken from a Google workshop. The process is like this:The accuracy is:So, which model is the best for this particular data set? I will leave it to you to decide.Jupyter notebook can be found on Github. Have a productive day!References:https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynbhttps://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynbhttps://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec",25/09/2018,11,5.0,2.0,1144.0,436.0,15.0,2.0,0.0,24.0,en
3998,What Is Pre-Training in NLP? Introducing 5 Key Technologies,"AI³ | Theory, Practice, Business",dan lee,306.0,8.0,36.0,"Welcome back to my blog for engineers who want to learn AI!Starting with this post, we’ll be launching into a new series of articles on pre-training in NLP. Today, we’ll begin by forming a big picture.",24/02/2020,0,0.0,1.0,728.0,600.0,1.0,0.0,0.0,0.0,en
3999,Does your model train too slow?,Towards Data Science,Pankaj Jainani,117.0,6.0,896.0,"You must definitely have encountered the problem when training a model is getting slower for a very Deep Neural Network. This phenomenon happens prominently during the backpropagation training (using Gradient Descent) of the DNNs, wherein, each parameter’s gradient error is propagated along its way to the lower layers of the network. Why? This usually happens because gradients usually get smaller and smaller. As a result, the lower layers weights never change and training never converges to the good solution.This post categorically discuss about the ways to alleviate the Vanishing Gradient (or the Exploding Gradient) problem while training the DNNsThere are various ways to overcome this challenge —Let’s look into all these in detail…We know that when partial derivative kicks-in during the backpropagation, using the activation functions such as sigmoid function will lead to the near zero-derivative when the inputs are very high (or too low). This causes the gradients to die (saturate) for lower-order layers due to chain-rule while calculating the derivative.In their paper published by Xavier Glorot and Yoshua Bengio, they suggest that if we want that gradient should neither saturate nor explode then we need to have a variance of the outputs of each layer to be equal to the variance of its inputs, and also gradients to have equal variance before and after flowing through a layer in the reverse direction. To achieve this condition — the number of neurons of the input layer should be equal to the number of neurons of the output layer (i.e. equal fan-in and fan-out of a layer). This solution is not practical.Therefore, there is a need for the initialization technique to compensate for this basic requirement — one such option is to have random weight-initialization based on fan-avg=(fan-in + fan-out)/2. This theory leads to the following practical options: —Normal distribution with mean 0 and variance: σ2 = 1/fan-avg. Or, a uniform distribution between −r and + r, with r = sqrt(3/fan-avg)This weight initialization is best suited while using activation functions: tanh, logistic, softmaxReplace fan-avg with fan-in the above values will become:Normal distribution with mean 0 and variance: σ2 = 1/fan-inOr, a uniform distribution between −r and + r, with r = sqrt(3/fan-in)LeCun initialization is equivalent to Glorot initialization when fan-in = fan-out This weight initialization is best suited while using the SELU activation function.It’s suitable for activation functions: Relu and its variantsKeras ImplementationBy default, Keras used Glorot Initialization with a normal distribution. We can explicitly define the kernel-initializer parameter to values, e.g, he_normal or he-uniform :The choice of the activation function is another reason for saturated gradients as described earlier, the problem is more prominent when using the activation functions such as sigmoid or tanh. The main reason is that the gradient tends to zero for higher-order weights, these functions are sometimes also called Saturated Activation Functions. The possible choices to overcome this issue is by using other class of activation functions which are non-saturated for their derivatives, few options are:Advantage — it does not saturate for x>0 (positive values)Disadvantage — the gradient completely ‘dies’ for x<0 . Making those neurons incapable during training.Advantage — It makes sure that x<0 the neuron doesn't ‘die’.RReLU is a variant of ReLU, wherein the α is chosen randomly from a given range during training and fixed to the average value during testing.The ‘α’ is learned during the training and it is one of the parameters which will be modified during the backpropagation.Disadvantage — it can overfit easily for training using a small dataset.As it is evident from the activation equation itself that for x<0 it alleviates the dead neurons problem and vanishing gradients problem, moreover, the derivative of a function is extremely smooth at x=0 thereby it converges fairly quickly.The SELU activation function often significantly outperforms other activation functions for neural nets composed exclusively of a stack of dense layers. This allows the network to self-normalize to converge faster while resolving the gradient issues. This is valid for a network with certain preconditions — the network architecture must be Sequential and Dense with kernel_initializer=""lecun_normal"".This technique is now ubiquitous while training the model to deal with the problem of Vanishing (Exploding) Gradient. This operation is applied just after (sometimes before) the activation functions. Essentially, this operation: — i. Zero-center, ii. Normalize, iii. Scales & iv. Shift each input applied to it. In a gist, this operation will help find the optimal mean and scale for the given set of input features.The Keras implementation for BatchNormalization() layer learn 4-vectors: 1. γ: the scale vector2. β: the offset (shift) vector3. μ: the mean vector4. σ: the standard deviation vectorBatchNormalization vectors are estimated/learned during the training and applied as-is during testing.This technique is so successful that during training for a large dataset we could even use the saturated activation functions such as sigmoid or tanh. Somewhere, it also overcomes the training challenges of weight initialization. Lastly, it is a good regularization option, similar to l1 or l2 regularizers or as good as other Dropout methods.In this post, I was roughly able to scratch the surface to help you introduce the concepts to help you alleviate the Vanishing and Exploding gradients problem which is most prominent while training Neural Nets. Have you ever faced these issues? How did you solve it? Do let me know various other techniques which you have experienced practically to solve the problem.Connect with me on LinkedIn to discuss further",11/03/2021,7,27.0,39.0,584.0,244.0,6.0,10.0,0.0,9.0,en
4000,Graph Neural Networks through the lens of Differential Geometry and Algebraic Topology,Towards Data Science,Michael Bronstein,5500.0,11.0,2268.0,"“Symmetry, as wide or as narrow as you may define its meaning, is one idea by which man through the ages has tried to comprehend and create order, beauty, and perfection.”This somewhat poetic description by Hermann Weyl [1] underlines the cornerstone role of symmetry in science. Felix Klein’s 1872 “Erlangen Programme” [2] characterised geometries through symmetry groups. Not only was this a breakthrough in mathematics, unifying the “zoo of geometries,” but also led to the development of modern physical theories that can be entirely derived from the first principles of symmetry [3]. Similar principles have emerged in machine learning under the umbrella of Geometric Deep Learning, a general blueprint for deriving the majority of popular neural network architectures through group invariance and equivariance [4].The Geometric Deep Learning Blueprint can be applied to different domains, such as grids, meshes, or graphs [5]. Yet, while the former two have clear continuous analogy objects (grids can be considered as a discretisation of Euclidean or more generally homogeneous spaces such as the sphere, and meshes are a common discretisation of 2-dimensional manifolds), there is no immediate continuous analogy for graphs [6]. This inequity is somewhat disturbing and motivates us to take a closer look at continuous models for learning on graphs.Graph Neural Diffusion. Graph Neural Networks (GNNs) learn by performing some form of message passing on the graph, whereby features are passed from node to node across the edges. Such a mechanism is related to diffusion processes on graphs that can be expressed in the form of a partial differential equation (PDE) called “diffusion equation”. In a recent paper [7], we showed that the discretisation of such PDEs with nonlinear learnable diffusivity functions (referred to as “Graph Neural Diffusion” or very modestly, GRAND) generalises a broad class of GNN architectures such as Graph Attention Networks (GAT) [8].The PDE mindset offers multiple advantages, such as the possibility to exploit efficient numerical solvers (e.g. implicit, multistep, adaptive, and multigrid schemes) with guaranteed stability and convergence properties. Some of these solvers do not have an immediate analogy among the zoo of popular GNN architectures, potentially promising new interesting Graph Neural Network designs. Such architectures might be at least slightly more interepretable than the typical ones, owing to the fact that the diffusion PDE we consider can be seen as the gradient flow [9] of some associated energy.At the same time, while the GRAND model offers continuous time in the place of layers in traditional GNNs, the spatial part of the equation is still discrete and relies on the input graph. Importantly, in this diffusion model, the domain (graph) is fixed, and some property defined on it (features) evolves.A different concept commonly used in differential geometry is that of geometric flows, evolving the properties of the domain itself [10]. This idea was adopted by my PhD advisor Ron Kimmel and his co-authors in the 1990s in the field of image processing [11]. They modelled images as manifolds embedded in a joint positional and colour space and evolved them by a PDE minimising the harmonic energy of the embedding [12]. Such a PDE, named Beltrami flow, has the form of isotropic non-euclidean diffusion and produces edge-preserving image denoising.We adapted this paradigm to graphs in the “Beltrami Neural Diffusion” (BLEND) framework [13]. The nodes of the graph are now characterised by positional and feature coordinates, both of which are evolved and both of which determine the diffusivity properties. In this mindset, the graph itself passes into an auxiliary role: it can be generated from the positional coordinates (e.g. as a k-nearest neighbour graph) and rewired throughout the evolution. The following Figure illustrates this simultaneous evolution process:Significant attention has been devoted in recent works to the expressive power of Graph Neural Networks. Message-passing GNNs are equivalent to the Weisfeiler-Lehman graph isomorphism test [14–16], a classical method attempting to determine whether two graphs are structurally equivalent (“isomorphic”) by means of iterative colour refinement. This test is a necessary but insufficient condition: in fact, some non-isomorphic graphs might be deemed equivalent by Weisfeler-Lehman. The following Figure illustrates what message passing GNNs “see”: the two highlighted nodes appear indistinguishable, though the graphs clearly have a different structure:Positional Encoding. A common remedy to this problem is to “colour” the nodes by assigning to them some additional features representing the role or “position” of the node in the graph. Popularised in Transformers [17] (which are a special case of attentional GNNs operating on a complete graph [4]), positional encoding methods have become a common way of increasing the expressive power of Graph Neural Networks.Perhaps the most straightforward approach is to endow each node with a random feature [18]; however, while being more expressive, such an approach would provide poor generalisation (since it is impossible to reproduce random features across two graphs). The eigenvectors of the graph Laplacian operator [19] provide a neighbourhood-preserving embedding of the graph and have been successfully used as positional encoding. Finally, we showed in our paper with Giorgos Bouritsas and Fabrizio Frasca [20] that graph substructure counting can be used as a form of positional or “structural” encoding that can be made provably more powerful than the basic Weisfeiler-Lehman test.However, with a variety of choices for positional encoding, there is no clear recipe as to how to choose one, and no clear answer to the question of which method works better in which cases. I believe that geometric flows such as BLEND can be interpreted in the light of this question: by evolving the positional coordinates of the graph through non-euclidean diffusion, the positional encoding is adapted for the downstream task. The answer is, therefore, “it depends”: the best positional encoding is a function of the data and task at hand.Higher-order Message Passing. An alternative take on expressivity is to stop thinking about graphs in terms of nodes and edges. Graphs are examples of objects called cell complexes, one of the main objects of study in the field of algebraic topology. In this terminology, nodes are 0-cells and edges are 1-cells. One does not have to stop there: we can construct 2-cells (faces) like shown in the following Figure, which make the two graphs from our previous example perfectly distinguishable:In two recent papers coauthored with Cristian Bodnar and Fabrizio Frasca [21–22], we show that it is possible to construct a “lifting transformation” that augments the graph with such higher-order cells, on which one can perform a more complex form of hierarchical message passing. This scheme can be made provably more expressive than the Weisfeiler-Lehman test and has shown promising results in computational chemistry where many molecules exhibit structures better modeled as cell complexes rather than graphs.Another common plight of GNNs is the “oversquashing” phenomenon, or the failure of message passing to propagate information efficiently due to certain structural characteristics of the input graph (“bottleneck”) [23]. Oversquashing typically occurs in graphs with exponential volume growth such as small-world networks [24] and in problems dependent on long-range information. Put differently, the input graph on which GNNs operate is not always necessarily friendly for message passing.Over-squashing, Bottlenecks, and Graph Rewiring. Empirically, it was observed that decoupling the input graph from the computational graph and allowing passing messages on a different graph helps alleviate the problem; such techniques are generally referred to as “graph rewiring”.It is fair to say that many popular GNN architectures implement some form of graph rewiring, which can take the form of neighbourhood sampling (originally proposed in GraphSAGE to cope with scalability [25]) or multi-hop filters [26]. Topological message passing discussed above can also be seen as a form of rewiring, whereby information flow between distant nodes can be “shortcut” through higher-order cells. Alon and Yahav [23] showed that even as simplistic an approach as using a fully connected graph may help improve over-squashing in graph ML problems. Klicpera and coauthors enthusiastically proclaimed that “diffusion improves graph learning”, proposing a universal preprocessing step for GNNs (named “DIGL”) consisting in denoising the connectivity of the graph by means of a diffusion process [27]. Overall, despite the significant empirical study, the over-squashing phenomenon has been elusive and insufficiently understood.In a recent paper [28], we show that bottlenecks resulting in over-squashing can be attributed to the local geometric properties of the graph. Specifically, by defining a graph analogy of Ricci curvature, we can show that negatively-curved edges are the culprits. This interpretation leads to a graph rewiring procedure akin to “backward Ricci flow” that surgically removes problematic edges and produces a graph that is more message passing-friendly while being structurally similar to the input one.These examples show that differential geometry and algebraic topology bring a new perspective to important and challenging problems in graph machine learning. In the following posts in this series, I will show in further detail how tools from these fields can be used in order to address the aforementioned problems of Graph Neural Networks. Part II will discuss how algebraic topology can improve the expressive power of GNNs. Part III will deal with geometric diffusion PDEs. Part IV will show how the over-squashing phenomena can be related to graph curvature, and offer a geometric approach to graph rewiring inspired by the Ricci flow.[1] H. Weyl, Symmetry (1952), Princeton University Press.[2]F. Klein, Vergleichende Betrachtungen über neuere geometrische Forschungen (1872).[3] J. Schwichtenberg, Physics from symmetry (2018), Springer.[4] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (2021); see an accompanying post and the project website.[5] In the above GDL proto-book, we call these the “5G” of Geometric Deep Learning.[6] Geometric graphs naturally arise as discrete models of objects living in a continuous space. A prominent example is molecules, modeled as graphs where every node represents an atom with 3D spatial coordinates. On the other hand, it is possible to embed general graphs into a continuous space, thus (approximately) realising their connectivity using the metric structure of some space.[7] B. Chamberlain, J. Rowbottom et al., GRAND: Graph Neural Diffusion (2021) ICML.[8] P. Veličković et al., Graph Attention Networks (2018) ICLR.[9] A gradient flow can be seen as a continuous analogy of gradient descent in variational problems. It arises from the optimality conditions (known in the calculus of variations as Euler-Lagrange equations) of a functional.[10] Geometric flows are gradient flows of functionals defined on manifolds. Perhaps the most famous of them is the Ricci flow, used by Grigori Perelman in the proof of the century-old Poincaré conjecture. Ricci flow evolves the Riemannian metric of the manifold and is structurally similar to the diffusion equation (hence often, with gross simplification, presented as “diffusion of the metric”).[11] N. Sochen et al., A general framework for low-level vision (1998) IEEE Trans. Image Processing 7(3):310–318 used a geometric flow minimising the embedding energy of a manifold as a model for image denoising. The resulting PDE is a linear non-euclidean diffusion equation ẋ = Δx (here Δ is the Laplace-Beltrami operator of the image represented as an embedded manifold), as opposed to the nonlinear diffusion ẋ = div(a(x)∇x) used earlier by P. Perona and J. Malik, Scale-space and edge detection using anisotropic diffusion (1990) PAMI 12(7):629–639.[12] Beltrami flow minimises a functional known in string theory as the Polyakov action. In the Euclidean case, it reduces to the classical Dirichlet energy.[13] B. P. Chamberlain et al., Beltrami Flow and Neural Diffusion on Graphs (2021) NeurIPS.[14] B. Weisfeiler, A. Lehman, The reduction of a graph to canonical form and the algebra which appears therein (1968) Nauchno-Technicheskaya Informatsia 2(9):12–16.[15] K. Xu et al., How powerful are graph neural networks? (2019) ICLR.[16] C. Morris et al., Weisfeiler and Leman go neural: Higher-order graph neural networks (2019) AAAI.[17] A. Vaswani et al., Attention is all you need (2017) NIPS.[18] R. Sato, A survey on the expressive power of graph neural networks (2020). arXiv: 2003.04078. Uses random features for positional encoding.[19] V. P. Dwivedi et al. Benchmarking graph neural networks (2020). arXiv: 2003.00982. Uses Laplacian eigenvectors as positional encoding, though the idea of spectral graph embedding is way older and has been widely used in nonlinear dimensionality reduction such as the classical work of M. Belkin and P. Niyogi, Laplacian eigenmaps and spectral techniques for embedding and clustering (2001), NIPS.[20] G. Bouritsas et al., Improving graph neural network expressivity via subgraph isomorphism counting (2020). arXiv:2006.09252. Uses graph substructure counts as positional encoding; see an accompanying post.[21] C. Bodnar, F. Frasca, et al., Weisfeiler and Lehman go topological: Message Passing Simplicial Networks (2021) ICML.[22] C. Bodnar, F. Frasca, et al., Weisfeiler and Lehman go cellular: CW Networks (2021) NeurIPS.[23] U. Alon and E. Yahav, On the bottleneck of graph neural networks and its practical implications (2020). arXiv:2006.05205[24] In such graphs, the size of neighbours up to k-hops away grows very fast with k, resulting in “too many neighbours” that have to send their updates to a node.[25] W. Hamilton et al., Inductive representation learning on large graphs (2017) NIPS.[26] F. Frasca et al., SIGN: Scalable Inception Graph Neural Networks (2020). ICML workshop on Graph Representation Learning and Beyond. Uses multihop filters; see an accompanying post.[27] J. Klicpera et al., Diffusion improves graph learning (2019) NeurIPS. Rewires the graph by computing Personalised Page Rank (PPR) node embedding and then computing a k-NN graph in the embedding space.[28] J. Topping, F. Di Giovanni et al., Understanding over-squashing and bottlenecks on graphs via curvature (2021) arXiv:2111.14522. See an accompanying blog post.I am grateful to Cristian Bodnar, Ben Chamberlain, Fabrizio Frasca, Francesco Di Giovanni, and Nils Hammerla for proofreading this post. For additional articles about deep learning on graphs, see my other posts in Towards Data Science, subscribe to my posts, get Medium membership, or follow me on Twitter.",18/11/2021,0,4.0,37.0,1425.0,594.0,8.0,0.0,0.0,49.0,en
4001,Anomaly detection in brightfield microscopy images,Medium,Nurlan Kerimov,11.0,13.0,2319.0,"Disclaimer: This project was developed by Kaspar Hollo and Nurlan Kerimov for the Neural Networks course at the University of Tartu. The data and the code used in this project are not public and, in this blog-post only a few examples from the dataset will be shown. The data was provided by PerkinElmer.Nowadays, microscopy images are often used for doing medical diagnosis. For example, in this paper, a deep learning model was developed to count mitotic cells to help diagnose breast cancer. There is a problem though — the captured microscopy images may contain some so-called anomalies which can be considered as noise. It is found that the cell count and position predictions (cell segmentation) are performing badly in areas with anomalies. In our project, we tried to predict the pixels belonging to anomalies (instance segmentation) in the microscopy images. If the anomaly segmentation is successful, problematic areas in the images can be avoided and better performance in image processing can be achieved.For more information about different tasks of computer vision (classification, object detection, semantic segmentation, instance segmentation) feel free to take a look at the following link:The brightfield images in our datasets are 1080x1080 pixel grayscale images and the dataset itself was split into 3 parts:We carefully selected the images with potential anomalies and the balance of anomaly/normal images in our dataset turned out to be:The dataset contains images of nuclei from seven different cell lines, namely, human lung carcinoma (A549), canine kidney epithelial cells (MDCK), human cervical adenocarcinoma (HeLa), human breast adenocarcinoma (MCF7), mouse fibroblasts (NIH3T3), human hepatocellular carcinoma (HepG2) and human fibrosarcoma (HT1080) (for more info about cell types see this article). The images in our dataset also contain several types of anomalies (dark spots, scratches, bacterial colonies, hair, dust, etc.). As can be seen from the example images, each image has at least one anomaly (more often than not there were multiple anomalies on each contaminated image).Since we needed to apply instance segmentation, we considered multiple existing models for transfer learning (a.k.a. feature extraction). The finalists turned out to be YOLO and Mask R-CNN. According to the literature review we made, YOLO was considered faster since it only sees each image once. However, Mask R-CNN was considered to have better performance and to be more flexible to fit onto your own model. For these reasons, we decided to use Mask R-CNN in our project.As the data was unannotated, we had to annotate the anomalies manually in all of the contaminated images. We hear what you’re saying: ”You are not experts in the field, you should not do that!”. Yes, we totally agree. But, you know:We decided to annotate all of the anomalies as one big base class — “anomaly” (in total we had 2 classes — background and anomaly). As expected, the annotation of the anomalies turned out to be a pretty gruelling and time-demanding task, although we only had one class to assign the annotations to. Fortunately, we found a tool which somewhat alleviated the pain through its ease of use — VGG Image Annotator (VIA). Even though the tool was easy and comfortable to use, it still had some little bugs in it — from time to time the exported JSON file didn’t include some of the annotations or the format of the attribute (in our case “anomaly”) was somewhat wrong. For these reasons, the JSON file had to be manually checked before it was being used.The share amount of images (and remember, each image had at least one anomaly) and the somewhat buggy tool weren’t the only problems which accompanied the annotation task. In particular, there were 3 more problems:There just had to be a minimum size threshold for the anomalies, otherwise, we would still be annotating the images as there were loads of small scratches on many of the images. But this, in turn, raised a question — how big should an anomaly be to be classified as an anomaly? As a rule of thumb, we chose to classify anomalies as such if they were twice as big as their surrounding cells. But this was also not an ideal method because the images were not captured in the same zoom-in levels and therefore the cell sizes alternated.In some cases, the borders of the anomalies caused a little bit of a headache, because they were not clear-cut or even blurry. So these anomalies were mainly annotated based on our subjective gut feeling.As normal human beings, while annotating images, we are only able to spot anomalies in the context of a specific image. What we mean by this is that there were multiple cases where we would classify an object as an anomaly in one specific image context but not in another. Simply put, we were able to detect an anomaly in an image in comparison to the rest of the same image.Initially we thought about using the university’s HPC for training the network. However, it took some effort to set up the environment and we decided to use Google Colab’s free 12GB GPU environment. Since we had a relatively small dataset and Colab did not need much configuration effort it was a better choice in our opinion. Having a premium GDrive account (which offered 100GBs of storage) came in handy in this situation. Taking into consideration that the weights of one epoch of training is ~250MBs, without a premium account it would not be possible to train for multiple epochs.Mask R-CNN is a model used to solve the instance segmentation task. It is an extension of the FasterRCNN model, which itself is the evolved version of FastRCNN. You can find in-depth information about Mask R-CNN and the other models in the following articles:engineering.matterport.comtowardsdatascience.commachinelearningmastery.comWe did not see the point of re-explaining the model, because the creators of the model have probably done it better than we ever could :). However, we will briefly explain what kind of problems we encountered while implementing the Mask R-CNN model.The widely available version of Mask R-CNN is developed to work with regular 3-channel (RGB) images. Although they have added some instructions on how to make the model work with grayscale images, implementing all of the given instructions was not even nearly enough to run the model with grayscale images. After multiple changes in the source code and dozens of WTHs we managed to run the training with our grayscale images. Ultimately, we had trained weights and we were very proud of this:Then it turned out that our problems were far from solved — we found out that we should do more changes in the source code to be able to predict with the trained model. Because, you know, it was implemented (hard-coded) to work only with RGB images (and no instructions were provided on how to make this part work :) ).When we were trying to come up with a solution for this issue, we were like:“Wait, actually converting our images to RGB will probably not affect the performance too much. Yes, we will probably lose some time in training, but this change is only related to the very first layer in the multi-dozen layered model. So, we should be ok!”.So, we started to train the Mask R-CNN model with our RGB converted grayscale images. We used cell-nuclei weights with ResNet101 as the backbone and re-trained only the heads (the closest layers to the output layer).Another problem we encountered with Mask R-CNN was that it does not work with the latest versions of Tensorflow. It uses Tensorflow version 1.13.1. Currently, if you install tensorflow e.g. with pip it can automatically use your GPU if you have an eligible one. So, you don’t need to install tensorflow-gpu explicitly. But, guess what: IT WAS NOT LIKE THAT IN VERSION 1.13.x. So we thought that we are training on GPU but apparently we were wrong:One epoch was taking like 4 hours. After figuring out our dumb mistake each epoch was taking less than 3 minutes.To sum it up, we wouldn’t suggest making the model work with grayscale images specifically, let Mask R-CNN convert the images to RGB and make sure you are using GPU while training your model.In order to fight overfitting and increase our training data we also used augmentation. Since we had grayscale images we used only very basic augmentation techniques, such as rotation, mirroring, adding gaussian noise and multiplying (changing the magnitude of the colours to brighter or darker tones)The Mask R-CNN model has dozens of parameters — it takes quite a long time to understand the functionality and/or meaning of all of these parameters. Although we kept most of the parameters as their default values, we were highly influenced by the cell-nuclei detecting model’s parameters.In order to give some intuition how the predictions of our model look like we added some masked images below. In the left column we put only the original images. In the right column we put the ground truths (highlighted in green — the annotation we manually made) and predictions of the model (highlighted in red). The model also predicts the bounding boxes of the anomalies, but we are not specifically interested in them in this project.As you can see from the predicted images above, there are multiple overlapping predictions/masks in the areas of the ground truths (annotated areas). Mask R-CNN basically scans the whole image in these arbitrarily set window sizes and looks for anomalies in each window.As we set multiple values for the window sizes, there are also multiple smaller and overlapping predictions instead of one big prediction. In addition, we think that the bigger predictions were often excluded because we set a minimum threshold of 0.7 for the predictions and these bigger predictions just didn’t reach the threshold.In order to measure the metrics we first merged (took the union) the predictions if they overlap. Then we calculated the Intersection over Union (IoU) for each of the predictions.So, if the IoU is 0 then it means the model predicted an anomaly where we did not annotate (a.k.a. False positive). If IoU is equal or more than 0.5 (IoU >= 0.5) we encountered it as a correct prediction (a.k.a. True positive), if not (IoU < 0.5) it is a false positive. After having all the pixelwise metrics we counted the metrics per merged prediction mask. In this case we can not have any true negative results, but we have false negatives when the annotated region has not been predicted as an anomaly. Numeric results are as following:One thing we wanted to address is that our model had some rather poor results in one specific image type — HepG2. We think that the background structure of HepG2 and the patterns of some of the anomalies were too similar and that is why our model made some pretty bad predictions in this specific image type.In order to see the effect of this problematic image type we excluded them to see the difference in metrics.As it can be seen from the results above, this problematic image type causes small differences in validation result metrics (~ 4%).In test datasets the difference was a little higher (~ 7%), due to more of this kind of images.We think that the main reason for the low accuracy is the preciseness of our annotations. Since we are not experts in the field and were doing it for the first time, all the decisions which were made in the annotation process were totally subjective. We decided not to annotate small anomalies (anomalies with the size of one cell or less in that particular image), however our model did a good job predicting smaller anomalies which we did not annotate. Secondly, we overlooked some (relatively big) annotations which were detected by the model. When we look into what makes the numeric performance of our model this low, we can see that most of the damage is done by false positive predictions (see no_overlap_count — simply, the anomalies which we did not care to annotate but had been predicted by the model and some really wrong predictions):Although we got rather poor metric results, we think that our model generalized well and was even able to do predictions which were definitely anomalies but weren’t annotated (e.g. considered too small to be annotated or simply overlooked by the human eye, etc.). We are happy with the results and we think if the images were annotated by specialists then the model would get better metric results.So, what could be done to improve the results even further? Luckily, we have some ideas that might help to do just that.The first idea would be to add annotation classes. As we are dealing with grayscale images it can be beneficial to classify anomalies based on their patterns (e.g. black spots, bacteria, etc.). This way it would be possible to identify, which anomalies are predicted better and apply more data/augmentation to the more problematic classes.The second idea would be to train a model with each image type separately. As we described earlier, we suspect that the background structure of some image types and the patterns of some of the anomalies were very similar and that caused some confusion in the model. Therefore separating the different image types would probably eliminate this problem.The last idea would be to create an ensemble model (Mask R-CNN and U-Net) to get better results. In the following paper, it was claimed that this kind of ensemble model achieved far better results than each of these models separately. It would be interesting to see what results would the ensemble model achieve with our dataset.Despite the rather poor metric results, we think that our model generalized well and was even able to find anomalies which were definitely anomalies but weren’t annotated.We would like to thank Dmytro Fishman and Mohammed Ali for their excellent supervision. Additionally we would like to thank the University of Tartu and the teaching staff of Neural Networks course for their great effort.https://arxiv.org/abs/1901.10170https://github.com/matterport/Mask_RCNN/tree/master/samples/nucleushttps://www.nature.com/articles/s41374-019-0275-0https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46http://www.robots.ox.ac.uk/~vgg/software/via/https://www.biorxiv.org/content/10.1101/764894v1.full.pdfhttps://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141ahttps://machinelearningmastery.com/object-recognition-with-deep-learning/https://github.com/matterport/Mask_RCNN/wiki#training-with-rgb-d-or-grayscale-images",14/06/2020,6,11.0,1.0,950.0,895.0,18.0,5.0,0.0,31.0,en
4002,Beam Search Decoding in CTC-trained Neural Networks,Towards Data Science,Harald Scheidl,1000.0,9.0,1704.0,"Neural networks (NN) consisting of convolutional NN layers and recurrent NN layers combined with a final connectionist temporal classification (CTC) layer are a good choice for (handwritten) text recognition.The output of the NN is a matrix containing character-probabilities for each time-step (horizontal position), an example is shown in Fig 1. This matrix must be decoded to get the final text. One algorithm to achieve this is beam search decoding which can easily integrate a character-level language model.We will start our discussion with a recap of CTC and best path decoding. Then we will discuss the building blocks (basic algorithm, CTC scoring, language model) of the CTC beam search decoding algorithm. Finally, I will point you to a Python implementation which you can use to do your own tests and experiments.Reading the article “An Intuitive Explanation of Connectionist Temporal Classification” helps you to understand the following discussion. Here I will give a short recap.CTC allows training text recognition systems with pairs of images and ground truth texts. Text is encoded in the NN output matrix by paths, which contain one character per time-step, e.g. ‘ab’ or ‘aa’ are possible paths in Fig. 1. I will show text in double quotes “text” and paths in single quotes ‘path’.A path encodes text in the following way: each character of a text can be repeated an arbitrary number of times. Further, an arbitrary number of CTC blanks (non-character, not to be confused with a white-space character, denoted as “-” in this article) can be inserted between characters. In the case of repeated characters (e.g. “pizza”), at least one blank must be placed in between these repeated characters on the path (e.g. ‘piz-za’).Here are examples of texts with corresponding paths:As you see, there may be more than one path corresponding to a text. When we are interested in the probability of a text, we have to sum over the probabilities of all corresponding paths. The probability of a single path is the product of the character-probabilities on this path, e.g. for the path ‘aa’ in Fig. 1 it is 0.2·0.4=0.08.Best path decoding is the simplest method to decode the output matrix:Let’s look at an example: the matrix is shown in Fig. 2. The highest-scoring character is blank for both time-steps t0 and t1. So, the best path is ‘--’. We then undo the encoding and get the text “”. Further, we can calculate the probability of the path by multiplying the character-probabilities, which is 0.8·0.6=0.48 in this example.Best path decoding is fast, we only have to find the character with the highest score for each time-step. If we have C characters and T time-steps, the algorithm has a running time of O(T·C).Best path decoding is both fast and simple, which are of course nice properties. But it may fail in certain situations like the one shown in Fig 2. In Fig. 3 all paths corresponding to the text “a” are shown: ‘aa’, ‘a-’ and ‘-a’. The probability of the text “a” is the sum over all probabilities of these mentioned paths: 0.2·0.4+0.2·0.6+0.8·0.4=0.52. So, “a” is more probable than “” (0.52>0.48). We need a better algorithm than best path decoding which can handle such situations.Beam search decoding iteratively creates text candidates (beams) and scores them. Pseudo-code for a basic version is shows in Fig 4.: the list of beams is initialized with an empty beam (line 1) and a corresponding score (2). Then, the algorithm iterates over all time-steps of the NN output matrix (3–15). At each time-step, only the best scoring beams from the previous time-step are kept (4). The beam width (BW) specifies the number of beams to keep. For each of these beams, the score at the current time-step is calculated (8). Further, each beam is extended by all possible characters from the alphabet (10) and again, a score is calculated (11). After the last time-step, the best beam is returned as a result (16).Let’s visualize how the algorithm decodes our example NN output with BW 2 and alphabet {“a”, “b”}. Fig. 5 shows both the NN output to be decoded and the tree of beams. The algorithm starts with an empty beam “”, which corresponds to the root node of the tree. The beam is then both copied and extended by all possible characters from the alphabet. This gives us the beams “a”, “b” and “”. Later, we will take a closer look at how to calculate the beam-scores. For now, we use our intuition and see that there is only one path corresponding to each beam: ‘a’ with probability 0.2, ‘b’ with 0 and ‘-’ with 0.8.In the next iteration, we just keep the 2 best beams (according to BW) from the previous time-step, i.e. we throw away the beam “b”. Then, we again both copy and extend the surviving beams and get “aa”, “ab”, “a”, “a”, “b”, “”. If two beams are equal as it is the case for “a”, we simply merge them: we add up the scores and only keep one of the beams. We again use our intuition to compute the scores. Each beam containing a “b” has a probability of 0. “aa” also has 0 probability because to encode a text with repeated characters, we have to put a blank in between (e.g. ‘a-a’), which is not possible for a path of length 2. Finally, what remains are the beams “a” and “”. We already calculated the probabilities for them: 0.52 and 0.48.We finished the last iteration and the final step of the algorithm is to return the beam with the highest score, which is “a” in this example.We didn’t talk about how to score the beams yet. We split the beam-score into the score of paths ending with a blank (e.g. ‘aa-’) and paths ending with a non-blank (e.g. ‘aaa’). We denote the probability of all paths ending with a blank and corresponding to a beam b at time-step t by Pb(b, t) and by Pnb(b, t) for the non-blank case. The probability Ptot(b, t) of a beam b at time-step t is then simply the sum of Pb and Pnb, i.e. Ptot(b, t)=Pb(b, t)+Pnb(b, t).Fig. 6 shows what happens when we extend a path. There are three main cases: extend by blank, extend by repeating last character and extend by some other character. When we collapse the extended paths, we either get the unchanged (copied) beam (“a” → “a”), or we get an extended beam (“a” → “aa” or “ab”). We can use this information the other way round too: if we extend a beam, we know which paths we have to consider to calculate the score.Let’s look at how to iteratively compute Pb and Pnb. Note that we are always adding instead of assigning the computed values (+= instead of =), this implicitly implements the beam merging discussed earlier. All Pb and Pnb values are initially set to 0.To copy a beam, we can extend corresponding paths by a blank and get paths ending with a blank: Pb(b, t)+=Ptot(b, t-1)·mat(blank, t).Further, we may extend paths ending with a non-blank by the last character (if the beam is non-empty): Pnb(b, t)+=Pnb(b, t-1)·mat(b[-1], t), where -1 indexes the last character in the beam.There are two cases. Either we extend the beam by a character c different from the last character, then there is no need for separating blanks in the paths: Pnb(b+c, t)+=Ptot(b, t-1)·mat(c, t).Or the last character b[-1] is repeated, then we must ensure that the paths end with a blank: Pnb(b+c, t)+=Pb(b, t-1)·mat(c, t).We don’t have to care about Pb(b+c, t) because we added a non-blank character.A character-level language model (LM) scores a sequence of characters. We restrict our LM to score single characters (unigram LM) and pairs of characters (bigram LM). We denote a unigram probability of the character c as P(c) and the bigram probability of characters c1, c2 as P(c2|c1). The score of a text “hello” is the probability of seeing a single “h”, and the probability of seeing a pair “h” and “e” next to each other, and a pair “e” and “l” next to each other, …The probability of a character sequence c1, c2, c3, … is: P(c1, c2, c3, …)=P(c1)·P(c2|c1)·P(c3|c2)·…Training such a LM from a large text is easy: we simply count how often a character occurs and divide by the total number of characters to get the unigram probability. And we count how often a pair of characters occurs and normalize it to get the bigram probability.The CTC beam search algorithm is shown in Fig. 7. It is similar to the already shown basic version, but includes code to score the beams: copied beams (lines 7–10) and extended beams are scored (15–19). Further, the LM is applied when extending a beam b by a character c (line 14). In case of a single-character beam, we apply the unigram score P(c), while for longer beams, we apply the bigram score P(b[-1], c). The LM score for a beam b is put into the variable Ptxt(b). When the algorithm looks for the best scoring beams, it sorts them according to Ptot·Ptxt (line 4) and then takes the BW best ones.The running time can be derived from the pseudo code: the outer-most loop has T iterations. In each iteration, the N beams are sorted, which accounts for N·log(N). The BW best beams are selected and each of them is extended by C characters. Therefore we have N=BW·C beams and the overall running time is O(T·BW·C·log(BW·C)).A Python implementation of beam search decoding (and other decoding algorithms) can be found in the CTCDecoder repository: the relevant code is located in src/BeamSearch.py and src/LanguageModel.py. TensorFlow provides the ctc_beam_search_decoder operation, however, it does not include a LM.Decoding a NN on the IAM dataset gives a character error rate of 5.60% with best path decoding and 5.35% with beam search decoding. The running time increases from 12ms to 56ms per sample.Here is a sample from the IAM dataset (see Fig. 8) to get a better feeling for how beam search improves the results. Decoding is done with best path decoding and beam search with and without LM.CTC beam search decoding is a simple and fast algorithm and outperforms best path decoding. A character-level LM can easily be integrated.",10/07/2018,1,2.0,0.0,644.0,451.0,9.0,3.0,0.0,8.0,en
4003,Data Scientists Will be Extinct in 10 Years,Towards Data Science,Mikhail Mew,1000.0,4.0,840.0,"As advances in AI continue to progress in leaps and bounds, accessibility to data science at a base level has become increasingly democratized. Traditional entry barriers to the field such as a lack of data and computing power have been swept aside with a continuous supply of new data startups popping up(some offering access for as little as a cup of coffee a day) and all powerful cloud computing removing the need for expensive onsite hardware. Rounding out the trinity of prerequisites, is the skill and know-how to implement, which has arguably become the most ubiquitous aspect of data science. One does not need to look far to find online tutorials touting taglines like “implement X model in seconds” , “apply Z method to your data in just a few lines of code”. In a digital world, instant gratification has become the name of the game. While improved accessibility is not detrimental on face value, beneath the dazzling array of software libraries and shiny new models, the true purpose of data science has become obscured and at times even forgotten. For it is not to run complex models for the sake of doing so, or to optimize an arbitrary performance metric, but to use as a tool to solve real world problems.A simple but relatable example is the Iris data set. How many have used it to demonstrate an algorithm without sparing a thought for what a sepal is let alone why we measure its length? While these may seem like trivial considerations for the budding practitioner who might be more interested in adding a new model to their repertoire, it was less than trivial for Edgar Anderson, a botanist, who cataloged the attributes in question to understand variations in Iris flowers. Despite this being a contrived example it demonstrates a simple point; the mainstream has become more focused on “doing” data science rather than “applying” data science. However, this misalignment is not the cause for the decline of the data scientist but a symptom. To understand the origin of the problem we must step back and take a bird’s eye view.Data science has the curious distinction of being one of the few fields of study that leaves the practitioner without a domain. Pharmacy students become pharmacists, law students become lawyers, accounting students become accountants. Data science students must therefore become data scientists? But data scientists of what? The broad application of data science proves to be a double edged sword. On one side, it is a powerful toolbox that can be applied to any industry where data is generated and captured. On the other, the general applicability of these tools means that rarely will the user have true domain knowledge of said industries before the fact. Nevertheless, the problem was insignificant during the rise of data science as employers rushed to harness this nascent technology without fully understanding what it was and how it could be fully integrated into their company.However, nearly a decade later, both businesses and the environment they operate in have evolved. They now strive for data science maturity with large entrenched teams benchmarked by established industry standards. The pressing hiring demand has shifted to problem solvers and critical thinkers who understand the business, the respective industry as well as its stakeholders. No longer will the ability navigate a couple of software packages or regurgitate a few lines of code suffice, nor will a data science practitioner be defined by the ability to code. This is evidenced by the increasing popularity of no code, autoML solutions such as Data Robot, Rapid Miner and Alteryx.Data scientists will be extinct in 10 years (give or take), or at least the role title will be. Going forward, the skill set collectively known as data science will be borne by a new generation of data savvy business specialists and subject matter experts who are able to imbue analysis with their deep domain knowledge, irrespective of whether they can code or not. Their titles will reflect their expertise rather than the means by which they demonstrate it, be it compliance specialists, product managers or investment analysts. We don’t need to look back far to find historic precedents. During the advent of the spreadsheet, data entry specialists were highly coveted, but nowadays, as Cole Nussbaumer Knaflic (the author of “Storytelling With Data”) aptly observes, proficiency with Microsoft Office suite is a bare minimum. Before that, the ability to touch type with a typewriter was considered a specialist skill, however with the accessibility of personal computing it has also become assumed.Lastly, for those considering a career in data science or commencing their studies, it may serve you well to constantly refer back to the Venn diagram that you will undoubtedly come across. It describes data science as an confluence of statistics, programming and domain knowledge. Despite each occupying an equal share of the intersecting area, some may warrant a higher weighting than others.Disclaimer: Views are my own, based on my observations and experiences. It’s ok if you don’t agree, productive discussion is welcome.",10/05/2021,0,1.0,0.0,1400.0,933.0,1.0,0.0,0.0,2.0,en
4004,Seq2Seq model in TensorFlow,Towards Data Science,Park Chansung,667.0,9.0,1140.0,"In this project, I am going to build language translation model called seq2seq model or encoder-decoder model in TensorFlow. The objective of the model is translating English sentences to French sentences. I am going to show the detailed steps, and they will answer to the questions likehow to define encoder model, how to define decoder model, how to build the entire seq2seq model, how to calculate the loss and clip gradients.Please visit the Github repo for more detailed information and actual codes in Jupyter notebook. It will cover a bit more topics like how to preprocess the dataset, how to define inputs, and how to train and get prediction.This is a part of Udacity’s Deep Learning Nanodegree. Some codes/functions (save, load, measuring accuracy, etc) are provided by Udacity. However, majority part is implemented by myself along with much richer explanations and references on each section. Also, base figures (about model) is borrowed from Luong (2016).You can separate the entire model into 2 small sub-models. The first sub-model is called as [E] Encoder, and the second sub-model is called as [D] Decoder. [E] takes a raw input text data just like any other RNN architectures do. At the end, [E] outputs a neural representation. This is a very typical work, but you need to pay attention what this output really is. The output of [E] is going to be the input data for [D].That is why we call [E] as Encoder and [D] as Decoder. [E] makes an output encoded in neural representational form, and we don’t know what it really is. It is somewhat encrypted. [D] has the ability to look inside the [E]’s output, and it will create a totally different output data (translated in French in this case).In order to build such a model, there are 6 steps overall. I noted what functions to be implemented are related to each steps.(1) define input parameters to the encoder model(2) build encoder model(3) define input parameters to the decoder model(4) build decoder model for training(5) build decoder model for inference(6) put (4) and (5) together(7) connect encoder and decoder models(8) define loss function, optimizer, and apply gradient clippingenc_dec_model_inputs function creates and returns parameters (TF placeholders) related to building model.inputs placeholder will be fed with English sentence data, and its shape is [None, None]. The first None means the batch size, and the batch size is unknown since user can set it. The second None means the lengths of sentences. The maximum length of setence is different from batch to batch, so it cannot be set with the exact number.targets placeholder is similar to inputs placeholder except that it will be fed with French sentence data.target_sequence_length placeholder represents the lengths of each sentences, so the shape is None, a column tensor, which is the same number to the batch size. This particular value is required as an argument of TrainerHelper to build decoder model for training. We will see in (4).max_target_len gets the maximum value out of lengths of all the target sentences(sequences). As you know, we have the lengths of all the sentences in target_sequence_length parameter. The way to get the maximum value from it is to use tf.reduce_max.On the decoder side, we need two different kinds of input for training and inference purposes repectively. While training phase, the input is provided as target label, but they still need to be embeded. On the inference phase, however, the output of each time step will be the input for the next time step. They also need to be embeded and embedding vector should be shared between two different phases.In this section, I am going to preprocess the target label data for the training phase. It is nothing special task. What all you need to do is add <GO> special token in front of all target data. <GO> token is a kind of guide token as saying like ""this is the start of the translation"". For this process, you need to know three libraries from TensorFlow.TF strided_sliceTF fillTF concatAfter preprocessing the target label data, we will embed it later when implementing decoding_layer function.As depicted in Fig 3, the encoding model consists of two different parts. The first part is the embedding layer. Each word in a sentence will be represented with the number of features specified as encoding_embedding_size. This layer gives much richer representative power for the words useful explanation. The second part is the RNN layer(s). You can make use of any kind of RNN related techniques or algorithms. For example, in this project, multiple LSTM cells are stacked together after dropout technique is applied. You can use different kinds of RNN cells such as GRU.Embedding layerRNN layersEncoding modelDecoding model can be thought of two separate processes, training and inference. It is not they have different architecture, but they share the same architecture and its parameters. It is that they have different strategy to feed the shared model. For this(training) and the next(inference) section, Fig 4 shows clearly shows what they are.While encoder uses TF contrib.layers.embed_sequence, it is not applicable to decoder even though it may require its input embeded. That is because the same embedding vector should be shared via training and inferece phases. TF contrib.layers.embed_sequence can only embed the prepared dataset before running. What needed for inference process is dynamic embedding capability. It is impossible to embed the output from the inference process before running the model because the output of the current time step will be the input of the next time step.How we can embed? We will see soon. However, for now, what you need to remember is training and inference processes share the same embedding parameters. For the training part, embeded input should be delivered. On the inference part, only embedding parameters used in the training part should be delivered.Let’s see the training part first.Embed the target sequencesConstruct the decoder RNN layer(s)Create an output layer to map the outputs of the decoder to the elements of our vocabularyIn this section, previously defined functions, encoding_layer, process_decoder_input, and decoding_layer are put together to build the big picture, Sequence to Sequence model.seq2seq_model function creates the model. It defines how the feedforward and backpropagation should flow. The last step for this model to be trainable is deciding and applying what optimization algorithms to use. In this section, TF contrib.seq2seq.sequence_loss is used to calculate the loss, then TF train.AdamOptimizer is applied to calculate the gradient descent on the loss. Let's go over eatch steps in the code cell below.load data from the checkpointcreate inputsbuild seq2seq modelcost functionOptimizerGradient ClippingMy background in deep learning is Udacity {Deep Learning ND & AI-ND with contentrations(CV, NLP, VUI)}, Coursera Deeplearning.ai Specialization (AI-ND has been split into 4 different parts, which I have finished all together with the previous version of ND). Also, I am currently taking Udacity Data Analyst ND, and I am 80% done currently.",02/05/2018,0,57.0,4.0,1253.0,794.0,6.0,25.0,0.0,46.0,en
4005,Computer Vision — A journey from CNN to Mask R-CNN and YOLO -Part 1,Towards Data Science,Renu Khandelwal,3900.0,12.0,1503.0,"In this article we will explore and understand the architecture and workings of different computer vision algorithm CNN, Region-based CNN(R-CNN), Fast R-CNN, Faster R-CNN. In the next article, we will explore Mask R-CNN and YOLO(You only look once)What is the purpose of Computer Vision?Computer vision is a subfield of AI. It is used to enable computers to understand, identify and generate intelligent understanding of the digital images the same way human vision does.What does Computer Vision do?Using Computer vision we can identifyWhen we view an image, we scan the image. We may view an image from left to right or top to bottom to understand the different features of the image. Our brain combines different local features that we scanned to classify the image. This is exactly how CNN works.CNN takes input as an image “x”, which is a 2-D array of pixels with different color channels(Red,Green and Blue-RGB).To the input image we apply different filters or feature detector to output feature maps. Filters or feature detectors are spatially small compared to the input image. These filters extend through the full depth of the input image.Multiple convolutions are performed in parallel by applying nonlinear function ReLU to the convolutional layer.Multiple feature detector identifies different things like edge detection, different shapes, bends or different colors etc.We apply Pooling to the convolutional layer. We can apply Min Pooling, Max Pooling or Average Pooling. Max pooling function provides better performance compared to min or average pooling.Pooling helps with Translational Invariance. Translational invariance means that when we change the input by a small amount the pooled outputs does not change.Invariance of image implies that even when an image is rotated, sized differently or viewed in different illumination an object will be recognized as the same object.In the next step, we flatten the pooled layer to input it to a fully connected(FC) neural network.We use a softmax activation function for multi class classification in the final output layer of the fully connected layer.For a binary classification we use a sigmoid activation function in the final output layer of the fully connected layer.Strength of CNNCNN is used forLimitations of CNNSo how do we identify multiple objects present in an image and draw bounding boxes around all the different objects?We now explore Region-based CNN’s that will help solve the problem of multiple objects present in an image and draw bounding boxes around all the different objects.R-CNN is used for classification as well as objection detection with bounding boxes for multiple objects present in an imageHow does R-CNN work?R-CNN works on a premise that only a single object of interest will dominate in a given region.R-CNN uses selective search algorithm for object detection to generate region proposals.so what forms a region in an image?Regions in an image can be identified byFigure(a), spoons, bowls are in different scales. Figure(b), kittens are distinguishable based on colors and not texture. Figure(c), Chameleon is distinguishable by texture, but not color. Figure(d), Wheels are part of the car, but not similar in color or texture. They are part of an enclosure.What is selective search and how will we use it to identify multiple objects in an image?Step 1: Generate initial sub-segmentation. We generate as many regions, each of which belongs to at most one object.Step 2: Recursively combine similar regions into larger ones. Here we use Greedy algorithm.This yields a hierarchy of successively larger regions, just like we wantStep 3: Use the generated regions to produce candidate object locations.Now that we know how Selective Search works, let’s get into the details of R-CNNR-CNN combines region proposal with CNN.Region proposals is a set of candidate detection available to the detector. CNN runs the sliding windows over the entire image however R-CNN instead select just a few windows. R-CNN uses 2000 regions for an image.Region proposals run an algorithm called a segmentation algorithm which uses selective search.but how does object detection in R-CNN work?To all scored regions in an image, apply a greedy non-maximum suppression.Non-Max suppression rejects a region if it has an intersection-over union (IoU) overlap with a higher scoring selected region larger than a learned threshold.Our objective with object detection is to detect an object just once with one bounding box. However, with object detection, we may find multiple detections for the same objects. Non-Max suppression ensures detection of an object only onceTo understand Non-Max suppression, we need to understand IoU.IoU computes intersection over the union of the two bounding boxes, the bounding box for the ground truth and the bounding box for the predicted box by algorithmWhen IoU is 1 that would imply that predicted and the ground-truth bounding boxes overlap perfectly.To detect an Object once in an image, Non-Max suppression considers all bounding boxes with IoU >0.5What if I have multiple bounding boxes with IoU greater than 0.5?For e.g. if we have three rectangles with the 0.6 and the 0.7 and 0.9. For IoU to identify the vehicle in the image below, Non-Max Suppression will keep the bounding box with IoU 0.9 and will suppress the remaining bounding boxes of 0.6 and 0.7 IoU.For the car in the image below, Non-Max Suppression will keep IoU with 0.8 and suppress or remove IoU bounding box with 0.7Biggest Challenges with R-CNN is that Training is slow and expensiveWhat makes training slow and expensive in R-CNN ?•CNN for feature extraction•Linear SVM classifier for identifying objects•Regression model for tightening the bounding boxesSo how do we make the algorithm more efficient and fast?Few things to be improved in R-CNN would beAll this is done in Fast R-CNN.Fast R-CNN is a fast framework for object classification and object detection with deep ConvNetsFast R-CNN network takes image and a set of object proposals as an input.Unlike R-CNN, Fast R-CNN uses a single deep ConvNet to extract features for the entire image once.We also create a set of ROI(Region of Interest) for the image using selective search. Region of interest (RoI) layer extracts a fixed-length feature vector from the feature map for each object proposal for object detection. RoI layer is a special-case of the spatial pyramid pooling layer with only one pyramid levelFully Connected layers(FC) needs fixed-size input. Hence we use ROI Pooling layer to warp the patches of the feature maps for object detection to a fixed size.ROI pooling layer is then fed into the FC for classification as well as localization. RoI pooling layer uses max pooling. It converts features inside any valid region of interest into a small feature map.Fully connected layer branches into two sibling output layersFast R-CNN uses selective search as a proposal method to find the Regions of Interest, which is slow and time consuming process. Not suitable for large real-life data setsFaster R-CNN does not use expensive selective search instead uses Region Proposal Network.It is a single, unified network for object detectionFaster R-CNN consists of two stagesRegion Proposal Network takes an image of any size as input and outputs a set of rectangular object proposals each with an objectness score. It does this by sliding a small network over the feature map generated by the convolutional layerRPN shares computation with a Fast R-CNN object detection network.Feature generated from RPN is fed into two sibling fully connected layers — a box-regression layer for the bounding box and a box-classification layer for object classification.RPN is efficient and processes 10 ms per image to generate the ROI’s.An anchor is centered at the sliding window in question and is associated with a scale and aspect ratio. Faster R-CNN uses 3 scales and 3 aspect ratio, yielding 9 anchors at each sliding windows.Anchors help with translational invariance.At each sliding window location, we simultaneously predict multiple region proposals. The number of maximum possible proposals for each location is denoted as k.Reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate the probability of object or not object for each proposalFaster R-CNN is composed of 3 different neural networksFaster R-CNN takes image as an input and is passed through the Feature network to generate the feature map.RPN uses the feature map from the Feature network as an input to generate the rectangular boxes of object proposals and the objectness score.The predicted region proposals from RPN are then reshaped using a RoI pooling layer. Warped into a fixed vector size.Warped fixed-size vector is then fed into two sibling fully connected layers, a regression layer to predict the offset values for the bounding box and a classification layer for object classificationWe started with a simple CNN used for image classification and object detection for a single object in the image.R-CNN is used for image classification as well as localization for multiple objects in an image.R-CNN was slow and expensive so Fast R-CNN was developed as a fast and more efficient algorithm. Both R-CNN and Fast R-CNN used selective search to come up with regions in an image.Faster R-CNN used RPN(Region Proposal Network) along with Fast R-CNN for multiple image classification, detection and segmentation.In the next article, we will explore YOLO and Mask R-CNN.References:http://vision.stanford.edu/teaching/cs231b_spring1415/slides/ssearch_schuyler.pdfhttps://arxiv.org/pdf/1406.4729.pdfhttps://arxiv.org/pdf/1506.01497.pdfhttps://arxiv.org/pdf/1311.2524.pdfhttp://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdfhttp://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdfhttps://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdfhttp://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf",22/07/2019,0,67.0,13.0,693.0,379.0,20.0,20.0,0.0,9.0,en
4006,The Dropout Tag I Wear,Medium,Kerish Heik,25.0,3.0,663.0,"*On a personal note, before reading this article take a deep breath and relax yourself. In this article, you will neither hear any neighbor’s aunties gossiping ills about you nor see your parents hesitations when you say something cause you are wearing a dropout tag that isn’t sugar coated. This is an article on the bright side of the moon about how I get the inspiration to ultimately drop out.This moment in my life about a year ago I got the ultimate boredom to drop out of my class to do something of my own that I am really passionate about. Everyone in the class was doing the same thing, solving Irodov’s problems where the task itself would be a terror for every country’s layman and more importantly, till now I don’t find any usefulness of that things beside teaching to someone else. Some were busy with the war on organic chemistry — attacking, adding…I was simply thinking why everyone in the class,so is the same with everyone in the country, doing the same topics again and again for decades. Why don’t I try something different, something unique instead of memorizing the facts inside the classroom.Then a new page begins in my life… [background music]…Then instead of memorizing the facts to get high marks in exam, I started putting a fullstop in that line. I started reading the life histories of some founders and inventors of the formulae… Their life histories even tapped my passion for doing something unique, something of my own. New things started and new habits formed. I started sleeping during daytime and read articles, watched online tutorials at night, cause at night (12AM- 6AM) you can get 1GB 3G internet with high speed for just 49 rupees which costs 249 rupees for the same plan during daytime. This continues for months… which ultimately deteriorates my marks so I couldn’t impress the eyes of my teachers & parents anymore.While reading some life histories of some dot com founders I came to heard of a foundation called Thiel Fellowship where I found young people doing unique things, founding new companies, inventing new gadgets… showing that Age is simply a number. By seeing that I felt pains of losing chances, I kept thinking why I should wait for someone to do things first.And I also found out this article which I would love to share -“Never let your schooling interfere your education.”which motivated me so much that I even hung the quote on my room wall to start seeing things differently.This all sums up to the ultimate decision to drop out to do something unique of my own pretty unknowing the future that lies ahead, I returned back to my native village.*Pause here and relax yourself. You are just seeing the green world outside. You have to go depth before you drop out because things don’t turn out well sometimes. If you drop out you won’t hear the birds’ melodies in the spring, but annoying neighbor’s aunties gossiping about you. Things will turn out like you are being hitted by a brick in your forehead.[ The Story of how I spend the year as a dropout will be on the next episode.]I say again Don’t drop out. Only people with guts drop out.Now after one year I’m coming back to the city and this time I’m here to show something unique(in my term) to my friends with my first startup - Dukaan.IO.Its not because we are back by top rated venture capitalists, not because we are pool of top talented people in the field. But we are that kind of guy who are young and really passionate about to make the world a place that sucks less.We may fail miserably fighting the giants, but I strongly feel that learning by doing things outside the classroom will be far greater than sitting inside the classroom. We may be called as the crazy ones but we hope that we will influence them one day.",08/01/2016,0,3.0,5.0,995.0,720.0,1.0,0.0,0.0,0.0,en
4007,Attention and its Different Forms,Towards Data Science,Anusha Lihala,108.0,6.0,883.0,"I assume you are already familiar with Recurrent Neural Networks (including the seq2seq encoder-decoder architecture).In the encoder-decoder architecture, the complete sequence of information must be captured by a single vector. This poses problems in holding on to information at the beginning of the sequence and encoding long-range dependencies.The core idea of attention is to focus on the most relevant parts of the input sequence for each output. By providing a direct path to the inputs, attention also helps to alleviate the vanishing gradient problem.Assume you have a sequential decoder, but in addition to the previous cell’s output and hidden state, you also feed in a context vector c.Where c is a weighted sum of the encoder hidden states.Here αᵢⱼ is the amount of attention the ith output should pay to the jth input and hⱼ is the encoder state for the jth input.αᵢⱼ is computed by taking a softmax over the attention scores, denoted by e, of the inputs with respect to the ith output.whereHere f is an alignment model which scores how well the inputs around position j and the output at position i match, and sᵢ₋₁ is the hidden state from the previous timestep.The alignment model can be approximated by a small neural network, and the whole model can then be optimised using any gradient optimisation method such as gradient descent.The context vector cᵢ can also be used to compute the decoder output yᵢ.Attention was first proposed by Bahdanau et al.[1] for Neural Machine Translation. The mechanism is particularly useful for machine translation as the most relevant words for the output often occur at similar positions in the input sequence.The matrix above shows the most relevant input words for each translated output word.Such attention distributions also help provide a degree of interpretability for the model.Given a query q and a set of key-value pairs (K, V), attention can be generalised to compute a weighted sum of the values dependent on the query and the corresponding keys. The query determines which values to focus on; we can say that the query ‘attends’ to the values.In the previous computation, the query was the previous hidden state sᵢ₋₁ while the set of encoder hidden states h₀ to hₙ represented both the keys and the values.The alignment model, in turn, can be computed in various ways.With self-attention, each hidden state attends to the previous hidden states of the same RNN.Here sₜ is the query while the decoder hidden states s₀ to sₜ₋₁ represent both the keys and the values.The paper ‘Pointer Sentinel Mixture Models’[2] uses self-attention for language modelling.The basic idea is that the output of the cell ‘points’ to the previously encountered word with the highest attention score. However, the model also uses the standard softmax classifier over a vocabulary V so that it can predict output words that are not present in the input in addition to reproducing words from the recent context.The probability assigned to a given word in the pointer vocabulary distribution is the sum of the probabilities given to all token positions where the given word appearswhere I(w, x) results in all positions of the word w in the input x and pₚₜᵣ∈ Rⱽ. This technique is referred to as pointer sum attention.The model combines the softmax vocabulary distribution with the pointer vocabulary distribution using a gate g which is calculated as the product of the query and a sentinel vector.The paper ‘A Deep Reinforced Model for Abstractive Summarization’[3] introduces a neural network model with a novel self-attention that attends over the input and continuously generated output separately,The computations involved can be summarised as follows.When we have multiple queries q, we can stack them in a matrix Q.If we compute alignment using basic dot-product attention, the set of equations used to calculate context vectors can be reduced as follows.Multi-head attention takes this one step further.Q, K and V are mapped into lower dimensional vector spaces using weight matrices and then the results are used to compute attention (the output of which we call a ‘head’).We have h such sets of weight matrices which gives us h heads.The h heads are then concatenated and transformed using an output weight matrix.The Transformer was first proposed in the paper ‘Attention Is All You Need’[4]. It is based on the idea that the sequential models can be dispensed with entirely, and the outputs can be calculated using only attention mechanisms.The Transformer uses word vectors as the set of keys, values as well as queries.It contains blocks of Multi-Head Attention, while the attention computation itself is Scaled Dot-Product Attention.where dₖ is the dimensionality of the query/key vectors.The scaling is performed so that the arguments of the softmax function do not become excessively large with keys of higher dimensions.Below is the diagram of the complete Transformer model along with some notes with additional details. For more in-depth explanations, please refer to the additional resources.[1] D. Bahdanau, K. Cho, and Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate (2014)[2] S. Merity, C. Xiong, J. Bradbury, and R. Socher, Pointer Sentinel Mixture Models (2016)[3] R. Paulus, C. Xiong, and R. Socher, A Deep Reinforced Model for Abstractive Summarization (2017)[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention Is All You Need by (2017)",29/03/2019,0,27.0,8.0,714.0,341.0,26.0,1.0,0.0,22.0,en
4008,Movix.ai — movie recommendations with Deep Learning,Supervisely,Supervise.ly,3000.0,11.0,2049.0,"“What movie should i watch this evening?” — have you ever had to answer this question at least once when you came home from work? As for us — yes, and more than once. Here we will say a few words about what we’ve been working on for the past six months: an interactive movie recommender system Movix.ai. The system is based on Deep Learning and it adapts to the user preferences in real time. As big movie fans we felt the need for such a service, and we believe that it will be useful for every movie lover.At Deep Systems we are engaged in creating solutions and products based on machine learning and Deep Learning. Among our projects: developing a “mind” for self-driving car prototype and automatic defects detection for roads and airport runways. The important part of our work are recommender systems. The strong desire to create our version of recommender system has long prevented us from sleeping peacefully.The reason of writing this post is to share our service with the World, to get feedback about our work, vision and, at the same time, to share experiences that may be of interest to deep learning practitioners, and other people.Here is the phrase from J.Schmidhuber*, which we can not get rid of:Google of the future together with all its services is just a single giant LSTM.Here we mean that there is one large neural network that interacts with the user and solves a variety of his tasks.*not completely sure that Jurgen actually said this but deep learning researchers are aware now of how dangerous it is not to cite him :-)The idea seems too ambitious, perhaps utopian. We tried to “land” this idea and find the domain where a single large neural network can solve all the tasks for the user. So the idea was born to build a movie recommender system, which will interact with the user in a smart way, and to model the interaction in “end to end” manner with deep lstm like network.Today there is a huge hype around chatbots. As for the academic community, at the end of the day, it’s all about passing the Turing test. For the large companies operational cost optimization is a concern, so guys from tech support should keep a weather eye open. All jokes aside. In many cases, typing text as a way to talk to the computer may be inconvenient and the “language of clicks” is more appropriate.Many recommender systems are built on the concept of similar items — that is, for each movie there is a predefined set of movies similar to it. This does not take into account the preferences of a particular user. As a consequence, the user is forced to explore the static content and have no tool to tell the system about his preferences. This is not interactive approach. One the other hand, we believe that interactivity is a “must have” component of a good recommender system.Our concept is the following: no registration is required, the user visits the site, makes a few clicks on movies or tags and receives recommendations reflecting his current mood and preferences. There are two entities which the system predicts: the movies and tags. The movie is the ultimate goal, i.e. the user is here because he wants to find a movie to watch, whereas tags are additional user interaction tool allowing the user to feed his current preferences faster into the system.There is one reasonable question to ask: “Why using neural networks after all? Collaborative filtering approaches exist for many years, well understood and work fine.”We will answer step by step. When talking about collaborative filtering, we should clearly distinguish the following two tasks: (1) rating prediction and (2) top N recommendations.The task of rating prediction is much more popularized and, as a consequence, tons of papers and open source libraries are there. However, speaking about top N recommendation task, the situation is quite the opposite. The reason — Netflix Challenge (2006–2009) with a prize fund of $ 1 million, where the participants were asked to predict “how user u will rate the movie m”.However, in most business applications, it is required to give top N recommendations. Typical cases are: based on historical data for a particular user, show 10 items that he is most likely would buy or, as in our case, show 10 films that he most likely wants to see.Without a doubt, the task (2) can be reduced to the task (1) in a following naive way: take the user, predict the ratings for all the movies from our catalog, sort the movies in descending order by the predicted ratings, then take the top 10 movies and recommend them. It sounds like a good idea, but there is one problem — it does not work (we were the ones who did this mistake). So when you look at the recommendations you do not like it (metrics also reflect inner feelings)!We are done with rating prediction problem, let’s return to the top N recommendations. Again, there are two ways of solving the problem: (a) Matrix Factorization, (b) Nearest neighbours approach.Matrix Factorization methods have the following drawback — they are not interactive, in the sense that if a user has rated to a movie, then to update the recommendations for him, you need to re-do the factorization procedure. Since we want to recommend “on the fly”, for us it is unacceptable.Nearest neighbours are interactive. Pedro Domingos classifies this approach to “Lazy machine learning methods”, as training procedure is equal to saving new training examples in the database. So, in terms of computational costs, training is free and all the work is done during inference stage. But when it comes to a metric, the best one can do is to rely on some sort of heuristics. In case we want to go beyond movies and work also with other entities, like tags, the metric issue is even bigger.We are not saying that standard approaches are bad, we are just pointing out the evident advantage of deep learning approach for this task: just feed all the available data to the deep model and formulate training objective that is somehow correlated with quality of user experience. If one does it in a right way, all we have to do is to wait until the training procedure is converged to some local minimum.The Deep Learning revolution first came to the area of ​​speech recognition, then to computer vision, and, after that, to natural language processing (NLP). Many NLP tasks are reduced to answering the question: what is the probability distribution for the next word, if we know N previous words? Or simply — to predict the next word in the sentence (in the text).Today, in most NLP tasks, large recurrent neural networks (LSTMs) dominate other approaches, i.e. neural networks are pretty good at predicting the next word in a sequence.Many recommender systems are built on the concept of similar items — that is, for each movie there is a predefined set of movies similar to it. This does not take into account the preferences of a particular user. As a consequence, the user is forced to explore the static content and have no tool to tell the system about his preferences. This is not interactive approach. One the other hand, we believe that interactivity is a “must have” component of a good recommender system.The point is that the database of user ratings can be represented as one very long text. This text will consist of sentences, and each sentence is a list of movie IDs that a particular user liked.Consider a very simple example:“100 200 123/0 100 10 300/0 1 2 3 4 5/0”We can see thatUser IDs are not important, only movie IDs (and its relative order) are importantAfterwards, in theory, one can take the state of the art NLP model and train it to predict the next identifier in our “text”, which during the serve phase will represent the actual recommendation.More than a year ago, we took MovieLens dataset, torch7 based NLP project, and done the above procedure to obtain our first movie recommender prototype.But we wanted more, both in terms of the quality of the recommendations, and in the way we utilize Deep Learning techniques for our task.The hypothesis is that by allowing the user to operate both movies and tags, we speed up his way to a list of relevant movies reflecting his current mood.The task of constructing a neural network architecture, capable of working with the two mentioned entities, arises. See figure 1.With each movie that the user has liked, a fixed, predefined set of tags is associated. For both movies and tags embedding takes place that is just the mapping from movie and tag identifiers to the fixed size vectors. For tags, the vectors obtained as a result of embedding, are averaged. So, for each movie that the user liked, the LSTM cell takes as input the concatenation of the following vectors:The output of a 2-layer LSTM (output vector of the upper right LSTM cell) goes to two separated fully connected layers (FC). Then softmax layers allow to estimate the “like” probability for each move and tag in the database. Top N movies and tags are shown to user.Let’s say a few more words about tags. In terms of recommendations quality, tags may be useful even if we do not directly predict them. They give the model more information that some movies are similar to each other. For example, consider two movies in a case when there is no user in the database that liked both of them. The fact that these movies may have a lot of common tags gives an opportunity for the system to figure out that the movies are, indeed, similar. The type of tags we have just talked about is called “tags associated with movies” (figure 1).Another scenario is to allow the user to like tags along with the movies ( “tags chosen by user” in figure 1). It is important that we can simulate this scenario at the training stage. Initially, the neural network predicts the next movie that the user likes based on previously “liked” movies, but we know the tags for the next movie in a sequence. Therefore, a significant amount of the training time, we can force the model to solve the following problem: knowing the movie history for a user and some set of tags associated with the next movie in sequence, guess what exactly the next movie is (it would be convenient to formulate the last statement in terms of conditional probability, but in this post we decided to do without formulas, if there is interest, we will write a more technical post). Also note, that in the scenario under consideration there may be no “liked” movies at all — the user, for example, choose a group of diverse tags and still receives recommendations.Our deep model is LSTM based neural network that is built using the TensorFlow framework.To create a training data, we’ve used the MovieLens dataset, where we took user’s movie preferences. We parsed IMDB and used the Movie DB API to form tags database.The API interacts with TensorFlow through ZeroMQ, and Elastic Search acts as a storage for information retrieval about the movies.The frontend is made using Vue.js and Element UI.Movix allows you to perform the following actions:The features mentioned above, allow you to interact with the system in a flexible way. For example, choosing a few old favorite movies, and then, turning on the filter “2010s”, allows you to discover the most recent movies that are similar to the ones you have chosen (“liked”). The same logic works in opposite direction — to discover the old movies.Let’s say a few words about the improvements to make the system more interactive and intelligent. We will focus more on Deep Learning aspects, rather than possible features and GUI:It was a pleasure to work on the service that may be useful to so many people. We love the service we managed to build and regularly use it to discover new movies to watch.Any feedback, comments, ideas and suggestions are very much appreciated!We hope, you’ll discover your movie!Press about us: Movix uses artificial intelligence to hit you with the best movie suggestions",02/05/2017,0,13.0,2.0,1253.0,690.0,3.0,5.0,0.0,15.0,en
4009,How to do Deep Learning on Graphs with Graph Convolutional Networks,Towards Data Science,Tobias Skovgaard Jepsen,1000.0,9.0,1313.0,"Machine learning on graphs is a difficult task due to the highly complex, but also informative graph structure. This post is the first in a series on how to do deep learning on graphs with Graph Convolutional Networks (GCNs), a powerful type of neural network designed to work directly on graphs and leverage their structural information. The posts in the series are:In this post, I will give an introduction to GCNs and illustrate how information is propagated through the hidden layers of a GCN using coding examples. We’ll see how the GCN aggregates information from the previous layers and how this mechanism produces useful feature representations of nodes in graphs.GCNs are a very powerful neural network architecture for machine learning on graphs. In fact, they are so powerful that even a randomly initiated 2-layer GCN can produce useful feature representations of nodes in networks. The figure below illustrates a 2-dimensional representation of each node in a network produced by such a GCN. Notice that the relative nearness of nodes in the network is preserved in the 2-dimensional representation even without any training.More formally, a graph convolutional network (GCN) is a neural network that operates on graphs. Given a graph G = (V, E), a GCN takes as inputA hidden layer in the GCN can thus be written as Hⁱ = f(Hⁱ⁻¹, A)) where H⁰ = X and f is a propagation [1]. Each layer Hⁱ corresponds to an N × Fⁱ feature matrix where each row is a feature representation of a node. At each layer, these features are aggregated to form the next layer’s features using the propagation rule f. In this way, features become increasingly more abstract at each consecutive layer. In this framework, variants of GCN differ only in the choice of propagation rule f [1].One of the simplest possible propagation rule is [1]:f(Hⁱ, A) = σ(AHⁱWⁱ)where Wⁱ is the weight matrix for layer i and σ is a non-linear activation function such as the ReLU function. The weight matrix has dimensions Fⁱ × Fⁱ⁺¹; in other words the size of the second dimension of the weight matrix determines the number of features at the next layer. If you are familiar with convolutional neural networks, this operation is similar to a filtering operation since these weights are shared across nodes in the graph.Let’s examine the propagation rule at its most simple level. LetIn other words, f(X, A) = AX. This propagation rule is perhaps a bit too simple, but we will add in the missing parts later. As a side note, AX is now equivalent to the input layer of a multi-layer perceptron.As a simple example, we’ll use the the following graph:And below is its numpy adjacency matrix representation.Next, we need features! We generate 2 integer features for every node based on its index. This makes it easy to confirm the matrix calculations manually later.Alright! We now have a graph, its adjacency matrix A and a set of input features X. Let’s see what happens when we apply the propagation rule:What happened? The representation of each node (each row) is now a sum of its neighbors features! In other words, the graph convolutional layer represents each node as an aggregate of its neighborhood. I encourage you to check the calculation for yourself. Note that in this case a node n is a neighbor of node v if there exists an edge from v to n.You may have already spotted the problems:In the following, I discuss each of these problems separately.To address the first problem, one can simply add a self-loop to each node [1, 2]. In practice this is done by adding the identity matrix I to the adjacency matrix A before applying the propagation rule.Since the node is now a neighbor of itself, the node’s own features is included when summing up the features of its neighbors!The feature representations can be normalized by node degree by transforming the adjacency matrix A by multiplying it with the inverse degree matrix D [1]. Thus our simplified propagation rule looks like this [1]:f(X, A) = D⁻¹AXLet’s see what happens. We first compute the degree matrix.Before applying the rule, let’s see what happens to the adjacency matrix after we transform it.BeforeAfterObserve that the weights (the values) in each row of the adjacency matrix have been divided by the degree of the node corresponding to the row. We apply the propagation rule with the transformed adjacency matrixand get node representations corresponding to the mean of the features of neighboring nodes. This is because the weights in the (transformed) adjacency matrix correspond to weights in a weighted sum of the neighboring nodes’ features. Again, I encourage you to verify this observation for yourself.We now combine the self-loop and normalization tips. In addition, we’ll reintroduce the weights and activation function that we previously discarded to simplify the discussion.Adding back the WeightsFirst order of business is applying the weights. Note that here D_hat is the degree matrix of A_hat = A + I, i.e., the degree matrix of A with forced self-loops.And if we want to reduce the dimensionality of the output feature representations we can reduce the size of the weight matrix W:We choose to preserve the dimensionality of the feature representations and apply the ReLU activation function.Voila! A complete hidden layer with adjacency matrix, input features, weights and activation function!Now, finally, we can apply a graph convolutional network on a real graph. I will show you how to produce the feature representations we saw early in the post.Zachary’s karate club is a commonly used social network where nodes represent members of a karate club and the edges their mutual relations. While Zachary was studying the karate club, a conflict arose between the administrator and the instructor which resulted in the club splitting in two. The figure below shows the graph representation of the network and nodes are labeled according to which part of the club. The administrator and instructor are marked with ‘A’ and ‘I’, respectively.Now let us build the graph convolutional network. We won’t actually train the network, but simply initialize it at random to produce the feature representations we saw at the start of this post. We will use networkx which has a graph representation of the club easily available, and compute the A_hat and D_hat matrices.Next, we’ll initialize weights randomly.Stack the GCN layers. We here use just the identity matrix as feature representation, that is, each node is represented as a one-hot encoded categorical variable.We extract the feature representations.And voila! Feature representations that separate the communities in Zachary’s karate club quite well. And we haven’t even begun training yet!I should note that for this example the randomly initialized weights were very likely to give 0 values on either the x- or the y-axis as result of the ReLU function, so it took a few random initializations to produce the figure above.In this post I have given a high-level introduction to graph convolutional networks and illustrated how the feature representation of a node at each layer in the GCN is based on an aggregate of the its neighborhood. We saw how we can build these networks using numpy and how powerful they are: even randomly initialized GCNs can separate communities in Zachary’s Karate Club.In the next post, I will go a bit more into technical detail and show how to implement and train a recently published GCN using semi-supervised learning. You find the next post in the series here.Liked what you read? Consider following me on Twitter where I share papers, videos, and articles related to the practice, theory, and ethics of data science and machine learning that I find interesting in addition to my own posts.For professional inquiries, please contact me on LinkedIn or by direct message on Twitter.[1] Blog post on graph convolutional networks by Thomas Kipf.[2] Paper called Semi-Supervised Classification with Graph Convolutional Networks by Thomas Kipf and Max Welling.",18/09/2018,15,34.0,75.0,1058.0,561.0,4.0,4.0,0.0,12.0,en
4010,Generative AI: Visual Search as a Bridge between Fiction and Reality,Towards Data Science,Merzmensch,2800.0,7.0,1091.0,"First, tell me, please, what is fiction and what is reality — in the context of Generative Adversarial Networks?We’ve seen a lot of things, which hadn’t existed before its AI-driven creation. Sure, the GAN-generated images in This Person Does Not Exist or This Artwork Does Not Exist have no direct reference in the material world — they are products of knowledge and AI models training. But being transported into our world, they might get their own story, specific meaning, and particular use, leaving the Latent Space and become more real than fiction.Indeed, you can use them for making movies; you also can generate fraud and fakes. AI is not to blame for misuse, but us, humans. You cannot fix society by breaking technology.Nevertheless, in Digital Times, it’s important to distinguish between the generative and non-generative items, prevent fakes, and advantage of artistic power until now non-existent possibilities.Here are three use cases that can inspire you or also raise awareness.StyleGAN2 researchers have a built-in “backdoor” or the possibility to trace back GAN-generated images to the Latent Space: StyleGAN2 Projection.This might be very useful if you try to identify an image as a generative one. I have to say, playing around for a longer time with StyleGAN2 Colab Notebooks and Artbreeder (where StyleGAN2-model, trained on FFHQ-faces is used) you can learn the intuition to detect portraits that might be generative.But using Projection you can do it as well.Here is the best example of detecting a generative image and discovering it with Latent Space:Of course, the original image was here already weird enough. Compare it with another portrait:As you see, the found portrait is not the same. Why? Because I modified it using other tools like ArtBreeder (mixing various images, including my photograph). Here is the “genealogical tree” of the image:This is the first and probably a significant flaw of StyleGAN2-Projection. To detect the image 1:1 following conditions should be fulfilled:StyleGAN2 projection is a great possibility but can be cheated in several ways; that’s why it is not reliable. Nevertheless, it can be used as the first tool — and in particular cases, it can be beneficial.You remind probably “Hunter Biden Conspiracy” story, featuring Martin Aspen, who claimed to have evidence in this fabricated political image-damaging pro-Trump campaign. Yet, Martin Aspens didn’t exist; his face was StyleGAN2 generated, and here is a StyleGAN2 projection:As you see, the faces aren’t entirely the same — the political string-puller behind this story might have used StyleGAN2 with several modifications.But even if this approach doesn’t work to 100%, it should be emphasized that the AI researchers put many efforts into possibilities to prevent AI-solutions from misuse (even if rather post-factum, as fake detection).SideNote: after all, you can use StyleGAN2 projection for artistic needs, looking for faces where are not such. It works with the Pareidolia method and might deliver some fascinating / or frightening results:Another way to detect fake images is Reverse Image Search. Here is an excellent use case of backtracking fakes using Russian Search Engine Yandex:In this case, a face taken from “This Person Does Not Exist” page is used for the search of identical images. So a fake company page was detected, with a couple of GAN-generated faces:To be honest, there are two things about it:Google Search might be a good solution to look up fake companies worldwide, but it has its flaws:Google Image Search is looking for a similar image, not an identical one. I’d want to use TinEye instead. This tool is usually perfect for detecting whether a user contacted you on a social network is a fake. Matching it with other identical web images might prove that this userpic is a stock image, and a person using this userpic might be a scammer. There is even a TinEye plugin.The problem with TinEye is: there is no complete open repository of ThisPersonDoesNotExist to double-check (which should mean an entire Latent space being posted with open access, and that is probably not feasible).As the opposite, Yandex can find another GAN-generated face if you upload just one of them. And this Similarity of Yandex works way better than the Similarity of Google Image Search.Probably, Yandex looks for specific patterns that are typical for GAN-faces? The downside of Yandex is that it’s still searching predominantly in Russian webspace, which might cause a biased view of “fake faces cyberscapes”.I wish a TinEye or Yandex alike services also StyleGAN2 or other GAN-generated images, probably even as a Chrome PlugIn, looking globally for such fake photos. Probably using StyleGAN2 projection, which might take some time: with StyleGAN2 Colab Notebook, the Projection takes around 10–15 minutes to generate.But it isn’t just about politics, fraud, and fakes. Artificial Intelligence as a creative machine opened new horizons for artists, writers, moviemakers. Trained on real data, AI delivers its “hallucinations” or “dreams” about our world. And the visual manifestation of AI dreams becomes our world.Recently, a fascinating Visual Search Engine was unveiled: Same.Energy by multimedia artist and game designer Jacob Jacobson. As its title suggests, this engine looks for images with the “same energy”, stylistically and visually similar to the image you’ve input into search. The engine works using machine learning and is fuelled by a visual classification model similar to CLIP by OpenAI (the model released among with acknowledging the DALL-E).If you enter here an arbitrary image, the system identifies it in specific features, patterns, and objects — and looks for semantical analogies.For example, if we enter a self-portrait of Da Vinci, we’ll get a group of other images describing something like “an old man with a beard as a painting or a drawing”.Playing around with BigGAN images (generated by Colab Notebook and Artbreeder) I wondered howa) might Machine Learning detect generative images from mixed seedsb) can this Visual Search Engine find similar images from the real world. Sure, many of them will be the fruit of imagination, but still of a human one.Uploading some of the images, I found a fantastic possibility for inspiration and brainstorming.Let’s take, for example, this one. Here you can see the seeds and their intensity use for generating this black&white artistic image:And here are images, same.energy could find:Or another one:Same.energy, with pretty haunting images.Another semi-abstract vision:The result is stunning:Thus, the combination of AI-generative images and results from the visual search can bring new insights, discoveries, and ways of visual exploration of our world. Sure, it works with the usual images as input as well. But after my experiences with using the neo-abstractions by GAN I can see the power of unknown queues, triggering unexpected visual associations.",08/02/2021,0,20.0,11.0,1096.0,671.0,14.0,2.0,0.0,28.0,en
4011,EfficientNet B6+AutoAugと同等程度の精度で5倍早いAssemble-ResNet,Medium,Akihiro FUJII,447.0,13.0,50.0,"この記事は、EfficientNet B6+AutoAugと同等程度の精度で5倍早いAssemble-ResNetを提案した2020/1/17投稿の論文””Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network [1]の解説記事です。この記事では以下のこと説明します。この論文のサマリは以下のような感じです。既存のあらゆるテクニックを組み合わせて、EfficientNet B6+AutoAugと同等程度の精度で5倍早いネットワークを構築した研究。著者たちがいうにはAugMix等の最新のものはここでは使ってないので、まだ精度があがる可能性があるとのこと。ここでは、Assemble-ResNetのベースライン比較となっているEfficientNet+AutoAugmentの解説をします。EfficientNetは2019年に発表された既存のネットワークより大幅に軽くて高精度なネットワークです。AutoAugmentは2018年に発表された論文で、最適なデータ拡張を自動で探索する研究です。どちらも画像認識では頻繁にベースラインとして登場する強力な手法です。EfficientNet[2]は2019/5/28に投稿された論文で、それまでの既存のネットワークより高速で高精度なネットワークです。論文の内容をまとめると下記のような感じです。今まで成されていなかった解像度・深さ・チャネル数を同時に最適化することによって、高速かつ高精度なネットワークを構築。式3におけるφ＝１にしてMnasNetの探索空間でαβγを最適化（B0）、後にφを変えて再最適化を繰り返してB1->B7を構築する。パラメータ探索はグリッドサーチで行う。パラメータ探索は深さ、モデルの幅(CNNのチャネル数)、解像度の3つを同時に探索します。α、β、γの3つを制御する拘束条件を設け、その範囲でgrid searchを行うことにより、最適なネットワークを見つけます。探索の拘束条件が厳しい順に、EfficientNetB0, B1…B7となっています。AutoAugmentは2018/5/24に投稿された論文で、最適なデータ拡張手法を強化学習で探索するという内容です。5つのsub policyから成るデータ拡張policyをvalidation dataのロスが小さくなるように最適化していきます。下記がAutoAugmentの結果です。このようにAutoAugmentだけでかなり強力な手法なのですが、EfficientNetにAutoAugmentを加えたモデルにすら勝ったしまったのが今回紹介するAssemble-ResNetです。EfficientNetに勝ってしまうほど強力なAssemble-ResNetのポイントは、「既存の良さげなネットワーク構造と正則化手法を全部ResNetに載せた」ということに尽きます。ImageNetの精度改善において、アーキテクチャ改善に使ったのは、ResNet-D[4], Selective-Kernel[5], Anti-Alias Downsampling[6]、Big Little Network[10]です。正則化はLabel Smoothing, Mixup[7], DropBlock[8], Knowledge Distillation[9]、AutoAugment[2]を使っています。以下で、Assemble-ResNetに使用したネットワーク構造の改善手法と正則化手法を説明します。ResNet-DはCVPR2019に提案されたResNetの改善アーキテクチャ[4]です。下図のような構造をしており、あまり計算コストを上げずに精度を押し上げることができる手法です。Selective Kernel[5]は、人間の画像認識の受容野の大きさがニューロン毎に異なっていることにヒントを得て、CNNのカーネルサイズ3x3, 5x5それぞれでたたみ込んだものにAttentionをかけて統合する仕組みになっています。Assemble-ResNetでは、速度と精度のバランスを鑑みて5x5サイズのカーネルを使う代わりにチャネル数を倍にした3x3のフィルターで代替することを検討しています。検討の結果、Table2のC3の構造を採用しています。Anti-Alias Downsampling[6]は、画像の並行移動に対して頑健性をもたせるために考えられた機構です。MaxPooling等のDownsampling手法は、画素上だとshift invariantにならなりません。そのため、CNNの出力が位置ずれに対して頑健性がありませんでした。donwsamplingの過程で、anti-aliasing処理を入れること(BlurPool)により、shift invariant性を持たせるという手法です。Big Little Networkは、複数の解像度の画像を効率よく処理するためのネットワーク構造です。下図は解像度の枝分かれ数K＝２の例で、下のルートでは入力の解像度と同じ画像を処理し、上のルートでは縦横の大きさ1/2の解像度の画像を処理しています。Assemble-ResNetでは、通常のResidual Blockに加えて、解像度を下げた画像を処理するルートを加えています。以上のテクニックを用いて構成したネットワークが下図です。通常のResNet-50に上記のテクニックを加えた構成にしてあります。ラベル平滑化は、[0.0, 1.0] のようなone-hot形式のhard labelではなく、[0.1, 0.9]のようなsoft labelというラベルを使う手法です。この研究[11]によるとラベル平滑化はカテゴリ毎の分布をまとまらせる効果があり、またデータへの過適合(softmaxの値が0.99でもペナルティが発生する)を抑えるため、有効だということです。MIXUP[7]は入力と出力両方を混ぜ合わせるデータ拡張手法です。β分布から混合比率をサンプリングし、2つのデータの入出力両方を混ぜ合わせます。各クラスのクラスタ間に内挿データを作るため、潜在空間を滑らかにする働きがあると言われています。DropBlock[8]はCNN用のDropoutです。通常のDropoutだと、下図bのように画像が歯抜け状態になりますが、Dropされた画素に隣接する画素はDropされていないことが多いため、画像の正則化としては弱くなってしまいます。そこでDropBlockでは、2次元空間上で隣り合った画素もまとめてDropすることにより正則化効果を高めています。Knowledge Distillation(蒸留)は、巨大で高精度なネットワークの知識を、小規模で軽いネットワークに移植する手法です。蒸留元の大規模なネットワークは教師モデル、蒸留先の小規模なネットワークは生徒モデルと呼ばれ、生徒モデルは、通常の教師ラベル(hard label)とのクロスエントロピーだけでなく教師モデルの出力(soft label)も教師信号として目的関数に取り込み、学習させます。この手法を使うと、生徒モデルとラベル付きデータだけで学習さえるよりも、高精度な教師モデルに近い精度が得られることが知られています。以上のテクニックを用いたAssemble-ResNetの成果は以下の通りです。Assemble-ResNet-152は、EfficientNet B6 + AutAugmentとtop-1 Accuracyで同等の精度ですが、推論速度は5倍高速です。mCEは、画像のノイズに対してどれほど頑健性があるかを測る指標で、ImageNetにノイズを載せたデータセットで計測します。mCEの詳細が気になる方は以前発表した資料をご覧ください。また、導入した手法それぞれが効果を上げていることがわかります。AutoAugmentなしでもEfficientNetB3+AutoAugmentに匹敵する精度で、3倍以上の推論速度になっています。このブログでは、高精度で高速なAssemble-ResNetの解説をしました。Knowledge distillationやAutoAugmentを使っているので、このネットワーク構造さえあれば高精度な予測ができる！というわけではないですが、このように既存の有効な手法を組み合わせることで実務でも高精度なモデルが実装できるのではないかと期待しています。twitter.com",02/02/2020,0,4.0,9.0,1006.0,606.0,22.0,2.0,0.0,2.0,fr
4012,How are Logistic Regression & Ordinary Least Squares Regression (Linear Regression) Related? Why the “Regression” in Logistic?,Towards Data Science,Rakshith Vasudev,878.0,8.0,1238.0,"If you are like me bothered by “regression” in “logistic regression” which realistically should be called “logistic classification”, considering it does classification, I have an answer for your botheration!Logistic regression is useful for situations where there could be an ability to predict the presence or absence of a characteristic or outcome, based on values of a set of predictor variables. It is similar to a linear regression model but is suited to models where the dependent variable is dichotomous. It’s coefficients can be used to estimate odd ratios for each of the independent variables in the model. It is applicable to a broader range of research situations than discriminant analysis. Logistic Regression on the other hand is used to ascertain the probability of an event, this event is captured in binary format, i.e. 0 or 1.Linear Regression aka least square regression estimates the coefficients of the linear equation, involving one or more independent variables, that best predict the value of the dependent variable. For example, it’s possible to predict a salesperson’s total yearly sales (the dependent variable) from independent variables such as age, education, and years of experience.Linear regression is continuous while logistic regression is discrete.More on continuous vs discrete variables here.logit(p) =B0 + B1X1 + B2X2+ ……. + BkXk(0, 1, 2, k are all subscripts for the lack of medium’s ability to subscripting at the time)Y = B0 + B1X1 + B2X2+ ……. + BkXk + ϵ(0, 1, 2, k are all subscripts)where (B0 … Bk) are the regression coefficients, Xs are column vectors for the independent variables and e is a vector of errors of predictionWhere they align together:Linear regression uses the general linear equation Y=b0+∑(biXi)+ϵ where Y is a continuous dependent variable and independent variables Xi are usually continuous (but can also be binary, e.g. when the linear model is used in a t-test) or other discrete domains. ϵ is a term for the variance that is not explained by the model and is usually just called “error”. Individual dependent values denoted by Yj can be solved by modifying the equation a little:Yj=b0+∑(biXij)+ϵj(j, 0, i, j are all subscripts having the same representations as explained)The output of a linear regression looks as follows:Logistic regression is another generalized linear model (GLM) procedure using the same basic formula, but instead of the continuous Y, it is regressing for the probability of a categorical outcome. In simplest form, this means that we’re considering just one outcome variable and two states of that variable- either 0 or 1.The equation for the probability of Y=1 looks like this:P(Y=1)=1/(1+e−^(b0+∑(biXi)))(0,i are all subscripts)where the terms are the same as previously explained.Independent variables Xi can be continuous or binary. The regression coefficients bi can be exponentiated to give the change in odds of Y per change in Xi.The output is a sigmoid curve as follows:Both of them are linear models, however:Logistic regression is emphatically not a classification algorithm on its own. It is only a classification algorithm in combination with a decision rule that makes dichotomous the predicted probabilities of the outcome. Logistic regression is a regression model because it estimates the probability of class membership as a (transformation of a) multi-linear function of the features.This might elude us into asking why is it called “logistic regression”, why not “logistic classification”?To answer this question, we have to go back all the way to 19th century where logistic regression found it’s purpose. It was extensively used to find the growth of the population and the course of auto-catalytic chemical reactions as indicated here.Also to be clear, as some experts point out that the name “logistic regression” was coined way long before any “supervised learning” came along. Additionally, the term “regression” doesn’t mean that the outcomes are always continuous, as pointed out in this paper here. So, not every “regression” is a continuous variable prediction.Linear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically. Logistic regression is just the opposite.FYI: The following is the loss function for linear regression:Using the logistic loss function causes large errors to be penalized to an asymptotic constant.Consider linear regression on a categorical {0,1} outcomes to see why this is a problem. If the model predicts the outcome is 67 when truth is 1, there’s not much loss. Linear regression would try to reduce that 67 while logistic wouldn’t (as much), meaning, using logistic regression on this continuous output wouldn’t explain for more loss. It treats as if the loss is not much at all, in other words, logistic regression doesn’t punish for the loss which makes the “line of best fit” not the “best fit” at all.Logistic regression results will be comparable to those of least square regression in many respects, but gives more accurate predictions of probabilities on the dependent outcome. Least square regression is accurate in predicting continuous values from dependent variables.Also as discovered above, it is important to know that “regression” is an abstract term. It has different interpretations depending on the context.Here is a comparative image:Linear Regression: Continuous values [2 or more outputs].Logistic Regression: Discrete values. Usually 2 outputs{0,1}. The outputs are derived after rounding off to the nearest value either 0 or 1. Remember, Multi-classes are allowed.Linear Regression: linear Regression coefficients represent the mean change in the response variable for one unit of change in the predictor variable while holding other predictors in the model constant. In other words, holding all other variables constant, with a unit increase in this variable, the dependent variable is expected to increase or decrease by some value X.Logistic Regression: Interpreting logistic regression co-efficients require the interpretation of odds which in itself is another topic. However, there’s an intuitive explanation for that here.Linear Regression: Linear regression is a way to model the relationship between two variables. You might also recognize the equation as the slope formula. The equation has the form Y=a+bX, where Y is the dependent variable (that’s the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept.There’s always an error term aka residual term ϵ as shown:Logistic Regression: Logistic regression uses an equation as a representation, very much like the linear regression. Input values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter, beta) to predict an output value (y). A key difference from the linear regression is that the output value being modeled is a binary value (0 or 1), rather than a numeric value (from Safari Books Online).Logistic Regression: Logistic regression’s outputs are probabilities which later gets classified into classes. The bottom line is, you can’t use logistic regression to do linear regression as seen before.The usual cost or loss function aka error equation for logistic regression is called the “categorical cross entropy” as seen with neural networks.However, since this is a classification the usual metrics from sklearn for classification are here.Linear regression: needs a linear relationship between the dependent and independent variables.Logistic regression: does not need a linear relationship between the dependent and independent variables.Linear regression: requires error term to be normally distributed.Logistic regression: does not require error term to be normally distributed.Now that we know what is the relationship between Linear and Logistic regression.If you liked this article, then clap it up! :) Maybe a follow?Connect with me on Social:www.linkedin.comwww.facebook.comwww.youtube.comhttp://papers.tinbergen.nl/02119.pdfhttps://pdfs.semanticscholar.org/5a20/ff2760311af589617ba1b82192aa42de4e08.pdfhttps://stats.stackexchange.com/questions/29325/what-is-the-difference-between-linear-regression-and-logistic-regressionhttps://stats.stackexchange.com/questions/24904/least-squares-logistic-regressionhttp://www.statisticssolutions.com/what-is-logistic-regression/https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression",05/06/2018,0,25.0,4.0,893.0,444.0,10.0,3.0,0.0,26.0,en
4013,Meta-Learning: Learning to Learn,DataThings,Thomas HARTMANN,22.0,7.0,1458.0,"Although artificial intelligence and machine learning are currently extremely fashionable, applying machine learning on real-life problems remains very challenging. Data scientists need to evaluate various learning algorithms and tune their numerous parameters, based on their assumptions and experience, against concrete problems and training data sets. This is a long, tedious, and resource expensive task. Meta-learning is a recent technique to overcome, i.e. automate this problem. Meta-learning aims at using machine learning itself to automatically learn the most appropriate algorithms and parameters for a machine learning algorithm.Artificial intelligence and machine learning are currently extremely fashionable. In recent years, this technology has left the realm of research and universities behind and is getting more and more attention from industries, governments, and public bodies. Moreover, it is increasingly subject of public debates. In recent years it has been shown that advanced machine learning algorithms, like neural networks, have the potential to be successfully applied to many domains, e.g. machine translation, image recognition, self-driving cars, and automation.  Driven by emerging technologies, like the Internet of Things (IoT), cyber-physical systems (CPS), and the so-called Industry 4.0, today more data than ever is produced and collected across many different industries. Often measured by sensors, this data captures the behaviour of a system. By detecting patterns, valuable insights and future predictions can be derived from the collected data. Therefore, it is no surprise that more and more companies, aside from the big players, start to explore how artificial intelligence and machine learning can be an asset to their businesses and how the collected data can be used to create value.Unfortunately, without deep machine learning and mathematical expertise it is very difficult or even impossible to build suitable learning models — not to mention implementing ones own algorithms. The choice of different algorithms and their parameters are numerous and it is extremely difficult to understand their impacts. When to apply restricted Boltzmann machines, long short-term memory neural networks, deep believe networks, convolutional neural networks, auto-encoders, random forests, k-nearest neighbours? Or perhaps a Gaussian mixture model performs better for this specific case? How to choose the number of layers in a neural network, how to determine the best value for k in k-nearest neighbours? What is a ReLU or Pooling layer? What is a vanishing gradient and why it matters?While in theory, machine learning models should be applicable like black boxes, i.e. should be usable without knowing the internals, just like we don’t have to know all the internals of how a motor works to drive a car. In practice, data scientists need to evaluate various learning algorithms and tune their numerous parameters, based on their assumptions, against concrete problems and training data sets. This is known as inductive bias or learning bias and is usually a long, tedious, and expensive task. Currently, machine learning is very hard to use with its thousands knobs and parameters and most of the time default settings don’t work. It is like we have to configure thousands of parameters on our motors before we can drive a car. In addition, the field is moving extremely fast so that nearly every day new algorithms, refinements to existing ones and new findings are proposed. Today, often only big tech companies have the resources (know-how, data, and budget) to follow this and to successfully apply machine learning & AI to large-scale projects.But even if a model is once successfully created and trained, the challenges do not stop here. While the choices made might be satisfactory today, they can be wrong tomorrow, when the conditions change. Learning algorithms are designed to converge, which naturally creates a resistance to learn changes and adapt to new situations. This is extremely challenging for domains like IoT and Industry 4.0, which need to handle continuously evolving data.The question is how this situation can be improved? Machine learning algorithms have been created to learn from data without being explicitly programmed. Recently, research from different universities and big tech companies, like Google, started to investigate if machine learning algorithms could be used to actually learn how to learn, i.e. which algorithms and which parameters would be the most appropriate ones for a given problem at hand. This process is referred to as meta-learning and gets currently much attention. With AutoML Google rolled out a commercial platform, others, like Auto-Keras are open source and free to use. While current offerings are more experimental and in a very early stage, they provide an interesting glance into what could be the future of machine learning.The current work on meta-learning focuses around neural networks and deep learning. Neural networks are powerful, generic, and versatile tools to learn relationships between inputs and outputs. They can be used to solve a wide variety of problems, from regression to classification, to computer vision. They are suitable for both supervised and unsupervised learning. However, this versatility and generic problem solving capability comes at a high price. There are lots of possible configurations to build a neural network. In fact, neural networks maybe show most clearly how challenging it is to find the right algorithms and configuration parameters to train a model. This can be so confusing and unclear that it is sometimes referred to as “the neural zoo”. Manually designing a neural network is difficult because the search space of all possible networks can be combinatorially large. Therefore, it is no surprise that today’s efforts around meta-learning focus on this type of learning, although it is not limited to it. Meta-learning can help to reduce the burden of finding the best configuration of a neural network for a specific problem using machine learning for this process itself. This is known as neural architecture search.Different approaches of neural architecture search have been suggested and it has been shown that they can compare well with or even outperform the best hand-designed neural networks, both in terms of accuracy and performance.The basic neural architecture search algorithm works as follows: it first proposes a candidate model, then it evaluates this candidate model against a data set, and finally uses the results as a feedback to teach the neural architecture search network.Different variants of this algorithm have been purposed. For example, Google suggests a neural architecture search with reinforcement learning. Their algorithm uses a recurrent neural network to generate the model descriptions of neural networks and train this recurrent neural network with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. Other approaches use ensemble learning. Neural architecture search algorithms largely rely on vast computational resources to optimize the accuracy of the network. To address this problem, Baidu suggested a multi-objective neural architecture search that uses a multi-objective reward function that takes network accuracy, computational resources, and training time into consideration. And it is safe to say that this is just the beginning. Over time, we will see more and more clever algorithms finding more and more accurate neural network architectures for specific problems in ever decreasing times.The meta-learning level can be viewed from different angles, either as a hyper-parameter search or as an ensemble method, where a strong model out of several weak ones can be created. This ensemble can contain different neural networks with different parameters, but could also contain completely different machine learning algorithms, not just neural networks. Ultimately, this can allow a system to automatically learn the best learning strategy but also to adapt its own learning strategy to changing conditions.The potential of meta-learning is huge. While a far-off future promises truly intelligent, self-learning, and self-adaptive systems, which pushes the limits of automation to a whole new level, in the near future meta-learning can provide deep learning tools to domain experts with limited data science or machine learning background. The training of high-quality custom machine learning models can become a lot more accessible. This will enable machine learning to entirely new domains and businesses.As the domain evolves, we should transition towards a situation where machine learning can actually be used as a black box, without requiring deep computer science expertise. Besides the complexity of meta-learning, the required computational resources are still a major challenge. It is often already very costly to train a single neural network, not to mention to train and evaluate a complete ensemble. However, with the constantly improving computational power, dedicated machine learning hardware like Tensor Processing Units, and advances made in meta-learning algorithms, this will become better and better with time. While it will probably still take some time until we can see the fruits of meta-learning, this technology has the potential to be the next big thing in machine learning & AI.This work is supported by the Luxembourg National Research Fund, FNR (https://www.fnr.lu/), under the Industrial Fellowship program (grant 12490856).If you enjoyed reading, follow us on: Facebook, Twitter, LinkedIn",06/02/2019,0,11.0,6.0,1447.0,1184.0,4.0,0.0,0.0,12.0,en
4014,Training Custom NER,Towards Data Science,Nishanth N,17.0,3.0,343.0,"The article explains what is spacy, advantages of spacy, and how to get the named entity recognition using spacy. Now, all is to train your training data to identify the custom entity from the text.SpaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. As of version 1.0, spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like Tensor Flow, PyTorch, or MXNet through its machine learning library Thinc.While NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it. It provides the fastest and most accurate syntactic analysis of any NLP library released to date. It also offers access to larger word vectors that are easier to customize.After installation, you need to download a language modelYou can start the training once you completed the first step.→ Initially, import the necessary packages required for the custom creation process.→ Now, the major part is to create your custom entity data for the input text where the named entity is to be identified by the model during the testing period.→ Define the variables required for the training model to be processed.→ Next, load a blank model for the process to carry out the NER action and set up the pipeline with only NER using create_pipe function.→ Here, we want to train the recognizer by disabling the unnecessary pipeline except for NER. The nlp_update function can be used to train the recognizer.Output: Training losses→ To test the trained model,→ Finally, save the model to your path which stored in the output_dir variable.The full source code available on GitHub.I hope you have now understood how to train your own NER model on top of the spaCy NER model. Thanks for reading!",24/07/2020,1,0.0,18.0,1050.0,667.0,2.0,0.0,0.0,13.0,en
4015,ML入門（十）Gradient Descent,程式設計之旅,Chung-Yi,164.0,5.0,35.0,"簡單回顧在ML入門（五）Linear Regression有介紹什麼是Gradient Descent，就是對loss function做偏微分（切線斜率）就是找極大極小值的概念，找一組參數讓loss function越小越好。在ML入門（五）Linear Regression，我們要更新的是w, b，在這邊用一個theta表示。Gradient Descent如何運行這邊可以搭配公式一起看，紅色箭頭就是loss function的gradient方向，當乘上learning rate後再乘上負號（改變方向）就會變成藍色箭頭，一直重複這樣的動作，這就是Gradient Descent的運行模式。Learning Rate對 Loss Function的影響調整learning rate的方法既然learning rate有時候不是太大不然就是太小，是不是有什麼方法可以來讓機器自己調整。當一開始起始點離最低點還很遠的時候，learning rate可以大一點；當越來越接近最低點時，learning rate要小一點，這樣才能收斂在最低點附近。下面那張圖所示，假設定義learning rate是每次跟著更新次數做調整，也就是說你的更新次數越多，learning rate會跟次數的開根號成反比，learning rate會越小。那有人就覺得說可以根據不同參數調整不同的learning rate，以下會列出幾種方法：現在對於每一個參數w都要給一個不同的𝜂，就是每次更新的𝜂就是等於前一次的𝜂再除以𝜎^t，而 σ^t則代表的是第 t 次以前的所有梯度更新值之平方和開根號(root mean square)，而𝜖只是為了不讓分母為0而加上去的值。下面圖中的式子可以清楚看出，分子的部分（紅色框框）顯示，當gradient越大，參數update越多；但分母則相反，分母的部分（藍色框框）顯示，當gradient越大，參數update越少。假設要選一個起始點當作參數更新的最初點，最佳的起始點就如下圖所示的x0（因為只要踏一步就能到最低點，就是對y做一次微分），但這只是在一維函式有用。為什麼我會這樣說呢？如第二張圖所示，比較a, c兩點，a的一次微分 < c 的一次微分，但是a卻是距離最低點是比較遠的。所以為了解決這個問題，必須要除以2a，就是y的二次微分，如此才能在多個參數時，真正比較與最低點的位置。之前有提過，如果要最小化loss function，當要從整個data set計算出梯度後，要朝著下降的方向前進，函數才會收斂。假設有一個data set非常大，那每次在執行梯度下降時，是非常消耗時間的事情，因為要朝著global minimun前進，就需要對整個data計算一次。而隨機梯度下降法，可以被視為一種近似梯度下降的方法，通常會更快達到收斂。因為它只需考慮一個example，只對一個example的loss做計算。就像如果也看過棋靈王就會知道，假設今天有20筆資料，用梯度下降法去下圍棋，你必須考慮完這20筆資料才開始下第一步，但如果你用隨機梯度下降法，你已經下了二十步了。",22/09/2019,0,6.0,0.0,1400.0,910.0,9.0,4.0,0.0,2.0,de
4016,"Why I dropped out of college, but you shouldn’t",Medium,João Fernandes,126.0,5.0,1015.0,"This article’s title is even a surprise to me. This is not something that I expected to write and you’re probably wondering what happened to all that advice about “you grow your wings on your way down”. I know, but this is the kind of theme that creates a lot of fuss by itself and a lot of irresponsible advice is given.Here I will clarify my position on pursuing an academic career and how is the life of a college drop out.I’ve always been a decent student, I always knew that I could be one of the top students in the class, but I never felt like going after that status. Video games always seemed more interesting than boring themes with zero practical implication. So school never presented itself as a challenge when it came to studying. Even in college I pass at every subject on the first try with above average grades.My method was simple: being as lazy as possible for as long as possible and one week before an exame I would lock myself in the bedroom and I would devour those books. It always worked.I always found classes utterly boring. I never had that teacher who motivated me to be better, I never learned anything essential in school or college that I couldn’t have learned by myself. I was the smart ass student whose teacher’s either hated or loved. I had no problem correcting a teacher in front of the entire class and if some valued my ballsy approach many saw it as a direct attack on their authority.Now I can understand it, I could be pretty much of an asshole.The motivation has never been there and it all got worse with the first years of college. I was taking a degree in History and my one week before method was still working. I passed the time in classes writing fiction or just skipping classes and smoking weed in the campus gardens. But the worst part, the one which made me really disdain the academic world was the arrogance, self-entitlement and stupid way that teachers addressed us, students. They were the elite, we were the peasants.But I can’t thank enough a fat, round and way too annoying teacher that I had on my first Masters semester. She decided that I would be the example, she decided that no self thinking was allowed, she pushed me to the edge in every single class… One day, more precisely a Sunday afternoon I decided to quit.“Fuck you!”, I said out loud in my bedroom. I felt good.For every Steve Jobs, Bill Gates and Mark Zuckerberg there are thousands of guys working in gas stations, being waiters and working on Wendy’s handing greasy fast food day after day.To simply say I am going to quit and I am going to be an entrepreneur is not only ridiculous, but also dangerous. I did it but I knew what I would do. I had a set of skills that allowed me to do other things besides waving a fucking paper in companies offices.Really ask yourself: what value do I bring to the table? Would anyone hire me or pay me for something that I know how to do?If the answer is no, you’re fucked. Seriously. I can’t conceive how some twenty something year old people complain about being unemployed and still they don’t have a clue about value exchange or think that it’s normal to not have a skill set. That’s nonsense. One friend asked me if I could hire him as my personal assistant. But even that I wanted he had no idea how to work on Wordpress, schedule social media posts, review articles, do some in depth research, had no idea how to send a proper email or how to handle costumer service. (By the way, I am still looking for a P.A. Email me on info at adventurousartist dot com if you’re interested)What will I do with someone like that?I dropped out but I was writing every single day for 8 years. I had experience with online writing. I was establishing myself as an authority in the self development field. I was charging 300$ for a 8 hours seminar. One month after I left college I closed a 1k deal that allowed me to move to Budapest for 2 months.Those were very difficult times, I kid you not, but I had an idea of what I would do next. Even that everyone around me thought that I was being stupid and a fool.Because it’s easier. Because nobody will complain if you spend 5 years on a useless degree, because that is part of the beaten path.The illusion of formal education is one of the greatest illusions of our time and if you think that college is difficult you have no idea where your life is heading towards to. Sometimes I get delusion and wish that I only had to wake up, attend some classes that I don’t give a fuck about and come back home with a sense of fulfilled duty. When you are on your own… that doesn’t exist. Sorry, welcome to real life.The pain and confusion are even greater if you are a solopreneur, like I am. The amount of days by yourself, the self motivation, the love for your craft that vanishes in the days that you needed it the most, the obstacles and lack of help. This will make you grow, this will make you shed your old skin but… It isn’t easy.The worst part is that you won’t have a safety net below you, that money will be short sometimes, that people around you will be more critical than supportive because “Who do you think you are? You’re no Richard Branson.”Formal education is dangerous because you are risking having no skill set. You’re trading your comfort for evolution and if one day you end up falling from your very narrow rope you will understand that the safety net has never been there, that it was all an illusion.Like Seth Godin says: “The Recession is here forever.” You have been warned.",26/10/2015,0,0.0,4.0,425.0,282.0,1.0,0.0,0.0,3.0,en
4017,Automatic Extractive Text Summarization using TF-IDF,Voice Tech Podcast,ASHNA JAIN,39.0,10.0,973.0,"In the recent years, information grows rapidly along with the development of social media. With the increasing amount of information, it takes more effort and time to review the entire text document and understand its contents. One possible solution to the above problem is to read the summary of the document. The summary will not only retain the essence of the document, but will also save a lot of time and effort. An effective summary of the document will concise and fluent while preserving key information and overall meaning.There are two major text summarization approaches, abstractive and extractive summarization. The approach of Abstractive summarization selects words on the basis of semantic understanding, and even includes those words which do not appear in the original text. On the other hand, extractive summarization extracts the most important and meaningful sentences from the text document and forms a summary.Extractive summarization works as follows:Input document -> Finding most important words from the document -> Finding sentence scores on the basis of important words ->Choosing the most important sentences on the basis of scores obtained.As shown in the figure above, this approach considers only nouns and verbs to compute the sentence score, as it gives greater accuracy and improves the speed of the algorithm.TFIDF, short for term frequency–inverse document frequency, is a numeric measure that is use to score the importance of a word in a document based on how often did it appear in that document and a given collection of documents. The intuition behind this measure is : If a word appears frequently in a document, then it should be important and we should give that word a high score. But if a word appears in too many other documents, it’s probably not a unique identifier, therefore we should assign a lower score to that word.Formula for calculating tf and idf:Hence tfidf for a word can be calculated as:TFIDF(w) = TF(w) * IDF(w)Retention rate given by the user : 56%Can’t wait to start coding…..Let’s get started….The most important library for working with text in python is NLTK. It stands for Natural Language toolkit. It contains methods such as sent_tokenize, word_tokenize in the corpus package, which can split the text into sentences and words respectively. Stem package of NLTK contains methods for lemmatization, namely WordNetLemmatizer . Stopwords contains list of english stop words, which needs to be removed during preprocessing step.Text pre-processing is the most crucial step in getting consistent and good analytical result. The pre-processing steps applied in this algorithm include, removing special characters, digits and one letter word and stop words from the text .The first step includes, reading text from a file. Here we store the contents of the file in variable text. After reading the contents, remove_special_characters function removes special characters from the text. It is important to remove digits from the document, which can be done using regular expression. After eliminating special character and digits, the individual words can be tokenized and one letter word, stop words can be removed.To avoid any ambiguity in case, we lowercase all the tokenized words. The remove special characters function is as follows:Step-3 Calculating the frequency of each word in the documentWhile working with text it becomes important to calculate the frequency of words, to find the most common or least common words based on the requirement of the algorithm.Here we take the list of words as input and append all the unique words in a new list.The unique words are stored in words_unique list. After finding the unique words, the frequency of the word can be found using count function.Step-4 Calculating sentence scoreAs the score given to each sentence decides the importance of the sentence, it becomes extremely important to choose the correct algorithm to find the score. In this approach, we will be using TFIDF score of each word to calculate the total sentence score.The score of each sentence can be calculated using sentence_importance function. It involves POS tagging of words in the sentence by pos_tagging function.This function returns only the noun and verb tagged words. The returned words from pos_tagging function are sent to word_tfidf function to calculate the score of that word in the document by calculating its tfidf score.Let’s explore all the functions one by one..This function uses nltk library to pos tag all the words in the text and returns only the nouns and verbs from the text.Build better voice apps. Get more articles & interviews from voice technology experts at voicetechpodcast.com2. Word tfidf functionThe above function calls tf_score, idf_score and tf_idf function.tf_score calculates the tf score, idf_score calculates the idf score and tf_idf_score calculates the tfidf score. It returns the tfidf score.3. tf score functionThis function calculates the tf score of a word. tf is calculated as the number of times the word appears in the sentence upon the total number of words in the sentence.4. idf score functionThis function finds the idf score of the word, by dividing the total number of sentences by number of sentences containing the word and then taking a log10 of that value.5. tfidf score functionThis function returns the tfidf score, by simply multiplying the tf and idf values.Step 5 Finding most important sentencesTo find the most important sentences, take the individual sentences from tokenized sentences and compute the sentence score. After calculating the scores, the top sentences based on the retention rate provided by the user are included in the summary.Here number of sentences are calculated as follows:Number of sentences to include in the summary, takes into consideration the retention rate provided by the user.The complete code is given as follows:Here input.txt is the file containing the document to be summarized and summary.txt stores the summary of the input file.If this article helped you, please like and share with others. Feel free to write suggestions as well in the comments below!Thank You",01/04/2019,15,23.0,1.0,1089.0,597.0,2.0,2.0,0.0,1.0,en
4018,Top JavaScript Frameworks and Tech Trends for 2021,JavaScript Scene,Eric Elliott,108000.0,11.0,2019.0,"Happy New Year! It’s time to review the big trends in JavaScript and technology in 2020 and consider our momentum going into 2021.Our aim is to highlight the learning topics and technologies with the highest potential job ROI. This is not about which ones are best, but which ones have the most potential to land you (or keep you in) a great job in 2021. We’ll also look at some larger tech trends towards the end.JavaScript still reigns supreme on GitHub and Stack Overflow. Tip #1: Learn JavaScript, and in particular, learn functional programming in JavaScript. Most of JavaScript’s top frameworks, including React, Redux, Lodash, and Ramda, are grounded in functional programming concepts.TypeScript jumped past PHP, and C# into 4th place, behind only Java, Python, and JavaScript. Python climbed past Java for 2nd place, perhaps on the strength of the rapidly climbing interest in AI and the PyTorch library for GPU-accelerated dynamic, deep neural networks, which makes experimentation with network structures easier and faster.JavaScript is also #1 on Stack Overflow for the 8th year in a row. Python, Java, C#, PHP, and TypeScript beat out languages like C++, C, Go, Kotlin, and Ruby.When it comes to front-end frameworks, a large majority of JavaScript developers use React, Vue.js, or Angular. jQuery still makes a surprisingly large showing, almost double the Vue.js showings, but it’s my guess that jQuery is used less in application work, and more in content sites and WordPress templates, so we’re going to exclude it this year.React dominates search volume at 57.5%, with Angular collecting a large 31.5% share, and Vue.js picking up a respectable 11% slice.*Methodology: All search trends were selected by topic rather than by keyword to exclude false positives.If you want to learn the framework that will give you the best odds of landing a job in 2021, your best bet is still React, and has been since 2017. React is mentioned in 47.6% of the listings which mention a common front-end framework, Angular picks up 41.2%, and Vue.js trails at 11.2%.It’s important to mention that most job listings say that they require experience with one of a few named frameworks, but a large share of those listings are actually hiring for React work when you look at their listed tech stack, and will show preference to candidates with a strong knowledge of React. You’ll see some supporting evidence of that in the download trends, below.*Methodology: Job searches were conducted on Indeed.com. To weed out false positives, I paired searches with the keyword “software” to strengthen the chance of relevance. I also omitted the “.js” from “Vue.js” because many listings don’t include the “.js”. All SERPS were sorted by date and spot checked for relevance.The npm download counts look fairly similar to the search trends, but reveal something interesting: The number of downloads for Angular 2+ and Vue.js are pretty much neck-and-neck, but if you add in the number of people using the old Angular framework, Angular has a solid lead over Vue.js in downloads.If we look at recent download shares on a pie chart, it shows React at ~66%, Angular (all versions) at ~20%, and Vue at ~15%.10.6% of employers specifically mention TypeScript in job listings, up from 7.4% last year.Developer interest in TypeScript is undeniably strong, and growing rapidly. I predict that this trend will continue in 2021, and users will learn to work around some of the costs of using TypeScript (for example, by favoring interfaces over inline type annotations).The number of jobs that specifically mention TypeScript is still relatively small, but some experience with TypeScript will slightly increase your odds of landing a job in 2021. By 2022, some experience with TypeScript might give you an edge in the job market. However, because it’s easier for a JavaScript developer to learn TypeScript than a completely new language, TypeScript teams are usually willing to hire and train good JavaScript developers.On the server side, Express still dominates in download counts, so much so that it’s difficult to see how popular contenders are doing relative to each other.As I predicted last year, excluding express, we see that Next.js has emerged as the top contender, which is unsurprising because Next.js is a flexible, full-stack, React-based framework which can help you deliver statically optimized content, but can also fall-back on serverless functions for API routes and SSR when you need to generate content dynamically. You can even statically generate content on-demand the first time it’s requested, and subsequently serve cached static content served from CDN — useful for apps based on user-generated content.Next has many other advantages, including automatic optimization of page bundles, automatic image optimization with the new Image tag and built-in performance analytics to help you improve your user’s page load experience.If you use GitHub and deploy on Vercel, you’ll also get automatic deploys for every PR, and a buttery smooth CI/CD pipeline. Essentially, it’s like having the best full-time DevOps team on staff, but instead of paying them salaries, you save a significant amount of money in hosting bills.Expect Next.js to continue to explode in 2021.In 2020, teams were forced to learn to collaborate remotely by a global pandemic. In 2021, remote work will continue to be an important topic. First, because it will probably be June before vaccination against COVID-19 is widespread, and second, because a lot of teams experienced increased productivity and reduced costs during lockdown, many employees will not return to offices in 2021.Remote work has also led to more location freedom, prompting developers to move to places where they have access to things that are important to them, such as family and more affordable housing. Additionally, 72% of employers surveyed by KPMG said that remote work has widened their potential talent pool.Remote-first and hybrid-remote teams will be the new normal in the new decade.Average JavaScript Developer salaries dipped slightly in 2020, from $114k/year to $113k/year, according to Indeed, perhaps due in part to remote work expanding the employee pool beyond tech centers like San Francisco and New York, which tend to have a much higher cost of living, and demand higher salaries to compensate. The average JavaScript Developer salary in San Francisco is $130k.Still, lots of companies with roots in San Francisco and other tech centers are paying remote workers somewhere between the US national average and San Francisco pay, which provides a premium on market rates to attract better talent, and still saves money over hiring locally and paying for office space.Because of this trend, lots of remote jobs exist in the $115k — $130k range for mid-level developers. Senior developers often find jobs in the $120k — $150k range, regardless of location.GitHub data suggests that rather than slowing down, teams were more productive working remotely in 2020. GitHub activity spiked when lockdowns began.Volume of work on GitHub increased substantially, and average pull request merge times dropped by 7.5 hours.Toss that onto the growing pile of evidence that remote work works.Passwords are obsolete, insecure technology and absolutely should not be used to protect your users or your app in 2021.The crux of the matter is that about half of all users reuse passwords on multiple applications and websites, and attackers are financially incentivized to bring massive computing power to the problem of cracking your user’s passwords so they can try them on bank accounts, Amazon, etc.If you’re not Google, Microsoft, or Amazon, chances are you can’t afford the computing power required to defend against modern password crackers. Don’t believe me? Check out HaveIBeenPwned. Spoiler: If you’ve used the internet, your passwords have been stolen.I’ve been warning about the dangers of passwords for years, but in 2020, new options emerged which allow us to leave passwords behind, permanently. It was true in 2020, and it remains true: No new app should use passwords in 2021.But once you leave passwords behind in exchange for cryptographic key pairs, your app also gains Web3 superpowers. Which leads me to the next topic: Crypto.Crypto will continue to be one of the most important and globally transformational technologies in 2021. Here are some highlights from 2020:2020 was a seminal year for AI. Via the GPT-3 launch, we learned that language models and transformers in general may be a viable path towards Artificial General Intelligence (AGI).The human mind’s ability to generally solve a wide variety of problems by relating them to things we already know is known in AI circles as zero-shot and few-shot learning. We don’t need a lot of instruction or examples to take on tasks that are new to us. We can often figure out new kinds of problems with just a few (or no) examples (shots).That general applicability of human cognitive skills is known as general intelligence. In AI, Artificial General Intelligence (AGI) is “the hypothetical intelligence of a machine that has the capacity to understand or learn any intellectual task that a human being can.”GPT-3 demonstrated that it could teach itself math, how to code, how to translate text, and a virtually infinite variety of other skills via its gigantic training set which includes basically the whole public web (Common Crawl, WebText2, Books1, Books2, and Wikipedia), combined with its enormous model size. GPT-3 uses 175 billion parameters. For context, that’s an order of magnitude (10x) the previous state of the art, but still orders of magnitude smaller than the human brain.Scaling up GPT-3 is likely to lead to even more breakthroughs in what it is capable of.In October 2020, Waymo began offering fully driverless rides (with no human in the driver seat) on 100% of their rides. At the time of launch, there were 1500 monthly active users and hundreds of cars serving the Phoenix metro area.In December, 2020, General Motors’ Cruise launched fully driverless rides on the streets of San Francisco.UPS launched 2 drone trials in 2020. One to deliver prescriptions to a retirement community in Florida, and another to deliver medical supplies including Personal Protective Equipment (PPE) between health care facilities in North Carolina.Regulations, safety, noise, and technical challenges will likely continue to mean slow growth for Drone delivery services in 2021, but with continued COVID restrictions that will likely continue off and on through at least June, there has never been a better time to make quick progress on more efficient and contactless delivery.Researchers in China have reported that they have achieved quantum supremacy that is 10 billion times faster than the quantum supremacy reported by Google last year. Researchers are making rapid progress, but quantum computing still requires extremely expensive hardware, and there are only a small handful of quantum computers in the world that have achieved any kind of quantum superiority.Quantum-resistant cryptography, quantum-assisted cryptography, and quantum computing for machine learning are potential areas of focus where breakthroughs would have a significant industry-spanning, global impact. I believe that one day, the application of quantum computing in the field of AI will propel the technology forward many orders of magnitude — a feat that will have a profound impact on the human race.In my opinion, that is unlikely to happen in the 2020s, but I expect to hear more quantum supremacy announcements in 2021, and perhaps breakthroughs in the variety of algorithms state of the art quantum computers can compute. We may also see more practical quantum-computing APIs services and use-cases.Composing Software will teach you the foundations of functional programming in JavaScript. You can get the Composing Software e-book, print edition, or the blog post series that started it all.Learn React, Redux, Next.js, TDD and more on EricElliottJS.com. Access a treasure trove of video lessons and interactive code exercises for members.1:1 Mentorship is hands down, the best way to learn software development. DevAnywhere.io provides lessons on functional programming, React, Redux, and more, guided by an experienced mentor using detailed curriculum designed by Eric Elliott.Eric Elliott is a tech product and platform advisor, author of “Composing Software”, cofounder of EricElliottJS.com and DevAnywhere.io, and dev team mentor. He has contributed to software experiences for Adobe Systems, Zumba Fitness, The Wall Street Journal, ESPN, BBC, and top recording artists including Usher, Frank Ocean, Metallica, and many more.He enjoys a remote lifestyle with the most beautiful woman in the world.",31/12/2020,0,22.0,27.0,919.0,431.0,11.0,1.0,0.0,51.0,en
4019,Building Seq2Seq LSTM with Luong Attention in Keras for Time Series Forecasting,Level Up Coding,Huangwei Wieniawska,75.0,10.0,1173.0,"Do you want to try some other methods to solve your forecasting problem rather than traditional regression? There are many neural network architectures, which are frequently applied in NLP field, can be used for time series as well. In this article, we are going to build two Seq2Seq Models in Keras, the simple Seq2Seq LSTM Model, and the Seq2Seq LSTM Model with Luong Attention, and compare their forecasting accuracy.First of all, let’s create some time series data.We’ve just created two sequences, x1 and x2, by combining sin waves, trend, and random noise. Next we will preprocess x1 and x2.Since the sequence length is n_ = 1000, the first 800 data points will be used as our train data, while the rest will be used as our test data.It is not a must to detrend time series. However stationary time series will make model training much easier. There are many ways to detrend time series, such as taking difference of sequence with its lag1. Here for the simplicity, we assume the order of trend is known and we are just going to simply fit separate trend lines to x1 and x2, and then subtract the trend from the corresponding original sequence.We will create index number of each position in the sequence, for easier detrending and trend recover.Here we will use np.polyfit to complete this small task. Note that only the first 800 data points are used to fit the trend lines, this is because we want to avoid data leak.Based on the above values we got, we can now come up with the trend lines for x1 and x2.Let’s plot the trend lines together with x1 and x2, and check if they look good.The above result looks fine, now we can deduct the trend.After removing the trend, x1 and x2 become stationary.For easier preprocessing in next several steps, we can combine the sequences and their relevant information together into one array.In the combined array we created x_lbl:Normalisation can help model avoid favouring large features while ignoring very small features. Here we can just simply normalise x1 and x2 by dividing the corresponding maximum values in train set.Note that the above code only calculates maximum value of column 1 (detrended x1) and column 2 (detrended x2), the denominator of column 3 (index) and column 4 (label) are set to 1. This is because we do not input column 3 and column 4 into neural network, and hence no need to normalise them.After normalisation, all the values are more or less within range from -1 to 1.Next, we will cut sequence into smaller pieces by sliding an input window (length = 200 time steps) and an output window (length = 20 time steps), and put these samples in 3d numpy arrays.The function truncate generates 3 arrays:Now the data is ready to be input into neural network!The above figure represents unfolded single layer of Seq2Seq LSTM model:Set number of hidden layers:The input layerThe encoder LSTMWe need to pay attention to 2 import parameters return_sequences and return_state, because they decide what LSTM returns.For simple Seq2Seq model, we only need last state_h and last state_c.Batch normalisation is added because we want to avoid gradient explosion caused by the activation function ELU in the encoder.Next, we make 20 copies of the last hidden state of encoder and use them as input to the decoder. The last cell state and the last hidden state of the encoder are also used as the initial states of decoder.Then we put everything into the model, and compile it. Here we simply use MSE as the loss function and MAE as the evaluation metric. Note that we set clipnorm=1 for Adam optimiser. This is to normalise the gradient, so as to avoid gradient explosion during back propagation.We can also plot the model:The next step is training:The model prediction as well as the true values are unnormalised:Then we combine the unnormalised outputs with their corresponding index, so that we can recover the trend.Next, we put all the outputs with recovered trend into a dictionary data_final.Just a quick check to see if the prediction value distribution is reasonable:The data distribution of prediction and true values are almost overlapped, so we are good.We can also plot MAE of all samples in time order, to see if there is clear pattern. The ideal situation is when line is random, otherwise it may indicate that the model is not sufficiently trained.Based on the above plots, we can say that there are still certain periodical pattens in both train and test MAE. Training for more epochs may lead to better results.Next we are going to check some random samples and see if the predicted lines and corresponding true lines are aligned.We can also check the nth prediction of each time step:Take a closer look at the prediction on test set:One of the limitations of simple Seq2Seq model is: only the last state of encoder RNN is used as input to decoder RNN. If the sequence is very long, the encoder will tend to have much weaker memory about earlier time steps. Attention mechanism can solve this problem. An attention layer is going to assign proper weight to each hidden state output from encoder, and map them to output sequence.Next we will build Luong Attention on top of Model 1, and use Dot method to calculate alignment score.The Input layerIt is the same as in Model 1:The encoder LSTMThis is slightly different from Model 1: besides returning the last hidden state and the last cell state, we also need to return the stacked hidden states for alignment score calculation.Next, we apply batch normalisation to avoid gradient explosion.The Decoder LSTMNext, we repeat the last hidden state of encoder 20 times, and use them as input to decoder LSTM.We also need to get the stacked hidden state of de decoder for alignment score calculation.Attention LayerTo build the attention layer, the first thing to do is to calculate the alignment score, and apply softmax activation function over it:Then we can calculate the context vector, and also apply batch normalisation on top of it:Now we concat the context vector and stacked hidden states of decoder, and use it as input to the last dense layer.Then we can compile the model. The parameters are the same as those in Model 1, for the sake of comparing the performance of the 2 models.How data flow through the model:The training and evaluation process is the same as illustrated in Model 1. After training for 100 epochs (the same number of training epochs as Model 1), we can evaluate the result.Below are the plots of sample MAE vs. sample order for train set and test set. Again, the model is not sufficiently trained since we can still see some periodical pattern. But for easier comparison of the 2 models, we are not going to train it more for now. Note that the overall MAE of both train set and test set are slightly improved compare to Model 1.After adding the attention layer:Random sampling to check predicted line vs. true line:",25/06/2020,0,59.0,65.0,1136.0,262.0,50.0,5.0,0.0,1.0,en
4020,Finger Dasher,Banapana,R. E. Warner,86.0,1.0,69.0,"Dasher is a novel piece of software that lets you point at what you want to write. Honestly, it’s kind of difficult to describe without [seeing the demonstration](http://www.youtube.com/watch?v=0d6yIquOKQ0). It’s very novel and makes novel use of some simple AI. I wonder if Apple would ever integrate this in to the iPhone? And it would seem to be of great use were it to be integrated into eye tracking software.",05/01/2008,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,en
4021,A news-analysis NeuralNet learns from a language NeuralNet,Becoming Human: Artificial Intelligence Magazine,m.zaradzki,208.0,7.0,1176.0,"Python notebook, using Keras library, available on this GitHub repo.A common way to solve a complex computing task is to chain together specialized components. In data-science this is the pipeline approach. Each component mostly treats the other components as I/O black-boxes. As developers we potentially have the full picture but the system does not.With Neural Network what happens between I and O is often too interesting to be ignored. One Neural Network can leverage the way another Neural Network processes its inputs.In this post I discuss the following scenario :To “understand” english is necessary to analyse news. Thus during training a standalone ’N’ NeuralNet would learn about the semantics of english as a by-product. Unfortunately, as a human language vocabulary is vast, it would take a lot of training data to get accurate results.To help ‘N’ we can instead allow it to access the semantics knowledge of ‘W’. This simplifies a lot the whole problem because finding data to train ‘W’ on english semantics is relatively easy : we can slices the sentences in the News feed data set.Here is an example from the Newswire dataset :the farmers home administration the us agriculture department’s farm lending arm could lose about seven billion dlrs in outstanding principal on its severely delinquent borrowers or about one fourth of its farm loan portfolio the general accounting office gao said in remarks prepared for delivery to the senate agriculture committee […]We extract text chunks (3 inputs, 1 output) to train ‘W’ as follow :If each News article in the ’N’ data-set is about 100-words long then the training set for ‘W’ will be 100 times larger than the training set of ’N’.There are several ways of learning a “words to vectors” representation with Neural Net :The method I chose in the notebook is the CBOW (continuous bag of word) as it is simple to train. In the below diagram only the ‘Words to Vectors’ and ‘Linear Transform’ needs to be trained on the data while the ‘Sum’ and ‘SoftMax’ operators are imposed. This way most of the knowledge is stored into the Word to Vectors mapping that is shared with the ’N’ network.To analyse a news article each word is sequentially turned into a vector using the semantic knowledge of the ‘W’ net. Usually the resulting “time serie” of vector is processed either using :The following diagram illustrates the convolution based method implemented in the notebook. The idea behind convolution in text modeling is the same as the one behind CBOW : surrounding words provide context. In the below diagram several convolution are used to vary the size of the context window (from 1 word to 4 words).To share the word-to-vector mapping layer across models we only need to declare it outside of the model definition are refer to it via its name when needed. Such mapping is traditionally called a word-embedding or simply an embedding. The Keras syntax is as clear as possible :Using Keras (or a similar library) the CBOW neural-network can be coded in only 5 instructions :As explained above we have enough data to train Embedding layer of ‘Word’ net but not to train it as part of the ‘News’ net. To deal with this we tell Keras when to apply the gradient descent to EMBEDDING and when to keep it fixed using the “trainable” property of layers :‘W’ models the input data itself : this is a case of un-supervised learning. As finding a training set for ‘W’ is as easy as finding english text we don’t even have to restrict ourself to the News dataset. For example we can collect Wikipedia text to have a good sample of contemporary ‘informative’ english.We can also directly load an existing embedding matrix and fine-tune it on our dataset. In the notebook implementing this post model I chose the GloVe (Global Vectors for Word Representation) weights.As the GloVe weights do not provide a full CBOW model it is best to first train the Linear transform of the model while keeping the Embedding fix. Then a full estimation is performed.Compare the previous sample from the Newswire dataset with the following news excerpt from the Financial Times (link) :Berlin backs the European Commission’s insistence that Britain’s exit terms must be negotiated before talks begin on any new relationship with the EU. Ms Merkel’s view is that agreement on exit must be struck in principle, probably in outline, before future arrangements are discussed.In both cases the news relates to policies yet the writing styles are very different. The newswire is very rough and telegraphic, it often lacks punctuation marks, the spelling is not full-proofed… as the wire needs to be delivered as fast as possible. This tells a lot about the mismatch risk between language corpus and the limitation of portability. Thus when enough data is available it is best to use pre-trained model only as optimization starting point and to fine-tune them.While we often train classifier using entropy as the optimization target, this metric is not intelligible enough to get a sense of the model useful-ness.The most intuitive metric is the accuracy, defined as the % of correctly classified news. After only 5 training iterations (“epochs”) the model correctly classifies 70% of news articles on the test set. The accuracy metric as one significant pitfall tough : if one category is much larger than the other one a simple yet accurate model is to always predict this category !To get a better idea of the performance with use a confusion matrix :A confusion matrix C is such that C(i,j) is equal to the number of observations known to be in group ‘i’ but predicted to be in group ‘j’.When the model is 100% accurate the confusion matrix is diagonal only.Finally we should also look at examples of misclassified news to check we cannot see a simple improvement to the model.Example 1 : labelled as category 5 but classified as category 1grain traders said they were still awaiting results of yesterday’s u k intervention feed wheat tender for the home market the market sought to buy 340 000 tonnes more than double the remaining 150 000 tonnes available under the current tender however some of the tonnage included duplicate bids for supplies in the same stores since the tenders started last july […]Example 2 : labelled as category 9 but classified as category 1mexico has no intention of leaving the international coffee organization ico in the event of brazil withdrawing from the group the mexican coffee institute imc said the imc said in a statement the ico is an important instrument for ensuring producers obtain an adequate price mexico currently produces around five mln 60 kilo bags of coffee per year […]The issue seems to be that the real data has too many categories that the model ‘stuffs’ into larger commodity topic. As the Confusion Matrix shows many categories are very rare (count, not just percentage) thus difficult to calibrate using machine learning.If you enjoyed reading this article, you can improve its visibility with the little green heart button below. Thanks!",28/03/2017,3,16.0,10.0,494.0,337.0,9.0,7.0,0.0,12.0,en
4022,Building smart robots using AI + ROS: Part 1,Kredo.ai Engineering,karthic Rao,745.0,6.0,1082.0,"Motivation for writing blog series on AI + Robotic Operating Systems:The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.ROS is used to create application for a physical robot without depending on the actual machine, thus saving cost and time. These applications can be transferred onto the physical robot without modifications.The decision making capability of the robots can be aided with AI. The cases where the robot agent has to learn optimal strategies in high dimensional state space often means that it is impractical to generate sufficient training data with real-world experiments. These are the kinds of tasks where reinforcement learning excels. For lot many other perception tasks we could use Deep learning.We could train a robot using reinforcement learning or Deep learning in simulation, with the potential of then transferring this to a real robot. Today this technique is widely used in training drones, autonomous vehicles, robotic arms, warehouse robots and the list goes on. There never hasn’t been a better time to take a deep plunge into the area.I witnessed that the materials around using AI to train Robots in ROS are scarce on internet, thus decided to write series of articles on them.The articles in the series will be categorized into 3 parts,1. Focused on opening up an intuitive insight into Reinforcement learning (RL), Deep Reinforcement Learning(DeepRL) and Deep learning(DL).2. Focused solely on fundamentals of ROS.3. Training Robot agents using RL, DeepRL and DL techniques in ROS simulation.This article is about introduction to Reinforcement learning and Open AI gym.Reinforcement learning, in simple terms, is a mathematical approach where an agent interacts with an environment by taking actions in which it tries to maximize an accumulated reward.An agent in a current state (St) takes an action (At) to which the environment reacts and responds, and it returning a new state(St+1) and reward (Rt+1) to the agent. Given the new updated state and reward, the agent chooses the next action, and the loop repeats until an environment is solved or terminated. In the process the agent is expected to learn the optimal set of actions to be taken to achieve the goal.But what are these environments?It depends! For a drone which is being trained to maneuver, the open space where its being trained is its environment. The velocity, its location and fuel levels could define its state. For agent which is trained to play chess, the chess board is its environment.What do you actually mean when you say that the agent interacts with the environment?The environment should allow the agent to take actions in any given state of the environment, which should result in transition of the state and return a reward for moving onto the new state.What are these rewards? Why should the environment return reward for moving onto a new state?Rewards are just signals by which we communicate to the agent on what it should ultimately accomplish, its the means by which the agent understands the goal, it also uses the reward values to figure out the optimal way to achieve the goal.What are some important differences between Reinforcement leaning and supervised learning ?In case of a reinforcement learning the reward will usually be delayed. For example, in a game of chess the reward for winning is obtained only after series of moves which led to the positive outcome. This is different from the supervised learning setting wherein the feedback is immediately available by means of labels.The action taken at a given point in time has an impact on the future observations made, For example the direction at which the robot decides to move has an impact on its future world view.What are some of the challenging scenarios where Reinforcement learning thrives to figure out the optimal strategy?Inroder to train an agent using RL, one needs an environment where the agent can interact with, where the state of the agent is well defined, actions can be taken and rewards are obtained right?Yes, that’s true.Before I apply RL algorithms to real world problems I need an environment to learn, try, test and experiment with the RL techniques!!! What should I do?Well!! OpenAI gym is here for your rescue!!!!!OpenAI Gym is a toolkit/library for developing and comparing reinforcement learning algorithms. This is the gym open-source library, which gives you access to a standardized set of environments.Gym is a collection of environments designed for testing and developing reinforcement learning algorithms. It saves the overhead of having to build an environment to try out RL techniques. Gym is written in Python. There is also an online leaderboard for people to compare results and code.Here are some details about the gym library.Imports the open AI gym python moduleIt fetches the carpole environment, https://gym.openai.com/envs has the list of all the available environments.Here is the description of the cartpole environment and the goal, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart’s velocity.At any point in time the state of the system in the cartpole environment is defined by 4 variables,There are only 2 possible actions that be performed.So the goal of the task is to look at the states and decide on one of the two actions inorder to balance the pole.Information about any given environment can be obtained by env.observation_space , various methods on this object helps you to understand the state and environment better. Similarly methods on env.action_space object helps one to understand the action space. These methods are very helpful since the state representation and the possible actions differ with each environment.One needs to run the reset function before running the episode,Now, Here is how to perform an action and obtain new state and its associated reward as a consequence of the action,Let’s run an episode through the environment, let us pick a random action each time and see how long can we hold the pole upright.Since the actions are random we’ll not be able to balance the pole for long,That’s it for now! In the next blog I’ll be discussing about the intuition behind Q value and using Q learning to solve a simple Open AI gym environment. Thank you for reading. Feel free to comment and express your opinion.Additional Resources:",08/12/2017,4,3.0,0.0,808.0,444.0,4.0,2.0,0.0,5.0,en
4023,Clustering on mixed type data,Towards Data Science,Thomas Filaire,194.0,7.0,618.0,"Clustering unsupervised data is not an easy task. Indeed, data crunching and exploration is in such a context often driven by domain knowledge, if not pure intuition, and made difficult as there is no way to measure the accuracy of the resulting segmentation (as opposed to supervised learning).In addition, introductory courses to unsupervised learning quite often discuss ideal use cases, such as k-means tutorials, which only apply to numerical features.However, real business situations often deviate from these ideal use cases, and need to analyze datasets made of mixed-type data, where numeric (the difference between two values is meaningful), nominal (categorical, not ordered) or ordinal (categorical, ordered) features coexist.In this post, I’m focusing on such situations, in the context of unsupervised classification exercise using R.Let’s get started!Distance is a numerical measurement of how far apart individuals are, i.e. a metrics used to measure proximity or similarity across individuals. Many distance metrics exist, and one is actually quite useful to crack our case, the Gower distance (1971).Gower distance is computed as the average of partial dissimilarities across individuals. Each partial dissimilarity (and thus Gower distance) ranges in [0 1].Partial dissimilarities (d_ij^f) computation depend on the type of variable being evaluated. This implies that a particular standardization will be applied to each feature, and the distance between two individuals is the average of all feature-specific distances.Note: Gower distance is available in R using daisy()function from the cluster package. Features are first automatically standardized (i.e. rescaled to fall in a [0 1] range).The Gower distance fits well with the k-medoids algorithm. k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori.Pretty similar to k-means algorithm, PAM has the following caracteristics:Unless you have a good a priori rationale to force a specific number of clusters k, you might be interested in asking the computer for a recommendation based on statistics. Several approaches exist to qualify the relevancy of chosen number of clusters. In part II, we use the silhouette coefficient.The silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-medoids clustering, and is also used to determine the optimal number of clusters. Please read the Wikipedia page for further details around computation and interpretation.There are basically two ways to investigate the results of such a clustering exercise, in order to derive some business-relevant interpretation.We cover both cases in the use case (Part II). Enough for the theory, let’s apply and illustrate!In this use case, we will try to cluster bank clients according to the following characteristics:Most similar and dissimilar clients according to Gower distance:In business situation, we usually search for a number of clusters both meaningful and easy to remember, i.e. 2 to 8 maximum. The silhouette figure helps us identify the best option(s).InterpretationHere one can attempt to derive some common patterns for clients within a cluster. As an example, cluster 1 is made of “management x tertiary x no default x no housing” clients, cluster 2 is made of “blue-collar x secondary x no default x housing” clients, etc.Although not perfect (especially cluster 3), colors are mostly located in similar areas, confirming the relevancy of the segmentation.This article is a recap on my thoughts while trying to perform a clustering exercise on mixed type unsupervised datasets. I thought it could be of added value to other data scientists, thus the sharing.Several challenges remain however, including:Please do not hesitate to comment and share your vision on how to tackle this challenge and improve this proposed methodology.",17/07/2018,5,57.0,0.0,713.0,464.0,7.0,10.0,0.0,11.0,en
4024,"What is the difference between Ridge Regression, the LASSO, and ElasticNet?",Medium,Alex Lenail,190.0,7.0,1264.0,"This article is about different ways of regularizing regressions. In the context of classification, we might use logistic regression but these ideas apply just as well to any kind of regression or GLM.With binary logistic regression, the goal is to find a way to separate your two classes. There are a number of ways of visualizing this.No matter which of these you choose to think of, we can agree logistic regression defines a decision ruleh(x|theta) = sigmoid(x dot theta + b)and seeks a theta which minimizes some objective function, usuallyloss(theta)= ∑ y*log(h(x|theta)) + (1−y)log(1−h(x|theta))which is obfuscated by a couple clever tricks. It is derived from the intuitive objective function:loss(theta)= ∑ (y - h(x|theta))i.e. the number of misclassified x, which makes sense to try to minimize.In many cases, you wish to regularize your parameter vector theta. This means you want to both minimize the number of misclassified examples while also minimizing the magnitude of the parameter vector. These objectives are in opposition, and so the data scientist needs to decide on the appropriate balance between those objectives using their intuition, or via many empirical tests (e.g. by cross validation).Let’s rename our previous loss functionloss(theta)= ∑ y*log(h(x|theta)) + (1−y)log(1−h(x|theta))the basic_loss(theta). Our new, regularized loss function will look like:loss(theta) = basic_loss(theta) + k * magnitude(theta)Recall we’re trying to minimize loss(theta) which means we’re applying downwards pressure on both the number of mistakes we make as well as the magnitude of theta. In the above loss function, k is a hyperparameter which modulates the tradeoff of how much downwards pressure we apply to the error of the classifier defined by theta versus the magnitude of theta. Therefore, k encodes our prior beliefs, our intuitions, as to how the process we’re modeling is most likely to behave.Now on to the interesting part. It turns out there is not one, but many ways of defining the magnitude (also called the Norm) of a vector. The most commonly used norms are the p-norms, which have the following character:For p = 1 we get the L1 norm (also called the taxicab norm), for p = 2 we get the L2 norm (also called the Euclidean norm), and as p approaches ∞ the p-norm approaches the infinity norm (also called the maximum norm). The Lp nomenclature comes from the work of a mathematician called Lebesgue.Returning to our loss function, if we choose L1 as our norm,loss(theta) = basic_loss(theta) + k * L1(theta)is called “the LASSO”. If we choose the L2 norm,loss(theta) = basic_loss(theta) + k * L2(theta)is called Ridge Regression (which also turns out to have other names). If we decide we’d like a little of both,loss(theta) = basic_loss(theta) + k(j*L1(theta) + (1-j)L2(theta))is called “Elastic Net”. Notice the addition of a second hyperparameter here. Notice also that ElasticNet encompasses both the LASSO and Ridge, by setting hyperparameter j to 1 or 0.Academia has a complicated incentive structure. One aspect of that incentive structure is that it is desirable to have a unique name for your algorithmic invention, even when that invention is a minor derivative of another idea, or even the same idea applied in a different context. Take, for example, Principal Component Analysis.PCA was invented in 1901 by Karl Pearson,[1] as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.[2] Depending on the field of application, it is also named the discrete Kosambi-Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis, Eckart–Young theorem (Harman, 1960), or Schmidt–Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.That’s 14 unique names for PCA.I’m writing this article because the question at the top of this piece was quite hard to find an answer to online. I ended up finding part of the answer in The Elements of Statistical Learning (written by the authors of the regularization methods above, in fact) and the rest from Karen Sachs.I personally believe that the words Lasso, Ridge, and ElasticNet should not exist. We should call these for what they are: L1-regularization, L2-regularization, and mixed-L1-L2-regularization. A mouthful, sure, but dramatically more unambiguous.The organization of scikit-learn may have been what caused my confusion in the first place. When looking through their list of regression models, LASSO is its own class, despite the fact that the logistic regression class also has an L1-regularization option (the same is true for Ridge/L2). This is unexpected from a python library, since one of the core dogmas of python is:Now that we have disambiguated what these regularization techniques are, let’s finally address the question: What is the difference between Ridge Regression, the LASSO, and ElasticNet?The intuition is as follows:Consider the plots of the abs and square functions.When minimizing a loss function with a regularization term, each of the entries in the parameter vector theta are “pulled” down towards zero. Think of each entry in theta lying on one the above curves and being subjected to “gravity” proportional to the regularization hyperparameter k. In the context of L1-regularization, the entries of theta are pulled towards zero proportionally to their absolute values — they lie on the red curve. In the context of L2-regularization, the entries are pulled towards zero proportionally to their squares — the blue curve.At first, L2 seems more severe, but the caveat is that, approaching zero, a different picture emerges:The result is that L2 regularization drives many of your parameters down, but will not necessarily eradicate them, since the penalty all but disappears near zero. Contrarily, L1 regularization forces non-essential entries of theta all the way to zero.Adding ElasticNet (with 0.5 of each L1 and L2) to the picture, we can see it functions as a compromise between the two. One can imagine bending the yellow curve towards either red or blue by tuning the hyperparameter j.There are a number of reasons to regularize regressions. Typically, the goal is to prevent overfitting, and in that case, L2 has some nice theoretical guarantees built into it. Another purpose for regularization is often interpretability, and in that case, L1-regularization can be quite powerful.In my work, I deal with a lot of proteomics data. In proteomics data, you have counts for some number of proteins for some number of patients — a matrix of patients by protein abundances, and the goal is to understand which proteins play a role in separating your patients by label.This is an Ovarian Cancer dataset. Let’s first perform logistic regression with an L2-penalty and try to understand how the cancer subtypes are distinct. This is a plot of the learned theta:You see that many, if not all proteins are registering as significant.Now consider the same approach but with L1-regularization:A much clearer picture emerges of the relevant proteins to each Ovarian Cancer subtype. This is the power of L1-regularization for interpretability.Regularization can be very powerful, but it’s somewhat under-appreciated, partially, I think, because the intuitions aren’t always well explained. The ideas are mostly very simple, but not terribly well documented much of the time. I hope this article helps mend that deficit.Thanks to Karen Sachs for explaining the intuitions behind these norms many years ago.Thanks also to the developers of scikit-learn. Despite the occasional unintuitive APIs, the code you make available is invaluable to data scientists like myself.",01/08/2017,1,11.0,30.0,1011.0,462.0,7.0,0.0,0.0,21.0,en
4025,Improving neural networks by preventing co-adaptation of feature detectors,Medium,Michael L. Peng,62.0,4.0,652.0,"This blog post aims to provide readers some insights on deep neural networks and intuition about dropout technique.Deep neural networks are models composed of multiple layers of simple, non-linear neurons. With composition of enough neurons, the model can learn extremely complex functions that can accurately perform complicated tasks that are impossibly difficult to hard code, such as image classification, translation, speech recognition, etc. The key aspect of deep neural networks is that they are able to automatically learn data representation needed for features detection or classification without any a priori knowledge¹.For example, VGG16 (shown below) is a convolutional neural network that is trained on ImageNet Large Scale Visual Recognition Competition (ILSVRC) data. It has 16 layers and 138 millions parameters and is capable to classify 1000 different objects with high accuracy.Often time, models with such complexity often can have exponentially many combinations of active neurons that can achieve high performance on the training set, but not all of them are robust enough to generalized well on unseen data (or testing data). This problem is also known as overfitting.One of the most prominent reasons for causing overfitting is co-adaptation. According to wiki, at genetic level, co-adaptation is the accumulation of interacting genes in the gene pool of a population by selection. Selection pressures on one of the genes will affect its interacting proteins, after which compensatory changes occur. So in neural network, co-adaptation means that some neurons are highly dependent on others. If those independent neurons receive “bad” inputs, then the dependent neurons can be affected as well, and ultimately it can significantly alter the model performance, which is what might happen with overfitting.The simplest solution to overfitting is dropout, and works as followed: each neuron in the network is randomly omitted with probability between 0 and 1.By implementing such technique, the model is forced to have neurons that can learn good features (or data representation needed) while not relying on other neurons. Therefore, the resulting model can be more robust to unseen data. Hinton also makes an interesting observation on similarity between dropout and the role of sex in evolution:One possible interpretation of mixability articulated in is that sex breaks up sets of co-adapted genes and this means that achieving a function by using a large set of co-adapted genes is not nearly as robust as achieving the same function, perhaps less than optimally, in multiple alternative ways, each of which only uses a small number of co-adapted genes. This allows evolution to avoid dead-ends in which improvements in fitness require co-ordinated changes to a large number of co-adapted genes. It also reduces the probability that small changes in the environment will cause large decreases in fitness a phenomenon which is known as “overfitting” in the field of machine learning.Additionally, dropout makes possible to train very deep neural networks such as VGG16 in a reasonable time because a large of amount computations are reduced as many neurons are omitted from the network.Hinton conducts a number of experiments to validate the effectiveness of dropout. Below is one of the experiments using MNIST data set, which consists of 60,000 28x28 images of handwritten digits.The best previously published result for this task using backpropagation without pre-training or weight-sharing or enhancements of the training set is shown as a horizontal line. The upper set of lines are test error rate on the MNIST test set for a variety of neural network architectures trained 50% dropout for all hidden layers. The lower set of lines also use 20% dropout for the input layer.Below is the result of another experiment using TIMIT, a widely used benchmark for recognition of clean speech with a small vocabulary.The frame classification error rate on the core test set of the TIMIT benchmark. Comparison of standard and dropout finetuning for different network architectures. Dropout of 50% of the hidden units and 20% of the input units improves classification.Try using dropout when overfitting persists.",07/05/2018,0,1.0,0.0,849.0,466.0,4.0,1.0,0.0,2.0,en
4026,Extreme Event Forecasting with LSTM Autoencoders,Towards Data Science,Marco Cerliani,4300.0,7.0,1290.0,"Dealing with extreme event prediction is a frequent nightmare for every Data Scientist. Looking around I found very interesting resources that deal with this problem. Personally, I literally fall in love with the approach released by Uber Researchers. In their papers (two versions are available here and here) they developed an ML solution for daily future prediction of traveler demand. Their methodology stole my attention for its geniality, good explanation, and easy implementation. So my purpose is to reproduce their discovery in pythonic language. I’m very satisfied with this challenge and in the end, I improved my knowledge of regression forecasting.The most important takeaways from this post can be summarized as:But Keep Kalm and let’s proceed step by step.At Uber accurate prediction for completed trips (particularly during special events) provides a series of important benefits: more efficient driver allocation resulting in a decreased wait time for the riders, budget planning and other related tasks.In order to reach high accurate predictions of driver demand for ride-sharing, Uber Researchers developed a high-performance model for time series forecasting. They are able to fit (one-shot) a single model with a lot of heterogeneous time series, coming from different locations/cities. This process permits us to extract relevant time patterns. In the end, they were able to forecast demand, generalizing for different locations/cities, outperforming the classical forecasting methods.For this task Uber made use of an internal dataset of daily trips among different cities, including additional features; i.e. weather information and city-level information. They aimed to forecast the next day’s demand from a fixed window of past observations.Unfortunately, we don’t have at our disposal this kind of data, so we, as Kaggle Fans, chose the nice Avocado Prices Dataset. This data shows historical avocado prices, of two different species, and sales volume in multiple US markets.Our choice was due to the need for a nested dataset with temporal dependency: we have time series for each US market, 54 in total, a number that grows to 108 if we consider one time series for each type (conventional and organic). This data structure is highlighted as important by Uber Researchers because it permits our model to discover important invisible relations. Also, the correlation among series brings advantages for our LSTM Autoencoder during the process of feature extraction.To build our model we utilized the time series of prices at our disposal up to the end of 2017. The first 2 months of 2018 are stored and used as a test set. For our analysis, we will take into consideration also all the provided regressors. The observations are shown with a weakly frequency so our purpose is: given a fixed past window (4 weeks) of features, predict the upcoming weakly price.Due to the absence of exponential growth and trending behavior, we don’t need to scale our price series.In order to solve our prediction task, we replicate the novel model architecture, proposed by Uber, which provides a single model for heterogeneous forecasting. As the below figure shows, the model first primes the network by auto feature extraction, training an LSTM Autoencoder, which is critical to capture complex time-series dynamics at scale. Features vectors are then concatenated with the new input and fed to LSTM Forecaster for prediction.Our forecasting workflow is easy to imagine: we have our initial windows of weekly prices (plus other features) for different markets. We start to train our LSTM Autoencoder on them; next, we remove the encoder and utilize it as a features creator. The second and final step required to train a prediction LSTM model for forecasting. Based on real/existing regressors and the previous artificial generated features, we are able to provide next week’s avocado price prediction.We easily recreate this logic with Keras.Our LSTM Autoencoder is composed of a simple LSTM encoder layer, followed by another simple LSTM decoder. You will understand the utility of dropout during the evaluation, at this point they are harmless, trust me!We compute features extraction and concatenate the result with other variables. At this point I made a little deviation from the Uber solution: they suggest manipulating the feature vectors extracted by our encoder aggregating them via an ensemble technique (e.g., averaging). I decided to let them original and free. I make this choice because it permits me to achieve better results in my experiments.In the end, the prediction model is another simple LSTM based neural network:Finally, we are almost ready to see some results and make predictions. The last steps involve the creation of a rival model and the consequence definition of a robust forecasting methodology for results comparison.Personally, the best way to evaluate two different procedures is to replicate them as much as possible, in order to mark attention only at the points of real interest. In this implementation, I want to show evidence of LSTM Autoencoder power as a tool for relevant feature creation for time series forecasting. In this sense, to evaluate the goodness of our methodology, I decided to develop a new model for price forecasting with the same structure as our previous forecasting NN.The only difference between them is the features they received as input. The first one receives the encoder output plus the external regressors; the second one receives past raw prices plus the external regressors.Time series forecasting is critical in nature for the extreme variability of the domain of interest. In addition, if you try to build a model based on Neural Network your results are also subject to internal weight initialization. To overcome this drawback a number of approaches exist for uncertainty estimation: from Bayesian to those based on the bootstrap theory.In their work Uber Researchers combine Bootstrap and Bayesian approaches to produce a simple, robust and tight uncertainty bound with good coverage and provable convergence properties.This technique is extremely simple and practical… indirectly we have already implemented it! As you can see in the figure below, during the feedforward process, dropout is applied to all layers in both the encoder and the prediction network. As a result, the random dropout in the encoder perturbs the input intelligently in the embedding space, which accounts for potential model misspecification and gets further propagated through the prediction network.Pythonic speaking we have simply to add trainable dropout layers in our Neural Network and reactivate them during prediction (Keras used to cut dropout during prediction).For the final evaluation, we must iterate the calling of the prediction function and store the results. I also compute the scoring of the prediction at each interaction (I chose Mean Absolute Error). We must set the number of times we compute evaluation (100 times in our case). With the scores stored, we are able to compute the mean, the standard deviation, and the relative uncertainty of MAE.We replicate the same procedure for our ‘rival model’ made by only the LSTM prediction network. After averaging scores and computing uncertainty, the final results are better for the LSTM Autoencoder + LSTM Forecaster instead of the single LSTM Forecaster.In this post, I replicate an end-to-end neural network architecture developed at Uber for special event forecasting. I want to emphasize: the power of LSTM Autoencoder in the role of feature extractor; the scalability of this solution to generalize well avoiding training multiple models for every time series; the ability to provide a stable and profitable method for neural networks evaluation.I also remark that this kind of solution suits well when you have at your disposal an adequate number of time series that share common behaviors… It’s not important that these are immediately visible, the Autoencoder makes this for us.CHECK MY GITHUB REPOKeep in touch: LinkedinREFERENCES[1] Deep and Confident Prediction for Time Series at Uber: Lingxue Zhu, Nikolay Laptev[2] Time-series Extreme Event Forecasting with Neural Networks at Uber: Nikolay Laptev, Jason Yosinski, Li Erran Li, Slawek Smyl",22/05/2019,2,7.0,3.0,917.0,586.0,5.0,1.0,0.0,9.0,en
4027,Natural Language Processing for Fuzzy String Matching with Python,Towards Data Science,Susan Li,25000.0,5.0,556.0,"In computer science, fuzzy string matching is the technique of finding strings that match a pattern approximately (rather than exactly). In another word, fuzzy string matching is a type of search that will find matches even when users misspell words or enter only partial words for the search. It is also known as approximate string matching.Fuzzy string search can be used in various applications, such as:Speaking of dedupe, it may not as easy as it sounds, in particular if you have hundred thousands of records. Even Expedia does not make it 100% right:This post will explain what fuzzy string matching is together with its use cases and give examples using Python’s Fuzzywuzzy library.Each hotel has its own nomenclature to name its rooms, the same scenario goes to Online Travel Agency (OTA). For example, one room in the same hotel, Expedia calls “Studio, 1 King Bed with Sofa bed, Corner”, Booking.com may find safe in showing the room simply as a “Corner King Studio”.There is nothing wrong here, but it could lead to confusion when we want to compare room rate between OTAs, or one OTA wants to make sure another OTA follows the rate parity agreement. In another word, to be able to compare price, we must make sure that we are comparing apples to apples.One of most consistently frustrating issues for price comparison websites and apps is trying to figure out whether two items (or hotel rooms) are for the same thing, automatically.Fuzzywuzzy is a Python library uses Levenshtein Distance to calculate the differences between sequences in a simple-to-use package.In order to demonstrate, I create my own data set, that is, for the same hotel property, I take a room type from Expedia, lets say “Suite, 1 King Bed (Parlor)”, then I match it to a room type in Booking.com which is “King Parlor Suite”. With a little bit experience, most human would know they are the same thing. Follow this methodology, I create a small data set with over 100 room type pairs that can be found on Github.Using this data set, we are going to test how Fuzzywuzzy thinks. In another words, we are using Fuzzywuzzy to match records between two data sources.The data set was created by myself, so, it is very clean.There are several ways to compare two strings in Fuzzywuzzy, let’s try them one by one.62This is telling us that the “Deluxe Room, 1 King Bed” and “Deluxe King Room” pair are about 62% the same.69The “Traditional Double Room, 2 Double Beds” and “Double Room with Two Double Beds” pair are about 69% the same.74The “Room, 2 Double Beds (19th to 25th Floors)” and “Two Double Beds — Location Room (19th to 25th Floors)” pair are about 74% the same.I am disappointed with these. It turns out, the naive approach is far too sensitive to minor differences in word order, missing or extra words, and other such issues.We are still using the same data pairs.698363For my data set, comparing partial string does not bring better results overall. Let’s continue.847883Best so far.1007897Looks like token_set_ratio is the best fit for my data. According to this discovery, I decided to apply token_set_ratio to my entire data set.0.9029126213592233When setting ratio > 70, over 90% of the pairs exceed a match score of 70. Not so shabby!Jupyter notebook can be found on Github. Happy Friday!",12/10/2018,14,13.0,13.0,1175.0,662.0,3.0,5.0,0.0,11.0,en
4028,SocialDefender — Social Reputation Management Platform — Aji Abraham,Medium,Aji Abraham,533.0,2.0,302.0,"Social media can be hard to control. From small and medium-sized businesses lacking additional manpower to large companies requiring a method for scheduling numerous team members, we decided to create a tool that will add additional value to social media efforts.Social Defender provides real-time social media monitoring, insights and gives the ability to accurately moderate social media efforts and understand customer sentiment.By using the tool, businesses can manage multiple social media networks including Facebook, Twitter, Tumblr, YouTube, G+, blogs and forums using just one login. This social media management tool gives businesses the ability to analyze what is being said online about a brand, service, industry, and competitors. Analytics provided by Social Defender may be incorporated with future goals or business plans.Social Defender has a Facebook page app, Twitter app, LinkedIn app, G+ app and more. Users receive real-time info for each channel. Each of these messages was fed to an NLP algorithm for sentiment analysis. We used a proprietary Bayesian algorithm for the sentiment analysis is developed by the team at Armia. This algorithm takes feedback from manual changes to the sentiment making self-learning of the accuracy and effectiveness. Right now we achieved an accuracy level of 90% and fine-tuning to make it better.The brand manager can decide if they want the platform to auto moderate based on time, sentiment, brand etc. Or if they want to be notified. It has a decent workflow manager to drive notifications to different users in the team. Also, the same workflow manager can schedule the publishing of different contents to different platforms.The user and admin area analytics were created using Flash. Know any businesses that need help with social media? Do them a favor and tell them about Social Defender (a free plan is also currently available).Originally published at https://www.ajiabraham.com on January 11, 2013.",11/01/2013,0,0.0,3.0,0.0,0.0,0.0,0.0,0.0,2.0,en
4029,The hype on AlphaFold keeps growing with this new preprint,Towards Data Science,LucianoSphere,476.0,9.0,1794.0,"I am sure you read about AlphaFold in late 2020 when it “won” the CASP14 “contest” on modeling protein structures, and in July 2021 when the peer-reviewed paper and AI model were released. If not, or if you want to refresh what protein structures are, why biologists prayed for decades for programs to accurately predict them, and how AlphaFold works and performs, then check this story and this one, then come back here.This new story brings you the latest news, based on a just-published preprint.Table of contentsThis story is based on a preprint just posted in the bioRxiv that formally describes a tool dubbed ColabFold under the moto Making protein folding accessible to all (which I would have rather phrased Making modern protein structure modeling accessible to all).Created by Milot Mirdita from the Max Planck Institute for Biophysical Chemistry in Germany, Sergey Ovchinnikov from Harvard University in the U.S. and Martin Steinegger from the Seoul National University in South Korea, ColabFold is a set of Google Colab notebooks evolved from the early prototypes, that allow users having nothing more than a computer, internet connection and a free Google account, to run protein structure predictions using the latest cutting-edge machine learning technologies on hardware provided by Google; in addition benefiting from several optimizations that reduce running time without compromising the quality of the results and capitalizing on a modern tool for the fast generation of multiple protein sequence alignments -which as I explained in other stories are important to ensure accurate results.More precisely, ColabFold allows users to run AlphaFold2 or RoseTTAFold (an academic AI-based program -from the Baker lab, one of the academic leaders in protein structure prediction- that came up after CASP14 so it hasn’t yet been formally evaluated, but apparently performs close to AlphaFold2) combined with fast multiple sequence alignment generation by the program MMseqs2. Users can upload their own alignments too, which may be handy for very tricky proteins or protein families, or for alignments coming from proprietary data such as metagenomics projects.Optimized generation of protein sequence alignments with MMseqs2 improves models and reduces running timeAs shown widely in the CASP papers of the recent years (see for example my peer-reviewed CASP13 assessment), it is key for the performance of all these methods to count with vast multiple sequence alignments, rich in numbers of sequences that, in the ideal case, cover the whole target protein smoothly. The initial compilation of the alignment is thus critical. Further complicating this, typical databases of protein sequences contain millions to billions of sequences, but of course only very small subsets of them correspond to proteins of the same structural family which are the ones that one wants to show up in the multiple sequence alignment. That’s where the MMseqs2 component of ColabFold comes in.MMseqs2 by Martin Steinegger and Johannes Söding is a program for sensitive protein sequence searches inside huge sequence dabatases. I will not go into any detail but the paper describing MMSeqs2 might be of interest to data scientists as the core aim of the program is to accelerate sequence searches. ColabFold executes the MMseqs2 program through API calls to a dedicated server. The authors optimized the size and variability of the protein sequences contained in their sequence databases so that by running the program iteratively a few times they can produce ample, informative yet tractable alignments. In the preprint the authors in fact show that these multiple sequence alignments produced by MMseqs2 lead AlphaFold 2 to more accurate predictions than those it obtains through its custom multiple sequence alignments, and runs around one order of magnitude faster.Beyond simple modeling of monomeric “isolated” proteinsA vast majority of proteins do not work as isolated molecules but rather as complexes, either with themselves (so-called homo-dimers, homo-trimers, and so on, or homooligomers in general) or with other proteins (called hetero-dimer, hetero-trimer, etc.). The main evaluation were AlphaFold2 was found to “win” in CASP14 was in modeling the structures of proteins by themselves, but there was also some indication that AlphaFold2 was correctly modeling protein complexes too. This was further explored in the early Google Colab notebooks by Minkyung Baek and Yoshitaka Moriwaki, and then by the authors of ColabFold who ended up integrating this possibility into the released notebooks. Thus, ColabFold users can easily model isolated proteins as well as their homo- and hetero-complexes. For heterocomplexes it is tricky to develop alignments, but the ColabFold authors already dealt with all this burden making a very simple interface for the users to simply tick for oligomerization states and enter the different sequences of the proteins involved.Notably, AlphaFold has no way to tell by itself of a protein is monomeric, homodimeric, heterodimeric with another protein, etc. This information is sometimes known from biochemical or biophysical experiments, in which case it is used as an input. If no hint is available about this, then users should probably run the predictions in different modes and compare the results critically.Flavors for lay and advanced usersAs described in the official GitHub page here, ColabFold includes various notebooks tailored for different kinds of runs: one for RoseTTAFold, one for AlphaFold2 in a simple mode that allows running only monomeric proteins but with minimal decisions to be made, and another for AlphaFold2 with fully exposed features which allows the full control of oligomerization states among other options that users can experiment with.Depending on the exact GPU resources allocated when the users logs into Google Colab, it is possible to model proteins of as many as 1000 to 1400 amino acids, which covers a substantial number of the proteins of interest. For bigger proteins, or for more privacy or convenience, users can also get the generated code, download the whole AlphaFold program and MMseqs2-generated alignments, and run everything locally with its own GPUs (and even benefit from the code in the notebook).Quality estimatesThese user-friendly interfaces not only provide users with models of the protein structures, but also with estimates of their qualities. Such estimates are essential, I’d say as important as the predictions themselves, so they should rather be accurate. That’s because the end user should know what regions of the models are reliable, i.e. likely to be similar to the true structure, and which regions might not be well predicted. When I was an academic assessor during CASP13 (this academic paper) I stressed the importance of producing three kinds of quality estimates: one that measures the global quality of the overall fold, another that measures the quality of each amino acid individually, and another that measures the quality of the relative distances and orientations between any pair of amino acids in the protein. The ColabFold notebooks provides all three metrics, as I show in the example below.Full online visualization of the results -an example runAll tunable variables are input in the fields of a quite rich GUI, on top of which users can of course modify the code by hand. But the cozy interface is not limited to the inputs. The notebooks feature rich graphical outputs with which users can inspect the run as it goes, and at the end inspect all the quality estimate plots and even the 3D models right inside the browser.See these example outputs from an actual run I did on a protein that I know is an homodimer but whose structure I have very few clues about. After installing the required software with a first click in the ColabFold notebook, then setting up the protein’s sequence and telling the notebook that I know my protein is an homodimer, I first run the MMseqs2 module of the notebook to get a multiple sequence alignment that the notebook will feed into AlphaFold in the next step. The alignment returned by MMseqs2 is characterized by this summary graph, where we would like to have more green-cyan-bluish tones at the top and ideally a flatter black profile, although this doesn’t look too bad:Once the alignment is done (ideally one should download and explore that its sequences are good enough, but this is hard because MMseqs2 returns very large numbers of hits) I move on to the next module of the notebook, where I start AlphaFold2. While it runs, the notebook allows me to monitor each of the 5 predicted models as they are produced:At the end of the AlphaFold run I can inspect each of the 3D models right there thanks to a 3Dmol plugin. Notice that each of the 5 models comes out with an average predicted LDDT score, which is used to rank the models. In this case all 5 models are quite good all along the sequence (indicated by high pLDDT values), and they are all very similar to each other with quite high average pLDDT. The PAE plots do reveal however some uncertainty in the relative position of the two protein chains that make the dimer (shades of white to red in the Predicted Aligned Error plots):Refinement of the models with molecular simulationsThe models produced by AlphaFold or any other modeling method can have various problems such as clashes between atoms, unfulfilled interactions, etc. When models are predicted to be of high quality, it then makes sense to further improve them through molecular dynamics simulations. Briefly, in such simulations the protein models are described at full-atom level, sometimes even with simulated water molecules around, and allowed to fluctuate under realistic physics at a given temperature and pressure. The aim is to remove any unrealistic clashes, optimize geometries, and satisfy interactions and packing, especially of the amino acid side chains.There are several programs that allow running molecular dynamics simulations. Users could download all models and run these simulations in their local computers with their own methods and pipelines. But ColabFold allows users to run the simulations right in the notebook, provided equilibrated versions of the models in-place.Closing wordsThe coupling of MMseqs2 to AlphaFold2 within Google Colab provides free access to the state of the art of protein structure prediction, without the need of any specialized, expensive hardware, nor any software installs. As I discussed here, this is poised to revolutionize biology, and the best is that it is accessible to researchers all around the world, for free.Links to literature, code and notebooksThe preprint at bioRxiv: https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1.full.pdfColabFold GitHub, with links to all release and prototype notebooks: https://github.com/sokrypton/ColabFoldMMseqs2 in Github: https://github.com/soedinglab/MMseqs2MMseqs2 paper: https://www.nature.com/articles/nbt.3988Previous stories by me on AlphaFold2:Liked this article and want to tip me? [Paypal] -Thanks!I am a nature, science, technology, programming, and DIY enthusiast. Biotechnologist and chemist, in the wet lab and in computers. I write about everything that lies within my broad sphere of interests. Check out my lists for more stories. Become a Medium member to access all stories by me and other writers, and subscribe to get my new stories by email (original affiliate links of the platform).",18/08/2021,0,20.0,13.0,903.0,565.0,4.0,2.0,0.0,26.0,en
4030,Neural Image Retrieval,Medium,Eitan Kosman,1.0,6.0,615.0,"Assume you have an image I and an image database X containing thousands of other images. You want to find a subset S⊆X containing images that are most similar to I. This is a task called image retrieval. However, before solving this, you may ask yourself, what is the meaning of similar images? Is it based on the colors in the images? Or maybe the content? In the second case, two images containing dogs could be considered similar regardless of their breed, which obviously may have different colors. In this post, I will describe a simple implementation of this. The implementation is based on neural networks and is done by comparing the similarity between the embeddings of the two images.Given a query image I, I extract the features using VGG-19 and use the output of the first fully connected layer as the features I compare distances between. Below is an image describing the architecture of this network. As can be seen, the features will be 4096-dimensional vectors.The database I’ll use is the train-set of CIFAR-100 (https://www.cs.toronto.edu/~kriz/cifar.html) which contains 50K images from 100 classes of various animals, flowers, vehicles, people, etc.The first thing I’ll do is importing all the necessary libraries for this projectFirstly, I will describe how I search for the k closest points. I create a class for string all the features of my database and the corresponding images. If the database is too large, you may not be able to store all this data in the memoryAs we conisder KNN, the fit method simply stores the data. All the hard work is done in the predict method. Given features of the images used as queries, we calculate the l_2 norm of difference between these features and every features of every image in the databse. The norm calculation is done by np.linalg.norm from numpy. Later, I sort all the distances based on their value and return the indexes of images of the k closest ones.For features extraction, I will use the mentioned VGG-19 architecture, pre-trained on image-net. It is publicly available in torchvision. For simplicity, I create a class that defines the model.The first line loads a pre-trained model from torchvision.You can find other pre-trained models in https://pytorch.org/docs/stable/torchvision/models.html. The second line removes the last fully connected layers. Lastly, transfer the model to our GPU and use theeval()mode.I will assume the query images are stored in a directory, because I want to use the built-in torchvision’s loader.The following method will extract the features of the images. It is supplied with a model (e.g. our VGG-19) and an image loader (whether the query images or the CIFAR-100 loader).Now that we have all the code components, we’re ready to run the queries. For this project, the query images are stored in a directory called test_images. All the images are randomly selected from google search.Get both loaders and model and extract the features. Later, use the KNN class to retrieve the 7 nearest neighborsThe following function will show the query image together with its neighbors from the databseFinally, call this function for each query imageThe neighbors this algorithm retrieved looks pretty much similar to the query images. That means the the embedding I used is a good representation of the images in these domains. However, for other domains it might be less representative. In addition, one may choose to use other layers (or a combination of those) as embeddings.In this post I’ve showed you a simple algorithm for image retrieval, using neural networks. I hope you enjoyed this and learned something useful :)[1] Implementing Content-Based Image Retrieval with Siamese Networks in PyTorch[2] Neural Codes for Image Retrieval[3] Content Based Image Retrieval Using Embedded Neural Networks with Bandletized Regions",04/07/2020,9,0.0,0.0,720.0,344.0,15.0,0.0,0.0,4.0,en
4031,Top 20 Must-Watch Artificial Intelligence movies,Towards Data Science,Benedict Neo,4000.0,14.0,2549.0,"Movies are more than just blockbusters hit with explosions and superpowers, it’s the main idea behind the movie that changes people and injects a notion in the viewer’s head.To illustrate, the movie Joker wasn’t a hero vs villain film, fighting with superpowers and wreaking havoc on New York City. It portrayed how there is a distinct chasm between the rich and the poor, the lucky and the unlucky, and how mental illness can distort a person’s morality and value system.So, movies are more than just an activity for enjoyment and amusement, it plays an imperative role in shaping our view on the world and communal consciousness.In short, movies educate people and spread ideas in ways a paperback book early does today.One reason for the effectiveness of movies is also the stark difference between movies and books, which is visualization. A visual content displays content that quickly registers to our visual stimulus as compared to a book, which explains why people spend hours on YouTube instead of reading a book. Moreover, movies help generate concrete graphics that help people hold on to the idea for a longer time. Evidently, movies are the perfect medium to learn about an idea or to dive into a topic with your head first.Artificial Intelligence, Machine Learning, Data Science are popular terms in this era. And knowing what it is and its value in the coming years is more crucial than ever.Although Hollywood movies tend to exaggerate for dramatic effects, such as the Terminator movie, where killer robots from SKYNET turn sentient and start destroying humans, there is still some truth in these movies. There are experts in the field today who fear a super-intelligent AI who will outsmart us and eventually decide to wipe us off the face of the Earth.On the other side of the spectrum, AI is depicted as a miracle invention, the intelligent machine that can perform analytical tasks such as simulations and predictions much better than humans. In the movies, you see AI as a helper to humans, assisting us in all kinds of tasks, from being a sidekick on space adventures to a lover.If you’re interested in AI, Machine Learning and Data Science, check out the list of movies (in no particular order) below and watch them.I write articles on Data Science and AI. If you want to be notified when I post, subscribe to my medium newsletter now!If you’re not a medium member, consider subscribing today for only just $5. You’ll be supporting me directly and you’ll get access to tons of great writing on medium!Blade Runner is a classic dystopian movie where bio-engineered replicas of humans powered by AI live amongst real humans and the only distinction between them is they only live for 4 years. The sequel for this film, Blade Runner 2049 is also a great film that dives even deeper into the topic of AI and its future.www.imdb.comWestworld is a movie about how AI can be used to entertain us and allow us to live out our fantasies. It reminds us of the fine line where technology and ethics collide and makes us question ourselves.www.imdb.comHAL 9000, a supercomputer on a spacecraft heading to Jupiter is in charge of most operations, which leads to terrifying events when it glitches and decides to destroy the world. As HAL kills off the crew one by one, we are reminded of the dangers of letting AI take control of our world and be mindful of the consequences.www.imdb.comMoon is similar to 2001: Space Odessy, as this movie has its own HAL 9000, named GERTY. GERTY is a computer and companion of Sam Bell, a lone astronaut on the face of the Moon. As his term on the Moon comes to an end, he faces an accident and meets his younger self. The movie’s minimalistic quality in addition to the philosophical and thought-provoking scenes makes it one of the best sci-fi films out there.www.imdb.comThe Matrix the quintessential dystopia where humanity’s fear of superintelligent AI becomes a reality. In “The Matrix”, a simulation of reality created by AI to keep us subdued, Neo meets Morpheus, which sets him free from the simulation and starts a new chapter in his life where he battles the intelligent begins in the form of secret agents and discovers his true identity. The Matrix gives us a glimpse of the grim future of the dark side of AI as well as a few life lessons to ponder upon.www.imdb.comThe famous droid duo in Star Wars — The beloved C3-PO and his fellow droid friend R2D2 — are both sentient AI droids that have human-like personalities and are capable of emotions. The interesting thing is that they are treated as pets, or even slaves. This is akin to a relationship between a dog and a man, except the dog can speak and express its opinions. This take on Artificial Intelligence by Star Wars makes us think about how they will be embedded into the fabric of our society and how society will treat AI in the future either as helpers, pets, friends or not trust them at all.www.imdb.comThe AI star of the movie is Lieutenant Commander Data, a sentient, self-aware android who serves as a senior officer aboard the USS Enterprise. This AI-powered super brain is also equipped with an emotion chip that simulates human emotions. This makes Data a human-like droid that can feel like us, but with a sophisticated brain that can calculate risks better than any human.www.imdb.comThe Terminator is one of the most popular movies when it comes to portraying the bleak future of a world where AI suddenly turns evil and starts killing everyone. It illustrates the possibility of AI becoming an existential threat. In this film, Skynet, an AI system that sparks a nuclear holocaust sends back a cyborg assassin (The Terminator) to prevent the birth of John Connor who will spark a rebel group that fights against Skynet. This movie serves as a reminder to create an AI that is safe and to think twice about creating an AI that can wipe us out.www.imdb.comRobocop is said to be a movie that truly unmasks the ethics of AI. This is because Robocop is an amalgamation of AI and human, which proves the fact that morality and ethics cannot be artificial or automated. Our underlying humanity still needs to be a part of the equation or consequences will follow. This relates to the dangers of weaponizing AI in the military (lethal autonomous weapon systems) and why regulations of AI use must be set in place to prevent misuse of AI.www.imdb.comWALL-E is the epitome of Artificial Narrow Intelligence that we see today, though not as sophisticated as the ones in the movie (WALL-E being capable of emotions such as loneliness and love). Narrow AI is an intelligent system that is very good at doing a specific thing. We see that in our everyday life today, such as self-driving cars and voice assistants. In this movie, WALL-E cleans garbage, EVE looks for life and Auto pilots the ship. WALL-E is one of the rare sci-fi films out there that shows the bright side of AI and the good it can bring to the world.www.imdb.comAva is an AGI (Artificial General Intelligence) created by Caleb Smith, a genius programmer. AGI is an AI that possesses substantially greater skills and knowledge than a human being. Ava can converse just as well as we do and even manipulate the emotions of others. This film raises ethical concerns on treating and experimentation of self-aware robots such as Ava, which leads to a dark ending.www.imdb.comSet in a tyrannical future, the police force is now mechanized with autonomous droids that patrol the streets and handles the lawbreakers. One of the droids is stolen and given an upgrade from ANI to AGI, meaning it becomes sentient. This droid, Chappie with his newfound emotion, battles with the corruption in society and is labeled as a threat to mankind an order. This brings up the issue of ethics as well, and how AGI can learn by itself by exploring its surroundings, just like Chappie does in the film.www.imdb.comVIKI (Virtual Interactive Kinetic Intelligence) is a supercomputer turned evil that uses data collected from around the world and its computational powers control robots all over the world. In this world, robots are everywhere and they are given laws embedded into their system that ensure the safety of society. But with VIKI, those laws no longer hold the robots back, and it's up to a technophobic cop and a good robot stop VIKI from ending the world. This film brings about the harrowing possibility of AI taking over the world and how even laws set in place to control them can backfire.www.imdb.comInterstellar takes a completely different approach to present AI in a sci-fi film. Instead of making it the antagonist such as HAL 9000 or The Terminator, the pair of quadrilateral robots named TARS and CASE which surprisingly takes on a form that has no resemblance to the popular human-like form as seen in Star Wars and Star Trek. This design of TARS and CASE represent the design that puts function above humanness, and it represents the many possibilities of complex designs in robotics and AI.www.imdb.com“Her” is about a man falling in love with an artificially intelligent OS. We see virtual assistants today in our smartphones, but the technology is not as sophisticated as “Samantha” the smart OS in “Her”. Voice assistants like Siri and Google Assistant are only able to do simple tasks such as setting an alarm or texting a friend, but Samantha is an impressive conversationalist with an amazing command of language, common sense, emotion and is able to handle complex tasks such as filtering emails and downloading millions of books in a split of a second. A highly advanced operating system like her is able to learn through experiences and evolve, something that is not achievable with the level of technology we have today. This film gives a glimpse of how voice assistants will be like in the future and how we can even fall in love with them.www.imdb.com“Moneyball” shows the amazing power of predictive analytics and its capabilities in the real world. In this film, data analytics and the “Moneyball theory” was used to pick the best team of undervalued players under a minimal budget. Through the power of data mining on players, as well as two significant data points (slugging percentage and on-base percentage), a champion baseball team was formed. This film shows the importance of data in decision making and choosing the right statistics in predictive modeling. But more importantly, is the courage to make decisions based on analytics, and believing in the data to make a change.www.imdb.com“Margin Call” demonstrates the impacts of erroneous predictive modeling trickles down to everyone in society. Evidently, technology does not live in a bubble and a tiny mistake can lead to terrible consequences. This film centers on the 2008 financial crisis, where risk analysts and financial analysts working in banks discover that their risk model is leading the firm to a financial catastrophe. This film teaches about the dangers of greed and how crucial it is to failproof every side of a predictive model to prevent catastrophes from happening.www.imdb.com“Minority Report” displays data science at its best. In this film, a team of humans with psychic abilities or “Data scientists” called PreCogs is able to predict future crimes by analyzing massive amounts of data. With this analysis, the visual data is transferred to the PreCrime, a police unit that is sent to prevent the crime. This brings up the idea of how data can be used to do great things in the real world, like preventing disasters and saving millions of lives.www.imdb.comAlan Turing, the creator of the well-known “Turing Test” and father of modern computer science is featured in this movie. Set in WWII, Alan Turing is the legendary mathematician who cracks the enigma. The enigma is a strategic code that the Nazis use to encode their messages, and Turing decided to build a computer that is able to perform complex permutations faster than any human being could. With this computer, Turing paved the way for creating machines as well as enforcing the field of cryptography and cybersecurity. Moreover, an important lesson in the film is the importance of creativity and novelty when solving an “enigma” as well as the understanding of communication and human behavior which are key components to solving any problem.www.imdb.com“A Beautiful Mind” features John Nash, a Nobel laureate in economics. He is famous for his contributions to the game theory or a.k.a the economic theory, which resulted in breakthroughs in the field of economics. His work has been utilized heavily in computer science. In this film, see how game theory is slowly being unraveled and discover its significance, not just the mathematical world, but in everyday life.www.imdb.comJust an extra one for the irony, “21” is a true story about 6 students from MIT heads to Las Vegas to count cards using secret codes, hand signals, and a sharp eye. This movie shows the power of predictive analysis and teaches the viewers how data-based decisions can be profitable for any business.I hope that through these amazing sci-fi films, you are able to be fully engrossed into a world where AI becomes a friend, a lover, a killer, a soldier, a slave, and discover the power of mathematics and algorithms and its ability to achieve amazing things like stopping a war and winning big in a casino.Artificial Intelligence is truly the greatest invention of humanity. As seen in the movies, AI will either be the death of us, or an augmentation of our human cognitive and physical abilities, allowing us to do incredible things.AI will be everywhere in the near future, and the first step towards learning about it is to bust the myths around AI.One of the most important myth is that AI will become conscience and turn evil, killing every human in its path, even though that is unnerving, the real worry is that AI will have goals which are not aligned with ours. When that happens, there won’t be much we can do to stop them.Another big myth is that Super AI is coming soon and we should all be worried now. The truth is, no one, not even the most intelligent experts in the field right now knows when Super AI will be achieved. It could be decades, centuries, or even never.The movies always portray AI as an intimidating robot figure that shoots lasers and is indestructible. However, an AI won’t need a body to do its worst, all it needs is a place on the internet, and all hell can break loose.I hope you found this article useful and that you got an amazing “Movies to watch list”! Stay safe at home and all the best.To end, here is a quote about AGI by Elon Musk“I’m increasingly inclined to think that there should be some regulatory oversight, maybe at the national and international level, just to make sure that we don’t do something very foolish. I mean with artificial intelligence we’re summoning the demon.” — Elon Musk warned at MIT’s AeroAstro Centennial Symposiumtowardsdatascience.commedium.comtowardsdatascience.comIf you want to be updated with my latest articles follow me on Medium.Other means of contact:Happy streaming!",09/04/2020,0,0.0,0.0,1400.0,906.0,3.0,2.0,0.0,44.0,en
4032,Transfer Learning in TensorFlow on the Kaggle Rainforest competition,Medium,Luuk Derksen,706.0,6.0,1109.0,"When I first noticed the Kaggle competition: “Planet: Understanding the Amazon from space” I was immediately thinking of trying out Transfer Learning using a pre-trained model. I had never really played with Transfer Learning before so I thought this would be a good one to try it out on. Transfer Learning is described by Wikipedia as:“a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem”where in this case the ‘relatedness’ of the problem is that both the Kaggle competition and the pre-trained model(s) are addressing computer vision problems. For more information on Transfer Learning there is a good resource from Stanfords CS class and a fun blog by Sebastian Ruder.The model I choose to use is the ResNet50 model that was developed by Microsoft (original paper can be found here) and that “won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.” As can be seen from the original github repository the ResNet50 model is one of a number of models (some others are ResNet101, ResNet152) where the number denotes the depth of the model. For more information about how residual networks work recently Vincent Fung wrote this amazing writeup of the architecture and the recent innovations around it.So lets see how we can get the pre-trained model, implement it in TensorFlow and use it to do some transfer learning!Note: this notebook is very much for myself to keep track of what I did and why I did some steps. But otherwise: enjoy and I hope its usefulKeras has a number of implementations of the well known vision models in its github repository, but I wanted to try and build it myself to properly understand how to load weights into a TensorFlow graph. In the Keras implementation of ResNet50 it has a link to the weights it uses:So those weights are the ones I downloaded to get started.To load the data using python you will need to use the hp5y library:The data_h5 object now holds all the weights and is accessible like you would access a dictionary. To get all the keys that actually hold weights (the activation functions will not for example) we’ll see if their dictionary holds any keys:Each of these relates to a tensor and holds the variables associated with that tensor. To see what variables it holds use the key and:which shows you that this is a tensor for Batch Normalization. You can now also inspect the shape or values of the tensor by:Perfect, this gives us all the information we need to access the variables and load them into the graph. All we need is the graph itself.To reconstruct the graph itself I first followed visualization by netscope which will give you all the information on the blocks you need to replicate the network. The network consists of two big building blocks: a convolutional block and an identity block that are both built up out of blocks of convolutional layers and batch normalization layers.Before we can build the bigger blocks of the network we first need be able to easily create the convolution and batch-normalization layers.Notice how we are passing the data and the layer_name into the layers. This allows us to use the pre-trained weights (because that is using the layer_name as the keys for its dictionary) and set those as constants for the network (essentially creating a static network that you can not retrain since the constants are not variables.Now that we have those two building blocks we can create the larger blocks. Lets start with the Identity block:Here we are passing a stage variable because we can then dynamically generate the layer_name from the stage. Similar to the Identity block we can now also create a function to provide us with the Convolutional block:and this gives us all the building blocks we need to configure the actual model:I have put a full notebook online that has all the code shown. I also tried out the model on some images to see if it is working properly:So, to the main bit: how can we now leverage this pre-trained network to use it on a different problem set (i.e., satellite images of the rainforest)? Can we use all or some of the layers of the original ResNet50 model and have it output the different classes for the satellite images…From reading bits and pieces of how one could do transfer learning I assumed I had a few options to try out:I thought the last option made most sense. Probably the network would benefit from keeping some of the early layers constant, but retraining the last stage of the network for example. To do this I had to augment my code from above and allow for locking a layer (i.e., having constant variables) or setting them as trainable variables so they can be updated. To do this I modified the convolutional layer and batch normalization layer to:where I have added a lock argument that indicates if you want to lock the layer and use constants or if you want the layer to be trainable and use variables. In addition I modified the blocks to be passing on the lock argument:So now we can pass on per stage/block if we want to make those layers available for fine-tuning. To do this I separated the learning into the fine-tuning and all the rest of the network that I added myself. To do this I am grabbing the variables separately for the stage-5 and the final stages (the part I added):One final tweak I made was to pre-calculate the stage-4 output for all images in the dataset so that I would not have to feed-forward every image through the network for all the epochs. This saved me a lot of time during the training but has one massive downside: you can not do any image augmentation (e.g., rotation, flipping, brightness) on the fly, you would need to do that during the pre-calculation and that would mean saving the images multiple times. So I ended up training without any image augmentation and with only a few added layers and a lot of dropout in between those to avoid overfitting.In the end it gave me a leaderboard score of 0.92125 on a single model, which is not too bad I guess without image augmentation. I did not explore further training as I assumed the competition would be won by (big) ensembles with heavy image augmentation, like the solution described by the team Urucu who finished at place 13 overall, and I would not stand a chance anyways ;).",31/07/2017,5,1.0,15.0,1400.0,527.0,1.0,1.0,0.0,16.0,en
4033,Clustering on Mixed Data Types in Python,Analytics Vidhya,Ryan Kemmer,28.0,5.0,777.0,"During my first ever data science internship, I was given a seemingly simple task to find clusters within a dataset. Given my basic knowledge of clustering algorithms like K-Means, DBSCAN, and GMM I thought that I could easily get this task done. However, as I took a closer look into the dataset, I realized the data contained a mixture of categorical and continuous data, and many common methods of clustering I knew would not easily work.Categorical data consists of multiple discrete categories that commonly do not have any clear order or relationship to each-other. This data might look like “Android” or “iOS”.Continuous data consists of real numbers that can take any value. This data might look like “3.14159” or “43"".Many datasets contain a mixture of categorical and continuous data. However, it is not straightforward how to cluster datasets with mixed data types. So how do we cluster on data that has both categorical and continuous features? Lets take a look at two simple ways to approach this problem using Python.In this post, I am going to cluster a small dataset I created that has a mixture of categorical and continuous features. My fake data represents customer data that might be used to understand customers of an E-commerce website/app. Our fake dataset will have 4 features:Here is the code used to generate our fake data.Here is what our fake dataset looks like. Now lets get our hands dirty and do some clustering!The first clustering method we will try is called K-Prototypes. This algorithm is essentially a cross between the K-means algorithm and the K-modes algorithm.To refresh our memory, K-means clusters data using euclidean distance. Meanwhile, K-modes clusters categorical data based off the number of matching categories between data points. A mixture of both of these: the K-prototype algorithm, is just what we need to cluster our fake customers!First, lets normalize the continuous features in our data to ensure that one feature is not interpreted as being more important than the other.Now, lets use the K-prototypes algorithm to cluster our data. My implementation of the algorithm is very simple, with 3 clusters and the Cao initialization. When training the model, we specify which columns in the data are categorical (columns 0 and 1). More information on implementing and fine tuning this algorithm can be found here: https://github.com/nicodv/kmodes.Finally, lets make some swarm plots to see how our clustering performed.It worked! It looks like we have 3 fairly distinct clusters. Our first cluster seems to represent users that spent a lot of time on the website. Our second cluster has mostly Android users who have medium/ low time spent. Finally, our third cluster has mostly iOS users who also have medium/ low time spent.An alternative to using the K-prototypes algorithm is using the K-means algorithm and one hot encoding categorical variables. One hot encoding involves creating a new column for each categorical value in the dataset. Then a 1 or 0 is assigned depending on if that categorical value is in the data or not. Lets one hot encode the categorical values in our dataset using the pandas “get_dummies” function.Now, our data looks like this. COOL! Time to do more clustering.Lets cluster this data using a basic implementation of K-means, and make some more swarm plots to see how this method works.Okay! It is interesting to see how much different these clusters look from the previously shown K-prototypes clusters. At first glance, it looks like the categorical features in our data contributed significantly more to our clusters, and our continuous features did not contribute much at all. The first cluster contains iOS users who DO NOT use AT&T. Our second cluster contains iOS users who DO use AT&T. Finally, our third cluster contains all Android users. Meanwhile, Age and Time Spent have a large distribution in all clusters.In this post, we looked at two different methods of clustering mixed categorical/continuous data in Python. To begin with, we implemented the K-prototypes algorithm, an algorithm specifically designed to cluster mixed data using a combination of the K-means/K-modes. As an alternative, we tried using the K-means algorithm with one hot encoding.Using our fake dataset, there are significant differences in the clusters determined by these two methods. K-prototypes seemed to evenly consider categorical and continuous features. Meanwhile, K-means seemed to weigh categorical features MUCH more heavily, which would likely be undesirable.How do these methods work with your mixed dataset? Lets discuss!Github repo with all code/visuals here: https://github.com/ryankemmer/ClusteringMixedData[1] de Vos, Nelis J. kmodes categorical clustering library. 2015–2021, https://github.com/nicodv/kmodes[2] Huang, Z.: Clustering large data sets with mixed numeric and categorical values, Proceedings of the First Pacific Asia Knowledge Discovery and Data Mining Conference, Singapore, pp. 21–34, 1997.",25/01/2021,5,25.0,12.0,1306.0,528.0,5.0,1.0,0.0,3.0,en
4034,Insight into Faster R-CNN for Object Detection.,Analytics Vidhya,Haripriya Reddy,24.0,6.0,936.0,"Faster R-CNN is an object detection architecture presented by Ross Girshick, Shaoqing Ren, Kaiming He and Jian Sun in 2015, and is one of the famous object detection architectures that uses convolution neural networks.It detects and classifies objects in an image as shown below:Before diving into Faster R-CNN let’s learn about R-CNN, Fast R-CNN and RPN which are the building blocks for Faster R-CNN.R-CNN is one among the first architectures used for object detection.In R-CNN we first try to construct different proposed regions. Proposed region is the area on the image which has high probability of containing the object.You must be wondering how we construct these proposed regions in R-CNN!To construct proposed regions we use External region proposal methods like Selective Search.Selective Search Algorithm:1. Generate initial sub-segmentation, we generate many candidate regions2. Use greedy algorithm to recursively combine similar regions into larger ones 3. Use the generated regions to produce the final candidate region proposalsLets understand the working of R-CNN with the help of the image below:In R-CNN, we feed the input image to, region proposal algorithm like selective search and perform CNN on each proposed region. The output of CNN is given to SVM’s for classification of objects detected. So if we have 2000 proposed regions, we need to run 2000 CNN networks!!This model is an improvement over R-CNN.It is 25x more effective than R-CNN. To reduce the overhead of multiple CNN networks in R-CNN, we first feed the input image to CNN which gives an insight on the features in the image and then perform selection search to get proposed regions.The Below images explains it all!Once we have generated the Region of Interest(RoI) using selective search , we feed it to RoI pooling layer, which in turn is connected to Fully Connected Layers (FC’s) and Classification is performed using the softmax classifier.What is RoI pooling layer?The proposed Regions generated by selective search are of various dimensions and cannot be fed to FC’s directly. To facilitate this, We take the RoI’s and stack them to a common dimension which forms the RoI pooling layer and then these features are given to the fully connected layer.RoI Pooling splits the input feature map into a fixed number (let’s say k) of roughly equal regions, and then apply Max-Pooling on every region. Therefore the output of RoI Pooling is always k regardless the size of input.RPN provides a time effective way of generating region proposals/regions of interest. It is more effective than selection search used in R-CNN/Fast R-CNN. RPN ranks region boxes (called anchors) and proposes the ones most likely containing objects.Anchors play an important role in Faster R-CNN. An anchor is a box. In the default configuration of Faster R-CNN, there are 9 anchors at a position of an image.For each location, k (k=9) anchor boxes are used (3 scales of 128, 256 and 512, and 3 aspect ratios of 1:1, 1:2, 2:1) for generating region proposals. Look at the figure below:Suppose we consider an image with dimensions (600*100) and choose one position at a stride of 16, then we will have 2400 (40*60)anchors. This leads to 21600 (2400*9) anchors to consider. 21600 is still huge to examine!Isn’t it? Can this count be reduced?Yes! There are two premises based on which we can reduce the anchors to be considered.2. Apply NMS (Non-Maximum Suppression) : Suppose we consider the figure shown below, where the blue box represents the ground truth and red&green boxes represent anchor. We see that red & green anchors have greater area of intersection among them , and IoU for green anchor is less than that of red, hence the green anchor can be ignored. This mechanism is known as NMS.Doing this reduces the anchors further down to 2000!How to decide if an anchor under consideration contains object?We use the concept of Intersection Over Union (IoU) to decide if an anchor is good enough to be considered for object detection. IoU is defined as the ratio of Area of overlap to Area of Union. If IoU value is greater than 0.7 then the anchor has the object. If IoU value is less than 0.3 then anchor does not contain the object and not considered for object detection.In the above diagram green box represents the ground truth bounding box and the red box represents anchor.Note:- In Faster R-CNN we use 256 positive anchors and 256 negative anchors. An anchor which has object is called positive anchor. An anchor which doesn’t contain the object is called negative anchor.Faster R-CNN is the most effective approach for object detection. It is 250x more effective than R-CNN. Having read Fast R-CNN and RPN, it must be easy to grasp Faster R-CNN. Faster R-CNN is an improvement over Fast R-CNN where in, RPN is used as proposed region generator instead of selective search. Faster R-CNN is a combination of Fast R-CNN and RPN. In Faster R-CNN, RPN acts as the region proposer and Fast R-CNN acts as a detector. The Proposed regions obtained are given to a classifier for object classification.Faster R-CNN investigates two CNN networks:1. ZFnet (ILSVRC 2013 Winner)2. VGG-16 (Runner up in ILSVRC 2014)In Faster R-CNN the input image is fed to the convolution neural network, and the resulting feature map is given to the RPN where we use 9 anchors of different scales and aspect ratios as discussed in the above section. The resulting feature vector of RPN is fed to Softmax and Regressor for region proposals.The resulting layer of RPN is given to RoI pooling layer which is connected to Fully Connected layers and help in classification of objects.Comparison between R-CNN , Fast R-CNN and Faster R-CNN.",23/01/2020,0,20.0,0.0,605.0,468.0,12.0,1.0,0.0,0.0,en
4035,Intelligent Agent Based Wastewater Management System,mutsuda,Masumi Mutsuda,113.0,2.0,245.0,"Fa dos anys, a l’assignatura d’AIA (Aplicacions de la Intel·ligència Artificial), ens van fer implementar un sistema intel·ligent que dominaria tot el procés de depuració d’aigua de Catalunya. Les diferents plantes havien de ser intel·ligents i tenir suficient coneixement del seu entorn com per decidir, entre elles, de quina manera actuar en cas de detecció d’un contaminant, pluja torrencial, etc. Elles soles decidien mitjançant diverses polítiques què fer en cadascuna de les situacions per tal de resoldre els problemes.Les plantes entre si es comunicaven mitjançant missatges en format d’ontologia, que ve a ser una representació lògica del context en què s’està treballant. En aquest cas l’ontologia contenia informació sobre els tòxics, l’aigua, així com informació sobre les connexions entre plantes.Per tal de fer la simulació (ja que lògicament era només una pràctica i no ho vam dur més enllà amb plantes reals…) vam dissenyar una interfície gràfica on hi havia representades totes les plantes de tractament de Catalunya en què podies interactuar amb elles fent ploure, abocant residus, etc.Un cop seleccionada una planta (o una indústria), es podien provocar pluges torrencials, abocaments residuals, i visualment podies observar com les diferents plantes es comuniquen entre sí per tal de poder gestionar l’imprevist.Tot plegat una pràctica molt interessant en què pots veure realment la utilitat d’algunes de les coses que aprens al llarg de la carrera!Us enllaço el document, el codi i el programa per si esteu cursant AIA o simplement teniu curiositat.Documentació — Programa — Codi",23/03/2012,0,1.0,0.0,556.0,505.0,5.0,0.0,0.0,3.0,ca
4036,Springer has released 65 Machine Learning and Data books for free,Towards Data Science,Uri Eliabayev,1100.0,4.0,530.0,"Hundreds of books are now free to downloadSpringer has released hundreds of free books on a wide range of topics to the general public. The list, which includes 408 books in total, covers a wide range of scientific and technological topics. In order to save you some time, I have created one list of all the books (65 in number) that are relevant to the data and Machine Learning field.Among the books, you will find those dealing with the mathematical side of the domain (Algebra, Statistics, and more), along with more advanced books on Deep Learning and other advanced topics. You also could find some good books in various programming languages such as Python, R, and MATLAB, etc.If you are looking for more recommended books about Machine Learning and data you can check my previous article about it.The Elements of Statistical LearningTrevor Hastie, Robert Tibshirani, Jerome Friedmanhttp://link.springer.com/openurl?genre=book&isbn=978-0-387-84858-7Introductory Time Series with RPaul S.P. Cowpertwait, Andrew V. Metcalfehttp://link.springer.com/openurl?genre=book&isbn=978-0-387-88698-5A Beginner’s Guide to RAlain Zuur, Elena N. Ieno, Erik Meestershttp://link.springer.com/openurl?genre=book&isbn=978-0-387-93837-0Introduction to Evolutionary ComputingA.E. Eiben, J.E. Smithhttp://link.springer.com/openurl?genre=book&isbn=978-3-662-44874-8Data AnalysisSiegmund Brandthttp://link.springer.com/openurl?genre=book&isbn=978-3-319-03762-2Linear and Nonlinear ProgrammingDavid G. Luenberger, Yinyu Yehttp://link.springer.com/openurl?genre=book&isbn=978-3-319-18842-3Introduction to Partial Differential EquationsDavid Borthwickhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-48936-0Fundamentals of Robotic Mechanical SystemsJorge Angeleshttp://link.springer.com/openurl?genre=book&isbn=978-3-319-01851-5Data Structures and Algorithms with PythonKent D. Lee, Steve Hubbardhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-13072-9Introduction to Partial Differential EquationsPeter J. Olverhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-02099-0Methods of Mathematical ModellingThomas Witelski, Mark Bowenhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-23042-9LaTeX in 24 HoursDilip Dattahttp://link.springer.com/openurl?genre=book&isbn=978-3-319-47831-9Introduction to Statistics and Data AnalysisChristian Heumann, Michael Schomaker, Shalabhhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-46162-5Principles of Data MiningMax Bramerhttp://link.springer.com/openurl?genre=book&isbn=978-1-4471-7307-6Computer VisionRichard Szeliskihttp://link.springer.com/openurl?genre=book&isbn=978-1-84882-935-0Data MiningCharu C. Aggarwalhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-14142-8Computational GeometryMark de Berg, Otfried Cheong, Marc van Kreveld, Mark Overmarshttp://link.springer.com/openurl?genre=book&isbn=978-3-540-77974-2Robotics, Vision and ControlPeter Corkehttp://link.springer.com/openurl?genre=book&isbn=978-3-319-54413-7Statistical Analysis and Data DisplayRichard M. Heiberger, Burt Hollandhttp://link.springer.com/openurl?genre=book&isbn=978-1-4939-2122-5Statistics and Data Analysis for Financial EngineeringDavid Ruppert, David S. Mattesonhttp://link.springer.com/openurl?genre=book&isbn=978-1-4939-2614-5Stochastic Processes and CalculusUwe Hasslerhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-23428-1Statistical Analysis of Clinical Data on a Pocket CalculatorTon J. Cleophas, Aeilko H. Zwindermanhttp://link.springer.com/openurl?genre=book&isbn=978-94-007-1211-9Clinical Data Analysis on a Pocket CalculatorTon J. Cleophas, Aeilko H. Zwindermanhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-27104-0The Data Science Design ManualSteven S. Skienahttp://link.springer.com/openurl?genre=book&isbn=978-3-319-55444-0An Introduction to Machine LearningMiroslav Kubathttp://link.springer.com/openurl?genre=book&isbn=978-3-319-63913-0Guide to Discrete MathematicsGerard O’Reganhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-44561-8Introduction to Time Series and ForecastingPeter J. Brockwell, Richard A. Davishttp://link.springer.com/openurl?genre=book&isbn=978-3-319-29854-2Multivariate Calculus and GeometrySeán Dineenhttp://link.springer.com/openurl?genre=book&isbn=978-1-4471-6419-7Statistics and Analysis of Scientific DataMassimiliano Bonamentehttp://link.springer.com/openurl?genre=book&isbn=978-1-4939-6572-4Modelling Computing SystemsFaron Moller, Georg Struthhttp://link.springer.com/openurl?genre=book&isbn=978-1-84800-322-4Search MethodologiesEdmund K. Burke, Graham Kendallhttp://link.springer.com/openurl?genre=book&isbn=978-1-4614-6940-7Linear Algebra Done RightSheldon Axlerhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-11080-6Linear AlgebraJörg Liesen, Volker Mehrmannhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-24346-7AlgebraSerge Langhttp://link.springer.com/openurl?genre=book&isbn=978-1-4613-0041-0Understanding AnalysisStephen Abbotthttp://link.springer.com/openurl?genre=book&isbn=978-1-4939-2712-8Linear ProgrammingRobert J Vanderbeihttp://link.springer.com/openurl?genre=book&isbn=978-1-4614-7630-6Understanding Statistics Using RRandall Schumacker, Sara Tomekhttp://link.springer.com/openurl?genre=book&isbn=978-1-4614-6227-9An Introduction to Statistical LearningGareth James, Daniela Witten, Trevor Hastie, Robert Tibshiranihttp://link.springer.com/openurl?genre=book&isbn=978-1-4614-7138-7Statistical Learning from a Regression PerspectiveRichard A. Berkhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-44048-4Applied Partial Differential EquationsJ. David Loganhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-12493-3RoboticsBruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolohttp://link.springer.com/openurl?genre=book&isbn=978-1-84628-642-1Regression Modeling StrategiesFrank E. Harrell , Jr.http://link.springer.com/openurl?genre=book&isbn=978-3-319-19425-7A Modern Introduction to Probability and StatisticsF.M. Dekking, C. Kraaikamp, H.P. Lopuhaä, L.E. Meesterhttp://link.springer.com/openurl?genre=book&isbn=978-1-84628-168-6The Python WorkbookBen Stephensonhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-14240-1Machine Learning in Medicine — a Complete OverviewTon J. Cleophas, Aeilko H. Zwindermanhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-15195-3Object-Oriented Analysis, Design and ImplementationBrahma Dathan, Sarnath Ramnathhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-24280-4Introduction to Data ScienceLaura Igual, Santi Seguíhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-50017-1Applied Predictive ModelingMax Kuhn, Kjell Johnsonhttp://link.springer.com/openurl?genre=book&isbn=978-1-4614-6849-3Python For ArcGISLaura Tateosianhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-18398-5Concise Guide to DatabasesPeter Lake, Paul Crowtherhttp://link.springer.com/openurl?genre=book&isbn=978-1-4471-5601-7Digital Image ProcessingWilhelm Burger, Mark J. Burgehttp://link.springer.com/openurl?genre=book&isbn=978-1-4471-6684-9Bayesian Essentials with RJean-Michel Marin, Christian P. Roberthttp://link.springer.com/openurl?genre=book&isbn=978-1-4614-8687-9Robotics, Vision and ControlPeter Corkehttp://link.springer.com/openurl?genre=book&isbn=978-3-642-20144-8Foundations of Programming LanguagesKent D. Leehttp://link.springer.com/openurl?genre=book&isbn=978-3-319-70790-7Introduction to Artificial IntelligenceWolfgang Ertelhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-58487-4Introduction to Deep LearningSandro Skansihttp://link.springer.com/openurl?genre=book&isbn=978-3-319-73004-2Linear Algebra and Analytic Geometry for Physical SciencesGiovanni Landi, Alessandro Zampinihttp://link.springer.com/openurl?genre=book&isbn=978-3-319-78361-1Applied Linear AlgebraPeter J. Olver, Chehrzad Shakibanhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-91041-3Neural Networks and Deep LearningCharu C. Aggarwalhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-94463-0Data Science and Predictive AnalyticsIvo D. Dinovhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-72347-1Analysis for Computer ScientistsMichael Oberguggenberger, Alexander Ostermannhttp://link.springer.com/openurl?genre=book&isbn=978-3-319-91155-7Excel Data AnalysisHector Guerrerohttp://link.springer.com/openurl?genre=book&isbn=978-3-030-01279-3A Beginners Guide to Python 3 ProgrammingJohn Hunthttp://link.springer.com/openurl?genre=book&isbn=978-3-030-20290-3Advanced Guide to Python 3 ProgrammingJohn Hunthttp://link.springer.com/openurl?genre=book&isbn=978-3-030-25943-3",26/04/2020,0,65.0,0.0,825.0,510.0,1.0,0.0,0.0,65.0,en
4037,Using LSTMs to forecast time-series,Towards Data Science,Ravindra Kompella,982.0,7.0,990.0,"There are several time-series forecasting techniques like auto regression (AR) models, moving average (MA) models, Holt-winters, ARIMA etc., to name a few. So, what is the need for yet another model like LSTM-RNN to forecast time-series? This is quite a valid question to begin with and here are the reasons that I could come up with (respond below if you are aware of more, I will be curious to know)—On the other hand, there are the usual downsides that one needs to be careful about, while using LSTM’s (or any DNN architectures for that matter) — requirement of lots of data, multiple hyper-parameters to be tuned etc., I also came across few articles that mentioned that LSTM’s are not supposedly good at auto regression type of series. So take this with a pinch of salt.A simple sine-wave as a model data set to model time series forecasting is used. You can find my own implementation of this example here at my github profile. The core idea and the data for this example has been taken from this blog but have made my own changes to it for easy understanding.So how does our given data look like? Below is the plot of the entire sine wave dataset.A brief about the overall approach before we dive deep into details —2. use a two layered LSTM architecture coupled with a dense output layer to make a prediction.3. We will look at couple of approaches to predict the output — a.) Forecasting step by step on the test data set, b.) Feed the previous prediction back into the input window by moving it one step forward and then predict at the current time step.Now lets dive into the details —Data preparation —2. Fix the moving window size to be 50. For this purpose we use pandas shift function that shifts the entire column by the number we specify. In the below code snippet, we shifted the column up by 1 (hence used -1. If we want to shift it down by 1, we will have to use +1) and then concatenate that to the original data.I tried to illustrate this on a toy data set below as to how the above for loop works for a window_size of 3.Note — we dropped all the rows that contain the Nan values in the above code snippet.If you look at the toy data set closely, you can observe that this models the input data in the fashion we want to input into the LSTM. The last column in the above table becomes the target y and the first three columns become our input x1,x2 and x3 features. If you are familiar with using LSTM for NLP, then you can look at this as a fixed sequence of length 3 of sentence containing 3 words each and we are tasked with predicting the 4th word.3.Preparing the 3D input vector for the LSTM. Remember, the input vector for LSTM is 3D array: (num_samples, num_time_steps, num_features). In this case we have num of time steps = 50 and num_features = 1 (Extending the same analogy we saw in the previous point, that I found very useful in understanding why the input shape has to be like this — lets say, we have 50 words in one sentence and each word is represented by a word vector. So we need 50 time steps to go through each word vector in the sentence as an input to the LSTM at each time step. There is one sentence per observation and hence num_features = 1. Like this, we need to iterate over all the sentences in the train data to extract the pattern between the words in all sentences. This is exactly what we want here in the time series forecast as well — we want to identify all the patterns that exist between each of the previous values in the window to predict the current time step!)Model Architecture —Below is the model architecture used that is quite self-explanatory—(Its a double stacked LSTM layers with the output from the first LSTM at each time step is being fed to the second LSTM)Making predictions —The plot of predictions vs actuals almost overlap with each other to the extent that we cannot distinguish the blue curve and red curve in the below plot.However, the above is usually not a realistic way in which predictions are done, as we will not have all the future window sequences available with us.2. So, if we want to predict multiple time steps into the future, then a more realistic way is to predict one time step at a time into the future and feed that prediction back into the input window at the rear while popping out the first observation at the beginning of the window (so that the window size remains same). Refer to the below code snippet that does this part — (the comments in the code are self explanatory if you go through the code in my github link that I mentioned above ) —Using this prediction model, the results are plotted below —As can be seen, quite understandably, the farther we try to predict in time, more the error at each time-step that builds up on the previous predicted error. However, the function still behaves like a dampening sine-wave! As I said earlier, this is more realistic modelling of any time series problem since we would not have all the future sequences in hand with us.This code can very well be extended to predicting any time series in general. Note that you may need to take care of other aspects of data preparation like de-trending the series, differencing to stationarize the data and so on before it is fed to LSTM to forecast.Thats it! Hope this article provides a good understanding on using LSTM’s to forecast time series. In case there are some takeaways from this article, please show your appreciation by clapping :)",17/01/2018,0,10.0,16.0,658.0,337.0,11.0,4.0,0.0,2.0,en
4038,"Extract Features, Visualize Filters and Feature Maps in VGG16 and VGG19 CNN Models",Towards Data Science,Roland Hewage,105.0,11.0,2126.0,"Keras provides a set of deep learning models that are made available alongside pre-trained weights on ImageNet dataset. These models can be used for prediction, feature extraction, and fine-tuning. Here I’m going to discuss how to extract features, visualize filters and feature maps for the pretrained models VGG16 and VGG19 for a given image.Here we first import the VGG16 model from tensorflow keras. The image module is imported to preprocess the image object and the preprocess_input module is imported to scale pixel values appropriately for the VGG16 model. The numpy module is imported for array-processing. Then the VGG16 model is loaded with the pretrained weights for the imagenet dataset. VGG16 model is a series of convolutional layers followed by one or a few dense (or fully connected) layers. Include_top lets you select if you want the final dense layers or not. False indicates that the final dense layers are excluded when loading the model. From the input layer to the last max pooling layer (labeled by 7 x 7 x 512) is regarded as feature extraction part of the model, while the rest of the network is regarded as classification part of the model. After defining the model, we need to load the input image with the size expected by the model, in this case, 224×224. Next, the image PIL object needs to be converted to a NumPy array of pixel data and expanded from a 3D array to a 4D array with the dimensions of [samples, rows, cols, channels], where we only have one sample. The pixel values then need to be scaled appropriately for the VGG model. We are now ready to get the features.Here also we first import the VGG16 model from tensorflow keras. The image module is imported to preprocess the image object and the preprocess_input module is imported to scale pixel values appropriately for the VGG16 model. The numpy module is imported for array-processing. In addition the Model module is imported to design a new model that is a subset of the layers in the full VGG16 model. The model would have the same input layer as the original model, but the output would be the output of a given convolutional layer, which we know would be the activation of the layer or the feature map. Then the VGG16 model is loaded with the pretrained weights for the imagenet dataset. For example, after loading the VGG model, we can define a new model that outputs a feature map from the block4 pooling layer. After defining the model, we need to load the input image with the size expected by the model, in this case, 224×224. Next, the image PIL object needs to be converted to a NumPy array of pixel data and expanded from a 3D array to a 4D array with the dimensions of [samples, rows, cols, channels], where we only have one sample. The pixel values then need to be scaled appropriately for the VGG model. We are now ready to get the features.For example here we extract features of block4_pool layer.Here we first import the VGG19 model from tensorflow keras. The image module is imported to preprocess the image object and the preprocess_input module is imported to scale pixel values appropriately for the VGG19 model. The numpy module is imported for array-processing. Then the VGG19 model is loaded with the pretrained weights for the imagenet dataset. VGG19 model is a series of convolutional layers followed by one or a few dense (or fully connected) layers. Include_top lets you select if you want the final dense layers or not. False indicates that the final dense layers are excluded when loading the model. From the input layer to the last max pooling layer (labeled by 7 x 7 x 512) is regarded as feature extraction part of the model, while the rest of the network is regarded as classification part of the model. After defining the model, we need to load the input image with the size expected by the model, in this case, 224×224. Next, the image PIL object needs to be converted to a NumPy array of pixel data and expanded from a 3D array to a 4D array with the dimensions of [samples, rows, cols, channels], where we only have one sample. The pixel values then need to be scaled appropriately for the VGG model. We are now ready to get the featuresHere also we first import the VGG19 model from tensorflow keras. The image module is imported to preprocess the image object and the preprocess_input module is imported to scale pixel values appropriately for the VGG19 model. The numpy module is imported for array-processing. In addition the Model module is imported to design a new model that is a subset of the layers in the full VGG19 model. The model would have the same input layer as the original model, but the output would be the output of a given convolutional layer, which we know would be the activation of the layer or the feature map. Then the VGG19 model is loaded with the pretrained weights for the imagenet dataset. For example, after loading the VGG model, we can define a new model that outputs a feature map from the block4 pooling layer. After defining the model, we need to load the input image with the size expected by the model, in this case, 224×224. Next, the image PIL object needs to be converted to a NumPy array of pixel data and expanded from a 3D array to a 4D array with the dimensions of [samples, rows, cols, channels], where we only have one sample. The pixel values then need to be scaled appropriately for the VGG model. We are now ready to get the features.For example here we extract features of block4_pool layer.Filters are simply weights, yet because of the specialized two-dimensional structure of the filters, the weight values have a spatial relationship to each other and plotting each filter as a two-dimensional image is meaningful. Here we review the filters in the VGG16 model. Here we import the VGG19 model from tensorflow keras. We can access all of the layers of the model via the model.layers property. Each layer has a layer.name property, where the convolutional layers have a naming convolution like block#_conv#, where the ‘#‘ is an integer. Therefore, we can check the name of each layer and skip any that don’t contain the string ‘conv‘. Each convolutional layer has two sets of weights. One is the block of filters and the other is the block of bias values. These are accessible via the layer.get_weights() function. We can retrieve these weights and then summarize their shape. The complete example of summarizing the model filters is given above and the results are shown below.All convolutional layers use 3×3 filters, which are small and perhaps easy to interpret. An architectural concern with a convolutional neural network is that the depth of a filter must match the depth of the input for the filter (e.g. the number of channels). We can see that for the input image with three channels for red, green and blue, that each filter has a depth of three (here we are working with a channel-last format). We could visualize one filter as a plot with three images, one for each channel, or compress all three down to a single color image, or even just look at the first channel and assume the other channels will look the same.Here we retrieve weights from the second hidden layer of VGG16 model. The weight values will likely be small positive and negative values centered around 0.0. We can normalize their values to the range 0–1 to make them easy to visualize. We can enumerate the first six filters out of the 64 in the block and plot each of the three channels of each filter. We use the matplotlib library and plot each filter as a new row of subplots, and each filter channel or depth as a new column. Here we plot the first six filters from the first hidden convolutional layer in the VGG16 model.It creates a figure with six rows of three images, or 18 images, one row for each filter and one column for each channel. We can see that in some cases, the filter is the same across the channels (the first row), and in others, the filters differ (the last row). The dark squares indicate small or inhibitory weights and the light squares represent large weights. Using this intuition, we can see that the filters on the first row detect a gradient from light in the top left to dark in the bottom right.The activation maps, called feature maps, capture the result of applying the filters to input, such as the input image or another feature map. The idea of visualizing a feature map for a specific input image would be to understand what features of the input are detected or preserved in the feature maps. The expectation would be that the feature maps close to the input detect small or fine-grained detail, whereas feature maps close to the output of the model capture more general features. In order to explore the visualization of feature maps, we need input for the VGG16 model that can be used to create activations.We need a clearer idea of the shape of the feature maps output by each of the convolutional layers and the layer index number. So we enumerate all layers in the model and print the output size or feature map size for each convolutional layer as well as the layer index in the model.Here we design a new model that is a subset of the layers in the full VGG16 model. The model would have the same input layer as the original model, but the output would be the output of a given convolutional layer, which we know would be the activation of the layer or the feature map. After loading the VGG model, we can define a new model that outputs a feature map from the first convolutional layer. Making a prediction with this model will give the feature map for the first convolutional layer for a given provided input image. After defining the model, we need to load the input image with the size expected by the model, in this case, 224×224. Next, the image PIL object needs to be converted to a NumPy array of pixel data and expanded from a 3D array to a 4D array with the dimensions of [samples, rows, cols, channels], where we only have one sample. The pixel values then need to be scaled appropriately for the VGG model. We are now ready to get the feature map. We can do this easy by calling the model.predict() function and passing in the prepared single image. We know the result will be a feature map with 224x224x64. We can plot all 64 two-dimensional images as an 8×8 square of images.For a given image,Here we collect feature maps output from each block of the model in a single pass, then create an image of each. There are five main blocks in the image (e.g. block1, block2, etc.) that end in a pooling layer. The layer indexes of the last convolutional layer in each block are [2, 5, 9, 13, 17]. We can define a new model that has multiple outputs, one feature map output for each of the last convolutional layer in each block. Making a prediction with this new model will result in a list of feature maps. We know that the number of feature maps (e.g. depth or number of channels) in deeper layers is much more than 64, such as 256 or 512. Nevertheless, we can cap the number of feature maps visualized at 64 for consistency. Here we create five separate plots for each of the five blocks in the VGG16 model for our input image.Running the example results in five plots showing the feature maps from the five main blocks of the VGG16 model. We can see that the feature maps closer to the input of the model capture a lot of fine detail in the image and that as we progress deeper into the model, the feature maps show less and less detail. This pattern was to be expected, as the model abstracts the features from the image into more general concepts that can be used to make a classification. Although it is not clear from the final image that the model saw a car, we generally lose the ability to interpret these deeper feature maps.For a given image,Hope you have gained some good knowledge about how to Extract Features, Visualize Filters and Feature Maps in VGG16 and VGG19 CNN Models. Stay tuned for more amazing articles.",12/05/2020,0,9.0,12.0,549.0,439.0,23.0,0.0,0.0,4.0,en
4039,帶你認識Vector-Quantized Variational AutoEncoder - 理論篇,Taiwan AI Academy,Tan,11.0,10.0,57.0,"說到近年來最火紅以深度學習為主的生成模型，大家必定會想到生成對抗網路(Generative Adversarial Network, GAN)，然而在GAN(2014)還沒被提出來之前，有另外一個同樣屬於生成模型的Variational AutoEnoder (VAE)常被大家所使用，很可惜的是當時GAN在許多任務上所產生的圖片清晰度較高，因此VAE類型的模型相對而言就勢弱了一些(當然GAN在訓練的特性上有一些難以克服的問題至今也尚未完全解決)。故事總不會就這樣結束，2017年DeepMind在NIPS研討會上提出了Vector-Quantized Variational AutoEncoder模型，雖然在效果上仍然是先與VAE做比較，但VQ-VAE提出的概念讓它擁有比其它生成模型更獨特的地方，甚至在後續2019年6月提出的VQ-VAE2甚至宣稱在生成1024*1024的高解析度人臉時與當時效果最佳的BigGAN可作比擬。如果你開始對VQ-VAE感到好奇，就跟著我們一起看下去吧。註1：如果你對Variational AutoEncoder甚至是AutoEncoder的概念還沒那麼熟的話，可以參考此篇AutoEncoder介紹、此篇VAE介紹、或是尋找其他資源唷。我們可以這樣解讀AutoEncoder家族在做的事情，Encoder試圖找出輸入圖片x在潛在空間上的表徵(representation)，在大多數的狀況中，大家使用連續型的分布去模擬z的樣貌(e.g. AE將輸入x投影至潛在空間的一個點；VAE則改為使用高斯分布模擬輸入x在潛在空間的樣貌)，然而VQVAE的作者提到離散的潛在表徵在很多情境上也許才是比較適合的，例如語言概念，因此VQ-VAE主要的突破就是試圖讓Encoder產出離散的表徵代表每一筆輸入資料，而Decoder則需要在接收這樣離散的表徵後還原原本的資料。當然我們可以直接從模型的名稱感受到它究竟是如何做到這件事情。Vector Quantization 向量量化的技巧在訊號處理領域中已經發展了一段時間，主要的做法是將影像/音訊切割不同群組並取得每個群組的代表向量(Figure 1)，另外維護一份有K個編碼向量的編碼簿(codebook)，針對每個群組，以編碼簿中最接近群組代表向量的編碼向量索引作為這個群組的代表，這樣我們就可以將原始的資料轉換為n個索引(n為群組數量)，再加上儲存編碼簿本身，就可以達到資料壓縮與特徵離散化的目的。由於資料壓縮必定會產生資訊的遺失，這個演算法最主要的任務就是在橫跨不同的影像/音訊中找到能讓資訊遺失最少的K個編碼向量。接著來看一下VQ-VAE的架構，以及它是如何在原本AutoEncoder的框架下加入Vector Quantization的技巧。從Figure 2 中可以看到VQ-VAE同樣維持著Encoder-Decoder的架構，然而這邊所提取的特徵保留了多維的結構，以圖中所使用的影像資料為例，Encoder最後輸出的潛在表徵Z_e(x)大小將為(h_hidden, w_hidden, D)，其實就是在CNN中我們熟知的Feature map。接著會進入到Vector Quantization的部分，同樣我們會有K個編碼向量(Figure 2 中 Embedding Space的部分)，每一個編碼向量同樣有D個維度，根據Feature Map中(h_hidden, w_hidden)的每個點位比對D維的特徵向量與Codebook中K個編碼向量的相似程度，並且以最接近的編碼向量索引作取代(Figure 2中央藍色的Feature Map部分)，這樣就達到了將原圖轉換為離散表徵的步驟(最後的表徵為(h_hidden, w_hidden, 1)的形狀)。在Decode階段，只要將Codebook中相對應的編碼向量置於表徵的相對應位置，就可以得到同樣為(h_hidden, w_hidden, D)的轉換後表徵Z_q(x)，接著同樣通過Decoder後就可得到還原後的原始資料。上述的流程事實上並不算複雜(如果有人沒有完全理解的話一定是我說明的不清楚Orz)，但在深度學習中我們期待的是end-to-end的訓練架構，因此如何設計損失函數讓我們可以同時訓練Encoder、Decoder，以及找到合適的Embedding Vectors同樣是需要思考的事情。先來聊聊Encoder和Decoder的部分，我們都知道目前深度學習模型的訓練高度依賴倒傳遞(back-propagation)方法，也就是使用微分的方式計算梯度後以此更新模型權重(參數)，這部分在AE/VAE上也相同。但是修但幾勒，在VQ-VAE的其中一個步驟中，我們使用了argmin (尋找與Z_e(x)最接近的codebook向量並進行取代)，這個操作本身是無法計算梯度的，因此單純地使用原始的倒傳遞沒辦法更新到Encoder的參數。若我們將VQ-VAE模型表示成上面的三個步驟，主要無法微分的是第二個步驟。那可怎麼辦呢？在此作者提出了一個Straight-through estimator的作法，在概念上也並無甚麼高深的地方，就是直接將Z_q(x)所造成的誤差直接當作Z_e(x)的誤差作倒傳遞，藉此就能讓decoder輸出與原圖的差異同時更新decoder與encoder(在Figure 2 中為紅色線條)。另外一個部分要考量的則是codebook中的編碼向量，我們同樣也希望手中的K個編碼向量能夠盡量涵蓋並且代表輸入資料的特徵，因此在訓練VQ-VAE的模型時，同樣也應調整codebook中的參數。在此作者使用l2 norm計算Z_e(x)的每個pixel上的向量與相對應的codebook編碼向量差異，並以此計算梯度修正codebook。Figure 3 呈現了VQ-VAE的損失函數，總共包含了三個部分，讓我們一一來看一下。第一項的loss為reconstruction loss，也就是在通過了整個VQ-VAE網路後所還原出來的資料與原始輸入資料的差距(在計算上仍然以pixel-wise mean square error估計)，值得提醒的是由於使用了Straight-through estimator方法，這一項的loss並不會調整到codebook中的內容。第二與第三項則與codebook有關，如同上一段所說的，我們計算Z_e(x)與codebook中相對應向量的l2 norm，並試圖同時更新Z_e(x)與codebook向量，然而相對而言我們希望codebook更新的幅度多一些，因此在這邊作者導入了stop gradient的符號並將l2 norm拆成兩項。在stop gradient(sg)符號內的內容在計算loss時會正常存在，但在更新時此項並不會提供任何梯度，也代表著不會被更新。第二項的代表的是計算Z_e(x)與相對應codebook向量(e)的l2 norm，但只更新codebook，第三項則計算相同的l2 norm但只針對Z_e(x)作梯度計算與更新，由於我們希望Z_e(x)不用那麼在意與codebook接近，因此第三項的前面會加上一個commitment cost作為權重(在原始論文中提的數值為0.25)。說了那麼多，VQ-VAE在還原資料的效果上如何呢？在原始論文中作者除了在cifar10這種較小的資料集上作測試外，也應用到了ImageNet等較大型的資料集上。從原始論文中的圖片(Figure 4)可以看到，即使在ImageNet這種有差異度極高的資料集上，VQ-VAE的還原效果也不錯。在上述的模型架構中我們主要以圖片作為示範，然而VQ-VAE的架構在Encoder與Decoder的選擇上是非常彈性的，因此除了圖片之外，作者也應用VQ-VAE到音訊甚至是影片資料上。由於VQ-VAE針對資料做壓縮後再還原將導致部分資訊會有遺失，但在音訊資料上，實驗發現VQ-VAE所還原的資料會保留講者的內容資訊而排除聲調或語氣的部分，這也證明了VQ-VAE後續可能的發展潛力。另外，上述提到的內容只描述到AutoEncoder系列作的資料壓縮與還原能力，但身為生成模型的一員，VQ-VAE同樣也能產生訓練資料中未見過的資料。在生成新資料時，我們只需要生成潛在表徵的codebook索引(同樣以Figure 1為例，只需生成中間藍色部分的feature map)，再使用decoder還原成原圖即可。原文中作者使用了另一類的自回歸生成模型PixelCNN作潛在表徵結構的生成，但由於篇幅安排跟主題聚焦關係，關於PixelCNN的模型介紹以及結合VQ-VAE進行圖像生成的部分請大家期待後續的系列文章。在GAN佔據大部分研究者的目光時，VQ-VAE試圖拓展大家對生成模型領域發展的想像。VQ-VAE擁有AutoEncoder家族維度縮減的能力，能夠應用於資料壓縮或是特徵提取，另外也能結合自回歸類的生成模型(ex. PixelRNN, PixelCNN)進行資料生成(甚至補足自回歸生成模型生成高解析度影像時效能不佳的問題)，非常期待後續能看到這類型的模型在不同場域有更加優秀的表現。這次簡介中，我盡量不使用貝氏模型的概念作說明，若大家對一些細節有興趣，以下我也整理了一些曾看過不錯的相關資源供大家參考。在這個系列的下一篇文章中，我將會實際帶大家訓練一個VQ-VAE模型，請大家敬請期待。",28/04/2020,0,0.0,5.0,1069.0,326.0,5.0,2.0,0.0,5.0,ko
4040,True Democratization of Analytics with Meta-Learning,Stories by Progress,Progress,1200.0,5.0,789.0,"The democratization of analytics has become a popular term, and a quick Google search will generate results that explore the necessity of empowering more people with analytics and the rise of citizen data scientists. The ability to easily make better use of your (constantly growing) pool of data is a critical driver of business success, but many of the existing solutions that claim to democratize analytics only do so within severe limits. If you have a complex business scenario and are looking to get revolutionary insights using them, it’s easy to come away disappointed.However, the democratization of analytics isn’t just a buzzword that refers to a narrow approach. It’s possible to do so much more. Let’s quickly review the current state of the market that you’re likely familiar with, and then dive into our proposed solution.One way this type of solution is marketed is as something that’s simple because it works in an environment business leaders are already familiar with, like Excel or Tableau. These solutions tend to be lightweight and are really about easily generating a digestible report. That’s all well and good, but it’s really democratizing report generation and lightweight analysis rather than enabling you to develop truly predictive scenarios that require Machine Learning.Another option that is gaining adoption is to use pre-trained models usable out-of-the-box for image analysis and classification, speech to text conversion, and translation services. While these make certain limited use cases available to more organizations, they don’t actually democratize the predictive analytics processing related to business specific time-series scenarios.Finally, there are numerous cloud vendors that take care of managing the infrastructure necessary for Big Data analytics and Machine Learning, whether it’s hosting Hadoop/MapReduce, Spark, etc., providing managed database support, or hosting machine intelligence software libraries like TensorFlow. At the end of the day, these options are really democratizing the infrastructure necessary to support Machine Learning — they aren’t democratizing the Data Scientist lifecycle itself, something we discuss in detail a little later in the post.The solutions above may technically “democratize” some form of analytics, but they fall short in democratizing Machine Learning for individual business use cases like predictive maintenance for the Industrial IoT, improving patient outcomes in healthcare, detecting fraud in financial services, etc. So while simple scenarios are becoming a commodity, business scenarios that provide the most value are beyond the reach of most organizations.Why?Because the Machine Learning or Data Scientist lifecycle is complex. A successful implementation includes a business requirements phase, data preparation, data modeling, and production deployment work. The last three phases are particularly resource intensive.It’s pretty clear that this is a completely different challenge that the options described above can’t address. While there are cloud options that will manage the infrastructure, and there are tools that make the data scientist more efficient, there is a dearth of solutions that tackle the democratization of complex Machine Learning.The need for democratization is driven by the amount of time and resources it takes to do this manually — even with a team of data scientists. And for those that don’t have data scientists, this is a non-starter given traditional tools and solutions.It’s evident that there is a need for a better way forward when it comes to solving these complex business challenges. Data scientists have to be freed from the laborious day to day grind that consumes so much of their time today, enabling them to more effectively support a higher number of business scenarios in less time.Data scientists have to be freed from the laborious day to day grind that consumes so much of their time today (tweet this)Progress DataRPM is designed specifically to meet this need. By developing an innovative machine automated approach, we are able to automate a range of complex tasks that the other solutions above simply can’t.This solution allows your team to focus the most strategic and actionable part of the process, which is analyzing and assessing the results. Whether you currently employ data scientists or not, it reduces the amount of time you need to allocate to evaluating and creating complex models.Rather than constrain analytics and generate a simple or limited result, the meta-learning approach looks fully at the unique problems facing your business, is flexible enough to be adapted to new problems as they arise and is constantly improving. By automating some of the most arduous components of data analysis, you’re free to focus on delivering the insights and outcomes you need — quickly. It’s all part of our cognitive-first vision for business applications. You can learn more about our platform for cognitive predictive maintenance here.This content originally appeared on the Progress blog, by Mark Troester. Looking for more great stories about technology and building tomorrow’s business apps? Check out the blog for more.",14/08/2017,0,0.0,5.0,870.0,220.0,1.0,2.0,0.0,8.0,en
4041,Machine Learning Quiz 05: Decision Tree (Part 1),Medium,Md Shahidullah Kawsar,153.0,3.0,438.0,"Let’s check your basic knowledge of Decision Tree. Here are 10 multiple-choice questions for you and there’s no time limit. Have fun!Question 1: Decision trees are also known as CART. What is CART?(A) Classification and Regression Trees(B) Customer Analysis and Research Tool(C) Communication Access Real-time Translation(D) Computerized Automatic Rating TechniqueQuestion 2: What are the advantages of Classification and Regression Trees (CART)?(A) Decision trees implicitly perform variable screening or feature selection(B) Can handle both numerical and categorical data(C) Can handle multi-output problems.(D) All of the aboveQuestion 3: What are the advantages of Classification and Regression Trees (CART)?(A) Decision trees require relatively less effort from users for data preparation(B) Nonlinear relationships between parameters do not affect tree performance.(C) Both (A) and (B)(D) None of theseQuestion 4: What are the disadvantages of Classification and Regression Trees (CART)?(A) Decision trees can be unstable because small variations in the data might result in a completely different tree being generated(B) Decision trees require relatively less effort from users for data preparation(C) Nonlinear relationships between parameters do not affect tree performance.(D) Decision trees implicitly perform variable screening or feature selectionQuestion 5: Decision tree learners may create biased trees if some classes dominate. What’s the solution of it?(A) balance the dataset prior to fitting(B) imbalance the dataset prior to fitting(C) balance the dataset after fitting(D) No solution possibleQuestion 6: Decision tree can be used for ______.(A) classification(B) regression(C) Both(D) None of theseQuestion 7: Decision tree is a ______ algorithm.(A) supervised learning(B) unsupervised learning(C) Both(D) None of theseQuestion 8: Suppose, your target variable is whether a passenger will survived or not using Decision Tree. What type of tree do you need to predict the target variable?(A) classification tree(B) regression tree(C) clustering tree(D) dimensionality reduction treeQuestion 9: Suppose, your target variable is the price of a house using Decision Tree. What type of tree do you need to predict the target variable?(A) classification tree(B) regression tree(C) clustering tree(D) dimensionality reduction treeQuestion 10: What is the maximum depth in a decision tree?(A) the length of the longest path from a root to a leaf(B) the length of the shortest path from a root to a leaf(C) the length of the longest path from a root to a sub-node(D) None of theseThe solutions will be published in the next quiz Machine Learning Quiz 06. Happy learning. If you like the questions and enjoy taking the test, leave a clap for me. Feel free to discuss/share your thoughts on these questions in the comment section.The solution of Machine Learning Quiz 04: Logistic Regression — 1(A), 2(B), 3(B,D), 4(C), 5(C), 6(A), 7(B), 8(A), 9(D), 10(C)References:[1] https://clevertap.com/blog/numerical-vs-categorical-variables-decision-trees/[2] Decision Trees in Machine Learning: https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052",01/06/2021,0,13.0,0.0,900.0,487.0,1.0,0.0,0.0,4.0,en
4042,Speeding up BERT Search in Elasticsearch,Towards Data Science,Dmitry Kan,203.0,13.0,2573.0,"In two previous blog posts on my journey with BERT: Neural Search with BERT and Solr and Fun with Apache Lucene and BERT I’ve taken you through the practice of what it takes to enable semantic search powered by BERT in Solr (in fact, you can plug in any other dense embeddings method, other than BERT, as long as it outputs a float vector; a binary vector can also work). While it feels cool and modern to empower your search experience with a tech like BERT, making it performant is still important for productization. You want your search engine operations team to be happy in a real industrial setting. And you want your users to enjoy your search solution.Devops cares about disk sizes, RAM and CPU consumption a lot. In some companies, they also care about electricity consumption. Scale for millions or billions of users and billions of documents is no cheap thing.In Neural Search with BERT and Solr I did touch upon measuring time and memory consumption when dealing with BERT, with the time being both for indexing and search. And with indexing time there were some unpleasant surprises.The search time is really a function of the number of documents, because from the algorithm complexity perspective it takes O(n), where n is the total number of documents in the index. This quickly becomes unwieldy, if you are indexing millions of docs and what’s even more important: you don’t really want to deliver n documents to your users: no one will have time to go through millions of documents in response to their searches. So why bother scoring all n? The reason why we need to visit all n documents is because we don’t know in advance which of these documents are going to correlate with the query in terms of dot-product or cosine distance between a document and query dense vectors.In this blog post, I will apply the BERT dense embedding technique to Elasticsearch — a popular search engine of choice for many companies. We will look at implementing vanilla vector search and then will take a leap forward to KNN in vector search — measuring every step of our way.Since there are plenty of blog posts talking about the intricacies of using vector search with Elasticsearch, I thought: what unique perspective could this blog post give you, my tireless reader? And this is what you will get today: I will share with you a substantially less known solution to handling vector search in Elasticsearch: using Associative Processing Unit (APU) implemented by GSI. I got a hold of this unique system that cares not only for the speed of querying vectors on large scale, but also for the amount of watts consumed (we do want to be eco-friendly to our Planet!). Sounds exciting? Let’s plunge in!Elasticsearch is using Apache Lucene internally as a search engine, so many of the low-level concepts, data structures and algorithms (if not all) apply equally to Solr and Elasticsearch. Documented in https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch the approach to vector search has exactly the same limitation as what we observed with Solr: it will retrieve all documents that match the search criteria (keyword query along with filters on document attributes), and score all of them with the vector similarity of choice (cosine distance, dot-product or L1/L2 norms). That is, vector similarity will not be used during retrieval (first and expensive step): it will instead be used during document scoring (second step). Therefore, since you can’t know in advance, how many documents to fetch to surface most semantically relevant, the mathematical idea of vector search is not really applied.Wait a sec, how is it different from TF-IDF or BM25 based search — why can’t we use the same trick with vector search? For BM25/TF-IDF algorithms you can precompute a bunch of information in the indexing phase to help during retrieval: term frequency, document frequency, document length and even a term position within the given document. Using these values, the scoring process can be applied during the retrieval step very efficiently. But you can’t apply cosine or dot-product similarities in the indexing phase: you don’t know what query your user will send your way, and hence can’t precompute the query embedding (except for some cases in e-commerce, where you can know this and therefore precompute everything).But back to practice.To run the indexer for vanilla Elasticsearch index, trigger the following command:If you would like to reproduce the experiments, remember to alter the MAX_DOCS variable and set it to the desired number of documents to index.As with every new tech, I’ve managed to run my Elasticsearch indexer code into an issue: the index became read-only during indexing process and would fail to advance! The reason is well explained here, in a nutshell: you need to ensure at least 5% of free disk space (51.5 gigabyte if you have a 1 TB disk!) in order to avoid this pesky issue or need to switch this safeguarding feature off (not recommended for production deployments).The error looks like this:In this situation you can turn to Kibana — the UI tool, that grew from only data visualizations to security and index management, alerting and observability capabilities. For this blog post I’ve been routinely collecting index size information and inspecting index settings and mappings via index management dashboard:If you still would like to get rid of this limitation , you can try something like this in Kibana Dev Tools (choose the suitable values for your use case — but be careful with the “cluster.routing.allocation.disk.watermark.flood_stage” value, since if it is too low, your OS might run into stability issues — consult official docs):After indexing, I’ve run 10 queries to measure the average speed. I’ve also recorded the time it took to index (including the time for computing vectors from text) and the size of the resulting index for each N=100, 200, 1000, 10000 and 20000. I did not record watt consumption, which could be an interesting idea for the next experiment.Here is the raw table behind the chart above:Since indexing was done with a single worker in bert-as-service, the indexing time grows exponentially, while the search speed advances sub-linearly to the number of documents. But how practical is this? For 20k short abstracts, 40ms for search seems to be too high. The index size grows linearly and that is a worrying factor as well (remember, that your devops teams can get concerned and you will need to prove the effectiveness of your algorithms).Because it became impractical to index this slow, I had to find another way to compute vectors (most of the time goes to computing vectors, rather than to indexing them: I will prove it experimentally soon). So I took a look at Hugging Face library, that allows to index sentence embeddings using Siamese BERT-Networks, described here. In the case of Hugging Face, we also don’t need to use an http server, unlike in bert-as-service. Here is a sample code to get started:SBERT approach managed to compute embeddings 6x faster, than bert-as-service. In practice, 1M vectors would take 18 days using bert-as-service and it took 3 days using SBERT with Hugging Face.And the approach is to use a KNN algorithm to efficiently seek for document candidates in the closest vector sub-space.I took 2 approaches that are available in the open source: elastiknn by Alex Klibisz and k-NN from Open Distro for Elasticsearch supported by AWS:Let’s compare our vanilla vector search with these KNN methods on all three dimensions:We will first need to install the elastiknn plugin, that you can download from the project page:To enable elastiknn on your index, you need to configure the following properties:# 1 refers to the number of shards in your index, and sharding is the way to speed up the query, since it will be executed in parallel in each shard.# 2 Elastiknn uses binary doc values for storing vectors. Setting this variable to true gives you significant speed improvements for Elasticsearch version ≥ 7.7, because it will prefer Lucene70DocValuesFormat with no compression of doc values over Lucene80DocValuesFormat, which uses compression, saving disk but increasing time for reading the doc values. It is worth to mention here, that Lucene80DocValuesFormat offers to trade compression ratio for better speed and vice versa starting Lucene 8.8 (relevant jira: https://issues.apache.org/jira/browse/LUCENE-9378, this version of Lucene is used in Elasticsearch 8.8.0-alpha1). So eventually these goodies will land both in Solr and Elasticsearch.There are quite a few options for indexing and searching with different similarities — I recommend studying the well-written documentation.To run the indexer for elastiknn index, trigger the following command:Indexing and search performance with elastiknn are summarized in the following table:I’ve run into many issues with Open Distro — and with this blog post I really hope to attract attention of OD developers, especially if you can find what can be improved in my configuration.Without exaggeration I spent several days figuring out the maze of different settings to let OD index 1M vectors. My setup was:I’ve also had index specific settings, and this is a suitable moment to show how to configure the KNN plugin:Setting index.refresh_interval = 1 allows to avoid frequent index refresh to maximize for indexing throughput. And merge.scheduler.max_thread_count=1 restricts merging to a single thread to spend more resource on the indexing itself.With these settings, I managed to index 200k documents into Open Distro Elasticsearch index. The important bit is to define the vector field like so:The plugin builds Hierarchical Navigable Small World (HNSW) graphs during indexing, which are used to speed up KNN vector search. The graphs are built for each KNN field / Lucene segment pair, making it possible to efficiently find K nearest neighbours for the given query vector.To run the indexer for Open Distro, trigger the following command:To search, you need to wrap the query vector into the following object:Maximum k supported by the plugin is 10000. Each segment/shard will return k vectors, nearest to the query vector. Then the candidates will “roll up” to size number of resultant documents. Bear in mind, that if you use post filtering on the candidates, it may well be you get < k candidates per segment/shard, so it can also impact the final size.The choice of KNN space type in the index settings defines the metric that will be used to find the K nearest neighbours. The “cosinesimil” setting corresponds to the following distance formula:From the plugin docs: “The cosine similarity formula does not include the 1 - prefix. However, because nmslib equates smaller scores with closer results, they return 1 - cosineSimilarity for their cosine similarity space—that’s why 1 - is included in the distance function.”In KNN space, the smaller distance corresponds to closer vectors. This is opposite to how Elasticsearch scoring works: the higher score represents a more relevant result. To solve this, KNN plugin will turn the distance upside down into a 1 / (1 + distance) value.I’ve run the measurements on indexing time, size and search speed, averaged across 10 queries (exactly the same queries were used for all the methods). Results:After two previous posts on BERT vector search in Solr and Lucene, I got contacted by GSI Technology and was offered to test their Elasticsearch plugin implementing distributed search powered by GSI’s Associative Processing Unit (APU) hardware technology. APU accelerates vector operations (like computing vector distance) and searches in-place directly in the memory array, instead of moving data back and forth between memory and CPU.The architecture looks like this:In order to use this solution, a user needs to produce two files:After these data files get uploaded to the GSI server, the same data gets indexed in Elasticsearch. The APU powered search is performed on up to 3 Leda-G PCIe APU boards.Since I’ve run into indexing performance with bert-as-service solution, I decided to take SBERT approach described above to prepare the numpy and pickle array files. This allowed me to index into Elasticsearch freely at any time, without waiting for days. You can use this script to do this on DBPedia data, which allows to choose between EmbeddingModel.HUGGING_FACE_SENTENCE (SBERT) and EmbeddingModel.BERT_UNCASED_768 (bert-as-service).Next, I’ve precomputed 1M SBERT vector embeddings and fast-forward 3 days for vector embedding precomputation, I could index them into my Elasticsearch instance. I had to change the indexing script to parse the numpy&pickle files with 1M vectors and simultaneously read from DBPedia archive file to combine into a doc by doc indexing process. Then I’ve indexed 100k and 1M vectors into separate indexes to run my searches (10 queries for each).For vanilla Elasticsearch 7.10.1 the results are as follows:If we proportionally increase the time it would take to compute 1M vectors with bert-as-service approach to 25920 minutes, then SBERT approach is 17x faster! Index size is not going to change, because it depends on Lucene encoding and not on choosing BERT vs SBERT. Compacting the index size by reducing the vector precision is another interesting topic of research.For elastiknn I got the following numbers:Combining vanilla and elastiknn’s average speed metrics we get:On my hardware elastiknn is 2,29x faster on average than Elasticsearch native vector search algorithm.What about GSI APU solution? First, the indexing speed is not directly compatible with the above methods, because I had to separately produce the numpy+pickle files and upload them to GSI server. Indexing into Elasticsearch is comparable to what I did above and results in 15G index for 1M SBERT vectors. Next I ran the same 10 queries to measure the average speed. In order to run the queries with GSI Elasticsearch plugin, I needed to wrap query vector into the following query format:Since I only uploaded 1M vectors, and experimented with 1M entries in the Elasticsearch index, I’ve got one number for the average speed over 10 queries: 92.6ms. This gives an upper bound on our speed graph for 1M mark on X axis:All times on this graph are from ‘took’ value reported by Elasticsearch. So none of these numbers include a network speed to transfer the results back to the client. However, the ‘took’ for GSI includes the communication between Elasticsearch and APU server.What’s interesting is that GSI Elasticsearch plugin supports batch queries — this can be used for various use cases, for instance when you’d like to run several pre-crafted queries to get a data update on them. One specific use case could be creating user alerts on the queries of user interest — a very common use case for a number of commercial search engines.To run a batch query, you will need to change the query like so:In response, GSI plugin will execute queries in parallel and return you the top N (controlled with topK parameter) document ids for each query, with the cosine similarity:The GSI’s search architecture scales to billions of documents. In this blog post, I have tested it with 1M DBPedia records and saw impressive results for performance.I’ve studied 4 methods in this blog post, that can be grouped like so:If you prefer to stay within the original Elasticsearch by Elastic, the best choice is elastiknn.If you are open to Open Distro as the open source alternative to Elasticsearch by Elastic, you will win 50–150 ms. However, you might need extra help in setting things up.If you like to try a commercial solution with hardware acceleration, then GSI plugin might be your choice, giving you comparatively the fastest vector query speed.If you like to further explore the topics discussed in this blog post, read the next section, and leave a comment if you have something to ask or add.",15/03/2021,12,21.0,27.0,811.0,436.0,7.0,6.0,0.0,35.0,en
4043,写在看完变形金刚II之后,Medium,Justin Chen,8.0,2.0,10.0,变形金刚II(Transformers:ROF)这样一部电影，从我从电影院看完他的第一部就开始期待了，昨天终于有幸去电影院看了。画面很，庞大，震撼说不上，可能在1的时候已经给我震完了吧。影院的效果就是好，所以看这个电影确实是一种享受的。当然就我个人看来他想超越1或者是原版动漫是没有多大可能了。我不是一个喜欢搞剧透的人，所以我非常不想说剧情。只说一下对于剧情的感受：1. 剧情太商业化，变形金刚这么强大的战斗力和防御力，我不知道拿着枪的人类可以对他们造成什么样的威胁呢？美国大兵们与变形金刚们短兵相接，难道是为了方便狂派们刷数据么？2. 由于是美国电影，所以一定要展现美国军备的强大，所以战舰上的秘密武器可以KO看上去甚是强大的纸老虎 — — 狂派合体机器人；3. 主角一定得是人类，为了烘托人类主角的伟大性，不惜牺牲同样伟大但没有他伟大的各位领袖同志，主角由于起点比较低，所以随便摸一下某个能量体就可以吸收里面的精髓；4. 赶新潮，年轻人做网站办公司、经济危机，再在电影里融入一点青春元素，把学生宿舍比喻为霍格沃茨，可惜这方面的篇幅太短，如果开发一下说不定会为本集贫乏的剧情添加一点色彩；5. 冷兵器，变形金刚们之间的斗争，如果想要解决对方，就必须使用冷兵器或者蛮力，这方面是我所欣赏的，我可不希望擎天柱、威震天是被一把麦林爆头干掉的；6. HappyEnding，每一部想拍续集的电影都有那么一个HappyEnding，狂派还会回来的誓言也是必需的～综上所述，这的确是一部好电影，看的时候请放松您的大脑，因为没什么可以让你去思考的。BTW：搬到cosbeta的主机以后速度很快，很快，我非常欣慰。,29/06/2009,0,7.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,no
4044,Classes of Novels,Medium,Jhen Hilario,1.0,1.0,146.0,"According to Creative Writing Now, the publishing world tends to classify Fiction as either Commercial (built to make money), or Literary (a work of art). There are no further explanations why art cannot also make money, but things just doesn’t work that way. Observe how Commercial Fiction and Literary Fiction are handled as separate categories. Commercial Fiction is divided into several genres. This kind of classification can help readers to determine what kind of novel do they like to read. Each genre also has its own rubric. Literary fiction has been generally chunked all together in bookstores as “General Fiction”. Because the precedence of literary authors is to produce works of art, while selling books is only a second thought. Literary authors are less likely to think in ways of writing a specific genre or category of novel and proceeding to the pattern of that genre.",31/10/2015,0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,en
4045,Object Recognition with OpenCV on Android,Medium,Akshika Wijesundara,465.0,6.0,763.0,"This article is for a person who has some knowledge on Android and OpenCV. We will look at how to use the OpenCV library to recognize objects on Android using feature extraction.I am using Android Studio and you can follow this link to download and install Android studio and SDK tools but if you are a die hard eclipse fan you also can follow this tutorial( no hard feelings ;) )2. Setting up OpenCV library inside Android StudioYou have to download and import OpenCV library to android studio and there is a stackoverflow answer which you can follow to setup everything. If you are using Eclipse use this link.Now you are ready to mingle with me ;). The algorithm we are going to use is ORB(Oriented FAST and Rotated BRIEF). As an OpenCV enthusiast, the most important thing about the ORB is that it came from “OpenCV Labs”. This algorithm was brought up by Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary R. Bradski in their paper ORB: An efficient alternative to SIFT or SURF in 2011. It is a good alternative to SIFT and SURF in computation cost, matching performance and mainly the patents. Yes, SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not . You still can use SIFT and SURF but you have to compile them separately as they are not contained in the latest version of the OpenCV library and if you are going to make a commercial application you have to pay and get them.Here is a link for the other existing algorithms in OpenCV for object detectionNow you know about ORB and it’s history, lets start coding.3. Create a new project on Android StudioThis step is trivial therefore I am giving this link so you could follow that and create a new project. Please do read what is on that page it is very informative.4.Creating the XML for the UIIf you are not new to Android you know that very well that you need to create a XML file for the UI. These files are created inside the layout folder.You can create a XML file using the above code. First segment is the Camera View, second segment (TextView) is quite not necessary but that is there for testing purposes ( just in case if you want to get some data on the screen rather than the log)5. Write about the Main ActivityYou can create the Main Activity copying the above code. But I would like to point out few important factors in the above code. After all we are software engineers not code monkeys ;)First you would notice is that MainActivity is extending CameraBridgeViewBase.CvCameraViewListener2 . This interface would enforce us to implement few methods which are related to the camera. The method which is important to us is the onCameraFrame(CvCameraViewFrame inputFrame). This would receive the video as frames and you can do all the image processing inside this method and return a Mat with that image :)There are three main variablesdetector is used to detector features and the descriptor will compute the descriptors and the matcher would match the descriptors.Later in the code you can find there are distance values given which are to set the level of threshold that you want the feature matching has to happen, after that in the code best matches have been sorted out and drawn using Features2d.drawMatches() method. That image has been returned to the camera view.6. Image preprocessingIt is important to have the input image and the image that you receive from the camera has the same dimensions. You definitely can change those values in the code but for now I am changing that manually before I insert the image to the assets folder in the project. if there is a mismatch between your image’s dimensions, image’s type you will get few errors. I got them and I solved them using the following two links.Stackoverflow Link1Stackoverflow Link2Then you can create an assets folder on android and then you can copy paste the image to that folder. You can follow this link to create an assets folder inside your project.7. AndroidManifest.XML fileThis is the file which manages permissions and controls which layout loaded first etc.You don’t have to copy paste this, but make sure to add camera permission and also the it’s features in this file so the application can use the camera on the phone.7. SampleSorry about the background music, I will upload a better video later :)8. Git Hub Link : https://github.com/akshika47/OpencvAndroidPlease do leave your comments and feedback :)",20/12/2016,4,236.0,49.0,1400.0,786.0,1.0,1.0,0.0,9.0,en
4046,SSD object detection: Single Shot MultiBox Detector for real-time processing,Medium,Jonathan Hui,27000.0,11.0,1841.0,"SSD is designed for object detection in real-time. Faster R-CNN uses a region proposal network to create boundary boxes and utilizes those boxes to classify objects. While it is considered the start-of-the-art in accuracy, the whole process runs at 7 frames per second. Far below what real-time processing needs. SSD speeds up the process by eliminating the need for the region proposal network. To recover the drop in accuracy, SSD applies a few improvements including multi-scale features and default boxes. These improvements allow SSD to match the Faster R-CNN’s accuracy using lower resolution images, which further pushes the speed higher. According to the following comparison, it achieves the real-time processing speed and even beats the accuracy of the Faster R-CNN. (Accuracy is measured as the mean average precision mAP: the precision of the predictions.)The SSD object detection composes of 2 parts:SSD uses VGG16 to extract feature maps. Then it detects objects using the Conv4_3 layer. For illustration, we draw the Conv4_3 to be 8 × 8 spatially (it should be 38 × 38). For each cell (also called location), it makes 4 object predictions.Each prediction composes of a boundary box and 21 scores for each class (one extra class for no object), and we pick the highest score as the class for the bounded object. Conv4_3 makes a total of 38 × 38 × 4 predictions: four predictions per cell regardless of the depth of the feature maps. As expected, many predictions contain no object. SSD reserves a class “0” to indicate it has no objects.Making multiple predictions containing boundary boxes and confidence scores is called multibox.Convolutional predictors for object detectionSSD does not use a delegated region proposal network. Instead, it resolves to a very simple method. It computes both the location and class scores using small convolution filters. After extracting the feature maps, SSD applies 3 × 3 convolution filters for each cell to make predictions. (These filters compute the results just like the regular CNN filters.) Each filter outputs 25 channels: 21 scores for each class plus one boundary box (detail on the boundary box later).For example, in Conv4_3, we apply four 3 × 3 filters to map 512 input channels to 25 output channels.At first, we describe how SSD detects objects from a single layer. Actually, it uses multiple layers (multi-scale feature maps) to detect objects independently. As CNN reduces the spatial dimension gradually, the resolution of the feature maps also decrease. SSD uses lower resolution layers to detect larger scale objects. For example, the 4× 4 feature maps are used for larger scale object.SSD adds 6 more auxiliary convolution layers after the VGG16. Five of them will be added for object detection. In three of those layers, we make 6 predictions instead of 4. In total, SSD makes 8732 predictions using 6 layers.Multi-scale feature maps improve accuracy significantly. Here is the accuracy with different number of feature map layers used for object detection.The default boundary boxes are equivalent to anchors in Faster R-CNN.How do we predict boundary boxes? Just like Deep Learning, we can start with random predictions and use gradient descent to optimize the model. However, during the initial training, the model may fight with each other to determine what shapes (pedestrians or cars) to be optimized for which predictions. Empirical results indicate early training can be very unstable. The boundary box predictions below work well for one category but not for others. We want our initial predictions to be diverse and not looking similar.If our predictions cover more shapes, like the one below, our model can detect more object types. This kind of head start makes training much easier and more stable.In real-life, boundary boxes do not have arbitrary shapes and sizes. Cars have similar shapes and pedestrians have an approximate aspect ratio of 0.41. In the KITTI dataset used in autonomous driving, the width and height distributions for the boundary boxes are highly clustered.Conceptually, the ground truth boundary boxes can be partitioned into clusters with each cluster represented by a default boundary box (the centroid of the cluster). So, instead of making random guesses, we can start the guesses based on those default boxes.To keep the complexity low, the default boxes are pre-selected manually and carefully to cover a wide spectrum of real-life objects. SSD also keeps the default boxes to a minimum (4 or 6) with one prediction per default box. Now, instead of using global coordination for the box location, the boundary box predictions are relative to the default boundary boxes at each cell (∆cx, ∆cy, ∆w, ∆h), i.e. the offsets (difference) to the default box at each cell for its center (cx, cy), the width and the height.For each feature map layers, it shares the same set of default boxes centered at the corresponding cell. But different layers use different sets of default boxes to customize object detections at different resolutions. The 4 green boxes below illustrate 4 default boundary boxes.Default boundary boxes are chosen manually. SSD defines a scale value for each feature map layer. Starting from the left, Conv4_3 detects objects at the smallest scale 0.2 (or 0.1 sometimes), and then increases linearly to the rightmost layer at a scale of 0.9. Combining the scale value with the target aspect ratios, we compute the width and the height of the default boxes. For layers making 6 predictions, SSD starts with 5 target aspect ratios: 1, 2, 3, 1/2, and 1/3. Then the width and the height of the default boxes are calculated as:and aspect ratio = 1.YOLO uses k-means clustering on the training dataset to determine those default boundary boxes.SSD predictions are classified as positive matches or negative matches. SSD only uses positive matches in calculating the localization cost (the mismatch of the boundary box). If the corresponding default boundary box (not the predicted boundary box) has an IoU greater than 0.5 with the ground truth, the match is positive. Otherwise, it is negative. (IoU, the intersection over the union, is the ratio between the intersected area over the joined area for two regions.)Let’s simplify our discussion to 3 default boxes. Only default box 1 and 2 (but not 3) have an IoU greater than 0.5 with the ground truth box above (blue box). So only box 1 and 2 are positive matches. Once we identify the positive matches, we use the corresponding predicted boundary boxes to calculate the cost. This matching strategy nicely partitions what shape of the ground truth that a prediction is responsible for.This matching strategy encourages each prediction to predict shapes closer to the corresponding default box. Therefore our predictions are more diverse and more stable in the training.Multi-scale feature maps & default boundary boxesHere is an example of how SSD combines multi-scale feature maps and default boundary boxes to detect objects at different scales and aspect ratios. The dog below matches one default box (in red) in the 4 × 4 feature map layer, but not any default boxes in the higher resolution 8 × 8 feature map. The cat which is smaller is detected only by the 8 × 8 feature map layer in 2 default boxes (in blue).Higher-resolution feature maps are responsible for detecting small objects. The first layer for object detection conv4_3 has a spatial dimension of 38 × 38, a pretty large reduction from the input image. Hence, SSD usually performs badly for small objects comparing with other detection methods. If it is a problem, we can mitigate it by using images with higher resolution.The localization loss is the mismatch between the ground truth box and the predicted boundary box. SSD only penalizes predictions from positive matches. We want the predictions from the positive matches to get closer to the ground truth. Negative matches can be ignored.The confidence loss is the loss of making a class prediction. For every positive match prediction, we penalize the loss according to the confidence score of the corresponding class. For negative match predictions, we penalize the loss according to the confidence score of the class “0”: class “0” classifies no object is detected.The final loss function is computed as:where N is the number of positive matches and α is the weight for the localization loss.However, we make far more predictions than the number of objects present. So there are many more negative matches than positive matches. This creates a class imbalance that hurts training. We are training the model to learn background space rather than detecting objects. However, SSD still requires negative sampling so it can learn what constitutes a bad prediction. So, instead of using all the negatives, we sort those negatives by their calculated confidence loss. SSD picks the negatives with the top loss and makes sure the ratio between the picked negatives and positives is at most 3:1. This leads to faster and more stable training.Data augmentation is important in improving accuracy. Augment data with flipping, cropping, and color distortion. To handle variants in various object sizes and shapes, each training image is randomly sampled by one of the following options:The sampled patch will have an aspect ratio between 1/2 and 2. Then it is resized to a fixed size and we flip one-half of the training data. In addition, we can apply photo distortions.Here is the performance improvement after data augmentation:SSD makes many predictions (8732) for better coverage of location, scale, and aspect ratios, more than many other detection methods. However, many predictions contain no object. Therefore, any predictions with class confidence scores lower than 0.01 will be eliminated.non-maximum suppression (nms)SSD uses non-maximum suppression to remove duplicate predictions pointing to the same object. SSD sorts the predictions by the confidence scores. Start from the top confidence prediction, SSD evaluates whether any previously predicted boundary boxes have an IoU higher than 0.45 with the current prediction for the same class. If found, the current prediction will be ignored. At most, we keep the top 200 predictions per image.The model is trained using SGD with an initial learning rate of 0.001, 0.9 momentum, 0.0005 weight decay, and batch size 32. Using an Nvidia Titan X on the VOC2007 test, SSD achieves 59 FPS with mAP 74.3% on the VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%.Here is the accuracy comparison for different methods. For SSD, it uses an image size of 300 × 300 or 512 × 512.This is the recap of the speed performance in frame per second.Here are some key observations:SSD is a single-shot detector. It has no delegated region proposal network and predicts the boundary boxes and the classes directly from feature maps in one single pass.To improve accuracy, SSD introduces:SSD can be trained end-to-end for better accuracy. SSD makes more predictions and has better coverage on location, scale, and aspect ratios. With the improvements above, SSD can lower the input image resolution to 300 × 300 with a comparative accuracy performance. By removing the delegated region proposal and using lower resolution images, the model can run at real-time speed and still beats the accuracy of the state-of-the-art Faster R-CNN.",14/03/2018,0,27.0,3.0,974.0,438.0,31.0,5.0,0.0,10.0,en
4047,Blockchain is not only crappy technology but a bad vision for the future,Medium,Kai Stinchcombe,10100.0,11.0,2450.0,"Blockchain is not only crappy technology but a bad vision for the future. Its failure to achieve adoption to date is because systems built on trust, norms, and institutions inherently function better than the type of no-need-for-trusted-parties systems blockchain envisions. That’s permanent: no matter how much blockchain improves it is still headed in the wrong direction.This December I wrote a widely-circulated article on the inapplicability of blockchain to any actual problem. People objected mostly not to the technology argument, but rather hoped that decentralization could produce integrity.Let’s start with this: Venmo is a free service to transfer dollars, and bitcoin transfers are not free. Yet after I wrote an article last December saying bitcoin had no use, someone responded that Venmo and Paypal are raking in consumers’ money and people should switch to bitcoin.What a surreal contrast between blockchain’s non-usefulness/non-adoption and the conviction of its believers! It’s so entirely evident that this person didn’t become a bitcoin enthusiast because they were looking for a convenient, free way to transfer money from one person to another and discovered bitcoin. In fact, I would assert that there is no single person in existence who had a problem they wanted to solve, discovered that an available blockchain solution was the best way to solve it, and therefore became a blockchain enthusiast.There is no single person in existence who had a problem they wanted to solve, discovered that an available blockchain solution was the best way to solve it, and therefore became a blockchain enthusiast.The number of retailers accepting cryptocurrency as a form of payment is declining, and its biggest corporate boosters like IBM, NASDAQ, Fidelity, Swift and Walmart have gone long on press but short on actual rollout. Even the most prominent blockchain company, Ripple, doesn’t use blockchain in its product. You read that right: the company Ripple decided the best way to move money across international borders was to not use Ripples.Why all the enthusiasm for something so useless in practice?People have made a number of implausible claims about the future of blockchain—like that you should use it for AI in place of the type of behavior-tracking that google and facebook do, for example. This is based on a misunderstanding of what a blockchain is. A blockchain isn’t an ethereal thing out there in the universe that you can “put” things into, it’s a specific data structure: a linear transaction log, typically replicated by computers whose owners (called miners) are rewarded for logging new transactions.There are two things that are cool about this particular data structure. One is that a change in any block invalidates every block after it, which means that you can’t tamper with historical transactions. The second is that you only get rewarded if you’re working on the same chain as everyone else, so each participant has an incentive to go with the consensus.The end result is a shared definitive historical record. And, what’s more, because consensus is formed by each person acting in their own interest, adding a false transaction or working from a different history just means you’re not getting paid and everyone else is. Following the rules is mathematically enforced—no government or police force need come in and tell you the transaction you’ve logged is false (or extort bribes or bully the participants). It’s a powerful idea.So in summary, here’s what blockchain-the-technology is: “Let’s create a very long sequence of small files — each one containing a hash of the previous file, some new data, and the answer to a difficult math problem — and divide up some money every hour among anyone willing to certify and store those files for us on their computers.”Now, here’s what blockchain-the-metaphor is: “What if everyone keeps their records in a tamper-proof repository not owned by anyone?”An illustration of the difference: In 2006, Walmart launched a system to track its bananas and mangoes from field to store. In 2009 they abandoned it because of logistical problems getting everyone to enter the data, and in 2017 they re-launched it (to much fanfare) on blockchain. If someone comes to you with “the mango-pickers don’t like doing data entry,” “I know: let’s create a very long sequence of small files, each one containing a hash of the previous file” is a nonsense answer, but “What if everyone keeps their records in a tamper-proof repository not owned by anyone?” at least addresses the right question!People treat blockchain as a “futuristic integrity wand”—wave a blockchain at the problem, and suddenly your data will be valid. For almost anything people want to be valid, blockchain has been proposed as a solution.It’s true that tampering with data stored on a blockchain is hard, but it’s false that blockchain is a good way to create data that has integrity.It’s true that tampering with data stored on a blockchain is hard, but it’s false that blockchain is a good way to create data that has integrity.To understand why this is the case, let’s work from the practical to the theoretical. For example, let’s consider a widely-proposed use case for blockchain: buying an e-book with a “smart” contract. The goal of the blockchain is, you don’t trust an e-book vendor and they don’t trust you (because you’re just two individuals on the internet), but, because it’s on blockchain, you’ll be able to trust the transaction.In the traditional system, once you pay you’re hoping you’ll receive the book, but once the vendor has your money they don’t have any incentive to deliver. You’re relying on Visa or Amazon or the government to make things fair—what a recipe for being a chump! In contrast, on a blockchain system, by executing the transaction as a record in a tamper-proof repository not owned by anyone, the transfer of money and digital product is automatic, atomic, and direct, with no middleman needed to arbitrate the transaction, dictate terms, and take a fat cut on the way. Isn’t that better for everybody?Hm. Perhaps you are very skilled at writing software. When the novelist proposes the smart contract, you take an hour or two to make sure that the contract will withdraw only an amount of money equal to the agreed-upon price, and that the book — rather than some other file, or nothing at all — will actually arrive.Auditing software is hard! The most-heavily scrutinized smart contract in history had a small bug that nobody noticed — that is, until someone did notice it, and used it to steal fifty million dollars. If cryptocurrency enthusiasts putting together a $150m investment fund can’t properly audit the software, how confident are you in your e-book audit? Perhaps you would rather write your own counteroffer software contract, in case this e-book author has hidden a recursion bug in their version to drain your ethereum wallet of all your life savings?It’s a complicated way to buy a book! It’s not trustless, you’re trusting in the software (and your ability to defend yourself in a software-driven world), instead of trusting other people.Another example: the purported advantages for a voting system in a weakly-governed country. “Keep your voting records in a tamper-proof repository not owned by anyone” sounds right — yet is your Afghan villager going to download the blockchain from a broadcast node and decrypt the Merkle root from his Linux command line to independently verify that his vote has been counted? Or will he rely on the mobile app of a trusted third party — like the nonprofit or open-source consortium administering the election or providing the software?These sound like stupid examples — novelists and villagers hiring e-bodyguard hackers to protect them from malicious customers and nonprofits whose clever smart-contracts might steal their money and votes?? — until you realize that’s actually the point. Instead of relying on trust or regulation, in the blockchain world, individuals are on-purpose responsible for their own security precautions. And if the software they use is malicious or buggy, they should have read the software more carefully.You actually see it over and over again. Blockchain systems are supposed to be more trustworthy, but in fact they are the least trustworthy systems in the world. Today, in less than a decade, three successive top bitcoin exchanges have been hacked, another is accused of insider trading, the demonstration-project DAO smart contract got drained, crypto price swings are ten times those of the world’s most mismanaged currencies, and bitcoin, the “killer app” of crypto transparency, is almost certainly artificially propped up by fake transactions involving billions of literally imaginary dollars.Blockchain systems do not magically make the data in them accurate or the people entering the data trustworthy, they merely enable you to audit whether it has been tampered with. A person who sprayed pesticides on a mango can still enter onto a blockchain system that the mangoes were organic. A corrupt government can create a blockchain system to count the votes and just allocate an extra million addresses to their cronies. An investment fund whose charter is written in software can still misallocate funds.How then, is trust created?In the case of buying an e-book, even if you’re buying it with a smart contract, instead of auditing the software you’ll rely on one of four things, each of them characteristics of the “old way”: either the author of the smart contract is someone you know of and trust, the seller of the e-book has a reputation to uphold, you or friends of yours have bought e-books from this seller in the past successfully, or you’re just willing to hope that this person will deal fairly. In each case, even if the transaction is effectuated via a smart contract, in practice you’re relying on trust of a counterparty or middleman, not your self-protective right to audit the software, each man an island unto himself. The contract still works, but the fact that the promise is written in auditable software rather than government-enforced English makes it less transparent, not more transparent.The same for the vote counting. Before blockchain can even get involved, you need to trust that voter registration is done fairly, that ballots are given only to eligible voters, that the votes are made anonymously rather than bought or intimidated, that the vote displayed by the balloting system is the same as the vote recorded, and that no extra votes are given to the political cronies to cast. Blockchain makes none of these problems easier and many of them harder—but more importantly, solving them in a blockchain context requires a set of awkward workarounds that undermine the core premise. So we know the entries are valid, let’s allow only trusted nonprofits to make entries—and you’re back at the good old “classic” ledger. In fact, if you look at any blockchain solution, inevitably you’ll find an awkward workaround to re-create trusted parties in a trustless world.Yet absent these “old way” factors—supposing you actually attempted to rely on blockchain’s self-interest/self-protection to build a real system—you’d be in a real mess.Eight hundred years ago in Europe — with weak governments unable to enforce laws and trusted counterparties few, fragile and far between — theft was rampant, safe banking was a fantasy, and personal security was at the point of the sword. This is what Somalia looks like now, and also, what it looks like to transact on the blockchain in the ideal scenario.Somalia on purpose. That’s the vision. Nobody wants it!Even the most die-hard crypto enthusiasts prefer in practice to rely on trust rather than their own crypto-medieval systems. 93% of bitcoins are mined by managed consortiums, yet none of the consortiums use smart contracts to manage payouts. Instead, they promise things like a “long history of stable and accurate payouts.” Sounds like a trustworthy middleman!Same with Silk Road, a cryptocurrency-driven online drug bazaar. The key to Silk Road wasn’t the bitcoins (that was just to evade government detection), it was the reputation scores that allowed people to trust criminals. And the reputation scores weren’t tracked on a tamper-proof blockchain, they were tracked by a trusted middleman!If Ripple, Silk Road, Slush Pool, and the DAO all prefer “old way” systems of creating and enforcing trust, it’s no wonder that the outside world had not adopted trustless systems either!A decentralized, tamper-proof repository sounds like a great way to audit where your mango comes from, how fresh it is, and whether it has been sprayed with pesticides or not. But actually, laws on food labeling, nonprofit or government inspectors, an independent, trusted free press, empowered workers who trust whistleblower protections, credible grocery stores, your local nonprofit farmer’s market, and so on, do a way better job. People who actually care about food safety do not adopt blockchain because trusted is better than trustless. Blockchain’s technology mess exposes its metaphor mess — a software engineer pointing out that storing the data a sequence of small hashed files won’t get the mango-pickers to accurately report whether they sprayed pesticides is also pointing out why peer-to-peer interaction with no regulations, norms, middlemen, or trusted parties is actually a bad way to empower people.Like the farmer’s market or the organic labeling standard, so many real ideas are hiding in plain sight. Do you wish there was a type of financial institution that was secure and well-regulated in all the traditional ways, but also has the integrity of being people-powered? A credit union’s members elect its directors, and the transaction-processing revenue is divided up among the members. Move your money! Prefer a deflationary monetary policy? Central bankers are appointed by elected leaders. Want to make elections more secure and democratic? Help write open source voting software, go out and register voters, or volunteer as an election observer here or abroad! Wish there was a trusted e-book delivery service that charged lower transaction fees and distributed more of the earnings to the authors? You can already consider stated payout rates when you buy music or books, buy directly from the authors, or start your own e-book site that’s even better than what’s out there!Projects based on the elimination of trust have failed to capture customers’ interest because trust is actually so damn valuable. A lawless and mistrustful world where self-interest is the only principle and paranoia is the only source of safety is a not a paradise but a crypto-medieval hellhole.As a society, and as technologists and entrepreneurs in particular, we’re going to have to get good at cooperating — at building trust, and, at being trustworthy. Instead of directing resources to the elimination of trust, we should direct our resources to the creation of trust—whether we use a long series of sequentially hashed files as our storage medium or not.Kai Stinchcombe coined the terms “crypto-medieval” “futuristic integrity wand” and “smart mango.” Please use freely: coining terms makes you a futurist.",05/04/2018,0,0.0,33.0,1072.0,600.0,9.0,0.0,0.0,41.0,en
4048,Clustering Analysis in R using K-means,Towards Data Science,Luiz Fonseca,59.0,8.0,1011.0,"The purpose of clustering analysis is to identify patterns in your data and create groups according to those patterns. Therefore, if two points have similar characteristics, that means they have the same pattern and consequently, they belong to the same group. By doing clustering analysis we should be able to check what features usually appear together and see what characterizes a group.In this post, we are going to perform a clustering analysis with multiple variables using the algorithm K-means. The intention is to find groups of mammals based on the composition of the species’ milk. The main points covered here are:The dataset used is part of the package cluster.datasets and contains 25 observations on the following 6 variables:name — a character vector for the name of the animals water — a numeric vector for the water content in the milk sample protein — a numeric vector for the amount of protein in the milk samplefat — a numeric vector for the fat content in the milk sample lactose — a numeric vector for the amount of lactose in the milk sample ash — a numeric vector for the amount of mineral in the milk sampleLet’s take a look at a sample of the dataThe charts below show us the distribution for each variable. Each point represents a mammal species (25 in total).Each variable has different behavior and we could identify groups of mammals on each one individually, but that’s not the purpose here.All the variables will be used in the clustering on a linear scale. Sometimes, when the values (for each feature) are in a big range, for example from 0 up to 1 million, it’s interesting to use a logarithmic scale because on a log scale we would highlight bigger differences between the values and smaller differences would be considered less important. Since the values in our dataset vary between 0 and 100, we are going to use a linear scale, which considers differences between values equally important.The clustering algorithm that we are going to use is the K-means algorithm, which we can find in the package stats. The K-means algorithm accepts two parameters as input:Conceptually, the K-means behaves as follows:The bigger is the K you choose, the lower will be the variance within the groups in the clustering. If K is equal to the number of observations, then each point will be a group and the variance will be 0. It’s interesting to find a balance between the number of groups and their variance. A variance of a group means how different the members of the group are. The bigger is the variance, the bigger is the dissimilarity in a group.How do we choose the best value of K in order to find that balance?To answer that question, we are going to run K-means for an arbitrary K. Let’s pick 3.The kmeans() function outputs the results of the clustering. We can see the centroid vectors (cluster means), the group in which each observation was allocated (clustering vector) and a percentage (89.9%) that represents the compactness of the clustering, that is, how similar are the members within the same group. If all the observations within a group were in the same exact point in the n-dimensional space, then we would achieve 100% of compactness.Since we know that, we will use that percentage to help us decide our K value, that is, a number of groups that will have satisfactory variance and compactness.The function below plots a chart showing the “within sum of squares” (withinss) by the number of groups (K value) chosen for several executions of the algorithm. The within sum of squares is a metric that shows how dissimilar are the members of a group., the greater is the sum, the greater is the dissimilarity within a group.By Analysing the chart from right to left, we can see that when the number of groups (K) reduces from 4 to 3 there is a big increase in the sum of squares, bigger than any other previous increase. That means that when it passes from 4 to 3 groups there is a reduction in the clustering compactness (by compactness, I mean the similarity within a group). Our goal, however, is not to achieve compactness of 100% — for that, we would just take each observation as a group. The main purpose is to find a fair number of groups that could explain satisfactorily a considerable part of the data.So, let’s choose K = 4 and run the K-means again.Using 3 groups (K = 3) we had 89.9% of well-grouped data. Using 4 groups (K = 4) that value raised to 95.1%, which is a good value for us.We may use the silhouette coefficient (silhouette width) to evaluate the goodness of our clustering.The silhouette coefficient is calculated as follows:So, the interpretation of the silhouette width is the following:The silhouette plot below gives us evidence that our clustering using four groups is good because there’s no negative silhouette width and most of the values are bigger than 0.5.The following plot shows the final result of our clustering. The actual plot is interactive, but the image below is not. You can reproduce the plot using the code below. In the interactive plot, you may isolate the groups to better understand each one individually.The purpose of clustering analysis is to identify patterns in the data. As we can see in the plot above, observations within the same group tend to have similar characteristics.Let’s take the green group as an instance to evaluate. The two mammal species that belong to that group, namely seal and dolphin, they have the lowest percentage of water (44.9% and 46.4%); they both have around 10% of protein in their milk; they have the highest percentage of fat in the milk among all other species as well as the lowest percentage of lactose. This is the pattern found that puts seals and dolphins together in the same group. We can identify such patterns in the other groups as well.Thank you for reading. I hope it was a pleasurable and useful reading.",16/08/2019,4,28.0,0.0,1226.0,1082.0,5.0,5.0,0.0,2.0,en
4049,Transformer-based Sentence Embeddings,The Startup,Haaya Naushan,662.0,8.0,15.0,Natural language processing (NLP) is a diverse field; the approaches and techniques are as varied…,22/12/2020,0,0.0,0.0,1400.0,1050.0,1.0,0.0,0.0,2.0,en
4050,Stash Data Center ベータ版リリース。Git を大規模に利用,Medium,Jerome Bouchon,4.0,5.0,105.0,"これまで Stash は常に、最高のスピードと安全性を実現する Git リポジトリ管理ツール製品となってきました。そして今回、最高の拡張性も提供します。Stash Data Center のリリースの発表です (本日、ベータ版リリース)! このクラスタリング搭載の Stash Data Center デプロイメント オプションは、エンタープライズの大規模システムでのニーズを満たすことを目的としています。今すぐベータ版トライアルStash Data Center はアクティブ/アクティブ クラスタリングを提供し、ユーザーは途切れることなく確実に Git リポジトリにアクセスできます。 Data Center は負荷バランシング技術と冗長化技術を使い、ハードウェア障害による予期せぬシステムのダウンタイムのリスクを軽減します。データベースクラスタリングと共有ファイルシステムの業界標準技術を組み合わせ、Stash は単一障害点を排除します。Stash Data Center の初期設定プロセスの中で、クラスタリングは簡単に設定できますので、チームはすぐに立ち上げ稼働することができます。さらに、稼働規模を拡張するためのノードの追加や削除にダウンタイムは不要ですので、開発チームやビルドプロセスを妨害することはありません。組織内で Git ベースのソリューションを利用するチームが増えるにつれ、開発者とビルドサーバーからのトラフィック量が急速に増加し、リソースを圧迫することがあります。Stash Data Center は、負荷継続時やピーク負荷時に、より高いアプリケーションスループットに対応でき、ユーザーやビルドの追加はアプリケーション全体のパフォーマンスに影響を与えることはありません。Data Center はユーザー単位でライセンス提供されます。他社製品のように CPU 単位ではありませんので、拡張に応じた費用が予測可能です。お客様の組織は Git の規模増大に依存していますよね? ノードを追加しましょう。継続的インテグレーション・サーバーに悩まされていませんか? さらなるビルドに対応するためノードを追加しましょう。ニーズに合わせてシステムをカスタマイズすれば、ソフトウェアの費用を追加することなく、チームは最上級の性能を手に入れられます。この即時拡張性により、規模を問わずパフォーマンスへの不安がなくなります。Stash Data Center は、真の意味でエンタープライズ向けのセキュリティとコラボレーションワークフローを提供する唯一の Git ソリューションです。オンプレミス型ソリューションとして、リポジトリは常に、お客様のファイアウォールの安全性により保護されます。また、Stash は独自の 4 層構造のセキュリティを有し、グローバル、プロジェクト、リポジトリ、ブランチの 4 段階でパーミッションのカスタマイズが可能です。これが、エンタープライズ環境で使用される多様なワークフローが必要とする柔軟性とセキュリティを実現します。お客様の LDAP システムと結合する性能と組み合わせれば、Data Center はあらゆるエンタープライズ環境に最適なものとなります。さらに、Stash Data Center では各チームにあらゆる種類 (または、複数) のワークフローを利用するオプションも提供します。チーム固有のニーズに対して柔軟に対応しつつ、ブランチベースのワークフローとプルリクエストに対するベストプラクティスを実現します。Stash Data Center に装備されたワンクリックでのブランチ作成機能とオプションのプルリクエスト行使は、あらゆるチームの Git ワークフローへのニーズに対応します。本日より、Stash Data Center ベータ版の試用ができるようになりました。Stash Data Center リリース後は、アトラシアン Premier Support、Technical Account Management、認定エンタープライズパートナーと合わせてご購入いただけます。JIRA Data Center や Confluence Data Center と同様の非常にお得な価格設定で、1,000 ユーザーごと年間 $24,000 の価格で提供いたします。Stash Data Center ベータ版にサインアップ、または、さらに詳しい情報をお求めの方は、go.atlassian.com/StashDC をご覧ください。お問い合わせ*本ブログは Atlassian Blogs の翻訳です。本文中の日時などは投稿当時のものですのでご了承ください。*原文 : 2014 年 9 月 10 日投稿 “Scaling Git with Stash Data Center“",16/09/2014,0,0.0,2.0,1166.0,595.0,3.0,0.0,0.0,3.0,ja
4051,CNN-LSTM-Based Models for Multiple Parallel Input and Multi-Step Forecast,Towards Data Science,Halil Ertan,213.0,19.0,3431.0,"Time series forecasting is a very popular field of machine learning. The reason behind this is the widespread usage of time series in daily life in almost every domain. Going into details for time series forecasting, we encounter lots of different kinds of sub-fields and approaches. In this writing, I will focus on a specific subdomain that is performing multi-step forecasts by receiving multiple parallel time series, and also mention basic key points that should be taken into consideration in time series forecasting. Note that forecasting models differ from predictive models at various points.Let's think about lots of network devices spread over a large geography, and traffic flows through these devices continuously. Another example might be about lots of different temperature devices which measure the temperature of the weather in different locations continuously or another one might be to calculate the energy consumption of numerous devices in a system. We can just extend these kinds of examples easily. The goal is simple; forecast the next multiple time steps; this corresponds to forecasting traffic value, temperature, or the energy consumption of many devices respectively in our case.The first thing that comes to mind is certainly modeling each device separately. I think we will probably get the highest forecast results. What about if there are more than 100 different devices or even more? That means designing 100 different models. It is not much applicable considering the deploying, monitoring, and maintenance costs. Therefore, we aim to cover the entire problem with just a single model. I think it is still a very popular and searching field of time series forecasting, although time series forecasting has been on the table for a long time. I encounter lots of different academic research in this field.This problem might also be defined as seq2seq prediction. In particular, it is a very common case in speech recognition and translations. LSTM is very convenient for these kinds of problems. CNN can also be considered as another type of neural network and is commonly used in image processing tasks. Typically, it is used in feature extraction and time series forecasting as well. I will mention the appliance of LSTM and CNN for time series forecasting in multiple parallel inputs and multi-step forecasting cases. Explanation of LSTM and CNN is simply beyond the scope of the writing.To make it more clear, I depict a simple data example below. It is an example of forecasting traffic values of 3 different devices for the next 2 hours by looking at the previous 3 hours time slots.In this writing, I will utilize a data set consisting of a CPU usage of 288 different servers within 15 minutes time interval. For simplicity, I will narrow down the scope into 3 servers.Pre-modelling is very critical in neural network implementations. You mostly can not simply feed the neural network with raw data directly, a simple transformation is needed. However, we should apply a few more additional steps to the raw data before the transformation.The following is for pivoting the raw data in our case.The following is for interpolating the missing values in the data set. Imputing null values in time series is very crucial on its own, and a challenging task as well. We don’t have many null values in this data set and imputing null values is beyond the scope of this writing, therefore I perform a simple implementation.The first thing before passing into the modeling phase, at the very beginning of the data preprocessing step for time series forecasting is plotting the time series in my opinion. Let's see what tells the data to us. As I mentioned before, I select only 3 different servers for simplicity. As it can be seen, all of them follow very similar patterns in different scales.The next step is to split the data set into train and test sets. It is a bit different in time series from conventional machine learning implementations. We can intuitively determine a split date for separating the data set.We also normalize the data before feeding it into any neural network model because of its sensitivity.And the final stage is to transform both train and test set into an acceptable format for neural networks. The following method can be implemented to transform the data. It simply expects 2 parameters except for the sequence itself, which are time lag(steps of looking back), and forecasting range respectively. You can also take a look at TimeSeriesGenerator class defined in Keras to transform the data set.An LSTM model expects data to have the shape; [samples, timesteps, features]. Similarly, CNN also expects 3D data as LSTMs.Additional NotesI want to also mention additional pre-modeling steps for time series forecasting. These are not directly scope of the writing but worth knowing.Stationary is a very important issue for time series, most forecasting algorithm like ARIMA expects time series to be stationary. Stationary time series basically keep very similar mean, variance values over time and do not include seasonality and trend. To make time series stationary, the most straightforward method is to take the difference of subsequent values in the sequence. If variance fluctuates very much compared to mean, it also might be a good idea to take the log of the sequence to make it stationary. Unlike most of the other forecasting algorithms, LSTMs are capable of learning nonlinearities and long-term dependencies in the sequence. Thus, stationary is a less of concern of LSTMs. Nevertheless, it is a good practice to make the time series stationary and increase the performance of LSTMs a bit higher. There are many stationary tests, the most famous one is Augmented Dicky-Fuller Test, which has off-the-shelf implementations in python.There is an excellent article here, that simply designs a model to forecast a time series generated by a random walk process, and evaluates a very high accuracy. In fact, forecasting a random walk process is impossible. However, this misleading inference might occur if you use directly raw data instead of stationary differenced data.Another thing to mention about time series is to plot ACF and PACF plots and investigate dependencies of time series with respect to different historical lag values. That will definitely give an insight into the time series you are dealing with.It is always a good idea to have a baseline model to measure the performance of your model relatively. Baseline models are expected to be simple, fast, and repeatable. The most common method to establish a baseline model is the persistence model. It has a very simple working principle, just forecast the next time step as the previous one, in other saying t+1 is the same as t. For multi-step forecasting, it might be adapted forecast t+1, t+2, t+3 as t, entire forecast horizon will be the same. In my opinion, that is not very reasonable. Instead of that, I prefer to forecast each time step in the forecast horizon as the mean of the previous same time of the same devices. A simple code snippet is the following.As it can be seen, our baseline model forecast with approximately 16.56% error margin. I will go into detail for the evaluation metrics in the Evaluation of the Model part. Let’s see if we might outperform this result.I will mention different neural network-based models for Multiple Parallel Input and Multi-Step Forecast. Before describing the models, let me share a few common things and code snippets like Keras callbacks, applying the inverse transformation, and evaluating the results.The used callbacks while compiling the models are the following. ModelCheckpoint is to save the model(weights) at certain frequencies. EarlyStopping is used for stopping the progress if the monitored evaluation metric is no longer improved. ReduceLROnPlateau is for decreasing the learning rate when the monitored metric has stopped improving.Since we apply normalization to the data before feeding it to the model, we should transform it back to the original scale to evaluate the forecast. We might simply utilize the following method. In the case of differencing the data to make it stationary, you should first invert scaling and then invert differencing sequentially. The order is opposite for forecasting, that is you first apply differencing and then normalize the data. For inverting the differencing data, the simple approach is to cumulatively add the difference forecasts to the last cumulative observation.To evaluate the forecast, I simply take into consideration mean square error(mse), mean absolute error(mae), and mean absolute percentage error(mape) respectively. You can extend the evaluation metrics. Note that, root mean square error(rmse) and mae give error in the same units as the variable itself and are widely used. I will just compare the models according to mape in this writing.The common imported packages are below.At this step, I think mentioning about TimeDistributed and RepeatVector layer in Keras would be beneficial. These are not very frequently used layers, however, they might be very useful in some specific cases like in our case.In short, TimeDistributed layer is a kind of wrapper and expects another layer as an argument. It applies this layer to every temporal slice of input and therefore allows to build models that have one-to-many, many-to-many architectures. Similarly, it expects the input at least as 3 dimensions. I am aware that it is not very clear for beginners, you can find a useful discussion here.RepeatVector basically repeats inputs n times. In other words, it increases the dimension of the output shape by 1. There is a good explanation and diagram for RepeatVector here, take a look.The above are defined for compiling the model.Encoder-Decoder ModelEncoder-decoder architecture is a typical solution for sequence to sequence learning. This architecture contains at least two RNN/LSTMs, and one of them behaves as an encoder while the other one behaves as a decoder. The encoder is basically responsible for reading and interpreting the input. The encoder part compresses the input into a small representation(a fixed-length vector) of the original input, and this context vector is given to the decoder part as input to be interpreted and perform forecasting. A RepeatVector layer is used to repeat the context vector we obtain from the encoder part. It is repeated for the number of future steps you want to forecast and is fed into the decoder part. The output received from the decoder in terms of each time step is mixed. A fully connected Dense layer is applied to each time step via TimeDistributed wrapper, so separates the output for each time step. In each encoder and decoder, different kinds of architectures might be utilized. One example is given in the next part in which CNN is used as a feature extractor in the encoder.CNN-LSTM Encoder-Decoder ModelThe following model is an extension of encoder-decoder architecture where the encoder part consists of Conv1D layers, unlike the previous model. First of all, two subsequent Conv1D layers are placed at the beginning to extract features, and then it is flattened after pooling the results of Conv1D. The rest of the architecture is very similar to the previous model.For simplicity, I just do not share plotting, fitting of the model, forecasting of the test set, and inverse transforming step those will be exactly the same as the rest of the models.Vector Output ModelThis architecture might be thought of as a much more common architecture compared to the above-mentioned models, however, it is not very suitable for our case. Nevertheless, I share an example model to give an idea. At variance with encoder-decoder architecture, neither RepeatVector nor TimeDistributed layer exists. The point is to add a Dense layer with FORECAST_RANGE*n_features node, and then reshape it accordingly at the next layer. You might also design a similar architecture with RepeatVector layer instead of Reshape layer.The time steps of each series would be flattened in this structure and must interpret each of the outputs as a specific time step for a specific series during training and prediction. That means we also might reshape our label set as 2 dimensions rather than 3 dimensions, and interpret the results in the output layer accordingly without using Reshape layer. For simplicity, I did not change the shape of the label set, but just keep it in mind this alternative way as well. I also utilized Conv1D layers at the beginning of the architecture.This structure might be also called a multi-channel model. Do not overestimate the names. When dealing with multiple time series, CNNs with multiple channels are utilized traditionally. In this structure, each channel corresponds to a single time series and similarly extracts convolved features separately for each time series. Since all of the extracted features are combined before feeding into the LSTM layer, some typical features of each time series might be lost.Multi-Head CNN-LSTM ModelThis architecture is a bit different from the above-mentioned models. It is explained very clearly in the study of Canizo. The multi-head structure uses multiple one-dimensional CNN layers in order to process each time series and extract independent convolved features from each time series. These separate CNNs are called “head” and flattened, concatenated, and reshaped respectively before feeding into the LSTM layer. To summarize, multi-head structures utilize multiple CNNs rather than only one CNN like in multi-channel structure. Therefore, they might be more successful to keep significant features of each time series and make better forecasts in this sense.Evaluation is basically the same with any forecasting modeling approach and the general performance of the models might be calculated with the evaluate_forecast method I shared in previous parts. However, there are 2 different points in our case, and it would be beneficial to take them into consideration as well. The first one is that we are performing multi-step forecasting, so we might want to analyze our forecasting accuracy for each of the time steps separately. This would give an insight into selecting an accurate forecast horizon. The second one is that we are performing a forecast for multiple parallel time series. Therefore, it would also be beneficial to observe the results of each time series forecasting. The model might work significantly worse for a specific time series than the rest of the time series.You can simply utilize different kinds of evaluation metrics, I prefer Mean Absolute Percentage Error(mape) in this article that handles the different time series scales.Given the stochastic nature of the models, it is good practice to evaluate a given model multiple times and report the mean performance on a test data set. For simplicity, I just run them only once in this article.With respect to each time step output, I just share a simple code snippet and figure that represents the mape values of forecasting for each time step. As expected, mape increases along with an increasing forecasting range. The figure represents the Multi-Head CNN-LSTM architecture and might be applied directly to other architectures I mentioned above. If you observe a sharp increase in mape, you might decrease your forecast horizon and set it to the point just before the sharp increase.The following code snippet might be used for analyzing model performance with respect to different input time series. The forecast performance of the model is better for all CPUs compared to the overall results of the baseline model. Although mae of the second CPU is significantly low compared to the other CPUs, its mape is significantly higher than the others. Actually, it is one of the reasons why I am using mape in this article as an evaluation criterion. Each of the CPU usage values behaves similarly, but completely on different scales. To be honest, it has been a good example of my point. Why error ratio for the second CPU is so high? What should I do in such circumstances? Dividing the entire time series into subparts, and developing separate models for each of them might be an option. What else? I really appreciate reading any valuable approaches in the comments.Even though it is not very meaningful for our case, plotting a histogram for the performance of each time series would be expressive for data sets including more parallel time series.Before finalizing the article, I want to touch on a few important points about time series forecasting in real life. First of all, traditional methods used in machine learning for model evaluation are mostly not valid in time series forecasting. It is because they do not take into consideration temporal dependencies that are the case for time series.3 different methodologies are mentioned here. The most straightforward one is to determine a split point and separate the data set as train and test without shuffling just as we have done in this article. However, in real life, it does not give a robust insight into the performance of the model.The other one is to repeat the same strategy multiple times, that is multiple train test splits. In this method, you create multiple train and test sets and so multiple models. You should consider keeping the test set size the same to compare the models' performance correctly. You might also keep the train set size the same by only taking into account a fixed length of the latest data. Note that there is a package in sklearn called as TimeSeriesSplit to perform this methodology.The final most robust methodology is walk forward validation which is the k-fold cross-validation of the time series world. In this approach, a new model is created after each forecast by including new known value to the train set. It is repeated continuously with a sliding window approach and takes into a minimum number of observations for training the model. It is the most robust method but comes with a cost obviously. To create many models for especially mass amounts of data might be cumbersome.Apart from the traditional ML approaches, time series forecasting models should be updated more frequently in order to capture changing trend behaviors. However, in my opinion, the model does not need to be retrained whenever you want to generate a new prediction as stated here. It will be very costly and unnecessary if the time series does not change in a very frequent and drastic way. What I agree totally with the above article is that accurately defining confidence intervals for forecasting is as important as forecasting itself, even more important in real-life scenarios considering anomaly detection use cases. It deserves to be the topic of another article.Last but not definitely least, running a multi-step forecasting model in production is quite different from traditional machine learning approaches. There are a few options, the most common ones are recursive and direct strategies. In recursive strategy, at each forecasting step, the model is used to predict one step ahead and the value obtained from the forecasting is then fed into the same model to predict the following step. The same model is used for each forecast. However, at this strategy, the forecasting error propagates along the forecast horizon as you can expect. And it might be a burden, especially for long forecast horizons. On the other hand, a separate model is designed for each forecast horizon in direct strategy. It will be obviously overwhelmed with the increasing length of the forecast horizon, which means an increasing number of models. Furthermore, it does not take into account statistical dependencies among the predictions since each model is independent of each others. As another strategy, you might also design a model that is capable of performing multi-step forecasting at once like what we did in this article.In this article, I focus on a very specific use case for time series forecasting but a common use case at the same time in real-life scenarios. Additionally, I mention a few general key points that should be taken into consideration while implementing forecasting models.When you encounter multiple time series and are supposed to perform multi-step forecasting for each of them, firstly try to create separate models to achieve the best result. However, the number of time series might be extremely much in real life, you should also consider using above mentioned single model architectures in such cases. You might also utilize different kinds of layers like Bidirectional, ConvLSTM adhering to the architectures, and get better results by tuning parameters. The use cases I mention mostly consist of the same unit time series like traffic values flowing through each device or temperature values of different devices spread over a large area. To see the results with different kinds of time series would be interesting as well. For instance, a single model approach for forecasting temperature, humidity, pressure at once. I believe it is still an open research area considering lots of different academic studies in this area. In this sense, I appreciate hearing different approaches as well in the comments.www.sciencedirect.comlink.springer.comieeexplore.ieee.orgmachinelearningmastery.commachinelearningmastery.comirosyadi.netlify.app",17/11/2021,19,5.0,56.0,713.0,585.0,9.0,0.0,0.0,34.0,en
4052,Sentiment analysis using RNNs(LSTM),Towards Data Science,Manish Chablani,1700.0,4.0,445.0,"Here we use the example of reviews to predict sentiment (even though it can be applied more generically to other domains for example sentiment analysis for tweets, comments, customer feedback, etc). Whole idea here is that movie reviews are made of sequence of words and order of words encode lot of information that is useful to predict sentiment. Step 1 is to map words to word embeddings (see post 1 and 2 for more context on word embeddings). Step 2 is the RNN that receives a sequence of vectors as input and considers the order of the vectors to generate prediction.The architecture for this network is shown below.Here, we’ll pass in words to an embedding layer. You can actually train up an embedding with word2vec and use it here. But it’s good enough to just have an embedding layer and let the network learn the embedding table on it’s own.From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. We’re using the sigmoid because we’re trying to predict if this text has positive or negative sentiment. The output layer will just be a single unit then, with a sigmoid activation function.We don’t care about the sigmoid outputs except for the very last one, we can ignore the rest. We’ll calculate the cost from the output of the last step and the training label.Have fixed length reviews encoded as integers and then converted to embedding vectors passed to LSTM layers in recurrent manner and pick the last prediction as output sentiment.One thing in my experiments I could not explain is when I encode the words to integers if I randomly assign unique integers to words the best accuracy I get is 50–55% (basically the model is not doing much better than random guessing). However if the words are encoded such that highest frequency words get the lowest number then the model accuracy is 80% in 3–5 epochs. My guess is this is necessary to train the embedding layer but cannot find an explanation on why anywhere.https://github.com/mchablani/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN.ipynbTake all the words in reviews and encode them with integers. Now each review is an ordered array of integers. Make each review fixed size (say 200), so shorter reviews get padded with 0’s in front and longer reviews get truncated to 200. Since we are padding with 0’s the corpus of words to int mapping starts with 1. Labels are are encoded as 1s and 0s for ‘positive’ and ‘negative’.Credits: From lecture notes: https://classroom.udacity.com/nanodegrees/nd101/syllabus",21/06/2017,2,0.0,0.0,489.0,578.0,1.0,0.0,0.0,4.0,en
4053,Activation Functions,Medium,Chinmay,3.0,5.0,545.0,"What are Activation Functions? Why are they used? why are there so many types? Does one works better than other?Firstly, lets recap. A deep layer neural network as seen below receives the input and makes the decision based on its weights and biases which are learned during its backpropagation. As the Hidden layers increases , the decision making becomes more complex and sometimes leads to taking noise into consideration. When output is produced , mot all the neurons in the layers have equal say/contribution, and its because of the weights and bias updated during backpropagation. Done.Then were is Activation function used? And Why?So basically, Activation functions decide whether the particular neuron or node to be fired /activated or not.As said, Activation function is additional step and also it introduces complexity. Then why to use it, why cant we use just a linear function?Let us consider the neural network in figure with two hidden layers and no activationNo matter how many hidden layers are included, the end result is just another linear function. Although linear transformations make the neural network simpler, but this network would be less powerful and will not be able to learn the complex patterns from the data. Hence non linear activation functions are used.Moreover, Linear function in algebraic terms f(x)=x. Rule of Activation function is , function should be differentiable, and yes above function is differentiable ie f ’(x)=1.But,if there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input xYou will thus find the same gradient for any neuron output when you use the linear activation function, namely 1.The Sigmoid function is the most frequently used activation function in the beginning of deep learning. It is a smoothing function that is easy to derive.Pros:Cons:It is very similar to the sigmoid activation function and even has the same S-shape.The function takes any real value as input and outputs values in the range -1 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.Pros:Cons:The ReLU (Rectified Linear Unit) function is an activation function that is currently more popular. Compared with the sigmoid function and the tanh function, it has the following advantages:1) When the input is positive, there is no gradient saturation problem.2) The calculation speed is much faster. The ReLU function has only a linear relationship. Whether it is forward or backward, it is much faster than sigmod and tanh. (Sigmoid and tanh need to calculate the exponent, which will be slower.).Since negative values of X are capped to 0 fewer neuron are activated hence the network is lighter and computation is faster.Cons:Instead of making negative values complete 0, for Leaky ReLU 0.01x instead of 0 is used, so that negative values are considered during backpropogation. Similarly, parametric Relu has alpha*x in place of 0.01.Here alpha is learnable parameter.Sigmoid return values form 0 to 1,which can be considered as the probability of data point belonging to one class. Sigmoid is used for Binary classification.Whereas, Softmax Function is used for multiclass classification. It calculates the probability of data-points belonging to each class.Thank You, If you find it helpful, please vote.",15/08/2021,0,4.0,5.0,606.0,254.0,12.0,6.0,0.0,0.0,en
4054,Sentence correction using Deep learning techniques,Medium,Sourav kumar,28.0,12.0,1722.0,"Most of us use social media platforms to communicate with people or express ourselves in text. Generally, Most of the ML/DL models used this text to determine the sentiments or to predict any criminal activities and many more NLP-related tasks. The ML and DL models are trained in traditional language, mostly English for any NLP-related task.But in actual, we use informal English to communicate with friends especially short forms or abbreviations. So, this kind of text might not be very much helpful in doing NLP-based task.So, it will be better if we convert those short forms or informal words or text to standard English so that it helps most of NLP tasks in various areas like sentimental analysis, chat box. Etc. Therefore, we need to build a model to convert this corrupted text(informal) to standard English, here another important thing is it need to be converted by preserving the semantic meaning of textAs we are giving text as input, and we are also getting text as output i.e. we are converting corrupted text to a standard English one. It’s kind of a machine translation problem i.e. translating the SMS text to standard English one. So, we are going for a deep learning approach.As it is kind of translation like we need to take entire sentence at a time and we need to translate to standard English sentence. So, we consider this problem as of sequence to sequence modelThe performance of most of the NLP based models decrease due to corrupted text or informal text ,for that we need proper english words .Here, input data will have random corruption which is a superset of target data.We will be converting them into target data while preserving the semantic meaning of text.This dataset contains social media text along with their normalized text and Chinese translation of the normalized text. For our problem we need only social media text and their normalized English text. Social media text contains 2000 dataset.Since our data is in txt format which contains SMS text in one-line, Standard English in second line and Chinese Translation of standard English in 3rd line. We would be using only SMS Text and Standard English for our problem Statement. After splitting, converted the txt format into csv files having 2 columns SMS_TEXT and ENGLISH_TEXT.Dataset link: https://www.comp.nus.edu.sg/~nlp/corpora.htmlHere in this problem we have to convert text to text i.e informal to normal english sentences .So, it is clear that it is a machine tranlation probem(text to text).So, we will use seq2seq traditional and some advance models to solve this problem.As mentioned in the research paper, we will be using categorical cross entropy.As we have sequence of input and outputs, so for a given a word we try to output the probability over all output words to which it is denoting. So, we can treat it as multi class. Therefore, we got SoftMax because for each input word we need to get the probability of all output words in vocabulary and also addition to this we also had log which helps to penalize the small errors, so while training the model by using this loss we try to minimize the error in translating the corrupt word to standard English word and finally it helps us to give good results for test data i.e. unseen corrupted text.Credit : https://nlpaug.readthedocs.io/en/latest/augmenter/augmenter.htmlWe will perform 3 kind of augmentation technique to increase the dataset size:After augmentation we have made dataset of size 12k from 2k.In this EDA I will be doing following things:a.Character level analysis :  * Number of characters present in each sentence. This gives us a rough idea about the sentence length. * We will also calculate frequency of texts(datapoints) having same sentence length in our data.b. Word level analysis:2. N-gram exploration:* Generally n-grams help us to know what words are expected before and after a particular word. * It also helps in knowing context of the word. We will be focusing on bi-grams and tri-grams.3. Word Cloud:1.1 Number of characters present in each sentence of informal text.Inference1.The histogram shows that informal text ranges from 0 to 200 characters and generally, it is between 0 to 160 characters.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of character lies between 0 to 160 with some outliers lying in beyond 200 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.8 percetile is around 161 and 99.9 is 202 so this means there are very few words which are greater than 161.So, we will fix the length to 160.5.so we can fix 161 as padd sequence length1..2 Number of characters present in each sentence of normal text:Inference1.The histogram shows that normal text ranges from 0 to 250 characters and generally, it is between 0 to 190 characters.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of character lies between 0 to 190 with some outliers lying in beyond 190 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.7 percetile is around 200 and 99.8 is 215 so this means there are very few words which are greater than 200.So, we will fix the length to 2005.So we can fix 200 as padd sequence length.1.3 Frequency of texts having same sentence length in dataInference1.We can see that maximum percentage of text is around 20–30 range of length as people tends to write short in social media.2.After the informal text in unwrapped into proper sentences we find that the number of sentence length range changes from 20–30 to nearly 20–60 to be most frequent.2.1 Number of words present in informal sentencesInference1.The histogram shows that number of words in informal text ranges from 1 to 40 words.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of words lies between 1 to 35 with some outliers lying in beyond 35 mark.3.We will zoom more to see the exact numbers in clear manner by using percentiles4.We can see that 99.9 percetile is around 39 and 100 percentile is 49 so this means there are very few words which are greater than 39.So, we will fix the length to 395.Above 39 all can be considerd as outliers.2.2 Number of words present in normal sentencesInference1.The histogram shows that number of words in normal text ranges from 1 to 40 words.2.While looking at the boxplot we can say that informal text data is positively-skewed or right-skewed as the modian is closer on the lower end of box. The range of number of words lies between 1 to 35 with some outliers lying in beyond 35 mark.4.We can see that 99.9 percetile is around 48 and 100 percentile is 59 so this means there are very few words which are greater than 48.So, we will fix the length to 485.Above 48 all can be considerd as outliers.2.3 Average Word length and common words in data2.3 Frequency of stop words in dataBy looking at the graphs above we came to know that words “to” and “I” were most frequently used in informal data. While words “you” and “to” have been most frequently used in normal data.1.1 Using Bi-gramsInference1.Looking at the bi-grams of the informal text it seems that the data is personal-type of data. Also, many of the sentences indicate some action such as “to go”, “call me”.2.Looking at the bi-grams of the normal text it seems that the data is personal-type of data. Also, many of the sentences indicate some action such as “are you”, “want to”.Looking at the tri-grams it is safer to say that the data is personal chats kind of data and we are more sure about the data type ,its background.I have taken basic encoder decoder without attention mechanismas with char-char encoding a basline model and got minimum of 0.285 as val lossModel architectureLets see some output of baseline model:Inference :1.Predicted sentences are very bad in baseline model as it is capturing context vector of only the last encoder time step .2.So, we will now move to advanced methods like attention mechanism for better resultsWe will use 3 type of scoring function for itLoss plot of train vs validation:Now we can see that our validation loss is reaching upto 0.15 which was earlier 0.28.So, it is clear that we have increased our accuracy of prediction .Let’s see some output to get it more clear:I have used bleu score as a metric to see how good or bad my translations are and i got these results:We will finalise concat scoring function as our final scoring function for the attention mechanism and make one final function in which we will send one raw data and it will give prediction of the input sentence.1.Let’s plot the distribution of BLEU score to see its pattern.InferenceWe can see that bleu score is kind of following normal distribution plot and important insights that we can observe are:2.Finding relation between sentence length and bleu score of predicted sentenceLet’s see the average length of sentences for all three cases:Predicted sentence: Sharis, Gen asks if we want to meet up today. Are you free? Please reply as soon as possible. original Sentence : Sharis, Gen asks if we want to meet up today. Are you free? Please reply as soon as possible.Predicted sentence: Today then. Where will you be? original Sentence : 2:30 then. Where will you be?>Predicted sentence: Not arrang Millian, you don’t you?original Sentence : TIP,WHAT arR yoU DOING ?>I have deployed this DL model using flask API on localhost. I am taking informal text as input from user and and using the model. pkl file to predict normal text corresponding to the informal text given by user.https://cs224d.stanford.edu/reports/Lewis.pdfwww.appliedaicourse.comarxiv.orgblog.keras.ionlpaug.readthedocs.iostackoverflow.comI would like to thank team Applied AI for their tremendous support and guideline because of which I reached up to this position to do a complete end to end case study on such a interesting problem statement.You can find complete code on my Github hereHappy to connect with you on Linkedin.www.linkedin.com",11/09/2021,0,36.0,11.0,677.0,280.0,29.0,13.0,0.0,10.0,en
4055,Introduction,Project AGI,ProjectAGI,10.0,5.0,1133.0,"by David Rawlinson and Gideon KowadloThis blog will be written by several people. Other contributors are welcome — send us an email to introduce yourself!The content will be a series of short articles about a set of common architectures for artificial general intelligence (AGI). Specifically, we will look at the commonalities in Deep Belief Networks and Numenta’s Memory Prediction Framework (MPF). MPF is these days better known by its concrete implementations CLA (Cortical Learning Algorithm) and HTM (Hierarchical Temporal Memory). For an introduction to Deep Belief Networks, read one of the papers by Hinton et al.This blog will typically use the term MPF to collectively describe all the current implementations — CLA, HTM, NUPIC etc. We see MPF as an interface or specification, and CLA, HTM as implementations of the MPF.Both MPFs and DBNs try to build efficient and useful hierarchical representations from patterns in input data. Both use unsupervised learning to define local variables to represent the state-space at a particular position in the hierarchy; modelling of the state in terms of these local variables — be they “sequence cells” or “hidden units” — constitutes a nonlinear transformation of the input. This means that both are “Deep Learning” methods. The notion of local variables within a larger graph relates this work to general Bayesian Networks and other graphical models.We are also very interested in combining these structures with the representation and selection of behaviour, eventually resulting in the construction of an agent. This is a very exciting area of research that has not received significant attention.A very incomplete phylogeny of Deep Learning methods, specifically to contrast well known implementations of Numenta’s Memory Prediction Framework and Deep Belief Networks. Some assumptions (guesses?) about corporate technologies have been made (Vicarious, Grok, DeepMind).Readers would be forgiven for not having noted any similarity between MPFs and DBNs. The literature rarely describes both in the same terms. In an attempt to clarify our perspective, we’ve included a phylogeny showing the relationships between these methods — of course, this is only one perspective. We’ve also noted some significant organisations using each method.MPF/CLA/HTM aims to explain the function of the human neocortex. Deep Learning methods such as Convolutional Deep Neural Networks are explicitly inspired by cortical processing, particularly in the vision area. “Deep” means simply that the network has many layers; in earlier artificial neural networks, it was difficult to propagate signals through many layers, so only “shallow” networks were effective. “Deep” methods do some special (nonlinear) processing in each layer to ensure the propagated signal is meaningful, even after many layers of processing.A cross-section of part of a cerebrum showing the cortex (darker outline). The distinctively furrowed brain appearance is an attempt to maximize surface area within a constrained volume. Image from Wikipedia.Cortex means surface, and this surface is responsible for a lot of processing. The cortex covers the top half of the brain, the cerebrum. The processing happens in a thin layer on the surface, with the “filling” of the cerebrum being mainly connections between different areas of the cortex/surface.Remarkably, it has been known for at least a century that the neocortex is remarkably similar in structure throughout, despite being associated with ostensibly very different brain functions such as speech, vision, planning and language. Early analysis of neuron connection patterns within the cortex revealed that it is organised into parallel stacks of tiny columns. The columns are highly connected internally, with limited connections to nearby columns. In other words, each column can be imagined as an independent processor of data.Let’s assume you’re a connectionist: This means you believe the function of a neural network is determined by the degree and topology of the connections it has. This suggests that the same algorithm is being used in each cortical column: the same functionality is being repeated throughout the cortex despite being applied to very different data. This theory is supported by evidence of neural plasticity: Cortex areas can change function if different data is provided to them, and can learn to interpret new inputs.So, to explain the brain all we need to figure out is what’s happening in a typical cortical column and how the columns are connected!!*(*a gross simplification, so prepare to be disappointed…!)Whether the function of a cortical column is described as a “neural network” or as a graphical model is irrelevant so long as the critical functionality is captured. Both MPF and Deep Belief Networks create tree-like structures of functionally-identical vertices that we can call a hierarchy. The processing vertices are analogous to columns; the white matter filling the cerebrum passes messages between the vertices of the tree. The tree might really be a different type of graph; we don’t know whether it is better to have more vertices in lower or higher levels.Deep Belief Networks have been particularly successful in the analysis of static images. MPF/CLA/HTM is explicitly designed to handle time-varying data. But neither is expressly designed to generate behaviour for an artificial agent.Recently, a company called DeepMind combined Deep Learning and Reinforcement Learning to enable a computer program to play Atari games. Reinforcement Learning teaches an algorithm to associate world & self states with consequences by providing only a nonspecific “quality” function. The algorithm is then able to pick actions that maximize the quality expected in future states.Reinforcement Learning is the right type of feedback because it avoids the need to provide a “correct” response in every circumstance. For a “general” AI this is important, because it would require a working General Intelligence to define the “correct” response in all circumstances!The direction taken by DeepMind is exactly what we want to do: Automatic construction of a meaningful hierarchical representation of the world and the agent, in combination with reinforcement learning to allow prediction of state quality. Technically, the problem of picking a suitable action for a specific state is called a Markov Decision Process (MDP). But often, the true state of the world is not directly measurable; instead, we can only measure some “evidence” of world-state, and must infer the true state. This harder task is called a Partially-Observable MDP (POMDP).In summary this blog is concerned with algorithms and architectures for artificial general intelligence, which we will approach by tackling POMDPs using unsupervised hierarchical representations of the state space and reinforcement learning for action selection. Using Hawkins et al’s MPF concept for the representation of state-space as a hierarchical sequence-memory, and adding adaptive behaviour selection via reinforcement learning, we arrive at the adaptive memory prediction framework (AMPF).This continues a theme we developed in an earlier paper (“Generating adaptive behaviour within a memory-prediction framework”).Since that publication we have been developing more scalable methods and aim to release a new software package in 2014. In the meantime we will use this blog to provide context and discussion of new ideas.Originally published at Project AGI.",07/04/2014,0,0.0,3.0,959.0,542.0,2.0,0.0,0.0,16.0,en
4056,Implementing Grad-CAM in PyTorch,Medium,Stepan Ulyanin,175.0,11.0,2022.0,"Recently I have come across a chapter in François Chollet’s “Deep Learning With Python” book, describing the implementation of Class Activation Mapping for the VGG16 network. He implemented the algorithm using Keras as he is the creator of the library. Hence, my instinct was to re-implement the CAM algorithm using PyTorch.Grad-CAMThe algorithm itself comes from this paper. It was a great addition to the computer vision analysis tools for a single primary reason. It provides us with a way to look into what particular parts of the image influenced the whole model’s decision for a specifically assigned label. It is particularly useful in analyzing wrongly classified samples. The Grad-CAM algorithm is very intuitive and reasonably simple to implement.The intuition behind the algorithm is based upon the fact that the model must have seen some pixels (or regions of the image) and decided on what object is present in the image. Influence in the mathematical terms can be described with a gradient. On the high-level, that is what the algorithm does. It starts with finding the gradient of the most dominant logit with respect to the latest activation map in the model. We can interpret this as some encoded features that ended up activated in the final activation map persuaded the model as a whole to choose that particular logit (subsequently the corresponding class). The gradients are then pooled channel-wise, and the activation channels are weighted with the corresponding gradients, yielding the collection of weighted activation channels. By inspecting these channels, we can tell which ones played the most significant role in the decision of the class.In this post I am going to re-implement the Grad-CAM algorithm, using PyTorch and, to make it a little more fun, I am going to use it with different architectures.VGG19In this part I will try to reproduce the Chollet’s results, using a very similar model — VGG19 (note that in the book he used VGG16). The main idea in my implementation is to dissect the network so we could obtain the activations of the last convolutional layer. Keras has a very straight forward way of doing this via Keras functions. However, in PyTorch I had to jump through some minor hoops.The strategy is defined as follows:I set aside a few images (including the images of the elephants Chollet used in his book) from the ImageNet dataset to investigate the algorithm. I also applied the Grad-CAM to some photographs from my Facebook to see how the algorithm works in the “field” conditions. Here are the original images we are going to be working with:Ok, let’s load up the VGG19 model from the torchvision module and prepare the transforms and the dataloader:Here I import all the standard stuff we use to work with neural networks in PyTorch. I use the basic transform needed to use any model that was trained on the ImageNet dataset, including the image normalization. I am going to feed one image at a time, hence I define my dataset to be the image of the elephants, in attempt to obtain similar results as in the book.Here comes the tricky part (trickiest in the whole endeavor, but not too tricky). We can compute the gradients in PyTorch, using the .backward() method called on a torch.Tensor . This is exactly what I am going to do: I am going to call backward() on the most probable logit, which I obtain by performing the forward pass of the image through the network. However, PyTorch only caches the gradients of the leaf nodes in the computational graph, such as weights, biases and other parameters. The gradients of the output with respect to the activations are merely intermediate values and are discarded as soon as the gradient propagates through them on the way back. So what are our options?Hook ‘EmIf you graduated from the University of Texas at Austin as I did you will like this part. There is a callback instrument in PyTorch: hooks. Hooks can be used in different scenarios, ours is one of them. This part of the PyTorch documentation tells us exactly how to attach a hook to our intermediate values to pull the gradients out of the model before they are discarded. The documentation tells us:The hook will be called every time a gradient with respect to the Tensor is computed.Now we know that we have to register the backward hook to the activation map of the last convolutional layer in our VGG19 model. Let’s find where to hook.We can easily observe the VGG19 architecture by calling the vgg19(pretrained=True) :Pretrained models in PyTorch heavily utilize the Sequential() modules which in most cases makes them hard to dissect, we will see the example of it later.In the image we see the whole VGG19 architecture. I highlighted the last convolutional layer in the feature block (including the activation function). Well, now we know that we want to register the backward hook at the 35th layer of the feature block of our network. This is exactly what I am going to do. Also, it is worth mentioning that it is necessary to register the hook inside the forward() method, to avoid the issue of registering hook to a duplicate tensor and subsequently losing the gradient.As you can see there is a remaining max pooling layer left in the feature block, not to worry, I will add this layer in the forward() method.This looks great so far, we can finally get our gradients and the activations out of the model.Drawing CAMFirst, let’s make the forward pass through the network with the image of the elephants and see what the VGG19 predicts. Don’t forget to set your model into the evaluation mode, otherwise you can get very random results:As expected, we get same results as Chollet gets in his book:Now, we are going to do the back-propagation with the logit of the 386th class which represents the ‘African_elephant’ in the ImageNet dataset.Finally, we obtain the heat-map for the elephant image. It is a 14x14 single channel image. The size is dictated by the spacial dimensions of the activation maps in the last convolutional layer of the network.Now, we can use OpenCV to interpolate the heat-map and project it onto the original image, here I used the code from the Chollet’s book:In the image bellow we can see the areas of the image that our VGG19 network took most seriously in deciding which class (‘African_elephant’) to assign to the image. We can assume that the network took the shape of the head and ears of the elephants as a strong sign of the presence of an elephant in the image. What is more interesting, the network also made a distinction between the African elephant and a Tusker Elephant and an Indian Elephant. I am not an elephant expert, but I suppose the shape of ears and tusks is pretty good distinction criterion. In general, this is exactly how a human would approach such a task. An expert would examine the ears and tusk shapes, maybe some other subtle features that could shed light on what kind of elephant it is.Ok, let’s repeat the same procedure with some other images.The sharks are mostly identified by the mouth/teeth area in the top image and body shape and surrounding water in the bottom image. Pretty cool!Going Beyond VGGVGG is a great architecture, however, researchers since came up with newer and more efficient architectures for image classification. In this part we are going to investigate one of such architectures: DenseNet.There are some issues I came across while trying to implement the Grad-CAM for the densely connected network. First, as I have already mentioned, the pretrained models from the PyTorch model zoo are mostly built with nested blocks. It is a great choice for readability and efficiency; however it raises an issue with the dissection of such nested networks. Notice that VGG is formed with 2 blocks: feature block and the fully connected classifier. DenseNet is made of multiple nested blocks and trying to get to the activation maps of the last convolutional layer is impractical. There are 2 ways we can go around this issue: we can take the last activation map with the corresponding batch normalization layer. This yields pretty good results as we will see shortly. The second thing we could do is to build the DenseNet from scratch and repopulate the weights of the blocks/layers, so we could access the layers directly. The second approach seems too complicated and time consuming, so I avoided it.The code for the DenseNet CAM is almost identical to the one I used for the VGG network, the only difference is in the index of the layer (block in the case of the DenseNet) we are going to get our activations from:It is important to follow the architecture design of the DenseNet, hence I added the global average pooling to the network before the classifier (you can always find these guides in the original papers).I am going to pass both iguana images through our densely connected network in order to find the class that was assigned to the images:Here, the network predicted that this is the image of an ‘American Alligator’. Hmm, let’s run our Grad-CAM algorithm against the ‘American Alligator’ class. In the images below I show the heat-map and the projection of the heat-map onto the image. We can see that the network mostly looked at the “creature”. It is evident that alligators may look like iguanas since they both share body shape and overall structure.However, notice that there is another part of the image that has influenced the class scores. The photographer in a picture may throw the network off with his position and pose. The model took both the iguana and the human in consideration while making the choice. Let’s see what will happen if we crop the photographer out of the image. Here are the top-3 class predictions for the cropped image:We now see that cropping the human from the image actually helped to obtain the right class label for the image. This is one of the best applications of the Grad-CAM: being able to obtain information of what possibly could go wrong in misclassified images. Once we figure out what could have happened we can efficiently debug the model (in this case cropping the human helped).The second iguana was classified correctly and here is the corresponding heat-map and projection.Going Beyond ImageNetLet’s try some of the images I have downloaded from my Facebook page. I am going to use our DenseNet201 for this purpose.The image of me holding my cat is classified as follows:Let’s look at the class activation map for this image.In the images below we can see that the model is looking in the right place.Let’s see if cutting myself out will help with the classification.Cropping myself out helped dramatically:Now Luna is predicted at least as a cat, which is much closer to the real label (which I don’t know because I don’t know what kind of cat she is).The last image we are going to look at is the image of me, my wife and my friend taking a bullet train from Moscow to Saint-Petersburg.The image is classified correctly:We are indeed in front of a bullet train. Let’s look at the class activation map just for fun then.It is important to note that the DenseNet’s last convolutional layer yields 7x7 spacial activation maps (in contrast to 14x14 in the VGG network), hence the resolution of the heat-map may be a little exaggerated when projected back into the image space (corresponds to the red attention color on our faces).Another potential question that can arise is why wouldn’t we just compute the gradient of the class logit with respect to the input image. Remember that a convolutional neural network works as a feature extractor and deeper layers of the network operate in increasingly abstract spaces. We want to see which of the features actually influenced the model’s choice of the class rather than just individual image pixels. That is why it is crucial to take the activation maps of deeper convolutional layers.I hope you enjoyed this article, thank you for reading.",22/02/2019,6,9.0,6.0,521.0,440.0,28.0,1.0,0.0,4.0,en
4057,Creating Bitcoin trading bots don’t lose money,Towards Data Science,Adam King,2500.0,14.0,2489.0,"In this article we are going to create deep reinforcement learning agents that learn to make money trading Bitcoin. In this tutorial we will be using OpenAI’s gym and the PPO agent from the stable-baselines library, a fork of OpenAI’s baselines library.The purpose of this series of articles is to experiment with state-of-the-art deep reinforcement learning technologies to see if we can create profitable Bitcoin trading bots. It seems to be the status quo to quickly shut down any attempts to create reinforcement learning algorithms, as it is “the wrong way to go about building a trading algorithm”. However, recent advances in the field have shown that RL agents are often capable of learning much more than supervised learning agents within the same problem domain. For this reason, I am writing these articles to see just how profitable we can make these trading agents, or if the status quo exists for a reason.Many thanks to OpenAI and DeepMind for the open source software they have been providing to deep learning researchers for the past couple of years. If you haven’t yet seen the amazing feats they’ve accomplished with technologies like AlphaGo, OpenAI Five, and AlphaStar, you may have been living under a rock for the last year, but you should also go check them out.While we won’t be creating anything quite as impressive, it is still no easy feat to trade Bitcoin profitably on a day-to-day basis. However, as Teddy Roosevelt once said,Nothing worth having comes easy.So instead of learning to trade ourselves… let’s make a robot to do it for us.towardsdatascience.comWhen you’ve read this article, check out TensorTrade — the successor framework to the codebase produced in this article.If you are not already familiar with how to create a gym environment from scratch, or how to render simple visualizations of those environments, I have just written articles on both of those topics. Feel free to pause here and read either of those before continuing.For this tutorial, we are going to be using the Kaggle data set produced by Zielak. The .csv data file will also be available on my GitHub repo if you’d like to download the code to follow along. Okay, let’s get started.First, let’s import all of the necessary libraries. Make sure to pip install any libraries you are missing.Next, let’s create our class for the environment. We’ll require a pandas data frame to be passed in, as well as an optional initial_balance, and a lookback_window_size, which will indicate how many time steps in the past the agent will observe at each step. We will default the commission per trade to 0.075%, which is Bitmex’s current rate, and default the serial parameter to false, meaning our data frame will be traversed in random slices by default.We also call dropna() and reset_index() on the data frame to first remove any rows with NaN values, and then reset the frame’s index since we’ve removed data.Our action_space here is represented as a discrete set of 3 options (buy, sell, or hold) and another discrete set of 10 amounts (1/10, 2/10, 3/10, etc). When the buy action is selected, we will buy amount * self.balance worth of BTC. For the sell action, we will sell amount * self.btc_held worth of BTC. Of course, the hold action will ignore the amount and do nothing.Our observation_space is defined as a continuous set of floats between 0 and 1, with the shape (10, lookback_window_size + 1). The + 1 is to account for the current time step. For each time step in the window, we will observe the OHCLV values, our net worth, the amount of BTC bought or sold, and the total amount in USD we’ve spent on or received from those BTC.Next, we need to write our reset method to initialize the environment.Here we use both self._reset_session and self._next_observation, which we haven’t defined yet. Let’s define them.An important piece of our environment is the concept of a trading session. If we were to deploy this agent into the wild, we would likely never run it for more than a couple months at a time. For this reason, we are going to limit the amount of continuous frames in self.df that our agent will see in a row.In our _reset_session method, we are going to first reset the current_step to 0. Next, we’ll set steps_left to a random number between 1 and MAX_TRADING_SESSION, which we will now define at the top of the file.Next, if we are traversing the frame serially, we will setup the entire frame to be traversed, otherwise we’ll set the frame_start to a random spot within self.df, and create a new data frame called active_df, which is just a slice of self.df from frame_start to frame_start + steps_left.One important side effect of traversing the data frame in random slices is our agent will have much more unique data to work with when trained for long periods of time. For example, if we only ever traversed the data frame in a serial fashion (i.e. in order from 0 to len(df)), then we would only ever have as many unique data points as are in our data frame. Our observation space could only even take on a discrete number of states at each time step.However, by randomly traversing slices of the data frame, we essentially manufacture more unique data points by creating more interesting combinations of account balance, trades taken, and previously seen price action for each time step in our initial data set. Let me explain with an example.At time step 10 after resetting a serial environment, our agent will always be at the same time within the data frame, and would have had 3 choices to make at each time step: buy, sell, or hold. And for each of these 3 choices, another choice would then be required: 10%, 20%, …, or 100% of the amount possible. This means our agent could experience any of (1⁰³)¹⁰ total states, for a total of 1⁰³⁰ possible unique experiences.Now consider our randomly sliced environment. At time step 10, our agent could be at any of len(df) time steps within the data frame. Given the same choices to make at each time step, this means this agent could experience any of len(df)³⁰ possible unique states within the same 10 time steps.While this may add quite a bit of noise to large data sets, I believe it should allow the agent to learn more from our limited amount of data. We will still traverse our test data set in serial fashion, to get a more accurate understanding of the algorithm’s usefulness on fresh, seemingly “live” data.It can often be helpful to visual an environment’s observation space, in order to get an idea of the types of features your agent will be working with. For example, here is a visualization of our observation space rendered using OpenCV.Each row in the image represents a row in our observation_space. The first 4 rows of frequency-like red lines represent the OHCL data, and the spurious orange and yellow dots directly below represent the volume. The fluctuating blue bar below that is the agent’s net worth, and the lighter blips below that represent the agent’s trades.If you squint, you can just make out a candlestick graph, with volume bars below it and a strange morse-code like interface below that shows trade history. It looks like our agent should be able to learn sufficiently from the data in our observation_space, so let’s move on. Here we’ll define our _next_observation method, where we’ll scale the observed data from 0 to 1.It’s important to only scale the data the agent has observed so far to prevent look-ahead biases.Now that we’ve set up our observation space, it’s time to write our step function, and in turn, take the agent’s prescribed action. Whenever self.steps_left == 0 for our current trading session, we will sell any BTC we are holding and call _reset_session(). Otherwise, we set the reward to our current net worth and only set done to True if we’ve run out of money.Taking an action is as simple as getting the current_price, determining the specified action, and either buying or selling the specified amount of BTC. Let’s quickly write _take_action so we can test our environment.Finally, in the same method, we will append the trade to self.trades and update our net worth and account history.Our agents can now initiate a new environment, step through that environment, and take actions that affect the environment. It’s time to watch them trade.Our render method could be something as simple as calling print(self.net_worth), but that’s no fun. Instead we are going to plot a simple candlestick chart of the pricing data with volume bars and a separate plot for our net worth.We are going to take the code in StockTradingGraph.py from the last article I wrote, and re-purposing it to render our Bitcoin environment. You can grab the code from my GitHub.The first change we are going to make is to update self.df['Date'] everywhere to self.df['Timestamp'], and remove all calls to date2num as our dates already come in unix timestamp format. Next, in our render method we are going to update our date labels to print human-readable dates, instead of numbers.First, import the datetime library, then we’ll use the utcfromtimestamp method to get a UTC string from each timestamp and strftime to format the string in Y-m-d H:M format.Finally, we change self.df['Volume'] to self.df['Volume_(BTC)'] to match our data set, and we’re good to go. Back in our BitcoinTradingEnv, we can now write our render method to display the graph.And voila! We can now watch our agents trade Bitcoin.The green ghosted tags represent buys of BTC and the red ghosted tags represent sells. The white tag on the top right is the agent’s current net worth and the bottom right tag is the current price of Bitcoin. Simple, yet elegant. Now, it’s time to train our agent and see how much money we can make!One of the criticisms I received on my first article was the lack of cross-validation, or splitting the data into a training set and test set. The purpose of doing this is to test the accuracy of your final model on fresh data it has never seen before. While this was not a concern of that article, it definitely is here. Since we are using time series data, we don’t have many options when it comes to cross-validation.For example, one common form of cross validation is called k-fold validation, in which you split the data into k equal groups and one by one single out a group as the test group and use the rest of the data as the training group. However time series data is highly time dependent, meaning later data is highly dependent on previous data. So k-fold won’t work, because our agent will learn from future data before having to trade it, an unfair advantage.This same flaw applies to most other cross-validation strategies when applied to time series data. So we are left with simply taking a slice of the full data frame to use as the training set from the beginning of the frame up to some arbitrary index, and using the rest of the data as the test set.Next, since our environment is only set up to handle a single data frame, we will create two environments, one for the training data and one for the test data.Now, training our model is as simple as creating an agent with our environment and calling model.learn.Here, we are using tensorboard so we can easily visualize our tensorflow graph and view some quantitative metrics about our agents. For example, here is a graph of the discounted rewards of many agents over 200,000 time steps:Wow, it looks like our agents are extremely profitable! Our best agent was even capable of 1000x’ing his balance over the course of 200,000 steps, and the rest averaged at least a 30x increase!It was at this point that I realized there was a bug in the environment… Here is the new rewards graph, after fixing that bug:As you can see, a couple of our agents did well, and the rest traded themselves into bankruptcy. However, the agents that did well were able to 10x and even 60x their initial balance, at best. I must admit, all of the profitable agents were trained and tested in an environment without commissions, so it is still entirely unrealistic for our agent’s to make any real money. But we’re getting somewhere!Let’s test our agents on the test environment (with fresh data they’ve never seen before), to see how well they’ve learned to trade Bitcoin.Clearly, we’ve still got quite a bit of work to do. By simply switching our model to use stable-baseline’s A2C, instead of the current PPO2 agent, we can greatly improve our performance on this data set. Finally, we can update our reward function slightly, as per Sean O’Gorman’s advice, so that we reward increases in net worth, not just achieving a high net worth and staying there.These two changes alone greatly improve the performance on the test data set, and as you can see below, we are finally able to achieve profitability on fresh data that wasn’t in the training set.However, we can do much better. In order for us to improve these results, we are going to need to optimize our hyper-parameters and train our agents for much longer. Time to break out the GPU and get to work!However, this article is already a bit long and we’ve still got quite a bit of detail to go over, so we are going to take a break here. In my next article, we will use Bayesian optimization to zone in on the best hyper-parameters for our problem space, and improve the agent’s model to achieve highly profitable trading strategies.In this article, we set out to create a profitable Bitcoin trading agent from scratch, using deep reinforcement learning. We were able to accomplish the following:While our trading agent isn’t quite as profitable as we’d hoped, it is definitely getting somewhere. Next time, we will improve on these algorithms through advanced feature engineering and Bayesian optimization to make sure our agents can consistently beat the market. Stay tuned for my next article, and long live Bitcoin!towardsdatascience.comtowardsdatascience.comIt is important to understand that all of the research documented in this article is for educational purposes, and should not be taken as trading advice. You should not trade based on any algorithms or strategies defined in this article, as you are likely to lose your investment.Thanks for reading! As always, all of the code for this tutorial can be found on my GitHub. Leave a comment below if you have any questions or feedback, I’d love to hear from you! I can also be reached on Twitter at @notadamking.You can also sponsor me on Github Sponsors or Patreon via the links below.github.comGithub Sponsors is currently matching all donations 1:1 up to $5,000!patreon.com",27/04/2019,16,37.0,60.0,1094.0,609.0,9.0,2.0,0.0,23.0,en
4058,"Hi Tal,. Maybe I’m missing something here, but... ",Medium,Sachin Abeywardana,910.0,1.0,119.0,"Hi Tal,Maybe I’m missing something here, but 1. I don’t think your first layer is a embedding layer but a dense layer. You are converting 4000 NUMBERS into 300 by multiplying by a matrix. Embedding layer is when you have categorical variables mapped to vectors, which doesn’t seem to be what’s happening. Unless you are using the name of the stock.2. You mentioned that your output is 5 minute data, but your input is daily data. This is a bit confusing since the time intervals of the input and output have to be the same? If you are getting 5 minute stock data, where do you download them from (I haven’t been able to find anything of the sort).Cheers",22/12/2016,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,en
4059,Using Word Embeddings to Identify Company Names and Stock Tickers,Towards Data Science,Brian Ward,80.0,8.0,1400.0,"Project Goal: Using word embeddings identify company names and stock tickers from natural text.Assumption: Stock tickers and company names are used in similar context in natural text such as a Reddit post or a tweet.Under this assumption, word embeddings should be a good fit for identifying these target words as word embeddings are trained by the context in which words are found.In this post, I will skip describing what word embeddings are and how the Word2Vec algorithm works. I have written a much more detailed paper on the same project which can be found here. In this paper, I explain the details of what word embeddings are as well as how the Word2Vec Algorithm works. I also detail sentiment analysis via Naive Bayes. In this post, I will just present the code in an abridged format.The full repo can be found here.Two different data sources were used. Both are collections of Reddit posts from the subreddit r/wallstreetbets, that I found on Kaggle.In this first step, we are going to import this data and extract every sentence. In one of the datasets we are pulling the Reddit titles and the other we are pulling the body of the text and splitting on ‘.’ to separate sentences. Note: Gensim’s Word2Vec models are trained on sentences.The next step is to pre-process our text before we can use it to train the model. We will do the following :Now lets take a look at one of our processed sentences :Excellent, we now have roughly 1.2 M sentences to train our model.Now we are ready to train or Word2Vec model. Text 8 is a data dump from Wikipedia which is great for normalizing the word vectors. I would encourage you to compare a model with and without this extra training data to see how much it helps. More information on text8 can be found here.Now we can use Gensim’s most_similar() function to look at the most similar words in the vocabulary to a target word such as ‘gme’.Perfect, so we can see that at least the top 10 most similar words are also companies or stock tickers. This gives us hope that our assumption was correct.We need a representative vector that we can use to compare to other words in the vocabulary. We can use this vector and some similarity threshold to identify target words in the posts.Unfortunately, Gensim doesn’t have a method for creating an averaged vector from other vectors. So I pulled some of the code out of Gensim’s most_similar() function to get the functionality I was looking for. I also created a cosine similarity method which we will use to compare any two vectors. Cosine similarity is a way to compare two non-zero vectors (two identical vectors will have a cosine similarity = 1.0).Awesome, now we can use it to create our own vector from some hand-picked target words.Now let’s go ahead and use the vector that we created and take a look at the most similar vectors in our models’ vocabulary. Note: the numbers are the cosine similarity of the term and the representative vector.I’m pretty happy with the results here. If you follow along you should explore all of these top 500, almost all of them appear to be stock tickers or company names. We can also take a look at the distribution of the similarity of our vector to each word in the vocabulary.I added a vertical line at 0.55 which is the similarity threshold that I ended up choosing. Any word to the right of that line will be identified as a target. Based on this figure it looks like the threshold should probably be greater than 0.55. This choice was a result of a parameter analysis, and would hopefully increase with a more robust ground truth. More on this below.Now we need some way to test our model. I randomly selected 1000 Reddit posts then hand-extracted any company names or stock tickers. Here’s what it looks like:Now let’s go ahead and test it. There are a few different ways to score this, I chose a simple method of averaging a missed score and an over score.Great, let’s put it to work.Gensim's Word2Vec model has a few parameters that I was interested in testing systematically. I chose to focus on 4 different parameters:Note: more on these parameters below.We can iterate through all of these parameters and test against our ground truth.This obviously takes a long time as we end up having to train the model 60 times. I just let it run overnight. Let’s take a look at the results.Now we can consider the parameters individually to see if we find any trends.The similarity threshold is the threshold we use to determine whether or not a word is extracted:After some trial and error, I chose to consider 4 different thresholds for these tests. I also chose to include the graphs for each of these four thresholds as I compare the other parameters as the parameters might have different effects at different thresholds.Although N-Gram is not a parameter of the word2vec model it is an important step in the pre-processing of the data fed into the model and can have a great effect on the task we are trying to achieve.We see that the trends are not the same for each threshold. I chose to continue with bigrams as it is both the middle ground and It allows us to keep bigram company names together (e.g. ‘home_depot’).Window size is a parameter of the Word2vec model and specifies the number of words to the left and right of the target word that we will consider a context word (more detail can be found in project description). This can have a large effect on the embeddings especially with less structured text like Reddit posts.Vector size is another parameter of the word2vec model. Vector size is the length or dimensionality of the word embeddings that we are creating (more detail can be found in project description). Choosing the correct vector size really needs to be done by testing. We will consider 4 different options.Once determining the best parameters : [n-gram=bigrams, window=1, vector-size=100, threshold=5.5] we were able to achieve an overall accuracy score of 89% on our ground truth. I was pretty happy with the results given this simple parameter analysis. Now we can go ahead and use the model to extract companies.The extraction function is pretty simple. We basically just need to compare each word to our representative vector. The reason why we have it wrapped in a try statement is that the library of our model won’t have every possible term (param in word2vec model: min_count=25).When applied to our original dataset of Reddit posts with a threshold = 0.6 this method was able to extract companies from nearly 300k posts. At this same threshold, 800 unique companies/tickers were extracted. Here’s a subset of some of the extracted companies/tickers and their counts.The first thing I would like to note is that the principal factor in this project, the representative vector, was kind of glossed over. That’s because in the time I had for this project I didn’t think I could implement a systematic way of creating this vector. I think there is A LOT that could be done to make this algorithm much more accurate. The first thing that would need to be done would be to create a much larger ground truth, ideally pulling from Twitter or other forms of social media as well. With a more robust ground truth to test against we could probably figure out some cool ways to learn a better representative vector. A couple of ideas:I also think a lot more could be done with the processing of the strings. In this implementation, I chose to remove numbers and emojis. I think this is throwing out a lot of rich information, especially emojis. Im sure that our target words have a contextual relationship to both numbers and emojis. If I get around to messing with any of these ideas, I will certainly update this write-up.Please let me know If you have any suggestions, questions, or comments, I am a student and am learning all of these tools myself. In the full project, I also use Naive Bayes to determine sentiment for the different companies/tickers. You can read more on that in the full write up here, and can see the full repo here. Thanks for reading.",14/07/2021,1,10.0,0.0,1400.0,711.0,11.0,5.0,0.0,11.0,en
4060,Building a Bayesian deep learning classifier,Towards Data Science,Kyle Dorman,174.0,19.0,3957.0,"In this blog post, I am going to teach you how to train a Bayesian deep learning classifier using Keras and tensorflow. Before diving into the specific training example, I will cover a few important high level concepts:I will then cover two techniques for including uncertainty in a deep learning model and will go over a specific example using Keras to train fully connected layers over a frozen ResNet50 encoder on the cifar10 dataset. With this example, I will also discuss methods of exploring the uncertainty predictions of a Bayesian deep learning classifier and provide suggestions for improving the model in the future.This post is based on material from two blog posts (here and here) and a white paper on Bayesian deep learning from the University of Cambridge machine learning group. If you want to learn more about Bayesian deep learning after reading this post, I encourage you to check out all three of these resources. Thank you to the University of Cambridge machine learning group for your amazing blog posts and papers.Bayesian statistics is a theory in the field of statistics in which the evidence about the true state of the world is expressed in terms of degrees of belief. The combination of Bayesian statistics and deep learning in practice means including uncertainty in your deep learning model predictions. The idea of including uncertainty in neural networks was proposed as early as 1991. Put simply, Bayesian deep learning adds a prior distribution over each weight and bias parameter found in a typical neural network model. In the past, Bayesian deep learning models were not used very often because they require more parameters to optimize, which can make the models difficult to work with. However, more recently, Bayesian deep learning has become more popular and new techniques are being developed to include uncertainty in a model while using the same number of parameters as a traditional model.Uncertainty is the state of having limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome. As it pertains to deep learning and classification, uncertainty also includes ambiguity; uncertainty about human definitions and concepts, not an objective fact of nature.There are several different types of uncertainty and I will only cover two important types in this post.Aleatoric uncertainty measures what you can’t understand from the data. It can be explained away with the ability to observe all explanatory variables with increased precision. Think of aleatoric uncertainty as sensing uncertainty. There are actually two types of aleatoric uncertainty, heteroscedastic and homoscedastic, but I am only covering heteroscedastic uncertainty in this post. Homoscedastic is covered more in depth in this blog post.Concrete examples of aleatoric uncertainty in stereo imagery are occlusions (parts of the scene a camera can’t see), lack of visual features (i.e a blank wall), or over/under exposed areas (glare & shading).Epistemic uncertainty measures what your model doesn’t know due to lack of training data. It can be explained away with infinite training data. Think of epistemic uncertainty as model uncertainty.An easy way to observe epistemic uncertainty in action is to train one model on 25% of your dataset and to train a second model on the entire dataset. The model trained on only 25% of the dataset will have higher average epistemic uncertainty than the model trained on the entire dataset because it has seen fewer examples.A fun example of epistemic uncertainty was uncovered in the now famous Not Hotdog app. From my own experiences with the app, the model performs very well. But upon closer inspection, it seems like the network was never trained on “not hotdog” images that included ketchup on the item in the image. So if the model is shown a picture of your leg with ketchup on it, the model is fooled into thinking it is a hotdog. A Bayesian deep learning model would predict high epistemic uncertainty in these situations.In machine learning, we are trying to create approximate representations of the real world. Popular deep learning models created today produce a point estimate but not an uncertainty value. Understanding if your model is under-confident or falsely over-confident can help you reason about your model and your dataset. The two types of uncertainty explained above are import for different reasons.Note: In a classification problem, the softmax output gives you a probability value for each class, but this is not the same as uncertainty. The softmax probability is the probability that an input is a given class relative to the other classes. Because the probability is relative to the other classes, it does not help explain the model’s overall confidence.Aleatoric uncertainty is important in cases where parts of the observation space have higher noise levels than others. For example, aleatoric uncertainty played a role in the first fatality involving a self driving car. Tesla has said that during this incident, the car’s autopilot failed to recognize the white truck against a bright sky. An image segmentation classifier that is able to predict aleatoric uncertainty would recognize that this particular area of the image was difficult to interpret and predicted a high uncertainty. In the case of the Tesla incident, although the car’s radar could “see” the truck, the radar data was inconsistent with the image classifier data and the car’s path planner ultimately ignored the radar data (radar data is known to be noisy). If the image classifier had included a high uncertainty with its prediction, the path planner would have known to ignore the image classifier prediction and use the radar data instead (this is oversimplified but is effectively what would happen. See Kalman filters below).Epistemic uncertainty is important because it identifies situations the model was never trained to understand because the situations were not in the training data. Machine learning engineers hope our models generalize well to situations that are different from the training data; however, in safety critical applications of deep learning hope is not enough. High epistemic uncertainty is a red flag that a model is much more likely to make inaccurate predictions and when this occurs in safety critical applications, the model should not be trusted.Epistemic uncertainty is also helpful for exploring your dataset. For example, epistemic uncertainty would have been helpful with this particular neural network mishap from the 1980s. In this case, researchers trained a neural network to recognize tanks hidden in trees versus trees without tanks. After training, the network performed incredibly well on the training set and the test set. The only problem was that all of the images of the tanks were taken on cloudy days and all of the images without tanks were taken on a sunny day. The classifier had actually learned to identify sunny versus cloudy days. Whoops.Uncertainty predictions in deep learning models are also important in robotics. I am currently enrolled in the Udacity self driving car nanodegree and have been learning about techniques cars/robots use to recognize and track objects around then. Self driving cars use a powerful technique called Kalman filters to track objects. Kalman filters combine a series of measurement data containing statistical noise and produce estimates that tend to be more accurate than any single measurement. Traditional deep learning models are not able to contribute to Kalman filters because they only predict an outcome and do not include an uncertainty term. In theory, Bayesian deep learning models could contribute to Kalman filter tracking.Aleatoric and epistemic uncertainty are different and, as such, they are calculated differently.Aleatoric uncertainty is a function of the input data. Therefore, a deep learning model can learn to predict aleatoric uncertainty by using a modified loss function. For a classification task, instead of only predicting the softmax values, the Bayesian deep learning model will have two outputs, the softmax values and the input variance. Teaching the model to predict aleatoric variance is an example of unsupervised learning because the model doesn’t have variance labels to learn from. Below is the standard categorical cross entropy loss function and a function to calculate the Bayesian categorical cross entropy loss.The loss function I created is based on the loss function in this paper. In the paper, the loss function creates a normal distribution with a mean of zero and the predicted variance. It distorts the predicted logit values by sampling from the distribution and computes the softmax categorical cross entropy using the distorted predictions. The loss function runs T Monte Carlo samples and then takes the average of the T samples as the loss.In Figure 1, the y axis is the softmax categorical cross entropy. The x axis is the difference between the ‘right’ logit value and the ‘wrong’ logit value. ‘right’ means the correct class for this prediction. ‘wrong’ means the incorrect class for this prediction. I will use the term ‘logit difference’ to mean the x axis of Figure 1. When the ‘logit difference’ is positive in Figure 1, the softmax prediction will be correct. When ‘logit difference’ is negative, the prediction will be incorrect. I will continue to use the terms ‘logit difference’, ‘right’ logit, and ‘wrong’ logit this way as I explain the aleatoric loss function.Figure 1 is helpful for understanding the results of the normal distribution distortion. When the logit values (in a binary classification) are distorted using a normal distribution, the distortion is effectively creating a normal distribution with a mean of the original predicted ‘logit difference’ and the predicted variance as the distribution variance. Applying softmax cross entropy to the distorted logit values is the same as sampling along the line in Figure 1 for a ‘logit difference’ value.Taking the categorical cross entropy of the distorted logits should ideally result in a few interesting properties.I was able to use the loss function suggested in the paper to decrease the loss when the ‘wrong’ logit value is greater than the ‘right’ logit value by increasing the variance, but the decrease in loss due to increasing the variance was extremely small (<0.1). During training, my model had a hard time picking up on this slight local minimum and the aleatoric variance predictions from my model did not make sense. I believe this happens because the slope of Figure 1 on the left half of the graph is ~ -1. Sampling a normal distribution along a line with a slope of -1 will result in another normal distribution and the mean will be about the same as it was before but what we want is for the mean of the T samples to decrease as the variance increases.To make the model easier to train, I wanted to create a more significant loss change as the variance increases. Just like in the paper, my loss function above distorts the logits for T Monte Carlo samples using a normal distribution with a mean of 0 and the predicted variance and then computes the categorical cross entropy for each sample. To get a more significant loss change as the variance increases, the loss function needed to weight the Monte Carlo samples where the loss decreased more than the samples where the loss increased. My solution is to use the elu activation function, which is a non-linear function centered around 0.I applied the elu function to the change in categorical cross entropy, i.e. the original undistorted loss compared to the distorted loss, undistorted_loss - distorted_loss. The elu shifts the mean of the normal distribution away from zero for the left half of Figure 1. The elu is also ~linear for very small values near 0 so the mean for the right half of Figure 1 stays the same.In Figure 2 right < wrong corresponds to a point on the left half of Figure 1 and wrong < right corresponds to a point on the right half of Figure 2. You can see that the distribution of outcomes from the 'wrong' logit case, looks similar to the normal distribution and the 'right' case is mostly small values near zero. After applying -elu to the change in loss, the mean of the right < wrong becomes much larger. In this example, it changes from -0.16 to 0.25. The mean of the wrong < right stays about the same. I call the mean of the lower graphs in Figure 2 the 'distorted average change in loss'. The 'distorted average change in loss' should should stay near 0 as the variance increases on the right half of Figure 1 and should always increase when the variance increases on the right half of Figure 1.I then scaled the ‘distorted average change in loss’ by the original undistorted categorical cross entropy. This is done because the distorted average change in loss for the wrong logit case is about the same for all logit differences greater than three (because the derivative of the line is 0). To ensure the loss is greater than zero, I add the undistorted categorical cross entropy. The ‘distorted average change in loss’ always decreases as the variance increases but the loss function should be minimized for a variance value less than infinity. To ensure the variance that minimizes the loss is less than infinity, I add the exponential of the variance term. As Figure 3 shows, the exponential of the variance is the dominant characteristic after the variance passes 2.These are the results of calculating the above loss function for binary classification example where the ‘right’ logit value is held constant at 1.0 and the ‘wrong’ logit value changes for each line. When the ‘wrong’ logit value is less than 1.0 (and thus less than the ‘right’ logit value), the minimum variance is 0.0. As the wrong ‘logit’ value increases, the variance that minimizes the loss increases.Note: When generating this graph, I ran 10,000 Monte Carlo simulations to create smooth lines. When training the model, I only ran 100 Monte Carlo simulations as this should be sufficient to get a reasonable mean.One way of modeling epistemic uncertainty is using Monte Carlo dropout sampling (a type of variational inference) at test time. For a full explanation of why dropout can model uncertainty check out this blog and this white paper. In practice, Monte Carlo dropout sampling means including dropout in your model and running your model multiple times with dropout turned on at test time to create a distribution of outcomes. You can then calculate the predictive entropy (the average amount of information contained in the predictive distribution).To understand using dropout to calculate epistemic uncertainty, think about splitting the cat-dog image above in half vertically.If you saw the left half, you would predict dog. If you saw the right half you would predict cat. A perfect 50–50 split. This image would high epistemic uncertainty because the image exhibits features that you associate with both a cat class and a dog class.Below are two ways of calculating epistemic uncertainty. They do the exact same thing, but the first is simpler and only uses numpy. The second uses additional Keras layers (and gets GPU acceleration) to make the predictions.Note: Epistemic uncertainty is not used to train the model. It is only calculated at test time (but during a training phase) when evaluating test/real world examples. This is different than aleatoric uncertainty, which is predicted as part of the training process. Also, in my experience, it is easier to produce reasonable epistemic uncertainty predictions than aleatoric uncertainty predictions.Besides the code above, training a Bayesian deep learning classifier to predict uncertainty doesn’t require much additional code beyond what is typically used to train a classifier.For this experiment, I used the frozen convolutional layers from Resnet50 with the weights for ImageNet to encode the images. I initially attempted to train the model without freezing the convolutional layers but found the model quickly became over fit.The trainable part of my model is two sets of BatchNormalization, Dropout, Dense, and relu layers on top of the ResNet50 output. The logits and variance are calculated using separate Dense layers. Note that the variance layer applies a softplus activation function to ensure the model always predicts variance values greater than zero. The logit and variance layers are then recombined for the aleatoric loss function and the softmax is calculated using just the logit layer.I trained the model using two losses, one is the aleatoric uncertainty loss function and the other is the standard categorical cross entropy function. This allows the last Dense layer, which creates the logits, to learn only how to produce better logit values while the Dense layer that creates the variance learns only about predicting variance. The two prior Dense layers will train on both of these losses. The aleatoric uncertainty loss function is weighted less than the categorical cross entropy loss because the aleatoric uncertainty loss includes the categorical cross entropy loss as one of its terms.I used 100 Monte Carlo simulations for calculating the Bayesian loss function. It took about 70 seconds per epoch. I found increasing the number of Monte Carlo simulations from 100 to 1,000 added about four minutes to each training epoch.I added augmented data to the training set by randomly applying a gamma value of 0.5 or 2.0 to decrease or increase the brightness of each image. In practice I found the cifar10 dataset did not have many images that would in theory exhibit high aleatoric uncertainty. This is probably by design. By adding images with adjusted gamma values to images in the training set, I am attempting to give the model more images that should have high aleatoric uncertainty.Unfortunately, predicting epistemic uncertainty takes a considerable amount of time. It takes about 2–3 seconds on my Mac CPU for the fully connected layers to predict all 50,000 classes for the training set but over five minutes for the epistemic uncertainty predictions. This isn’t that surprising because epistemic uncertainty requires running Monte Carlo simulations on each image. I ran 100 Monte Carlo simulations so it is reasonable to expect the prediction process to take about 100 times longer to predict epistemic uncertainty than aleatoric uncertainty.Lastly, my project is setup to easily switch out the underlying encoder network and train models for other datasets in the future. Feel free to play with it if you want a deeper dive into training your own Bayesian deep learning classifier.My model’s categorical accuracy on the test dataset is 86.4%. This is not an amazing score by any means. I was able to produce scores higher than 93%, but only by sacrificing the accuracy of the aleatoric uncertainty. There are a few different hyperparameters I could play with to increase my score. I spent very little time tuning the weights of the two loss functions and I suspect that changing these hyperparameters could greatly increase my model accuracy. I could also unfreeze the Resnet50 layers and train those as well. While getting better accuracy scores on this dataset is interesting, Bayesian deep learning is about both the predictions and the uncertainty estimates and so I will spend the rest of the post evaluating the validity of the uncertainty predictions of my model.The aleatoric uncertainty values tend to be much smaller than the epistemic uncertainty. These two values can’t be compared directly on the same image. They can however be compared against the uncertainty values the model predicts for other images in this dataset.To further explore the uncertainty, I broke the test data into three groups based on the relative value of the correct logit. In Figure 5, ‘first’ includes all of the correct predictions (i.e logit value for the ‘right’ label was the largest value). ‘second’, includes all of the cases where the ‘right’ label is the second largest logit value. ‘rest’ includes all of the other cases. 86.4% of the samples are in the ‘first’ group, 8.7% are in the ‘second’ group, and 4.9% are in the ‘rest’ group. Figure 5 shows the mean and standard deviation of the aleatoric and epistemic uncertainty for the test set broken out by these three groups. As I was hoping, the epistemic and aleatoric uncertainties are correlated with the relative rank of the ‘right’ logit. This indicates the model is more likely to identify incorrect labels as situations it is unsure about. Additionally, the model is predicting greater than zero uncertainty when the model’s prediction is correct. I expected the model to exhibit this characteristic because the model can be uncertain even if it’s prediction is correct.Above are the images with the highest aleatoric and epistemic uncertainty. While it is interesting to look at the images, it is not exactly clear to me why these images images have high aleatoric or epistemic uncertainty. This is one downside to training an image classifier to produce uncertainty. The uncertainty for the entire image is reduced to a single value. It is often times much easier to understand uncertainty in an image segmentation model because it is easier to compare the results for each pixel in an image.If my model understands aleatoric uncertainty well, my model should predict larger aleatoric uncertainty values for images with low contrast, high brightness/darkness, or high occlusions To test this theory, I applied a range of gamma values to my test images to increase/decrease the pixel intensity and predicted outcomes for the augmented images.The model’s accuracy on the augmented images is 5.5%. This means the gamma images completely tricked my model. The model wasn’t trained to score well on these gamma distortions, so that is to be expected. Figure 6 shows the predicted uncertainty for eight of the augmented images on the left and eight original uncertainties and images on the right. The first four images have the highest predicted aleatoric uncertainty of the augmented images and the last four had the lowest aleatoric uncertainty of the augmented images. I am excited to see that the model predicts higher aleatoric and epistemic uncertainties for each augmented image compared with the original image! The aleatoric uncertainty should be larger because the mock adverse lighting conditions make the images harder to understand and the epistemic uncertainty should be larger because the model has not been trained on images with larger gamma distortions.The model detailed in this post explores only the tip of the Bayesian deep learning iceberg and going forward there are several ways in which I believe I could improve the model’s predictions. For example, I could continue to play with the loss weights and unfreeze the Resnet50 convolutional layers to see if I can get a better accuracy score without losing the uncertainty characteristics detailed above. I could also try training a model on a dataset that has more images that exhibit high aleatoric uncertainty. One candidate is the German Traffic Sign Recognition Benchmark dataset which I’ve worked with in one of my Udacity projects. This dataset is specifically meant to make the classifier “cope with large variations in visual appearances due to illumination changes, partial occlusions, rotations, weather conditions”. Sounds like aleatoric uncertainty to me!In addition to trying to improve my model, I could also explore my trained model further. One approach would be to see how my model handles adversarial examples. To do this, I could use a library like CleverHans created by Ian Goodfellow. This library uses an adversarial neural network to help explore model vulnerabilities. It would be interesting to see if adversarial examples produced by CleverHans also result in high uncertainties.Another library I am excited to explore is Edward, a Python library for probabilistic modeling, inference, and criticism. Edward supports the creation of network layers with probability distributions and makes it easy to perform variational inference. This blog post uses Edward to train a Bayesian deep learning classifier on the MNIST dataset.If you’ve made it this far, I am very impressed and appreciative. Hopefully this post has inspired you to include uncertainty in your next deep learning project.Originally published at gist.github.com.",17/07/2017,0,0.0,23.0,818.0,487.0,26.0,2.0,0.0,24.0,en
4061,"Every single Machine Learning course on the internet, ranked by your reviews",We’ve moved to freeCodeCamp.org/news,David Venturi,17300.0,20.0,4207.0,"A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master’s program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost.I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science.For the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization.For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below.For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews.Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.Each course must fit three criteria:We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only.There’s always a chance that we missed something, though. So please let us know in the comments section if we left a good course out.We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings.We made subjective syllabus judgment calls based on three factors:A popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives “computers the ability to learn without being explicitly programmed.” In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience.A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you’ll find helpful visualization of these core steps:The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves.First off, let’s define deep learning. Here is a succinct description:“Deep learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.”— Jason Brownlee from Machine Learning MasteryAs would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we’ve got you covered with the following article:medium.freecodecamp.comMy top three recommendations from that list would be:Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline.Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar.Stanford University’s Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews.Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available.Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning.Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice:In the past, I’ve tried to teach machine learning using a large variety of different programming languages including C++, Java, Python, NumPy, and also Octave … And what I’ve seen after having taught machine learning for almost a decade is that you learn much faster if you use Octave as your programming environment.Though Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn’t stop you from taking the course.A few prominent reviewers noted the following:Of longstanding renown in the MOOC world, Stanford’s machine learning course really is the definitive introduction to this topic. The course broadly covers all of the major areas of machine learning … Prof. Ng precedes each segment with a motivating discussion and examples.Andrew Ng is a gifted teacher and able to explain complicated subjects in a very intuitive and clear way, including the math behind all concepts. Highly recommended.The only problem I see with this course if that it sets the expectation bar very high for other courses.Columbia University’s Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn’t have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews.The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia’s is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding).Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course’s total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase.Below are a few of the aforementioned sparkling reviews:Over all my years of [being a] student I’ve come across professors who aren’t brilliant, professors who are brilliant but they don’t know how to explain the stuff clearly, and professors who are brilliant and know how explain the stuff clearly. Dr. Paisley belongs to the third group.This is a great course … The instructor’s language is precise and that is, to my mind, one of the strongest points of the course. The lectures are of high quality and the slides are great too.Dr. Paisley and his supervisor are … students of Michael Jordan, the father of machine learning. [Dr. Paisley] is the best ML professor at Columbia because of his ability to explain stuff clearly. Up to 240 students have selected his course this semester, the largest number among all professors [teaching] machine learning at Columbia.Machine Learning A-Z™ on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can’t be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered.It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an “intuition” video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R.As a “bonus,” the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren’t the strong points of the course.Eremenko and the SuperDataScience team are revered for their ability to “make the complex simple.” Also, the prerequisites listed are “just some high school mathematics,” so this course might be a better option for those daunted by the Stanford and Columbia offerings.A few prominent reviewers noted the following:The course is professionally produced, the sound quality is excellent, and the explanations are clear and concise … It’s an incredible value for your financial and time investment.It was spectacular to be able to follow the course in two different programming languages simultaneously.Kirill is one of the absolute best instructors on Udemy (if not the Internet) and I recommend taking any class he teaches. … This course has a ton of content, like a ton!Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let’s look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide — you can find those here.The Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews.Python for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews.Data Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla’s above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews.Machine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course’s description. Uses Python. Cost varies depending on Udemy discounts, which are frequent.Machine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Bite-sized videos, as is Udacity’s style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews.Implementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews.Data Science and Machine Learning with Python — Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews.Scala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): “Big data” focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews.Machine Learning Engineer Nanodegree (Udacity): Udacity’s flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews.Learning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech’s independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews.Learning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): “A real Caltech course, not a watered-down version.” Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn’t as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews.Mining Massive Datasets (Stanford University): Machine learning with a focus on “big data.” Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews.AWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews.Introduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews.StatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, “An Introduction to Statistical Learning, with Applications in R” and taught by the professors who wrote it. Reviewers note that the MOOC isn’t as good as the book, citing “thin” exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews.Machine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford’s or Caltech’s). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews.From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): “A down-to-earth, shy but confident take on machine learning techniques.” Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews.Principles of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews.Big Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews.Genomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD’s Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews.Intro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity’s Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews.Machine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan’s Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews.Programming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews.Machine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews.Practical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU’s Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews.Machine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews.Recommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning — recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews.Machine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD’s Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews.Practical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW’s Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews.The following courses had one or no reviews as of May 2017.Machine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review.Applied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available.Applied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase.Machine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free.Machine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free.Machine Learning for Data Science (University of California, San Diego/edX): Doesn’t launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase.Introduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase.Predictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise’s Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase.Introducción al Machine Learning (Universitas Telefónica/Miríada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks.Machine Learning Path Step (Dataquest): Taught in Python using Dataquest’s interactive in-browser platform. Multiple guided projects and a “plus” project where you build your own machine learning system using your own data. Subscription required.The following six courses are offered by DataCamp. DataCamp’s hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course.Introduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours.Supervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours.Unsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours.Machine Learning Toolbox (DataCamp): Teaches the “big ideas” in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours.Machine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school’s budget. DataCamp’s “Supervised Learning with scikit-learn” is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours.Unsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours.Machine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon’s graduate introductory machine learning course. A prerequisite to their second graduate level course, “Statistical Machine Learning.” Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free.Statistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon’s Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free.Undergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below).Machine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas’ undergraduate course (above) apply here as well.This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth.medium.freecodecamp.comThe final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering.If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s Data Science and Big Data subject page.If you enjoyed reading this, check out some of Class Central’s other pieces:medium.freecodecamp.commedium.freecodecamp.comIf you have suggestions for courses I missed, let me know in the responses!If you found this helpful, click the 💚 so more people will see it here on Medium.This is a condensed version of my original article published on Class Central, where I’ve included detailed course syllabi.",03/05/2017,0,13.0,11.0,1307.0,477.0,9.0,6.0,0.0,104.0,en
4062,Art & AI: The Logic Behind Deep Learning ‘Style Transfer’,Center for Open Source Data and AI Technologies,Nick Kasten,100.0,9.0,1517.0,"When humans and machines collaborate, we can produce things neither would create on their own. The intersection of art and AI is an area that I find really exciting, but with all the business impact AI can have, I personally feel it doesn’t always get enough attention. In this spirit, I recently set out on a personal quest to learn more about PyTorch, the machine learning library that’s been creating a lot of buzz since its 1.0 release late last year, and I was pleasantly surprised by what I found.For me, PyTorch turned out to be more than just an interesting alternative to TensorFlow, with dynamic graphs and an imperative coding style. One of the examples from their official docs inspired me to track down some academic papers and take a closer look at the inner-workings of a deep learning technique that’s fascinated me for quite some time — ‘Neural Style Transfer’.In this post, I want to share what I learned about style transfer during my deep dive, along with a couple of pretty cool examples.From a high level, what the algorithm does is pretty straightforward. Take 3 images as input: one for content, one for style, and a target. The ‘target’ image is typically a blank canvas that we build our output from, so it can be ignored for now. What we’re really concerned with are the other two — our content image and the image that we’ll extract artistic style from. What we end up with is one brand new image, representing the original content from one image in the other image’s style. For an even better idea of what style transfer looks like, all of the images in this article have been produced with this method to give some examples.When I realized what was going on here, all kinds of possibilities flooded my mind. I thought of all the iconic artists throughout history that could be applied to modern-day works of art, creating never-before-possible collaborations. Famous band photos could be recreated in the style of popular album covers, I could “draw myself into” my favorite video games, and more!At this point I was eager to start playing around with some images of my own, but I still had a lot to learn about how this all worked. Luckily, there’s a model available on the Model Asset eXchange (MAX) that makes it possible to experiment with a working PyTorch implementation of Neural Style Transfer without having to write any code. After seeing what was possible with some of the sample styles included with the MAX model, I was more determined than ever to understand how this algorithm worked and to come up with some creations using models trained on styles of my own.The question I thought I needed to answer to figure all of this out was “How do I train a style transfer model?” I quickly found, however, that this was entirely the wrong question to be asking! I guess I knew even less than I thought, but that realization is often times the first step towards learning more. The correct questions, and answers, are all laid out in the paper “Image Style Transfer Using Convolutional Neural Networks” written by Leon Gatys, Alexander Ecker, and Mathias Bethge, in which they described a method of using a well-known, existing architecture in a brand new way. I was surprised to learn this, since I hadn’t yet seen a model being repurposed in this way and just assumed a task like this would require a new type of model.The architecture Gatys and his team used was called VGG-19 and was already well known as an image classification model, but I didn’t understand how it could be used to separate style from content in an image. How could a ‘classifier’ type model like this even be used to generate something new? It seemed like I was generating more questions than answers, but as I worked through each head-scratching moment I was inching closer to the truth.Before I was going to understand the details of how this architecture could be used for something unexpected like style transfer, I knew I’d need to refresh myself on some deep learning basics. How do Convolutional Neural Networks (CNNs) typically work? I’ll go over the relevant parts here, but if you’re looking for more in-depth information on CNNs, I’d recommend this course on Coursera or this entertaining video from Siraj Rival.The first step towards understanding how Gatys, et al, pulled off something new and impressive like style transfer is to take a look at the way CNNs handle input.Normally, with this kind of neural network, an image gets fed in as input from the left-hand side, processed as it moved through the convolutional and max pooling layers in the network, and then gets classified by the final ‘classifier’ layers that you can see on the right-hand side of the diagram.The data used by these convolutional layers (pictured above in sea green) to identify an image is very different from what humans would use. As an image works its way through the network, it is being analyzed and transformed into smaller, more computer-vision-friendly representations called ‘feature maps’. These feature maps are built with data extracted from the image as the network scans over it with different ‘filters’, each looking for different details. To the human eye, these feature maps can look increasingly like scribbles or random blurred lines as they get deeper into the layers. To the neural network, however, these feature maps highlight precisely the shapes and patterns contained in an image which are then used to identify content.Again, this is a lightning-fast summary of a topic that can take a lifetime to master, but these details helped me to form a basis for what the paper was introducing with its “Neural Algorithm of Artistic Style” that I was finally ready to dive into.To recap: As the name implies, the VGG-19 architecture features 19 convolutional layers which are separated into stacks by 5 max pooling layers. Without getting too technical, you can think of each max pooling layer as a ‘soft reset’ where the process of extracting data into feature maps starts all over again, with each max pooling layer starting with a smaller, more optimized image than the last.Now that I understood a little more about the architecture I‘d been reading about in the paper, I was ready to take on the details. Let’s take a look at just how style gets transferred:The observation that different parts of the network were looking at the image in different ways was key to the discovery made by Gatys and his team, and is central to the idea of separating content from style. It also nicely highlights some of the ‘dynamic’ properties of PyTorch.There’s still one important part of this process that we haven’t talked about yet - training the model. Now that we’ve figured out how to get the data we need from our source images, how do we create a new image that reflects both of these? Well, those answers are in the paper as well:With a VGG network set up in this manner, and a few pre- and post-processing functions defined to assist in some of the data-wrangling, this is all you need to know to be able to train your own models with styles of your own, that can then be used or deployed anywhere you wish to create new images with a certain style. Be advised, training a model like this can take some fairly serious hardware, or a powerful cloud environment. If you just want to have some fun and experiment with style transfer, the quickest and easiest way to get going is still going to be the MAX Fast Neural Style Transfer model I mentioned earlier.Without a doubt, the work of Leon Gatys, Alexander Ecker, and Mathias Bethge is groundbreaking. While they were among the first to explore techniques like this and publish their research, it’s important to note that concepts like artistic style transfer aren’t yet an exact science. For example, there are other, more performance-focused algorithms that can deliver similar results much more quickly, like the ‘Fast Neural Style Transfer’ algorithm used in the MAX model. This technique improves upon the methods I’ve described here, and they claim to be three orders of magnitude faster. If that excites you, you can read all about it in this paper, “Perceptual Losses for Real-Time Style Transfer and Super-Resolution”, by Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Much like the paper I based this article on it’s pretty intense reading, but nothing beats getting your info from the source! If you’d like to check out more from Lean Gatys from the original paper, he released a Jupyter Notebook showing his PyTorch implementation.Before I go, there are couple other exciting Art & AI projects I’ve found recently that are well worth a look: GANPaint from the MIT-IBM Watson AI Lab, and Veremin, a ‘video-based theremin’ developed by my colleague Thanks for reading! If you know of any good Art & AI projects that I’ve missed, please share in the comments!",21/02/2019,0,5.0,3.0,1066.0,322.0,7.0,2.0,0.0,23.0,en
4063,How to Perform Abstractive Summarization with PEGASUS,Towards Data Science,Jiahao Weng,240.0,5.0,696.0,"Note: For those of you who prefer watching videos, please feel free to play above video on the same content.Given long documents to read, our natural preference is to not read, or at least, to scan just the main points. So having a summary would always be great to save us time ⏳ and brain processing power.However, auto-summarization used to be an impossible task. Specifically, abstractive summarization is very challenging. Differing from extractive summarization (which extracts important sentences from a document and combines them to form a “summary”), abstractive summarization involves paraphrasing words and hence, is more difficult but can potentially give a more coherent and polished summary.It was not until the development of techniques like seq2seq learning and unsupervised language models (e.g., ELMo and BERT) that abstractive summarization becomes more feasible.Building upon earlier breakthroughs in natural language processing (NLP) field, Google’s PEGASUS further improved the state-of-the-art (SOTA) results for abstractive summarization, in particular with low resources. To be more specific, unlike previous models, PEGASUS enables us to achieve close to SOTA results with 1,000 examples, rather than tens of thousands of training data.And in this article, we shall look at the high level workings of PEGASUS and how it can help us in our summarization tasks.On a high level, PEGASUS uses an encoder-decoder model for sequence-to-sequence learning. In such a model, the encoder will first take into consideration the context of the whole input text and encode the input text into something called context vector, which is basically a numerical representation of the input text. This numerical representation will then be fed to the decoder whose job is decode the context vector to produce the summary.In line with recent SOTA NLP models, PEGASUS also adopts the transformer architecture, and if you would like to find out more about what is a transformer, I strongly encourage you to read this article titled “The Illustrated Transformer” by Jay Alammar.What differentiates PEGASUS from previous SOTA models is the pre-training.The authors (Jingqing Zhang et. al.) hypothesizes that pre-training the model to output important sentences is suitable as it closely resembles what abstractive summarization needs to do. Using a metric called ROUGE1-F1, the authors were able to automate the selection of “important” sentences and perform pre-training of the model on a large corpus, i.e., 350 million web pages and 1.5 billion news articles.With the pre-trained model, we can then perform fine-tuning of the model on the actual data which is of a much smaller quantity. In fact, evaluation results on various datasets showed that with just 1,000 training data, the model achieved comparable results to previous SOTA models. This has important practical implications as most of us will not have the resources to collect tens of thousands of document-summary pairs.Upon seeing the evaluation results for PEGASUS, you are probably wondering how you can write the code to use the model. Fortunately for us, Hugging Face 🤗 has PEGASUS model in-store, making it easy for us to leverage on PEGASUS.To perform inference, we can follow the example script provided on Hugging Face’s website.You can swap the model_name with various other fine-tuned models (except for google/pegasus-large) listed here, based on how similar your use case is to the dataset used for fine-tuning.If you would like to have a customized model for your use case, you can fine-tune the google/pegasus-large model on your dataset.To do so, please refer to our Github code which we have adapted from Hugging Face’s example code on fine-tuning.Do however note that fine-tuning both the encoder and decoder can be very memory-intensive. If your local computer is unfortunately not up to the task (like mine 😅), you can consider using Google Cloud. And since Google Cloud has a free trial for new signups, you can experiment at no cost. 😎Through the guide above, we hope that you are now able to adapt and adopt PEGASUS for your abstractive summarization tasks. To put it analogously, we no longer need to work like a horse and can instead ride on PEGASUS to achieve great heights 😉.Thanks for reading and I hope the article was useful :) Please also feel free to comment with any questions or suggestions that you may have.arxiv.orghuggingface.co",04/02/2021,0,0.0,4.0,904.0,423.0,8.0,0.0,0.0,13.0,en
4064,text summarization: applications,Medium,Wenchen's ai fantasy,84.0,3.0,164.0,"this article is mainly a summarization of Yasemin Altun’s presentation in May 2014 on how google applies text summarization.text summarization is highly related to google knowledge graph project：entities description within red circle use text summarization from wiki to give a one sentence description of the entity.She mentioned google then mainly focus on Entity-centric summarization， describe the entities through news-worthy events.Entity-centric summarization:text summarization applies to five categories in google：Given an entity and a time period, provide a summary of the most memorable events involving this entity.describe the entity through the time dimension.Identify and summarize events that leads to the event of interestGiven a sentence, generate a shorter one while preserving the essential information.Learn how events are referred to in text and represent event mentioning text in predicate argument structure.events description are subjective, event understanding is used to keep the neutrality of event description and represents them in predicate arguments formats.Identify main aspects and summarize for machine or user consumption.summarize youtube’s user comments on the video.",25/05/2017,0,8.0,0.0,792.0,449.0,13.0,3.0,0.0,2.0,en
4065,🦄 How to build a State-of-the-Art Conversational AI with Transfer Learning,HuggingFace,Thomas Wolf,5500.0,12.0,2237.0,"A few years ago, creating a chatbot -as limited as they were back then- could take months 🗓, from designing the rules to actually writing thousands of answers to cover some of the conversation topics.With the recent progress in deep-learning for NLP, we can now get rid of this petty work and build much more powerful conversational AI 🌟 in just a matter of hours 🍃 as you will see in this tutorial.We’ve set up a demo running the pretrained model we’ll build together in this tutorial at convai.huggingface.co. Be sure to check it out! 🎮Here is what we will learn and play with today:Together with this post, we released a clean and commented code base with a pretrained model! Check the Github repo here ✈️The story of this post began a few months ago in Montreal 🇨🇦 where Hugging Face finished 1st 🏆 in the automatic track of the Conversational Intelligence Challenge 2 (ConvAI2), a dialog competition at NeurIPS 2018.Our secret sauce was a large-scale pre-trained language model, OpenAI GPT, combined with a Transfer Learning fine-tuning technique.With the fast pace of the competition, we ended up with over 3k lines of code exploring many training and architectural variants.Clearly, publishing such raw code would not have been fair.In the meantime, we had started to build and open-source a repository of transfer learning models called pytorch-pretrained-BERT which ended up being downloaded more than 150 000 times and offered implementations of large-scale language models like OpenAI GPT and it’s successor GPT-2 🦄A few weeks ago, I decided to re-factor our competition code in a clean and commented code-base built on top of pytorch-pretrained-BERT and to write a detailed blog post explaining our approach and code.So here we are, let’s dive in 🚀We’ll build a conversational AI with a persona.Our dialog agent will have a knowledge base to store a few sentences describing who it is (persona) and a dialog history. When a new utterance will be received from a user, the agent will combine the content of this knowledge base with the newly received utterance to generate a reply.Here is the general scheme:When we train a deep-learning based dialog agents, in an end-to-end fashion, we are facing a major issue:Dialog datasets are small and it’s hard to learn enough about language and common-sense from them to be able to generate fluent and relevant responses.Some approaches try to solve this by filtering the output of the model to improve the quality using smart beam search. Here we’ll take another path that gathered tremendous interest over the last months: Transfer Learning.The idea behind this approach is quite simple:Pretraining a language model is an expensive operation so it’s usually better to start from a model that has already been pretrained and open-sourced.What would be a good pretrained model for our purpose?The bigger the better, but we also need a model that can generate text. The most commonly used pretrained NLP model, BERT, is pretrained on full sentences only and is not able to complete unfinished sentences. Two other models, open-sourced by OpenAI, are more interesting for our use-case: GPT & GPT-2.Let’s have a quick look at them 🔎In 2018 and 2019, Alec Radford, Jeffrey Wu and their co-workers at OpenAI open-sourced two language models trained on a very large amount of data: GPT and GPT-2 (where GPT stands for Generative Pretrained Transformer).GPT and GPT-2 are two very similar Transformer-based language models. These models are called decoder or causal models which means that they use the left context to predict the next word (see left figure).Many papers and blog posts describe Transformers models and how they use attention mechanisms to process sequential inputs so I won’t spend time presenting them in details. A few pointers if you are not familiar with these models: Emma Strubell’s EMNLP slides are my personal favorite and Jay Alammar’s “Illustrated Transformer” is a very detailed introduction.For our purpose, a language model will just be a model that takes as input a sequence of tokens and generates a probability distribution over the vocabulary for the next token following the input sequence. Language models are usually trained in a parallel fashion, as illustrated on the above figure, by predicting the token following each token in a long input sequence.Pretraining these models on a large corpus is a costly operation, so we’ll start from a model and tokenizer pretrained by OpenAI. The tokenizer will take care of splitting an input string in tokens (words/sub-words) and convert these tokens in the correct numerical indices of the model vocabulary.In pytorch-pretrained-BERT OpenAI GPT’s model and its tokenizer can be easily created and loaded from the pretrained checkpoint like this:You probably noticed we’ve loaded a model called OpenAI GPT Double Heads Model which sounds a bit more complex than the language model we’ve just talked about and you’re right!This is because we need to adapt our model to dialog. Let’s see how this goes!Our language model is trained with a single input: a sequence of words.But as we saw earlier, in a dialog setting, our model will have to use several types of contexts to generate an output sequence:How can we build an input for our model from these various contexts?A simple answer is just to concatenate the context segments in a single sequence, putting the reply at the end. We can then generate a completion of the reply token by token by continuing the sequence:There are two issues with this simple setup:An easy way to add this information is to build three parallel input sequences for word, position, and segments, and fuse them in a single sequence, summing three types of embeddings: word, position, and segments embeddings:How do we implement this?First, we’ll add special tokens to our vocabulary for delimiters and segment indicators. These tokens were not part of our model’s pretraining so we will need to create and train new embeddings for them.Adding special tokens and new embeddings to the vocabulary/model is quite simple with pytorch-pretrained-BERT classes. Let’s add five special tokens to our tokenizer’s vocabulary and model’s embeddings:These special-tokens methods respectively add our five special tokens to the vocabulary of the tokenizer and create five additional embeddings in the model.Now we have all we need to build our input sequence from the persona, history, and beginning of reply contexts. Here is a simple example:We have now initialized our pretrained model and built our training inputs, all that remains is to choose a loss to optimize during the fine-tuning.We will use a multi-task loss combining language modeling with a next-sentence prediction objective.The next-sentence prediction objective is a part of BERT pretraining. It consists in randomly sampling distractors from the dataset and training the model to distinguish whether an input sequence ends with a gold reply or a distractor. It trains the model to look at the global segments meaning besides the local context.Now you see why we loaded a “Double-Head” model. One head will compute language modeling predictions while the other head will predict next-sentence classification labels. Let’s have a look at how losses are computed:The total loss will be the weighted sum of the language modeling loss and the next-sentence prediction loss which are computed as follow:Let’s see how we can code this:We now have all the inputs required by our model and we can run a forward pass of the model to get the two losses and the total loss (as a weighted sum):We are ready to start the training 🎉The ConvAI2 competition used an interesting dataset released by Facebook last year: PERSONA-CHAT.It’s a rather large dataset of dialog (10k dialogs) which was created by crowdsourcing personality sentences and asking paired crowd workers to chit-chat while playing the part of a given character (an example is given on the left figure).This dataset is available in raw tokenized text format in the nice Facebook’s ParlAI library. To bootstrap you, we also uploaded a JSON formatted version that you can download and tokenize using GPT’s tokenizer like this:The JSON version of PERSONA-CHAT gives quick access to all the relevant inputs for training our model as a nested dictionary of lists:Using the awesome PyTorch ignite framework and the new API for Automatic Mixed Precision (FP16/32) provided by NVIDIA’s apex, we were able to distill our +3k lines of competition code in less than 250 lines of training code with distributed and FP16 options!We’ve covered the essential parts of the code in the above gists so I’ll just let you read the commented code to see how it all fits together.The training (train.py) code is here ➱ 🎮Training this model on an AWS instance with 8 V100 GPU takes less than an hour (currently less than $25 on the biggest p3.16xlarge AWS instance) and gives results close to the SOTA obtained during the ConvAI2 competition with Hits@1 over 79, perplexity of 20.5 and F1 of 16.5.A few differences explain the slightly lower scores vs our competition model, they are detailed in the readme of the code repo here and mostly consists in tweaking the position embeddings and using a different decoder.The amazing thing about dialog models is that you can talk with them 🤗To interact with our model, we need to add one thing: a decoder that will build full sequences from the next token predictions of our model.Now there have been very interesting developments in decoders over the last few months and I wanted to present them quickly here to get you up-to-date.The two most common decoders for language generation used to be greedy-decoding and beam-search.Greedy-decoding is the simplest way to generate a sentence: at each time step, we select the most likely next token according to the model until we reach end-of-sequence tokens. One risk with greedy decoding is that a highly probable token may be hiding after a low-probability token and be missed.Beam-search try to mitigate this issue by maintaining a beam of several possible sequences that we construct word-by-word. At the end of the process, we select the best sentence among the beams. Over the last few years, beam-search has been the standard decoding algorithm for almost all language generation tasks including dialog (see the recent [1]).However several developments happened in 2018/early-2019. First, there was growing evidence that beam-search was strongly sensitive to the length of the outputs and best results could be obtained when the output length was predicted before decoding ([2, 3] at EMNLP 2018). While this makes sense for low-entropy tasks like translation where the output sequence length can be roughly predicted from the input, it seems arbitrary for high-entropy tasks like dialog and story generation where outputs of widely different lengths are usually equally valid.In parallel, at least two influential papers ([4, 5]) on high-entropy generation tasks were published in which greedy/beam-search decoding was replaced by sampling from the next token distribution at each time step. These papers used a variant of sampling called top-k sampling in which the decoder sample only from the top-k most-probable tokens (k is a hyper-parameter).The last stone in this recent trend of work is the study recently published by Ari Holtzman et al. [6] which showed that the distributions of words in texts generated using beam-search and greedy decoding is very different from the distributions of words in human-generated texts. Clearly, beam-search and greedy decoding fail to reproduce some distributional aspects of human texts as it has also been noted in [7, 8] in the context of dialog systems:Currently, the two most promising candidates to succeed beam-search/greedy decoding are top-k and nucleus (or top-p) sampling. The general principle of these two methods is to sample from the next-token distribution after having filtered this distribution to keep only the top k tokens (top-k) or the top tokens with a cumulative probability just above a threshold (nucleus/top-p).Here is how we can decode using top-k and/or nucleus/top-p sampling:We are now ready to talk with our model 🚀The interactive script is here (interact.py) and if you don’t want to run the script you can also just play with our live demo which is here 🎮Here is an example of dialog:We’ve come to the end of this post describing how you can build a simple state-of-the-art conversational AI using transfer learning and a large-scale language model like OpenAI GPT.As we learned at Hugging Face, getting your conversational AI up and running quickly is the best recipe for success so we hope it will help some of you do just that!Be sure to check out the associated demo and code:As always, if you liked this post, give us a few 👏 to let us know and share the news around you![1] ^ Importance of a Search Strategy in Neural Dialogue Modelling by Ilya Kulikov, Alexander H. Miller, Kyunghyun Cho, Jason Weston (http://arxiv.org/abs/1811.00907)[2] ^ Correcting Length Bias in Neural Machine Translation by Kenton Murray, David Chiang (http://arxiv.org/abs/1808.10006)[3] ^ Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation by Yilin Yang, Liang Huang, Mingbo Ma (https://arxiv.org/abs/1808.09582)[4] ^ Hierarchical Neural Story Generation by Angela Fan, Mike Lewis, Yann Dauphin (https://arxiv.org/abs/1805.04833)[5] ^ Language Models are Unsupervised Multitask Learners by Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever (https://openai.com/blog/better-language-models/)[6] ^ The Curious Case of Neural Text Degeneration by Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi (https://arxiv.org/abs/1904.09751)[7] ^ Retrieve and Refine: Improved Sequence Generation Models For Dialogue by Jason Weston, Emily Dinan, Alexander H. Miller (https://arxiv.org/abs/1808.04776)[8] ^ The Second Conversational Intelligence Challenge (ConvAI2) by Emily Dinan et al. (https://arxiv.org/abs/1902.00098)",09/05/2019,0,59.0,71.0,1156.0,638.0,13.0,6.0,0.0,56.0,en
4066,A Guide For Time Series Prediction Using Recurrent Neural Networks (LSTMs),Cube Dev,Neelabh Pant,457.0,11.0,2092.0,"The Statsbot team has already published the article about using time series analysis for anomaly detection. Today, we’d like to discuss time series prediction with a long short-term memory model (LSTMs). We asked a data scientist, Neelabh Pant, to tell you about his experience of forecasting exchange rates using recurrent neural networks.As an Indian guy living in the US, I have a constant flow of money from home to me and vice versa. If the USD is stronger in the market, then the Indian rupee (INR) goes down, hence, a person from India buys a dollar for more rupees. If the dollar is weaker, you spend less rupees to buy the same dollar.If one can predict how much a dollar will cost tomorrow, then this can guide one’s decision making and can be very important in minimizing risks and maximizing returns. Looking at the strengths of a neural network, especially a recurrent neural network, I came up with the idea of predicting the exchange rate between the USD and the INR.There are a lot of methods of forecasting exchange rates such as:In this article, we’ll tell you how to predict the future exchange rate behavior using time series analysis and by making use of machine learning with time series.Let us begin by talking about sequence problems. The simplest machine learning problem involving a sequence is a one to one problem.In this case, we have one data input or tensor to the model and the model generates a prediction with the given input. Linear regression, classification, and even image classification with convolutional network fall into this category. We can extend this formulation to allow for the model to make use of the pass values of the input and the output.It is known as the one to many problem. The one to many problem starts like the one to one problem where we have an input to the model and the model generates one output. However, the output of the model is now fed back to the model as a new input. The model now can generate a new output and we can continue like this indefinitely. You can now see why these are known as recurrent neural networks.A recurrent neural network deals with sequence problems because their connections form a directed cycle. In other words, they can retain state from one iteration to the next by using their own output as input for the next step. In programming terms this is like running a fixed program with certain inputs and some internal variables. The simplest recurrent neural network can be viewed as a fully connected neural network if we unroll the time axes.In this univariate case only two weights are involved. The weight multiplying the current input xt, which is u, and the weight multiplying the previous output yt-1, which is w. This formula is like the exponential weighted moving average (EWMA) by making its pass values of the output with the current values of the input.One can build a deep recurrent neural network by simply stacking units to one another. A simple recurrent neural network works well only for a short-term memory. We will see that it suffers from a fundamental problem if we have a longer time dependency.As we have talked about, a simple recurrent network suffers from a fundamental problem of not being able to capture long-term dependencies in a sequence. This is a problem because we want our RNNs to analyze text and answer questions, which involves keeping track of long sequences of words.In late ’90s, LSTM was proposed by Sepp Hochreiter and Jurgen Schmidhuber, which is relatively insensitive to gap length over alternatives RNNs, hidden markov models, and other sequence learning methods in numerous applications.This model is organized in cells which include several operations. LSTM has an internal state variable, which is passed from one cell to another and modified by Operation Gates.1. Forget GateIt is a sigmoid layer that takes the output at t-1 and the current input at time t and concatenates them into a single tensor and applies a linear transformation followed by a sigmoid. Because of the sigmoid, the output of this gate is between 0 and 1. This number is multiplied with the internal state and that is why the gate is called a forget gate. If ft=0 then the previous internal state is completely forgotten, while if ft=1 it will be passed through unaltered.2. Input GateThe input gate takes the previous output and the new input and passes them through another sigmoid layer. This gate returns a value between 0 and 1. The value of the input gate is multiplied with the output of the candidate layer.This layer applies a hyperbolic tangent to the mix of input and previous output, returning a candidate vector to be added to the internal state.The internal state is updated with this rule:.The previous state is multiplied by the forget gate and then added to the fraction of the new candidate allowed by the output gate.3. Output GateThis gate controls how much of the internal state is passed to the output and it works in a similar way to the other gates.These three gates described above have independent weights and biases, hence the network will learn how much of the past output to keep, how much of the current input to keep, and how much of the internal state to send out to the output.In a recurrent neural network, you not only give the network the data, but also the state of the network one moment before. For example, if I say “Hey! Something crazy happened to me when I was driving” there is a part of your brain that is flipping a switch that’s saying “Oh, this is a story Neelabh is telling me. It is a story where the main character is Neelabh and something happened on the road.” Now, you carry a little part of that one sentence I just told you. As you listen to all my other sentences you have to keep a bit of information from all past sentences around in order to understand the entire story.Another example is video processing, where you would again need a recurrent neural network. What happens in the current frame is heavily dependent upon what was in the last frame of the movie most of the time. Over a period of time, a recurrent neural network tries to learn what to keep and how much to keep from the past, and how much information to keep from the present state, which makes it so powerful as compared to a simple feed forward neural network.I was impressed with the strengths of a recurrent neural network and decided to use them to predict the exchange rate between the USD and the INR. The dataset used in this project is the exchange rate data between January 2, 1980 and August 10, 2017. Later, I’ll give you a link to download this dataset and experiment with it.The dataset displays the value of $1 in rupees. We have a total of 13,730 records starting from January 2, 1980 to August 10, 2017.Over the period, the price to buy $1 in rupees has been rising. One can see that there was a huge dip in the American economy during 2007–2008, which was hugely caused by the great recession during that period. It was a period of general economic decline observed in world markets during the late 2000s and early 2010s.This period was not very good for the world’s developed economies, particularly in North America and Europe (including Russia), which fell into a definitive recession. Many of the newer developed economies suffered far less impact, particularly China and India, whose economies grew substantially during this period.Now, to train the machine we need to divide the dataset into test and training sets. It is very important when you do time series to split train and test with respect to a certain date. So, you don’t want your test data to come before your training data.In our experiment, we will define a date, say January 1, 2010, as our split date. The training data is the data between January 2, 1980 and December 31, 2009, which are about 11,000 training data points.The test dataset is between January 1, 2010 and August 10, 2017, which are about 2,700 points.The next thing to do is normalize the dataset. You only need to fit and transform your training data and just transform your test data. The reason you do that is you don’t want to assume that you know the scale of your test data.Normalizing or transforming the data means that the new scale variables will be between zero and one.A fully Connected Model is a simple neural network model which is built as a simple regression model that will take one input and will spit out one output. This basically takes the price from the previous day and forecasts the price of the next day.As a loss function, we use mean squared error and stochastic gradient descent as an optimizer, which after enough numbers of epochs will try to look for a good local optimum. Below is the summary of the fully connected layer.After training this model for 200 epochs or early_callbacks (whichever came first), the model tries to learn the pattern and the behavior of the data. Since we split the data into training and testing sets we can now predict the value of testing data and compare them with the ground truth.As you can see, the model is not good. It essentially is repeating the previous values and there is a slight shift. The fully connected model is not able to predict the future from the single previous value. Let us now try using a recurrent neural network and see how well it does.The recurrent model we have used is a one layer sequential model. We used 6 LSTM nodes in the layer to which we gave input of shape (1,1), which is one input given to the network with one value.The last layer is a dense layer where the loss is mean squared error with stochastic gradient descent as an optimizer. We train this model for 200 epochs with early_stopping callback. The summary of the model is shown above.This model has learned to reproduce the yearly shape of the data and doesn’t have the lag it used to have with a simple feed forward neural network. It is still underestimating some observations by certain amounts and there is definitely room for improvement in this model.There can be a lot of changes to be made in this model to make it better. One can always try to change the configuration by changing the optimizer. Another important change I see is by using the Sliding Time Window method, which comes from the field of stream data management system.This approach comes from the idea that only the most recent data are important. One can show the model data from a year and try to make a prediction for the first day of the next year. Sliding time window methods are very useful in terms of fetching important patterns in the dataset that are highly dependent on the past bulk of observations.Try to make changes to this model as you like and see how the model reacts to those changes.I made the dataset available on my github account under deep learning in python repository. Feel free to download the dataset and play with it.I personally follow some of my favorite data scientists like Kirill Eremenko, Jose Portilla, Dan Van Boxel (better known as Dan Does Data), and many more. Most of them are available on different podcast stations where they talk about different current subjects like RNN, Convolutional Neural Networks, LSTM, and even the most recent technology, Neural Turing Machine.Try to keep up with the news of different artificial intelligence conferences. By the way, if you are interested, then Kirill Eremenko is coming to San Diego this November with his amazing team to give talks on Machine Learning, Neural Networks, and Data Science.LSTM models are powerful enough to learn the most important past behaviors and understand whether or not those past behaviors are important features in making future predictions. There are several applications where LSTMs are highly used. Applications like speech recognition, music composition, handwriting recognition, and even in my current research of human mobility and travel predictions.According to me, LSTM is like a model which has its own memory and which can behave like an intelligent human in making decisions.Thank you again and happy machine learning!blog.statsbot.coblog.statsbot.coblog.statsbot.c",07/09/2017,0,21.0,16.0,660.0,257.0,20.0,1.0,0.0,15.0,en
4067,ICO TokenGo заканчивается!,Medium,TokenGo Platform_RU,294.0,3.0,631.0,"Дорогие друзья! Подходит к концу май месяц, наступает долгожданное для многих лето. Сегодня я хочу подвести итоги и рассказать о планах на самое ближайшее будущее.Во-первых, сегодня — 31 мая, очень важный для нас день, мы завершаем Баунти-кампанию TokenGo! Выполнен огромный объем задач, распределены все выделенные на баунти-кампанию токены! Руководство платформы TokenGo от всей души благодарит участников-баунтистов за неоценимый вклад в развитие и продвижение наших идей и поздравляет с окончанием большого и важного этапа! Мы надеемся, что все вы продолжите работу в данном направлении в баунти-кампаниях наших партнеров!Во-вторых, хочу ответить на один из самых часто задаваемых вопросов! Можно ли теперь выводить токены? Да. Токены выводить можно! Причем, можно выводить и токены, которые вы приобрели в процессе проведения ICO, и токены, которые получили за выполнение задач Баунти-кампании. Для вывода купленных токенов по-прежнему достаточно подать соответствующую заявку в своем личном кабинете. Вывод токенов осуществляется в течение 24–96 часов. Комиссия на вывод купленных токенов GoPower оплачивается за счет TokenGo.Вывод токенов заработанных в Баунти-кампании осуществляется при условии, что GAS Prise ETH составляет не более 20 GWEI, что в переводе 0.00107066 ETH, которые должны быть оплачены Вами с адреса, на который Вы получаете токены GoPower. Оплата производится на адрес TokenGo https://etherscan.io/address/0x36c5d50007a2ce29ca329cdeac8217a8438dfa23 Адрес для проверки GWEI https://ethgasstation.info/После оплаты Вами комиссии в размере 0.00107066 ETH (примерно 0,62$) и при условии, что GAS Price ETH не более 20 GWEI, вывод токенов GoPower производится в течении 96 часов.Gas — это единица оплаты комиссий при проведении любой операции на платформе Ethereum. Т.е. данная комиссия — это сумма, необходимая для совершения транзакции по переводу токенов GoPower. Так как стоимость газа постоянно изменяется, сумма комиссии, которую вносит участник TokenGo, зафиксирована в ETH. В случае, если стоимость газа превышает 20 Gwei по курсу, транзакция изменит статус на «ожидание» и будет находиться в нем до тех пор, пока газ не будет равен или меньше 20 Gwei — тогда и будет произведен обмен.Без подтвержденного факта оплаты комиссии, вывод заработанных в Баунти-кампании токенов невозможен. Оплата комиссии должна быть произведена именно с того же адреса, на который будут выводиться токены. При запросе токенов на вывод, платформа автоматически найдет оплаченную с этого же адреса комиссию, и разрешит создание транзакции на вывод.Напоминаю, что данный алгоритм работает как временный вариант вывода токенов. Согласно дорожной карте в TokenGo появится собственный блокчейн и будет подключена процедура миграции токенов с блокчейна Ethereum на блокчейн TokenGo согласно правилам, описанным в WhitePaper. Таким образом, вывод токенов не потребует уплаты дополнительных комиссий в ETH. Кроме этого в скором времени будет запущена собственная биржа, а также ожидается выход GoPower на альтернативные криптобиржи. Также стоит заметить, что держатели GoPower обладают обширными привилегиями, одними из которых является возможность участия в минтинге (майнинге) и распределении внутренних монет GoCoin между держателями GoPower пропорционально объему их доли, а также «сила голоса». Подробнее о данных привилегиях также можно прочитать в WhitePaper TokenGo.И третий пункт, о котором я хотел бы сказать вам — это сбор средств посредством процедуры ICO. На текущий момент мы собрали более 5100 ETH, что, как я уже неоднократно говорил, является очень хорошей стартовой суммой для нашего продукта. Весь следующий месяц мы планируем активно заниматься выводом токенов GoPower на подходящие нам биржи, и до наступления этого момента мы приняли решение о продолжении приема средств. Это также обусловлено тем, что оплата участия партнеров в Баунти-кампаниях до выхода токена на биржи оплачивается стандартным методом. Как только токен будет выпущен на первую биржу, мы прекратим прием средств, и процедура ICO будет окончательно завершена.Я благодарю всех, кто поддерживает платформу с истоков ее развития, а также тех, кто только присоединился! В сообществе наша сила! Желаю всем отличного летнего сезона!Больше о TokenGo:Как работает TokenGo. Примеры использованияТокенизация бизнеса на платформе TokenGoКонсенсус TokenGoЭкосистема TokenGoХарвестинг TokenGoОснователь TokenGo Антон Бендерский о перспективах платформы.Основатель TokenGo Антон Бендерский о допущенных ошибках и текущих достижениях.ICO TokenGo — новый этап!TokenGo запускает ICO!Официальный сайт TokenGoRU ветка TokenGo на bitcointalkОфициальный RU блог TokenGoОфициальный EN блог TokenGoWhitepaper RUWhitepaper ENGithubTwitterFacebookTelegram чат",31/05/2018,0,3.0,0.0,1200.0,630.0,1.0,0.0,0.0,23.0,ru
4068,Basics of the Classic CNN,Towards Data Science,Chandra Churh Chatterjee,119.0,8.0,1420.0,"Convolutional neural networks. Sounds like a weird combination of biology and math with a little CS sprinkled in, but these networks have been some of the most influential innovations in the field of computer vision and image processing.The Convolutional neural networks are regularized versions of multilayer perceptron (MLP). They were developed based on the working of the neurons of the animal visual cortex.Let’s say we have a color image in JPG form and its size is 480 x 480. The representative array will be 480 x 480 x 3. Each of these numbers is given a value from 0 to 255 which describes the pixel intensity at that point. RGB intensity values of the image are visualized by the computer for processing.The idea is that you give the computer this array of numbers and it will output numbers that describe the probability of the image being a certain class (.80 for a cat, .15 for a dog, .05 for a bird, etc.). It works similar to how our brain works. When we look at a picture of a dog, we can classify it as such if the picture has identifiable features such as paws or 4 legs. In a similar way, the computer is able to perform image classification by looking for low-level features such as edges and curves and then building up to more abstract concepts through a series of convolutional layers. The computer uses low-level features obtained at the initial levels to generate high-level features such as paws or eyes to identify the object.Contents of a classic Convolutional Neural Network: -1.Convolutional Layer.2.Activation operation following each convolutional layer.3.Pooling layer especially Max Pooling layer and also others based on the requirement.4.Finally Fully Connected Layer.1.Input to a convolutional layerThe image is resized to an optimal size and is fed as input to the convolutional layer.Let us consider the input as 32x32x3 array of pixel values2. There exists a filter or neuron or kernel which lays over some of the pixels of the input image depending on the dimensions of the Kernel size.Let the dimensions of the kernel of the filter be 5x5x3.3. The Kernel actually slides over the input image, thus it is multiplying the values in the filter with the original pixel values of the image (aka computing element-wise multiplications).The multiplications are summed up generating a single number for that particular receptive field and hence for sliding the kernel a total of 784 numbers are mapped to 28x28 array known as the feature map.**Now if we consider two kernels of the same dimension then the obtained first layer feature map will be (28x28x2).•Let us take a kernel of size (7x7x3) for understanding. Each of the kernels is considered to be a feature identifier, hence say that our filter will be a curve detector.The sum of the multiplication value that is generated is = 4*(50*30)+(20*30) = 6600 (large number)The sum of the multiplication value that is generated is = 0 (small number).1. The value is much lower! This is because there wasn’t anything in the image section that responded to the curve detector filter. Remember, the output of this convolution layer is an activation map. So, in the simple case of a one filter convolution (and if that filter is a curve detector), the activation map will show the areas in which there at most likely to be curved in the picture.2. In the previous example, the top-left value of our 26 x 26 x 1 activation map (26 because of the 7x7 filter instead of 5x5) will be 6600. This high value means that it is likely that there is some sort of curve in the input volume that caused the filter to activate. The top right value in our activation map will be 0 because there wasn’t anything in the input volume that caused the filter to activate. This is just for one filter.3. This is just a filter that is going to detect lines that curve outward and to the right. We can have other filters for lines that curve to the left or for straight edges. The more filters, the greater the depth of the activation map, and the more information we have about the input volume.In the picture, we can see some examples of actual visualizations of the filters of the first conv. layer of a trained network. Nonetheless, the main argument remains the same. The filters on the first layer convolve around the input image and “activate” (or compute high values) when the specific feature it is looking for is in the input volume.1.When we go through another conv. layer, the output of the first conv. layer becomes the input of the 2nd conv. layer.2. However, when we’re talking about the 2nd conv. layer, the input is the activation map(s) that result from the first layer. So each layer of the input is basically describing the locations in the original image for where certain low-level features appear.3. Now when you apply a set of filters on top of that (pass it through the 2nd conv. layer), the output will be activations that represent higher-level features. Types of these features could be semicircles (a combination of a curve and straight edge) or squares (a combination of several straight edges). As you go through the network and go through more conv. layers, you get activation maps that represent more and more complex features.4. By the end of the network, you may have some filters that activate when there is handwriting in the image, filters that activate when they see pink objects, etc.1.The way this fully connected layer works is that it looks at the output of the previous layer (which as we remember should represent the activation maps of high-level features) and the number of classes N (10 for digit classification).2. For example, if the program is predicting that some image is a dog, it will have high values in the activation maps that represent high-level features like a paw or 4 legs, etc. Basically, an FC layer looks at what high level features most strongly correlate to a particular class and has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes.3. The output of a fully connected layer is as follows [0 .1 .1 .75 0 0 0 0 0 .05], then this represents a 10% probability that the image is a 1, a 10% probability that the image is a 2, a 75% probability that the image is a 3, and a 5% probability that the image is a 9 (Softmax approach) for digit classification.§We know kernels also known as feature identifiers, used for identification of specific features. But how the kernels are initialized with the specific weights or how do the filters know what values to have.Hence comes the important step of training. The training process is also known as backpropagation, which is further separated into 4 distinct sections or processes.•Forward Pass•Loss Function•Backward Pass•Weight UpdateThe Forward Pass:For the first epoch or iteration of the training the initial kernels of the first conv. layer is initialized with random values. Thus after the first iteration output will be something like [.1.1.1.1.1.1.1.1.1.1], which does not give preference to any class as the kernels don’t have specific weights.The Loss Function:The training involves images along with labels, hence the label for the digit 3 will be [0 0 0 1 0 0 0 0 0 0], whereas the output after a first epoch is very different, hence we will calculate loss (MSE — Mean Squared Error)The objective is to minimize the loss, which is an optimization problem in calculus. It involves trying to adjust the weights to reduce the loss.The Backward Pass:It involves determining which weights contributed most to the loss and finding ways to adjust them so that the loss decreases. It is computed using dL/dW, where L is the loss and the W is the weights of the corresponding kernel.The weight update:This is where the weights of the kernel are updated using the following equation.Here the Learning Rate is chosen by the programmer. Larger value of the learning rate indicates much larger steps towards optimization of steps and larger time to convolve to an optimized weight.Finally, to see whether or not our CNN works, we have a different set of images and labels (can’t double dip between training and test!) and pass the images through the CNN. We compare the outputs to the ground truth and see if our network works!",31/07/2019,0,15.0,0.0,565.0,229.0,15.0,2.0,0.0,0.0,en
4069,Semantic Image Segmentation using Fully Convolutional Networks,Towards Data Science,Arun Kumar,29.0,16.0,2406.0,"Humans have the innate ability to identify the objects that they see in the world around them. The visual cortex present in our brain can distinguish between a cat and a dog effortlessly in almost no time. This is true not only with cats and dogs but with almost all the objects that we see. But a computer is not as smart as a human brain to be able to this on its own. Over the past few decades, Deep Learning researchers have tried to bridge this gap between human brain and computer through a special type of artificial neural networks called Convolutional Neural Networks(CNNs).After a lot of research to study mammalian brains, researchers found that specific parts of the brain get activated to specific type of stimulus. For example, some parts in the visual cortex get activated when we see vertical edges, some when we see horizontal edges, and some others when we see specific shapes, colors, faces, etc. ML researchers imagined each of these parts as a layer of neural network and considered the idea that a large network of such layers could mimic the human brain.This intuition gave rise to the advent of CNN, which is a type of neural network whose building blocks are convolutional layers. A convolution layer is nothing but a set of weight matrices called kernels or filters which are used for convolution operation on a feature matrix such as an image.Convolution:2D convolution is a fairly simple operation, you start with a kernel and ‘stride’ (slide) it over the 2D input data, performing an element-wise multiplication with the part of the input it is currently on, and then summing up the results into a single output cell. The kernel repeats this process for every location it slides over, converting a 2D matrix of features into another 2D matrix of features.The step size by which the kernel slides on the input feature matrix is called stride. In the below animation, the input matrix has been added with an extra stripe of zeros from all four sides to ensure that the output matrix is of the same size as the input matrix. This is called (zero)padding.Image segmentation is the task of partitioning a digital image into multiple segments (sets of pixels) based on some characteristics. The objective is to simplify or change the image into a representation that is more meaningful and easier to analyze.Semantic Segmentation refers to assigning a class label to each pixel in the given image. See the below example.Note that segmentation is different from classification. In classification, complete image is assigned a class label whereas in segmentation, each pixel in an image is classified into one of the classes.Having a fair idea about convolutional networks and semantic image segmentation, let’s jump into the problem we need to solve.Severstal is among the top 50 producers of steel in the world and Russia’s biggest player in efficient steel mining and production. One of the key products of Severstal is steel sheets. The production process of flat sheet steel is delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it’s ready to ship. To ensure quality in the production of steel sheets, today, Severstal uses images from high-frequency cameras to power a defect detection algorithm.Through this competition, Severstal expects the AI community to improve the algorithm by localizing and classifying surface defects on a steel sheet.Our task is toTo put it together, it is a semantic image segmentation problem.The evaluation metric used is the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:where X is the predicted set of pixels and Y is the ground truth.Read more about Dice Coefficient here.We have been given a zip folder of size 2GB which contains the following:More details about data have been discussed in the next section.The first step in solving any machine learning problem should be a thorough study of the raw data. This gives a fair idea about what our approaches to solving the problem should be. Very often, it also helps us find some latent aspects of the data which might be useful to our models.Let’s analyze the data and try to draw some meaningful conclusions.train.csv tells which type of defect is present at what pixel location in an image. It contains the following columns:Number of train and test imagesLet’s get some idea about the proportion of train and test images and check how many train images contain defects.Sizes of train and test imagesLet’s check if all images in train and test are of the same size. If not, we must make them of the same size.Let’s see how train data is distributed among various classes.Note that the Sum of percentage values in the above analysis is more than 100, which means some images have defects belonging to more than one class.Number of labels tagged per imageBefore we move ahead to training deep learning models, we need to convert the raw data into a form that can be fed to the models. Also, we need to build a data pipeline, which would perform the required pre-processing and generate batches of input and output images for training.As the first step, we create a pandas dataframe containing filenames of train images under the column ImageId, and EncodedPixels under one or more of the columns Defect_1, Defect_2, Defect_3, Defect_4 depending on the ClassId of the image in train.csv. The images that do not have any defects have all these 4 columns blank. Below is a sample of the dataframe:I would train my models on 85% of train images and validate on 15%.Let’s visualize some images from each class along with their masks. The pixels belonging to the defective area in the steel sheet image are indicated by yellow color in the mask image.Our deep learning model would take steel sheet image as input (X) and return four masks (Y)(corresponding to 4 classes) as output. This implies, for training our model we would need to feed batches of train images and their corresponding masks to the model.I have generated masks for all the images in the train_images folder and stored them into a folder called train_masks.The below code is data pipeline for applying pre-processing, augmentation to input images and generating batches for training.I have used a hybrid loss function which is a combination of binary cross-entropy (BCE) and dice loss. BCE corresponds to binary classification of each pixel (0 indicating false prediction of defect at that pixel when compared to the ground truth mask and 1 indicating correct prediction). Dice loss is given by (1- dice coefficient).BCE dice loss = BCE + dice lossThere are several models/architectures that are used for semantic image segmentation. I have tried two of them in this case study: i)U-Net and ii) Google’s DeepLabV3+.This model is based on the research paper U-Net: Convolutional Networks for Biomedical Image Segmentation, published in 2015 by Olaf Ronneberger, Philipp Fischer, and Thomas Brox of University of Freiburg, Germany. In this paper, the authors build upon an elegant architecture, called “Fully Convolutional Network”. They have used this for segmentation of neuronal structures in electron microscopic stacks and few other biomedical image segmentation datasets.5.1.1. Architecture The Architecture of the network is shown in the image below. It consists of a contracting path (left side) and an expansive path (right side). The expanding path is symmetric to the contracting path giving the network a shape resembling the English letter ‘U’. Due to this reason, the network is called U-Net.The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled. In this path, the model captures the important features (similar to defects in steel sheet) from the image and discards the unimportant features, reducing the resolution of the image at each convolution+maxpool layer.  In the expansive path, every step consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer, a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes(4 in our case).  In order to localize precisely, high-resolution features from the contracting path are cropped and combined with the upsampled output and fed to a successive convolution layer which will learn to assemble more precise output.The code for U-Net model is available in my GitHub repository.5.1.2. Training I have trained the model using Keras Adam optimizer with the default learning rate for 50 epochs. The loss function that the optimizer tries to minimize is bce_dice_loss, defined earlier in section 4.4.I have used Keras model checkpoint to monitor the validation dice coefficient as the training progresses and save the model with the best validation dice coefficient score. TensorBoard has been used to dynamically plot the loss and score while training.5.1.3. Training plots5.1.4. TestingThe figure below shows a sample image from validation data alongside its ground truth mask and predicted mask.Since Kaggle requires us to submit predictions on original size and not on half size images, I have rebuilt the model with input size = (256, 1600, 3) and loaded it with the weights of the model trained on 128×800 images. I have taken this liberty because CNNs are fairly invariant to different input sizes.DeepLab is a state-of-the-art semantic segmentation model designed and open-sourced by Google in 2016. Since then, multiple improvements have been made to the model, including DeepLab V2, DeepLab V3, and the latest DeepLab V3+.DeepLab V3+ is based on the paper Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation, published in 2018 by Google.5.2.1. ArchitectureSimilar to U-Net discussed earlier, DeepLab V3+ is also an encoder-decoder architecture. The major difference is that it uses Atrous convolution instead of simple convolution. We would learn more about Atrous convolution later in this section.The encoder module encodes multi-scale contextual information by applying atrous convolution at multiple scales, while the simple yet eﬀective decoder module reﬁnes the segmentation results along object boundaries.Atrous ConvolutionAtrous convolution is a generalized form of standard convolution operation that allows us to explicitly control ﬁlter’s ﬁeld-of-view in order to capture multi-scale information. In the case of two-dimensional signals, for each location i on the output feature map y and a convolution ﬁlter w, atrous convolution is applied over the input feature map x as follows:where the atrous rate r determines the stride with which we sample the input signal. Note that standard convolution is a special case in which rate r = 1. The ﬁlter’s ﬁeld-of-view is adaptively modiﬁed by changing the dilation/atrous rate value.Depth-wise Separable ConvolutionDepth-wise separable convolution drastically reduces computation complexity by dividing a standard convolution into two sub-parts — i. Depth-wise convolutionii. Point-wise convolution.The first part is depth-wise convolution that performs a spatial convolution independently for each input channel. It is followed by a point-wise convolution (i.e., 1×1 convolution), which is employed to combine the output from the depth-wise convolution.Let us understand this with the help of an example. Suppose we have an image of size 12×12 composed of 3 channels. We want to apply a convolution of 5×5 on this input and get an output of 8×8×256.In the first part, depth-wise convolution, we give the input image a convolution without changing the depth. We do so by using 3 kernels of shape 5×5×1.The point-wise convolution is so named because it uses a 1×1 kernel or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; 3 in our case. Therefore, we iterate a 1×1×3 kernel through our 8×8×3 image, to get an 8×8×1 image.To get 8×8×256 output image, we need to simply increase the number of 1×1×3 kernels to 256.Encoder ArchitectureDeepLab V3+ encoder uses Xception architecture with the following modifications —The output of the encoder is a feature map of size 16 times smaller than the input feature map. This is compensated by the decoder which has a provision to up-sample the encoder feature map by 4x twice (refer to the model architecture diagram).5.2.2. TrainingI have trained the model using Keras Adam optimizer with the default learning rate for 47 epochs. The loss function that the optimizer tries to minimize is bce_dice_loss, defined earlier in section 4.4As in the case of U-Net, I have saved the weights of the model with the best validation dice_coefficient.5.2.3. Training plots5.2.4. TestingThe figure below shows some sample images from validation data alongside their ground truth mask and predicted mask.Rebuilding the model with original input size(256, 1600,3) and loading the weights of the model trained on half size did not work well in this case. I had to use a different strategy —I used the trained model to predict on 128×800 images and resized the predicted masks to 256×1600. This approach worked very well for DeepLab V3+.My final submission was DeepLab V3+, which gave a decent score in both Private and Public.This Kaggle competition was a popular one and many people have solved this problem using different approaches. However, most of them have used some or the other variant of U-Net or similar encoder-decoder architectures.I have used simple U-Net as my first cut solution, which gives a decent performance on test data, thanks to the train on half-size and predict on full-size strategy.I have implemented DeepLab V3+, which is a state of the art technique for semantic image segmentation from scratch. It helped me improve my score from 0.809 to 0.838.Thank you for reading such a long blog, I appreciate your patience. I thoroughly enjoyed writing it, hope you enjoyed reading it too.I have skipped most of the code as I didn’t want to overwhelm the readers with code. Please refer to my GitHub repository for the complete Keras code.If you have any queries, suggestions or discussions, please feel free to drop them in the comments section below. I will try to address them to the best of my knowledge.You can connect with me on LinkedIn, here’s my profile.",10/05/2020,6,67.0,21.0,814.0,387.0,29.0,14.0,0.0,34.0,en
4070,Building a Summarization System in Minutes,Veritable,Ceshine Lee,1500.0,6.0,735.0,"(This is sort of a sequel or an update to “Building a Translation System in Minutes” published a year ago. This time we use a publicly available dataset, a different NLP task, and some task-specific evaluation metrics)Summarization is the task of producing a shorter version of one or several documents that preserves most of the input’s meaning. [1]The text summarization task is mostly solved using variants of the seq2seq structure [2] these days. The seq2seq structure is much more complicated than the usual RNN models, and that makes implementing the model from scratch a rather daunting task. Luckily, OpenNMT project [3] provides ready-to-use implementations of seq2seq models that are close to state-of-the-art. We can use it as a starting point.In this post we are using OpenNMT-py, a Pytorch port of OpenNMT, to train a baseline model on the Gigaword summarization dataset. The official tutorial is somewhat out-dated and did not work right out-of-the-box. Here we provide an up-to-date tutorial that you can follow step-by-step.PyTorch 0.4.1:OpenNMT-py:files2rouge (this is needed to calculate the evaluation metrics):Follow the link here (harvardnlp/sent-summary) to download the archived dataset. Extract the content and put them in data/gigaword subfolder in your project directory. You should have these two folders in data/gigaword: train(train and validation sets) and Giga(test set).At this point you should have two folders in your project directory: OpenNMT-py and data.OpenNMT-py recently added a feature that prevents the dataset to have any special tokens. Unfortunately, the Gigaword dataset does contains a special token, <unk>, inside the train and validation sets, and it also expects models to output <unk> in the predicted summaries. Moreover, <unk> is inconsistent with what is used in the test set — UNK.One simple solution is to replace <unk> with UNK. Run these commands in your bash prompt:Test dataset contains some lines that have only one token — UNK. They can actually break the inference process of OpenNMT-py. It makes little sense to keep them.Run the scripts below to remove those lines:We need to let OpenNMT-py scan the raw texts, build a vocabulary, tokenize and truncate the texts if necessary, and finally save the results to the disk.Pick a shard_size that works with your local memory. That mainly affects the training process. I’ve found that it does not affect the preprocessing process, as larger datasets can quickly make the system out of memory regardless of the value of shared_size.This command trains a model similar to the Pointer-Generator Networks [4]:I’m almost certain that the decay rate (decay_steps) I set was too high. I stopped the training after just 2 epochs (~ 480000 steps) because of it. You can try to significantly increase the value of decay_steps.Please refer to the official documentation for the meaning of each arguments.The training took about 12 hours with a single GTX 1070.The model used here:Most of the arguments are directly copied from the official tutorial (targeted at the CNNDM dataset).Please refer to the official documentation for the meaning of each arguments.An example result:Document: the four candidates in algeria ‘s first free presidential election held final rallies monday amid tight security as some voters began casting their ballots three days ahead of the main poll .Predicted Summary: algeria holds final rallies amid tight securityReference Summary: algerian presidential candidates wind up campaign by richard palmer(Note here the reference summary mentioned a name “richard palmer” that is impossible to get from the source document.)Outputs:So we achieved Rouge-1: 0.32755, Rouge-2: 0.15004, Rouge-L: 0.30714 after just two epochs. However, they are significantly lower than those from the official pre-trained models (the best one has Rouge-1: 0.3551, Rouge-2: 0.1735, Rouge-L: 0.3317).We can further tune the hyper-parameters to improve the final evaluation scores. And we can also try the new transformer models (I’ll add a working training command here later if time permitted).What I’m really interested to do is some transfer learning with the pre-trained models. The main obstacle is adapting the preprocessing used by OpenNMT to work on a new dataset.Customizing the model structure is also another natural next step. However, we need to understand how the original model works first. The next post will probably be some analysis of the source code, along with some brief introduction of the related research papers.In the end I might build a system from scratch, but with a lot of components coming from OpenNMT-py. The plan is to incorporate the code into my Modern Chinese NLP project so it’ll support seq2seq applications as well.",01/11/2018,7,7.0,13.0,1400.0,978.0,1.0,4.0,0.0,18.0,en
4071,What are adversarial examples in NLP?,Towards Data Science,Jack Morris,21.0,7.0,1286.0,"This article talks about the concept of adversarial examples as applied to NLP (natural language processing). The terminology can be confusing at times, so we’ll begin with an overview of the language used to talk about adversarial examples and adversarial attacks. Then, we’ll talk about TextAttack, our open-source Python library for adversarial examples, data augmentation, and adversarial training in NLP that’s changing the way people research the robustness of NLP models. We’ll conclude with some thoughts on the future of this area of research.An adversarial example is an input designed to fool a machine learning model [1]. An adversarial example crafted as a change to a benign input is known as an adversarial perturbation. ‘Adversarial perturbation’ is more specific than just ‘adversarial example’, as the class of all adversarial examples also includes inputs designed from scratch to fool machine learning models. TextAttack attacks generate a specific kind of adversarial examples, adversarial perturbations.An adversarial attack on a machine learning model is a process for generating adversarial perturbations. TextAttack attacks iterate through a dataset (list of inputs to a model), and for each correctly predicted sample, search for an adversarial perturbation. If an example is incorrectly predicted to begin with, it is not attacked, since the input already fools the model. TextAttack breaks the attack process up into stages and provides a system of interchangeable components for managing each stage of the attack.Adversarial robustness is a measurement of a model’s susceptibility to adversarial examples. TextAttack often measures robustness using attack success rate, the percentage of attack attempts that produce successful adversarial examples, or after-attack accuracy, the percentage of inputs that are both correctly classified and unsuccessfully attacked.To improve our numeracy for talking about adversarial attacks, let’s take a look at a concrete example:This attack was run on 200 examples. Out of those 200, the model initially predicted 43 of them incorrectly; this leads to an accuracy of 157/200 or 78.5%. TextAttack ran the adversarial attack process on the remaining 157 examples to try to find a valid adversarial perturbation for each one. Out of those 157, 29 attacks failed, leading to a success rate of 128/157 or 81.5%. Another way to articulate this is that the model correctly predicted the original sample and then resisted adversarial attack for 29 out of 200 total samples, leading to an accuracy under attack (or “after-attack accuracy”) of 29/200 or 14.5%.TextAttack also logged some other helpful statistics for this attack. Among the 157 successful attacks, on average, the attack changed 15.5% of words to alter the prediction, and made 32.7 queries to find a successful perturbation. Across all 200 inputs, the average number of words was 18.97.Now that we have provided some terminology, let’s look at some concrete examples of proposed adversarial attacks. We will give some background on adversarial attacks in other domains and then examples of different attacks in NLP.Research in 2013 [2] showed neural networks are vulnerable to adversarial examples. These original adversarial attacks apply a small, well-chosen perturbation to an image to fool an image classifier. In this example, the classifier correctly predicts the original image to be a pig. After a small perturbation, however, the classifier predicts the pig to be an airliner (with extremely high confidence!).These adversarial examples exhibit a serious security flaw in deep neural networks. Therefore, adversarial examples pose a security problem for all downstream systems that include neural networks, including text-to-speech systems and self-driving cars. Adversarial examples are useful outside of security: researchers have used adversarial examples to improve and interpret deep learning models.As you might imagine, adversarial examples in deep neural networks have caught the attention of many researchers around the world. Their discovery in 2013 spawned an explosion of research into the topic.Many new, more sophisticated adversarial attacks have been proposed, along with defenses, procedures for training neural networks that are resistant (robust) against adversarial attacks. Training deep neural networks that are highly accurate while remaining robust to adversarial attacks remains an open problem [3].Naturally, many have wondered about what adversarial examples for NLP models might be. No natural analogy to the adversarial examples in computer vision (such as the pig-to-airliner bamboozle above) exists for NLP. In the above example, the pig-classified input and its airliner-classified perturbation are literally indistinguishable to the human eye. Unlike images, two sequences of text cannot be truly indistinguishable without being the same.Because two text sequences are never indistinguishable, researchers have proposed various alternative definitions for adversarial examples in NLP. We find it useful to group adversarial attacks based on their chosen definitions of adversarial examples.Although attacks in NLP cannot find an adversarial perturbation that is literally indistinguishable to the original input, they can find a perturbation that is very similar. Our mental model groups NLP adversarial attacks into two groups, based on their notions of ‘similarity’:Some researchers have raised concern that these attacks can be defended against quite effectively, either by using a rule-based spellchecker or a sequence-to-sequence model trained to correct adversarial typos.TextAttack attack recipes that fall under this category: deepwordbug, hotflip, pruthi, textbugger*, morpheusSome NLP models are trained to measure semantic similarity. Adversarial attacks based on the notion of semantic similarity typically use another NLP model to enforce that perturbations are grammatically valid and semantically similar to the original input.TextAttack attack recipes that fall under this category: alzantot, bae, bert-attack, faster-alzantot, iga, kuleshov, pso, pwws, textbugger*, textfooler*The TextBugger attack generates perturbations using both typo-like character edits and synonym substitutions. It could be considered to use both definitions of indistinguishability.TextAttack supports adversarial attacks based in both definitions of indistinguishability. Both types of attacks are useful for training more robust NLP models. Our goal is to enable research into adversarial examples in NLP by providing a set of intuitive, reusable components for building as many attacks from the literature as possible.We define the adversarial attack processing using four components: a goal function, constraints, transformation, and search method. (We’ll go into this in detail in a future post!) These components allow us to reuse many things between attacks from different research papers. They also make it easy to develop methods for NLP data augmentation.TextAttack also includes code for loading popular NLP datasets and training models on them. By integrating this training code with adversarial attacks and data augmentation techniques, TextAttack provides an environment for researchers to test adversarial training in many different scenarios.The following figure shows an overview of the main functionality of TextAttack:We are excited to see the impact that TextAttack has on the NLP research community! One thing we would like to see research in is the combination of components from various papers. TextAttack makes it easy to run ablation studies to compare the effects of swapping out, say, search method from paper A with the search method from paper B, without making any other changes. (And these tests can be run across dozens of pre-trained models and datasets with no downloads!)We hope that use of TextAttack leads to more diversity in adversarial attacks. One thing that all current adversarial attacks have in common is that they make substitutions on the word or character level. We hope that future adversarial attacks in NLP can broaden scope to try different approaches to phrase-level replacements as well as full-sentence paraphrases. Additionally, there has been a focus on English in the adversarial attack literature; we look forward to seeing adversarial attacks applied to more languages.If you are interested in TextAttack, or the broader problem of generating adversarial examples for NLP models, please get in touch! You might have a look at our paper on ArXiv or our repository on Github.[1] “Attacking Machine Learning with Adversarial Examples”, Goodfellow, 2013. [https://openai.com/blog/adversarial-example-research/][2] “Intriguing properties of neural networks”, Szegedy, 2013. [https://arxiv.org/abs/1312.6199][3] “Robustness May Be at Odds with Accuracy”, Tsipras, 2018. [https://arxiv.org/abs/1805.12152]",28/08/2020,0,10.0,8.0,821.0,401.0,6.0,2.0,0.0,12.0,en
4072,Reducing Memory Usage in R (especially for regressions),Medium,William Ryan,80.0,5.0,583.0,"R uses a ton of memory. Here are ways to make it use a little less. Definitely not an expert, this is largely a resource/reference for myself, but thought it might be useful for others as well.The best introduction to how R uses memory is likely this guide, by Hadley Wickham.Garbage collector: gc()My impression is that this function used to be more useful. R uses it to release memory it isn’t using, but will usually run it automatically. So you shouldn’t have to call it explicitly. However, if you want to see when this is happening, use gcinfo(TRUE) — you probably won’t want to leave this on all the time, it will get annoying. But, it can be very useful for finding the peak memory used by a function.Object size: object.size()To find the size of a given R object, use this function. There are many ways to apply this, which we will get into in other sections.One example of a usage is checking to see how much space each term in a regression fit takes up:List all objects in your environment in order of memory used: lsos()You’ll need to make this function yourself. It uses existing R functions to list out all of the objects in your environment in order of size. It comes from this StackExchange discussion.Strip GLMThis function gets rid of every part of a glm fit which isn’t used for making predictions. Helpful if you are only using a model for predictions and nothing else. This will hugely decrease the memory used by saving your fits, since ordinarily the fit saves all the data used to train the regression, as well as residuals etc. You will have to specify the newdata term in your predict function. Hat tip this post. Below is the function:Here is an example of how to use the function:For data manipulation and aggregationData.table is going to be less memory intensive and faster for many data manipulation and aggregation tasks. It can add, modify, group, and delete columns or subsets by reference using the := operator, which is much less memory intensive. You can see a short discussion of this operator here. There are a number of other operations for which it is also less memory intensive — some are discussed here.For running regressionsThere are two packages which both make regressions much less memory intensive than usual. They are speedglm and biglm. Each package essentially replaces the glm() function with their own version which will be faster and require less memory. If you use these functions to begin with, the stripglm() function above probably won’t be necessary.An example of the speed increases can be seen here, using speedglm():As you can see above, this is a big time savings. It also uses a bit less memory when making its fits — you can verify this with gcinfo(TRUE).For checking memory usageThe package pryr() will let you get information on memory usage easily. Great functions include mem_used(), which reports the overall memory you are using, and mem_change(), which will report the memory changed after the code you’ve wrapped it in is run.For checking peak memory usage when running a block of code, you will want to use the lineprof package, thought it’s time-consuming enough that it probably will not be worth running all the time.For checking run timesA useful package for checking run times is microbenchmark. It will run two commands against one another a number of times and compare their speed. You can find an explanation of it here.",28/06/2016,7,8.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,en
4073,7 Applications of Auto-Encoders every Data Scientist should know,Towards Data Science,Satyam Kumar,2500.0,5.0,826.0,"Auto-Encoders are a popular type of unsupervised artificial neural network that takes un-labeled data and learns efficient codings about the structure of the data that can be used for another context. Auto-Encoders approximates the function that maps the data from full input space to lower dimension coordinates and further approximates to the same dimension of input space with minimum loss.For classification or regression tasks, auto-encoders can be used to extract features from the raw data to improve the robustness of the model. There are various other applications of an Auto-Encoder network, that can be used for some other context. We will 7 of such applications of auto-encoder in this article:Before diving into the applications of AutoEncoders, let's discuss briefly what exactly is Auto-Encoder network is.Autoencoder is an unsupervised neural network that tries to reconstruct the output layer as similar as the input layer. An autoencoder architecture has two parts:The autoencoder first compresses the input vector into lower dimensional space then tries to reconstruct the output by minimizing the reconstruction error. The autoencoder tries to reconstruct the output vector as similar as possible to the input layer.There are various types of autoencoders including regularized, concrete, and variational autoencoders. Refer to the Wikipedia page for autoencoders to know more about the variations of autoencoders in detail.Autoencoders train the network to explain the natural structure in the data into efficient lower-dimensional representation. It does this by using decoding and encoding strategy to minimize the reconstruction error.The input and the output dimension have 3000 dimensions, and the desired reduced dimension is 200. We can develop a 5-layer network where the encoder has 3000 and 1500 neurons a similar to the decoder network.The vector embeddings of the compressed input layer can be considered as a reduced dimensional embedding of the input layer.Autoencoders can be used as a feature extractor for classification or regression tasks. Autoencoders take un-labeled data and learn efficient codings about the structure of the data that can be used for supervised learning tasks.After training an autoencoder network using a sample of training data, we can ignore the decoder part of the autoencoder, and only use the encoder to convert raw input data of higher dimension to a lower dimension encoded space. This lower dimension of data can be used as a feature for supervised tasks.Follow my another article to get a step-by-step implementation of autoencoder as a feature extractor:towardsdatascience.comThe real-world raw input data is often noisy in nature, and to train a robust supervised model requires cleaned and noiseless data. Autoencoders can be used to denoise the data.Image denoising is one of the popular applications where the autoencoders try to reconstruct the noiseless image from a noisy input image.The noisy input image is fed into the autoencoder as input and the output noiseless output is reconstructed by minimizing the reconstruction loss from the original target output (noiseless). Once the autoencoder weights are trained, they can be further used to denoise the raw image.Image compression is another application of an autoencoder network. The raw input image can be passed to the encoder network and obtained a compressed dimension of encoded data. The autoencoder network weights can be learned by reconstructing the image from the compressed encoding using a decoder network.Usually, autoencoders are not that good for data compression, rather basic compression algorithms work better.Autoencoders can be used to compress the database of images. The compressed embedding can be compared or searched with an encoded version of the search image.Anomaly detection is another useful application of an autoencoder network. An anomaly detection model can be used to detect a fraudulent transaction or any highly imbalanced supervised tasks.The idea is to train autoencoders on only sample data of one class (majority class). This way the network is capable of re-constructing the input with good or less reconstruction loss. Now, if a sample data of another target class is passed through the autoencoder network, it results in comparatively larger reconstruction loss.A threshold value of reconstruction loss (anomaly score) can be decided, larger than that can be considered an anomaly.Denoising autoencoders can be used to impute the missing values in the dataset. The idea is to train an autoencoder network by randomly placing missing values in the input data and trying to reconstruct the original raw data by minimizing the reconstruction loss.Once the autoencoder weights are trained the records having missing values can be passed through the autoencoder network to reconstruct the input data, that too with imputed missing features.In this article, we have discussed a brief overview of various applications of an autoencoder. For image reconstruction, we can use a variation of autoencoder called convolutional autoencoder that minimizes the reconstruction errors by learning the optimal filters.In my upcoming articles, I will implement each of the above-discussed applications.Loved the article? Become a Medium member to continue learning without limits. I’ll receive a small portion of your membership fee if you use the following link, with no extra cost to you.satyam-kumar.medium.comThank You for Reading",20/12/2021,1,4.0,4.0,1002.0,470.0,8.0,1.0,0.0,9.0,en
4074,Seq2Seq model in TensorFlow,Towards Data Science,Park Chansung,667.0,9.0,1140.0,"In this project, I am going to build language translation model called seq2seq model or encoder-decoder model in TensorFlow. The objective of the model is translating English sentences to French sentences. I am going to show the detailed steps, and they will answer to the questions likehow to define encoder model, how to define decoder model, how to build the entire seq2seq model, how to calculate the loss and clip gradients.Please visit the Github repo for more detailed information and actual codes in Jupyter notebook. It will cover a bit more topics like how to preprocess the dataset, how to define inputs, and how to train and get prediction.This is a part of Udacity’s Deep Learning Nanodegree. Some codes/functions (save, load, measuring accuracy, etc) are provided by Udacity. However, majority part is implemented by myself along with much richer explanations and references on each section. Also, base figures (about model) is borrowed from Luong (2016).You can separate the entire model into 2 small sub-models. The first sub-model is called as [E] Encoder, and the second sub-model is called as [D] Decoder. [E] takes a raw input text data just like any other RNN architectures do. At the end, [E] outputs a neural representation. This is a very typical work, but you need to pay attention what this output really is. The output of [E] is going to be the input data for [D].That is why we call [E] as Encoder and [D] as Decoder. [E] makes an output encoded in neural representational form, and we don’t know what it really is. It is somewhat encrypted. [D] has the ability to look inside the [E]’s output, and it will create a totally different output data (translated in French in this case).In order to build such a model, there are 6 steps overall. I noted what functions to be implemented are related to each steps.(1) define input parameters to the encoder model(2) build encoder model(3) define input parameters to the decoder model(4) build decoder model for training(5) build decoder model for inference(6) put (4) and (5) together(7) connect encoder and decoder models(8) define loss function, optimizer, and apply gradient clippingenc_dec_model_inputs function creates and returns parameters (TF placeholders) related to building model.inputs placeholder will be fed with English sentence data, and its shape is [None, None]. The first None means the batch size, and the batch size is unknown since user can set it. The second None means the lengths of sentences. The maximum length of setence is different from batch to batch, so it cannot be set with the exact number.targets placeholder is similar to inputs placeholder except that it will be fed with French sentence data.target_sequence_length placeholder represents the lengths of each sentences, so the shape is None, a column tensor, which is the same number to the batch size. This particular value is required as an argument of TrainerHelper to build decoder model for training. We will see in (4).max_target_len gets the maximum value out of lengths of all the target sentences(sequences). As you know, we have the lengths of all the sentences in target_sequence_length parameter. The way to get the maximum value from it is to use tf.reduce_max.On the decoder side, we need two different kinds of input for training and inference purposes repectively. While training phase, the input is provided as target label, but they still need to be embeded. On the inference phase, however, the output of each time step will be the input for the next time step. They also need to be embeded and embedding vector should be shared between two different phases.In this section, I am going to preprocess the target label data for the training phase. It is nothing special task. What all you need to do is add <GO> special token in front of all target data. <GO> token is a kind of guide token as saying like ""this is the start of the translation"". For this process, you need to know three libraries from TensorFlow.TF strided_sliceTF fillTF concatAfter preprocessing the target label data, we will embed it later when implementing decoding_layer function.As depicted in Fig 3, the encoding model consists of two different parts. The first part is the embedding layer. Each word in a sentence will be represented with the number of features specified as encoding_embedding_size. This layer gives much richer representative power for the words useful explanation. The second part is the RNN layer(s). You can make use of any kind of RNN related techniques or algorithms. For example, in this project, multiple LSTM cells are stacked together after dropout technique is applied. You can use different kinds of RNN cells such as GRU.Embedding layerRNN layersEncoding modelDecoding model can be thought of two separate processes, training and inference. It is not they have different architecture, but they share the same architecture and its parameters. It is that they have different strategy to feed the shared model. For this(training) and the next(inference) section, Fig 4 shows clearly shows what they are.While encoder uses TF contrib.layers.embed_sequence, it is not applicable to decoder even though it may require its input embeded. That is because the same embedding vector should be shared via training and inferece phases. TF contrib.layers.embed_sequence can only embed the prepared dataset before running. What needed for inference process is dynamic embedding capability. It is impossible to embed the output from the inference process before running the model because the output of the current time step will be the input of the next time step.How we can embed? We will see soon. However, for now, what you need to remember is training and inference processes share the same embedding parameters. For the training part, embeded input should be delivered. On the inference part, only embedding parameters used in the training part should be delivered.Let’s see the training part first.Embed the target sequencesConstruct the decoder RNN layer(s)Create an output layer to map the outputs of the decoder to the elements of our vocabularyIn this section, previously defined functions, encoding_layer, process_decoder_input, and decoding_layer are put together to build the big picture, Sequence to Sequence model.seq2seq_model function creates the model. It defines how the feedforward and backpropagation should flow. The last step for this model to be trainable is deciding and applying what optimization algorithms to use. In this section, TF contrib.seq2seq.sequence_loss is used to calculate the loss, then TF train.AdamOptimizer is applied to calculate the gradient descent on the loss. Let's go over eatch steps in the code cell below.load data from the checkpointcreate inputsbuild seq2seq modelcost functionOptimizerGradient ClippingMy background in deep learning is Udacity {Deep Learning ND & AI-ND with contentrations(CV, NLP, VUI)}, Coursera Deeplearning.ai Specialization (AI-ND has been split into 4 different parts, which I have finished all together with the previous version of ND). Also, I am currently taking Udacity Data Analyst ND, and I am 80% done currently.",02/05/2018,0,57.0,4.0,1253.0,794.0,6.0,25.0,0.0,46.0,en
4075,Understanding 1D and 3D Convolution Neural Network | Keras,Towards Data Science,Shiva Verma,954.0,5.0,674.0,"When we say Convolution Neural Network (CNN), generally we refer to a 2 dimensional CNN which is used for image classification. But there are two other types of Convolution Neural Networks used in the real world, which are 1 dimensional and 3-dimensional CNNs. In this guide, we are going to cover 1D and 3D CNNs and their applications in the real world. I am assuming you are already familiar with the concept of Convolutions Networks in general.This is the standard Convolution Neural Network which was first introduced in Lenet-5 architecture. Conv2D is generally used on Image data. It is called 2 dimensional CNN because the kernel slides along 2 dimensions on the data as shown in the following image.The whole advantage of using CNN is that it can extract the spatial features from the data using its kernel, which other networks are unable to do. For example, CNN can detect edges, distribution of colours etc in the image which makes these networks very robust in image classification and other similar data which contain spatial properties.Following is the code to add a Conv2D layer in keras.Argument input_shape (128, 128, 3) represents (height, width, depth) of the image. Argument kernel_size (3, 3) represents (height, width) of the kernel, and kernel depth will be the same as the depth of the image.Before going through Conv1D, let me give you a hint. In Conv1D, kernel slides along one dimension. Now let’s pause the blog here and think which type of data requires kernel sliding in only one dimension and have spatial properties?The answer is Time-Series data. Let’s look at the following data.This data is collected from an accelerometer which a person is wearing on his arm. Data represent the acceleration in all the 3 axes. 1D CNN can perform activity recognition task from accelerometer data, such as if the person is standing, walking, jumping etc. This data has 2 dimensions. The first dimension is time-steps and other is the values of the acceleration in 3 axes.Following plot illustrate how the kernel will move on accelerometer data. Each row represents time series acceleration for some axis. The kernel can only move in one dimension along the axis of time.Following is the code to add a Conv1D layer in keras.Argument input_shape (120, 3), represents 120 time-steps with 3 data points in each time step. These 3 data points are acceleration for x, y and z axes. Argument kernel_size is 5, representing the width of the kernel, and kernel height will be the same as the number of data points in each time step.Similarly, 1D CNNs are also used on audio and text data since we can also represent the sound and texts as a time series data. Please refer to the images below.Conv1D is widely applied on sensory data, and accelerometer data is one of it.In Conv3D, the kernel slides in 3 dimensions as shown below. Let’s think again which data type requires the kernel moving across the 3 dimension?Conv3D is mostly used with 3D image data. Such as Magnetic Resonance Imaging (MRI) data. MRI data is widely used for examining the brain, spinal cords, internal organs and many more. A Computerized Tomography (CT) Scan is also an example of 3D data, which is created by combining a series of X-rays image taken from different angles around the body. We can use Conv3D to classify this medical data or extract features from it.One more example of 3D data is Video. Video is nothing but a sequence of image frames together. We can apply Conv3D on video as well since it has spatial features.Following is the code to add the Conv3D layer in keras.Here argument Input_shape (128, 128, 128, 3) has 4 dimensions. A 3D image is a 4-dimensional data where the fourth dimension represents the number of colour channels. Just like a flat 2D image has 3 dimensions, where the 3rd dimension represents colour channels. Argument kernel_size (3,3,3) represents (height, width, depth) of the kernel, and 4th dimension of the kernel will be the same as the colour channel.Up Nextshiva-verma.medium.com",20/09/2019,0,39.0,0.0,1093.0,629.0,7.0,1.0,0.0,2.0,en
4076,When Neural Networks saw the first image of Black Hole.,Analytics Vidhya,Anuj shah (Exploring Neurons),156.0,6.0,917.0,"On April 10th, scientists and engineers from Event Horizon Telescope team achieved a remarkable breakthrough in quest to understand the cosmos by unveiling the first image of black hole. This furthers strengthens Einstein theory of general relativity — “ massive objects cause a distortion in space-time, which is felt as gravity”.Well I am not a physicist or astronomer to comprehend and explain in detail about this but like me there are millions and millions of people who despite being in different fields are fascinated by cosmos and specially black hole. The first image of black hole has send wave of excitement all over the world. I am a Deep learning engineer who mainly works with convolution neural network and I wanted to see what AI algorithms thinks about the black hole picture and this blog is about that.This excerpt from The Epoch Times describe black hole — The Black Holes are made up of “a great amount of matter packed into a very small area,” mostly formed from “the remnants of a large star that dies in a supernova explosion.” They have so strong gravitational fields that even light can’t escape it. The pictured M87 Black Hole is shown below. The black picture is very well explained in the blog by vox-How to make sense of the black hole image, according to 2 astrophysicists.Kindly visit this blog post which shows cool animation explaining why black hole picture looks like — How to make sense of the black hole.CNN — Convolution Neural Network are class of deep learning algorithms which are quite efficient in recognizing real world objects. CNNs are the best neural nets for interpreting and understanding images. This networks are trained on million of images and they have learned to recognize nearly 1000 different kind of real world objects. I thought of showing the black hole image to two of such trained CNNs and see what they interpret from the image, to which real world object the picture of black hole resembles according to the network. This is not a smart idea as black hole image was generated after interpreting and integrating lot of different signal from space, but I just wanted to see how is the interpretation with just the picture , without any other signal information.As we can see from above picture that pre-trained vgg16 and vgg19 predicts the black hole image as match stick and ResNet50 thinks its a candle. If we make some analogy we can see that it make some sense as both the burning matchstick and candle has a dark center surrounded by a strong bright yellow light.2. What Features CNN learns from the black hole imageAnother thing I did was to visualize what the intermediate layers of the VGG16 was generating. Deep learning networks are called deep because they have number of layers and each layer learns some representation and features of the input image. So let’s see what different layers of the network learns from the input image. The results are quite beautiful.If you look closely you can see that the lower bright region of the black hole is a strong feature and is being learned by many of the filters. some of the interesting filter output are shown below and they already look like some celestial object.Lets zoom in to some of the interesting feature maps of second convolution layer.Now lets go deeper and have a look at the third convolution layerZooming in we kinda see the similar kind of patternFurther going deeper, we get something like thisAs we go deeper we get higher level abstract information and when we visualize the 7th, 8th and 10th convolution layer we will see only high level information.As we can see many of the feature maps are dark and are only learning specific high level features required for recognizing that class. This becomes more prominent in further deeper layers. For now lets zoom in and see some of the filters.Now lets see the 512 feature maps of the 10th convolution layerNow you can clearly see that in most of the output feature map only a region of the image is being learnt as feature. Those are high level features seen by the neurons. Lets see some of the above feature maps in a little large sizeNow that we saw what CNN is trying to learn from the black hole image, lets try passing this image to some other popular neural network algorithms like Neural Style Transfer and DeepDream3. Trying out Neural Style Transfer and Deep Dream on the black hole imageNeural style transfer are smart networks which transfer the style of a style image to a source image and generates an artistic image out of it. If it doesn’t make sense, the results below will totally elucidate the concept. I used the website deepdreamgenerator.com to generate different kind of artistic image out of the original black hole image. The pictures are quite alluring.DeepDream as mentioned in wikipedia -DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.This videos showing deep dream has quite a hallucinating effect — Journey on the Deep DreamWell this is it for now, I was quite excited seeing the first picture of black hole and hence this blog post. It may not be that useful but the pictures generated above are totally worth it. Enjoy the pictures!!",19/04/2019,0,3.0,0.0,981.0,575.0,19.0,1.0,0.0,6.0,en
4077,Weekend Diversion: The Ultimate Superhero Cake,Starts With A Bang!,Ethan Siegel,134000.0,5.0,709.0,"Thanks to 3D printing, creativity and a lot of effort, this DIY Optimus Prime cake is unlike any other.“When he came home, I could see a change. He was quieter and he was a man and a hero to me. I watched him and listened to him. I’d never had an opportunity to do a superhero, and when that came, [that voice] just came right out of me and I sounded like Optimus.” -Peter Cullen, on his brotherBeing a hero is something we all dream about in our own way. On our birthdays, everyone deserves to live out that fantasy, if only for a day. Have a listen to Tracy Chapman’s reflective and provocative song, Change,while you consider the ultimate in “changing” superheros: Optimus Prime.Unlike the flashy Decepticons, who transformed from robots into fighter jets, stereos or even guns, the Autobots were all cars and trucks, led by Optimus Prime: a semi truck. The key to what made transformers awesome, of course, was the ability to go from a vehicle to a robot and back again, something that easily captured a child’s imagination and — of course — led to tremendous toy sales.What kid wouldn’t want to be an Optimus Prime-caliber superhero, if not Optimus Prime himself? Well, lucky for Russell Munro’s six-year-old son, he was just the person to make this dream come true, in cake form.This idea came simply from my eldest son asking for a Transformers cake for his birthday, my wife makes our two boys cakes each year. My inner child immediately pictured a cake that actually transformed and that was it, I was hooked. […] I wanted it to be like a magic trick and fool the viewer (mostly 6 year old children) into thinking the cake was standing up by itself. So that was the challenge, make Optimus Prime come to life in an invisible way.And so he went to work on exactly that.He built a multi-stage design, with:You’ll note that this is just a “skeleton” of a cake, because the goal was to have this not be a mechanical device with icing or fondant atop it, but a real, genuine, edible and delicious cake! So he made a mere skeleton out of 3D printable parts (using PLA, which is food-safe) that was capable of holding the weight of the custom-made cake components that his wife was baking.And with a combination of 3D printing, steel fishing wire, elastic bands and motors, the parts started to come together.Underneath the main platform, of course, were the electronics and motors necessary for operating the cake itself, and enabling the “transformation” to take place.And when the (very well done!) cake pieces were complete, onto the individual platform pieces they went.In “beta testing,” everything looked fantastic. But remember, this is how the device operates before any cake is added. And cake — as anyone with a sweet tooth (like me) can attest — gets heavy fast.This almost led to disaster, as Russell was quick to attest:The cake is where all very nearly came unstuck. I drastically under calculated (totally stuffed up actually) how much mud cake weighs. So the cab had some Styrofoam in it to help lighten the load a little.The cake was chocolate mud cake covered Bakels Pettinice (Fondant) rolled as thinly as possible to reduce weight. Rolkem Super silver was used where a metal look was needed.But the birthday party came around, and it was time for the ultimate moment of truth. You can watch the transformer struggle to make it… but (spoiler) it makes it!This was the coolest thing I’ve seen in quite some time, and I’m so happy to get to share it with you. For all you Optimus Prime fans out there, remember what he says:There’s a thin line between being a hero and being a memory.Although Optimus Prime the cake is just a memory, Optimus Prime the idea is still a hero. Thanks to Laughing Squid and PSFK for bringing this to my attention, and to Russell Munro himself for documenting in such great detail how this cake came to life. Hope you enjoyed it!Missed the best of our comments of the past week? Check them out here!Leave your comments at our forum, and support Starts With A Bang on Patreon!",30/08/2015,0,1.0,13.0,797.0,1068.0,12.0,1.0,0.0,16.0,en
4078,Deep Generative Models,Towards Data Science,Prakash Pandey,290.0,12.0,2544.0,"A Generative Model is a powerful way of learning any kind of data distribution using unsupervised learning and it has achieved tremendous success in just few years. All types of generative models aim at learning the true data distribution of the training set so as to generate new data points with some variations. But it is not always possible to learn the exact distribution of our data either implicitly or explicitly and so we try to model a distribution which is as similar as possible to the true data distribution. For this, we can leverage the power of neural networks to learn a function which can approximate the model distribution to the true distribution.Two of the most commonly used and efficient approaches are Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN). VAE aims at maximizing the lower bound of the data log-likelihood and GAN aims at achieving an equilibrium between Generator and Discriminator. In this blogpost, I will be explaining the working of VAE and GANs and the intuition behind them.Variational AutoencoderI am assuming that the reader is already familiar with the working of a vanilla autoencoder. We know that we can use an autoencoder to encode an input image to a much smaller dimensional representation which can store latent information about the input data distribution. But in a vanilla autoencoder, the encoded vector can only be mapped to the corresponding input using a decoder. It certainly can’t be used to generate similar images with some variability.To achieve this, the model needs to learn the probability distribution of the training data. VAE is one of the most popular approach to learn the complicated data distribution such as images using neural networks in an unsupervised fashion. It is a probabilistic graphical model rooted in Bayesian inference i.e., the model aims to learn the underlying probability distribution of the training data so that it could easily sample new data from that learned distribution. The idea is to learn a low-dimensional latent representation of the training data called latent variables (variables which are not directly observed but are rather inferred through a mathematical model) which we assume to have generated our actual training data. These latent variables can store useful information about the type of output the model needs to generate. The probability distribution of latent variables z is denoted by P(z). A Gaussian distribution is selected as a prior to learn the distribution P(z) so as to easily sample new data points during inference time.Now the primary objective is to model the data with some parameters which maximizes the likelihood of training data X. In short, we are assuming that a low-dimensional latent vector has generated our data x (x ∈ X) and we can map this latent vector to data x using a deterministic function f(z;θ) parameterized by theta which we need to evaluate (see fig. 1[1]). Under this generative process, our aim is to maximize the probability of each data in X which is given as,Pө(X) = ∫Pө(X, z)dz = ∫Pө(X|z)Pө(z)dz (1)Here, f(z;θ)has been replaced by a distribution Pө(X|z).The intuition behind this maximum likelihood estimation is that if the model can generate training samples from these latent variables then it can also generate similar samples with some variations. In other words, if we sample a large number of latent variables from P(z) and generate x from these variables then the generated x should match the data distribution Pdata(x). Now we have two questions which we need to answer. How to capture the distribution of latent variables and how to integrate Equation 1 over all the dimensions of z?Obviously it is a tedious task to manually specify the relevant information we would like to encode in latent vector to generate the output image. Rather we rely on neural networks to compute z just with an assumption that this latent vector can be well approximated as a normal distribution so as to sample easily at inference time. If we have a normal distribution of z in n dimensional space then it is always possible to generate any kind of distribution using a sufficiently complicated function and the inverse of this function can be used to learn the latent variables itself.In equation 1, integration is carried over all the dimensions of z and is therefore intractable. However, it can be calculated using methods of Monte-Carlo integration which is something not easy to implement. So we follow an another approach to approximately maximize Pө(X) in equation 1. The idea of VAE is to infer P(z) using P(z|X) which we don’t know. We infer P(z|X) using a method called variational inference which is basically an optimization problem in Bayesian statistics. We first model P(z|X) using simpler distribution Q(z|X) which is easy to find and we try to minimize the difference between P(z|X) and Q(z|X) using KL-divergence metric approach so that our hypothesis is close to the true distribution. This is followed by a lot of mathematical equations which I will not be explaining here but you can find it in the original paper. But I must say that those equations are not very difficult to understand once you get the intuition behind VAE.The final objective function of VAE is :-The above equation has a very nice interpretation. The term Q(z|X) is basically our encoder net, z is our encoded representation of data x(x ∈ X) and P(X|z) is our decoder net. So in the above equation our goal is to maximize the log-likelihood of our data distribution under some error given by D_KL[Q(z|X) || P(z|X)]. It can easily seen that VAE is trying to minimize the lower bound of log(P(X)) since P(z|X) is not tractable but the KL-divergence term is >=0. This is same as maximizing E[logP(X|z)] and minimizing D_KL[Q(z|X) || P(z|X)]. We know that maximizing E[logP(X|z)] is a maximum likelihood estimation and is modeled using a decoder net. As I said earlier that we want our latent representation to be close to Gaussian and hence we assume P(z) as N(0, 1). Following this assumption, Q(z|X) should also be close to this distribution. If we assume that it is a Gaussian with parameters μ(X) and Ʃ(X), the error due to the difference between these two distributions i.e., P(z) and Q(z|X) given by KL-divergence results in a closed form solution given below.Considering we are optimizing the lower variational bound, our optimization function is :log(P(X|z)) − D_KL[Q(z|X)‖P(z)], where the solution of the second is shown above.Hence, our loss function will contain two terms. First one is reconstruction loss of the input to output and the second loss is KL-divergence term. Now we can train the network using backpropagation algorithm. But there is a problem and that is the first term doesn’t only depend on the parameters of P but also on the parameters of Q but this dependency doesn’t appear in the above equation. So how to backpropagate through the layer where we are sampling z randomly from the distribution Q(z|X) or N[μ(X), Ʃ(X)] so that P can decode. Gradients can’t flow through random nodes. We use reparameterization trick (see fig) to make the network differentiable. We sample from N(μ(X), Σ(X)) by first sampling ε ∼ N(0, I), then computing z=μ(X) + Σ1/2(X)∗ε.This has been very beautifully shown in the figure 2[1]? . It should be noted that the feedforward step is identical for both of these networks (left & right) but gradients can only backpropagate through right network.At inference time, we can simply sample z from N(0, 1) and feed it to decoder net to generate new data point. Since we are optimizing the lower variational bound, the quality of the generated image is somewhat poor as compared to state-of-the art techniques like Generative Adversarial Networks.The best thing of VAE is that it learns both the generative model and an inference model. Although both VAE and GANs are very exciting approaches to learn the underlying data distribution using unsupervised learning but GANs yield better results as compared to VAE. In VAE, we optimize the lower variational bound whereas in GAN, there is no such assumption. In fact, GANs don’t deal with any explicit probability density estimation. The failure of VAE in generating sharp images implies that the model is not able to learn the true posterior distribution. VAE and GAN mainly differ in the way of training. Let’s now dive into Generative Adversarial Networks.Generative Adversarial NetworksYann LeCun says that adversarial training is the coolest thing since sliced bread. Seeing the popularity of Generative Adversarial Networks and the quality of the results they produce, I think most of us would agree with him. Adversarial training has completely changed the way we teach the neural networks to do a specific task. Generative Adversarial Networks don’t work with any explicit density estimation like Variational Autoencoders. Instead, it is based on game theory approach with an objective to find Nash equilibrium between the two networks, Generator and Discriminator. The idea is to sample from a simple distribution like Gaussian and then learn to transform this noise to data distribution using universal function approximators such as neural networks.This is achieved by adversarial training of these two networks. A generator model G learns to capture the data distribution and a discriminator model D estimates the probability that a sample came from the data distribution rather than model distribution. Basically the task of the Generator is to generate natural looking images and the task of the Discriminator is to decide whether the image is fake or real. This can be thought of as a mini-max two player game where the performance of both the networks improves over time. In this game, the generator tries to fool the discriminator by generating real images as far as possible and the discriminator tries not to get fooled by the generator by improving its discriminative capability. Below image shows the basic architecture of GAN.We define a prior on input noise variables P(z) and then the generator maps this to data distribution using a complex differentiable function with parameters өg. In addition to this, we have another network called Discriminator which takes in input x and using another differentiable function with parameters өd outputs a single scalar value denoting the probability that x comes from the true data distribution Pdata(x). The objective function of the GAN is defined asIn the above equation, if the input to the Discriminator comes from true data distribution then D(x) should output 1 to maximize the above objective function w.r.t D whereas if the image has been generated from the Generator then D(G(z)) should output 1 to minimize the objective function w.r.t G. The latter basically implies that G should generate such realistic images which can fool D. We maximize the above function w.r.t parameters of Discriminator using Gradient Ascent and minimize the same w.r.t parameters of Generator using Gradient Descent. But there is a problem in optimizing generator objective. At the start of the game when the generator hasn’t learned anything, the gradient is usually very small and when it is doing very well, the gradients are very high (see Fig. 4). But we want the opposite behaviour. We therefore maximize E[log(D(G(z))] rather than minimizing E[log(1-D(G(z))]The training process consists of simultaneous application of Stochastic Gradient Descent on Discriminator and Generator. While training, we alternate between k steps of optimizing D and one step of optimizing G on the mini-batch. The process of training stops when the Discriminator is unable to distinguish ρg and ρdata i.e. D(x, өd) = ½ or when ρg = ρdata.One of the earliest model on GAN employing Convolutional Neural Network was DCGAN which stands for Deep Convolutional Generative Adversarial Networks. This network takes as input 100 random numbers drawn from a uniform distribution and outputs an image of desired shape. The network consists of many convolutional, deconvolutional and fully connected layers. The network uses many deconvolutional layers to map the input noise to the desired output image. Batch Normalization is used to stabilize the training of the network. ReLU activation is used in generator for all layers except the output layer which uses tanh layer and Leaky ReLU is used for all layers in the Discriminator. This network was trained using mini-batch stochastic gradient descent and Adam optimizer was used to accelerate training with tuned hyperparameters. The results of the paper were quite interesting. The authors showed that the generators have interesting vector arithmetic properties using which we can manipulate images in the way we want.One of the most widely used variation of GANs is conditional GAN which is constructed by simply adding conditional vector along with the noise vector (see Fig. 7). Prior to cGAN, we were generating images randomly from random samples of noise z. What if we want to generate an image with some desired features. Is there any way to provide this extra information to the model anyhow about what type of image we want to generate? The answer is yes and Conditional GAN is the way to do that. By conditioning the model on additional information which is provided to both generator and discriminator, it is possible to direct the data generation process. Conditional GANs are used in a variety of tasks such as text to image generation, image to image translation, automated image tagging etc. A unified structure of both the networks has been shown in the diagram below.One of the cool thing about GANs is that they can be trained even with small training data. Indeed the results of GANs are promising but the training procedure is not trivial especially setting up the hyperparameters of the network. Moreover, GANs are difficult to optimize as they don’t converge easily. Of course there are some tips and tricks to hack GANs but they may not always help. You can find some of these tips here. Also, we don’t have any criteria for the quantitative evaluation of the results except to check whether the generated images are perceptually realistic or not.ConclusionDeep Learning models are really achieving human level performance in supervised learning but the same is not true for unsupervised learning. Nevertheless, deep learning scientists are working hard to improve the performance of unsupervised models. In this blogpost, we saw how two of the most famous unsupervised learning frameworks of generative models actually work. We got to know the problems in Variational Autoencoders and why Adversarial networks are better at producing realistic images. But there are problems with GANs such as stabilizing their training which is still an active area of research. However GANs are really powerful and currently they are being used in a variety of tasks such as high quality image (see this video) and video generation, text to image translation, image enhancement, reconstruction of 3D models of objects from images, music generation, cancer drug discovery etc. Besides this, many deep learning researchers are also working to unify these two models and to get the best of both these models. Seeing the increasing rate of advancement of Deep Learning, I believe that GANs will open many closed doors of Artificial Intelligence such as Semi-supervised Learning and Reinforcement Learning. In the next few years, generative models is going to be very helpful for graphics designing, designing of attractive User-Interfaces etc. It may also be possible to generate natural language texts using Generative Adversarial Networks.References:-[1]. https://arxiv.org/pdf/1606.05908.pdf[2]. https://arxiv.org/pdf/1406.2661.pdf[3]. https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/",01/02/2018,0,9.0,0.0,685.0,309.0,10.0,0.0,0.0,5.0,en
4079,Adversarial attacks on Explainable AI,ResponsibleML,Hubert Baniecki,37.0,4.0,576.0,"Are explainability methods black-box themselves?There are various adversarial attacks on machine learning models; hence, ways of defending, e.g. by using Explainable AI methods. Nowadays, attacks on model explanations come to light, so does the defense to such adversary. Here, we introduce fundamental concepts related to the domain. A further reference list is available at https://github.com/hbaniecki/adversarial-explainable-ai.When considering an explanation as a function of model and data, there is a possibility to change one of these variables to achieve a different result.The first concept is to manipulate model explanations via data change. Dombrowski et al. [2019] showcase that perturbed images produce arbitrarily made visual explanations (e.g. LRP, PA, IG). Looking at the above image, we may already deduce that a target explanation is needed for the attack.To achieve the fooling, the authors propose to optimize a specific loss functionLoss ~ distance(manipulated explanation, target explanation) +  γ * distance(manipulated prediction, original prediction)where the first term denotes an aim for the target. In contrast, the second term serves as a controlling parameter, not to change models’ outcome. The manipulation method requires calculating the gradient of models’ output with respect to the input; thus, neural networks are considered.Several metrics are used to measure the success of an attack so that various explanations can be evaluated. These findings raise awareness about the explainability use and overall uncertainty of deep learning solutions. Finally, methods for the construction of more robust explanations are proposed to overcome the issue. All this without changing the model.Another idea is to fool explanations via model change. Heo et al. [2019] propose fine-tuning of a neural network to undermine its interpretability capabilities. The assumption is to alter models’ parameters without a drop in performance. The example shows how explanations may differ after the attack.Similarly, to achieve the manipulation, an objective function is proposedObjective ~ performance of the manipulated model + λ*distance(manipulated explanations, target explanations)where the first term controls the models’ performance change, while the second term denotes an aim for the target. There are multiple fooling types proposed to define the second term, depending on the strategy, objective.Passive fooling makes explainability methods generate uninformative results, while active fooling intentionally makes these methods generate false explanations. Both of the strategies are evaluated with a novel Fooling Success Rate metric, which uses empirically defined thresholds for determining whether model explanations are successfully fooled or not.Follow the references for compelling views on both of these approaches, but how about a defense to such adversary?Rieger and Hansen [2020] present a defense strategy against the attack via data change of Dombrowski et al. [2019]. The main idea is to aggregate various model explanations, which produces robust results. The simple strategy doesn’t change the model and is computationally inexpensive.Considered explainability methods for the aggregation are Layerwise Relevance Propagation, Saliency Mapping, and Guided Backprop. Each of these explanations is normalized before applying the meanAgg-Mean = mean(LRP, SM, GB)so that each input sums up to one. In this case, no weighted average is required, following the assumption that all individual explanation methods are equally important. Authors conduct experiments showing that the aggregation presents itself as the most robust against the attacks.While Explainable AI becomes more and more popular, there should be a requirement to evaluate explainability performance, the same as we evaluate model performance. As shown in recent works, an estimation of models’ behavior may be a black-box itself.For further reading on this topic consider a blog post:Be careful! Some model explanations can be fooled. | ResponsibleML",23/01/2021,0,3.0,1.0,1101.0,567.0,4.0,0.0,0.0,9.0,en
4080,Adversarial Machine Learning,CLTC Bulletin,Charles Kapelke,18.0,10.0,2258.0,"A Brief Introduction for Non-Technical AudiencesRecent years have seen a rapid increase in the use of machine learning, through which computers can be programmed to identify patterns in information and make increasingly accurate predictions over time. Machine learning is a key enabling technology behind artificial intelligence (AI), and is used for such valuable applications as email spam filters and malware detection, as well as more complex technologies like speech recognition, facial recognition, robotics, and self-driving cars.While machine learning models have many potential benefits, they may be vulnerable to manipulation. Cybersecurity researchers refer to this risk as “adversarial machine learning,” as AI systems can be deceived (by attackers or “adversaries”) into making incorrect assessments. An adversarial attack might entail presenting a machine-learning model with inaccurate or misrepresentative data as it is training, or introducing maliciously designed data to deceive an already trained model into making errors.“Machine learning has great power and promise to make our lives better in a lot of ways, but it introduces a new risk that wasn’t previously present, and we don’t have a handle on that,” says David Wagner, Professor of Computer Science at the University of California, Berkeley.Some machine learning models already used in practical applications could be vulnerable to attack. For example, by placing a few small stickers on the ground in an intersection, researchers showed that they could cause a self-driving car to make an abnormal judgment and move into the opposite lane of traffic.Other studies have shown that making imperceptible changes to an image can trick a medical imaging system into classifying a benign mole as malignant with 100% confidence, and that placing a few pieces of tape can deceive a computer vision system into wrongly classifying a stop sign as a speed limit sign.Indeed, while much of the discussion around artificial intelligence has focused on the risks of bias (as the real-world data sets used to train the algorithms may reflect existing human prejudices), adversarial machine learning represents a different kind of challenge. As machine learning is adopted widely in business, transportation, the military, and other domains, attackers could use adversarial attacks for everything from insurance fraud to launching drone strikes on unintended targets.Below is a brief overview of adversarial machine learning for policymakers, business leaders, and other stakeholders who may be involved in the development of machine learning systems, but who may not be aware of the potential for these systems to be manipulated or corrupted. A list of additional resources can be found at the conclusion of this article.Machine learning models are computer programs that, in most cases, are designed to learn to recognize patterns in data. With the help from humans supplying “training data,” algorithms known as “classifiers” can be taught how to respond to different inputs. Through repeated exposure to training data, these models are designed to make increasingly accurate assessments over time.For example, by exposing a machine learning model to several pictures of blue objects — and pre-labeling them as “blue” — the classifier can begin to break down the unique characteristics that make the objects blue. Over time, the model “learns” to ascertain whether any other subsequent image is blue, with a degree of certainty ranging from 0% to 100%. The more data is fed into a machine-learning system, the better it learns — and the more accurate its predictions become, at least in theory. But this learning process can be unpredictable, particularly in “deep” neural networks.A neural network is a particular type of machine learning model loosely inspired by the biology of the human brain. “Deep” neural networks are composed of many decision-making layers that operate in sequence. Deep neural networks have proliferated in recent years, and their usage has led to major advances in the effectiveness of machine learning.Yet the calculations that computers make within deep neural networks are highly complex and evolve rapidly as the “deep learning” process unfolds. In neural networks with a large number of layers, the calculations that lead to a given decision in some cases cannot be interpreted by humans: the process cannot be observed in real time, nor can the decision-making logic be analyzed after the fact.A machine-learning system may be using different parameters to classify than can be intuitively understood by a human, so it looks like a “black box.” In addition, small manipulations to the data can have an outsized impact on the decision made by the neural network. That makes these systems vulnerable to manipulation, including through deliberate “adversarial attacks.”Small manipulations to the data can have an outsized impact on the decision made by the neural network. That makes these systems vulnerable to manipulation, including through deliberate “adversarial attacks.”The term “adversary” is used in the field of computer security to describe people or machines that may attempt to penetrate or corrupt a computer network or program. Adversaries can use a variety of attack methods to disrupt a machine learning model, either during the training phase (called a “poisoning” attack) or after the classifier has already been trained (an “evasion” attack).Attacks on machine-learning systems during the training phase are often referred to as “poisoning” or “contaminating.” In these cases, an adversary presents incorrectly labeled data to a classifier, causing the system to make skewed or inaccurate decisions in the future. Poisoning attacks require that an adversary has a degree of control over training data.“Some of the poisoned data can be very subtle, and it’s difficult for a human to detect when data have been poisoned,” says Dawn Song, Professor of Computer Science at UC Berkeley.” We’ve done research demonstrating a ‘back-door attack,’ where the model is accurate for most normal inputs, but it can be trained to behave wrongly on specific types of inputs. It’s very difficult to detect when a model has learned such behaviors and what kinds of inputs will trigger a model to behave wrongly. This makes it very hard to detect.”A poisoning attack may use a “boy who cried wolf” approach, i.e. an adversary might input data during the training phase that is falsely labeled as harmless, when it is actually malicious. “The idea is that an attacker will slowly put in instances that will cause some type of misclassification of input data and cause an erroneous result,” explained Doug Tygar, Professor of Computer Science and Information Management at UC Berkeley, in a 2018 presentation. “Adversaries can be patient in setting up their attacks and they can adapt their behavior.”In 2016, Microsoft launched “Tay,” a Twitter chat bot programmed to learn to engage in conversation through repeated interactions with other users. While Microsoft’s intention was that Tay would engage in “casual and playful conversation,” internet trolls noticed the system had insufficient filters and began to feed profane and offensive tweets into Tay’s machine learning algorithm. The more these users engaged, the more offensive Tay’s tweets became. Microsoft shut the AI bot down after just 16 hours after its launch.Evasion attacks generally take place after a machine learning system has already been trained; they occur when a model is calculating a probability around a new data input. These attacks are often developed by trial and error, as researchers (or adversaries) do not always know in advance what data manipulations will “break” a machine learning model.For example, if attackers wanted to probe the boundaries of a machine learning model designed to filter out spam emails, they might experiment with sending different emails to see what gets through. If a model has been trained to screen for certain words (like “Viagra”) but to make exceptions for emails that contain a certain number of other words, an attacker might craft an email that includes enough extraneous words to “tip” the algorithm (i.e. to move it from being classified as “spam” to “not spam”), thus bypassing the filter.Some attacks may be designed to affect the integrity of a machine learning model, leading it to output an incorrect result or produce a specific outcome that is intended by an attacker. Other adversarial attacks could aim at the confidentiality of a system, and cause an AI-based model to reveal private or sensitive information. For example, Professor Dawn Song and her colleagues demonstrated that they could extract social security numbers from a language processing model that had been trained with a large volume of emails, some of which contained sensitive personal information.“If we embed machine learning into our life and infrastructure without having a handle on this, we might be creating a big vulnerability that a future generation is going to have to deal with.”Outside of research laboratories, adversarial attacks thus far have been uncommon. But cybersecurity researchers are concerned that adversarial attacks could become a serious problem in the future as machine-learning is integrated into a broader array of systems — including self-driving cars and other technologies where human lives could be at risk.“This is not something the bad guys are exploiting today, but it’s important enough that we want to get ahead of this problem,” says David Wagner. “If we embed machine learning into our life and infrastructure without having a handle on this, we might be creating a big vulnerability that a future generation is going to have to deal with.”What can be done to limit or prevent adversarial machine learning? Cybersecurity researchers have been busy trying to address this problem, and hundreds of papers have been published since the field of adversarial machine learning came to the research community’s attention a few years ago.Part of the challenge is that many machine learning systems are “black boxes” whose logic is largely inscrutable not only to the models’ designers, but also to would-be hackers. Adding to the challenge, attackers only need to find one crack in a system’s defenses for an adversarial attack to go through.“Lots of people have come up with solutions that looked promising at first, but so far nothing seems to work,” says Wagner. “There are one or two things that help a lot but they’re not a complete solution.”One potential approach for improving the robustness of machine learning is to generate a range of attacks against a system ahead of time, and to train the system to learn what an adversarial attack might look like, similar to building up its “immune system.” While this approach, known as adversarial training, has some benefits, it is overall insufficient to stop all attacks, as the range of possible attacks is too large and cannot be generated in advance.Another possible defense lies in continually altering the algorithms that a machine learning model uses to classify data, i.e. creating a “moving target” by keeping the algorithms secret and changing the model on an occasional basis. As a different tactic, researchers from Harvard who examined the risks of adversarial attacks on medical imaging software proposed creating a ‘fingerprint’ hash of data might be “extracted and stored at the moment of capture,” then compared to the data fed through the algorithm.Most importantly, developers of machine learning systems should be aware of the potential risks associated with these systems, and put in place systems for cross-checking and verifying information. They should also regularly attempt to break their own models and identify as many potential weaknesses as possible. They can also focus on developing methods for understanding how neural networks make decisions (and translating findings to users).“Be aware of the shortcomings and don’t blindly believe the results, especially if the result is something you do not necessarily trust yourself,” says Sadia Afroz, Senior Researcher at the International Computer Science Institute. “When you are giving a decision, show at least some understanding of why this particular decision has been made, so maybe a human can look at this decision process and figure out, does this make sense or does it not? If you don’t understand how these models are making decisions and how is it processing the data and making decisions, it opens you up to adversarial attacks. Anyone can manipulate the decision-making process and cause problems.”Some additional resources for learning about AI and adversarial machine learning.Adversarial Attacks on Medical AI Systems: Overview of March 2019 paper published in Science by researchers from Harvard and MIT, including an overview of how medical AI systems could be vulnerable to adversarial attacks.Adversarial Machine Learning: A recently published textbook by Anthony D. Joseph, Blaine Nelson, Benjamin I.P. Rubinstein, and J.D. Tygar.AI Now Institute: An interdisciplinary research center at New York University dedicated to understanding the social implications of artificial intelligence.Attacking Artificial Intelligence: AI’s Security Vulnerability and What Policymakers Can Do About It: A relevant report by Marcus Comiter from the Belfer Center for Science and International Affairs, Harvard Kennedy School.CleverHans: Compiled by TensorFlow, CleverHans is an adversarial example library for “constructing attacks, building defenses, and benchmarking both.”Failure Modes in Machine Learning: Microsoft put together this document to jointly tabulate examples of machine learning failures, both intentional and unintentional.Google AI: A trove of resources for learning about AI and machine learning.Malicious Use of Artificial Intelligence: A report written by 26 authors from 14 institutions, spanning academia, civil society, and industry.Presentation on Adversarial Machine Learning: 2018 presentation by Ian Goodfellow, a staff research scientist at Google Brain, on adversarial techniques in AI.Skymind AI Wiki: A Beginner’s Guide to Important Topics in AI, Machine Learning, and Deep Learning.Unrestricted Adversarial Examples Contest: Sponsored by Google Brain, this was a “a community-based challenge to incentivize and measure progress towards the goal of zero confident classification errors in machine learning models.”Wild Patterns: Ten Years After the Rise of Machine Learning: An overview of the evolution of adversarial machine learning by Battista Biggioa and Fabio Rolia from the University of Cagliari, Italy.",03/12/2019,0,22.0,11.0,1340.0,739.0,2.0,0.0,0.0,19.0,en
4081,Intersection Over Union,BISA.AI,Alfi Salim,25.0,5.0,506.0,"Pada masalah deteksi objek, output yang dihasilkan berupa bounding box (kotak pembatas) hasil prediksi sistem terhadap objek yang telah ditentukan. Bounding box ini merepresentasikan posisi objek dalam sebuah gambar. Untuk mengevaluasi model deteksi objek yang telah kita latih terdapat beberapa cara, salah satu caranya adalah dengan menggunakan metode Intersection Over Union (IOU). IOU memanfaatkan bounding box yang terdapat pada gambar.Intersection Over Union (IOU) adalah nilai berdasarkan statistik kesamaan dan keragaman set sampel yang tujuannya untuk mengevaluasi area tumpang tindih (area yang beririsan) antara dua bounding box, yaitu bounding box hasil prediksi dan bounding box ground truth (kebenaran). Jadi, syarat untuk menerapkan IOU adalah mempunyai kedua bounding box tersebut. Berawal dari menerapkan IOU, kita dapat mengetahui nilai-nilai evaluasi yang lainnya, seperti precision, recall dan lain sebagainya. Persamaan intersection over union sebagai berikut:Persamaan Intersection Over Union dapat diilustrasikan dalam gambar berikut:Berdasarkan ilustrasi di atas, dapat kita lihat bahwa persamaan untuk mendapatkan nilai IOU hanyalah sebuah perbandingan dari area irisan dibagi dengan area gabungan. Dengan membagi kedua area tersebut, maka kita akan mendapatkan skor Intersection Over Union (IOU). Setelah mendapatkan skor IOU, aturan yang ada untuk menilai apakah skor IOU yang kita dapat baik atau buruk adalah semakin beririsan atau semakin dekat jarak antara bounding box prediksi dengan bounding box ground truth. Untuk lebih jelasnya, saya akan mengilustrasikan skor IOU yang kita kategorikan sebagai skor yang baik, lumayan dan buruk.Dari ilustrasi diatas dapat kita simpulkan bahwa skor akan semakin tinggi jika jarak antara bounding box prediksi dengan bounding box ground truth semakin dekat (area yang berisisan antara kedua bounding box semakin besar).Untuk teman-teman yang masih bertanya-tanya bagaimana kita mendapatkan bounding box ground truth akan saya jelaskan sedikit. Jadi, bounding box ground truth didapatkan dengan cara mengannotasi atau memberi label pada dataset secara manual. Telah banyak tools-tools yang mempermudah kita dalam mengannotasi sebuah data, khususnya data gambar. Pada artikel ini, kita tidak akan membahas mengenai data gambar dan cara mengannotasinya, mungkin materi tersebut akan dibahas secara lebih rinci pada artikel yang berbeda. Jadi, terus ikuti Medium kami.Setelah mengetahui beberapa hal tentang IOU, langsung saja kita implementasikan menggunakan bahasa pemrograman Python. Pada kasus ini, kita akan mencari nilai IOU pada gambar-gambar berikut:Sebelumnya, kita asumsikan bahwa kita telah memiliki model yang telah bisa memprediksi objek burung dalam gambar dengan cara mengeluarkan bounding box. Gambar-gambar di atas saya simpan di sebuah folder. Masing-masing gambar mempunyai satu file anotasi yang disimpan dalam folder yang sama dengan gambar.Langkah awalnya yaitu mengimport semua library yang dibutuhkan. selanjutnya read semua dataset :Selanjutnya, buat fungsi untuk mengitung skor IOU, disini saya beri nama fungsi_iou dengan 2 parameter yaitu bounding box ground truth (BBox_GroundT) dan bounding box prediksi (Bbox_prediksi)Koordinat bounding box ground truth dari semua gambar disimpan sebagai list pada variabel groundTruth. Output prediksi koordinat dari model kita, disimpan juga sebagai list pada variabel prediksi.Visualisasikan bounding box ground truth dan prediksi pada masing-masing gambar. Terakhir, hitung nilai IOU nya.Terakhir, kode di atas adalah untuk menampilkan hasil gambar dengan semua bounding box dan hasil IOU nya dari masing-masing gambar. Adapun output yang dihasilkan :Terima kasih, sekian dari saya. Semoga bermanfaat!",25/03/2020,5,14.0,39.0,693.0,383.0,7.0,4.0,0.0,1.0,id
4082,Variational Autoencoder In Finance,Towards Data Science,Marie Imokoyende,50.0,9.0,1116.0,"This article explores the use of a variational autoencoder to reduce the dimensions of financial time series with Keras and Python. We will further detect similarities between financial instruments in different markets and will use the results obtained to construct a custom index.Disclaimer: The research presented in this article comes from our Winter 2019 Term Project for the Deep Learning course at the University of Toronto School of Continuing Studies. It was done in collaboration with Humberto Ribeiro de Souza. The concepts and ideas are our own. We are in no way representing our current or previous employers.In this section, we will discuss:Creating The Geometric Moving Average DatasetIn order to compare time series of various price ranges, we have chosen to compute geometric moving average time series of returns defined as:We chose d=5, as it represents a typical trading week of 5 business days.The dataset used in this article contains 423 geometric moving average time series for a period going from January 4th, 2016 to March 1st, 2019.Readers can follow the steps described in the data treatment notebook to build their own dataset. It should be similar to this one:Results can be verified by plotting some sample stock price time series and their geometric moving average curves:Then, the dataframe just built can be divided in two time periods of equal length, transposing the one for the first period only. Period 1 goes from January 12th, 2016 to August 4th, 2017. Period 2, goes from August 7th, 2017 to March 1st, 2019.We will only use the period 1 data to obtain predictions.We transpose the dataframe so that each row will represent a time series for a given stock:Augmenting the data with stochastic simulationWe will use stochastic simulation to generate synthetic geometric moving average curves. The objective is not to precisely model returns but to obtain curves with a behavior similar to real data. By training the model with only simulated curves we can keep the real data to obtain the predictions.The synthetic curves are generated using Geometric Brownian Motion. We followed the steps below:Here is a sample of a simulated curve and a real curve:We have expanded a dataset of 423 time series to 100*100 = 10,000 new time series similar (but not equal) to the stock dataset.This will allow us to keep the actual stock dataset universe for predictions and not even have to use it for the validation.Before building the VAE model, create the training and test sets (using a 80%-20% ratio):Readers should also note that there is no need to remove the seasonality and trend of the time series before training the model.Building the Variational Autoencoder (VAE) ModelWe will use a variational autoencoder to reduce the dimensions of a time series vector with 388 items to a two-dimensional point.Autoencoders are unsupervised algorithms used to compress data. They are built with an encoder, a decoder and a loss function to measure the information loss between the compressed and decompressed data representations.Our goal is not to write yet another autoencoder article. Readers who are not familiar with autoencoders can read more on the Keras Blog and the Auto-Encoding Variational Bayes paper by Diederik Kingma and Max Welling.We will use a simple VAE architecture similar to the one described in the Keras blog.The encoder model has:The decoded model has:The code below is adapted from variational_autoencoder.py on the Keras team Github. It is used to build and train the VAE model.After training, we plot the training and validation loss curves:Obtaining the PredictionsWe will only use the encoder to obtain the predictions. We will use a matrix of real values including both the stock dataset and one or multiple time series of interest.In our project, we tested a stock dataset against a front month futures contract listed in another country and in a different currency.We obtained the following results:Before plotting the results, we have to:We can now plot the results obtained to visualize the closest 50 stocks:We’ve done our analysis for a futures contract listed in another country. However it is possible to follow the same steps in Part 1 for stocks from the same exchange.Let’s use the results obtained in Part 1 to create an index.Due to the randomness of the VAE model, we will not obtain the same exact list of top 50 stocks on each run. To get a fair representation of the closest 50 points, we will run the VAE model 10 times (re-initializing and retraining it on each run). Then we will take the 50 closest points found on each run to create a dataframe closest_points_df dataframe of length 500.Once the closest_points_df dataframe is built:After dropping the duplicates, we will only keep the 50 closest points.Compute the weights of each stockIn index construction, stock weights are calculated by using different methodologies such as market capitalization or stock prices.Instead, we will calculate the weight of each stock such that the points closest to the futures contract point will get a higher weight than the ones further from it.With non-anonymized stock data, it is important to filter the results obtained before computing the stock weights. Outliers should be removed and the market capitalization range should be refined.Compute the number of shares of each stockAfter computing the weights, we calculate the number of shares of each stock in our custom index. We need to:Construct the indexTo build the index, we will use the Laspeyres index computed as:We plot the custom index obtained:Compare our custom index with the futures time seriesWe have to scale the futures price data in order to plot it in the same graph as our custom index. To do so we have to:We now plot both curves in the same graph:Our index has mostly the same trend as the reference futures time series except for the second half of 2018. Because we use anonymized data, we did not filter the stocks for outliers and market capitalization limits. Furthermore there was no re-balancing throughout the two time periods observed and we ignored distributions.It is absolutely possible for the custom index to beat the futures index if tickers are identified and outliers are removed.We encourage our readers to take advantage of the free GPU instances available online to create their own indices. It was a fun experiment for us and we discovered some interesting stock patterns.Feel free to download the two notebooks available on GitHub:ConclusionThe use of variational autoencoders can speed up the development of new indices in foreign stock markets, even if analysts are unfamiliar with them. Furthermore, niche indices or portfolios could be created to match customers interests.While this method can be used to create ETFs, we believe that it can also create new investment possibilities for Direct Indexing and Robo Advisors firms worldwide.",15/04/2019,10,15.0,3.0,588.0,267.0,20.0,10.0,0.0,10.0,en
4083,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data",Becoming Human: Artificial Intelligence Magazine,Stefan Kojouharov,14200.0,8.0,744.0,"Over the past few months, I have been collecting AI cheat sheets. From time to time I share them with friends and colleagues and recently I have been getting asked a lot, so I decided to organize and share the entire collection. To make things more interesting and give context, I added descriptions and/or excerpts for each major topic.This is the most complete list and the Big-O is at the very end, enjoy…>>> Update: We have recently redesigned these cheat sheets into a Super High Definition PDF. Check them out below:becominghuman.aichatbotslife.comaijobsboard.comThis machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. The flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it.>>> See Latest Jobs in AI, ML & BIG DATA <<<Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.This machine learning cheat sheet from Microsoft Azure will help you choose the appropriate machine learning algorithms for your predictive analytics solution. First, the cheat sheet will asks you about the data nature and then suggests the best algorithm for the job.becominghuman.aiaijobsboard.comIn May 2017 Google announced the second-generation of the TPU, as well as the availability of the TPUs in Google Compute Engine.[12] The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs provide up to 11.5 petaflops.becominghuman.aiIn 2017, Google’s TensorFlow team decided to support Keras in TensorFlow’s core library. Chollet explained that Keras was conceived to be an interface rather than an end-to-end machine-learning framework. It presents a higher-level, more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library.NumPy targets the CPython reference implementation of Python, which is a non-optimizing bytecode interpreter. Mathematical algorithms written for this version of Python often run much slower than compiled equivalents. NumPy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy.The name ‘Pandas’ is derived from the term “panel data”, an econometrics term for multidimensional structured data sets.The term “data wrangler” is starting to infiltrate pop culture. In the 2017 movie Kong: Skull Island, one of the characters, played by actor Marc Evan Jackson is introduced as “Steve Woodward, our data wrangler”.becominghuman.aichatbotslife.comaijobsboard.comSciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack.[3]matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.[2] SciPy makes use of matplotlib.pyplot is a matplotlib module which provides a MATLAB-like interface.[6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free.>>> If you like this list, you can let me know here. <<<aijobsboard.combecominghuman.aibecominghuman.aiStefan is the founder of Chatbot’s Life, a Chatbot media and consulting firm. Chatbot’s Life has grown to over 150k views per month and has become the premium place to learn about Bots & AI online. Chatbot’s Life has also consulted many of the top Bot companies like Swelly, Instavest, OutBrain, NearGroup and a number of Enterprises.Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdfData Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basicsData Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfData Wrangling: https://en.wikipedia.org/wiki/Data_wranglingGgplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfKeras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMsKeras: https://en.wikipedia.org/wiki/KerasMachine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheetML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.htmlMatplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpYMatpotlib: https://en.wikipedia.org/wiki/MatplotlibNeural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-networkNumpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgENumPy: https://en.wikipedia.org/wiki/NumPyPandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxMPandas: https://en.wikipedia.org/wiki/Pandas_(software)Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIcPyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQScikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheetScikit-learn: https://en.wikipedia.org/wiki/Scikit-learnScikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.htmlScipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OISciPy: https://en.wikipedia.org/wiki/SciPyTesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.htmlTensor Flow: https://en.wikipedia.org/wiki/TensorFlow",09/07/2017,0,10.0,7.0,1146.0,894.0,37.0,0.0,0.0,103.0,en
4084,Region of Interest Pooling,Towards Data Science,Sambasivarao. K,238.0,4.0,523.0,"The major hurdle for going from image classification to object detection is fixed size input requirement to the network because of existing fully connected layers. In object detection, each proposal will be of a different shape. So there is a need for converting all the proposals to fixed shape as required by fully connected layers. ROI Pooling is exactly doing this.Region of Interest (ROI) pooling is used for utilising single feature map for all the proposals generated by RPN in a single pass. ROI pooling solves the problem of fixed image size requirement for object detection network.ROI pooling produces the fixed-size feature maps from non-uniform inputs by doing max-pooling on the inputs. The number of output channels is equal to the number of input channels for this layer. ROI pooling layer takes two inputs:ROI pooling takes every ROI from the input and takes a section of input feature map which corresponds to that ROI and converts that feature-map section into a fixed dimension map. The output fixed dimension of the ROI pooling for every ROI neither depends on the input feature map nor on the proposal sizes, It solely depends on the layer parameters.Pooled_width and pooled_height are hyperparameters which can be decided based on the problem at hand. These indicate the number of grids the feature map corresponding to the proposal should be divided into. This will be the output dimension of this layer. Let us assume that W, H are the width and height of the proposal and P_w,P_h are pooled width and height. Then the ROI will be divided into P_w*P_h blocks, each of dimensions (W/P_w, H/P_h).Spatial scale is a scaling parameter for resizing the proposal according to the feature map dimensions. Let's say in our network, the image size is 1056x640 and due to many convolution and pooling operations, the feature map size reduced to 66x40, which is being used by ROI pooling. Now the proposals are generated based on input image size, so we need to rescale the proposals to feature map size. In this case, we can divide all dimensions of proposal by 16 (1056/66=16 or 640/40=16). So the spatial scale will be 1/16 in our example.Now we got a clear understanding of each parameter, let us see how ROI pooling works. For every proposal in the input proposals, we take the corresponding feature map section and divide that section into W*H blocks defined by layer parameters. After that take the maximum element of each block and copy to the output. So the output size will be P_w*P_h for every ROI proposal and N*P_w*P_h for all N proposals which is a fixed dimension feature map irrespective of the various sizes of the input proposals.Below diagram illustrates the forward pass of ROI pooling layer.The main advantage of ROI pooling is that we can use the same feature map for all the proposals which enables us to pass the entire image to the CNN instead of passing all proposals individually.Hope this helps! Thanks, everyone!References:Subscribe to FOCUS — my weekly newsletter to get the latest updates and recent advances in AI along with curated stories from the medium on Machine Learning.",22/04/2019,4,5.0,0.0,753.0,512.0,2.0,2.0,0.0,3.0,en
4085,Clustering on Kubernetes & OpenShift3 using DNS,fabric8 io,Jimmi Dyson,165.0,5.0,936.0,"One of the big promises of Kubernetes & OpenShift is really easy management of your containerised applications. For standalone or load-balanced stateless applications, Kubernetes works brilliantly, but one thing that I had a bit of trouble figuring out was how do perform cluster discovery for my applications? Say one of my applications needs to know about at least one other node (seed node) that it should join a cluster with.There is an example in the Kubernetes repo for Cassandra that requests existing service endpoints from the Kubernetes API server & use those as the seed servers. You can see the code for it here. That works great for a cluster that allows unauthenticated/unauthorized access to the API server, but hopefully most people are going to lock down their API server (OpenShift comes with auth baked in by the way & secure by default). If you’re going to secure your API server then you’re going to have to distribute credentials via secrets around to every container that wants to call the API server. Personally I’d rather only distribute secrets when absolutely necessary: if there’s a way to achieve what we need to achieve without distributing secrets then I would prefer to do that.It would be cool if Kubernetes had the concept of cluster seeds baked in & could provide seeds to pods through configuration, but right now it can’t so we’re going to take advantage of a couple of things that Kubernetes provides to do that: headless services & DNS.Before we go any further, a quick recap of 3 Kubernetes concepts we’ll be using in this post (taken from the excellent Kubernetes documentation):A headless service is a service that has no IP address (& therefore no service environment variables, load-balancing or proxying). It is simply used to track what endpoints (pods) would be part of the service. Perfect for simple discovery.I’m going to assume you have a working Kubernetes cluster up & running. If you don’t then you can really easily set one up on any Docker-enabled host via a script that Fabric8 provides (see here if you’re interested). Note that the script will actually spin up OpenShift3 rather than vanilla Kubernetes as Fabric8 uses some of the extensions that OpenShift provides, like builds, deployment pipelines, etc for other things. Everything below will work on vanilla Kubernetes of course.You’re also going to need to have the DNS cluster add-on. Btw, this is another capability that OpenShift provides by default.For a working (hopefully!) example, I’m going to use Elasticsearch as I’m pretty familiar with it & it’s awesome horizontal scalability lends itself very well to hopefully explaining this clearly. This is an application that we provide for one click installation as part of Fabric8 to build up Elasticsearch clusters. To make this a little more interesting we’re actually going to create a cluster of the 3 different types of Elasticsearch node: Master, Data & Client. If you’re building a large cluster this is probably what you would want to do. We’re going to make it so that each type can be scaled individually by resizing the respective replication controller & each node is going to discover other nodes in the cluster via a headless service.So to action…Before we actually create anything, let’s prepare our Kubernetes manifests. We’ll send the create requests to the API server at the end — don’t jump the gun!First let’s create our replication controllers. All 3 look pretty similar — this one’s for the client nodes:Few things of importance here: notice the labels on the pod template:For the replication controllers for data & master nodes, you will need to update the type in the label — leave the component part alone: having a common subset in the labels for all the node types is what we will use when we create our headless service.The environment variables are something that the fabric8/elasticsearch-k8s uses to configure Elasticsearch so you will need to update the NODE_DATA & NODE_MASTER environment variables appropriately for the other two replication controllers for the other node types.We want all access to the Elasticsearch cluster to go through the client nodes so let’s create a service to do just that:Notice that the selector matches the labels of the client nodes replication controller only.Finally we create a headless service that we’re going to use to discover our cluster nodes:Notice the PortalIP is set to None — that means no IP address will be allocated to the service. Sadly we still have to specify the containerPort & port although these are not used at all.The final thing to take not of is the id: elasticsearch-cluster. This is the DNS name that the service can be discovered under. With a normal service, the DNS entry that is registered is an A record with the IP address that is allocated to the service. With a headless service, however, an A record is created for each service endpoint (pod targeted by the specified selector) for the service name.Go ahead & create your resources — create the services first so that cluster discovery service is available when the nodes first come up.Once your resources are created & the pods are up, let’s check the DNS entries have been created properly with a quick dig. On my setup, this is the result:And for the Elasticsearch client service:If we use the Elasticsearch client service to check the health of the cluster:Yay — it worked! 3 nodes in our cluster discovered without any need for credentials to be distributed using DNS.Now play with resizing each of the 3 node types & see what happens — easy cluster resizing FTW.Originally published at jimmidyson.github.io.",17/04/2015,3,0.0,11.0,0.0,0.0,0.0,2.0,0.0,19.0,en
4086,"Support Vector Machine: MATLAB, R and Python codes — All you have to do is just preparing data set (very simple, easy and practical)",Medium,DataAnalysis For Beginneｒ,279.0,4.0,498.0,"I release MATLAB, R and Python codes of Support Vector Machine (SVM). They are very easy to use. You prepare data set, and just run the code! Then, SVM and prediction results for new samples can be obtained. Very simple and easy!You can buy each code from the URLs below.https://gum.co/XdZSo Please download the supplemental zip file (this is free) from the URL below to run the SVM code. http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.ziphttps://gum.co/OyXVZ Please download the supplemental zip file (this is free) from the URL below to run the SVM code. http://univprofblog.html.xdomain.jp/code/R_scripts_functions.ziphttps://gum.co/AtOvT Please download the supplemental zip file (this is free) from the URL below to run the SVM code. http://univprofblog.html.xdomain.jp/code/supportingfunctions.zipTo perform appropriate SVM, the MATLAB, R and Python codes follow the procedure below, after data set is loaded.1. Autoscale explanatory variable (X) Autoscaling means centering and scaling. Mean of each variable becomes zero by subtracting mean of each variable from the variable in centering. Standard deviation of each variable becomes one by dividing standard deviation of each variable from the variable in scaling.2. Determine candidates of C and gamma C controls the balance between accuracy and generality of SVM. Gamma is the parameter in Gaussian kernel, which is one of the most famous kernel functions. For example,C: 2^-10, 2^-9, …, 2⁹, 2¹⁰,Gamma: 2^-20, 2^-19, …, 2⁹, 2¹⁰.3. Calculate gram matrix of Gaussian kernel and its variance for each gamma candidateIf the size of gram matrix is 100×100, for example, variance is calculated for resized 10000×1 vector.4. Decide the optimal gamma with the maximum variance value This means that gram matrix with the optimal gamma has diverse kernel values.5. Estimate objective variable (Y) with cross-validation (CV) for each C candidate Leave-one-out CV is very famous, but it causes over-fitting when the number of training samples is high. So, 5-fold or 2-fold CV is better. First, training samples are divided into 5 or 2 groups. Second, one group is handled as test samples and model is built with the other group(s). This is repeated 5 or 2 times until every group is handled as test samples. Then, not calculated Y but estimated Y can be obtained.6. Calculate accuracy rate between actual Y and estimated Y for each C candidate7. Decide the optimal C with the maximum accuracy value8. Construct SVM model with the optimal C and gamma9. Calculate confusion matrix between actual Y and calculated Y, and that between actual Y and estimate Y with cross-validation, for the optimal C and gamma Accuracy rate, detection rate, precision and so on can be calculated from each confusion matrix if necessary.10. In prediction, subtract the mean in the autoscalling of X in 1. from X-variables, and then, divide X-variables by the standard deviation in the autoscalling of X in 1., for new samples11. Estimate Y based on SVM in 8.MATLAB: https://gum.co/XdZSoR: https://gum.co/OyXVZPython: https://gum.co/AtOvTMATLAB: http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.zipR: http://univprofblog.html.xdomain.jp/code/R_scripts_functions.zipPython: http://univprofblog.html.xdomain.jp/code/supportingfunctions.ziphttps://medium.com/@univprofblog1/data-format-for-matlab-r-and-python-codes-of-data-analysis-and-sample-data-set-9b0f845b565a#.3ibrphs4h*Caution! SVM can be used for only binary classification data set. Y must be 1 or -1.Estimated values of Y for “data_prediction2.csv” are saved in ”PredictedY2.csv”.Please see the article below.https://medium.com/@univprofblog1/settings-for-running-my-matlab-r-and-python-codes-136b9e5637a1#.paer8scqy",17/08/2016,0,17.0,0.0,434.0,350.0,3.0,0.0,0.0,14.0,en
4087,"Intro to reinforcement learning: temporal difference learning, SARSA vs. Q-learning",Towards Data Science,Viet Hoang Tran Duong,20.0,9.0,1704.0,"Reinforcement learning (RL) is surely a rising field, with the huge influence from the performance of AlphaZero (the best chess engine as of now). RL is a subfield of machine learning that teaches agents to perform in an environment to maximize rewards overtime.Among RL’s model-free methods is temporal difference (TD) learning, with SARSA and Q-learning (QL) being two of the most used algorithms. I chose to explore SARSA and QL to highlight a subtle difference between on-policy learning and off-learning, which we will discuss later in the post.This post assumes you have basic knowledge of the agent, environment, action, and rewards within RL's scope. A brief introduction can be found here.The outline of this post include:We will compare these two algorithms via the CartPole game implementation. This post's code can be found here: QL code, SARSA code, and the fully functioning code. (the fully-functioning code has both algorithms implemented and trained on cart pole game)The TD learning will be a bit mathematical, but feel free to skim through and jump directly to QL and SARSA.One of the problems with the environment is that rewards usually are not immediately observable. For example, in tic-tac-toe or others, we only know the reward(s) on the final move (terminal state). All other moves will have 0 immediate rewards.TD learning is an unsupervised technique to predict a variable's expected value in a sequence of states. TD uses a mathematical trick to replace complex reasoning about the future with a simple learning procedure that can produce the same results. Instead of calculating the total future reward, TD tries to predict the combination of immediate reward and its own reward prediction at the next moment in time. (more info can be found here)Mathematically, the key concept of TD learning is the discounted return:Where the reward at time t is the combination of discounted rewards in the future. It implies that future rewards are valued less. The TD Error is the difference between the ultimate correct reward (V*_t) and our current prediction (V_t).And similar to other optimization methods, the current value will be updated by its value + learning_rate * error:Alright, that’s enough math for the day. The equations above are the core idea for TD learning, which will help us understand QL and SARSA codes.Alpha (α): learning rate. This parameter shows how much we should adjust our estimates based on the error. The learning rate is between 0 and 1. A large learning rate adjusts aggressively and might lead to fluctuating training results — not converging. A small learning rate adjusts slowly, which will take more time to converge.Gamma (γ): the discount rate. How much we are valuing future rewards. The discount rate is between 0 and 1. The bigger the discount rate, we more we valuing the future rewards.e (coming up in the next section on “e-greedy” policy): the ratio reflective of exploration vs. exploitation. We explore new options with probability e and stay at the current max with probability 1-e. The larger e implies more exploration while training.QL and SARSA both store the rewards of the current state and the corresponding action for future updating. A state is usually represented by the coordinate of its components. If the environment is continuous, we will have an infinite number of states. To counter this problem, we need to discretize the state by segmenting them into buckets.The way I did this is by splitting the continuous space into grids and have a separate function to discretize them (4 variables, I split them into 10 boxes each → 10⁴ boxes in my weight matrix). For more rigorous applications, you can segment them into more boxes (split into 100 boxes instead of 10 like above). The more boxes you have, the more details your model can learn and longer to learn, and more memory space.Be cautious about the number of boxes you split. If too small, you can have an underperforming model. If too large, you would need large memory space. Also, I recommend limiting the region of the game. For example, in the cart pole game in OpenAI’s gym, I ignore the white space above the image and the area near the boundary to limit my state space. These are some design choices you have to make while making the models.As it needs to memorize the state space, QL and SARSA will work best for constrained and limited actions games (like CartPole moving left and right) and not well on more complex (like chess with many possible moves). To avoid storing all the state space for games with larger state space, we can use a deep q network. This approach combines reinforcement learning with neural networks. We shall talk more about this deep q network in some later posts. For now, back to QL and SARSA.Quick definition: “policy” is a strategy that an agent uses to pursue a goal. Greedy (choosing the best value) is a policy. The greedy algorithm can make us stuck in the local minima. Hence, we have “e-greedy,” a policy ask that e chance it will explore, and (1-e) chance of following the optimal path. e-greedy is applied to balance the exploration and exploration of reinforcement learning. (learn more about exploring vs. exploiting here). In this implementation, we use e-greedy as the policy.Back to QL & SARSA, they are both updated using the TD learning formula above, with a slight difference: (Q is the storage to keep all the rewards at discrete space and action, s represents the state, and a represents action)Math (equation 6.8 in “Reinforcement Learning: An Introduction”)Code:Math (equation 6.7 in “Reinforcement Learning: An Introduction”):Code (e-greedy policy):The difference is very subtle: For QL, which is the off-policy algorithm, when passing the reward from the next state (s_, a_) to the current state, it takes the maximum possible reward of the new state (s_) and ignores whatever policy we are using. For SARSA, which is on-policy, we still follow the policy (e-greedy), compute the next state (a_), and pass the reward corresponding to that exact a_ back the previous step.To reiterate, QL considers the best possible case if you get to the next state, while SARSA considers the reward if we follow the current policy at the next state. Hence, if our policy is greedy, SARSA and QL will be the same. But we are using e-greedy here, so there is a slight difference.QL and SARSA are both excellent initial approaches for reinforcement learning problems. A few key notes to select when to use QL or SARSA:For the CartPole game, OpenAI’s gym has a prebuilt environment. A few gym syntaxes are listed here: (learn more about OpenAI gym here)Next, as QL and SARSA work best in discrete state space, and the cart pole game is continuous, we will discretize them to into smaller bins. More bins, the better the performance. The more bins would help the model account for more specific state space, leading to better overall performance. However, more bins would require training with more games, costing computational power. If time, computational power, and storage space are your constraints, stay with a small number of bins. Otherwise, you are welcomed to try a larger number of bins. Also, try with a small number of bins to check the performance before scaling up.Putting a few graphs showing the performance of my algorithms. Note that in OpenAI’s gym cart pole game, the maximum step you can reach is 200 (and the game will self terminate by then).QL training:SARSA training:QL testing:SARSA testing:The training graphs show the performance of the agent after many games. The x_axis indicates the number of games we trained on, and the y_axis represents the max step they can take (capped at 200 due to OpenAI gym’s setup). The testing graphs show the performance in the implementation phase (after we finished the training). The histogram shows what is the distribution of the outcomes for each model when we play 1000 games. The results are as expected because SARSA chooses to play safer (usually) compared to QL. Hence, it might take less dramatic steps along with the game, leading to better performance. That is one possible explanation for the performance of SARSA over QL.Random thoughts: if both models aren’t working awesomely (if only my SARSA didn’t work as great), we can try combining both models.The decision will be based on “beta * QL + (1-beta) * SARSA”.In some cases, it could help the performance if you tune the“beta” well. Adjusting the beta means varying the beta to see which results are best. This tuning is more of an art to see which works best. You can try looping through multiple values of beta and see which yields the highest result.Our SARSA is always successful for this use case, so in this particular case there is no need for aggregation, but it might be worth trying for other games! Let me know in the comment section if this combined version works better than the individual model in any other games!Here are a quick introduction and comparison between QL and SARSA. I hope it helps you understand SARSA and QL better and see the differences between on-policy and off-policy learning.As promised, here is the code for all the algorithms and agents (with existing Q tables). If you want to see a running video of the agent playing game, run it in VS Code instead of GG Colab.Feel free to leave a response with any thoughts or questions and give my blog a follow if you enjoyed this post and want to see more! You can also find me on LinkedIn. Enjoy learning!References:1. Ashraf, M. (2018, December 3). Reinforcement Learning Demystified: Exploration vs. Exploitation in Multi-armed Bandit setting. Retrieved from https://towardsdatascience.com/reinforcement-learning-demystified-exploration-vs-exploitation-in-multi-armed-bandit-setting-be950d2ee9f62. Dabney, W., & Kurth-Nelson, Z. (n.d.). Dopamine and temporal difference learning: A fruitful relationship between neuroscience and AI. Retrieved from https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI3. Lee, D. (2020, April 12). Reinforcement Learning, Part 1: A Brief Introduction. Retrieved from https://medium.com/ai³-theory-practice-business/reinforcement-learning-part-1-a-brief-introduction-a53a849771cf 4. OpenAI. (n.d.). A toolkit for developing and comparing reinforcement learning algorithms. Retrieved from https://gym.openai.com/docs/5. Stanford PDP Lab. (2015, December 16). Chapter 9 Temporal-Difference Learning. Retrieved from https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html6. Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. Cambridge (Mass.): The MIT Press.7. Tabor, P. (2020, June 23). SARSA.py. Retrieved from https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/Fundamentals/sarsa.py",24/02/2021,0,16.0,21.0,704.0,258.0,12.0,2.0,0.0,16.0,en
4088,Object detection and tracking in PyTorch,Towards Data Science,Chris Fotache,501.0,7.0,907.0,"In my previous story, I went over how to train an image classifier in PyTorch, with your own images, and then use it for image recognition. Now I’ll show you how to use a pre-trained classifier to detect multiple objects in an image, and later track them across a video.What’s the difference between image classification (recognition) and object detection? In classification, you identify what’s the main object in the image and the entire image is classified by a single class. In detection, multiple objects are identified in the image, classified, and a location is also determined (as a bounding box).There are several algorithms for object detection, with YOLO and SSD among the most popular. For this story, I’ll use YOLOv3. I won’t get into the technical details of how YOLO (You Only Look Once) works — you can read that here — but focus instead of how to use it in your own application.So let’s jump into the code! The Yolo detection code here is based on Erik Lindernoren’s implementation of Joseph Redmon and Ali Farhadi’s paper. The code snippets below are from a Jupyter Notebook you can find in my Github repo. Before you run this, you’ll need to run the download_weights.sh script in the config folder to download the Yolo weights file. We start by importing the required modules:Then we load the pre-trained configuration and weights, as well as the class names of the COCO dataset on which the Darknet model was trained. As always in PyTorch, don’t forget to set the model in eval mode after loading.There are also a few pre-defined values above: The image size (416px squares), confidence threshold and the non-maximum suppression threshold.Below is the basic function that will return detections for a specified image. Note that it requires a Pillow image as input. Most of the code deals with resizing the image to a 416px square while maintaining its aspect ratio and padding the overflow. The actual detection is in the last 4 lines.Finally, let’s put it together by loading an image, getting the detections, and then displaying it with the bounding boxes around detected objects. Again, most of the code here deals with scaling and padding the image, as well as getting different colors for each detected class.You can put together these code fragments to run the code, or download the notebook from my Github. Here are a few examples of object detection in images:So now you know how to detect different objects in an image. The visualization might be pretty cool when you do it frame by frame in a video and you see those tracking boxes moving around. But if there are multiple objects in those video frames, how do you know if an object in one frame is the same as one in a previous frame? That’s called object tracking, and uses multiple detections to identify a specific object over time.There are several algorithms that do it, and I decided to use SORT, which is very easy to use and pretty fast. SORT (Simple Online and Realtime Tracking) is a 2017 paper by Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft which proposes using a Kalman filter to predict the track of previously identified objects, and match them with new detections. Author Alex Bewley also wrote a versatile Python implementation that I’m gonna use for this story. Make sure you download the Sort version from my Github repo since I had to make a few small changes to integrate it in my project.Now on to the code, the first 3 code segments will be the same as in the single image detection, since they deal with getting the YOLO detections on a single frame. The difference comes in the final part where for each detection we call the Update function of the Sort object in order to get references to the objects in the image. So instead of the regular detections from the previous example (which include the coordinates of the bounding box and a class prediction), we’ll get tracked objects which, besides the parameters above, also include an object ID. Then we display the almost the same way, but adding that ID and using different colors so you can easily see the objects across the video frames.I also used OpenCV to read the video and display the video frames. Note that the Jupyter notebook is quite slow in processing the video. You can use it for testing and simple visualizations, but I also provided a standalone Python script that will read the source video, and output a copy with the tracked objects. Playing an OpenCV video in a notebook is not easy, so you can keep this code for other experiments.After you play with the notebook, you can use the regular Python script both for live processing (you can take input from a camera) and to save videos. Here’s a sample of videos I generated with this program.And that’s it, you can now try on your own to detect multiple objects in images and to track those objects across video frames.If you want to detect and track your own objects on a custom image dataset, you can read my next story about Training Yolo for Object Detection on a Custom Dataset.Chris Fotache is an AI researcher with CYNET.ai based in New Jersey. He covers topics related to artificial intelligence in our life, Python programming, machine learning, computer vision, natural language processing and more.",10/12/2018,5,6.0,3.0,697.0,432.0,4.0,0.0,0.0,12.0,en
4089,I Asked GPT-3 About Covid-19. Its Responses Shocked Me.,OneZero,Thomas Smith,30000.0,9.0,41.0,"OpenAI’s GPT-3 is the most powerful AI system I’ve ever used. Trained on billions of web pages and tens of thousands of books, the system can generate nearly any kind of text, from news articles to computer code to sea shanties.",01/09/2021,0,0.0,0.0,1400.0,933.0,1.0,0.0,0.0,4.0,en
4090,A detailed explanation of the Attention U-Net,Towards Data Science,Robin Vinod,37.0,5.0,608.0,"In this story, I explain the Attention U-Net from Attention U-Net:Learning Where to Look for the Pancreas written by Oktay et. al. The paper was written in 2018 and proposed a novel attention gate (AG) mechanism that allows the U-Net to focus on target structures of varying size and shape.Attention, in the context of image segmentation, is a way to highlight only the relevant activations during training. This reduces the computational resources wasted on irrelevant activations, providing the network with better generalisation power. Essentially, the network can pay “attention” to certain parts of the image.a. Hard AttentionAttention comes in two forms, hard and soft. Hard attention works on the basis of highlighting relevant regions by cropping the image or iterative region proposal. Since hard attention can only choose one region of an image at a time, it has two implications, it is non-differentiable and requires reinforcement learning to train.Since it is non-differentiable, it means that for a given region in an image, the network can either pay “attention” or not, with no in between. As a result, standard backpropagation cannot be done, and Monte Carlo sampling is needed to calculate the accuracy across various stages of backpropagation. Considering the accuracy is subject to how well the sampling is done, there is a need for other techniques such as reinforcement learning to make the model effective.b. Soft AttentionSoft attention works by weighting different parts of the image. Areas of high relevance is multiplied with a larger weight and areas of low relevance is tagged with smaller weights. As the model is trained, more focus is given to the regions with higher weights. Unlike hard attention, these weights can be applied to many patches in the image.Due to the deterministic nature of soft attention, it remains differentiable and can be trained with standard backpropagation. As the model is trained, the weighting is also trained such that the model gets better at deciding which parts to pay attention to.Hard Attention:Soft Attention:To understand why attention is beneficial in the U-Net, we need to look at the skip connections used.During upsampling in the expanding path, spatial information recreated is imprecise. To counteract this problem, the U-Net uses skip connections that combine spatial information from the downsampling path with the upsampling path. However, this brings across many redundant low-level feature extractions, as feature representation is poor in the initial layers.Soft attention implemented at the skip connections will actively suppress activations in irrelevant regions, reducing the number of redundant features brought across.The attention gates introduced by Oktay et al. uses additive soft attention.a. Breakdown of attention gatesOktay et al. also proposed a grid-based gating mechanism, which takes the g vector from the upsampling path rather than the downsampling path (except for the lowest layer), as the vector would have been conditioned to spatial information from multiple scales by previous attention gates.As seen in the figure above, the network learns to focus on the desired region as training proceeds. The differentiable nature of the attention gate allows it to be trained during backpropagation, which means the attention coefficients get better at highlighting relevant regions.b. Implementation in KerasResults obtained by Oktay et al. show that the Attention U-Net has outperformed a plain U-Net in the overall Dice Coefficient Score by a sizeable margin. While the Attention U-Net has more parameters, it is not significantly more and the inference time is only marginally longer.In conclusion, attention gates are a simple way to improve the U-Net consistently in a large variety of datasets without a significant overhead in terms of computational cost.To get the full implementation of a U-Net with attention, recurrence and inception layers pre-implemented, please check https://github.com/robinvvinod/unet/.",01/05/2020,0,22.0,11.0,831.0,388.0,5.0,4.0,0.0,4.0,en
4091,Overview of Conditional Random Fields,ML 2 Vec,Ravish Chawla,410.0,7.0,1194.0,"Conditional Random Fields are a discriminative model, used for predicting sequences. They use contextual information from previous labels, thus increasing the amount of information the model has to make a good prediction. In this post, I will go over some topics that will introduce CRFs. I will go over:Machine Learning models have two common categorizations, Generative and Discriminative. Conditional Random Fields are a type of Discriminative classifier, and as such, they model the decision boundary between the different classes. Generative models, on the other hand, model how the data was generated, which after having learnt, can be used to make classifications. As a simple example, Naive Bayes, a very simple and popular probabilistic classifier, is a Generative algorithm, and Logistic Regression, which is a classifier based on Maximum Liklihood estimation, is a discriminative model. Let’s look at how these models can be used to calculate label predictions:Naive Bayes classifier is based on the Naive Bayes algorithm, which states the following:The prediction we are trying to make with the classifier can be represented as a conditional probability, which we can use the Naive Bayes algorithm to decompose:Logistic Regression classifier is based on the Logistic function, known as follows:To learn the decision boundary between the two classes in Logistic Regression, the classifier learns weights associated with each data point (Theta values), and written as follows:With the Logistic Regression classifier, we see that we are maximizing the conditional probability, which by applying the Bayes rule, we can obtain the Generative classifier that is used by the Naive Bayes classifier.We replace P(y | x) with the bayes equation:and set it equivalent to the product of the prior and the liklihood, since in an arg-max, the denominator P(x) does not contribute any information.The result is the Generative classifier obtained earlier for Naive Bayes algorithm.We can further see that P(x | y) * P(y) is equal to P(x, y), the joint distribution of x and y. This observation supports the earlier definition of Generative classifiers. By modeling the joint probability distribution between the classes, the Generative model can be used to obtain and “generate” the input points X, given the label Y and the joint probabality distribution. Similarly, the discriminative model, by learning the conditional probability distribution, has learnt the decision boundary that separates the data points. So, given an input point, it can use the conditional probabality distribution to calculate it’s class.How do these definitions apply to Conditional Random Fields? Conditional Random Fields are a Discriminative model, and their underlying principle is that they apply Logistic Regression on sequential inputs. If you are familiar with Hidden Markov Models, you will find that they share some similarities with CRFs, one in that they are also used for sequential inputs. HMMs use a transition matrix and the input vectors to learn the emission matrix, and are similar in concept to Naive Bayes. HMMs are a Generative model.Having discussed the above definitions, we will now go over Conditional Random Fields, and how they can be used to learn sequential data.As we showed in the previous section, we model the Conditional Distribution as follows:In CRFs, our input data is sequential, and we have to take previous context into account when making predictions on a data point. To model this behavior, we will use Feature Functions, that will have multiple input values, which are going to be:We define the feature function as:The purpose of the feature function is to express some kind of characteristic of the sequence that the data point represents. For instance, if we are using CRFs for Parts-of-Speach tagging, thanf (X, i, L{i - 1}, L{i} ) = 1 if L{i - 1} is a Noun, and L{i} is a Verb. 0 otherwise.Similarly, f (X, i, L{i - 1}, L{i} ) = 1 if L{i - 1} is a Verb and L{i} is an Adverb. 0 otherwise.Each feature function is based on the label of the previous word and the current word, and is either a 0 or a 1. To build the conditional field, we next assign each feature function a set of weights (lambda values), which the algorithm is going to learn:To estimate the parameters (lambda), we will use Maximum Liklihood Estimation. To apply the technique, we will first take the Negative Log of the distribution, to make the partial derivative easier to calculate:To apply Maximum Liklihood on the Negative Log function, we will take the argmin (because minimizing the negative will yield the maximum). To find the minimum, we can take the partial derivative with respect to lambda, and get:We use the Partial Derivative as a step in Gradient Descent. Gradient Descent updates parameter values iteratively, with a small step, until the values converge. Our final Gradient Descent update equation for CRF is:As a summary, we use Conditional Random Fields by first defining the feature functions needed, initializing the weights to random values, and then applying Gradient Descent iteratively until the parameter values (in this case, lambda) converge. We can see that CRFs are similar to Logistic Regression, since they use the Conditional Probability distribution, but we extend the algorithm by applying Feature functions as our sequential inputs.From the previous sections, it must be obvious how Conditional Random Fields differ from Hidden Markov Models. Although both are used to model sequential data, they are different algorithms.Hidden Markov Models are generative, and give output by modeling the joint probability distribution. On the other hand, Conditional Random Fields are discriminative, and model the conditional probability distribution. CRFs don’t rely on the independence assumption (that the labels are independent of each other), and avoid label bias. One way to look at it is that Hidden Markov Models are a very specific case of Conditional Random Fields, with constant transition probabilities used instead. HMMs are based on Naive Bayes, which we say can be derived from Logistic Regression, from which CRFs are derived.Given their ability to model sequential data, CRFs are often used in Natural Language Processing, and have many applications in that area. One such application we discussed is Parts-of-Speech tagging. Parts of speech of a sentence rely on previous words, and by using feature functions that take advantage of this, we can use CRFs to learn how to distinguish which words of a sentence correspond to which POS. Another similar application is Named Entity recognition, or extracting Proper nouns from sentences. Conditional Random Fields can be used to predict any sequence in which multiple variables depend on each other. Other applications include parts-recognition in Images and gene prediction.To read more about Conditional Random Fields and other topics discussed in this post, refer to the following links:https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm [There are several answers in this post that provide a good overview of Discriminative and Generative algorithms.]https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf [A detailed review between the differences in Naive Bayes and Logistic Regression.]http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf [Section 2.2 on Discriminative and Generative classifiers, and how they apply to Naive Bayes vs Logistic Regression.]https://prateekvjoshi.com/2013/02/23/why-do-we-need-conditional-random-fields/ and https://prateekvjoshi.com/2013/02/23/what-are-conditional-random-fields/ [These are both very good posts on introduction to Conditional Random Fields and how they differ from graphical models (like HMMs)]http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/ [A light overview of Conditional Random Fields.]http://www.lsi.upc.edu/~aquattoni/AllMyPapers/crf_tutorial_talk.pdf [Slide deck that goes over Conditional Random Fields, how to define feature functions, and steps on it’s derivation.]",07/08/2017,0,9.0,7.0,375.0,151.0,12.0,2.0,0.0,7.0,en
4092,The Charm of a Gritty City,Bonsal Capital,Frank Bonsal III,3000.0,8.0,1377.0,"Some people ask me Why Baltimore? Couldn’t you do what you do from anywhere in the U.S.? Isn’t this the place where the acclaimed HBO series The Wire was filmed? Has Baltimore ever been on a tech entrepreneur-friendly list? Aren’t there more voluminous entrepreneurial hubs? While the concise response is pegged to an authentic and ever-congealing entrepreneurial ecosystem more focused on the act of doing (the scoreboard) than a Top Ten List, the more interesting answer is found in an array of professional and personal attributes. Let me paint a picture as to why Baltimore is a great city to build a business, a career, a life.Baltimore has a nearly three hundred year history of resilience and determination. What many do not grasp is that Baltimore is a top twenty U.S. city by population with a small town feel. Okay Cheers (“Where Everybody Knows Your Name”) was set in Boston but could easily have been at Mother’s in Federal Hill or The Horse You Came In On in Fells Point. But while Baltimore was once the towering urban hub over its Potomac River cousin, the District of Columbia, it is now a federal feeder of sorts. While it was once a thriving manufacturing, railroad and port town, a conduit for most things Midwest, now the maker movement portends to enliven some manufacturing roots but is well overshadowed by service orientation at the feet of university, healthcare and federal institutions.Baltimore has a rich and continuous history of supporting and imbuing music and the arts. From the Peabody Institute and the Baltimore Symphony Orchestra to the Modell Center at the Lyric Opera, Baltimore ‘gets’ classical music across at least two centuries. In 2008, the Rolling Stone chose Baltimore as the best music scene in the Country, not for classical but for contemporary music. For the visual arts, MICA, the Baltimore Museum of Art, and the Walters Art Museum have international renown. The more recent Visionary Arts Museum depicts Baltimore’s unique acceptance of the macabre. Even the rich film scene has long utilized Baltimore; Baltimoreans Barry Levinson (Diner, Tin Men, Avalon) and John Waters (Hairspray and Serial Mom) are some of the main reasons Baltimore has attracted many Hollywood and New York filmmakers.No city is complete without a lively sports offering. Baltimore has long been a baseball and football town; after ‘replacing’ the Colts with the Ravens and a vastly improved performance by the Orioles, the citizenry cheers loudly again. We’ve had long tenures with baseball hall of famers such as Jim Palmer, Brooks Robinson , Eddie Murray, Cal Ripken, Jr., and Frank Robinson.On the gridiron in two organizations over a decade apart, we cheered on several championship teams by the likes of Johnny Unitas and Raymond Berry and have been amazed by the Ravens rise to prominence on the backs of legends such as Ray Lewis and Jonathan Ogden. These men and others are indelible reasons why Baltimore has tenacious sports fans and whose teams are apt metaphors for the city’s style: A never give in mentality, the underdog, the lesser respected opponent. Of course, there is also great college and high school lacrosse and acclaimed horse racing, not to mention Baltimore as the birthplace of Babe Ruth and Michael Phelps who did fairly well in their own right.At its essence, what truly defines the City of Baltimore is its neighborhoods. The residential and workplace hubs are charming in places, desolate in others. Each of Baltimore’s 300+ officially designated neighborhoods has its own uniqueness, authenticity that, in aggregate, produces a diverse and distributed urban community, adorned with crime and tattered building in in places, vibrant, safe and family-centric in others. Baltimore is a very real depiction of an urban U.S. city struggling with the tension of right versus wrong, affluence versus poverty, at the center of a 2.7 million metro region.As in most mature urban centers, institutions of higher education play a critical part of Baltimore’s economy, its cultural legacy. Johns Hopkins University and Johns Hopkins Hospital are world renowned and together are the largest single employer in the city. The University of Maryland at Baltimore, the University of Baltimore, Loyola University Maryland and other postsecondary institutions have long been key determinants of sustainability and intellectual and social design for the region.The Greater Baltimore metro area has a several decades long history of tech company incubation, pegged to government and university economic development, as well as out of private and corporate coffers. The first Maryland business incubator, today called Mtech, was founded in 1983 at the University of Maryland, College Park’s Clark School of Engineering.The Maryland Business Incubation Association, Maryland TEDCO and Maryland DBED (Department of Business and Economic Development) indicate that the State has over 20 incubators, 7 in greater Baltimore. Since 2002, Maryland incubators have created nearly 12,000 jobs, the fifth largest job producer in Maryland.UMBC (University of Maryland, Baltimore County) created bwtech@UMBC Research and Technology Park in 1989 and has led the way for scaled university incubation, particularly in cyber security with the 2011 launch of its Cyber Incubator.The Baltimore Development Corp-backed ETC (Emerging Technology Centers) began in 1999, at the run-up of the dot-com bubble, and has proven sustain worthy with graduate winners such as CSA Medical, Millenial Media, Moodlerooms, R2 Integrated, Straighterline, and Visicu. The ETC has two sites: one at the old Eastern High School, the south side of the site of former Memorial Stadium and a new site in Highlandtown in an economic zone.In a partnership with Baltimore County in 2007, Towson University launched Towson Global Business Incubator, now TU Incubator, with an international differentiation and very definite interoperability with the TU undergraduate population. A more recent tilt to edtech, born largely by yours truly, provides two legs of a generalist incubator eight miles north of downtown Baltimore, serving Baltimore County and points north.Howard County Economic Development launched the Maryland Center for Entrepreneurship in 2011 and is showing the fruits of thoughtful design and implementation with the 2013 launch of 3D Maryland and the Conscious Venture Lab.A group of private investors and entrepreneurs launched Betamore in 2012 to much acclaim. The ‘incubator’ is more of a community and adult training hub for all things tech than a classic business incubator. Betamore has attracted over over twenty seed and early stage businesses since inception and is a key cog in the Federal Hill and Baltimore Inner Harbor entrepreneurial community.Alongside the formal incubator community is an active and ever-growing seed funding and mentor community. Angels are diffuse but dozens are active in the Baltimore Angels Network. With regard to angels, Baltimore is more about who you know than where to go. Of course, the incubator network, a slew of meetups, and Maryland TEDCO, the Propel Baltimore Fund and the Maryland Venture Fund and the Invest Maryland Challenge as part of Invest Maryland and are valid destinations for the right seed or late seed stage equity or grant opportunities.Baltimore has produced many inspirational stories of entrepreneurship and personal disruption, of the socioeconomic and social impact journey going from a little to a lot. Since the dot-com downturn, Baltimore has availed such winning stories as Kevin Plank (UnderArmour), Vince Talbert (Bill Me Later, now GiveCorps), Scott Ferber (Advertising.com, now Videology), and Wes Moore (The Other Wes Moore, stealth startup). Baltimore represents a gritty city where entrepreneurial intent can put a dent in the world, can be the hub of most things right with the world.On the personal side of Why Baltimore, there’s a longer story to be told that stems from more of the give, than the get. Simply put, my return to Baltimore ten years ago has availed a successful career as an edupreneur and more recently a facilitator of university entrepreneurship. Most of all, this Charming Gritty City and its concentric rings have provided an environment from which to build and grow career, family, friendships for the long haul. For that, I am eternally thankful.Thanks for reading. If you enjoyed this article, please hit the 👏🏼 button below so other prospective readers can see it, too.I am a contrarian educator working at the nexus of impact, efficacy, and economic upside. From Baltimore, as Towson University’s Director of Venture Creation, I help support Maryland’s largest cluster of edtech companies via TU Incubator and associated programs.",26/01/2014,0,27.0,22.0,481.0,300.0,23.0,0.0,0.0,42.0,en
4093,Understanding Latent Space in Machine Learning,Towards Data Science,Ekin Tiu,646.0,9.0,1634.0,"If I have to describe latent space in one sentence, it simply means a representation of compressed data.Imagine a large dataset of handwritten digits (0–9) like the one shown above. Handwritten images of the same number (i.e. images that are 3’s) are the most similar to each other compared to other images of different numbers (i.e. 3s vs. 7s). But can we train an algorithm to recognize these similarities? How?If you have trained a model to classify digits, then you have also trained the model to learn the ‘structural similarities’ between images. In fact, this is how the model is able to classify digits in the first place- by learning the features of each digit.If it seems that this process is ‘hidden’ from you, it’s because it is. Latent, by definition, means “hidden.”The concept of “latent space” is important because it’s utility is at the core of ‘deep learning’ — learning the features of data and simplifying data representations for the purpose of finding patterns.Intrigued? Let’s break latent space down bit by bit:Why do we compress data in ML?Data compression is defined as the process of encoding information using fewer bits than the original representation. This is like taking a 19D data point (need 19 values to define unique point) and squishing all that information into a 9D data point.More often than not, data is compressed in machine learning to learn important information about data points. Let me explain with an example.Say we would like to train a model to classify an image using a fully convolutional neural network (FCN). (i.e. output digit number given image of digit). As the model ‘learns’, it is simply learning features at each layer (edges, angles, etc.) and attributing a combination of features to a specific output.But each time the model learns through a data point, the dimensionality of the image is first reduced before it is ultimately increased. (see Encoder and Bottleneck below). When the dimensionality is reduced, we consider this a form of lossy compression.Because the model is required to then reconstruct the compressed data (see Decoder), it must learn to store all relevant information and disregard the noise. This is the value of compression- it allows us to get rid of any extraneous information, and only focus on the most important features.This ‘compressed state’ is the Latent Space Representation of our data.What do I mean by space?You may be wondering why we call it a latent space. After all, compressed data, at first glance, may not evoke any sort of “space.”But here’s the parallel.In this rather simplistic example, let’s say our original dataset are images with dimensions 5 x 5 x 1. We will set our latent space dimensions to be 3 x 1, meaning our compressed data point is a vector with 3-dimensions.Now, each compressed data point is uniquely defined by only 3 numbers. That means we can graph this data on a 3D Plane (One number is x, the other y, the other z).This is the “space” that we are referring to.Whenever we graph points or think of points in latent space, we can imagine them as coordinates in space in which points that are “similar” are closer together on the graph.A natural question that arises is how would we imagine space of 4D points or n-dimensional points, or even non-vectors (since the latent space representation is NOT required to be 2 or 3-dimensional vectors, and is oftentimes not since too much information would be lost).The unsatisfying answer is, we can’t. We are 3-dimensional creatures that cannot fathom n-dimensional space (such that n > 3). However, there are tools such as t-SNE which can transform our higher dimensional latent space representations into representations that we can visualize (2D or 3D). (See Visualizing Latent Space section below.)But you may be wondering, what are ‘similar’ images, and why does reducing the dimensionality of our data make similar images ‘closer’ together in space?What do I mean by similar?If we look at three images, two of a chair and one of a desk, we would easily say that the two chair images are the most similar whereas the desk is the most different from either of the chair images.But what makes these two chair images “more similar?” A chair has distinguishable features (i.e. back-rest, no drawer, connections between legs). These can all be ‘understood’ by our models by learning patterns in edges, angles, etc.As explained, such features are packaged in the latent space representation of data.Thus, as dimensionality is reduced, the ‘extraneous’ information which is distinct to each image (i.e. chair color) is ‘removed’ from our latent space representation, since only the most important features of each image are stored in the latent space representations.As a result, as we reduce dimensionality, the representations of both chairs become less distinct and more similar. If we were to imagine them in space, they would be ‘closer’ together.*Please note that the ‘closeness’ metric I have referred to throughout the article is an ambiguous term and not a definitive Euclidian distance, because there are multiple definitions of distance in space.The latent space concept is definitely intriguing. But how is it used? When do we use it? And most importantly, why?What we’ll find is that the latent space is ‘hidden’ in many of our favorite image processing networks, generative models, etc.Although the latent space is hidden from most, there are certain tasks in which understanding the latent space is not only helpful, but necessary.The latent space representation of our data contains all the important information needed to represent our original data point.This representation must then represent the features of the original data.In other words, the model learns the data features and simplifies its representation to make it easier to analyze.This is at the core of a concept called Representation Learning, defined as a set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data.In this use case, our latent space representations are used to transform more complex forms of raw data (i.e. images, video), into simpler representations which are ‘more convenient to process’ and analyze.Listed below are specific instances of representation learning.The latent space is an essential concept in manifold learning, a subfield of representation learning.Manifolds in data science can be understood as groups or subsets of data that are ‘similar’ in some way.These similarities, usually imperceptible or obscured in higher-dimensional space, can be discovered once our data has been represented in the latent space.Take the example of a ‘swiss roll’ below.In 3D, we know that there are groups of similar data points that exist, but it is much more difficult to delineate such groups with higher dimensional data.By reducing the dimensionality of our data to 2D, which in this case could be considered a ‘latent space’ representation, we are able to more easily distinguish the manifolds (groups of similar data) in our dataset.To learn more about manifolds and manifold learning, I recommend the following articles:towardsdatascience.comscikit-learn.orgA common type of deep learning model that manipulates the ‘closeness’ of data in the latent space is the autoencoder — a neural network that acts as an identity function. In other words, an autoencoder learns to output whatever is inputted.Now, if you’re new to the field, you may be wondering, why in the world would we need a model that does this? It seems rather useless if all it outputs is itself…Though this reasoning is valid, we don’t care so much about what the model outputs. We care more about what the model learns in the process.When we force a model to become an identity function, we are forcing it to store all of the data’s relevant features in a compressed representation so that there is enough information in that compressed form such that the model can ‘accurately’ reconstruct it. Sound familiar? It should, because this compressed representation is our latent space representation (red block in image above).We have seen how patterns can be more easily discovered in the latent space since similar data points will tend to cluster together, but we have not yet seen how we can sample points from this latent space to seemingly generate ‘new’ data.In the example above, we can generate different facial structures by interpolating on latent space, and using our model decoder to reconstruct the latent space representation into a 2D image with the same dimensions as our original input.What do I mean by interpolating on latent space?Let’s say that I have compressed the chair images from the previous section into the following 2D vectors, [0.4, 0.5] and [0.45, 0.45]. Let’s say the desk is compressed to [0.6, 0.75]. If I were to interpolate on latent space, I would sample points in latent space between the ‘chair’ cluster and the ‘desk’ cluster.We can feed these sampled 2D vectors into the model’s decoder, and voila! We get ‘new’ images that look like a morph between a chair and a desk. *new is in quotes because these generated images are not technically independent of the original data sample.Below is an example of linear interpolation between two types of chairs in latent space.Image generation is still an active area of research, and the latent space is an essential concept that must be understood. See the following articles for more use cases of generative models, and a hands-on example of latent space interpolation using a GAN (Generative Adversarial Network), another generative model that uses latent space representations.machinelearningmastery.commachinelearningmastery.comFor more on latent space visualization, I recommend Hackernoon’s article which provides a hands-on example of visualizing similarities between digit images in a 2D space with the t-SNE algorithm.hackernoon.comWhile learning about the latent space, I was fascinated by this ‘hidden,’ yet essential concept. I hope that this article demystified the latent space representation, and provided the ‘deeper understanding’ of deep learning that I longed for as a novice.",04/02/2020,0,15.0,43.0,885.0,659.0,14.0,1.0,0.0,7.0,en
4094,Bitcoin Bonanza!,Towards Data Science,Ryan Burke,137.0,9.0,1936.0,"I have been following crypto prices for several years now. I am fascinated with the evolution of the blockchain and its implications. I’ve chuckled more than once at the idea of digital currency. Not that it’s new, but I was born in the 80’s when we had to fill out a paper and speak with a human if we wanted to withdraw actual paper money…Remember paper money?In any case, today I want to share one of my recent projects with you. I will be comparing three models to determine their efficacy at predicting the price of Bitcoin, the King of Crypto. For this project, I used gated recurrent units (GRU), long short term memory units (LSTM), and bidirectional LSTM units (BiLSTM). First, let’s take a quick dive into the workings of these mysterious predictive models.Recurrent neural networks (RNNs) have been heavily studied since the 90’s. The bread-and-butter of RNNs is their capacity to deal with sequential data, making them valuable tools to explore tasks involving text¹, audio², images³, videos⁴, and financial markets, such as the one we are about to explore. For a fun way to learn how RNNs work, check out this creative post!A limitation of RNNs becomes clear when we begin using large amounts of data⁵. For example, for this article, I have gathered data from roughly 7 years of Bitcoin prices. This means I have more than 2,500 timesteps of input. Consequently, every time the model is updated, derivatives will be calculated on each of those inputs. This can cause the weights to drop close to zero (vanishing gradient) or explode (exploding gradient) meaning the model is slow to learn. Because RNNs have difficulty learning early inputs on large datasets, they are said to have short-term memory.To overcome this problem, specialized RNNs were developed. Long short term memory units (LSTM), bidirectional LSTM (BiLSTM), and gated recurrent units (GRU). Using internal mechanisms, called gates, these models can regulate how information flows through the network. Ultimately, they decide which information is important to keep or discard (If you want to go deeper, you can check out this post).Now that we have covered some of the key concepts, let’s get to work and see how these models perform as predictors of Bitcoin prices!To begin, let’s take a look at our data. Figure 1 below presents the evolution of Bitcoin price over time. Although it’s quite volatile, I must admit that I’m a little bummed that I didn’t purchase Bitcoin in 2013!Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.There are several important steps required before we can start building our model. The first step is to shift our outcome variable up (-1) by one timestep. The purpose is to organize our data such that the predictor variables will be used to predict the following days closing price, TOMORROW_CLOSE.Since I didn’t want to impute the empty cell with a value that doesn’t exist, I filled it with 0 and dropped it from the dataset. An alternative method would be to shift all of the predictor values down (+1). This would have resulted in a 0 being inserted at the beginning of the dataset, which would have been close the the initial value in 2013.Next, using df.dtypes we can inspect the types of variables present in our dataset. This is always a good idea when using .csv (or other) file types to ensure the column has been appropriately read. For example, the ‘Date’ column was in a date format in Excel, yet when imported it was interpreted as an object. Therefore, we must convert it to a datetime using the code below.Next, we have to set the column ‘Date’ as the index. If you skip this step, you will get the following error: invalid type promotion. This step was a real chin-scratcher for me. Try typing ‘invalid type promotion’ into Google and you will find 29,100,000 results!Just as a side note, you could also set the index column when you read your .csv file using index_col = ‘Date’Now our data is ready to split into a training set and a test set. I chose an 80/20 split for training and test data, respectively. Let’s take a look at what that gives us (figure 2).So far, everything looks good! Now we need to separate our predictor (X) and outcome (y) variables before we start to train our model.A good practice for many machine learning models is to scale numeric variables to a specified range. Since the default activation function for these models is the hyperbolic tangent (tanh) function, which outputs a range from -1 to 1, I scaled values to fit this range. To accomplish this, I used sklearn’s MinMaxScaler. It is considered good practice to follow these 3 guidelines:Input data for our models must be 3D with a shape [batch, timestep, feature] (see Keras instructions here). Below, a function called threeD_dataset reshapes the data into the necessary format. The timestep was set to 10, which means the model will make predictions of ‘TOMORROW_CLOSE’ based on the input from the 10 previous days.Finally, we will create our 3 models that will be used to predict Bitcoin price. All models were kept simple with two layers containing 64 neurons and a dense layer with 1 neuron. I also added 20% dropout to prevent over-fitting. Finally, I used adam as the optimizing function and mean square error as the metric for these models. For more information on how these layers & functions work, check out my last article where I go into quite a bit of detail!Now it’s time to fit the models and see how well they predict Bitcoin prices! I will be running 100 epoch for these models; however, I added EarlyStopping with a patience of 10, which stops the model from continuing when the validation loss hasn’t improved in 10 epochs. This can save a fair amount of time, especially if you’re working with enormous datasets.It’s important to note that because we are working with time-series data, where the order of the timesteps are meaningful, we must set shuffle = False.Before we can make predictions on our data, we must use an inverse transform to return our outcome variable back to its original form. Recall that we scaled the data to a range from -1 to 1. If we don’t use an inverse function, then our predictions will be within this scaled range.Now is the moment we’ve all been waiting for! We will see which model performed best to predict Bitcoin price on the unseen test set. The results from these trials are presented in figure 3 below.As we can see, when inspected visually both the predicted values from the LSTM and BiLSTM follow the test data remarkably well until the sharp rise in values toward the end of the data set. On the other hand, the GRU model appears to have the best overall fit. Even if it doesn’t tightly follow the true data, it seems to be better at following the trends present in the data.Above, a qualitative interpretation of the data was presented. Here, a function was created to provide a quantitative measure of accuracy for each model. Both Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are provided for all three models. For an explanation of these two metrics, check out this article.How should we interpret these values? First, let me say that I am not a professional trader. I can only provide an opinion. It appears that the MAE is more in line with the tight fit observed above in figure 3. Both the BiLSTM and LSTM have lower MAE values, indicating a more accurate model (when using this metric). Conversely, the GRU has the lower RMSE suggesting a more accurate model (using this metric).The way I look at it, trading is not like the lottery. The goal is not to know exactly what the future price of an asset may be. Rather, a better strategy would be to predict the direction of the future price.For example, knowing that tomorrow 1 Bitcoin would be worth 42,749.67 is not useful when it has no context. However, if I know that today 1 Bitcoin is worth 39,118.33, the predicted upward trend would be more informative. This would guide my decision to buy or sell.With that in mind, I feel that the GRU model performed better as an indicator of trend, rather than price. Next, we will use the GRU model to forecast future prices!I often use these three models when investigating time series data, and I have to admit that I’m surprised every time. If you look back at figure 2, which plots the training set and test set, they don’t share that much in common. The fact that the models could learn patterns in the training set that would be useful to predict relatively different patterns in the test test is incredible!In this final section, I waited until enough data accumulated to forecast Bitcoin values using a new dataset. Since I used a timestep of 10, I needed at least 10 additional days. The previous dataset finished on January 12, 2021, however, after shifting the closing price up it was removed from the dataset to avoid having to impute the value with an imaginary one. Therefore, the forecasting data set contains data from January 12 - 23, 2021.It’s important to mention that all the same data preparation methods were applied to the new forecasting dataset. I omitted them from the text to avoid repetitive material.Below, in figure 4, you will find the results from my quest to predict future values of Bitcoin. I had enough data to predict 2 future values.In this article, I tested the efficacy of LSTM, BiLSTM, and GRU models on predicting Bitcoin prices. After providing a general overview of the models, I described methods to prepare the data to avoid the dreaded cryptic error messages.After training the models, they were each put to the test. Each performed rather well, but the GRU model was found to be a better indicator of trend, which I felt would be more relevant from a trading perspective.When the GRU model was used to forecast values, it performed quite well. Although the prices were not predicted exactly, the trend was on point!If you want to play with these models, check out the full notebook on my Github. I wanted to keep everything simple, but the model could certainly be improved. Try:I had a lot of fun writing this article. Thank you so much for taking the time to read it!And remember that the best way to approach your research questions is with childlike curiosity! Have fun!1. Wang, F., Guo, Q., Lei, J., & Zhang, J. (2017). Convolutional recurrent neural networks with hidden Markov model bootstrap for scene text recognition. IET Computer Vision, 11(6), 497–504.2. Sak, H., Senior, A., Rao, K., & Beaufays, F. (2015). Fast and accurate recurrent neural network acoustic models for speech recognition. arXiv preprint arXiv:1507.06947.3. Mou, L., Ghamisi, P., & Zhu, X. X. (2017). Deep recurrent neural networks for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing, 55(7), 3639–3655.4. Güera, D., & Delp, E. J. (2018, November). Deepfake video detection using recurrent neural networks. In 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) (pp. 1–6). IEEE.5. Stérin, T., Farrugia, N., & Gripon, V. (2017). An intrinsic difference between vanilla rnns and gru models. COGNTIVE 2017, 84.",25/01/2021,0,1.0,15.0,1400.0,708.0,5.0,7.0,0.0,13.0,en
4095,The Complete Beginner’s Guide To Chatbots,Chatbots Magazine,Matt Schlicht,39000.0,11.0,1744.0,"What are chatbots? Why are they such a big opportunity? How do they work? How can I build one? How can I meet other people interested in chatbots?These are the questions we’re going to answer for you right now.Ready? Let’s do this.(Do you work in ecommerce? Stop reading and click here, we made something for you.)(p.s. here is where I believe the future of bots is headed, you will probably disagree with me at first.)(p.p.s. My newest guide about conversational commerce is up, I think you’ll find it super interesting.)“~90% of our time on mobile is spent on email and messaging platforms. I would love to back teams that build stuff for places where the consumers hang out!” — Niko Bonatsos, Managing Director at General CatalystA chatbot is a service, powered by rules and sometimes artificial intelligence, that you interact with via a chat interface. The service could be any number of things, ranging from functional to fun, and it could live in any major chat product (Facebook Messenger, Slack, Telegram, Text Messages, etc.).“Many businesses already have phone trees and they do work though most users get grumpy using them. Text based response trees are much easier and faster and that is what I expect a lot of early bot interactions to be. Sometimes with ability to chat with a live person.” — Josh Elman, Partner at GreylockIf you haven’t wrapped your head around it yet, don’t worry. Here’s an example to help you visualize a chatbot.If you wanted to buy shoes from Nordstrom online, you would go to their website, look around until you find the shoes you wanted, and then you would purchase them.If Nordstrom makes a bot, which I’m sure they will, you would simply be able to message Nordstrom on Facebook. It would ask you what you’re looking for and you would simply… tell it.Instead of browsing a website, you will have a conversation with the Nordstrom bot, mirroring the type of experience you would get when you go into the retail store.Watch this video from Facebook’s recent F8 conference (where they make their major announcements). At the 7:30 mark, David Marcus, the Vice President of Messaging Products at Facebook, explains what it looks like to buy shoes in a Facebook Messenger bot.Buying shoes isn’t the only thing chatbots can be used for. Here are a couple of other examples:See? With bots, the possibilities are endless. You can build anything imaginable, and I encourage you to do just that.But why make a bot? Sure, it looks cool, it’s using some super advanced technology, but why should someone spend their time and energy on it?It’s a huge opportunity. HUGE. Scroll down and I’ll explain.You are probably wondering “Why does anyone care about chatbots? They look like simple text based services… what’s the big deal?”Great question. I’ll tell you why people care about chatbots.It’s because for the first time ever people are using messenger apps more than they are using social networks.Let that sink in for a second.People are using messenger apps more than they are using social networks.“People are now spending more time in messaging apps than in social media and that is a huge turning point. Messaging apps are the platforms of the future and bots will be how their users access all sorts of services.” — Peter Rojas, Entrepreneur in Residence at BetaworksSo, logically, if you want to build a business online, you want to build where the people are. That place is now inside messenger apps.“Major shifts on large platforms should be seen as an opportunities for distribution. That said, we need to be careful not to judge the very early prototypes too harshly as the platforms are far from complete. I believe Facebook’s recent launch is the beginning of a new application platform for micro application experiences. The fundamental idea is that customers will interact with just enough UI, whether conversational and/or widgets, to be delighted by a service/brand with immediate access to a rich profile and without the complexities of installing a native app, all fueled by mature advertising products. It’s potentially a massive opportunity.” — Aaron Batalion, Partner at Lightspeed Venture PartnersThis is why chatbots are such a big deal. It’s potentially a huge business opportunity for anyone willing to jump headfirst and build something people want.“There is hope that consumers will be keen on experimenting with bots to make things happen for them. It used to be like that in the mobile app world 4+ years ago. When somebody told you back then… ‘I have built an app for X’… You most likely would give it a try. Now, nobody does this. It is probably too late to build an app company as an indie developer. But with bots… consumers’ attention spans are hopefully going to be wide open/receptive again!” — Niko Bonatsos, Managing Director at General CatalystBut, how do these bots work? How do they know how to talk to people and answer questions? Isn’t that artificial intelligence and isn’t that insanely hard to do?Yes, you are correct, it is artificial intelligence, but it’s something that you can totally do yourself.Let me explain.There are two types of chatbots, one functions based on a set of rules, and the other more advanced version uses machine learning.What does this mean?Chatbot that functions based on rules:Chatbot that functions using machine learning:“Beware though, bots have the illusion of simplicity on the front end but there are many hurdles to overcome to create a great experience. So much work to be done. Analytics, flow optimization, keeping up with ever changing platforms that have no standard. For deeper integrations and real commerce like Assist powers, you have error checking, integrations to APIs, routing and escalation to live human support, understanding NLP, no back buttons, no home button, etc etc. We have to unlearn everything we learned the past 20 years to create an amazing experience in this new browser.” — Shane Mac, CEO of AssistBots are created with a purpose. A store will likely want to create a bot that helps you purchase something, where someone like Comcast might create a bot that can answer customer support questions.“Messaging is where we spend a ton of our time and expect to communicate. It is ridiculous we still have to call most businesses.” — Josh Elman, Partner at GreylockYou start to interact with a chatbot by sending it a message. Click here to try sending a message to the CNN chatbot on Facebook.So, if these bots use artificial intelligence to make them work well… isn’t that really hard to do? Don’t I need to be an expert at artificial intelligence to be able to build something that has artificial intelligence?Short answer? No, you don’t have to be an expert at artificial intelligence to create an awesome chatbot that has artificial intelligence. Just make sure to not over promise on your application’s abilities. If you can’t make the product good with artificial intelligence right now, it might be best to not put it in yet.“Everyone going after AI to try make this scale seems a little too soon. Texting to a computer that doesn’t understand many things you are saying can be very aggravating. So be careful early not to over promise, and give users guard rails” — Josh Elman, Partner at GreylockHowever, over the past decade quite a bit of advancements have been made in the area of artificial intelligence, so much in fact that anyone who knows how to code can incorporate some level of artificial intelligence into their products.How do you build artificial intelligence into your bot? Don’t worry, I’ve got you covered, I’ll tell you how to do it in the next section of this post.Building a chatbot can sound daunting, but it’s totally doable. You’ll be creating an artificial intelligence powered chatting machine in no time (or, of course, you can always build a basic chat bot that doesn’t have a fancy AI brain and strictly follows rules).“The difficulty in building a chatbot is less a technical one and more an issue of user experience. The most successful bots will be the ones that users want to come back to regularly and that provide consistent value.” — Matt Hartman, Director of Seed Investments at BetaworksYou will need to figure out what problem you are going to solve with your bot, choose which platform your bot will live on (Facebook, Slack, etc), set up a server to run your bot from, and choose which service you will use to build your bot.“We believe that you don’t need to know how to program to build a bot, that’s what inspired us at Chatfuel a year ago when we started bot builder. We noticed bots becoming hyper-local, i.e. a bot for a soccer team to keep in touch with fans or a small art community bot. Bots are efficient and when you let anyone create them easily magic happens.” — Dmitrii Dumik, Founder of ChatfuelHere are a ton of resources to get you started.Platform documentation:“It’s hard to balance that urge to just dogpile the latest thing when you’re feeling like there’s a land grab or gold rush about to happen all around you and that you might get left behind. But in the end quality wins out. Everyone will be better off if there’s laser focus on building great bot products that are meaningfully differentiated.” — Ryan Block, Cofounder of Begin.comOther Resources:Don’t want to build your own?Now that you’ve got your chatbot and artificial intelligence resources, maybe it’s time you met other people who are also interested in chatbots.Chatbots have been around for decades, but because of the recent advancements in artificial intelligence and machine learning, there is a big opportunity for people to create bots that are better, faster, and stronger.If you’re reading this, you probably fall into one of these categories:Wouldn’t it be awesome if you had a place to meet, learn, and share information with other people interested in chatbots? Yeah, we thought so too.That’s why I created a forum called “Chatbot News”, and it has quickly become the largest community related to Chatbots.The members of the Chatbots group are investors who manage well over $2 billion in capital, employees at Facebook, Instagram, Fitbit, Nike, and Ycombinator companies, and hackers from around the world.We would love if you joined. Click here to request an invite private chatbots community.I have also created the Silicon Valley Chatbots Meetup, register here to be notified when we schedule our first event.",20/04/2016,0,38.0,14.0,948.0,548.0,20.0,7.0,0.0,54.0,en
4096,An Introduction To Conditional GANs (CGANs),DataDrivenInvestor,Manish Nayak,145.0,5.0,401.0,"Conditional GANs (CGANs) are an extension of the GANs model. You can read about a variant of GANs called DCGANs in my previous post here. CGANs are allowed to generate images that have certain conditions or attributes.Like DCGANs, Conditional GANs also has two components.www.datadriveninvestor.comConditional GANs (CGANs): The Generator and Discriminator both receive some additional conditioning input information. This could be the class of the current image or some other property.For example, if we train a DCGANs to generate new MNIST images, There is no control over which specific digits will be produced by the Generator. There is no mechanism for how to request a particular digit from the Generator. This problem can be addressed by a variation of GAN called Conditional GAN (CGAN). we could add an additional input layer with values of one-hot-encoded image labels.NOTE: CGANs have one disadvantage. CGANs are not strictly unsupervised and we need some kind of labels for them to work.The CGAN Discriminator’s model is similar to DCGAN Discriminator’s model except for the one-hot vector, which is used to condition Discriminator outputs. You can read about Discriminator’s Network in my previous post hereThe CGAN Generator’s model is similar to DCGAN Generator’s model except for the one-hot vector, which is used to condition Generator outputs. You can read about Generator’s Network in my previous post hereThe Discriminator has two taskWe need to calculate two losses for the Discriminator. The sum of the “fake” image and “real” image loss is the overall Discriminator loss. So the loss function of the Discriminator is aiming at minimizing the error of predicting real images coming from the dataset and fake images coming from the Generator given their one-hot labels.The Generator network has one taskThe loss function of the Generator minimizes the correct prediction of the Discriminator on fake images conditioned on the specified one-hot labels.The following steps are repeated in trainingAccompanied jupyter notebook for this post can be found here.CGANs can be used to build a model which can generate an image of an imaginary actor of given class like male or female. It can also use to build Face Aging system, Age synthesis and age progression have many practical industrial and consumer applications like cross-age face recognition, finding lost children, entertainment, visual effects in movies.I hope this article helped you get started building your own CGANs. I think it will at least provides a good explanation and understanding about CGANs.",09/05/2019,0,69.0,22.0,845.0,359.0,8.0,5.0,0.0,9.0,en
4097,Perturbation Theory in Deep Neural Network (DNN) Training,Towards Data Science,Prem Prakash,22.0,7.0,1485.0,"Vanishing Gradient, Saddle Point, Adversarial TrainingPrerequisite- this post assumes the reader has an introductory-level understanding of neural network architectures, and have trained some form of deep networks, during which might have faced some issues related to training or robustness of a model.A small perturbation or nudge in various parameters/components associated with training such as gradients, weights, inputs etc. can affect DNN training in overcoming some of the issues one might bump into, for example, vanishing gradient problem, saddle point trap, or creating a robust model to avoid malicious attacks through adversarial training etc.Typically, perturbation theory is the study of a small change in a system which can be as a result of a third object interacting with the system. For example, how the motion of a celestial (planet, moon etc.) objects around the sun is affected by other planets/moons, even though the mass of the sun is almost 99.8% of the solar system. Almost similar to this, in DNN training a small perturbation in its components- gradients, weights, inputs- is used to solve some of the issues one might encounter during training or from a trained model.Disclaimer: It should be made crystal clear that there is no formal perturbation theory in deep-learning/machine learning. Nevertheless, the use of the term ‘perturbation’ is not alien to the literature of machine learning. This has often been used to describe a nudge in encompassing component of the subject. This blog is a result of the accumulation of techniques related to perturbation in literature.A neural network is simply a way to approximate a function that takes an input and produces an output. The final trained black-box model underneath is actually a composition of many functions (f(g(x))) i.e. each hidden layers represent a function in the composition series, followed by a composition of non-linear activation function and then again hidden layer, and so on; the non-linear activation function is to induce non-linearity, otherwise, it is simply a series of matrix multiplication that can be reduced to a single matrix that can only model linear functions, for which merely one layer will suffice.Neural network parameters (weights and biases) are initialized with some initial set of values. These are then updated based on training data. The updates are performed using gradient in descent direction to find minima (well actually local minima since the function is non-convex and reaching global minima is not guaranteed), which works in practice regardless of depth or complexity of the network.To compute gradient we start from the last layer’s parameters and then propagate backward (a solved illustration back-propagation) to the first layer parameters. As we move from last to initial layers, its gradient-computation for each layer parameters increases in the number of multiplication terms (gradient of a composition of functions). This multiplication of many terms can cause vanishing of the gradient for initial layers (as the number of terms goes up higher the depth more the terms), and if these terms have value in the range of [0, 1]- which is what is the rate of change for activation functions (i.e. gradient, ignore for the moment hidden layer contribution as they may or may not be in that range), for example, sigmoid (0, 1/4), hyperbolic-tangent (0, 1) etc. Said differently, in vanishing gradient the last layer’s parameters will learn just fine but as we move towards initial layers gradient may start to vanish or is low enough that can significantly increase training time. To avoid this, one can use ReLU or its variant or batch-normalization, but here we will discuss how a little perturbation in gradient (from the paper Neelakantan et al. (2015) can also help in assuaging the problem.The perturbation in gradients is by adding gaussian-distributed noise with zero mean and decaying-variance- works better than fixed-variance, moreover, we do not want a constant perturbation even when cost function (J) value is approaching to converge. This process also helps in avoiding overfitting, and can further result in lower training loss. The perturbed gradient at every training step t is computed as follows:where decaying-variance (σ) at training step t is given as:where the value of η (eta) is typically taken to be 1 (nevertheless it can be tuned but has to between zero and one), γ parameter is set to 0.55.On an additional note, this process actually adds more stochasticity in the training process that can further help in avoiding plateau phase in the early learning process.It is a stationary point on a curve, where the shape is that of the saddle (like in horseback riding as shown in Figure 2). For example, in the curve for minimization of loss-function, a stationary point is saddle point if it is local minima when one of the axes is fixed and maxima when the other (without-the-loss-of-generality we will consider the curve in three dimensions for the purpose of explaining). From the saddle point, a shift along one of the axes the function-value increase, while on the other it decreases, whereas, for a point to be minima it should increase in all the directions it moves, and for the maxima decrease in all the directions.Sometimes, the training process can get stuck at the saddle point, since the gradient evaluates to zero which results in no update in weight-parameters (w). To escape saddle point efficiently one can perturb the weights. The perturbing of weights are conditioned on the gradient of weights, for example when L2-norm of the gradient is less than some constant value c then apply the perturbation. The perturbation is given by:where wt is weight at t’th iteration of training, and ξt is sampled uniformly from a ball centred at zero with a suitably small radius. Nevertheless, it is not necessary to add uniform-noise, one can also go ahead with gaussian-noise, however, this does not have any additional advantage empirically. Uniform-noise is used for analytical convenience. Also, it is neither necessary nor essential to add noise only when the gradient is small; it is up to us to decide when and how to perturb the weights, for example, an intermittent perturbation (every few iterations with no condition) will also work which has polynomial time guarantee.The dictionary defines adversary as a force that opposes or attacks; an opponent; an enemy; a foe. On a similar note, an adversarial example for deep learning model will be a malicious, spam, or poisonous input that can fool it to malfunction by predicting incorrect output with high-confidence.To make a neural network model robust to these adversarial examples Goodfellow et al. proposed perturbation of inputs. The perturbation produces plurality in inputs that are obtained adding input with sign value of gradient (computed with-respect-to training input) multiplied with a constant value. The creation of a perturbed image is given as:where for x is the training input, xˆ is a new perturbed image, ∇x(J) is the gradient of the loss-function (J) with respect to the training input x, ϵ is a predetermined constant value- small enough to be discarded by a data storage apparatus due to limited precision of it, and the sign function outputs 1 if the input is positive, -1 if negative and 0 if zero. This technique has recently been patented under US patent law, one can read it at this link in details. I highly recommend its reading for better clarification and also as an example of how a patent in software-algorithm is written.Training the network with perturbed input extends the input distribution with its plurality. This makes the network robust against malicious input, for example, it can help avoid pixel attack in an image classification task.We have learned how perturbation helps in solving various issues related to neural network training or trained model. Here, we have seen perturbation in three components (gradients, weights, inputs) associated with neural-network training and trained model; perturbation, in gradients is to tackle vanishing gradient problem, in weights for escaping saddle point, and in inputs to avoid malicious attacks. Overall, perturbations in different ways play the role of strengthening the model against various instabilities, for example, it can avoid staying at correctness wreckage point (Figure 1) since such position will be tested with perturbation (input, weight, gradient) which will make the model approach towards correctness attraction point.As of now, perturbation is mainly contingent to empirical-experimentation designed from intuition to solve encountering problems. One needs to experiment if perturbing a component of the training process makes sense intuitively, and further verify empirically if it helps mitigate the problem. Nevertheless, in future, we will see more on perturbation theory in deep learning or machine learning in general which might also be backed by a theoretical guarantee.References[1]. Neelakantan, Arvind, et al. “Adding gradient noise improves learning for very deep networks.” arXiv preprint arXiv:1511.06807 (2015).[2]. Jin, Chi, et al. “How to escape saddle points efficiently.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.[3]. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adversarial examples.” arXiv preprint arXiv:1412.6572 (2014).",23/03/2020,0,21.0,10.0,803.0,144.0,6.0,0.0,0.0,10.0,en
4098,A Step by Step approach to Solve DBSCAN Algorithms by tuning its hyper parameters,Medium,Mohantysandip,18.0,4.0,615.0,"DBSCAN is a clustering method that is used in machine learning to separate clusters of high density from clusters of low density region. Its a very efficient clustering algorithm as it used to segregate the data points with high density observations vs data points of low density observations in form of various clusters.It can sort the data into various shapes of clusters as well. Major challenge of using DBSCAN algorithm is to find right set hyper parameters(eps and min_samples values) to fit in to the algorithm for getting accurate result.Let’s look at a Spatial data of two dimensional coordinates (x,y) using we need to find out various possible star coagulation or dense clusters from this data.Read the input data using Pandas dataframe.An initial plotting of the data suggests few clusters are there as shown below.How to find ideal no. of clusters for the dataThere are various methods to find ideal no. of clusters in a data. One of them is to Use K means algorithm along with Silhouette distance. Silhouette metric is a distance calculation algorithm using euclidean or Manhattan distance. A Silhouette Score always ranges between -1 to 1. A high Silhouette score suggests that the objects are well matched to their own cluster and poorly matched to their neighborhood clusters. Below picture suggests no. of clusters should be 5. Now let’s look at finding the hyper parameters of DBSCAN algorithm.Using Euclidean distance method to find optimum epsilon distance(eps)Biggest challenge with DBSCAN algorithm is to find right hyper parameters(eps and min_samples values) to model the algorithm.In this method, we are trying to sort the data and try to find the distance among its neighbors to find the minimum distance between them and plot the minimum distance. This will essentially give us the elbow curve to find density of the data points and their minimum distance(eps) valuesSilhouette distance to find ideal eps value for DBSCANNow from above graph, though its not conclusive yet we are sure “epsilon(eps) value” will lie somewhere within 0.1 to 0.5. Using various possible eps values between 0.1 to 0.5 we can find silhouette score for each eps value and try to select the best eps value with highest silhouette score. Note: Here we have assumed the ‘min_samples’ parameter to be 5 which can be changed later. From below graph it suggests the eps value to be 0.2 as it gives highest silhouette score.Find the ‘min_samples’ hyper parameter through right cluster formation methodAs we have already found the ‘eps value’ to be 0.2. Now feeding that value to DBSCAN algorithm through various ranges of min samples from 1 to 10, we can find the right ‘min_samples’ value which yields us right no. of clusters. From below graph its evident ‘min_samples’ value should be 10 as it yields no. of clusters to be 5 which is proven by K means with silhouette score algorithm above.Use Optics Algorithm to prove the ‘min_samples’ value chosen is correctOrdering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data.Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. It use core distance and reachability distance method to identify the clusters. It has only one hyper parameter i.e ‘min_samples’ used to find the clusters. Internally it uses DBSCAN method. OPTICS algorithm also suggests that with ‘min_samples’ as 10, it provides us 5 clusters which is right.Build the DBSCAN algorithm using above ‘eps’ value and ‘min_samples’Using eps as 0.2 and min_samples as 10, we got 5 clusters and cluster value as -1 being the outliers.Plot the model output to find resulting clusters",12/03/2020,0,14.0,0.0,1155.0,623.0,11.0,0.0,0.0,0.0,en
4099,Deep Neural Networks for Regression Problems,Towards Data Science,Mohammed AL-Ma'amari,475.0,5.0,411.0,"Neural networks are well known for classification problems, for example, they are used in handwritten digits classification, but the question is will it be fruitful if we used them for regression problems?In this article I will use a deep neural network to predict house pricing using a dataset from Kaggle .You can download the dataset from HereI highly recommend you to try running the code using my notebook on Google colab [Here]1- Process the dataset2- Make the deep neural network3- Train the DNN4- Test the DNN5- Compare the result from the DNN to another ML algorithmFirst of all, we will import the needed dependencies :We will not go deep in processing the dataset, all we want to do is getting the dataset ready to be fed into our models .We will get rid of any features with missing values, then we will encode the categorical features, that’s it.let’s define a function to get the columns that don’t have any missing valuesGet the columns that do not have any missing values .Let’s see how many columns we gotThe correlation between the featuresFrom the correlation heat map above, we see that about 15 features are highly correlated with the target.One Hot Encode The Categorical Features :We will encode the categorical features using one hot encoding.Now, split back combined dataFrame to training data and test dataInitializers define the way to set the initial random weights of Keras layers.Define a checkpoint callback :We see that the validation loss of the best model is 18738.19We will submit the predictions on the test data to Kaggle and see how good our model is.Not bad at all, with some more preprocessing, and more training, we can do better.Now, let us try another ML algorithm to compare the results.We will use random forest regressor and XGBRegressor.Split training data to training and validation dataWe will try Random forest model first:Make a submission file and submit it to Kaggle to see the result :Now, let us try XGBoost model :Make a submission file and submit it to Kaggle to see the result :Isn’t that a surprise, I really did not think that neural networks will beat random forests and XGBoost algorithms, but let us try not to be too optimistic, remember that we did not configure any hyperparameters on random forest and XGBoost models, I believe if we did so, these two models would outscore neural networks.[Stacked Regressions : Top 4% on LeaderBoard | Kaggle]You can follow me on Twitter @ModMaamari",29/09/2018,7,18.0,0.0,910.0,483.0,7.0,7.0,0.0,11.0,en
4100,"Machine Learning for Humans, Part 2.1: Supervised Learning",Machine Learning for Humans,Vishal Maini,12800.0,13.0,2211.0,"How much money will we make by spending more dollars on digital advertising? Will this loan applicant pay back the loan or not? What’s going to happen to the stock market tomorrow?In supervised learning problems, we start with a data set containing training examples with associated correct labels. For example, when learning to classify handwritten digits, a supervised learning algorithm takes thousands of pictures of handwritten digits along with labels containing the correct number each image represents. The algorithm will then learn the relationship between the images and their associated numbers, and apply that learned relationship to classify completely new images (without labels) that the machine hasn’t seen before. This is how you’re able to deposit a check by taking a picture with your phone!To illustrate how supervised learning works, let’s examine the problem of predicting annual income based on the number of years of higher education someone has completed. Expressed more formally, we’d like to build a model that approximates the relationship f between the number of years of higher education X and corresponding annual income Y.One method for predicting income would be to create a rigid rules-based model for how income and education are related. For example: “I’d estimate that for every additional year of higher education, annual income increases by $5,000.”You could come up with a more complex model by including some rules about degree type, years of work experience, school tiers, etc. For example: “If they completed a Bachelor’s degree or higher, give the income estimate a 1.5x multiplier.”But this kind of explicit rules-based programming doesn’t work well with complex data. Imagine trying to design an image classification algorithm made of if-then statements describing the combinations of pixel brightnesses that should be labeled “cat” or “not cat”.Supervised machine learning solves this problem by getting the computer to do the work for you. By identifying patterns in the data, the machine is able to form heuristics. The primary difference between this and human learning is that machine learning runs on computer hardware and is best understood through the lens of computer science and statistics, whereas human pattern-matching happens in a biological brain (while accomplishing the same goals).In supervised learning, the machine attempts to learn the relationship between income and education from scratch, by running labeled training data through a learning algorithm. This learned function can be used to estimate the income of people whose income Y is unknown, as long as we have years of education X as inputs. In other words, we can apply our model to the unlabeled test data to estimate Y.The goal of supervised learning is to predict Y as accurately as possible when given new examples where X is known and Y is unknown. In what follows we’ll explore several of the most common approaches to doing so.The rest of this section will focus on regression. In Part 2.2 we’ll dive deeper into classification methods.Regression predicts a continuous target variable Y. It allows you to estimate a value, such as housing prices or human lifespan, based on input data X.Here, target variable means the unknown variable we care about predicting, and continuous means there aren’t gaps (discontinuities) in the value that Y can take on. A person’s weight and height are continuous values. Discrete variables, on the other hand, can only take on a finite number of values — for example, the number of kids somebody has is a discrete variable.Predicting income is a classic regression problem. Your input data X includes all relevant information about individuals in the data set that can be used to predict income, such as years of education, years of work experience, job title, or zip code. These attributes are called features, which can be numerical (e.g. years of work experience) or categorical (e.g. job title or field of study).You’ll want as many training observations as possible relating these features to the target output Y, so that your model can learn the relationship f between X and Y.The data is split into a training data set and a test data set. The training set has labels, so your model can learn from these labeled examples. The test set does not have labels, i.e. you don’t yet know the value you’re trying to predict. It’s important that your model can generalize to situations it hasn’t encountered before so that it can perform well on the test data.In our trivially simple 2D example, this could take the form of a .csv file where each row contains a person’s education level and income. Add more columns with more features and you’ll have a more complex, but possibly more accurate, model.How do we build models that make accurate, useful predictions in the real world? We do so by using supervised learning algorithms.Now let’s get to the fun part: getting to know the algorithms. We’ll explore some of the ways to approach regression and classification and illustrate key machine learning concepts throughout.“Draw the line. Yes, this counts as machine learning.”First, we’ll focus on solving the income prediction problem with linear regression, since linear models don’t work well with image recognition tasks (this is the domain of deep learning, which we’ll explore later).We have our data set X, and corresponding target values Y. The goal of ordinary least squares (OLS) regression is to learn a linear model that we can use to predict a new y given a previously unseen x with as little error as possible. We want to guess how much income someone earns based on how many years of education they received.Linear regression is a parametric method, which means it makes an assumption about the form of the function relating X and Y (we’ll cover examples of non-parametric methods later). Our model will be a function that predicts ŷ given a specific x:β0 is the y-intercept and β1 is the slope of our line, i.e. how much income increases (or decreases) with one additional year of education.Our goal is to learn the model parameters (in this case, β0 and β1) that minimize error in the model’s predictions.To find the best parameters:1. Define a cost function, or loss function, that measures how inaccurate our model’s predictions are.2. Find the parameters that minimize loss, i.e. make our model as accurate as possible.Graphically, in two dimensions, this results in a line of best fit. In three dimensions, we would draw a plane, and so on with higher-dimensional hyperplanes.Mathematically, we look at the difference between each real data point (y) and our model’s prediction (ŷ). Square these differences to avoid negative numbers and penalize larger differences, and then add them up and take the average. This is a measure of how well our data fits the line.For a simple problem like this, we can compute a closed form solution using calculus to find the optimal beta parameters that minimize our loss function. But as a cost function grows in complexity, finding a closed form solution with calculus is no longer feasible. This is the motivation for an iterative approach called gradient descent, which allows us to minimize a complex loss function.“Put on a blindfold, take a step downhill. You’ve found the bottom when you have nowhere to go but up.”Gradient descent will come up over and over again, especially in neural networks. Machine learning libraries like scikit-learn and TensorFlow use it in the background everywhere, so it’s worth understanding the details.The goal of gradient descent is to find the minimum of our model’s loss function by iteratively getting a better and better approximation of it.Imagine yourself walking through a valley with a blindfold on. Your goal is to find the bottom of the valley. How would you do it?A reasonable approach would be to touch the ground around you and move in whichever direction the ground is sloping down most steeply. Take a step and repeat the same process continually until the ground is flat. Then you know you’ve reached the bottom of a valley; if you move in any direction from where you are, you’ll end up at the same elevation or further uphill.Going back to mathematics, the ground becomes our loss function, and the elevation at the bottom of the valley is the minimum of that function.Let’s take a look at the loss function we saw in regression:We see that this is really a function of two variables: β0 and β1. All the rest of the variables are determined, since X, Y, and n are given during training. We want to try to minimize this function.The function is f(β0,β1)=z. To begin gradient descent, you make some guess of the parameters β0 and β1 that minimize the function.Next, you find the partial derivatives of the loss function with respect to each beta parameter: [dz/dβ0, dz/dβ1]. A partial derivative indicates how much total loss is increased or decreased if you increase β0 or β1 by a very small amount.Put another way, how much would increasing your estimate of annual income assuming zero higher education (β0) increase the loss (i.e. inaccuracy) of your model? You want to go in the opposite direction so that you end up walking downhill and minimizing loss.Similarly, if you increase your estimate of how much each incremental year of education affects income (β1), how much does this increase loss (z)? If the partial derivative dz/β1 is a negative number, then increasing β1 is good because it will reduce total loss. If it’s a positive number, you want to decrease β1. If it’s zero, don’t change β1 because it means you’ve reached an optimum.Keep doing that until you reach the bottom, i.e. the algorithm converged and loss has been minimized. There are lots of tricks and exceptional cases beyond the scope of this series, but generally, this is how you find the optimal parameters for your parametric model.Overfitting: “Sherlock, your explanation of what just happened is too specific to the situation.” Regularization: “Don’t overcomplicate things, Sherlock. I’ll punch you for every extra word.” Hyperparameter (λ): “Here’s the strength with which I will punch you for every extra word.”A common problem in machine learning is overfitting: learning a function that perfectly explains the training data that the model learned from, but doesn’t generalize well to unseen test data. Overfitting happens when a model overlearns from the training data to the point that it starts picking up idiosyncrasies that aren’t representative of patterns in the real world. This becomes especially problematic as you make your model increasingly complex. Underfitting is a related issue where your model is not complex enough to capture the underlying trend in the data.Remember that the only thing we care about is how the model performs on test data. You want to predict which emails will be marked as spam before they’re marked, not just build a model that is 100% accurate at reclassifying the emails it used to build itself in the first place. Hindsight is 20/20 — the real question is whether the lessons learned will help in the future.The model on the right has zero loss for the training data because it perfectly fits every data point. But the lesson doesn’t generalize. It would do a horrible job at explaining a new data point that isn’t yet on the line.Two ways to combat overfitting:1. Use more training data. The more you have, the harder it is to overfit the data by learning too much from any single training example.2. Use regularization. Add in a penalty in the loss function for building a model that assigns too much explanatory power to any one feature or allows too many features to be taken into account.The first piece of the sum above is our normal cost function. The second piece is a regularization term that adds a penalty for large beta coefficients that give too much explanatory power to any specific feature. With these two elements in place, the cost function now balances between two priorities: explaining the training data and preventing that explanation from becoming overly specific.The lambda coefficient of the regularization term in the cost function is a hyperparameter: a general setting of your model that can be increased or decreased (i.e. tuned) in order to improve performance. A higher lambda value will more harshly penalize large beta coefficients that could lead to potential overfitting. To decide the best value of lambda, you’d use a method called cross-validation which involves holding out a portion of the training data during training, and then seeing how well your model explains the held-out portion. We’ll go over this in more depthHere’s what we covered in this section:In the next section — Part 2.2: Supervised Learning II — we’ll talk about two foundational methods of classification: logistic regression and support vector machines.For a more thorough treatment of linear regression, read chapters 1–3 of An Introduction to Statistical Learning. The book is available for free online and is an excellent resource for understanding machine learning concepts with accompanying exercises.For more practice:To actually implement gradient descent in Python, check out this tutorial. And here is a more mathematically rigorous description of the same concepts.In practice, you’ll rarely need to implement gradient descent from scratch, but understanding how it works behind the scenes will allow you to use it more effectively and understand why things break when they do.More from Machine Learning for Humans 🤖👶",19/08/2017,8,83.0,109.0,736.0,372.0,10.0,3.0,0.0,27.0,en
4101,Simple SGD implementation in Python for Linear Regression on Boston Housing Data,Medium,Nikhil Parmar,25.0,6.0,822.0,"Hello Folks, in this article we will build our own Stochastic Gradient Descent (SGD) from scratch in Python and then we will use it for Linear Regression on Boston Housing Dataset. Just after a short recap of SGD, we will start building our own custom SGD.To keep the concept simple and easy to understand, we will touch the math calculations in an extremely simple step by step manner with its Python Code.Then in the end we will combine all the code to solve the Linear Regression on Boston Housing Dataset.Wikipedia says: “ Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. ”Let’s begin, the Linear Regression optimization problem is to optimize or MINimize the SQUARED ERROR as shown below.But what exactly is the ERROR that needs to be minimized ?Well, the ERROR = (Actual Value - Predicted Value), so now our minimization problem looks like thisThat looks simple, isn’t it ! We know that the Actual Value is given to us in the data set and lets call our Actual Value = Yi for a particular data point i.Now for calculating the Predicted Value (~Yi), we need the Weights (Wi) and the Test data points (Xi). We need to do the multiplication of W and X and add Bias b to calculate the predicted value. Since W and X are vectors, we will take the Transpose of W ie W_T so that we can multiply it with X.Now our prediction formulation looks like this:Checkout the below small python code snippet which is the exact code for the equations. We will be providing many snippets so that we can later bundle them to build our Custom SGD.So now our final Loss function: L(W,b) that we need to minimize by finding the optimal W and b looks like this :NOTE: Here the equation is performing a summation over complete “ n ” training data points; since we are implementing SGD, we will be performing this over a random sample of BATCH SIZE “ k”, where k < nNow in order to find the optimal W, the W_j+1 will be updated constantly as per the error difference in the previous W_j.To do this we will subtract the W_j with the derivative of our above Loss Function L(W,b) with respect to W, ie:W_j+1 = W_j - Derivative(Loss_Function) with respect to “W”Here’s our Derivative(Loss_Function) w.r.t “W”:So now our update W_j+1 looks like this:NOTE: The “ r ”, is the Learning rate which is multiplied to the Derivatives, which we will divide by 2 for every iteration.Similarly our b_j+1 will also be updated as per the error difference in the previous b_j. To do this we will subtract the b_j with the derivative of our above Loss Function L(W,b) with respect to b, ie.b_j+1 = b_j — Derivative(Loss_Function) with respect to “b”Here’s our Derivative(Loss_Function) w.r.t “b”:So now our update b_j+1 looks like this:In order to obtain a good optimal value of W and b, we need to iterate the above calculations many times, therefore we keep the n_iteration= 100.As mentioned above, since we are implementing SGD, we will pick up random K points from the data to find our optimal W and b. Thus for every iteration, we need to iterate the gradient(derivative) calculations and we will update our W and for n_iteration times.Here’s the code snippet for the above process:In the above code, We are dividing the w_gradient with k to get better result. And we are also reducing the learning rate by dividing it by a small number for every iteration so as to achieve the performance similar to that of the SKLearns SGDRegressor.Cool…!!! Wasn’t that super easy ! So now we have our Custom SGD ready to use.So now we will perform Linear Regression on Boston Housing data using 2 waysThen we will compare the results.We will do the necessary imports for Linear Regression on Boston Housing Data as belowHere is our CustomSGD as shown previously:Here is a small Predict functionNow lets run our Custom SGD with the following parametersObserve the LARGE Mean Square Error(MSE) compared to the Scikit Learn’s MSE of just 27.9668 ! It is extremely high and from the plot, we can see our CustomSGD is performing very poor.Lets try to tweak the parameters and see if it helps us in improving the model performance.Wowww !!!!Observe the impressive Mean Square Error(MSE)= 27.9943 of our Custom SGD compared to the Scikit Learn’s MSE of just 27.9668 !OBSERVATION:As we can see that our Custom SGD Performed very impressive when we changed the Learning Rate and number of Iterations. Also Batch Size K plays important role. Thus these parameter are very important in the SGD and its effective selection can Break or Make result. So it is important to wisely select these parametersiPYTHON NOTEBOOK: You can download the complete iPython notebook from my Kaggle Page: https://www.kaggle.com/nikhilparmar9Thank you for reading my first article !! Hope you liked it :-)References:[1]https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/[2]https://www.kaggle.com/premvardhan/stocasticgradientdescent-implementation-lr-python[3]https://www.kaggle.com/arpandas65/simple-sgd-implementation-of-linear-regression/notebook[4]https://www.kaggle.com/tentotheminus9/linear-regression-from-scratch-gradient-descent",11/12/2019,10,66.0,16.0,863.0,279.0,13.0,3.0,0.0,5.0,en
4102,Binary Logistic Regression,Towards Data Science,Akanksha Rawat,99.0,6.0,811.0,"Have you ever come across a situation where you want to predict a binary outcome like:A very simple Machine Learning algorithm which will come to your rescue is Logistic Regression.Logistic Regression is a classification algorithm which is used when we want to predict a categorical variable (Yes/No, Pass/Fail) based on a set of independent variable(s).In the Logistic Regression model, the log of odds of the dependent variable is modeled as a linear combination of the independent variables.Let’s get more clarity on Binary Logistic Regression using a practical example in R.Consider a situation where you are interested in classifying an individual as diabetic or non-diabetic based on features like glucose concentration, blood pressure, age etc.Description of the dataFor our analysis, we’ll be using Pima Indians Diabetes database from ‘mlbench’ package in RDiabetes is the binary dependent variable in this dataset with categories — pos/neg. We have the following eight independent variablesLet’s now analyze the descriptive statistics for this dataset:It is evident from the summary statistic that there are certain missing values in the dataset, they are being highlighted as NA’s.As a conservative measure, we can remove such observations.Let’s analyze the distribution of each independent variable:From the above histograms, it is evident that the variables — Pregnant and Age are highly skewed, we can analyze them in buckets.For Age we can create following four buckets: 20–30, 31–40, 41–50 and 50+For Pregnant we can create following three buckets : 0–5, 6–10 and 10+For continuous independent variables, we can get more clarity on the distribution by analyzing it w.r.t. dependent variable.From the above plots, we can infer that the median glucose content is higher for patients who have diabetes. Similar inferences can be drawn for the rest of the variables.For categorical independent variables, we can analyze the frequency of each category w.r.t. the dependent variableWe’ll now create a new data frame of relevant modeling variables.Implementation of Logistic Regression to predict the binary outcome — diabetes in the dataset “newdata2”.Analysis of Model SummaryThe summary statistics helps us in understanding the model better by providing us with the following information:Interpretation of ResultsFor continuous variables, the interpretation is as follows:For every one unit increase in glucose, the log odds of being diabetic ‘pos’(versus being diabetic ‘neg’) increases by 0.039.Similarly, for one unit increase in pressure, the log odds of being diabetic ‘pos’(versus being diabetic ‘neg’) decreases by 0.0045.For categorical variables, the performance of each category is evaluated w.r.t. a base category. The base category for the variable ‘age_bucket’ is 20–30 and for ‘preg_bucket’ is 0–5. The interpretation of such variables is as follows:Being in the age bucket of 31–40, versus age bucket of 20–30, changes the log odds of being diabetic ‘pos’(versus being diabetic ‘neg’) by 0.854.Being in the pregnancy bucket of 6–10, versus pregnancy bucket of 0–5, changes the log odds of being diabetic ‘pos’(versus being diabetic ‘neg’) by -0.24.Variable SelectionThe model ‘logit_1', might not be the best model with the given set of independent variables.There are multiple methodologies for variable selection. In this article, we’ll explore only the ‘stepAIC’ function.The ‘stepAIC’ function in R performs a stepwise model selection with an objective to minimize the AIC value.Analyzing Model Summary for the newly created model with minimum AICAfter implementing ‘stepAIC’ function, we are now left with four independent variables — glucose, mass, pedigree, and age_bucket. Of all the possible models, this model (logit_2) has the minimum AIC value.Moreover, the shortlisted variables are highly significant.Analysis of the outcomeTo analyze the predicted probability of having the value of “diabetes” as “pos” we can use the summary function as belowWe can also analyze the distribution of predicted probability of ‘pos’ diabetes.Let’s now classify the prediction as “pos” if the fitted value exceeds 0.5 otherwise “neg”.Model Performance EvaluationWe can now evaluate the performance of the model using the following parameters:AIC stands for Akaike Information Criteria. It is analogous to adjusted R² and is the measure of fit which penalizes model for the number of independent variables. We always prefer a model with minimum AIC value.We can compare the AIC of the original model — logit_1 and the model derived by stepAIC function — logit_2.As expected, the model derived by stepAIC function corresponds to lower AIC value.2. Confusion MatrixIt is a tabular representation of Observed vs Predicted values. It helps to quantify the efficiency (or accuracy) of the model.Let’s now compare the observed values of “diabetes” with the predicted values:From Confusion Matrix, the accuracy of our model is 81.4%.3. ROC CurveROC stands for Receiver Operating Characteristic. It explains the model’s performance by evaluating Sensitivity vs Specificity.The area under the ROC Curve is an index of accuracy. Higher the area under the curve, better the prediction power of the model.AUC of a perfect predictive model equals 1.The area under the curve of model ‘logit_2’ is 0.863.In the next article, we’ll be learning about another widely used logistic regression technique — Ordinal Logistic RegressionThanks!",31/10/2017,21,68.0,10.0,622.0,362.0,16.0,4.0,0.0,1.0,en
4103,"Intuitive guide to word embedding, RNN (SimpleRNN, LSTM) with step by step implementation in keras for spam detection",Medium,Hemant Ranvir,14.0,9.0,1467.0,"This tutorial will guide you through the implementation and intuitive grasp on what is actually happening underneath the RNN networks.There has been extensive writing on this subject but I could not find a single source where the complete walk through of word embedding (what, why and how), SimpleRNN & LSTM(with detailed structure diagram, number of parameters calculation etc) were given at a single place. Hence this is an effort to do the same. I will quote the appropriate source from where the material has been refereed from.We will be using csv file containing messages and their labels (whether it is spam or ham)First of all our neural network cannot understand words (that’s what we are trying to make it learn). So we need an efficient way of representing words into mathematical concepts.As top of the mind one can think why not assign a unique integer to a unique word. Sure enough that is one way of doing things as shown below:Also one can go further and use one-hot encoding to represent one bit to one word as shown below:These are also called sparse matrix representations as you can see there are lot of zeros in the vector representation of words.These ways of representation has couple of drawbacks. First of all there is no relation captured between words. We want the mathematical structure of word representation to hold meaning rather than simple integers representing words. It would be good if there was some way we can capture similar words together. Secondly, the sparse representation of words need big vectors as our vocabulary size grows so it is not efficient.That’s where word embedding come in. It has a mathematical structure to represent words more efficiently, specifically we use dense representation to capture words relation. In such a representation words having similar meanings are closer to each other in vector space.As seen above this is a much better representation as we can capture the word similarity using closeness between two vectors. Also here we are using just 2 dimensions for so many words (dense representation), while the earlier methods would require much more dimensions.There are three ways to calculate these word vectors for the vocabulary you are using.We are gonna focus on the first method, for the second and third method I would recommend visiting this article for detailed explanation.For the first method what we really want is a word vector for a specified word. Hence we can imagine a matrix having the same number of rows as number of unique words in our vocabulary and each row representing the word vector. We will call this matrix as word embedding matrix.This matrix has rows as the number of unique words in the vocabulary and number of columns as the hyper parameter/user-specified (dimensions of vector space, in our example it is kept to be 32). We can keep such a layer at the beginning of the network and train the Embedding layer with the rest of the network for our custom data set.Keras provides us with such an Embedding layer to train, we will use it for our network’s first layer.We are going to train our network to detect spam messages (spam or ham). The data set looks like as following:There are in total 5572 messages with labels/category as spam or ham.The job of the Embedding layer would be to keep the words having similar meanings/contexts together in vector space.We read the data and save the messages as list of strings and corresponding labels as list of integers (0 for ham or not spam, 1 for spam) and convert them to numpy array.Next we will be using keras’ Tokenizer class to convert the array of sequences of strings (messages) to list of sequences of integers.Tokenizer class lets us specify the maximum number of vocabulary words to consider using num_words argument i.e keep the 10000 most frequent words, ignore the others.fit_on_texts method calculates the frequency of each word in our corpus/messages.texts_to_sequences method finally converts our array of sequences of strings to list of sequences of integers (most frequent word is assigned 1 and so on).Since our network expects array not list as input, convert the list to 2D array using pad_sequences method. maxlen specifies the maximum length of sequence (truncated if longer, padded if shorter)Now our data is ready to be fed into the network!data has a shape of (5572, 500) i.e we have 5572 messages in our csv file and we restricted each message to 500 words/integers.word_index attribute from Tokenizer class is a dictionary keeping track of word to their index/integer representation as calculated by fit_on_texts method. It will come handy later on when we feed custom message for testing the network.Split the data set for training/validation and testing. 80% for training and 20% for testing. We will later on split the 80% data set into training and validation .Now lets talk about the network to be used. We will train/test the data set with two RNN networks.SimpleRNNX 1,t … X m,t is input vector of size m at time instant tH 1,t … H n,t is output vector of size n at time instant tH 1,t-1 … H n,t-1 is output vector of size n at time instant t-1Here we are feeding the output of the hidden layer back to itself after one timestep delay.The input vector we are referring here is basically just the word vector. Hence we will be feeding the network one word vector at a time. Since we have at max 500 words in our each message, we will feeding 500 word vectors to the above model.Each element of the input vector ( 32 elements in one word vector) is connected to each node in the output layer (output dimension of SimpleRNN). If we consider word vector of size m and output dimension of size n. Then there are mxn weights to connect the input vector to output nodes.Also the output of previous timestep is also connected to the output nodes. Each element of the output vector (total n elements) of previous time step is connected to every element of the output vector of current time step. Thus there are nxn weights. (I have shown all connections for H 1, t-1 only for simplicity)Finally we have n number of biases (simply adding the value).Hence in total we have following number of trainable parameters:Now for ease of explanation and continuity with LSTM section, it is better to represent the above SimpleRNN as below:It is accomplishing the same task as before but now we can see that it is similar to fully connected network with the input concatenated with the previous time step output vector.Now, Keras’ SimpleRNN uses the final output (after the bias and tanh) to be fed back as concatenated input.It can be represented in a compact manner as below:Lets build the network in keras!Embedding class lets us create an word Embedding layer for the network. As discussed before it is simply a weights matrix with every row as word vector for all unique words in our vocabulary/corpus.input_dim argument is to specify the number of rows of the Embedding matrix.output_dim is to specify the number of columns of the Embedding matrix.input_length is to specify the maximum length of input sequence.SimpleRNN class is to construct the SimpleRNN/Elman Network discussed above. units is to specify the output dimension for the SimpleRNN. The default activation for SimpleRNN is tanh.Train the model:After training lets evaluate:Output:Lets try to give it a custom message and check the prediction:Output:where 1 means it is spam. Hence the model seems to be working!LSTMIn SimpleRNN we had input vector of size m and output dimensions of size n and we saw how they are connected to each other. In total there were (nxn + nXm + n) trainable parameters.Now in LSTM instead of one such fully connected recurrent network we have four. Specifically (in Figure SimpleRNN compact) the tanh block denotes 2 weight matrices of size nxn and nxm each and a bias vector of size n. In LSTM we will have four such FFNNs as shown below (the boxes in yellow):Each of these four FFNNs have their own 2 weight matrices (of size nxm and nxn each) and a bias vector of size n. Hence the total number of trainable parameters for LSTM would be:The connections diagram for LSTM with all nodes would be too complex to draw but having grasped SimpleRNN, one can imagine there are four SimpleRNN structures (three with sigmoid activation and one with tanh) at the input of LSTM.Another good source for visualization is:Lets build the network in keras!We just need to replace SimpleRNN class with LSTM class in the previous code snippets, everything else remains the same.The output for test accuracy is:As seen above, the accuracy of LSTM is better than SimpleRNN.The complete source code can be found at github.",20/06/2019,13,20.0,16.0,613.0,342.0,13.0,2.0,0.0,8.0,en
4104,YOLOv4,Medium,Jonathan Hui,27000.0,18.0,3256.0,"Even object detection starts maturing in the last few years, the competition remains fierce. As shown below, YOLOv4 claims to have state-of-the-art accuracy while maintains a high processing frame rate. It achieves an accuracy of 43.5% AP (65.7% AP₅₀) for the MS COCO with an approximately 65 FPS inference speed on Tesla V100. In object detection, high accuracy is not the only holy grail anymore. We want the model to run smoothly in the edge devices. How to process input video in real-time with low-cost hardware becomes important also.The fun part of reading the YOLOv4 development is what new technologies have been evaluated, modified, and integrated into YOLOv4. And it also makes changes to make the detector more suitable for training on a single GPU.Improvements can be made in the training process (like data augmentation, class imbalance, cost function, soft labeling etc…) to advance accuracy. These improvements have no impact on inference speed and called “bag of freebies”. Then, there are “bag of specials” which impacts the inference time slightly with a good return in performance. These improvements include the increase of the receptive field, the use of attention, feature integration like skip-connections & FPN, and post-processing like non-maximum suppression. In this article, we will discuss how the feature extractor and the neck are designed as well as all these Bof and BoS goodies.Dense Block & DenseNetTo improve accuracy, we can design a deeper network to extend the receptive field and to increase model complexity. And to ease the training difficulty, skip-connections can be applied. We can expand this concept further with highly interconnected layers.A Dense Block contains multiple convolution layers with each layer Hi composed of batch normalization, ReLU, and followed by convolution. Instead of using the output of the last layer only, Hi takes the output of all previous layers as well as the original as its input. i.e. x₀, x₁, …, and xᵢ₋₁. Each Hi below outputs four feature maps. Therefore, at each layer, the number of feature maps is increased by four — the growth rate.Then a DenseNet can be formed by composing multiple Dense Block with a transition layer in between that composed of convolution and pooling.Below is the detailed architectural design.Cross-Stage-Partial-connections (CSP)CSPNet separates the input feature maps of the DenseBlock into two parts. The first part x₀’ bypasses the DenseBlock and becomes part of the input to the next transition layer. The second part x₀’’ will go thought the Dense block as below.This new design reduces the computational complexity by separating the input into two parts — with only one going through the Dense Block.CSPDarknet53YOLOv4 utilizes the CSP connections above with the Darknet-53 below as the backbone in feature extraction.The CSPDarknet53 model has higher accuracy in object detection compared with ResNet based designs even they have a better classification performance. But the classification accuracy of CSPDarknet53 can be improved with Mish and other techniques discussed later. The final choice for YOLOv4 is therefore CSPDarknet53.Object detectors composed of a backbone in feature extraction and a head (the rightmost block below) for object detection. And to detect objects at different scales, a hierarchy structure is produced with the head probing feature maps at different spatial resolutions.To enrich the information that feeds into the head, neighboring feature maps coming from the bottom-up stream and the top-down stream are added together element-wise or concatenated before feeding into the head. Therefore, the head’s input will contain spatial rich information from the bottom-up stream and the semantic rich information from the top-down stream. This part of the system is called a neck. Let’s get more details in its design.Feature Pyramid Networks (FPN)YOLOv3 adapts a similar approach as FPN in making object detection predictions at different scale levels.In making predictions for a particular scale, FPN upsamples (2×) the previous top-down stream and add it with the neighboring layer of the bottom-up stream (see the diagram below). The result is passed into a 3×3 convolution filter to reduce upsampling artifacts and create the feature maps P4 below for the head.SPP (spatial pyramid pooling layer)SPP applies a slightly different strategy in detecting objects of different scales. It replaces the last pooling layer (after the last convolutional layer) with a spatial pyramid pooling layer. The feature maps are spatially divided into m×m bins with m, say, equals 1, 2, and 4 respectively. Then a maximum pool is applied to each bin for each channel. This forms a fixed-length representation that can be further analyzed with FC-layers.Many CNN-based models containing FC-layers and therefore, accepts input images of specific dimensions only. In contrast, SPP accepts images of different sizes. Nevertheless, there are technologies like fully convolution networks (FCN) that contain no FC-layers and accepts images of different dimensions. This type of design is particularly useful for image segmentation which spatial information is important. Therefore, for YOLO, convert 2-D feature maps into a fixed-size 1-D vector is not necessarily desirable.YOLO with SPPIn YOLO, the SPP is modified to retain the output spatial dimension. A maximum pool is applied to a sliding kernel of size say, 1×1, 5×5, 9×9, 13×13. The spatial dimension is preserved. The features maps from different kernel sizes are then concatenated together as output.The diagram below demonstrates how SPP is integrated into YOLO.Path Aggregation Network (PAN)In early DL, the model design is relatively simple. Each layer takes input from the previous layer. The early layers extract localized texture and pattern information to build up the semantic information needed in the later layers. However, as we progress to the right, localized information that may be needed to fine-tune the prediction may be lost.In later DL development, the interconnectivity among layers is getting more complex. In DenseNet, it goes to the extreme. Each layer is connected with all previous layers.In FPN, information is combined from neighboring layers in the bottom-up and top-down stream.The flow of information among layers becomes another key decision in the model design.The diagram below is the Path Aggregation Network (PAN) for object detection. A bottom-up path (b) is augmented to make low-layer information easier to propagate to the top. In FPN, the localized spatial information traveled upward in the red arrow. While not clearly demonstrates in the diagram, the red path goes through about 100+ layers. PAN introduced a short-cut path (the green path) which only takes about 10 layers to go to the top N₅ layer. This short-circuit concepts make fine-grain localized information available to top layers.As a side note, the neck design can be visualized as the following:However, instead of adding neighbor layers together, features maps are concatenated together in YOLOv4.In FPN, objects are detected separately and independently at different scale levels. This may produce duplicated predictions and not utilize information from other feature maps. PAN fuses the information together from all layers first using element-wise max operation (we will skip the details here).Spatial Attention Module (SAM)Attention has widely adopted in DL designs. In SAM, maximum pool and average pool are applied separately to input feature maps to create two sets of feature maps. The results are feed into a convolution layer followed by a sigmoid function to create spatial attention.This spatial attention mask is applied to the input feature to output the refined feature maps.In YOLOv4, a modified SAM is used without applying the maximum and average pooling.In YOLOv4, the FPN concept is gradually implemented/replaced with the modified SPP, PAN, and PAN.The BoF features for YOLOv4 backbone include:CutMix data augmentationCutout data augmentation removes a region of an image (see the diagram below). This forces the model not to be overconfident on specific features in making classifications. However, a portion of the image is filled with useless information and this is a waste. In CutMix, a portion of an image is cut-and-paste over another image. The ground truth labels are readjusted proportionally to the area of the patches, e.g. 0.6 like a dog and 0.4 like a cat.Conceptually, CutMix has a broader view of what an object may compose of. The cutout area forces the model to learn object classification with different sets of features. This avoids overconfidence. Since that area is replaced with another image, the amount of information in the image and the training efficient will not be impacted significantly also.Mosaic data augmentationMosaic is a data augmentation method that combines 4 training images into one for training (instead of 2 in CutMix). This enhances the detection of objects outside their normal context. In addition, each mini-batch contains a large variant of image (4×) and therefore, reduces the need for large mini-batch sizes in estimating the mean and the variance.DropBlock regularizationIn Fully-connected layers, we can apply dropoff to force the model to learn from a variety of features instead of being too confident on a few. However, this may not work for convolution layers. Neighboring positions are highly correlated. So even some of the pixels are dropped (the middle diagram below), the spatial information remains detectable. DropBlock regularization builds on a similar concept that works on convolution layers.Instead of dropping individual pixels, a block of block_size × block_size of pixels is dropped.Class label smoothingWhenever you feel absolutely right, you may be plainly wrong. A 100% confidence in a prediction may reveal that the model is memorizing the data instead of learning. Label smoothing adjusts the target upper bound of the prediction to a lower value say 0.9. And it will use this value instead of 1.0 in calculating the loss. This concept mitigates overfitting.Mish activationLet’s assume the activation function is in the form ofwith different function candidates (like cosine function) for the unary or binary operators. We can make random guesses in selecting these functions and evaluating the corresponding model performances based on different tasks (like classification) and datasets. Finally, we can pick an activation function that performs the best.Applying reinforcement learning, we can search the solution space more efficiently.Using this method followed by experiments, the new activation function below, called Swish, shows better performance than ReLU and many other activation functions.Mish is another activation function with a close similarity to ReLU and Swish. As claimed by the paper, Mish can outperform them in many deep networks across different datasets.Using Mish for the CSPDarknet53 and the detector, it increases both accuracies in YOLOv4.Multi-input weighted residual connections (MiWRC)For the past few years, researchers have paid a lot of attention to what feature maps will be fed into a layer. Sometimes, we break away from the tradition that only the previous layer is used.How layers are connected becomes more important now, in particular for object detectors. We have discussed FPN and PAN so far as examples. The diagram (d) below shows another neck design called BiFPN that has better accuracy and efficiency trade-offs according to the BiFPN paper.In YOLOv4, it compares its performance with the EfficientDet which is considered as one of the state-of-the-art technology by YOLOv4. So let’s spend some time studying it. As shown below, EfficientDet uses the EfficientNet as the backbone feature extractor and BiFPN as the neck.As a reference, the diagram below is the architecture for the EfficientNet which builds on the MBConv layers that compose of the inverted residual block.As quoted in the paper, the inverted residual block consists ofThe first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a 1 × 1 convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.Let’s assume the input is of dimension hᵢ × wᵢ × dᵢ. It applies dᵢ convolution filters of k × k — one for each channel. Then it applies 1× 1 convolution filters to all channels to produce an output of hᵢ × wᵢ × dⱼ. So the total computational complexity is:The key advantage is it requires much lower computation than the traditional convolution layer.In many ML and DL problems, we learn a low-dimensional representation of the input. We extract the core information of the data by creating an “information” bottleneck. That forces us to discover the most important information which is the core principle of learning. Following this principle, an Inverted residual block takes a low-dimensional representation as input and manipulate it with convolution (linear operation) and non-linear operations. But there is a major issue on the non-linear parts like ReLU. Non-linear operations stretch or compress regions un-proportionally. In such compression, inputs may map to the same regions/points. For example, ReLU may collapse the channel in this low dimensional space and inevitably lose information. As quoted from the paper:It is important to remove non-linearities in the narrow layersin order to maintain representational power.To address that, we can temporarily expand the dimension (the number of channels). With the hope that we have lots of channels now and information may still be preserved in some channels after the non-linear operations. Here is the detail of an inverted residual block:As shown, the low-dimensional representation is first expanded to tk channels. Then, it is filtered with lightweight 3 × 3 depthwise convolution. Features are then subsequently reduced back to a low-dimensional at the end of the module. Non-linear operations are added when it remains in the higher-dimensional space.A residual connection is added from the beginning to the end of the module. The diagram on the left is the traditional residual block and the right is the described inverted residual block.It is nice to understand the key concept of EfficientDet. But the main contribution of EfficientDet on YOLOv4 is the Multi-input weighted residual connections. In the EfficientDet paper, it observes that different input features are at different resolutions and it contributes to the output feature unequally. But in our previous discussion, we add those features equally. In EfficientDet, the input features are weighted differently in composing the output as:where wᵢ will be trained and learned like other trainable parameters.The BoF features for YOLOv4 detector include:CIoU-lossA loss function gives us signals on how to adjust weights to reduce cost. So in situations where we make wrong predictions, we expect it to give us direction on where to move. But this is not happening when IoU is used and the ground truth box and the prediction do not overlap. Consider two predictions that both do not overlap with the ground truth, the IoU loss function cannot tell which one is better even one may be closer to the ground truth than the other.Generalized IoU (GIoU) fixes this by refining the loss as:But this loss function tends to expand the prediction boundary box first until it is overlapped with the ground truth. Then it shrinks to increase IoU. This process requires more iterations than theoretically needed.First, Distance-IoU Loss (DIoU) is introduced as:It introduces a new goal to reduce the central points separation between the two boxes.Finally, Complete IoU Loss (CIoU) is introduced to:This is the final definition:CmBNThe original Batch normalization collects the mean and the variance of the samples within a mini-batch to whiten the layer input. However, if the mini-batch size is small, these estimations will be noisy. One solution is to estimate them among many mini-batches. However, as weights are changing in each iteration, the statistics collected under those weights may become inaccurate under the new weight. A naive average will be wrong. Fortunately, weights change gradually. In Cross-Iteration Batch Normalization (CBM), it estimates those statistics from k previous iterations with the adjustment below.CmBN is a modified option that collects statistics only between mini-batches within a single batch.Self-Adversarial Training (SAT)SAT is a data augmentation technique. First, it performs a forward pass on a training sample. Traditionally, in the backpropagation, we adjust the model weights to improve the detector in detecting objects in this image. Here, it goes in the opposite direction. It changes the image such that it can degrade the detector performance the most. i.e. it creates an adversarial attack targeted for the current model even though the new image may look visually the same. Next, the model is trained with this new image with the original boundary box and class label. This helps to generalize the model and to reduce overfitting.Eliminate grid sensitivityThe boundary box b is computed as:For the case bₓ = cₓ and bₓ = cₓ+1, we need tₓ to have a huge negative and positive value, respectively. But we can multiply σ with a scaling factor (>1.0) to make it easier. Here are the source code changes:Multiple anchors for a single ground truthUse multiple anchors for a single ground truth if IoU(ground truth, anchor) > IoU threshold. (Note, not enough information for me in determining its role in YOLOv4 yet.)Cosine annealing schedulerThe cosine schedule adjusts the learning rate according to a cosine function. It starts by reducing the large learning rate slowly. Then it reduces the learning rate quickly halfway and finally ends up with a tiny slope in reducing the learning rate.The diagram indicates how the learning rate is decay (learning rate warmup is also applied in the diagram below) and its impact on the mAP. It may not be very obvious, the new schedule has more constant progress rather than plateau for a long while before making progress again.Hyperparameter selection using genetic algorithms (Evolutionary Algorithms)The Evolutionary Algorithms are an educated guess method. It follows the concept of survival of the fittest. For example, we select 100 sets of hyperparameters randomly. Then, we use them for training 100 models. Later, we select the top 10 performed models. For each selected model, we create 10 slightly mutated hyperparameters according to its original. We retrain the models with the new hyperparameters and select the best models again. As we keep the iterations, we should find the best set of hyperparameters. Alternatively, we can start with the default hyperparameters and then we start the mutations. As quoted from the paper,Genetic algorithm used YOLOv3-SPP to train with GIoU loss and search 300 epochs for min-val 5k sets. We adopt searched learning rate 0.00261, momentum 0.949, IoU threshold for assigning ground truth 0.213, and loss normalizer 0.07 for genetic algorithm experiments.Random Training ShapesMany single-stage object detectors are trained with a fixed input image shape. To improve generalization, we can train the model with different image sizes. (Multi-Scale Training in YOLO)The BoS features for YOLOv4 detector include:DIoU-NMSNMS filters out other boundary boxes that predict the same object and retains one with the highest confidence.DIoU (discussed before) is employed as a factor in non-maximum suppression (NMS). This method takes IoU and the distance between the central points of two bounding boxes when suppressing redundant boxes. This makes it more robust for the cases with occlusionsWhile this article presents what technologies have been integrated into YOLOv4, YOLOv4 has spent a lot of effort in evaluating other technologies. To close this article, the diagram below lists the technologies considered by YOLOv4.YOLOv4: Optimal Speed and Accuracy of Object DetectionGithub for YOLOv4Densely Connected Convolutional NetworksCSPNet: A New Backbone that can Enhance Learning Capability of CNNSpatial Pyramid Pooling in Deep Convolutional Networks for Visual RecognitionPath Aggregation Network for Instance SegmentationMish: A Self Regularized Non-Monotonic Neural Activation FunctionSearching for Activation Functions (Swish)DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for Object DetectionPath Aggregation Network for Instance SegmentationCBAM: Convolutional Block Attention Module (SAM)Distance-IoU Loss: Faster and Better Learning for Bounding Box RegressionCross-Iteration Batch NormalizationCutMix: Regularization Strategy to Train StrongDropBlock: A regularization method for convolutional networksRethinking the Inception Architecture for Computer Vision (Class label smoothing)Distance-IoU Loss: Faster and better learning for bounding box regressionSGDR: Stochastic gradient descent with warm restarts (Cosine annealing scheduler)Bag of Freebies for Training Object Detection Neural NetworksEfficientNet: Rethinking Model Scaling for Convolutional Neural NetworksEfficientDet: Scalable and Efficient Object DetectionMobileNetV2: Inverted Residuals and Linear Bottlenecks",04/05/2020,1,25.0,22.0,1336.0,521.0,51.0,5.0,0.0,70.0,en
4105,癌細胞生長的開關：基因啟動子(Promoter)？,Medium,Geneonline-基因線上,40.0,4.0,106.0,"啟動子 (Promoter) 在人體遺傳基因扮演著重要角色，宛如人體一個開關，可以決定基因的活動，並控制細胞開始生產人體所需的蛋白質。當啟動子發生突變時，將可能導致基因表現的調節障礙。近期國際學者在癌細胞基因組的研究中發現，基因啟動子中的 DNA 突變數量增加，是因結合 DNA 控制基因表現的某些蛋白質，阻止人體的一個細胞修復系統去修復損傷的 DNA。啟動子突變的多寡與 DNA 修復系統相互作用，引發癌細胞生長有了重要的發現。皮膚癌啟動子的突變密度特別高2016 年 4 月發表在《Nature》的研究指出，科學家們分析來自 14 種癌症類型、1,161 個腫瘤的 2000 多萬 DNA 突變。他們發現在許多癌症類型，尤其是皮膚癌中，基因啟動子的基因組區域內突變數量特別高。研究進一步探究發現，人體控制基因表達的一些蛋白質，降低人體細胞修復系統的功能發揮，導致無法正常修復受損的 DNA，這個系統被稱為核苷酸切除修復 (NER, Nucleotide Excision-Repair)。NER 是唯一能修復紫外線造成的 DNA 損傷的系統，不僅如此，它還能處理抽煙誘導的遺傳損傷。(上圖為DNA修復示意圖)延伸閱讀：腫瘤的轉移與「偽轉移」 基因定序分析癌細胞親緣DNA 修復如何參與癌細胞生長西班牙研究團隊人員利用來自人類黑色素瘤樣本的全基因組序列分析調控區域的突變，並進一步分析核苷酸切除修復 (NER) 活性位點。結果發現，NER 功能的下降可導致一些轉錄因子位點的突變率增高。除此，在肺癌樣本中，他們也證實一些轉錄因子結合位點的突變率增高，尤其是與抽煙相關的突變。另一研究中，研究人員則分析多個癌症類型調控元件的突變。結果發現預測轉錄因子將結合的位置，即調控區域的核心，比側翼序列的突變率高達 5 倍。總結，這項研究提示，在 DNA 調節發生的區域（如啟動子）中的核苷酸切除修復 (NER) 受到 DNA 結合轉錄因子轉錄啟動機制的阻止，明顯導致癌細胞生長。雖然在癌症研究中發現腫瘤樣本內有較高的啟動子突變, 但是截至目前為止的國際研究中，科學家們只確定一種啟動子突變：端粒酶逆轉錄酶 (TERT) 基因可引發癌症。不過，研究也建議未來可需要進一步地研究基因啟動子突變在癌症形成中的作用。該研究的新發現將有助於醫生提早診斷癌症，為患者找到更具針對性的療法。延伸閱讀：胞外體與癌症系列專文(一)癌症的先鋒特攻隊-胞外體©www.geneonline.news. All rights reserved. 基因線上版權所有 未經授權不得轉載。合作請聯繫：service@geneonlineasia.com參考文獻:1. University of New South Wales. “Repairing DNA damage in the human body.” ScienceDaily. ScienceDaily, 13 April 2016. .2. Dilmi Perera et al. Differential DNA repair underlies mutation hotspots at active promoters in cancer genomes, Nature (2016). DOI: 10.1038/nature174373. Radhakrishnan Sabarinathan et al. Nucleotide excision repair is impaired by binding of transcription factors to DNA, Nature (2016). DOI: 10.1038/nature17661圖片來源：https://en.wikipedia.org/wiki/DNA_repair",19/05/2016,0,5.0,1.0,567.0,320.0,3.0,0.0,0.0,2.0,ko
4106,RNN vs GRU vs LSTM,Analytics Vidhya,Hemanth Pedamallu,18.0,7.0,1227.0,"In this post, I will make you go through the theory of RNN, GRU and LSTM first and then I will show you how to implement and use them with code.There are already many posts on these topics out there. But in this post, I wanted to provide a much better understanding and comparison with help of code.Let’s start with RNN!Recurrent Neural Networks (RNN) are designed to work with sequential data. Sequential data(can be time-series) can be in form of text, audio, video etc.RNN uses the previous information in the sequence to produce the current output. To understand this better I’m taking an example sentence.“My class is the best class.”At the time(T0 ), the first step is to feed the word “My” into the network. the RNN produces an output.At the time(T1 ), then at the next step we feed the word “class” and the activation value from the previous step. Now the RNN has information of both words “My” and “class”.And this process goes until all words in the sentence are given input. You can see the animation below to visualize and understand.At the last step, the RNN has information about all the previous words.Note: In RNN weights and bias for all the nodes in the layer are same.Let’s look at the architecture of the RNN unit. It takes input from the previous step and current input. Here tanh is the activation function, instead of tanh you can use other activation function as well.💡RNN’s face short-term memory problem. It is caused due to vanishing gradient problem. As RNN processes more steps it suffers from vanishing gradient more than other neural network architectures.Q. What is vanishing gradient problem?Ans: In RNN to train the network you backpropagate through time, at each step the gradient is calculated. The gradient is used to update weights in the network. If the effect of the previous layer on the current layer is small then the gradient value will be small and vice-versa. If the gradient of the previous layer is smaller then the gradient of the current layer will be even smaller. This makes the gradients exponentially shrink down as we backpropagate. Smaller gradient means it will not affect the weight updation. Due to this, the network does not learn the effect of earlier inputs. Thus, causing the short-term memory problem.The main problem is that it’s too difficult for RNN to learn to preserve information over many timesteps. In vanilla RNN the hidden state is constently being rewritten.How about an RNN with a separate memory?Solution for vanishing gradientTo overcome this problem two specialised versions of RNN were created. They are 1) GRU(Gated Recurrent Unit) 2) LSTM(Long Short Term Memory). Suppose there are 2 sentences. Sentence one is “My cat is …… she was ill.”, the second one is “The cats ….. they were ill.” At the ending of the sentence, if we need to predict the word “was” / “were” the network has to remember the starting word “cat”/”cats”. So, LSTM’s and GRU’s make use of memory cell to store the activation value of previous words in the long sequences. Now the concept of gates come into the picture. Gates are used for controlling the flow of information in the network. Gates are capable of learning which inputs in the sequence are important and store their information in the memory unit. They can pass the information in long sequences and use them to make predictions.The workflow of GRU is same as RNN but the difference is in the operations inside the GRU unit. Let’s see the architecture of it.Inside GRU it has two gates 1)reset gate 2)update gateGates are nothing but neural networks, each gate has its own weights and biases(but don’t forget that weights and bias for all nodes in one layer are same).Update gateUpdate gate decides if the cell state should be updated with the candidate state(current activation value)or not.Reset gateThe reset gate is used to decide whether the previous cell state is important or not. Sometimes the reset gate is not used in simple GRU.Candidate cellIt is just simply the same as the hidden state(activation) of RNN.Final cell stateThe final cell state is dependent on the update gate. It may or may not be updated with candidate state. Remove some content from last cell state, and write some new cell content.In GRU the final cell state is directly passing as the activation to the next cell.In GRU,Now you know about RNN and GRU, so let’s quickly understand how LSTM works in brief. LSTMs are pretty much similar to GRU’s, they are also intended to solve the vanishing gradient problem. Additional to GRU here there are 2 more gates 1)forget gate 2)output gate.First, look at the architecture of it.Now, look at the operations inside it.From GRU, you already know about all other operations except forget gate and output gate.All 3 gates(input gate, output gate, forget gate) use sigmoid as activation function so all gate values are between 0 and 1.Forget gateIt controls what is kept vs forgotten, from previous cell state. In laymen terms, it will decide how much information from the previous state should be kept and forget remaining.Output gateIt controls which parts of the cell are output to the hidden state. It will determine what the next hidden state will be.Phew! That's enough theory, now let us start coding.I’m taking airline passengers dataset and provide the performance of all 3 (RNN, GRU, LSTM) models on the dataset.My motive is to make you understand and know how to implement these models on any dataset. To make it simple, I’m not focusing on the number of neurons in the hidden layer or number of layers in the network( You can play with these to get better accuracy).About dataset:The dataset provides a record of the number of people travelling in US airlines in a particular month. It has a record of 142 months. It has 2 columns “Month” and “No. of Passengers”. But in this case, I want to use univariate dataset. Only “No. of Passengers” is used.Importing all the necessary libraries and dataset.To visualize the dataset, plt.plot(dataset)It shows that the number of passengers is linearly increasing over the months.Machine learning model/ Neural network works better if all the data is scaled.Divide the data for training and testing. I split the dataset into (75% training and 25% testing). In the dataset, we can estimate the ‘i’th value based on the ‘i-1’th value. You can also increase the length of the input sequence by taking i-1,i-2,i-3… to predict ‘i’th value.I made the sequential model with only 2 layers. Layers:In this code, I’m using LSTM. You can also use the other two just by replacing “LSTM” with “SimpleRNN”/”GRU” in the below code(line 2).In the LSTM layer, I used 5 neurons and it is the first layer (hidden layer) of the neural network, so the input_shape is the shape of the input which we will pass.Now, the model is ready. So start training the model.Finally, to visualize the real values and predicted result.For this dataset and with the simple network by using 50 epochs I got the following mean_squared_error values.Simple RNN: 3000GRU: 2584LSTM: 2657After learning about these 3 models, we can say that RNN’s perform well for sequence data but has short-term memory problem(for long sequences). It doesn’t mean to use GRU/LSTM always. Simple RNN has it’s own advantages (faster training, computationally less expensive).",14/11/2020,7,32.0,19.0,822.0,476.0,10.0,2.0,0.0,0.0,en
4107,Fancy and custom Neural Style Transfer filters for video conferencing,Towards Data Science,Maximus Mutschler,2.0,3.0,525.0,"My open-source GitHub script provides AI-based filters which apply a rather new technology called Artistic Neural Style Transfer to the input stream of your physical webcam device. In contrast to traditional filters, these AI-based filters are feature-aware. Depending on what kind of features are apparent in the video, the AI adapts the output. In addition, these kinds of filters can be learned from any real-world image. Since the provided filters are directly applied on the webcam video stream, they can be used in all types of video conferencing tools, such as Zoom, Skype, Discord, MS-Teams….In detail, my script sets up a virtual webcam device that applies Artistic Neural Style Transfer to the input stream of the physical webcam device. This new webcam device can then be used just like any physical webcam in any video-conferencing application. So far, this tool is only available for Linux, but I am working on a Windows version. To make the installation very convenient, all you need to do is build a Docker container. How this works in detail is explained in a simple step-by-step installation guide. Even your own custom styles can be easily created by training the PyTorch fast neural style transfer example model to extract the style of a particular image.What Neural Style Transfer is:This Deep Learning approach, originally developed by Leon Gatys et al. in 2016, can extract the style of a particular image and apply it to other images. The basic idea of this approach is to generate an image that, on the one hand, has similar content features as the content image and, on the other hand, tries to have the same amount of feature correlation as exists between the features of the style image. This feature correlation is a representation of the image’s style.Technical hurdles and why this was not created earlier:Since the invention of neural style transfer by Leon Gatys et al. in 2016, no tool has been built to date that integrates this approach into a virtual webcam device that can be used directly. One major hurdle has been that the processing time of good quality style transfer images was still too slow to achieve high frame rates in real-time. Another hurdle has been that it was difficult to access and customize webcam driver code to apply neural style transfer. Fortunately, two relatively recent technological achievements let me leap over these hurdles: First, Nvidia’s TensorRT libraries lead to a massive speedup in model inference time. I adapted the fast neural style transfer model from the PyTorch examples to be compatible with TensorRT, resulting in an inference time speedup of more than 75%. This makes it possible to run the webcam on HD at a frame rate of more than 25 frames per second on current Nvidia graphics cards. Secondly, the virtual webcam driver akvcam has paved the way for me to set up a virtual webcam on Linux that can be used like any physical webcam.Have I now sparked your interest to try video conferencing with fancy, wacky, and individual styles? Get started right away by following the installation guide at https://github.com/MaximusMutschler/Virtual-Neural-Style-Transfer-Webcam-for-Linux. Requirements are Linux and a rather new Nvidia GPU.",27/06/2021,0,5.0,7.0,821.0,389.0,3.0,0.0,0.0,15.0,en
4108,"3 метода детектирования объектов c Deep Learning: R-CNN, Fast R-CNN и Faster R-CNN",Medium,Nick Komissarenko,277.0,4.0,735.0,"Проблема классификации объекта на изображении уже решена — сверточные нейронные сети (Convolutional Neural Networks, CNN) уже неплохо справляются с определением кошек или собак. Но если на изображении много объектов, которые нужно найти, задача сразу усложняется. На смену обычным сверточным нейросетям пришли более сложные модели. В этой статье рассмотрим 3 популярных способа детектирования изображений методами Deep Learning: R-CNN, Fast R-CNN и Faster R-CNN.Распознавание образов — это общий термин, описывающий круг задач компьютерного зрения, которые решают проблему обнаружения объектов на изображении или видеокадрах. К ним относятся классификация изображения, локализация объектов, детектирование объектов и сегментация. Проведем между ними грань:Классификация изображений, где определяется тип или класс объектов на изображении, например, человек, кошка, самолет и т.д. Локализация объектов, когда требуется найти объекты и отметить как-то их, например, прямоугольником. Детектирование объектов, когда нужно найти объекты, отметить их и классифицировать. Сегментация, когда нужно найти объекты и отделить их от заднего фона путем подсвечивания.На следующем изображении показано, как решаются вышеперечисленные задачи на примере одиночного объекта и множества объектов (нажмите, чтобы увеличить):Сравнение задач распознавания образов с разным количеством объектов на изображенииДля детектирования объектов существуют разные методы глубокого обучения (deep learning). Сначала рассмотрим семейство R-CNN.Архитектура R-CNN (Region-Based Convolutional Neural Network) была разработана в 2014 году Ross Girshik и другими [1]. В основе этого метода лежит следующий алгоритм:Нахождение потенциальных объектов на изображении и разбиение их на регионы cпомощью метода selective search [2]. Извлечение признаков каждого полученного региона с помощью сверточных нейронных сетей. Классифицирование обработанных признаков с помощью метода опорных векторов (SVM, Support Vector Machine) и уточнение границ регионов с помощью линейной регрессии.Архитектура R-CNNВ итоге, получаем отдельные регионы с объектами и их классами. В центре стоят сверточные нейронные сети, которые показывают хорошую точность на примере изображений. Но у такой архитектуры есть недостатки:Энергозатратный — требует большого количество времени на обучение. В методе selective search изображение сначала сегментируется на 2000 регионов, которые затем в ходе итерирования с помощью жадного алгоритма объединяются в более крупные регионы. Кроме того, сами сверточные сети тоже требует вычислительных мощностей. Не может быть использован для видео. Опять же из вытекает из недостатка выше, так как все промежуточные методы энергозатратны, поэтому кадры просто не будут успевать обрабатываться. Selective search не является алгоритмом машинного обучения, поэтому могут возникнуть проблемы с выявлением потенциальных объектов на разных изображениях.На данный момент модель R-CNN устарела и не применяется.Недостатки R-CNN привели авторов в 2015 году к улучшению модели. Они назвали ее Fast R-CNN [3]. В ее основе лежит следующая архитектура:Изображение подается на вход сверточной нейронной сети и обрабатывается selectivesearch. В итоге, имеем карту признаков и регионы потенциальных объектов. Координаты регионов потенциальных объектов преобразуются в координаты на карте признаков. Полученная карта признаков с регионами передается слою RoI (Region of Interest) polling layer. Здесь на каждый регион накладывается сетка размером HxW. Затем применяется MaxPolling для уменьшения размерности. Так, все регионы потенциальных объектов имеют одинаковую фиксированную размерность. Полученные признаки подаются на вход полносвязного слоя (Fully-conectedlayer), который передается двум другим полносвязным слоям. Первый с функцией активацией softmax определяет вероятность принадлежности классу, второй — границы (смещение) региона потенциального объекта.Архитектура Fast R-CNNFast R-CNN показывает чуть более высокую точность и большой прирост времени обработки в отличие от R-CNN, так как не требуется подавать все регионы на сверточный слой. Но тем не менее, данный метод использует затратный Selective Search. Поэтому авторы пришли к Faster R-CNN.Авторы продолжили улучшение над Fast R-CNN и в 2016 предложили Faster R-CNN. Они разработали собственный метод локализации объекта взамен Selective Searc — RPN (Region Proporsal Networks) [4]. В основе RPN лежит система якорей. Архитектура Faster R-CNN образована следующим образом:Изображение подается на вход сверточной нейронной сети. Так, формируется карта признаков. Карта признаков обрабатывается слоем RPN. Здесь скользящее окно проходится по карте признаков. Центр скользящего окна связан с центром якорей. Якоря — это области, имеющие разные соотношения сторон и разные размеры. Авторы используют 3 соотношения сторон и 3 размера. На основе метрики IoF(intersection-over-union), степени пересечения якорей и истинных размеченных прямоугольников, выносится решение о текущем регионе — есть объект или нет. Далее используется алгоритм FastCNN: карта признаков с полученными объектами передаются слою RoI с последующей обработкой полносвязных слоев и классификацией, а также с определением смещения регионов потенциальных объектов.Модель Faster R-CNN справляется немного хуже с локализацией, но работает быстрее Fast R-CNN. Сейчас необязательно разбираться со всеми архитектурами, так имеются уже предобученные модели, например, tensorflow models. Также смотрите в видеобзоре, как на практике применять предобученные модели для детектирования изображений в Tensorflow.Как применять методы детектирования изображений на реальных Big Data проектах с помощью Python, вы узнаете на наших курсах в лицензированном учебном центре обучения и повышения квалификации ИТ-специалистов в Москве.Смотреть расписаниеИсточникиarxiv.orglink.springer.comhttps://arxiv.org/abs/1504.08083 https://arxiv.org/abs/1506.01497",24/07/2020,0,4.0,4.0,489.0,207.0,4.0,0.0,0.0,7.0,ru
4109,Stepwise Regression Tutorial in Python,Towards Data Science,Ryan Kwok,12.0,9.0,1762.0,"How do you find meaning in data? In our mini project, my friend @ErikaSM and I seek to predict Singapore’s minimum wage if we had one, and documented that process in an article over here. If you have not read it, do take a look.Since then, we have had comments on our process and suggestions to develop deeper insight into our information. As such, this follow-up article outlines two main objectives, finding meaning in data, and learning how to do stepwise regression.In the previous article, we discussed how the talk about a minimum wage in Singapore has frequently been a hot topic for debates. This is because Singapore uses a progressive wage model and hence does not have a minimum wage.The official stance of the Singapore Government is that a competitive pay structure will motivate the labour force to work hard, aligned with the value of Meritocracy embedded in Singapore culture. Regardless of the arguments for or against minimum wages in Singapore, the poor struggle to afford necessities and take care of themselves and their families.We took a neutral stance acknowledging the validity of both sides of the argument and instead presented a comparison of a prediction of Singapore’s minimum wage using certain metrics across different countries. The predicted minimum wage was also contrasted with the wage floors in the Progressive Wage Model (PWM) across certain jobs to spark some discussion about whether the poorest are earning enough.We used data from Wikipedia and World Data to collect data on minimum wage, cost of living, and quality of life. The quality of life dataset includes scores in a few categories: Stability, Rights, Health, Safety, Climate, Costs, and Popularity.The scores across the indicators and categories were fed into a linear regression model, which was then used to predict the minimum wage using Singapore’s statistics as independent variables. This linear model was coded on Python using sklearn, and more details about the coding can be viewed in our previous article. However, I will also briefly outline the modelling and prediction process in this article as well.The predicted annual minimum wage was US$20,927.50 for Singapore. A brief comparison can be seen in this graph below.Our professor encouraged us to use stepwise regression to better understand our variables. From this iteration, we incorporated stepwise regression to assist us in dimensionality reduction not only to produce a simpler and more effective model, but to derive insights in our data.So what exactly is stepwise regression? In any phenomenon, there will be certain factors that play a bigger role in determining an outcome. In simple terms, stepwise regression is a process that helps determine which factors are important and which are not. Certain variables have a rather high p-value and were not meaningfully contributing to the accuracy of our prediction. From there, only important factors are kept to ensure that the linear model does its prediction based on factors that can help it produce the most accurate result.In this article, I will outline the use of a stepwise regression that uses a backwards elimination approach. This is where all variables are initially included, and in each step, the most statistically insignificant variable is dropped. In other words, the most ‘useless’ variable is kicked. This is repeated until all variables left over are statistically significant.Before proceeding to analyse the regression models, we first modified the data to reflect a monthly wage instead of annual wage. This was because we recognised that most people tend to view their wages in months rather than across the entire year. Expressing our data as such would allow our audience to better understand our data. However, it is also worth noting that this change in scale would not affect the modelling process or the outcomes.Looking at our previous model, we produced the statistics to test the accuracy of the model. But before that, we would first have to specify the relevant X and Y columns, and obtain that information from the datafile.Next, to gather the model statistics, we would have to use the statmodels.api library. Here, a function is created which grabs the columns of interest from a list, and then fits an ordinary least squares linear model to it. The statistics summary can then be very easily printed out.Here we are concerned about the column “P > |t|”. Quoting some technical explanations from the UCLA Institute for Digital Research and Education, this column gives the 2-tailed p-value used in testing the null hypothesis.“Coefficients having p-values less than alpha are statistically significant. For example, if you chose alpha to be 0.05, coefficients having a p-value of 0.05 or less would be statistically significant (i.e., you can reject the null hypothesis and say that the coefficient is significantly different from 0).”In other words, we would generally want to drop variables with a p-value greater than 0.05. As seen from the initial summary above, the least statistically significant variable is “Safety” with a p-value of 0.968. Hence, we would want to drop “Safety” as a variable as shown below. The new summary is shown below as well.This time, the new least statistically significant variable is “Health”. Similarly, we would want to remove this variable.We continue this process until all p-values are below 0.05.Finally, we find that there are 5 variables left, namely Workweek, GDP per Capita, Cost of Living Index, Rights, and Popularity. Since each of the p-values are below 0.05, all of these variables are said to be statistically significant.We can now produce a linear model based on this new set of variables. We can also use this to predict Singapore’s minimum wage. As seen, the predicted monthly minimum wage is about $1774 USD.This is the most important part of the process. Carly Fiorina, former CEO of Hewlett-Packard, once said: “The goal is to turn data into information, and information into insight.” This is exactly what we aim to achieve.“The goal is to turn data into information, and information into insight.” ~ Carly Fiorina, former CEO of Hewlett-PackardFrom just looking at the variables, we would have easily predicted which were statistically significant. For example, the GDP per Capita and Cost of Living Index would logically be good indicators of the minimum wage in a country. Even the number of hours in a workweek would make sense as an indicator.However, we noticed that “Rights” was still included in the linear model. This spurred us to first look at the relationship between Rights and Minimum Wage. Upon plotting the graph, we found this aesthetically pleasing relationship.Initially, we wouldn’t have considered Rights to be correlated to Minimum Wage since the more obvious candidates of GDP and Cost of Living stood out more as contributors to the minimum wage level. This made us reconsider how we understood minimum wage and compelled us to dig deeper.From World Data, “Rights” involved civil rights, and revolved mainly around people’s participation in politics and corruption. We found that the Civil Rights Index includes democratic participation by the population and measures to combat corruption. This index also involves public perception of the government including data from Transparency.org.“In addition, other factors include democratic participation by the population and (with less emphasis) measures to combat corruption. In order to assess not only the measures against corruption, but also its perception by the population, the corruption index based on Transparency.org was also taken into account.”This forced us to consider the correlation between Civil Rights and minimum wage. Knowing this information, we did further research and found several articles that might explain this correlation.American civil rights interest group, The Leadership Conference on Civil and Human Rights, released a report about why minimum wage is a civil and human rights issue and the need for stronger minimum wage policy to reduce inequality and ensure that individuals and families struggling in low-paying jobs are paid fairly. It hence makes sense as a country with more democratic participation is also likely to voice concerns about minimum wage, forcing a discussion and consequently increasing it over time.The next variable we looked at was Popularity. We first searched how this was measured from World Data.“The general migration rate and the number of foreign tourists were therefore evaluated as indicators of a country’s popularity. A lower rating was also used to compare the refugee situation in the respective country. A higher number of foreign refugees results in higher popularity, while a high number of fleeing refugees reduces popularity.”At first glance, it seems like there is no correlation. However, if we consider China, France, USA, and Spain as outliers, the majority of the data points seem to better fit an exponential graph. This raises two questions. Firstly, why is there a relationship between Popularity and Minimum Wage? Secondly, why are these four countries outliers?To be very honest, this stumped us. We simply could not see any way where popularity could be correlated to a minimum wage. Nevertheless, there was an important takeaway: that popularity is somehow statistically significant in predicting a minimum wage of a country. While we might not be the people to discover that relationship, this gives insight into our otherwise less meaningful data.It is important to bring back the quote from Carly Fiorina, “The goal is to turn data into information, and information into insight.” We as humans require tools and methods to convert data into information, and experience/knowledge to convert that information into insight.We first used Python as a tool and executed stepwise regression to make sense of the raw data. This let us discover not only information that we had predicted, but also new information that we did not initially consider. It is easy to guess that Workweek, GDP, and Cost of Living would be strong indicators of the minimum wage. However, it is only through regression that we discovered that Civil Rights and Popularity are also statistically significant.In this case, there were research online that we found that could possibly explain this information. This resulted in new insight that minimum wage is actually seen as a human right, and an increase in democratic participation can possibly result in more conversations about a minimum wage and hence increasing it.However, it is not always possible to find meaning in data that easily. Unfortunately, we, as university students, may not be the best people to offer probable explanations to our information. This is seen in our attempts to explain the relationship between Popularity and Minimum Wage. However, it is within our capacity to take this information and spread it to the world, leaving it as an open ended question for discussions to flourish.That is how we can add value to the world using data.",09/03/2021,6,7.0,0.0,1108.0,758.0,8.0,1.0,0.0,10.0,en
4110,1. Creating a Q-A system (Introduction),techpsl,Puneet Singh,108.0,4.0,622.0,"Wikipedia says, “Question Answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.”As I have taken the course of “Information Retrieval” this fall at UB, my final project is decided to be a Q-A system. Well we (me and my 3 group partners) have not decided much on features, which can make our project stand out, but for now we would like to start with a small goal. Somehow index the infobox of Wikipedia, and try to query it using natural language. My initial research suggests that IBM has already created something similar but on a very large scale, and they call it Watson.As we don’t have enough resources in terms of man-hours (neither men nor hours, 4 people on this project and 25 days to create it), also we do not have computing resources, i.e. What IBM does with thousands of computers, we might have to do the same with 2–3 computers.Thus it becomes clear that:Where IBM Watson uses very complicated architecture, we cannot afford to do that.The architecture used by IBM WatsonFor now, with my limited understanding on this subject, I believe, if we are not working on very big data, we can ignore a few components mentioned here. Since it is mandatory to use Lucene/Solr for this project, we have decided to use a few other standard open-source tools.After reading and researching from other places I am clear of five main components of our system:1. NLP Engine2. Query Engine 3. Database/Index4. Retrieved result understanding5. Answer generatorI will take each and every component one by one..NLP Engine: This component reads the question, understands it and breaks it into a computer understandable query. We will use “Apache OpenNLP” or some other NLP toolkit, to determine entities, parsing questions, generating query-able terms for Solr. Query Engine: Takes the Semantic Query generated by the NLP engine, and pushes it to SIREn (Semantic Information Retrieval Engine). SIREn is an open source extension for Apache Lucene and Solr which can query RDF data indexed in Lucene. SIREn adds a new “Field Type” with a set of specific tools such as Analyzers, Query Operators and Query Parser. RDF is Resource Description Framework, which like XML, stores data and also stores semantic information about that particular query. Given below is an example of the RDF model of an article about Tony Benn, which says that Tony Benn is a person’s name. RDF can also stores relationship between two entities. This makes querying semantic information easier.Index:This component would store RDF, in Solr, using SIREn. We would like to use RDF data from DBpedia. Retrieved result understanding (Answer Analyzer):The retrieved results are ranked in the best possible manner, using disambiguation techniques.For example, the query “What is Apple” would give two results; fruit and Software and Hardware Company. What would you choose out of these two? Answer generator:This component tries to form a human readable answer. For example for the question “What is Apple?” the answer should be:Apple Inc., formerly Apple Computer, Inc., is an American multinational corporation headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software and personal computers.Because Apple here is starting with a capital ‘A’, which indicates that it is more probable that, the user wants to ask for Apple Inc.Tentative architecture of our Q-A systemI am really hopeful that we can create something purposeful in next 25 days, even though it is really less time to create something mammoth like a Q-A system. Let’s see where we would land in next 25 days, I would try to post more updates as we will pass by different milestones.",12/11/2013,1,21.0,0.0,300.0,158.0,2.0,1.0,0.0,7.0,en
4111,Model Agnostic Meta-Learning (MAML): An Intuitive Way,Medium,Saket Dingliwal,22.0,4.0,763.0,"There has been a great advancement in the research in the area of meta-learning in recent years. And so has been an expansion in the available literature and blog posts.Model Agnostic Meta-Learning (MAML) lies at the heart of the developments in the area. There have been many excellent blog-posts explaining meta-learning in general (here and here) and MAML in particular (here and here). The heavy terms and complex equations make the algorithm to look like a big shot rocket science. However, through this blog, I want to provide intuitive reasoning behind the algorithm that can be easy to understand for a person who has no idea about meta-learning. All one needs to know is the basic idea all machine learning researchers have been following from its inception: “Throw all your tasks to a neural network, sit back, relax and watch them learn!”Here, I will draw a parallel between any standard machine learning task and meta-learning. In any machine learning task, you are given data (maybe some (x,y) pairs) which can be thought to be as samples from some underlying distribution and the goal is to come up with a function which can predict (maybe some y’ values) for some unseen data (maybe x’) coming from the same distribution. Now, consider a situation where the given data is different tasks which can again be though to be as samples from some underlying distribution (let’s call it tasks distribution) and the goal is to come up with a function which can perform “well” (I will come back to this later) for some unseen task coming from the same distribution. This is exactly the goal of meta-learning and the solution is also the same: “learn it”Each task can be thought of as a pair of training data (coming from some distribution) and a loss function. By performing well for a task, it means having a low value for the loss of unseen data that comes from the same distribution from where the training data for the task came. Now, the unseen task also has its own training data which can enhance the performance of the task. We know only one way of using any data ie to learn some parameters of some prediction function that solves that task. And hence, the objective of our endeavor now becomes to come up with some initial set of parameters for a function which when updated with training data from an unseen task, performs well on that task.A question that might strike your mind now is that why are we not using some random set of parameters for this unseen task as we have always done? We obviously can (I will add to this later) but we can also do a lot better. This is because we have task samples from task distribution which themselves have their own training data each. So what we can look for is what initializations of parameters worked best of these tasks that were adaptive enough to learn from training data in each task easily. We can then apply the same initialization to the unseen task coming from the same task distribution. So we search for the best possible initialization using our old friend gradient descent. We sample some batch of tasks from training data and jump to that initialization which when updated using individual training data of each task achieves a lower average loss for this batch of tasks. I deliberately omit any maths required for such a jump (you will have to compute gradient across the gradient update steps) as it is not necessary for understanding the intuition behind this wonderful algorithm. Feel free to read blogs mentioned above for the details but I am not penning down any equation here.Lastly, I will like to address the motivation for doing all this. Why are these initializations so important? Why meta-learning is a hot-topic? (Some would argue that I should have begun here but I like how one understand a simple method first and then get what a terrific problem he/she has solved) These initializations become all the more important because the training data we have for each task is very limited and learning from random scratch may not be feasible. Instead, an initialization that is adaptive enough to quickly learn from a small number of updates with very few examples is crucial for us. And as we have started to use ML everywhere, the approach for getting lots and lots of data for every task is not sustainable. Time has come for sharing information across tasks and initialization is a good start 😅.",22/01/2020,0,0.0,0.0,498.0,284.0,6.0,0.0,0.0,5.0,en
4112,The Current State of Machine Intelligence,Medium,Shivon Zilis,6200.0,10.0,2507.0,"(The 2016 Machine Intelligence landscape and post can be found here)I spent the last three months learning about every artificial intelligence, machine learning, or data related startup I could find — my current list has 2,529 of them to be exact. Yes, I should find better things to do with my evenings and weekends but until then…Why do this?A few years ago, investors and startups were chasing “big data” (I helped put together a landscape on that industry). Now we’re seeing a similar explosion of companies calling themselves artificial intelligence, machine learning, or somesuch — collectively I call these “machine intelligence” (I’ll get into the definitions in a second). Our fund, Bloomberg Beta, which is focused on the future of work, has been investing in these approaches. I created this landscape to start to put startups into context. I’m a thesis-oriented investor and it’s much easier to identify crowded areas and see white space once the landscape has some sort of taxonomy.What is “machine intelligence,” anyway?I mean “machine intelligence” as a unifying term for what others call machine learning and artificial intelligence. (Some others have used the term before, without quite describing it or understanding how laden this field has been with debates over descriptions.) I would have preferred to avoid a different label but when I tried either “artificial intelligence” or “machine learning” both proved to too narrow: when I called it “artificial intelligence” too many people were distracted by whether certain companies were “true AI,” and when I called it “machine learning,” many thought I wasn’t doing justice to the more “AI-esque” like the various flavors of deep learning. People have immediately grasped “machine intelligence” so here we are. ☺Computers are learning to think, read, and write. They’re also picking up human sensory function, with the ability to see and hear (arguably to touch, taste, and smell, though those have been of a lesser focus). Machine intelligence technologies cut across a vast array of problem types (from classification and clustering to natural language processing and computer vision) and methods (from support vector machines to deep belief networks). All of these technologies are reflected on this landscape.What this landscape doesn’t include, however important, is “big data” technologies. Some have used this term interchangeably with machine learning and artificial intelligence, but I want to focus on the intelligence methods rather than data, storage, and computation pieces of the puzzle for this landscape (though of course data technologies enable machine intelligence).Which companies are on the landscape?I considered thousands of companies, so while the chart is crowded it’s still a small subset of the overall ecosystem. “Admissions rates” to the chart were fairly in line with those of Yale or Harvard, and perhaps equally arbitrary. ☺I tried to pick companies that used machine intelligence methods as a defining part of their technology. Many of these companies clearly belong in multiple areas but for the sake of simplicity I tried to keep companies in their primary area and categorized them by the language they use to describe themselves (instead of quibbling over whether a company used “NLP” accurately in its self-description).If you want to get a sense for innovations at the heart of machine intelligence, focus on the core technologies layer. Some of these companies have APIs that power other applications, some sell their platforms directly into enterprise, some are at the stage of cryptic demos, and some are so stealthy that all we have is a few sentences to describe them.The most exciting part for me was seeing how much is happening in the application space. These companies separated nicely into those that reinvent the enterprise, industries, and ourselves.If I were looking to build a company right now, I’d use this landscape to help figure out what core and supporting technologies I could package into a novel industry application. Everyone likes solving the sexy problems but there are an incredible amount of ‘unsexy’ industry use cases that have massive market opportunities and powerful enabling technologies that are begging to be used for creative applications (e.g., Watson Developer Cloud, AlchemyAPI).Reflections on the landscape:We’ve seen a few great articles recently outlining why machine intelligence is experiencing a resurgence, documenting the enabling factors of this resurgence. (Kevin Kelly, for example chalks it up to cheap parallel computing, large datasets, and better algorithms.) I focused on understanding the ecosystem on a company-by-company level and drawing implications from that.Yes, it’s true, machine intelligence is transforming the enterprise, industries and humans alike.On a high level it’s easy to understand why machine intelligence is important, but it wasn’t until I laid out what many of these companies are actually doing that I started to grok how much it is already transforming everything around us. As Kevin Kelly more provocatively put it, “the business plans of the next 10,000 startups are easy to forecast: Take X and add AI”. In many cases you don’t even need the X — machine intelligence will certainly transform existing industries, but will also likely create entirely new ones.Machine intelligence is enabling applications we already expect like automated assistants (Siri), adorable robots (Jibo), and identifying people in images (like the highly effective but unfortunately named DeepFace). However, it’s also doing the unexpected: protecting children from sex trafficking, reducing the chemical content in the lettuce we eat, helping us buy shoes online that fit our feet precisely, and destroying 80's classic video games.Many companies will be acquired.I was surprised to find that over 10% of the eligible (non-public) companies on the slide have been acquired. It was in stark contrast to big data landscape we created, which had very few acquisitions at the time.No jaw will drop when I reveal that Google is the number one acquirer, though there were more than 15 different acquirers just for the companies on this chart. My guess is that by the end of 2015 almost another 10% will be acquired. For thoughts on which specific ones will get snapped up in the next year you’ll have to twist my arm…Big companies have a disproportionate advantage, especially those that build consumer products.The giants in search (Google, Baidu), social networks (Facebook, LinkedIn, Pinterest), content (Netflix, Yahoo!), mobile (Apple) and e-commerce (Amazon) are in an incredible position. They have massive datasets and constant consumer interactions that enable tight feedback loops for their algorithms (and these factors combine to create powerful network effects) — and they have the most to gain from the low hanging fruit that machine intelligence bears.Best-in-class personalization and recommendation algorithms have enabled these companies’ success (it’s both impressive and disconcerting that Facebook recommends you add the person you had a crush on in college and Netflix tees up that perfect guilty pleasure sitcom). Now they are all competing in a new battlefield: the move to mobile. Winning mobile will require lots of machine intelligence: state of the art natural language interfaces (like Apple’s Siri), visual search (like Amazon’s “FireFly”), and dynamic question answering technology that tells you the answer instead of providing a menu of links (all of the search companies are wrestling with this).Large enterprise companies (IBM and Microsoft) have also made incredible strides in the field, though they don’t have the same human-facing requirements so are focusing their attention more on knowledge representation tasks on large industry datasets, like IBM Watson’s application to assist doctors with diagnoses.The talent’s in the New (AI)vy League.In the last 20 years, most of the best minds in machine intelligence (especially the ‘hardcore AI’ types) worked in academia. They developed new machine intelligence methods, but there were few real world applications that could drive business value.Now that real world applications of more complex machine intelligence methods like deep belief nets and hierarchical neural networks are starting to solve real world problems, we’re seeing academic talent move to corporate settings. Facebook recruited NYU professors Yann LeCun and Rob Fergus to their AI Lab, Google hired University of Toronto’s Geoffrey Hinton, Baidu wooed Andrew Ng. It’s important to note that they all still give back significantly to the academic community (one of LeCun’s lab mandates is to work on core research to give back to the community, Hinton spends half of his time teaching, Ng has made machine intelligence more accessible through Coursera) but it is clear that a lot of the intellectual horsepower is moving away from academia.For aspiring minds in the space, these corporate labs not only offer lucrative salaries and access to the “godfathers” of the industry, but, the most important ingredient: data. These labs offer talent access to datasets they could never get otherwise (the ImageNet dataset is fantastic, but can’t compare to what Facebook, Google, and Baidu have in house). As a result, we’ll likely see corporations become the home of many of the most important innovations in machine intelligence and recruit many of the graduate students and postdocs that would have otherwise stayed in academia.There will be a peace dividend.Big companies have an inherent advantage and it’s likely that the ones who will win the machine intelligence race will be even more powerful than they are today. However, the good news for the rest of the world is that the core technology they develop will rapidly spill into other areas, both via departing talent and published research.Similar to the big data revolution, which was sparked by the release of Google’s BigTable and BigQuery papers, we will see corporations release equally groundbreaking new technologies into the community. Those innovations will be adapted to new industries and use cases that the Googles of the world don’t have the DNA or desire to tackle.Opportunities for entrepreneurs:“My company does deep learning for X”Few words will make you more popular in 2015. That is, if you can credibly say them.Deep learning is a particularly popular method in the machine intelligence field that has been getting a lot of attention. Google, Facebook, and Baidu have achieved excellent results with the method for vision and language based tasks and startups like Enlitic have shown promising results as well.Yes, it will be an overused buzzword with excitement ahead of results and business models, but unlike the hundreds of companies that say they do “big data”, it’s much easier to cut to the chase in terms of verifying credibility here if you’re paying attention.The most exciting part about the deep learning method is that when applied with the appropriate levels of care and feeding, it can replace some of the intuition that comes from domain expertise with automatically-learned features. The hope is that, in many cases, it will allow us to fundamentally rethink what a best-in-class solution is.As an investor who is curious about the quirkier applications of data and machine intelligence, I can’t wait to see what creative problems deep learning practitioners try to solve. I completely agree with Jeff Hawkins when he says a lot of the killer applications of these types of technologies will sneak up on us. I fully intend to keep an open mind.“Acquihire as a business model”People say that data scientists are unicorns in short supply. The talent crunch in machine intelligence will make it look like we had a glut of data scientists. In the data field, many people had industry experience over the past decade. Most hardcore machine intelligence work has only been in academia. We won’t be able to grow this talent overnight.This shortage of talent is a boon for founders who actually understand machine intelligence. A lot of companies in the space will get seed funding because there are early signs that the acquihire price for a machine intelligence expert is north of 5x that of a normal technical acquihire (take, for example Deep Mind, where price per technical head was somewhere between $5–10M, if we choose to consider it in the acquihire category). I’ve had multiple friends ask me, only semi-jokingly, “Shivon, should I just round up all of my smartest friends in the AI world and call it a company?” To be honest, I’m not sure what to tell them. (At Bloomberg Beta, we’d rather back companies building for the long term, but that doesn’t mean this won’t be a lucrative strategy for many enterprising founders.)A good demo is disproportionately valuable in machine intelligenceI remember watching Watson play Jeopardy. When it struggled at the beginning I felt really sad for it. When it started trouncing its competitors I remember cheering it on as if it were the Toronto Maple Leafs in the Stanley Cup finals (disclaimers: (1) I was an IBMer at the time so was biased towards my team (2) the Maple Leafs have not made the finals during my lifetime — yet — so that was purely a hypothetical).Why do these awe-inspiring demos matter? The last wave of technology companies to IPO didn’t have demos that most of us would watch, so why should machine intelligence companies? The last wave of companies were very computer-like: database companies, enterprise applications, and the like. Sure, I’d like to see a 10x more performant database, but most people wouldn’t care. Machine intelligence wins and loses on demos because 1) the technology is very human, enough to inspire shock and awe, 2) business models tend to take a while to form, so they need more funding for longer period of time to get them there, 3) they are fantastic acquisition bait.Watson beat the world’s best humans at trivia, even if it thought Toronto was a US city. DeepMind blew people away by beating video games. Vicarious took on CAPTCHA. There are a few companies still in stealth that promise to impress beyond that, and I can’t wait to see if they get there.Demo or not, I’d love to talk to anyone using machine intelligence to change the world. There’s no industry too unsexy, no problem too geeky. I’d love to be there to help so don’t be shy.I hope this landscape chart sparks a conversation. The goal to is make this a living document and I want to know if there are companies or categories missing. I welcome feedback and would like to put together a dynamic visualization where I can add more companies and dimensions to the data (methods used, data types, end users, investment to date, location, etc.) so that folks can interact with it to better explore the space.Questions and comments: Please email me. Thank you to Andrew Paprocki, Aria Haghighi, Beau Cronin, Ben Lorica, Doug Fulop, David Andrzejewski, Eric Berlow, Eric Jonas, Gary Kazantsev, Gideon Mann, Greg Smithies, Heidi Skinner, Jack Clark, Jon Lehr, Kurt Keutzer, Lauren Barless, Pete Skomoroch, Pete Warden, Roger Magoulas, Sean Gourley, Stephen Purpura, Wes McKinney, Zach Bogue, the Quid team, and the Bloomberg Beta team for your ever-helpful perspectives!Disclaimer: Bloomberg Beta is an investor in Adatao, Alation, Aviso, BrightFunnel, Context Relevant, Mavrx, Newsle, Orbital Insights, Pop Up Archive, and two others on the chart that are still undisclosed. We’re also investors in a few other machine intelligence companies that aren’t focusing on areas that were a fit for this landscape, so we left them off.For the full resolution version of the landscape please click here.",10/12/2014,0,5.0,8.0,1400.0,1050.0,1.0,0.0,0.0,23.0,en
4113,Understanding Input and Output shapes in LSTM | Keras,Medium,Shiva Verma,954.0,3.0,394.0,"Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM.Let’s first understand the Input and its shape in LSTM Keras. The input data to LSTM looks like the following diagram.You always have to give a three-dimensional array as an input to your LSTM network. Where the first dimension represents the batch size, the second dimension represents the time-steps and the third dimension represents the number of units in one input sequence. For example, the input shape looks like (batch_size, time_steps, units). Let’s look at an example in Keras.Let’s look at the input_shape argument. Though it seems input is a 2D array, we actually have to pass a 3D array with a shape of (batch_size, 2, 10). Means the value of time steps is 2, input units are 10 and you have the flexibility to feed any batch size at the time of fitting the data to the network.You can also give an argument called batch_input_shape instead of input_shape. The difference is now you have to give a fixed batch size and your input array shape will look like (8, 2, 10). If you try to feed a different batch size other than 8, you will get an error.Now, let’s look at the output and its shape in the LSTM network.Let’s look at the other arguments. Argument units represent the number of output units in the LSTM which is 3 here. So output shape is (None, 3). The first dimension of output is None because we do not know the batch size in advance. So the actual output shape will be (batch_size, 3) here.Here we see that I defined batch_size in advance and the output shape became (8, 3) which makes sense.Now, look at another argument return_sequences. This argument tells whether to return the output at each time step instead of the final time step. As we set the return_sequences to True, the output shape becomes a 3D array, instead of a 2D array. Now the shape of the output is (8, 2, 3). We see that there is one extra dimension in between representing the number of time steps.Please read my next article if you want to understand the Input and Output shapes for CNN.towardsdatascience.com",14/01/2019,0,31.0,10.0,630.0,231.0,7.0,1.0,0.0,1.0,en
4114,Keras Embedding layer and Programetic Implementation of GLOVE Pre-Trained Embeddings,Analytics Vidhya,Akash Deep,28.0,8.0,1428.0,"Keras Embedding layer is first of Input layer for the neural networks. After the conversion of our raw input data in the token and padded sequence, now its time to feed the prepared input to the neural networks. In our previous two post we had covered step by step conversion of words into token and padded sequence, so i highly recommend to just have a look in my previous two articles on Tokenization and word embedding. The links are belowmedium.commedium.comBefore going towards the Keras embedding layers first we will briefly cover what and all we have discussed till now. We had started with Tokenization, basically in tokenization we used to convert words into tokens because machine doesn’t understand text and able conversion into tokens we used to pad the sequence of the sentence because context need to be maintained and the conversion of word to sequence also leads to uniformity and same length. Also we need to add dimensions to our each and every word which states that each word has different meaning and we need to scale that all words in fixed dimension spaces based on their similarity in the form of vectors and that dimension we are free too choose. It could be 10,20,100,200,300 anything. For pre trained word embedding we had seen the word2vec step by step in our last tutorial. Now we will cover implementation of GLOVE pre trained embedding quickly before entering in the keras Embedding layerAs we have discussed the basics of GLOVE in our last tutorial that it is also a kind word embedding. In this tutorial we will see the programetic implementation of GLOVEFirst we will download the pre trainned word embedding vector for GLOVE from this link https://github.com/stanfordnlp/GloVe. Open this link and go to Download pre trained word vectors, after downloading we can use it in our IDE. I am using google colab and after downloading i will upload this file in my google colab environment. This file consists of 4 lakh words with 100 vector dimension.Now we had downloaded the pre trained embedding we will store all 4 lakh word as a key and their weight vector i.e 100 dimension vector as a value in one empty dictionary by using the below codeWe will import numpy as we are doing numerical computation. Now we will use the same 2 sentence what i had used in my Tokenization blog. i will convert that 2 sentence in token and apply padding to the sentence. Below are the code for same. If you have any doubt in code i highly recommend go through my tokentization blog and link i had mentioned at starting of my post.If we want to see all the items(items are nothing but our words that we had feed to tokenizer) in to tokenizer object we can use below commandNow we will be assigning the pre-trained weight(already we had stored this in embeddings_index disctionary earlier in this post) to all the items present in our tokenizer object. This we can do by writing below python codes. we have created a matrix of size (vocab_size,100). vocab_size is nothing but (total words+1). In embedding_matrix we have 100 vectorized weight for all the 8 words of our sentence.We have transferred the pre-trained embedding weight of GLOVE to our words in sentence. Now its time for passing this all in the keras embedding layer. Congratulation we have successfully completed our pre trained word embedding concepts. Now we are good to go with keras embedding layerComing to keras embedding layer,as we have already discussed it is the 1st layer that will act as a input to our neural networks layer so we need to understand each and every aspects related to keras embedding layer in detail like what is the dimension of input it accepts? what is the output after the data get processed in the keras embedding layer? so we will answer all these questions one by oneKeras Embedding requires that the input data be integer encoded so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras that we had done already.Also, This is one of the flexible layer that can be used in variety of ways. These ways are listed below:It can be used alone to learn a word embedding that can be saved and used in another model later.It can be used as part of a deep learning model where the embedding is learned along with the model itself.It can be used to load a pre-trained word embedding model, a type of transfer learning, that we had already prepared with word2vec and GLOVEIt is also the first hidden layers of neural networks because we had already prepared the input and now we are passing it to first layer and by architecture of neural network we know that input is passed to the hidden layers. It must specify 3 arguments:The above 3 arguments is the most important and we need to keep in mind while dealing with keras embedding layer. The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).We have seen enough theory, now we will implement the keras embedding layer programmatically.First we need to import the keras embedding class from keras.layers by using below codefrom keras.layers import EmbeddingNow we need to define the sequential model as we had already discussed embedding is the first step to the model creation. Detail of the model creation is out of the scope of this post. we will look the detail architecture of the neural networks and how to implement it using keras in our coming blog. So as of now just need to remember that first we need to create a model object after that only we will be adding all the different different layers to our created model object. We can create a model object by writing below python codeAs we have created the model successfully now we will add embedding layer to this model and we already discussed in details what all the arguments or parameter we need to provide to create embedding layers.First we will create the embedding layer for baseline embeddings(i.e in which we had not assigned any pre trained weights and that we had already created in our fist tokenization blog again i highly recommend just go through that blog for better understanding)This we can do by writing below python code. We are free to add the output dimension whatever we want in this case. Its recommended just play with the dimension values for the better model accuracy and at last also we will add it to created model objectNow we will create the embedding layer for GLOVE model and that is the case of pre-trained embedding. One thing need to keep in mind we already had defined the dimension as 100 in this case and also we have the weights for all the 100 dimensions so simply we will assign all this to embedding layers, this is the power of pre trained word embeddings. This can be done by writing below python code. We need to specify trainable as False because we don’t want our embedding layer to be trained because already we had assigned the weights and that we will pass to the embedding layer using weights=[embedding_matrix].Congratulations, we have successfully created the keras embedding layer and also now we had headed towards the journey of the neural networks model for natural language processing. Lots of things are coming i am very excited for explaining architecture of neural networks and how it works and most important how we can code each and every aspects of the architecture smoothly. I will be giving the complete code for the embedding implementation for both baseline and pre trained word embedding at the end of this post. If you have any questions related to this post or any bit of code you are not understanding, let me know in comment section. I will be very happy to explain.I am planning to explain the architecture of neural network in my next blog post after that it will be very easy to understand or implement the neural networks and slowly slowly we will see the real time use cases how NLP are used in the industry along with that i am planning for the step step explanation how the neural networks are used in the computer vision world. Lots of things we will understand step by step just wait and have patience :-)",24/08/2020,0,14.0,0.0,700.0,198.0,12.0,1.0,0.0,3.0,en
4115,Understanding Vector Quantized Variational Autoencoders (VQ-VAE),Medium,Shashank Yadav,85.0,5.0,628.0,"From my most recent escapade into the deep learning literature I present to you this paper by Oord et. al. which presents the idea of using discrete latent embeddings for variational auto encoders. The proposed model is called Vector Quantized Variational Autoencoders (VQ-VAE). I really liked the idea and the results that came with it but found surprisingly few resources to develop an understanding. Here’s an attempt to help other who might venture into this domain after me.Like numerous other people Variational Autoencoders (VAEs) are my choice of generative models. Unlike GANs they are easier to train and reason about (No offence intended dear GANs). Going forward I assume you have some understanding of VAEs. If you don’t I suggest going through this post, I found it to be one of the simpler ones.So what is the big deal here? As you might recall, VAEs consist of 3 parts:Typically we assume this prior and posterior to be normally distributed with diagonal variance. The encoder is then used to predict the mean and variances of the posterior.In the proposed work however, the authors use discrete latent variables (instead of a continuous normal distribution). The posterior and prior distributions are categorical, and the samples drawn from these distributions index an embedding table. In other words:Many important real-world objects are discrete. For example in images we might have categories like “Cat”, “Car”, etc. and it might not make sense to interpolate between these categories. Discrete representations are also easier to model since each category has a single value whereas if we had a continuous latent space then we will need to normalize this density function and learn the dependencies between the different variables which could be very complex.Moreover, the authors claim that their model doesn’t suffer from posterior collapse, an issue that plagues VAEs in general and prevents making use of complex decoders.Fig 1 shows various top level components in the architecture along with dimensions at each step. Assuming we run our model over image data, here’s some nomenclature we’ll be using going forward:n : batch sizeh: image heightw: image widthc: number of channels in the input imaged: number of channels in the hidden stateNow the working can be explained in the following steps:The working of VQ layer can be explained in six steps as numbered in Fig 2:The total loss is actually composed of three components:2. Codebook loss: due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm which uses an l2 error to move the embedding vectors eᵢ towards the encoder output:3. Commitment loss: since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings eᵢ do not train as fast as the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embeddingImportant: Note that we’re training both the dictionary embeddings as well as encoder and decoder networkThe paper presents state of the art results on images, text as well as videos.You can find the results on audio here: https://avdnoord.github.io/homepage/vqvae/There are several implementations available in Tensorflow, Pytorch as well as keras. You can look through them here.There are two main ideas to be learnt from this paper:For more details go through the paper, it’d be easier to understand after going through the article. You can also play around with this Jupyter Notebook, open it in colab here. Happy learning!Note: Feel free to ask any doubts or give feedback/suggestions. All the diagrams used here have been created by the author. Feel free to use them along with a note of acknowledgement :-)📝 Read this story later in Journal.👩‍💻 Wake up every Sunday morning to the week’s most noteworthy stories in Tech waiting in your inbox. Read the Noteworthy in Tech newsletter.",01/09/2019,3,12.0,7.0,1164.0,413.0,5.0,6.0,0.0,10.0,en
4116,What does it mean by Bidirectional LSTM?,Analytics Vidhya,Jaimin Mungalpara,22.0,7.0,561.0,"This has turn the old approach by giving an input from both the direction and by this it can remember the long sequences.In my previous article we discussed about RNN, LSTM and GRU. Now, there are certain limitations are still persist with LSTM because it is not able to remember the context for a longer period of time.You can see in this LSTM architecture that information is still have to pass from longer path. LSTM and GRU are introduced to overcome the problem of vanishing gradient and sequential data memory but the architecture of both are having multiple sequential path. Thus, vanishing gradient problem is still persist. Also, LSTM and GRU can remember sequences of 10s and 100s but not 1000s or more.Bidirectional NetworkNow, when we are dealing with long sequences of data and the model is required to learn relationship between future and past word as well. we need to send data in that manner. To solve this problem bidirectional network was introduced. We can use bidirectional network with LSTM and well as RNN but dur to limitations ofIn bidirectional LSTM we give the input from both the directions from right to left and from left to right . Make a note this is not a backward propagation this is only the input which is given from both the side. So, the question is how the data is combined in output if we are having 2 inputs.Generally in normal LSTM network we take output directly as shown in first figure but in bidirectional LSTM network output of forward and backward layer at each stage is given to activation layer which is a neural network and output of this activation layer is considered. This output contains the information or relation of past and future word also.Let’s take an example, assume we are having a sentence likeHere we can not predict the next word with normal RNN network but this can be solved in bidirectional RNN network. Also, RNN network can be LSTM or GRU.Implementation of Bidirectional RNN on Tensorflow(Keras)Tensorflow implementationWe will get below null values as a result so we need to drop it .We have deleted all null values so that it can not affect the accuracy of the model . Now we will define X and Y as an independent and dependent variableNow, the key part of NLP is text preprocessing which we perform on independent variable using NLTK library. We will use re library to remove punctuations then we will pass the data from stop words list and then do stemming on the data.Now we will one hot encode the data as we have word list and we will get the index w.r.t vocab_sizeNext step is padding, as the sentences we have are different in size so we have to do padding to make them equal in length. We can use pre or post padding.From above all output we can see that how our sentences are preprocessed for the LSTM input. Now we can implement model to train on our data.Post trainign on 20 epochs I got below result.We can test our model finally on testing data and can check the confusion metrix.We got accuracy of 90%. However, we can improve this accuracy by working on different parameters like vocab_size, sentence length, LSTM layer size, number of epochs.Reference[1] S. Hochreiter, J. Schmidhuber, Long Short-Term Memory (1997), Neural Computation[2]http://colah.github.io/posts/2015-08-Understanding-LSTMs/[3]https://www.youtube.com/watch?v=MXPh_lMRwAI&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=22[4]https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0",09/02/2021,9,4.0,0.0,925.0,384.0,3.0,0.0,0.0,6.0,en
4117,K-Means vs. DBSCAN Clustering — For Beginners,Towards Data Science,Ekta Sharma,29.0,9.0,830.0,"Clustering is grouping of unlabeled data points in such a way that: The data points within the same group are similar to each other, and the data points in different groups are dissimilar to each other.The goal is to create clusters that have high intra-cluster similarity and low inter-cluster similarity.K-Means cluster is one of the most commonly used unsupervised machine learning clustering techniques. It is a centroid based clustering technique that needs you decide the number of clusters (centroids) and randomly places the cluster centroids to begin the clustering process. The goal is to divide N observations into K clusters repeatedly until no more groups can be formed.1. Decide the number of clusters. This number is called K and number of clusters is equal to the number of centroids. Based on the value of K, generate the coordinates for K random centroids.2. For every point, calculate the Euclidean distance between the point and each of the centroids.3. Assign the point to its nearest centroid. The points assigned to the same centroid form a cluster.4. Once clusters are formed, calculate new centroid for each cluster by taking the cluster mean. Cluster mean is the mean of the x and y coordinates of all the points belonging to the cluster.5. Repeat step 2, 3 and 4 until the centroids cannot move any further. In other words, repeat these steps until convergence.Label values represent the cluster number.The quality of clusters formed using K-Means largely depends on the selected value of K. A wrong choice of K can lead to poor clustering. So how to select K? Let’s take a look at the commonly used technique called “Elbow Method”. The goal is to select the K at which an elbow is formed.Steps:1. For different values of K, execute the following steps:2. For each cluster, calculate the sum of squared distance of every point to its centroid.3. Add the sum of squared distances of each cluster to get the total sum of squared distances for that value of K.4. Keep adding the total sum of squared distances for each K to a list.5. Plot the sum of squared distances (using the list created in the previous step) and their K values.6. Select the K at which a sharp change occurs (looks like an elbow of the curve).Looking at this plot, we can see the elbow at K=3 and hence that is our optimal number of clusters for this dataset.Sometimes we can end up with multiple values showing an elbow. In this case, to find the best K, an evaluation metric like Silhouette Coefficient can be used. The K that will return the highest positive value for the Silhouette Coefficient should be selected.DBSCAN is a density-based clustering algorithm that forms clusters of dense regions of data points ignoring the low-density areas (considering them as noise).DBSCAN uses the following two user defined parameters for clustering:Epsilon (eps): It is defined as the maximum distance between two points to be considered as neighboring points (belonging to the same cluster).Minimum Points (min_samples or minPts): This defines the minimum number of neighboring points that a given point needs to be considered a core data point. This includes the point itself. For example, if minimum number of points is set to 4, then a given point needs to have 3 or more neighboring data points to be considered a core data point.If minimum number of points meet the epsilon distance requirement then they are considered as a cluster.1. Decide the value of eps and minPts.2. For each point:3. For each core point, if it not already assigned to a cluster than create a new cluster. Recursively find all its neighboring points and assign them the same cluster as the core point.4. Continue these steps until all the unvisited points are covered.Label = -1 means it is a noise point (outlier).Label = 0 or more, indicates the cluster number.DBSCAN clustering algorithm is sensitive to the eps value we choose. So how can we know that we have selected the optimal eps value? Here is a commonly used technique called “Knee method”. The goal is to find the average of distances for every point to its K nearest neighbors and select the distance at which maximum curvature or a sharp change happens. The value of K is set to be equal to minPoints.Here is an example to show optimal eps value selection using Scikit Learn’s NearestNeighbors module.The optimal value should be the value at which we see maximum curvature which in this case seems to be near 0.5.Sometimes we can end up with multiple values showing a sharp change. In this case, to find the best K, an evaluation metric like Silhouette Coefficient can be used. The K that will return the highest positive value for the Silhouette Coefficient should be selected.When to use which of these two clustering techniques, depends on the problem. Even though K-Means is the most popular clustering technique, there are use cases where using DBSCAN results in better clusters.",27/05/2020,9,16.0,4.0,476.0,336.0,7.0,5.0,0.0,0.0,en
4118,A gentle introduction to Deep Reinforcement Learning,Towards Data Science,Jordi TORRES.AI,1700.0,23.0,4855.0,"This is the first post of the series “Deep Reinforcement Learning Explained”; an introductory series that gradually and with a practical approach introduces the reader to the basic concepts and methods used in modern Deep Reinforcement Learning.Spanish version of this publication:medium.comDeep Reinforcement Learning (DRL), a very fast-moving field, is the combination of Reinforcement Learning and Deep Learning. It is also the most trending type of Machine Learning because it can solve a wide range of complex decision-making tasks that were previously out of reach for a machine to solve real-world problems with human-like intelligence.Today I’m starting a series about Deep Reinforcement Learning that will bring the topic closer to the reader. The purpose is to review the field from specialized terms and jargons to fundamental concepts and classical algorithms in the area, that newbies would not get lost while starting in this amazing area.My first serious contact with Deep Reinforcement Learning was in Cadiz (Spain), during the Machine Learning Summer School in 2016. I attended the three days seminar of John Schulman (at that time from UC Berkeley and cofounder of OpenAI) about Deep Reinforcement Learning.It was awesome, but I also have to confess that it was tremendously difficult for me to follow John’s explanations. It’s been a long time since then, and thanks to working with Xavier Giró and Ph.D. students like Victor Campos and MPh.D.am Bellver, I’ve been able to move forward and enjoy the subject.But even though several years have passed since then, I sincerely believe that the taxonomy of different approaches to Reinforcement Learning that he presented is still a good scheme to organize knowledge for beginners.Dynamic Programming is actually what most reinforcement learning courses in textbooks start. I will do that, but before, as John did in his seminar, I will introduce the Cross-Entropy method, a sort of evolutionary algorithm, although most books do not deal with it. It will go very well with this first method to introduce deep learning in reinforcement learning, Deep Reinforcement Learning, because it is a straightforward method to implement, and it works surprisingly well.With this method, we will be able to do a convenient review of how Deep Learning and Reinforcement Learning collaborate before entering the more classical approaches of treating an RL problem without considering DL such as Dynamic Programming, Monte Carlo, Temporal Difference Learning following the order of the vast majority of academic books on the subject. We will then dedicate the last part of this series to the most fundamental algorithms (not the state of the art because it is pervasive) of DL + RL as Policy Gradient Methods.Specifically, in this first publication, we will briefly present what Deep Reinforcement Learning is and the basic terms used in this research and innovation area.I think that Deep Reinforcement Learning is one of the most exciting fields in Artificial Intelligence. It’s marrying the power and the ability of deep neural networks to represent and comprehend the world with the ability to act on that understanding. Let’s see if I’m able to share that excitement. Here we go!Exciting news in Artificial Intelligence (AI) has just happened in recent years. For instance, AlphaGo defeated the best professional human player in the game of Go. Or last year, for example, our friend Oriol Vinyals and his team in DeepMind showed the AlphaStar Agent beat professional players at the game of StarCraft II. Or a few months later, OpenAI’s Dota-2-playing bot became the first AI system to beat the world champions in an e-sports game. All these systems have in common that they use Deep Reinforcement Learning (DRL). But what are AI and DRL?We have to take a step back to look at the types of learning. Sometimes the terminology itself can confuse us with the fundamentals. Artificial Intelligence, the main field of computer science in which Reinforcement Learning (RL) falls into, is a discipline concerned with creating computer programs that display humanlike “intelligence”.What do we mean when we talk about Artificial Intelligence? Artificial intelligence (AI) is a vast area. Even an authoritative AI textbook Artificial Intelligence, a modern approach written by Stuart Rusell and Peter Norvig, does not give a precise definition and discuss definitions of AI from different perspectives:Artificial Intelligence: A Modern Approach (AIMA) ·3rd edition, Stuart J Russell and Peter Norvig, Prentice Hall, 2009. ISBN 0–13–604259–7Without a doubt, this book is the best starting point to have a global vision of the subject. But trying to make a more general approach (purpose of this series), we could accept a simple definition in which by Artificial Intelligence we refer to that intelligence shown by machines, in contrast to the natural intelligence of humans. In this sense, a possible concise and general definition of Artificial Intelligence could be the effort to automate intellectual tasks usually performed by humans.As such, the area of artificial intelligence is a vast scientific field that covers many areas of knowledge related to machine learning; even many more approaches are not always cataloged as Machine Learning is included by my university colleagues who are experts in the subject. Besides, over time, as computers have been increasingly able to “do things”, tasks or technologies considered “smart” have been changing.Furthermore, since the 1950s, Artificial Intelligence has experienced several waves of optimism, followed by disappointment and loss of funding and interest (periods known as AI winter), followed by new approaches, success, and financing. Moreover, during most of its history, Artificial Intelligence research has been dynamically divided into subfields based on technical considerations or concrete mathematical tools and with research communities that sometimes did not communicate sufficiently with each other.Machine Learning (ML) is in itself a large field of research and development. In particular, Machine Learning could be defined as the subfield of Artificial Intelligence that gives computers the ability to learn without being explicitly programmed, that is, without requiring the programmer to indicate the rules that must be followed to achieve their task; the computers do them automatically.Generalizing, we can say that Machine Learning consists of developing a prediction “algorithm” for a particular use case for each problem. These algorithms learn from the data to find patterns or trends to understand what the data tell us, and in this way, build a model to predict and classify the elements.Given the maturity of the research area in Machine Learning, there are many well-established approaches to Machine Learning. Each of them uses a different algorithmic structure to optimize the predictions based on the received data. Machine Learning is a broad field with a complex taxonomy of algorithms that are grouped, in general, into three main categories:Orthogonal to this categorization, we can consider a powerful approach to ML, called Deep Learning (DL), a topic of which we have discussed extensively in previous posts. Remember that Deep Learning algorithms are based on artificial neural networks, whose algorithmic structures allow models composed of multiple processing layers to learn data representations with various abstraction levels.DL is not a separate ML branch, so it’s not a different task than those described above. DL is a collection of techniques and methods for using neural networks to solve ML tasks, either Supervised Learning, Unsupervised Learning, or Reinforcement Learning. We can represent it graphically in Figure 1.Deep Learning is one of the best tools that we have today to handle unstructured environments; they can learn from large amounts of data or discover patterns. But this is not decision-making; it is a recognition problem. Reinforcement Learning provides this feature.Reinforcement Learning can solve the problems using a variety of ML methods and techniques, from decision trees to SVMs, to neural networks. However, in this series, we only use neural networks; this is what the “deep” part of DRL refers to, after all. However, neural networks are not necessarily the best solution to every problem. For instance, neural networks are very data-hungry and challenging to interpret. Still, without doubt, neural networks are at this moment one of the most powerful techniques available, and their performance is often the best.In this section, we provide a brief first approach to RL, due it is essential for a good understanding of deep reinforcement learning, a particular type of RL, with deep neural networks for state representation and/or function approximation for value function, policy, and so on.Learning by interacting with our Environment is probably the first approach that comes to our mind when we think about the nature of learning. It is the way we intuit that an infant learns. And we know that such interactions are undoubtedly an essential source of knowledge about our environment and ourselves throughout people’s lives, not just infants. For example, when we are learning to drive a car, we are entirely aware of how the environment responds to what we do, and we also seek to influence what happens in our environment through our actions. Learning from the interaction is a fundamental concept that underlies almost all learning theories and is the foundation of Reinforcement Learning.The approach of Reinforcement Learning is much more focused on goal-directed learning from interaction than are other approaches to Machine Learning. The learning entity is not told what actions to take, but instead must discover for itself which actions produce the greatest reward, its goal, by testing them by “trial and error.” Furthermore, these actions can affect not only the immediate reward but also the future ones, “delayed rewards”, since the current actions will determine future situations (how it happens in real life). These two characteristics, “trial and error” search and “delayed reward”, are two distinguishing characteristics of reinforcement learning that we will cover throughout this series of posts.Reinforcement Learning (RL) is a field that is influenced by a variety of other well-established fields that tackle decision-making problems under uncertainty. For instance, Control Theory studies ways to control complex known dynamical systems; however, the dynamics of the systems we try to control are usually known in advance, unlike the case of DRL, which is not known in advance. Another field can be Operations Research that also studies decision-making under uncertainty but often contemplates much larger action spaces than those commonly seen in RL.As a result, there is a synergy between these fields, which is undoubtedly positive for science advancement. But it also brings some inconsistencies in terminologies, notations, and so on. That is why in this section, we will provide a detailed introduction to terminologies and notations that we will use throughout the series.Reinforcement Learning is essentially a mathematical formalization of a decision-making problem that we will introduce later in this series.In Reinforcement Learning there are two core components:For example, in the tic-tac-toe game, we can consider that the Agent is one of the players, and the Environment includes the board game and the other player.These two core components continuously interact so that the Agent attempts to influence the Environment through actions, and the Environment reacts to the Agent’s actions. How the environment reacts to specific actions is defined by a model that may or may not be known by the Agent, and this differentiates two circumstances:The Environment is represented by a set of variables related to the problem (very dependent on the type of problem we want to solve). This set of variables and all the possible values they can take are referred to as the state space. A state is an instantiation of the state space, a set of values the variables take.Due that we are considering that the Agent doesn’t have access to the actual full state of the Environment, it is usually called observation, the part of the state that the Agent can observe. However, we will often see in the literature observations and states being used interchangeably, so we will do this in this series of posts.At each state, the Environment makes available a set of actions, from which the Agent will choose an action. The Agent influences the Environment through these actions, and the Environment may change states as a response to the Agent’s action. The function responsible for this mapping is called in the literature transition function or transition probabilities between states.The Environment commonly has a well-defined task and may provide to the Agent a reward signal as a direct answer to the Agent’s actions. This reward is feedback on how well the last action contributes to achieving the task to be performed by the Environment. The function responsible for this mapping is called the reward function. As we will see later, the Agent’s goal is to maximize the overall reward it receives, and so rewards are the motivation the Agent needs to act in the desired behavior.Let’s summarize in the following Figure the concepts introduced earlier in the Reinforcement Learning cycle:Generally speaking, Reinforcement Learning is basically about turning this Figure into a mathematical formalism.The cycle begins with the Agent observing the Environment (step 1) and receiving a state and a reward. The Agent uses this state and reward for deciding the next action to take (step 2). The Agent then sends an action to the Environment in an attempt to control it in a favorable way (step 3). Finally, the environment transitions, and its internal state changes as a consequence due to the previous state and the Agent’s action (step 4). Then, the cycle repeats.The task the Agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Conversely, tasks that do not, are called continuous tasks, for example learning forward motion. The sequence of time steps from the beginning to the end of an episodic task is called an episode.As we will see, Agents may take several time steps and episodes to learn how to solve a task. The sum of rewards collected in a single episode is called a return. Agents are often designed to maximize the return.One of the limitations is that these rewards are not disclosed to the Agent until the end of an episode, which we introduced earlier as “delayed reward”. For example, in the game of tic-tac-toe the rewards for each movement (action) are not known until the end of the game. It would be a positive reward if the agent won the game (because the agent had achieved the overall desired outcome) or a negative reward (penalties) if the agent had lost the game.Another important characteristic, and challenge in Reinforcement Learning, is the trade-off between “exploration” and “exploitation”. Trying to obtain many rewards, an Agent must prefer actions that it has tried in the past and knows that will be effective actions in producing reward. But to discover such actions, paradoxically, it has to try actions that it has not selected never before.In summary, an Agent has to exploit what it has already experienced to obtain as much reward as possible, but at the same time, it also has to explore to make select better action in the future. The exploration-exploitation dilemma is a crucial topic and still an unsolved research topic. We will talk about this trade-off later in this series.Let’s strengthen our understanding of Reinforcement Learning by looking at a simple example, a Frozen Lake (very slippery) where our agent can skate:The Frozen-Lake Environment that we will use as an example is an ice skating rink, divided into 16 cells (4x4), and as shown in the figure below, some of the cells have broken the ice. The skater named Agent begins to skate in the top-left position, and its goal is to reach the bottom-right place avoiding falling into the four holes in the track.The described example is coded as the Frozen-Lake Environment from Gym. With this example of Environment, we will review and clarify the RL terminology introduced until now. It will also be useful for future posts in this series to have this example.OpenAI is an artificial intelligence (AI) research organization that provides a famous toolkit called Gym for training a reinforcement learning agent to develop and compare RL algorithms. Gym offers a variety of environments for training an RL agent ranging from classic control tasks to Atari game environments. We can train our RL agent to learn in these simulated environments using various RL algorithms. Throughout the series, we will use the Gym toolkit to build and evaluate reinforcement learning algorithms for several classic control tasks such as Cart-Pole balancing or mountain car climbing.Gym also provides 59 Atari game environments, including Pong, Space Invaders, Air Raid, Asteroids, Centipede, Ms. Pac-Man, etc. Training our reinforcement learning agent to play Atari games is an interesting as well as challenging task. Later in this series, we will train our DQN reinforcement learning agent to play Atari Pong game environment.Let’s introduce as an example one of the most straightforward environments called Frozen-Lake environment.Frozen-Lake Environment is from the so-called grid-world category when the Agent lives in a grid of size 4x4 (has 16 cells), which means a state space composed of 16 states (0–15) in the i, j coordinates of the grid-world.In Frozen-Lake, the Agent always starts at a top-left position, and its goal is to reach the bottom-right position of the grid. There are four holes in the fixed cells of the grid, and if the Agent gets into those holes, the episode ends, and the reward obtained is zero. If the Agent reaches the destination cell, it receives a reward of +1, and the episode ends. The following Figure shows a visual representation of the Frozen-Lake Environment:To reach the goal, the Agent has an action space composed of four directions movements: up, down, left, and right. We also know that there is a fence around the lake, so if the Agent tries to move out of the grid world, it will just bounce back to the cell from which it tried to move.Because the lake is frozen, the world is slippery, so the Agent’s actions do not always turn out as expected — there is a 33% chance that it will slip to the right or the left. If we want the Agent to move left, for example, there is a 33% probability that it will, indeed, move left, a 33% chance that it will end up in the cell above, and a 33% chance that it will end up in the cell below.This behavior of the Environment is reflected in the transition function or transition probabilities presented before. However, at this point, we do not need to go into more detail on this function and leave it for later.As a summary, we could represent all this information visually in the following Figure:Let’s look at how this Environment is represented in Gym. I suggest to use the Colab offered by Google to execute the code described in this post (Gym package is already installed). If you prefer to use your Python programming environment, you can install Gym using the steps provided here.The first step is to import Gym:Then, specify the game from Gym you want to use. We will use the Frozen-Lake game:The environment of the game can be reset to the initial state using:And, to see a view of the game state, we can use:The surface rendered by render()is presented using a grid like the following:Where the highlighted character indicates the position of the Agent in the current time step andThe official documentation can be found here to see the detailed usage and explanation of Gym toolkit.For the moment, we will create the most straightforward Agent that we can make that only does random actions. For this purpose, we will use the action_space.sample() that samples a random action from the action space.Assume that we allow a maximum of 10 iterations; the following code can be our “dumb” Agent:If we run this code, it will output something like the following lines, where we can observe the Timestep, the action, and the Environment state:In general, it is challenging, if not almost impossible, to find an episode of our “dumb” Agent in which, with randomly selected actions, it can overcome the obstacles and reach the goal cell. So how could we build an Agent to pursue it?. This is what we will present in the next installment of this series, where we will further formalize the problem and build a new Agent version that can learn to reach the goal cell.To finish this post, let’s review the basis of Reinforcement Learning for a moment, comparing it with other learning methods.In supervised learning, the system learns from training data that consists of a labeled pair of inputs and outputs. So, we train the model (Agent) using the training data in such a way that the model can generalize its learning to new unseen data (the labeled pairs of inputs and outputs guide the model in learning the given task).Let’s understand the difference between supervised and reinforcement learning with an example. Imagine we want to train a model to play chess using supervised learning. In this case, we will train the model to learn using a training dataset that includes all the moves a player can make in each state, along with labels indicating whether it is a good move or not. Whereas in the case of RL, our agent will not be given any sort of training data; instead, we just provide a reward to the agent for each action it performs. Then, the agent will learn by interacting with the environment, and it will choose its actions based on the reward it gets.Similar to supervised learning, in unsupervised learning, we train the model based on the training data. But in the case of unsupervised learning, the training data does not contain any labels. And this leads to a common misconception that RL is a kind of unsupervised learning due we don’t have labels as input data. But it is not. In unsupervised learning, the model learns the hidden structure in the input data, whereas, in RL, the model learns by maximizing the reward.A classic example is a movie recommendation system that wants to recommend a new movie to the user. With unsupervised learning, the model (agent) will find movies similar to the film the user (or users with a profile similar to the user) has viewed before and recommend new movies to the user. Instead, with Reinforcement Learning, the agent continually receives feedback from the user. This feedback represents rewards (a reward could be time spent watching a movie, time spent watching trailers, how many movies in a row have he watched, and so on). Based on the rewards, an RL agent will understand the user’s movie preference and then suggest new movies accordingly. It is essential to notice that an RL agent can know if the user’s movie preference changes and suggest new movies according to the user’s changed movie preference dynamically.We can think that we don’t have data in Reinforcement Learning as we have in Supervised or Unsupervised Learning. However, the data is actually the Environment because if you interact with this Environment, then data (trajectories) can be created, which are sequences of observations and actions. Then we can do some learning on top, and that’s basically the core of Reinforcement Learning.Sometimes, we can use extra data from people or trajectories that exist, for instance, in imitation learning. We might actually just observe a bunch of people playing the game, and we don’t need to know precisely how the Environment works. Sometimes we have explicitly given a data set, as a sort of a supervised data set, but in the pure Reinforcement Learning setting, the only data is the Environment.Reinforcement Learning has evolved rapidly over the past few years with a wide range of applications. One of the primary reasons for this evolution is the combination of Reinforcement Learning and Deep Learning. This is why we focus this series on presenting the basic state-of-the-art Deep Reinforcement Learning algorithms (DRL).The media has tended to focus on applications where DRL defeat humans at games, with examples as I mentioned at the beginning of this post: AlphaGo defeated the best professional human player in the game of Go; AlphaStar beat professional players at the game of StarCraft II; OpenAI’s Dota-2-playing bot beat the world champions in an e-sports game.Fortunately, there are many real-life applications of DRL. One of the well known is in the area of driverless cars. In manufacturing, intelligent robots are trained using DRL to place objects in the right position, reducing labor costs, and increasing productivity. Another popular application of RL is dynamic pricing that allows changing the price of products based on demand and supply. Also, in a recommendation system, RL is used to build a recommendation system where the user’s behavior continually changes.In today’s business activities, DRL is used extensively in supply chain management, demand forecasting, inventory management, handling warehouse operations, etc. DRL is also widely used in financial portfolio management, predicting, and trading in commercial transaction markets. DRL has been commonly used in several Natural Language Processing (NLP) tasks, such as abstractive text summarization, chatbots, etc.Many recent research papers suggest applications of DRL in healthcare, education systems, smart cities, among many others. In summary, no business sector is left untouched by DRL.DRL agents can sometimes control hazardous real-life Environments, like robots or cars, which increases the risk of making incorrect choices. There is an important field called safe RL that attempts to deal with this risk, for instance, learning a policy that maximizes rewards while operating within predefined safety constraints.Also, DRL agents are also at risk from an attack, like any other software system. But DRL adds a few new attack vectors over and above traditional machine learning systems because, in general, we are dealing with systems much more complex to understand and model.Considering the safety and security of DRL systems are outside the introductory scope of this post. Still, I would like the reader to be aware of it and if in the future you put a DRL system into operation, keep in mind that you should treat this point in more depth.Artificial Intelligence is definitely penetrating society, like electricity, what will we expect? The future we will “invent” is a choice we make jointly, not something that happens. We are in a position of power. With DRL, we have the power and authority to automate decisions and entire strategies.This is good! But as in most things in life, where there is light, can be the shadow, and DRL technology is hazardous in the wrong hands. I ask you that as engineers consider what we are building: Could our DRL system accidentally add bias? How does this affect individuals?. Or how does our solution affect the climate due to its energy consumption? Can be our DRL solution unintended used? Or use?. Or, in accordance with our ethics, can it have a type of use that we could consider nefarious?We must mull over the imminent adoption of Artificial Intelligence and its impact. Were we to go on to build Artificial Intelligence without regard to our responsibility of preventing its misuse, we can never expect to see Artificial Intelligence help humanity prosper.All of us, who are working or want to work on these topics, cannot shy away from our responsibility, because otherwise, we will regret it in the future.We started the post by understanding the basic idea of RL. We learned that RL is a trial and error learning process and the learning in RL happens based on a reward. We presented the difference between RL and the other ML paradigms. Finally, we looked into some real-life applications of RL and thought about the safety, security, and ethics of DRL.In the next post, we will learn about the Markov Decision Process (MDP) and how the RL environment can be modeled as an MDP. Next, we will review several important fundamental concepts involved in RL. See you in the next post!Post updated on 8/12/2020by UPC Barcelona Tech and Barcelona Supercomputing CenterA relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.Disclaimers — These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.",15/05/2020,5,49.0,16.0,1222.0,588.0,9.0,4.0,0.0,30.0,en
4119,Sketch-to-Color Image Generation | GANs,Towards Data Science,Tejas Morkar,59.0,11.0,2195.0,"This article is a part of the Gans-Series published by me on TowardsDataScience Publication on Medium. If you do not know what GANs are or if you have an idea about it but wish to quickly go over it again, I highly recommend you read the previous article which is just a 7 minutes long read and provides a simple understanding of GANs for people who are new to this amazing domain of Deep Learning.As you can tell from the gif shown above, this article is going to be all about learning how to create a Conditional GAN to predict colorful images from the given black and white sketch inputs without knowing the actual ground truth.Sketch to Color Image generation is an image-to-image translation model using Conditional Generative Adversarial Networks as described in the original paper by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros 2016, Image-to-Image Translation with Conditional Adversarial Networks.When I first came across this paper, it was amazing to see such great results shown by the authors and the fundamental idea was amazing on its own too.There are a lot of application scenarios of Conditional GANs that are depicted in the original paper by the authors. Some of which are listed below.We are going to build a Conditional Generative Adversarial Network which accepts a 256x256 px black and white sketch image and predicts the colored version of the image without knowing the ground truth. The model will be trained on the Anime Sketch-Colorization Pair Dataset available on Kaggle which contains 14.2k pairs of Sketch-Color Anime Images.When I trained the model on my system, I ran the model for 150 epochs which took approximately 23 hours on a single GeForce GTX 1060 6GB Graphic Card and 16 GB RAM. After all that hard work and patience, the results were totally worth it!To build this model I have used TensorFlow 2.x and most of the code is based on their awesome tutorial on Pix2Pix for CMP Facade Dataset which predicts building photos from facade labels. TensorFlow tutorials are a good way to understand the framework and work on some well-known projects. I highly recommend you to go through all the tutorials on the website — https://www.tensorflow.org/tutorials.To build this model, there are some basic requirements that you need to install on your system in order for it to work properly.If you are planning on using any cloud environments like Google Colab, you need to keep in mind that the training is going to take a lot of time as GANs are computationally quite heavy to run. Google Colab has an absolute timeout of 12 hours which means that the notebook kernel is reset so you’ll need to consider some points like mounting the Google Drive and saving checkpoints after regular intervals so that you can continue training from where it left off before the timeout.Download the Anime Sketch-Colorization Pair Dataset available on Kaggle and save it to a folder directory. The root folder will contain folders colorgram , train , and val . For everyone’s convenience let us call the path to the root folder as path/to/dataset/ .Once the basic requirements are checked and the dataset is downloaded to your machine, it’s time for you to get into coding your very own Conditional GAN.Before we jump right into it, note that the code which I’m going to provide shouldn't just be copied and pasted from here if you wish to understand the basic working behind it. And do not hesitate to ask your queries because that’s how things are learned — by asking.First, let’s initialize the parameters to configure the training of the model. As stated earlier, we will be using the TensorFlow framework so we’ll need to import it by using import tensorflow as tf .The os module is used to interact with the Operating System. We are going to use this for accessing and modifying the path variables to save checkpoints during training. The time module lets us display relative time and hence, we can check how much time each epoch took during the training.matplotlib is another cool python library which we will be using to plot and show images.BUFFER_SIZE is used when we shuffle the data samples while training. Higher the value of this more will be the degree of shuffling, and hence, higher will be the accuracy of the model. But with large data, it takes a lot of processing power to shuffle the images. For my system with Intel(R) Core(TM) i7–8750H CPU and 16 GB of RAM, it was possible to set it equal to the size of the train dataset samples i.e. 14,224.NOTE: The highest efficiency of shuffle() is when you set buffer_size equal to the size of data samples. This way it takes all the samples [in this case 14,224] in the primary memory and chooses a random one from those. If you set it to 10, it’ll take the 10 samples in the memory and choose a random one from those 10 samples and then repeat it for other remaining examples. So, check you machine capabilities and find out the sweet spot.BATCH_SIZE is used to divide the dataset into mini-batches for training. The higher this value is, the faster will be the process of training. But as you might have guessed already, higher batch size means a higher load on the machine.Now, if you take a look at the dataset, you have a single image of size 1024x512 px for one entry which has a colored image of size 512x512 px in the left and a black and white sketch image of size 512x512 px in the right.We will define a function load() that takes the image path as a parameter and returns an input_image which is the black and white sketch that we’ll give as an input to the model, and real_image which is the colored image that we want.Now that we have the data loaded, we need to do some preprocessing in order to prepare the data for the model.Given below are a few easy functions used for this purpose.resize() function is used to return the images as 286x286 px. This is done in order to have a uniform image size if by chance there is a differently sized image in the dataset. And decreasing size from 512x512 px to half of it also helps in speeding up the model training as it is computationally less heavy.random_crop() function returns the cropped input and real images which have the desired size of 256x256 px.normalize() function, as the name suggests, normalizes images to [-1, 1].In the random_jitter() function shown above, all the previous preprocessing functions are put together and random images are flipped horizontally. You can see what the preprocessing of data returns from images given below.load_image_train() function is used to put together all the previously seen functions and output the final preprocessed image.tf.data.Dataset.list_files() collects the path to all the png files available in the train/ folder of the dataset. Then the collection of these paths is mapped through and every path is sent individually as an argument to the load_image_train() function which returns the final preprocessed image and adds it to the train_dataset .Finally, this train_dataset is shuffled using the BUFFER_SIZE and then divided into mini-batches as discussed earlier.To load the test dataset, we will use a similar process except for a small change. Here we will omit the random_crop() and random_jitter() functions as there is no need to do this for testing the results. Also, we can omit to shuffle the dataset for the same reason.Let us build the generator model now which takes an input black and white sketch image of 256x256 px and outputs an image that hopefully resembles the colored ground truth image in the training dataset.The Generator model is a UNet Architecture Model and has skip connections to other layers than the intermediate one. Take note that it becomes complex to design such an architecture as the output and input shapes need to match to the connected layers, so design this carefully.The downsampling stack of layers has Convolutional layers which result in a decrease in the size of the input image. And once the decreased image goes through the upsampling stack of layers which has kind of “reverse” Convolutional layers, the size is restored back to 256x256 px. Hence, the output of the Generator Model is a 256x256 px image with 3 output channels.You can take a look at the model summary which is given below.The primary purpose of the discriminator model is to find out which image is from the actual training dataset and which is an output from the generator model.You can take a look at the model summary of the Discriminator given below. This is not as complex as the Generator model as it’s fundamental task is just to classify real and fake images.As we have two models with us, we are going to require two different loss functions to calculate their loss independently.The loss for the generator is calculated by finding the sigmoid cross-entropy loss of the output of the generator and an array of ones. This means that we are training it to trick the discriminator in outputting the value as 1, which means that it is a real image. Also, for the output to be structurally similar to the target image, we take L1 loss along with it. The value of LAMBDA is suggested to be kept 100 by authors of the original paper.For discriminator loss, we take the same sigmoid cross-entropy loss of the real images and an array of ones and add it with the cross-entropy loss of the output images of the generator model and array of zeros.Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rates in order to reduce the losses. Adam Optimizer is one of the best ones to use, in most of the use cases.As discussed earlier, cloud environments have a specific timeout which can interrupt the training process. Also, if you are using your local system, there may arise some cases where the training might be interrupted due to some reasons.GANs take a very long time to train and are computationally expensive. So, it is best to keep saving checkpoints at regular intervals so that you can restore to the latest checkpoint and continue from there without losing the previously done hard work by your machines.The above-given block of code is a basic python function which uses the pyplot module from matplotlib library to display the predicted images by the generator model.You can log the important metrics like losses in a file so that you can analyze it as the training progresses on tools like Tensorboard.A basic train step will consist of the following processes:TensorFlow is an awesome, easy to use framework for training models. And one small command like model.fit() does the magic for us.Unfortunately, it will not directly work over here as we have created two models that work together. But it is pretty easy to do this too.Here we iterate over for every epoch and assign the relative time to start variable. Then we display an example of the generated image by the generator model. This example helps us visualize how the generator gets better at generating better-colored images with every epoch. Then we call the train_step function for the model to learn from the calculated losses and gradients. And finally, we check if the epoch number is divisible by 5 to save a checkpoint. This means that we are saving a checkpoint after every 5 epochs of training are completed. After this entire epoch is completed, the start time is subtracted from the final relative time to count the time taken for that particular epoch.All we have to do now is run this one line of code and wait for the Model to do its magic on its own. Well let’s not give the entire credit to the model, we have done a lot of hard work and it’s time to see the results.Before moving forward, we must restore the latest checkpoint available in order to load the latest version of the trained model before testing it on the images.This randomly selects 5 images from the test_dataset and inputs them individually to the Generator Model. Now the model is trained well enough and predicts near-perfect colored versions of the input sketch images.Let’s not kill the model right after doing so much work, right?A model shouldn’t end its life in a Jupyter Notebook!- Rightly said by It takes only a line of code to save the entire model as a .H5 file which is supported by Keras models.So, that is it!We have not only seen how a Conditional GAN works but also have successfully implemented it to predict colored images from the given black and white input sketch images.You can go through the entire code and download it to see how it works on your system from my GitHub Repository.tejasmorkar.github.ioIf you face any problems, want to suggest some enhancements, or just want to leave a quick feedback, do not hesitate in contacting me through any medium that best suits you.LinkedIn: https://www.linkedin.com/in/tejasmorkar/GitHub: https://github.com/tejasmorkarTwitter: https://twitter.com/TejasMorkar",23/06/2020,0,25.0,26.0,913.0,701.0,9.0,3.0,0.0,21.0,en
4120,Batch vs Mini-batch vs Stochastic Gradient Descent with Code Examples,DataDrivenInvestor,Matheus Jacques,40.0,5.0,880.0,"One of the main questions that arise when studying Machine Learning and Deep Learning is the several types of Gradient Descent. Should I use Batch Gradient Descent? Mini-batch Gradient Descent or Stochastic Gradient Descent? In this post, we are going to understand the difference between those concepts and take a look at code implementations from Gradient Descent, to clarify these methods.Edit: Updated version here.At this point, we know that our matrix of weights W and our vector of bias b are the core values of our Neural Networks (NN) (Check the Deep Learning Basics post). We can make an analogy with these concepts with the memory in which a NN stores patterns, and it is through tuning these parameters that we teach a NN. The acting of tuning is done through the optimization algorithms, the amazing feature that allows NN to learn. After some time training the network, these patterns are learned and we have a set of weights and biases that hopefully correct classifies the inputs.One of the most common algorithms that help the NN to reach the correct values of weights and bias. The Gradient Descent (GD) is an algorithm to minimize the cost function J(W,b) in each step. It iteratively updates the weights and bias trying to reach the global minimum in a cost function.Reviewing this quickly, before we can compute the GD, first the inputs are taken and passed through all the nodes of a neural network, calculating the weighted sum of inputs, weights, and bias. This first pass is one of the main steps when calculating Gradient Descent and it is called Forward Propagation. Once we have an output, we compare this output with the expected output and calculate how far it is from each other, the error. With this error, we can now propagate it backward, updating every weight and bias and trying to minimize this error. And this part is called, as you may anticipate, Backward Propagation. The Backward Propagation step is calculated using derivatives and return the “gradients”, values that tell us in which direction we should follow to minimize the cost function.We are now ready to update the weight matrix W and the bias vector b. The gradient descent rule is as follows:In other words, the new weight/bias value will be the last one minus the gradient, moving it close to the global minimum value of the cost function. We also multiply this gradient to a learning rate alpha, which controls how big the step would be. For a more deep approach to Forward and Backward Propagation, Compute Losses, Gradient Descent, check this post.This classic Gradient Descent is also called Batch Gradient Descent. In this method, every epoch runs through all the training dataset, to only then calculate the loss and update the W and b values. Although it provides stable convergence and a stable error, this method uses the entire training set; hence it is very slow for big datasets.Imagine taking your dataset and dividing it into several chunks, or batches. So instead of waiting until the algorithm runs through the entire dataset to only after update the weights and bias, it updates at the end of each, so-called, mini-batch. This allows us to move quickly to the global minimum in the cost function and update the weights and biases multiple times per epoch now. The most common mini-batch sizes are 16, 32, 64, 128, 256, and 512. Most of the projects use Mini-batch GD because it is faster in larger datasets.To prepare the mini-batches, one most apply some preprocessing steps: randomizing the dataset to randomly split the dataset and then partitioning it in the right number of chunks. But what happens if we chose to set the number of batches to 1 or equal to the number of training examples?As stated before, in this gradient descent, each batch is equal to the entire dataset. That is:Where {1} denotes the first batch from the mini-batch. The downside is that it takes too long per iteration. This method can be used to training datasets with less than 2000 training examples.On another hand, in this method, each batch is equal to one example from the training set. In this example, the first mini-batch is equal to the first training example:Where (1) denotes the first training example. Here the downside is that it loses the advantage gained from vectorization, has more oscillation but converges faster.It is essential to understand the difference between these optimization algorithms, as they compose a key function for Neural Networks. In summary, although Batch GD has higher accuracy than Stochastic GD, the latter is faster. The middle ground of the two and the most adopted, Mini-batch GD, combine both to deliver good accuracy and good performance.It is possible to use only the Mini-batch Gradient Descent code to implement all versions of Gradient Descent, you just need to set the mini_batch_size equals one to Stochastic GD or the number of training examples to Batch GD. Thus, the main difference between Batch, Mini-batch, and Stochastic Gradient Descent is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.Reference: This blog post was based in the deeplearning.ai Coursera’s Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization course.",05/05/2020,3,8.0,9.0,467.0,389.0,5.0,3.0,0.0,5.0,en
4121,Lambda Functions with Practical Examples in Python,Towards Data Science,Susan Maina,401.0,6.0,857.0,"When I first came across lambda functions in python, I was very much intimidated and thought they were for advanced Pythonistas. Beginner python tutorials applaud the language for its readable syntax, but lambdas sure didn’t seem user-friendly.However, once I understood the general syntax and examined some simple use cases, using them was less scary.Simply put, a lambda function is just like any normal python function, except that it has no name when defining it, and it is contained in one line of code.A lambda function evaluates an expression for a given argument. You give the function a value (argument) and then provide the operation (expression). The keyword lambda must come first. A full colon (:) separates the argument and the expression.In the example code below, x is the argument and x+x is the expression.Before we get into practical applications, let’s mention some technicalities on what the python community thinks is good and bad with lambda functions.ProsConsAt the end of this article, we’ll look at commonly used code examples where Lambda functions are discouraged even though they seem legitimate.But first, let’s look at situations when to use lambda functions. Note that we use lambda functions a lot with python classes that take in a function as an argument, for example, map() and filter(). These are also called Higher-order functions.This is when you execute a lambda function on a single value.In the code above, the function was created and then immediately executed. This is an example of an immediately invoked function expression or IIFE.Filter(). This is a Python inbuilt library that returns only those values that fit certain criteria. The syntax is filter(function, iterable). The iterable can be any sequence such as a list, set, or series object (more below).The example below filters a list for even numbers. Note that the filter function returns a ‘Filter object’ and you need to encapsulate it with a list to return the values.Map(). This is another inbuilt python library with the syntax map(function, iterable).This returns a modified list where every value in the original list has been changed based on a function. The example below cubes every number in the list.A Series object is a column in a data frame, or put another way, a sequence of values with corresponding indices. Lambda functions can be used to manipulate values inside a Pandas dataframe.Let’s create a dummy dataframe about members of a family.Lambda with Apply() function by Pandas. This function applies an operation to every element of the column.To get the current age of each member, we subtract their birth year from the current year. In the lambda function below, x refers to a value in the birthyear column, and the expression is 2021(current year) minus the value.Lambda with Python’s Filter() function. This takes 2 arguments; one is a lambda function with a condition expression, two an iterable which for us is a series object. It returns a list of values that satisfy the condition.Lambda with Map() function by Pandas. Map works very much like apply() in that it modifies values of a column based on the expression.We can also perform conditional operations that return different values based on certain criteria.The code below returns ‘Male’ if the Status value is father or son, and returns ‘Female’ otherwise. Note that apply and map are interchangeable in this context.I mostly use Lambda functions on specific columns (series object) rather than the entire data frame, unless I want to modify the entire data frame with one expression.For example rounding all values to 1 decimal place, in which case all the columns have to be float or int datatypes because round() can’t work on strings.In the example below, we use apply on a dataframe and select the columns to modify in the Lambda function. Note that we must use axis=1 here so that the expression is applied column-wise.2. Passing functions inside Lambda functions. Using functions like abs which only take one number- argument is unnecessary with Lambda because you can directly pass the function into map() or apply().Ideally, functions inside lambda functions should take two or more arguments. Examples are pow(number,power) and round(number,ndigit). You can experiment with various in-built python functions to see which ones need Lambda functions in this context. I’ve done so in this notebook.3. Using Lambda functions when multiple lines of code are more readable. An example is when you are using if-else statements inside the lambda function. I used the example below earlier in this article.The same results can be achieved with the code below. I prefer this way because you can have endless conditions and the code is simple enough to follow. More on vectorized conditions here.Many programmers who don’t like Lambdas usually argue that you can replace them with the more understandable list comprehensions, built-in functions, and standard libraries. Generator expressions (similar to list comprehensions) are also handy alternatives to the map() and filter() functions.Whether or not you decide to embrace Lambda functions in your code, you need to understand what they are and how they are used because you will inevitably come across them in other peoples’ code.Check out the code used here in my GitHub. Thank you for reading!References:",17/06/2021,15,20.0,1.0,567.0,278.0,7.0,4.0,0.0,19.0,en
4122,Performance Metrics for Classification problems in Machine Learning,Medium,Mohammed Sunasra,555.0,10.0,2002.0,"“Numbers have an important story to tell. They rely on you to give them a voice.” — Stephen FewAfter doing the usual Feature Engineering, Selection, and of course, implementing a model and getting some output in forms of a probability or a class, the next step is to find out how effective is the model based on some metric using test datasets. Different performance metrics are used to evaluate different Machine Learning Algorithms. For now, we will be focusing on the ones used for Classification problems. We can use classification performance metrics such as Log-Loss, Accuracy, AUC(Area under Curve) etc. Another example of metric for evaluation of machine learning algorithms is precision, recall, which can be used for sorting algorithms primarily used by search engines.The metrics that you choose to evaluate your machine learning model is very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. Before wasting any more time, let’s jump right in and see what those metrics are.The Confusion matrix is one of the most intuitive and easiest (unless of course, you are not confused)metrics used for finding the correctness and accuracy of the model. It is used for Classification problem where the output can be of two or more types of classes.Before diving into what the confusion matrix is all about and what it conveys, Let’s say we are solving a classification problem where we are predicting whether a person is having cancer or not.Let’s give a label of to our target variable:1: When a person is having cancer 0: When a person is NOT having cancer.Alright! Now that we have identified the problem, the confusion matrix, is a table with two dimensions (“Actual” and “Predicted”), and sets of “classes” in both dimensions. Our Actual classifications are columns and Predicted ones are Rows.The Confusion matrix in itself is not a performance measure as such, but almost all of the performance metrics are based on Confusion Matrix and the numbers inside it.Ex: The case where a person is actually having cancer(1) and the model classifying his case as cancer(1) comes under True positive.2. True Negatives (TN): True negatives are the cases when the actual class of the data point was 0(False) and the predicted is also 0(FalseEx: The case where a person NOT having cancer and the model classifying his case as Not cancer comes under True Negatives.3. False Positives (FP): False positives are the cases when the actual class of the data point was 0(False) and the predicted is 1(True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one. (1)Ex: A person NOT having cancer and the model classifying his case as cancer comes under False Positives.4. False Negatives (FN): False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). False is because the model has predicted incorrectly and negative because the class predicted was a negative one. (0)Ex: A person having cancer and the model classifying his case as No-cancer comes under False Negatives.The ideal scenario that we all want is that the model should give 0 False Positives and 0 False Negatives. But that’s not the case in real life as any model will NOT be 100% accurate most of the times.When to minimise what?We know that there will be some error associated with every model that we use for predicting the true class of the target variable. This will result in False Positives and False Negatives(i.e Model classifying things incorrectly as compared to the actual class).There’s no hard rule that says what should be minimised in all the situations. It purely depends on the business needs and the context of the problem you are trying to solve. Based on that, we might want to minimise either False Positives or False negatives.1. Minimising False Negatives:Let’s say in our cancer detection problem example, out of 100 people, only 5 people have cancer. In this case, we want to correctly classify all the cancerous patients as even a very BAD model(Predicting everyone as NON-Cancerous) will give us a 95% accuracy(will come to what accuracy is). But, in order to capture all cancer cases, we might end up making a classification when the person actually NOT having cancer is classified as Cancerous. This might be okay as it is less dangerous than NOT identifying/capturing a cancerous patient since we will anyway send the cancer cases for further examination and reports. But missing a cancer patient will be a huge mistake as no further examination will be done on them.2. Minimising False Positives:For better understanding of False Positives, let’s use a different example where the model classifies whether an email is spam or notLet’s say that you are expecting an important email like hearing back from a recruiter or awaiting an admit letter from a university. Let’s assign a label to the target variable and say,1: “Email is a spam” and 0:”Email is not a spam”Suppose the Model classifies that important email that you are desperately waiting for, as Spam(case of False positive). Now, in this situation, this is pretty bad than classifying a spam email as important or not spam since in that case, we can still go ahead and manually delete it and it’s not a pain if it happens once a while. So in case of Spam email classification, minimising False positives is more important than False Negatives.Accuracy in classification problems is the number of correct predictions made by the model over all kinds predictions made.In the Numerator, are our correct predictions (True positives and True Negatives)(Marked as red in the fig above) and in the denominator, are the kind of all predictions made by the algorithm(Right as well as wrong ones).When to use Accuracy:Accuracy is a good measure when the target variable classes in the data are nearly balanced.Ex:60% classes in our fruits images data are apple and 40% are oranges.A model which predicts whether a new image is Apple or an Orange, 97% of times correctly is a very good measure in this example.When NOT to use Accuracy:Accuracy should NEVER be used as a measure when the target variable classes in the data are a majority of one class.Ex: In our cancer detection example with 100 people, only 5 people has cancer. Let’s say our model is very bad and predicts every case as No Cancer. In doing so, it has classified those 95 non-cancer patients correctly and 5 cancerous patients as Non-cancerous. Now even though the model is terrible at predicting cancer, The accuracy of such a bad model is also 95%.Let’s use the same confusion matrix as the one we used before for our cancer detection example.Precision is a measure that tells us what proportion of patients that we diagnosed as having cancer, actually had cancer. The predicted positives (People predicted as cancerous are TP and FP) and the people actually having a cancer are TP.Ex: In our cancer example with 100 people, only 5 people have cancer. Let’s say our model is very bad and predicts every case as Cancer. Since we are predicting everyone as having cancer, our denominator(True positives and False Positives) is 100 and the numerator, person having cancer and the model predicting his case as cancer is 5. So in this example, we can say that Precision of such model is 5%.Recall is a measure that tells us what proportion of patients that actually had cancer was diagnosed by the algorithm as having cancer. The actual positives (People having cancer are TP and FN) and the people diagnosed by the model having a cancer are TP. (Note: FN is included because the Person actually had a cancer even though the model predicted otherwise).Ex: In our cancer example with 100 people, 5 people actually have cancer. Let’s say that the model predicts every case as cancer.So our denominator(True positives and False Negatives) is 5 and the numerator, person having cancer and the model predicting his case as cancer is also 5(Since we predicted 5 cancer cases correctly). So in this example, we can say that the Recall of such model is 100%. And Precision of such a model(As we saw above) is 5%When to use Precision and When to use Recall?:It is clear that recall gives us information about a classifier’s performance with respect to false negatives (how many did we miss), while precision gives us information about its performance with respect to false positives(how many did we caught).Precision is about being precise. So even if we managed to capture only one cancer case, and we captured it correctly, then we are 100% precise.Recall is not so much about capturing cases correctly but more about capturing all cases that have “cancer” with the answer as “cancer”. So if we simply always say every case as “cancer”, we have 100% recall.So basically if we want to focus more on minimising False Negatives, we would want our Recall to be as close to 100% as possible without precision being too bad and if we want to focus on minimising False positives, then our focus should be to make Precision as close to 100% as possible.Specificity is a measure that tells us what proportion of patients that did NOT have cancer, were predicted by the model as non-cancerous. The actual negatives (People actually NOT having cancer are FP and TN) and the people diagnosed by us not having cancer are TN. (Note: FP is included because the Person did NOT actually have cancer even though the model predicted otherwise).Specificity is the exact opposite of Recall.Ex: In our cancer example with 100 people, 5 people actually have cancer. Let’s say that the model predicts every case as cancer.So our denominator(False positives and True Negatives) is 95 and the numerator, person not having cancer and the model predicting his case as no cancer is 0 (Since we predicted every case as cancer). So in this example, we can that that Specificity of such model is 0%.We don’t really want to carry both Precision and Recall in our pockets every time we make a model for solving a classification problem. So it’s best if we can get a single score that kind of represents both Precision(P) and Recall(R).One way to do that is simply taking their arithmetic mean. i.e (P + R) / 2 where P is Precision and R is Recall. But that’s pretty bad in some situations.Suppose we have 100 credit card transactions, of which 97 are legit and 3 are fraud and let’s say we came up a model that predicts everything as fraud. (Horrendous right!?)Precision and Recall for the example is shown in the fig below.Now, if we simply take arithmetic mean of both, then it comes out to be nearly 51%. We shouldn’t be giving such a moderate score to a terrible model since it’s just predicting every transaction as fraud.So, we need something more balanced than the arithmetic mean and that is harmonic mean.The Harmonic mean is given by the formula shown in the figure on the left.Harmonic mean is kind of an average when x and y are equal. But when x and y are different, then it’s closer to the smaller number as compared to the larger number.For our previous example, F1 Score = Harmonic Mean(Precision, Recall)F1 Score = 2 * Precision * Recall / (Precision + Recall) = 2*3*100/103 = 5%So if one number is really small between precision and recall, the F1 Score kind of raises a flag and is more closer to the smaller number than the bigger one, giving the model an appropriate score rather than just an arithmetic mean.So far, we have seen what the Confusion matrix is, what is Accuracy, Precision, Recall (or Sensitivity), Specificity and F1-score for a classification problem. In the next post, I will be discussing the other metrics that can be used in Classification problems like the AUC-ROC Curve, Log loss, F-Beta score etc.Till then stay tuned and Happy Machine Learning.",11/11/2017,0,26.0,24.0,596.0,416.0,8.0,1.0,0.0,0.0,en
4123,Few-Shot Learning with fast.ai,Towards Data Science,Igor Susmelj,54.0,5.0,953.0,"Lately, posts and tutorials about new deep learning architectures and training strategies have dominated the community. However, one very interesting research area, namely few-shot learning, is not getting the attention it deserves. If we want widespread adoption of ML we need to find ways to train them efficiently, with little data and code. In this tutorial, we will go through a Google Colab Notebook to train an image classification model using only 5 labeled samples per class. Using only 5 exemplary samples is also called 5-shot learning.Don’t forget to check out our Google Colab Notebook for the full code of this tutorial!Jupyter Notebook (Google Colab)The full code of this tutorial will be provided as a notebook. Jupyter Notebooks are python programming environments accessible by web browsers and are very useful for fast prototyping and experiments. Colab is a service from Google where you get access to notebooks running on instances for free.Fast.aiTraining a deep learning model can be quite complicated and involve 100s of lines of code. This is where fast.ai comes to the rescue. A library developed by former Kaggler Jeremy Howard specifically aimed to make training deep learning models fast and simple. Using fast.ai we can train and evaluate our classifier with just a few lines of code. Under the hood, fast.ai is using the PyTorch framework.LightlyLightly aims to solve the question of which samples you should work with. If you only label a few samples out of your dataset one of the key questions arising is how do you pick the samples? Lightly aims at solving exactly this problem by providing you with different methods and metrics for selecting your samplesWe start by installing the necessary dependencies and downloading the dataset. You can run any shell command in a notebook by start the code with an “!”E.g. to install our dependencies we can run the following code within a notebook cell:In this tutorial, we work with a dataset consisting of cats and dogs images. You can download it from Kaggle using the fastai CLI (command-line interface) by running the following command. Note that you need to adapt the token you get from Kaggle:In order to get robust results with our few-shot learning algorithm, we want our training set to cover the full space of samples. That means we don’t want lots of similar examples but rather select a very diverse set of images. To achieve this we can create an embedding of our dataset followed by a sampling method called coreset[1] sampling. Coreset sampling ensures that we build up our dataset by only adding samples which lie furthest apart from the existing set as possible.Now we will use the Lightly app and its python package lightly to select the most diverse 10 samples we want to work with. We first need to create an embedding. Lightly allows us to do this without any labels by leveraging recent success in self-supervised learning. We can simply run the following command to train the model for a few epochs and create our embedding:Finally, we need to upload our dataset and embedding to the Lightly app to run our selection algorithm. Since we don’t want to upload the images we can tell the CLI to consider only metadata of the samples.Once the data and embedding are uploaded we can go back to the web platform and run our sampling algorithm. This might take a minute to complete. If everything went smoothly you should see a plot with a slider. Move the slider to the left to only keep 10 samples in the new subset. Hint: You can use the arrow keys to move the slider one by one. Once we have our 10 samples selected we need to create a new tag (left menu). For this tutorial, we use “tiny” as the name and press the enter key to create it.Download the newly created subset using the following CLI command:You might see that the dataset you downloaded is not perfectly balanced. E.g. you might have 4 images of cats and 6 of dogs. This is due to the algorithm we chose for selecting the samples. Our goal was to cover the whole embedding/ feature space. It might very well be that there are more similar images of cats in our dataset than images of dogs. As a result, more images of dogs than cats will be selected.If you reach this point you should have a dataset we obtained using Lightly and Coreset sampling ready to be used to train our classifier.Fast.ai requires only a few lines of code to train an image classifier. We first need to create a dataset and then a learner object. Finally, we train the model using the .fit(...) method.To evaluate our model we use the test set of the cats and dogs dataset consisting of 2'000 images. Looking at the confusion matrix we see that our model mostly struggles with predicting dogs as being cats.Fast.ai also helps us here getting interpretable performance plots of our model with just a few lines of code.The library allows us also to look at the images from the test set with the highest loss of the trained model. You see that the model struggles with smaller dogs looking more similar to cats. We could improve accuracy by selecting more samples for the training routine. However, the goal of this tutorial was to show that by leveraging transfer learning and a smart data selection process you can get already high accuracy (>80%) with just a handful of training data.I hope you enjoyed this brief guide on how to use few-shot learning using fast.ai and Lightly. Follow me for further tutorials on Medium!Igor, co-founderLightly.ai[1]Ozean S., (2017), Active Learning for Convolutional Neural Networks: A Core-Set Approach",27/08/2020,0,5.0,0.0,544.0,600.0,3.0,0.0,0.0,11.0,en
4124,GAN — Introduction and Implementation — PART1: Implement a simple GAN in TF for MNIST handwritten digit generation,Towards Data Science,Manish Chablani,1700.0,5.0,629.0,"The idea behind GANs is that you have two networks, a generator GG and a discriminator DD, competing against each other. The generator makes fake data to pass to the discriminator. The discriminator also sees real data and predicts if the data it’s received is real or fake. The generator is trained to fool the discriminator, it wants to output data that looks as close as possible to real data. And the discriminator is trained to figure out which data is real and which is fake. What ends up happening is that the generator learns to make data that is indistinguishable from real data to the discriminator.This is equilibrium state and expectation is discriminator is emitting a probability of 0.5 for both real and fake data.The general structure of a GAN is shown in the diagram above, using MNIST images as data. The latent sample is a random vector the generator uses to construct it’s fake images. As the generator learns through training, it figures out how to map these random vectors to recognizable images that can fool the discriminator.The output of the discriminator is a sigmoid function, where 0 indicates a fake image and 1 indicates an real image. If you’re interested only in generating new images, you can throw out the discriminator after training.https://github.com/mchablani/deep-learning/blob/master/gan_mnist/Intro_to_GANs_Exercises.ipynbWe use a leaky ReLU to allow gradients to flow backwards through the layer unimpeded. TensorFlow doesn’t provide an operation for leaky ReLUs, you can just take the outputs from a linear fully connected layer and pass them to tf.maximum. Typically, a parameter alpha sets the magnitude of the output for negative values. So, the output for negative input (x) values is alpha*x, and the output for positive x is x:For the generator, we’re going to train it, but also sample from it as we’re training and after training. The discriminator will need to share variables between the fake and real input images. So, we can use the reuse keyword for tf.variable_scope to tell TensorFlow to reuse the variables instead of creating new ones if we build the graph again.The generator has been found to perform the best with tanh for the generator output. This means that we’ll have to rescale the MNIST images to be between -1 and 1, instead of 0 and 1.The discriminator network is almost exactly the same as the generator network, except that we’re using a sigmoid output layer.For the discriminator, the total loss is the sum of the losses for real and fake images, d_loss = d_loss_real + d_loss_fake.For the real image logits, we’ll use d_logits_real which we got from the discriminator in the cell above. For the labels, we want them to be all ones, since these are all real images. To help the discriminator generalize better, the labels are reduced a bit from 1.0 to 0.9, for example, using the parameter smooth. This is known as label smoothing, typically used with classifiers to improve performance. In TensorFlow, it looks something like labels = tf.ones_like(tensor) * (1 - smooth)The discriminator loss for the fake data is similar. The logits are d_logits_fake, which we got from passing the generator output to the discriminator. These fake logits are used with labels of all zeros. Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.Finally, the generator losses are using d_logits_fake, the fake image logits. But, now the labels are all ones. The generator is trying to fool the discriminator, so it wants to discriminator to output ones for fake images.We want to update the generator and discriminator variables separately.Note that when minimizing d_loss we want optimizer to only be updating the discriminator vars and similar for generator.Credits: From lecture notes: https://classroom.udacity.com/nanodegrees/nd101/syllabus",27/06/2017,6,34.0,28.0,723.0,350.0,2.0,0.0,0.0,2.0,en
4125,Solving 8-Puzzle using A* Algorithm.,Good Audience,Ajinkya Sonawane,173.0,5.0,941.0,"Solving the sliding puzzle using a basic AI algorithm.N-Puzzle or sliding puzzle is a popular puzzle that consists of N tiles where N can be 8, 15, 24, and so on. In our example N = 8. The puzzle is divided into sqrt(N+1) rows and sqrt(N+1) columns. Eg. 15-Puzzle will have 4 rows and 4 columns and an 8-Puzzle will have 3 rows and 3 columns. The puzzle consists of N tiles and one empty space where the tiles can be moved. Start and Goal configurations (also called state) of the puzzle are provided. The puzzle can be solved by moving the tiles one by one in the single empty space and thus achieving the Goal configuration.The tiles in the initial(start) state can be moved in the empty space in a particular order and thus achieve the goal state.Note: There are exceptions in N-Puzzle, visit the link below to check if a given problem is solvable or not.www.geeksforgeeks.orgInstead of moving the tiles in the empty space, we can visualize moving the empty space in place of the tile, basically swapping the tile with the empty space. The empty space can only move in four directions viz.,1. Up2.Down3. Right or4. LeftThe empty space cannot move diagonally and can take only one step at a time (i.e. move the empty space one position at a time).You can read more about solving the 8-Puzzle problem here.Basically, there are two types of searching techniques :1. Uninformed Search and2. Informed SearchYou might have heard about Linear Search, Binary Search, Depth-First Search, or the Breadth-First Search. These searching algorithms fall into the category of uninformed search techniques i.e. these algorithms do not know anything about what they are searching for and where they should search for it. That’s why the name “uninformed” search. Uninformed searching takes a lot of time to search as it doesn’t know where to head and where the best chances of finding the element are.Informed search is exactly opposite to the uninformed search. In this, the algorithm is aware of where the best chances of finding the element are and the algorithm heads that way! Heuristic search is an informed search technique. A heuristic value tells the algorithm which path will provide the solution as early as possible. The heuristic function is used to generate this heuristic value. Different heuristic functions can be designed depending on the searching problem. So we can conclude that Heuristic search is a technique that uses a heuristic value for optimizing the search.A* is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently traversable path between multiple points, called nodes. Noted for its performance and accuracy, it enjoys widespread use.The key feature of the A* algorithm is that it keeps a track of each visited node which helps in ignoring the nodes that are already visited, saving a huge amount of time. It also has a list that holds all the nodes that are left to be explored and it chooses the most optimal node from this list, thus saving time not exploring unnecessary or less optimal nodes.So we use two lists namely ‘open list‘ and ‘closed list‘ the open list contains all the nodes that are being generated and are not existing in the closed list and each node explored after it’s neighboring nodes are discovered is put in the closed list and the neighbors are put in the open list this is how the nodes expand. Each node has a pointer to its parent so that at any given point it can retrace the path to the parent. Initially, the open list holds the start(Initial) node. The next node chosen from the open list is based on its f score, the node with the least f score is picked up and explored.f-score = h-score + g-scoreA* uses a combination of heuristic value (h-score: how far the goal node is) as well as the g-score (i.e. the number of nodes traversed from the start node to current node).In our 8-Puzzle problem, we can define the h-score as the number of misplaced tiles by comparing the current state and the goal state or summation of the Manhattan distance between misplaced nodes.g-score will remain as the number of nodes traversed from a start node to get to the current node.From Fig 1, we can calculate the h-score by comparing the initial(current) state and goal state and counting the number of misplaced tiles.Thus, h-score = 5 and g-score = 0 as the number of nodes traversed from the start node to the current node is 0.We first move the empty space in all the possible directions in the start state and calculate the f-score for each state. This is called expanding the current state.After expanding the current state, it is pushed into the closed list and the newly generated states are pushed into the open list. A state with the least f-score is selected and expanded again. This process continues until the goal state occurs as the current state. Basically, here we are providing the algorithm a measure to choose its actions. The algorithm chooses the best possible action and proceeds in that path. This solves the issue of generating redundant child states, as the algorithm will expand the node with the least f-score.I have used two classes in my code: Node & Puzzle.Node class defines the structure of the state(configuration) and also provides functions to move the empty space and generate child states from the current state. Puzzle class accepts the initial and goal states of the N-Puzzle problem and provides functions to calculate the f-score of any given node(state).github.com",15/09/2018,0,20.0,0.0,765.0,658.0,3.0,0.0,0.0,4.0,en
4126,NLP: Building Text Summarizer — Part 1,DataPy.ai,Ashish Singhal,98.0,5.0,574.0,"Automatic text summarization is a very common problem in Natural Language Processing. The semantic understanding of the human language text exhibited by the summarization is the holy grail for the Natural Language Processing. But even extracting a “good understanding” from human language is not something we currently count on text summarization or for that matter any other thing under Natural Language Processing.However, there exists an array of techniques which can help in summarization with justifiable semantic meaning and summarization of the original text.There are few ways of going about classifying text summarization techniques, as can be seen in figure below.This series will talk about text summarization based on the Output Type. From the above figure; there are two techniques which can be employed based on output type:Extractive technique seems so naive upon reading for the first time, and it might even raise questions about its accuracy. Inspite of this, most summarization tools existing today employs extractive technique and enjoys a good accuracy rate.Following content explains how to implement extractive text summarization from scratch with a step by step guide.As the model consists quite a few modules and to keep this article short, this article will cover Text Pre-processing and Tokenization modules.The extractive model’s core lies within TF-IDF and Cosine Similarities modules. Before applying these two on the text document read by the model, its need to be processed and ready in the format required by the TF-IDF and Cosine Similarities.After model reading the text document, the text document goes to preprocessing module which does the followings in the sequence mentioned below:1. Lower case the text2. Removing special characters3. Removing numbers4. Tokenize the text5. Remove the stop words6. Remove the words which have length equal to 1 after removing special characters and numbers.Lower case the text to avoid any ambiguity between the words having same meaning but different case and leave mid-sentence words as they are because if they’re starting with upper case somewhere inside the sentence, that means it is a name or an entity and that word have more importance than other words. As the word has more importance hence the parent sentence has a greater importance and so it should and might be present in the summary text.After removing special characters and numbers and tokenizing the text, there might be words which only have one character. These words would not have any meaning; and hence they are removed.Before preprocessing the text, sentences are tokenized and stored. This is needed to generate the final summary based on the sentence index we get to put in the summary.This module takes processed tokenized words from first step and a number ’n’ as the inputs and gives n-grams as its output.N-grams have been used to group the words resulting into phrases which would give more insights about the sentence. These insights will help in assigning the importance score to each phrase and hence we can get the importance score for parent sentence too. These importance score will help in extracting sentences for summary text.Until now, the extraction model has read the text document, preprocessed it and have generated N-grams. Along with it, model also has sentences in tokenized format. These two things would be serve to TF-IDF module followed by Cosine Similarity module.TF-IDF and Cosine Similarity module is explained in Part 2.If you’ve come till this far, go through the part 2 as well and implement your own text summarization tool and make your weekend count.medium.commedium.com",19/11/2019,2,3.0,6.0,700.0,311.0,3.0,1.0,0.0,3.0,en
4127,From metaphor to reality,Medium,Hely Marleena,23.0,10.0,2236.0,"“A mind is like a computer program that is executed in our brain” says the computer metaphor of the mind. It was developed in the 1950s. It basically compares the human mind to a computer program, suggesting that computers and our brain function on the same principles.In the philosophy of artificial intelligence (AI), the brain is perceived as a similar information processing machine as a digital computer.There is no doubt that some of our thinking processes, such as mental calculation and logical reasoning, are algorithmic. The digital computer functions with binary computer language where the symbols ‘1’ and ‘0’ represent the state of circuit’s gate. This means that an electrical impulse either goes through (state ‘1’) or does not (state ‘0’). It is similar to the function of the nerve cell. The cell either fires the impulse or not. And when it does it will always be of a same magnitude. Alan Turing had a theoretical idea of a machine that is capable of solving any function that is computable by blindly following a certain algorithm. He called it the “Turing machine”.The purpose in the study of artificial intelligence is to create computer programs that are equally capable of performing information processes as is the human mind. There is a test to determine the success of such a program. It is called the “Turing test” also known as the “imitation game”.There are 3 participants in the imitation game:Player C: the interrogator.Player A and B: other is a computer and other a human being.Player C tries to determine which player is the computer and which is the human. If the player C does not succeed distinguishing between the human and the computer, the machine can be said to have genuine intelligence.So if the machine passes the Turing test, it literally has that same ability as the human mind does. Not only similar, but the same. An artificial intelligence would then be able to give a mind to a machine.Now, the computational theory of mind sees the human brain as a universal Turing machine and the mind as a program that is being executed in the brain.For some researches in the field of artificial intelligence, the computer metaphor is no longer a metaphor.Discussing the questions ”Can a machine think?” and “Is the mind a program of a ‘brain computer’?” it is essential to define the terms “mind”, “machine”, and “thought”. Machine is a physical system capable of performing functions. A mind consists of rudimentary biological functions, emotions, higher-level cognitive processes and consciousness. A “thought” is the result of the cognitive process of thinking.John R. Searle discusses question in his articles “Minds, brains and programs” (The Behavioral and Brain Sciences 1980 3, 417-457) and “Is the Brain’s Mind a Computer Program?” (Scientific American, 1990 January, 26-31).Searle distinguishes “strong AI” from “weak AI”. According to weak AI the principal value of the computer in the study of the mind is that it gives us a powerful tool to formulate and test hypotheses with precision. According to strong AI, the computer is not only a tool in the study of the mind but can really have a mind, when given the right programs and inputs. Searle does not object the claims of the weak AI. But he does object the strong AI. Searle discusses the work of Roger Schank and his colleagues at Yale.Schank’s program aims to simulate the human ability to understand stories.After hearing a story, a person can answer questions about the story.If a machine is capable of doing this in the way that a human does:1. the machine can literally be said to understand the story and provide the answers2. what the machine does, explains the human ability to understand the story and answer the questionsSearle responds to these hypotheses with the Chinese room argument. He asks you to imagine himself locked in a room and given a batch of Chinese writing. He does not know Chinese. All that he sees are meaningless squiggles. In the room he has a rule book which is in English so he can understand it. In the rule book there are set of rules how to correlate the Chinese symbols. Now, he must give out a batch of new Chinese squiggles that correlates with the symbols that first came into the room. Inside the room, he does not know that the people outside call the first batch “script”, the second batch “story”, the third batch “questions”, the rule book “the program”, and the symbols he gives out “answers”.Imagine he is so specific and fast operating the symbols by the rule book that the Chinese people outside the room cannot tell if he really understands Chinese or not.Searle points out that it is obvious in the example that he does not understand nor is learning Chinese. For the same reasons Schank’s computer understands nothing about the story.The same argument stands to prove Turing test invalid. So even if artificial intelligence passes Turing test, it does not mean the artificial intelligence understands anything nor is capable of producing thoughts. Turing test merely evaluates if a computer program is good enough to fool a person into thinking he is talking to a real person.Searle’s Chinese room argument triggered multiple different counter-arguments. He discusses and replies to some of them as follows:One of the popular objections states that the individual person locked in the room does not understand Chinese but the system does. The whole room understands. Searle’s response is to let the individual internalize all the elements in the system. He can memorize the rule book and do all the calculations in his head. We can put him outside the room, to work outdoors. Now there is nothing in the system that is not in him but he still does not understand the Chinese story.One view states that even if the man does not understand Chinese in the sense a native speaker does, still, as a formal symbol manipulation system he really does understand Chinese. There is a subsystem in him that understands Chinese, and another that understands English. According to this view, the subsystems are different. According to Searle, these subsystems are not even remotely alike. The English subsystem understands what the story is about. The Chinese subsystem only knows that “squiggle squiggle” is followed by “squiggle squiggle”. I do not understand why these subsystems still cannot ground from the same basic system. Without the squiggles would be no meaning to attach to them. We attach the meaning to them. If the man inside the room wants to invent new meaning for the symbols, he can. Then he would be learning (or rather inventing) a new language. As I see it one cannot do the process of thinking without a language.An objection that Searle does not bother to discuss, claims that semantics does not exist anyway. There is only syntax. All that exists in the brain is the same sort of syntactic symbol manipulation that goes on in computers. In my opinion, this is an intriguing view. Why should semantics be something more than syntax? There definitely cannot be semantics without syntax. There is not a “meaning” (semantics) without a “symbol” (syntax) for it. What if thoughts consist of manipulation of the symbols with the “language of thought”, like the Fodor’s hypothesis suggests? And that is what a computer program does: manipulates symbols by the means of a computer language.Another objection states that if the room would be inside a robot and it was given sensors and the ability to act like human, it would understand Chinese. Searle’s simple reply to this claim is that even if the room (or the computer) is inside a robot, the situation does not change. The man still does not know what he is doing or that he is inside a robot. All he is doing is manipulating formal symbols. Adding motoric and perceptual capacities does not add understanding or intentionality.According to the brain simulator argument, if the program was a simulation of a Chinese speaker’s brain it would understand Chinese. Searle reminds that the point of AI is that we do not need to know how the brain works to know how the mind works. “The problem with a brain simulator”, according to Searle, “is that it is simulating the wrong things as long as it simulates only the formal structure of the sequence of neuron firings at the synapses, it won’t have simulated what matters about the brain, namely its causal properties, its ability to produce intentional states.”.But what about the combination of a robot with a brain shaped computer that is programmed with all the synapses of a human brain and the ability to behave like a human? Searle’s reply is that this no longer fits the definition of strong AI. Also “the attributions of intentionality that we make to the robot in this example have nothing to do with formal programs.They are simply based on the assumption that if the robot looks and behaves sufficiently like us, then we would suppose, until proven otherwise, that it must have mental states like ours that cause and are expressed by its behavior and it must have an inner mechanism capable of producing such mental states.” Searle states that if we knew the robot to have a formal program, we would not attribute intentionality to it. In Searle’s opinion, the only locus of intentionality is the man and he does not know any of the intentional states. I think that if the robot was seen as a unified system, the man itself, we would be able to say that it understands Chinese.I believe that in the future we will be able to build artificial intelligence with the ability to produce thoughts and understanding.With certain algorithms it could be possible to imitate the function of the brain. But this is not enough. We need the programs to be a duplicate of the brain. We do not yet fully know how the brain works and develops through a lifetime but in time this could be possible. Maybe a set of programs executing in parallel and creating new programs that correlate the human brain’s neural net every time the computer learns a new thing could enable a new program to join the “program net” and make connections between the old ones.A simulation individual would develop in a simulation environment. The environment would provide stimuli so that the simulation individual would be able to develop its “brains” and learn by trial and error.The simulation individual would also need a robot body or a virtual avatar in a virtual reality to create a physical self-image to enhance consciousness of itself. An article “Researchers create the first lab-grown human ‘mini brains’” tells that Austrian scientists have been able to grow a miniature human brain from stem cells. So it is already partly reality. The ethical aspect of such experiment is another interesting question but I will not address them in this article.If the brain indeed can be simulated, even duplicated, this would mean that the AI could have an understanding, consciousness, and the ability to produce thoughts. But what about the mind?Could we then say that this AI has a mind? No, not until it has emotions too. Could it be possible then for an AI to have emotions, if it is able to create real cognitive processes?Yes.The cognitive functions are cortical. The emotional functions are exactly the same kind of nerve impulse patterns but in another “CPU” if you will, the limbic system of the brain. The biological principle is the same everywhere; the functions are only diverged due to evolution.If then we could create an AI that has a mind, would it be a simulation or duplication of a human being? Stanford Encyclopedia of Philosophy wonders: “Are artificial hearts simulations of hearts? Or are they functional duplicates of hearts, hearts made from different materials? Walking is a biological phenomenon performed using limbs. Do those with artificial limbs walk? Or do they simulate walking?” Another question arises: when does simulation become the “real thing”?There was an interesting episode on Doctor Who that addresses the real vs. simulation issue. In the episode humans were using ‘fully programmable flesh’ to clone themselves to work in dangerous factory conditions. The clones had all their human features, knowledge and memories. The clones were under the control of their counterpart-humans until a thunderstorm broke the connection enabling the clones to start acting for themselves. For Who himself it was obvious that these clone creatures were ‘real’ after that. For the humans not so much.The articles by Searle are written 20 to 30 years ago. The Chinese room argument, in my opinion, is valid for the strong AI. Today, it would be justifiable to define a new AI in addition to weak and strong AI. What I mean is that an artificial intelligence that can be provided by the technology of today and the future needs a definition, a new concept. Imagine that in the future we have an AI that matches the description above; we would then need a new Turing test to conclude if it has a mind. It may come clear in the future that such a thing is impossible to prove, but for now the thought intrigues me. For now I believe that nothing is impossible in the field of artificial intelligence.Thanks to Eero Karvonen for consultancy.Written by Hely Herranen during cognitive science studies.",20/07/2014,0,4.0,2.0,685.0,450.0,6.0,0.0,0.0,8.0,en
4128,1000x Faster Spelling Correction algorithm (2012),Medium,Wolf Garbe,412.0,6.0,1105.0,"Update1: An improved SymSpell implementation is now 1,000,000x faster.Update2: SymSpellCompound with Compound aware spelling correction. Update3: Benchmark of SymSpell, BK-Tree und Norvig’s spell-correct.Recently I answered a question on Quora about spelling correction for search engines. When I described our SymSpell algorithm I was pointed to Peter Norvig’s page where he outlined his approach.Both algorithms are based on Edit distance (Damerau-Levenshtein distance). Both try to find the dictionary entries with smallest edit distance from the query term.If the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. But SymSpell uses a different way to search the dictionary, resulting in a significant performance gain and language independence. Three ways to search for minimum edit distance in a dictionary:1. Naive approachThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. This exhaustive search is inordinately expensive.Source: Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze: Introduction to Information Retrieval.The performance can be significantly improved by terminating the edit distance calculation as soon as a threshold of 2 or 3 has been reached.2. Peter NorvigGenerate all possible terms with an edit distance (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary. For a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time.Source: Peter Norvig: How to Write a Spelling Corrector.This is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and language dependent (because the alphabet is used to generate the terms, which is different in many languages and huge in Chinese: a=70,000 Unicode Han characters)3. Symmetric Delete Spelling Correction (SymSpell) Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step. Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time.This is three orders of magnitude less expensive (36 terms for n=9 and d=2) and language independent (the alphabet is not required to generate deletes). The cost of this approach is the pre-calculation time and storage space of x deletes for every original dictionary entry, which is acceptable in most cases.The number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinations without repetitions, and k=n-d), E.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 dictionary entries we need to additionally store 1,500,000 deletes.The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup by using deletes only instead of deletes +transposes + replaces + inserts. It is six orders of magnitude faster (for edit distance=3) and language independent.Remark 1: During the precalculation, different words in the dictionary might lead to same delete term: delete(sun,1)==delete(sin,1)==sn. While we generate only one new dictionary entry (sn), inside we need to store both original terms as spelling correction suggestion (sun,sin)Remark 2: There are four different comparison pair types:The last comparison type is required for replaces and transposes only. But we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance (bank==bnak and bank==bink, but bank!=kanb and bank!=xban and bank!=baxn).Remark 3: Instead of a dedicated spelling dictionary we are using the search engine index itself. This has several benefits:Remark 4: We have implemented query suggestions/completion in a similar fashion. This is a good way to prevent spelling errors in the first place. Every newly indexed word, whose frequency is over a certain threshold, is stored as a suggestion to all of its prefixes (they are created in the index if they do not yet exist). As we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost. Multiple terms are sorted by the number of results stored in the index.ReasoningThe SymSpell algorithm exploits the fact that the edit distance between two terms is symmetrical:We are using variant 3, because the delete-only-transformation is language independent and three orders of magnitude less expensive.Where does the speed come from?Computational Complexity The SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1).Comparison to other approaches BK-Trees have a search time of O(log dictionary_size), whereas the SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size. Tries have a comparable search performance to our approach. But a Trie is a prefix tree, which requires a common prefix. This makes it suitable for autocomplete or search suggestions, but not applicable for spell checking. If your typing error is e.g. in the first letter, than you have no common prefix, hence the Trie will not work for spelling correction.If you need a very fast auto-complete then try my Pruning Radix TrieApplication Possible application fields of the SymSpell algorithm are those of fast approximate dictionary string matching: spell checkers for word processors and search engines, correction systems for optical character recognition, natural language translation based on translation memory, record linkage, de-duplication, matching DNA sequences, fuzzy string searching and fraud detection.For a single user or for small edit distances other algorithms might just be fine. But for search engines and search as a service search API where you have to serve thousands of concurrent users, while still maintaining a latency of a few milliseconds, and where spelling correction is not even the main procession task, but only one of many components in query preprocessing, you need the fastest spelling correction you can get.Source codeThe C# implementation of the Symmetric Delete Spelling Correction algorithm is released on GitHub as Open Source under the MIT License:https://github.com/wolfgarbe/symspellPortsThere are ports in C++, Crystal, Go, Java, Javascript, Python, Ruby, Rust, Scala, Swift available.Originally published at seekstorm.com on June 7, 2012.",07/06/2012,0,32.0,3.0,1024.0,681.0,1.0,4.0,0.0,19.0,en
4129,Training Provably-Robust Neural Networks,Towards Data Science,Klas Leino,29.0,11.0,2252.0,"Over the last several years, deep networks have extensively been shown to be vulnerable to attackers that can cause the network to make perplexing mistakes, simply by feeding maliciously-perturbed inputs to the network. Clearly, this raises concrete safety concerns for neural networks deployed in the wild, especially in safety-critical settings, e.g., in autonomous vehicles. In turn, this has motivated a volume of work on practical defenses, ranging from attack detection strategies to modified training routines that aim to produce networks that are difficult — or impossible — to attack. In this article, we’ll take a look at an elegant and effective defense I designed with my colleagues at CMU (appearing in ICML 2021) that modifies the architecture of a neural network to naturally provide provable guarantees of robustness against certain classes of attacks — at no additional cost during test time.We call the family of neural networks protected by our approach “GloRo Nets” (for “globally-robust networks”). For those interested in experimenting with the ideas presented in this article, a library for constructing and training GloRo Nets is publicly available here.While deep neural networks have quickly become the face of machine learning due to their impressive ability to make sense of large volumes of high-dimensional data and to generalize to unseen data points, in as early as 2014, researchers were beginning to notice that deep networks can easily be fooled by making imperceptible modifications to their inputs. These perturbed inputs that successfully cause erroneous behavior have been termed adversarial examples. A classic illustration of an adversarial example is shown below.Broadly speaking, for an input to be considered an adversarial example, it should resemble one class (e.g., “panda”), while being classified as another class (e.g., “gibbon”) by the network. This suggests that the perturbations to the input should be semantically meaningless. This requirement is rather nebulous, and inherently tied to human perception; it may mean that changes are imperceptible to the human eye, or simply that they are inconspicuous in the given context. Thus, we often consider more well-defined specifications of adversarial examples; most frequently, small-norm adversarial examples.An adversarial example is small-norm if its distance (according to some metric, e.g., Euclidean distance) from the original input is below some small threshold, typically denoted by ε. Geometrically, this means that the original point is near a decision boundary in the model, as shown in the figure below. In terms of perception, when ε is sufficiently small, any points that are ε-close to the original input will be perceptually indistinguishable from the original input, meaning that small-norm adversarial examples fit our broader requirements for adversarial examples.While the precise reasons behind the existence of adversarial examples is not the focus of this article, some high-level intuition might give a bit of an idea of why this phenomenon is so pervasive in deep networks. Essentially, we can consider the decision surface of the network, which maps the network’s output as a function of its input. If the decision surface is steep in some direction, the network will quickly change its mind on the label it assigns to an input as the input is modified slightly in that direction. As deep networks typically operate in very high-dimensional space (often thousands of dimensions), there are many different directions in which the decision surface is potentially steep, making the chance that any point is near an unexpected “cliff” more likely than not.Numerous defenses have been proposed to deal with the threat of adversarial examples — particularly small-norm adversarial examples. While many of these approaches are heuristic in nature — that is, they do not provide guarantees that tell us when, or if, the model’s predictions cannot be manipulated — in the most safety-critical applications, this may not be good enough. For our purposes, we are interested in provable defenses.The first question is, what, precisely, do we want to prove? In the case of small-norm adversarial examples, the property we are after is what is called local robustness. Local robustness on a point, x, stipulates that all points within a distance of ε from x are given the same label as x. Geometrically, this means that a ball of radius ε around x is guaranteed to be clear of any decision boundaries, as shown in the figure below. From this intuition, it should be clear that local robustness precludes the possibility of deriving an adversarial example from x.As the name suggests, local robustness is local in the sense that it applies to the neighborhood of a single point, x. But what would it mean for the entire network to be robust? Upon inspection, we see that any interesting network can’t be locally robust everywhere: if all points are ε-far from a decision boundary, there will be nowhere to place the boundary, so the network would have to make the same prediction everywhere — not a very interesting network.This suggests that we may need to be alright with the network not being locally robust in some places; this doesn’t have to be a problem as long as the network is robust in the places we care about — the regions covered by the data. Therefore, we will think of a model as being robust as long there is a margin of width at least ε on the boundary, separating regions of the input space that are given different labels.An example of how a robust model might look is shown in the figure below. The margin (shown in black) is essentially a “no man’s land” where the model indicates that points are not locally robust by labeling them with a special label, ⊥ (“bottom,” a mathematical symbol sometimes used to denote null values). Ideally, no “real” data points (e.g., training points, validation points, etc.) lie in this no man’s land.The key idea behind GloRo Nets is that we want to construct the network in such a way that a margin will automatically be imposed on the boundary.The output of a neural network classifier is typically what is called a logit vector, containing one dimension (i.e., one logit value) per possible output class. The network makes predictions by selecting the class corresponding to the highest logit value.When we cross a decision boundary, one logit value will surpass the previous highest logit value. Thus, the decision boundary corresponds to the points where there is a tie for the highest logit output. To create a margin along the boundary, we essentially want to thicken the boundary. We can do this by declaring a tie whenever the two highest logits are too close together; i.e., we will consider the decision a draw unless one logit surpasses the rest by at least δ.We can let the case of a draw correspond to the ⊥ class. This will create a no-man’s-land region separating the other classes as desired. But it remains to ensure that the width of this region is always at least ε (in the input space). To do this, we will make use of the Lipschitz constant of the network.The Lipschitz constant of a function tells us how much the output of the function can change as its input is changed. Intuitively, we can think of it as the maximum slope, or rate-of-change, of the function.Thus, if the Lipschitz constant of a function is K, then the most the function’s output can change by, if its input changes by a total of ε, is εK. We will use this fact to impose a margin of the correct width on our network’s decision surface.As a neural network is really just a high-dimensional function, we can also consider the Lipschitz constant of a neural network. For the sake of simplicity, we will consider each logit value as a distinct function with its own Lipschitz constant (this will be sufficient to explain how GloRo Nets work, but the analysis can be tightened by a slightly more complex accounting of the full network’s Lipschitz constant; the appendix of our paper provides these technical details).We now return to our intuition from earlier to demonstrate how to construct GloRo Nets, by way of an example.Suppose we have a network predicting between four classes that produces the logits shown below. If the difference between the logits for class 1 and 2 is sufficiently large, class 2 will not be able to surpass class 1 (and similarly for classes 3 and 4).As discussed, the Lipschitz constant tells us how much each logit can move when the input is perturbed within a radius of ε. By considering the amount by which class 1 can decrease, and the amount by which each of the other classes can increase, we can calculate the amount by which each class can gain on the predicted class (class 1). We then insert a new logit value, corresponding to the ⊥ class, which takes its value based on the most competitive class to class 1, after considering how much each class can move.As a result, we have the following important property: if the ⊥ logit is not the maximum logit at a given point, then we are guaranteed that the model is locally robust at that point. On the other hand, if the ⊥ logit is the maximum logit, the network will predict ⊥ as desired, indicating that the point lies in the margin between classes.While calculating the exact Lipschitz constant of a neural network is a fundamentally hard problem computationally, we can obtain an upper bound on the Lipschitz constant fairly easily and efficiently by breaking down the computation to consider one layer at a time. Essentially, we can multiply the Lipschitz constant of each of the individual layers together to obtain a bound for the entire network. The figure below gives some intuition as to why this works.Interestingly, while this bound would usually be very loose on a typical network — that is, it will vastly overestimate the Lipschitz constant — because the bound calculation is incorporated into the learning objective (more on this below), even this naive layer-wise bound ends up fairly tight on GloRo Nets, which are not “typical” networks. This means that using a simple calculation of the Lipschitz constant is actually an effective way to impose a sufficient margin between classes for obtaining provable robustness.Moreover, after training, the Lipschitz constant will remain fixed. This means that test-time certification does not require any additional computation to compute the Lipschitz bound, making certification essentially free. This is a major advantage compared to other robustness certification approaches, which often require expensive computation to obtain robustness guarantees.Conveniently, the way we encode the ⊥ region as an extra class allows us to train GloRo Nets to optimize for provable robustness quite easily. The key to this fact is that in GloRo Nets, robustness is captured by accuracy. Essentially, since ⊥ is never the correct class on our labeled data, by training the network to be accurate (as we usually do), it avoids picking ⊥, which in turn means it avoids being non-robust.We’ll now take a look at an example of GloRo Nets in action (to follow along interactively, check out this notebook). A library implementing GloRo Nets in TensorFlow/Keras is available here; it can also be installed with pip:Using the gloro library, we will see that GloRo Nets are simple to construct and train; and they achieve state-of-the-art performance for certifiably-robust models [1].We will use the MNIST dataset of hand-written digits as an example. This dataset can be easily obtained through tensorflow-datasets.We begin by defining a simple convolutional network. Constructing a GloRo Net with the gloro library looks pretty much exactly the same as it would in Keras; the only difference is that we instantiate it with the GloroNet class (rather than the default Keras Model class), which requires us to specify a robustness radius, epsilon. While the GloroNet model class is compatible out-of-the-box with any 1-Lipschitz activation function (including ReLU), we find that the MinMax activation, which sorts pairs of neighboring neurons, works best (the reasons why are beyond the scope of this article).We can then compile the model. There are a few choices for GloRo-Net-compatible loss functions, including standard categorical cross-entropy, but we typically find that Trades loss (found in gloro.training.losses) works best (the details for the different loss functions are not in the scope of this article).We also find the clean_acc and vra metrics to be useful for tracking progress. Clean accuracy is the accuracy the model achieves when the margin is ignored (i.e., if we don’t care about robustness). VRA, or verified-robust accuracy is the fraction of points that are both correctly classified and certified as locally robust; this is the typical success metric for robust models.Finally, we are ready to fit our model to our data. The gloro library contains a number of useful callbacks to define a schedule for the various hyper-parameters; these are helpful for getting the best performance out of training. For example, we find that slowly raising the robustness radius over the course of training results in a better outcome.And that’s it! This results in the following clean accuracy and VRA:GloRo Nets provide an elegant and effective way to guard against small-norm adversarial examples by automatically inducing an ε-margin into the construction of the network. Because robustness certification is naturally incorporated into the network’s output, GloRo Nets can be easily optimized for verified-robust accuracy, and can perform essentially-free certification at test time. Moreover, because the Lipschitz bounds used to certify the network are also incorporated into the network, GloRo Nets can achieve state-of-the-art VRA using simple, efficiently-computable upper bounds for the Lipchitz constant.",13/11/2021,6,2.0,44.0,1322.0,617.0,8.0,1.0,0.0,11.0,en
4130,Tensorflow or PyTorch : The force is strong with which one?,Medium,Udacity India,5100.0,3.0,651.0,"By — Yashwardhan JainSo, since you’re reading this article, I’m going to assume you have started your deep learning journey and have been playing around for a while with artificial neural nets. Or maybe, you’re just thinking of starting. Whichever case it be, you find yourself in a bit of a dilemma. You have read about various deep learning frameworks and libraries and maybe two really stand out. The two most popular deep learning libraries: Tensorflow and PyTorch. And you can’t quite figure out what exactly is the difference. Fret not! I’m here to add one more article to the unending repository of the Internet. And maybe, help you get some clarity. Also, I’m going to make it easier and quicker for you, and give you just five points. Five points of comparison, no more. So, let’s begin!Point #1:While both Tensorflow and PyTorch are open-source, they have been created by two different wizards. Tensorflow is based on Theano and has been developed by Google, whereas PyTorch is based on Torch and has been developed by Facebook.Point #2:The most important difference between the two is the way these frameworks define the computational graphs. While Tensorflow creates a static graph, PyTorch believes in a dynamic graph. So what does this mean? In Tensorflow, you first have to define the entire computation graph of the model and then run your ML model. But in PyTorch, you can define/manipulate your graph on-the-go. This is particularly helpful while using variable length inputs in RNNs.Point #3:Tensorflow has a more steep learning curve than PyTorch. PyTorch is more pythonic and building ML models feels more intuitive. On the other hand, for using Tensorflow, you will have to learn a bit more about it’s working (sessions, placeholders etc.) and so it becomes a bit more difficult to learn Tensorflow than PyTorch.Point #4:Tensorflow has a much bigger community behind it than PyTorch. This means that it becomes easier to find resources to learn Tensorflow and also, to find solutions to your problems. Also, many tutorials and MOOCs cover Tensorflow instead of using PyTorch. This is because PyTorch is a relatively new framework as compared to Tensorflow. So, in terms of resources, you will find much more content about Tensorflow than PyTorch.Point #5:This comparison would be incomplete without mentioning TensorBoard. TensorBoard is a brilliant tool that enables visualizing your ML models directly in your browser. PyTorch doesn’t have such a tool, although you can always use tools like Matplotlib. Although, there are integrations out there that let you use Tensorboard with PyTorch. But it’s not supported natively.Finally, Tensorflow is much better for production models and scalability. It was built to be production ready. Whereas, PyTorch is easier to learn and lighter to work with, and hence, is relatively better for passion projects and building rapid prototypes.Alright enough! Just tell me which one is better?There is no right answer.(I know, I hate it too when someone says that)The truth is, some people find it better to use PyTorch while others find it better to use Tensorflow. Both are great frameworks with a huge community behind them and lots of support. They both get the job done. They both are amazing magical wands that will let you do some machine learning magic.I hope I was able to help you in clearing your confusion(little bit, maybe?). And if you are really confused and haven’t used any of them yet, pick any and just start. You will develop more intuition which will help you decide.If you are just beginning your deep learning journey, and want to learn how to build deep learning models(like CNNs, RNNs or GANs) in Tensorflow and Keras, try out this Deep Learning Nanodegree by Udacity.And finally, these are just tools. You can pick any and start learning the science and art of machine learning.Happy Learning!About the Author | Yashvardhan JainComputer Science Undergrad. AI enthusiast. Bibliophile.Follow him on Medium, Twitter, and Quora",24/04/2018,0,8.0,6.0,1400.0,934.0,1.0,0.0,0.0,8.0,en
4131,Gaussian Mixture Models vs K-Means. Which One to Choose?,Towards Data Science,Kacper Kubara,462.0,6.0,946.0,"K-Means and Gaussian Mixtures (GMs) are both clustering models. Many data scientist, however, tend to choose a more popular K-Means algorithm. Even if GMs can prove superior in certain clustering problems.In this article, we will see that both models offer a different performance in terms of speed and robustness. We will also see that it is possible to use K-Means as an initializer for GMs which tends to boost the performance of the clustering model.First, let’s review the theoretical part of these algorithms. It will help us to understand their behaviour later in the article.K-Means is a popular non-probabilistic clustering algorithm. The goal of the algorithm is to minimize the distortion measure J. We achieve that by the following iterative procedure [1]:A GIF below nicely illustrates this iterative procedure:The K-Means algorithm will converge but it might not be a global minimum. To avoid a situation where it converges to a local minimum, K-Means should be re-run a few times with different parameters.K-Means performs hard assignment which means that each datapoint has to belong to a certain class and there is no probability assigned to each datapoint.The computational cost of the K-Means is O(KN), where K is a number of clusters and N is a number of datapoints.Gaussian Mixtures are based on K independent Gaussian distributions that are used to model K separate clusters. As a reminder, the multivariate Gaussian distribution is given as:The derivation of the Gaussian mixtures is rather complicated, so for an in-depth mathematical explanation, I suggest looking at this article.The most important thing to know about GMs is that the convergence of this model is based on the EM (expectation-maximization) algorithm. It is somewhat similar to K-Means and it can be summarized as follows [1]:The Gaussian Mixtures will also converge to a local minimum.We can easily visualize how the convergence works for the Gaussian mixtures with the GIF below:The first visible difference between K-Means and Gaussian Mixtures is the shape the decision boundaries. GMs are somewhat more flexible and with a covariance matrix ∑ we can make the boundaries elliptical, as opposed to circular boundaries with K-means.Another thing is that GMs is a probabilistic algorithm. By assigning the probabilities to datapoints, we can express how strong is our belief that a given datapoint belongs to a specific cluster.If we compare both algorithms, the Gaussian mixtures seem to be more robust. However, GMs usually tend to be slower than K-Means because it takes more iterations of the EM algorithm to reach the convergence. They can also quickly converge to a local minimum that is not a very optimal solution.In the remainder of the article, we will look at how these models perform in practice using Scikit-learn library.Google Colab notebook for this part can be found hereWe will start by creating a synthetic dataset for this task. To make it more challenging, we will create 2 overlapping Gaussian distributions and add a uniform distribution on the side.The resulting dataset looks as follows:Let’s get our hands dirty and do the initial clustering with K-Means and Gaussian Mixtures. We will use the models imported from Scikit-Learn. Additionally, we will set parameters in the same way for both models. Max no. iterations, number of clusters, and convergence tolerance are set the same for both models.From the first glance on the clustered data, we can see that they don’t perform very well. While K-Means clusters data in a similar fashion as from the true clusters, the GM clusters look pretty unreliable.The problem with GMs is that they have converged quickly to a local minimum that is not very optimal for this dataset. To avoid this issue, GMs are usually initialized with K-Means. This usually works quite well and it improves clusters generated with K-Means. We can create GM with K-Means initializer by changing one parameter in the GaussianMixture class:We can also leverage the probabilistic nature of GMs. By adding a threshold value, 0.33 in this case, we are able to flag labelled datapoints that model is unsure about. It gets quite interesting here because, for vanilla GM, most of the datapoints have low probabilities as seen in the middle plot below.Additionally, the GMs with K-Means initializer seem to perform the best and the clusters are nearly the same as for the original data.Let’s have a look now at the computation time of these algorithms. The results are quite surprising. The computation time was measured with a varying number of clusters and all 3 models described above.And the results are:It is quite strange that a plain K-Means is slower than GM with a K-Means initializer. Behind the hood, Scikit-Learn seems to apply an optimized version of K-Means that takes fewer iterations to converge.Also, vanilla GM takes a very short amount of time. This happens because it finds a local minimum quite fast which is not even close to the global minimum.If you look for robustness, GM with K-Means initializer seems to be the best option. K-Means should be theoretically faster if you experiment with different parameters, but as we can see from the computation plot above, GM with K-Means initializer is the fastest. GM on its own is not much of use because it converges too fast to a non-optimal solution for this dataset.Thank you for reading, I hope you enjoyed the article!I am an MSc Artificial Intelligence student at the University of Amsterdam. In my spare time, you can find me fiddling with data or debugging my deep learning model (I swear it worked!). I also like hiking :)Here are my other social media profiles, if you want to stay in touch with my latest articles and other useful content:[1] Pattern Recognition and Machine Learning (Information Science and Statistics)",08/10/2020,0,12.0,10.0,758.0,419.0,9.0,3.0,0.0,9.0,en
4132,"Multi-Label, Multi-Class Text Classification with BERT, Transformers and Keras",Towards Data Science,Emil Lykke Jensen,40.0,8.0,787.0,"The internet is full of text classification articles, most of which are BoW-models combined with some kind of ML-model typically solving a binary text classification problem. With the rise of NLP, and in particular BERT (take a look here, if you are not familiar with BERT) and other multilingual transformer based models, more and more text classification problems can now be solved.However, when it comes to solving a multi-label, multi-class text classification problem using Huggingface Transformers, BERT, and Tensorflow Keras, the number of articles are indeed very limited and I for one, haven’t found any… Yet!Therefore, with the help and inspiration of a great deal of blog posts, tutorials and GitHub code snippets all relating to either BERT, multi-label classification in Keras or other useful information I will show you how to build a working model, solving exactly that problem.And why use Huggingface Transformers instead of Googles own BERT solution? Because with Transformers it is extremely easy to switch between different models, that being BERT, ALBERT, XLnet, GPT-2 etc. Which means, that you more or less ‘just’ replace one model for another in your code.With data. Looking for text data I could use for a multi-label multi-class text classification task, I stumbled upon the ‘Consumer Complaint Database’ from data.gov. Seems to do the trick, so that’s what we’ll use.Next up is the exploratory data analysis. This is obviously crucial to get a proper understanding of what your data looks like, what pitfalls there might be, the quality of your data, and so on. But I’m skipping this step for now, simply because the aim of this article is purely how to build a model.If you don’t like googling around take a look at these two articles on the subject: NLP Part 3 | Exploratory Data Analysis of Text Data and A Complete Exploratory Data Analysis and Visualization for Text Data.We have our data and now comes the coding part.First, we’ll load the required libraries.Then we will import our data and wrangle it around so it fits our needs. Nothing fancy there. Note that we will only use the columns ‘Consumer complaint narrative’, ‘Product’ and ‘Issue’ from our dataset. ‘Consumer complaint narrative’ will serve as our input for the model and ‘Product’ and ‘Issue’ as our two outputs.Next we will load a number of different Transformers classes.Here we first load a BERT config object that controls the model, tokenizer and so on.Then, a tokenizer that we will use later in our script to transform our text input into BERT tokens and then pad and truncate them to our max length. The tokenizer is pretty well documented so I won’t get into that here.Lastly, we will load the BERT model itself as a BERT Transformers TF 2.0 Keras model (here we use the 12-layer bert-base-uncased).We are ready to build our model. In the Transformers library, there are a number of different BERT classification models to use. The mother of all models is the one simply called ‘BertModel’ (PyTorch) or ‘TFBertModel’ (TensorFlow) and thus the one we want.The Transformers library also comes with a prebuilt BERT model for sequence classification called ‘TFBertForSequenceClassification’. If you take a look at the code found here you’ll see, that they start by loading a clean BERT model and then they simply add a dropout and a dense layer to it. Therefore, what we’ll do is simply to add two dense layers instead of just one.Here what our model looks like:And a more detailed view of the model:If you want to know more about BERTs architecture itself, take a look here.Now that we have our model architecture, all we need to do is write it in code.Then all there is left to do is to compile our new model and fit it on our data.Once the model is fitted, we can evaluate it on our test data to see how it performs.As it turns out, our model performs fairly okay and has a relatively good accuracy. Especially considering the fact that our output ‘Product’ consists of 18 labels and ‘Issue’ consists of 159 different labels.There are, however, plenty of things you could do to increase performance of this model. Here I have tried to do it as simple as possible, but if you are looking for better performance consider the following:(remember to add attention_mask when fitting your model and set return_attention_mask to True in your tokenizer. For more info on attention masks, look here. Also I have added attention_mask to the gist below and commented it out for your inspiration.)That’s it — hope you like this little walk-through of how to do a ‘Multi-Label, Multi-Class Text Classification with BERT, Transformer and Keras’. If you have any feedback or questions, fire away in the comments below.",25/08/2020,9,0.0,4.0,918.0,542.0,2.0,2.0,0.0,18.0,en
4133,Unsupervised Classification Project: Building a Movie Recommender with Clustering Analysis and K-Means,Towards Data Science,Victor Roman,2100.0,13.0,1305.0,"The goal of this project is to find out similarities within groups of people in order to build a movie recommending system for users. We are going to analyze a dataset from Netflix database to explore the characteristics that people share in movies’ taste, based on how they rate them.Data will come from the MovieLens user rating dataset.This dataset has two files, we will import both and work with both of them.We will want to find out how the structure of the dataset works and how many records do we have in each of these tables.We will start by considering a subset of users and discovering what are their favourite genre. We will do this by defining a function that will calculate each user’s average rating for all science fiction and romance movies.In order to have a more delimited subset of people to study, we are going to bias our grouping to only get ratings from those users that like either romance or science fiction movies.We can see that there are 183 number of records ,and for each one, there is a rating for a romance and science fiction movie.Now, we will make some Visualization Analysis in order to obtain a good overview of the biased dataset and its characteristics.The biase that we have created previously is perfectly clear now. We will take it to the next level by applying K-Means to break down the sample into two distinct groups.It is evident that the grouping logic is based on how each person rated romance movies. People that averaged a rating on romance movies of 3 or higher will belong to one group, and people who averaged a rating of less than 3 will belong to the other.We will see now what happen if we divide the dataset into three groups.It is evident now that the science-fiction rating has started to come into play:Let us see what happens if we add another group.From this analysis we can realize that the more groups we split our datset into, the more similar are the preferences of the people that belong to each group.As we discussed on the article “Unsupervised Machine Learning: Clustering Analysis”:Choosing the right number of clusters is one of the key points of the K-Means algorithm. To find this number there are some methods:Field knowledgeBussiness decisionElbow MethodAs being aligned with the motivation and nature of Data Science, the elbow mehtod is the prefered option as it relies on an analytical method backed with data, to make a decision.Elbow MethodThe elbow method is used for determining the correct number of clusters in a dataset. It works by plotting the ascending values of K versus the total error obtained when using that K.The goal is to find the k that for each cluster will not rise significantly the varianceIn this case, we will choose the k=3, where the elbow is located.To better understand this method, when we talk about variance, we are referring to the error. One of the ways to calculate this error is by:So, now we want to find out the right number of clusters for our dataset. To do so, we are going to perform the elbow method for all the possible values of Kl which will range between 1 and all the elements of our dataset. That way we will consider every possibility within the extreme cases:Looking at the plot, we can see that the best choices of the K values are: 7, 22, 27, 31. Increasing the number of clusters beyond that range result in worst clusters according to the Silhouette Score.We will chose the K = 7 as it is the one that yields the best score and will be easier to visualize.Up to now, we have only analyzed romance and science-fiction movies. Let us see what happens when adding other genre to our analysis by adding Action movies.Here, we are still using the x and y axes of the romance and sci-fi ratings. In addition, we are plotting the size of the dot to represent the ratings of the action movies (the bigger the dot the higher the action rating).We can see that with the addition of the action genrem the clustering vary significantly. The more data that we add to our k-means model, the more similar the preferences of each group would be.The bad thing is that by plotting with this method we start loosing the ability to visualize correctly when analysing three or more dimensions. So, in the next section we will study other plotting method to correctlyy visualize clusters of up to five dimensions.Once we have seen and understood how the K-Means algorithm group the users by their movie genre preferences, we are going to take a bigger picture of the dataset and explore how users rate individual movies.To do so, we will subset the dataset by ‘userid’ vs ‘user rating’ as follows.Having a look at this subset of the datset, it is evident that there are a lot of ‘NaN’ values as most of the users have not rated most of the movies. This type of datasets with a number that high of ‘null’ values are called ‘sparse’ or ‘low-dense’ datasets.In order to deal with this issue, we will sort the datsaset by the most rated movies and the users that have rated the most number of movies. So we will obtain a much more ‘dense’ region at the top of the dataset.Now, we wil want to visualize it. As we have a high number of dimensions and data to be plotted, the preferred method on this situations are the ‘heatmaps’.To understand this heatmap:In order to improve the performance of the model, we’ll only use ratings for 1000 movies.In addition, as k-means algorithm does not deal well with sparse datasets, we will need to cast it as the sparse csr matrix type defined in the SciPi library. To do so, we will need first to convert the dataset to a Sparse Dataframe, and the use the to_coo() method in pandas to convert it to sparse matrix.We will take an arbitrary number of clusters in order to make an analysis of the results obtained and spot certain trends and commonalities within each group. This number will be K = 20. After that, we will plot each cluster as a heatmap.We can notice some things from these heatmaps:Now we will choose a cluster analyze it and try to make a prediction with it.And now we will show the ratings:Now we will take one of the blank cells, which are movies that haven’t been rated by the users, and we will try to predict wether if he/she would have liked it or not.Users aregrouped in a clusters with other users that presumably have similar taste to theirs, so it is reasonable to think that he/she would have rated a blank movie with the average of the rest of the users of its cluster. And thats how we will proceed.Using the logic of the previous step, if we calculate the average score in the cluster for every movie, we will have an understanding for how the custer feels about each movie in the dataset.This is really useful for us because we can use it as a recommendation engine that will recommend users to discover movies they’re likely to enjoy.When a user logs in to our app, we can now show them recommendations that are appropriate to their taste. The formula for these recommendations is to select the cluster’s highest-rated movies that the user did not rate yet.These would be our Top 20 recommendations to that user.If you liked this post then you can take a look at my other posts on Data Science and Machine Learning here.If you want to learn more about Machine Learning, Data Science and Artificial Intelligence follow me on Medium, and stay tuned for my next posts!",19/03/2019,24,66.0,6.0,634.0,347.0,27.0,5.0,0.0,6.0,en
4134,Causal inference (Part 2 of 3): Selecting algorithms,Data Science at Microsoft,Jane Huang,226.0,18.0,3488.0,"By Jane Huang, Daniel Yehdego, and Siddharth KumarThis is the second article of a series focusing on causal inference methods and applications. In Part 1, we discussed when and why causal models can help with different business problems. We also provided fundamentals for causal inference analysis and compared a few popular Python packages for causal analysis. In this article, we dive into details of various causal inference estimation methods and discuss algorithm selection for your own problem settings. Causal inference can be used on top of A/B tests in multiple ways to extract insights, but this article focuses mainly on estimation methods under unconfoundedness or on quasi-experimental bases when a randomized control trial (RCT) is not feasible.As discussed in Part 1, RCT is the traditional gold standard that enables unbiased estimation of treatment effects. However, in observational studies, the possibility of bias arises because a difference in the treatment outcome may be caused by a factor that predicts treatment rather than the treatment itself, known as confoundedness. As a result, it’s desirable to replicate a randomized experiment as closely as possible through various strategies.Causal inference consists of a family of statistical methods. In this article, we introduce two types of estimation methods: Estimation methods under unconfoundedness (also known as conditioning-based methods), and estimation methods for quasi-experiments (a research design that looks like an experimental design but lacks the key ingredient — random assignment of the treatment — studying instead pre-existing groups that received different treatments after the fact).In quasi-experiments, commonly used approaches include simple natural experiments, instrumental-variables (IV), and regression-discontinuity models. We will dive into IV approaches in a later section of this article and briefly introduce the other two here.Most algorithms that we discuss in this article mainly look at one snapshot of the outcome for treatment effects rather than at repeated observations of outcomes over time. We have carried out separate research to estimate the delay and decay of treatment effects over time, which we won’t discuss in this article due to length constraints. If you would like to include repeated observations of outcome over time into your causal research design, depending on whether you have control time series data from a non-treated unit, you can consider approaches like difference-in-difference and interrupted time series synthetic control, among others. Uber has also discussed various causal inference methods for time series observations in their post.Figure 1 shows high-level workflow recommendations for causal algorithm selection for an observational study with one snapshot of the outcome. Please keep in mind that the list is not exhaustive, just a small representation of the approaches that we use on the Customer Growth Analytics team here at Microsoft.Many heterogeneous treatment effect estimation methods are theoretically valid only when all potential confounders are observed. These methods attempt to approximate the gold standard of RCT. We refer to them as causal estimation methods under unfoundedness. It’s important to note that any attempt to use these methods without observing potential confounders can reduce, but not eliminate, bias relative to raw correlations.Matching methods parallel the covariates distribution that predicts the treatment assignment and create a “pseudo-population” in which the treatment is independent of the measured confounding variables. They first look for units with control variables that have the same values but receive different treatments, which can be done post hoc by taking a treated unit and then finding a non-treated unit with very similar values. That pair is called a match. The next step is to write down the difference in outcome between the treated and untreated unit in that match, a process that can resemble RCT. As such, matching can be used to reduce or eliminate the effects of confounding variables on observational data when estimating a treatment effect. To compare the closeness between two units, a wide variety of distance metrics can be used, such as Euclidean distance and Mahalanobis distance. Generally, distance metrics can be represented using the following equation:where xᵢ and xⱼ denote the features of the iᵗʰ and jᵗʰ unit.Some matching methods develop their own distance metrics depending on the transformation function f. One of the commonly used transformation methods is propensity score-based transformation. In propensity score matching, we first estimate the propensity score, which can be thought of as the likelihood or probability that an individual unit receives the treatment. Estimating the propensity score can be done in different ways, but typically it is done using multivariate or binary classification models such as logistic regression, random forest, XGBoost, and LightGBM, among others.But matching methods are not the only ones available. Several others are also explored below.Inverse propensity weighting (IPW): Like matching methods, re-weighting methods create a “pseudo-population” to address the challenge of selection bias due to different distributions of the treated and control groups. The main idea of weighting is to assign an appropriate magnitude to each sample in the observation dataset so that the distributions of the treated group and control group are similar. Then we can calculate statistics based on the re-weighted pseudo-population. When correctly applied, weighting can potentially improve efficiency and reduce the bias of unweighted estimators. If the propensity score e(x) is a conditional probability of assignment to a treatment given a vector of observed covariates x, then in the weighting method the outcome of the treated units is weighted by w(x) = 1 / e(x) while the control units are weighed as: w(x) = 1 / (1- e(x) )After re-weighting, the IPW estimator of ATE is defined as (see A Survey on Causal Inference):where n denotes the sample size, ê(x) is the estimated propensity score given features x, Tᵢ is the treatment assignment for iᵗʰ unit, and Yᵢ denotes the observed outcome for iᵗʰ unit.Theoretical results show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. However, IPW highly relies on the correctness of propensity scores, which can be remedied in doubly robust learning, as described in the next section.Doubly robust learning (DR): Doubly robust learning is a method for estimating heterogeneous treatment effects when the treatment is categorical and all potential confounders/controls are observed, but there are either too many for classical statistical approaches to be applicable or their effect on the treatment and outcome cannot be satisfactorily modeled by parametric functions (see EconML API documentation).Doubly robust methods reduce the problem by estimating two predictive tasks:Unlike the double machine learning method, which we introduce in a later section, the first model predicts the outcome from both the treatment and the controls, as opposed to just the controls. Then the method combines these two predictive models in a final estimation stage, creating a model of the heterogeneous treatment effect. In contrast to inverse propensity weighting (IPW), the DR approach first fits a direct regression model, and then debiases that model by applying an inverse propensity approach to the residual of that model, rather than to a training sample directly. The approach allows for arbitrary machine learning algorithms to be used for the two predictive tasks, while maintaining many favorable statistical properties related to the final model (e.g., small mean squared error, asymptotic normality, and construction of confidence intervals). The latter favorable statistical properties hold if either the first or second of the two predictive tasks achieves small mean squared error (hence the name “doubly robust”).Algorithms called meta-learners can take advantage of any supervised learning or regression methods in machine learning and statistics to estimate a treatment effect, such as the conditional average treatment effect (CATE) function discussed in Part 1. In meta-learner methods, the treatment space must be discrete. Meta-learners build on base algorithms — such as logistic regression (LR), random forests (RF), XGBoost, Bayesian additive regression trees (BART), or neural networks, among others — to estimate the CATE. In this section, we briefly introduce four types of meta-learners, including S-, T-, X-, and R-learners (see Causal ML API documentation).Currently, the S-learner, T-learner, and X-learner algorithms are available in both the CausalML and EconML packages for Python. R-learner is available in Causal ML, which is the same as the Non-Parametric DML CATE Estimator in EconML with different naming conventions. And more generally, all DML CATE Estimators in EconML are special instances of R-Learner. The following sections provide a quick walkthrough of the main ideas and formulation of each algorithm. Links to the raw papers are provided for those who want to go beyond the high-level introduction and learn more.S-learner: This algorithm estimates the target variable using all the covariate features and treatment indicator, without giving the treatment indicator any special role. The estimate is done using a single algorithm estimator, hence the name S-learner. The estimation consists of two stages. First, a predictive model is built using the outcome as a target and control for both treatment and other features. Then the difference is calculated among the estimated values when the treatment assignment indicator is changed from control to treatment, with all other features held fixed. The difference among the estimated values is the CATE for an individual unit. Different regressors, such as XGBoost, can be used as a base regressor in stage 1, and the one with lowest error in terms of SMAPE or other error metrics can be chosen.T-learner: This algorithm estimates the response functions separately for the treatment and control populations. First, it uses base learners to estimate the conditional expectations of the outcomes separately for units under control and those under treatment. Second, it takes the difference between these estimates. We refer to the general mechanism of estimating the response functions separately as the T-learner, “T” being short for “two.”where μ₀ (x) = E[ Y(0) | X=x] and μ₁(x) = E[ Y(1) | X=x] using arbitrary machine learning models.X-learner: When real-world data contains more control group than treatment group, the likelihood of overfitting the treatment group when using T-learner is high. X-learner tries to avoid this by using information from the control group to derive better estimators for the treatment group and vice versa. Like S- and T-learner, X-learner can use arbitrary machine learning algorithms like XGBoost as a base regressor with the lowest error in terms of SMAPE. X-learner is built on T-learner and uses each observation in the training set in an “X”-like shape, hence its name X-learner (see Künzel 2017). X-learner consists of three stages:where μ₀ (x) = E[ Y(0) | X=x ] and μ₁(x) = E[ Y(1) | X=x ] using arbitrary machine learning models.Then estimate τ₁(x) = E[ D₁ ∣ X = x ] and τ₀(x) = E[ D₀ ∣ X = x ] using machine learning models.where g ∈ [0,1] is a weighting function.X-learner uses a weighting function for variance minimization. It is recommended to use an estimate of the propensity score for g, but it also makes sense to choose g = 1 or 0 if the number of treated groups is very large or small compared to the number of control groups. This means that having accurate propensity estimates is not as important for X-learner.R-learner: The two steps within R-learner include (as outlined by Nie2017):One critical component of R-learner is the R-loss function for treatment effect estimation using cross-fitting. Given n independent and identically distributed examples with (xᵢ, yᵢ, tᵢ), i = 1, …, n, where xᵢ denotes per-person features, yᵢ ∈ R is the observed outcome, and tᵢ is the treatment assignment. The potential outcome {Yᵢ(0), Yᵢ(1)} corresponds to the outcome we would have observed given the treatment assignment tᵢ = 0 or 1, respectively, such that Yᵢ = Yᵢ(tᵢ). Assuming unconfoundedness, i.e., the treatment assignment is randomized once we control for the features xᵢ, we estimate the CATE function 𝜏∗ = E{Y(1)−Y(0) | X = x }. The treatment propensity and the conditional surfaces can be represented as:The CATE function 𝜏∗(x) can be re-written in terms of the conditional mean outcome as follows:The first formula is called Robinson’s transformation for flexible treatment effect estimation, and builds on modern machine learning approaches. The main idea of R-learner is to use this representation to construct a loss function that captures heterogeneous treatment effects, so that we can accurately estimate the treatment effects by seeking a regularized minimizer of this loss function.The formula can be equivalently expressed as (see Robins, 2004):Then we estimate the heterogeneous treatment effect function 𝜏∗(.) by empirical loss minimization:Sharing more details, we divide the data into K (e.g., 5 or 10) evenly sided folds. Let q(.) be a mapping from the i=1,…, n sample indices to K folds, and fit m^ and ê with cross-fitting over the K folds via methods tuned for optimal predicative accuracy. Then we can estimate the treatment effect via a plug-in version of the second formula:Forest-based methods utilize very flexible non-linear models of the heterogeneous treatment effect. They normally can perform well with many features. In addition, these methods can provide valid confidence intervals despite being data-adaptive and non-parametric. These methods are recommended if you have many features and would like to see the effect of heterogeneity and want confidence intervals. A few commonly used estimators are orthogonal random forests, causal forest (forest double machine learning) and forest doubly robust learner. For more details, please refer to Econ ML API documentation.To adjust selection bias, stratification (also known as subclassification or blocking) splits the entire group into homogeneous subgroups, where within each subgroup, ideally the treated group and the control group are similar under certain measurements over covariates. Then the treatment effect within each subgroup can be calculated using a method first developed on RCT data. With the CATE of each subgroup, the treatment effect over the interested group can be obtained by combining the CATEs of subgroups belonging to that group. For example, if we separate the whole dataset into j blocks, ATE for stratification is estimated as:where Ȳₜ(j) and Ȳ꜀(j) are the average of the treated and control outcomes in the jᵗʰ block, respectively. Furthermore, q(j) = N(j)/N is the portion of the units in the jᵗʰ block to the whole units. (See: A Survey on Causal inference.)Double machine learning is a method for estimating heterogeneous treatment effects when all potential confounders are observed, but are either too many for classical statistical approaches to be applicable, or their effect on the treatment and outcome cannot be satisfactorily modeled by parametric functions. Distinct from doubly robust learner and meta learners, this method can be applied to both discrete and continuous treatment types. The method reduces the problem to first estimating two predictive tasks:Then the method combines these two predictive models in a final stage estimation to create a model of the heterogeneous treatment effect. The approach allows for arbitrary machine learning algorithms to be used for the two predictive tasks, while maintaining many favorable statistical properties related to the final model (e.g., small mean squared error, asymptotic normality, and construction of confidence intervals). Mathematically, the double machine learning model is constructed as follows:Linear regression on residuals:Note: for 𝜃 constant, 𝜃 is the coefficient fromwhere Y is the observed outcome for the chosen treatment, T is the treatment, X represents the covariates used for heterogeneity, and W represents other observable covariates that we believe are affecting the potential outcome Y and potentially also the treatment T. We refer to variables W as controls. The variables X can also be thought of as control variables, but they are special in the sense that they are a subset of the controls with respect to which we want to measure treatment effect heterogeneity. We will refer to them as features.If you are working with a continuous heterogeneous treatment space, double machine learning is recommended as the default algorithm for you to start with. Or, if you are working with a discrete heterogeneous treatment space, meta learners or doubly robust learner are worth checking out. We compare more fundamental details about meta learners with double machine learning methods in Table 1. In Table 1, the meta learner formulas were provided on two response surfaces Y(0) and Y(1) for illustration purposes, even though they can model on multiple response surfaces, Y(0) to Y(K), as well.An instrumental variable is a random variable that influences the treatment but does not have any direct effect on the outcome, other than through the treatment. In practice, even when there are unobserved confounders (factors that simultaneously have a direct effect on the treatment decision in the collected data and the observed outcome), an instrumental variable (if it exits) represents an approach for estimating causal effects despite the presence of confounding latent variables. Instrumental variables (IV) can be used to cut correlations between the error term and independent variables in a model, hence conquering the problems related to endogeneity. The assumptions made are weaker than the unconfoundedness assumption needed in many other algorithms like double machine learning or meta learners. The cost is that when unconfoundedness holds, instrumental variable estimators will be less efficient. In the complex environment of the real world, the unconfoundedness assumption can rarely be satisfied. Unobserved confoundedness is a critical challenge of causal inference. Whenever instruments exist, it is better to prioritize the research design, but we need to be careful in model validation to make sure we have proper and strong instruments, otherwise estimates are very sensitive to violations.In addition to the IV methods discussed below, there are a few new and important orthogonal IV methods for heterogeneous treatment effects like DMLIV (double machine learning instrumental variables), DRIV (doubly robust IV), and Intent To Treat DRIV available in EconML, which implement new methods for non-parametric estimation of CATE with instruments and arbitrary ML. For more details, please refer to Vasilis 2019.Double least square: In the ordinary least square (OLS) method, there is a basic assumption that the value of the error term is independent of the predictor variables. When this assumption is broken, the double least square technique helps solve this problem. This analysis assumes that there is a secondary predictor that is correlated to the problematic predictor but not with the error term. Given the existence of the instrument variable, the following two methods are used:Sieve two-stage least square estimation (2SLS): In contrast to the parametric nature of the double least square model, Sieve two-stage least square estimation (2SLS) is a nonparametric model. We must specify the sieve basis for T, X, and Y (Hermite polynomial or a set of indicator functions) and the number of elements of the basis expansion to include. For more details, please refer to Bruce 2014.Deep instrumental variables (IV): The instrumental variables (IV) method is an approach for estimating causal effects despite the presence of confounding latent variables. The setup of the model is as follows:Where E[∈|X,W,Z ] = h(X,W) , so that the expected value of Y depends on (T, X, and W). This is known as the exclusion restriction. We assume that the conditional distribution F ( T | X, W, Z ) varies with Z . This is known as the relevant condition. The heterogeneous treatment effects are what we want to learn:The deep IV module learns the heterogenous causal effects by minimizing the “reduced-form” prediction error:where the hypothesis class g represents neural nets with a given architecture.This estimate is obtained by modeling F as a mixture of normal distributions, where the parameters of the mixture model are the output of a “first-stage” neural net whose inputs are (xᵢ, wᵢ, zᵢ). Optimization of the “first-stage” neural net is done by stochastic gradient descent on the (mixture-of-normals) likelihood, while optimization of the “second-stage” model for the treatment effects is done by stochastic gradient descent alone. The output is an estimated function ĝ. To obtain an estimate of τ, we take the difference the estimated function at t₀ and t₁, replacing the expectation with the empirical average over all observations with the specified x. For more details, please refer to Hardford 2017.In this post, we went through details of various algorithms and provided insights into algorithm selection for different problem settings. One thing that we need to keep in mind is that for causal inference, we are aiming for robust and valid causal estimates. A more complicated model doesn’t always result in a more accurate treatment effect estimate in a causal context. Business goals and available data are the main drivers of algorithm selection. Then we can try and compare multiple estimation methods.To validate the model, we do encourage trying multiple algorithms and comparing the estimates. We will introduce more details about model validation and application in the next article in our series. We hope this series helps you conquer your business problem through causal thinking. Please leave a comment to share your application scenarios and the techniques you are using today. We look forward to hearing from you.We’d like to thank the Microsoft Advanced AI school, Microsoft Research ALICE team, Finance, and Customer Program teams for being great partners in the research design and adoption of this work. We also would like to thank Ron Sielinski, Casey Doyle, and Deepsha Menghani for helping review the work.Related articles:",05/11/2020,0,27.0,138.0,452.0,97.0,23.0,15.0,0.0,29.0,cy
4135,"Convolutional Neural Networks, Explained",Towards Data Science,Mayank Mishra,24.0,9.0,1516.0,"A Convolutional Neural Network, also known as CNN or ConvNet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image. A digital image is a binary representation of visual data. It contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.The human brain processes a huge amount of information the second we see an image. Each neuron works in its own receptive field and is connected to other neurons in a way that they cover the entire visual field. Just as each neuron responds to stimuli only in the restricted region of the visual field called the receptive field in the biological vision system, each neuron in a CNN processes data only in its receptive field as well. The layers are arranged in such a way so that they detect simpler patterns first (lines, curves, etc.) and more complex patterns (faces, objects, etc.) further along. By using a CNN, one can enable sight to computers.A CNN typically has three layers: a convolutional layer, a pooling layer, and a fully connected layer.The convolution layer is the core building block of the CNN. It carries the main portion of the network’s computational load.This layer performs a dot product between two matrices, where one matrix is the set of learnable parameters otherwise known as a kernel, and the other matrix is the restricted portion of the receptive field. The kernel is spatially smaller than an image but is more in-depth. This means that, if the image is composed of three (RGB) channels, the kernel height and width will be spatially small, but the depth extends up to all three channels.During the forward pass, the kernel slides across the height and width of the image-producing the image representation of that receptive region. This produces a two-dimensional representation of the image known as an activation map that gives the response of the kernel at each spatial position of the image. The sliding size of the kernel is called a stride.If we have an input of size W x W x D and Dout number of kernels with a spatial size of F with stride S and amount of padding P, then the size of output volume can be determined by the following formula:This will yield an output volume of size Wout x Wout x Dout.Motivation behind ConvolutionConvolution leverages three important ideas that motivated computer vision researchers: sparse interaction, parameter sharing, and equivariant representation. Let’s describe each one of them in detail.Trivial neural network layers use matrix multiplication by a matrix of parameters describing the interaction between the input and output unit. This means that every output unit interacts with every input unit. However, convolution neural networks have sparse interaction. This is achieved by making kernel smaller than the input e.g., an image can have millions or thousands of pixels, but while processing it using kernel we can detect meaningful information that is of tens or hundreds of pixels. This means that we need to store fewer parameters that not only reduces the memory requirement of the model but also improves the statistical efficiency of the model.If computing one feature at a spatial point (x1, y1) is useful then it should also be useful at some other spatial point say (x2, y2). It means that for a single two-dimensional slice i.e., for creating one activation map, neurons are constrained to use the same set of weights. In a traditional neural network, each element of the weight matrix is used once and then never revisited, while convolution network has shared parameters i.e., for getting output, weights applied to one input are the same as the weight applied elsewhere.Due to parameter sharing, the layers of convolution neural network will have a property of equivariance to translation. It says that if we changed the input in a way, the output will also get changed in the same way.The pooling layer replaces the output of the network at certain locations by deriving a summary statistic of the nearby outputs. This helps in reducing the spatial size of the representation, which decreases the required amount of computation and weights. The pooling operation is processed on every slice of the representation individually.There are several pooling functions such as the average of the rectangular neighborhood, L2 norm of the rectangular neighborhood, and a weighted average based on the distance from the central pixel. However, the most popular process is max pooling, which reports the maximum output from the neighborhood.If we have an activation map of size W x W x D, a pooling kernel of spatial size F, and stride S, then the size of output volume can be determined by the following formula:This will yield an output volume of size Wout x Wout x D.In all cases, pooling provides some translation invariance which means that an object would be recognizable regardless of where it appears on the frame.Neurons in this layer have full connectivity with all neurons in the preceding and succeeding layer as seen in regular FCNN. This is why it can be computed as usual by a matrix multiplication followed by a bias effect.The FC layer helps to map the representation between the input and the output.Since convolution is a linear operation and images are far from linear, non-linearity layers are often placed directly after the convolutional layer to introduce non-linearity to the activation map.There are several types of non-linear operations, the popular ones being:1. SigmoidThe sigmoid non-linearity has the mathematical form σ(κ) = 1/(1+e¯κ). It takes a real-valued number and “squashes” it into a range between 0 and 1.However, a very undesirable property of sigmoid is that when the activation is at either tail, the gradient becomes almost zero. If the local gradient becomes very small, then in backpropagation it will effectively “kill” the gradient. Also, if the data coming into the neuron is always positive, then the output of sigmoid will be either all positives or all negatives, resulting in a zig-zag dynamic of gradient updates for weight.2. TanhTanh squashes a real-valued number to the range [-1, 1]. Like sigmoid, the activation saturates, but — unlike the sigmoid neurons — its output is zero centered.3. ReLUThe Rectified Linear Unit (ReLU) has become very popular in the last few years. It computes the function ƒ(κ)=max (0,κ). In other words, the activation is simply threshold at zero.In comparison to sigmoid and tanh, ReLU is more reliable and accelerates the convergence by six times.Unfortunately, a con is that ReLU can be fragile during training. A large gradient flowing through it can update it in such a way that the neuron will never get further updated. However, we can work with this by setting a proper learning rate.Now that we understand the various components, we can build a convolutional neural network. We will be using Fashion-MNIST, which is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. The dataset can be downloaded here.Our convolutional neural network has architecture as follows:[INPUT]→[CONV 1] → [BATCH NORM] → [ReLU] → [POOL 1]→ [CONV 2] → [BATCH NORM] → [ReLU] → [POOL 2]→ [FC LAYER] → [RESULT]For both conv layers, we will use kernel of spatial size 5 x 5 with stride size 1 and padding of 2. For both pooling layers, we will use max pool operation with kernel size 2, stride 2, and zero padding.Code snipped for defining the convnetWe have also used batch normalization in our network, which saves us from improper initialization of weight matrices by explicitly forcing the network to take on unit Gaussian distribution. The code for the above-defined network is available here. We have trained using cross-entropy as our loss function and the Adam Optimizer with a learning rate of 0.001. After training the model, we achieved 90% accuracy on the test dataset.Below are some applications of Convolutional Neural Networks used today:1. Object detection: With CNN, we now have sophisticated models like R-CNN, Fast R-CNN, and Faster R-CNN that are the predominant pipeline for many object detection models deployed in autonomous vehicles, facial detection, and more.2. Semantic segmentation: In 2015, a group of researchers from Hong Kong developed a CNN-based Deep Parsing Network to incorporate rich information into an image segmentation model. Researchers from UC Berkeley also built fully convolutional networks that improved upon state-of-the-art semantic segmentation.3. Image captioning: CNNs are used with recurrent neural networks to write captions for images and videos. This can be used for many applications such as activity recognition or describing videos and images for the visually impaired. It has been heavily deployed by YouTube to make sense to the huge number of videos uploaded to the platform on a regular basis.1. Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville published by MIT Press, 20162. Stanford University’s Course — CS231n: Convolutional Neural Network for Visual Recognition by Prof. Fei-Fei Li, Justin Johnson, Serena Yeung3. https://datascience.stackexchange.com/questions/14349/difference-of-activation-functions-in-neural-networks-in-general4. https://www.codementor.io/james_aka_yale/convolutional-neural-networks-the-biologically-inspired-model-iq6s48zms5. https://searchenterpriseai.techtarget.com/definition/convolutional-neural-network",26/08/2020,1,10.0,32.0,814.0,479.0,13.0,0.0,0.0,17.0,en
4136,Adversarial Examples to Break Deep Learning Models,Towards Data Science,Pau Labarta Bajo,319.0,10.0,1815.0,"Do you think it is impossible to fool the vision system of a self-driving Tesla car?Or that machine learning models used in malware detection software are too good to be evaded by hackers?Or that face recognition systems in airports are bulletproof?Like any of us machine learning enthusiasts, you might fall into the trap of thinking that deep models used out there are perfect.Well, you are WRONG.There are easy ways to build adversarial examples that can fool any deep learning model and create security issues. In this post, we will cover the following:Let’s start!In the last 10 years, deep learning models have left the academic kindergarten, become big boys, and transformed many industries. This is especially true for computer vision models. When AlexNet hit the charts in 2012, the deep learning era officially started.Nowadays, computer vision models are as good or better than human vision. You can find them in myriad places, including…Until recently, researchers trained and tested machine learning models in a laboratory environment, such as machine learning competitions and academic papers. Nowadays, when deployed in real-world scenarios, security vulnerabilities coming from model errors have become a real concern.Imagine for a moment that the state-of-the-art-super-fancy-deep learning vision system of your self-driving car was not able to recognize this stop sign as a stop sign.Well, this is exactly what happened. This stop sign image is an adversarial example. Think of it as an optical illusion for the model.Let us look at another example. Below you have two images of a dog that are indistinguishable to the human’s eye.The image on the left is an original picture of a puppy taken by Dominika Roseclay. The one on the right is a slight modification of the first, that I created by adding the noise vector in the central image.Inception v3 correctly classifies the original image as a dog breed (a redbone). However, this same model thinks, with very high confidence, that the modified image I created is a paper towel.In other words, I created an adversarial example. And you will too, as this is the example we will work on later in section 3.An adversarial example for a computer vision model is an input image with small perturbations, imperceptible to the human eye, that causes a wrong model prediction.Do not think these 2 examples are rare edge-case examples found after spending tons of time and computing resources. There are easy ways to generate adversarial examples, and this opens the door to serious vulnerabilities of machine learning systems in production.Let’s see how you can generate an adversarial example and fool a state-of-the-art image classification model.Adversarial examples 😈 are generated by taking a clean image 👼 that the model correctly classifies, and finding a small perturbation that causes the new image to be misclassified by the ML model.Let’s suppose you have complete information about the model you want to fool. In this case, you can compute the loss function of the modelwhereThis loss function is typically the negative log-likelihood for classification methods.Your goal is to find a new image X’ that is close to the original X and that produces a big change in the value of the loss function.Imagine you are inside the space of all possible input images, sitting on top of the original image X. This space has dimensions width x height x channels, so I will excuse you if you cannot visualize it well 😜.To find an adversarial example, you need to walk a little bit in some direction in this space until you find another image X’ with a remarkably different loss. You want to choose the direction that maximizes the change in the loss function J for a fixed small step epsilon.Now, if you refresh a bit your Maths Calculus course, the direction in the X space where the loss function changes the most is precisely the gradient of J with respect to the X.The gradient of a function with respect to one of its variables is precisely the direction of maximal change. And by the way, this is the reason people train Neural Networks using Stochastic Gradient Descend and not Stochastic Random-Direction Descend.An easy way to formalize this intuition is as follows:We take only the sign of the gradient and scale it using a small parameter epsilon, to guarantee that the distortion between X and X is small enough to be imperceptible to the human eye. This method is called the fast gradient sign method.In most scenarios, it is very likely you will not have complete information about the model. Hence, the previous method is not useful as you cannot compute the gradient.However, there exists a remarkable property called transferability of adversarial examples that malicious agents can exploit to break a model even if they do not know its internal architecture and parameters.Researchers have repeatedly observed that adversarial examples transfer quite well between models, meaning that they can be designed for a target model A, but end up being effective against any other model trained on a similar dataset.Adversarial examples can be generated as follows:A successful application of this strategy against a commercial Machine learning model is presented in this Computer Vision Foundation paper.Let’s get our hands dirty and implement a few attacks using Python and the great library PyTorch. It always comes in handy to know how the attacker thinks.You can find the complete code in this Github repo.Our target model is going to be Inception V3, a powerful image classification model developed by Google, which has around 27M parameters and was trained on around 14M images belonging to 20k categories.We download the list of classes the model was trained on, and build an auxiliary dictionary that maps classes ids to labels.Let us take the image of an innocent redbone dog as the starting image we will carefully modify to build adversarial examples:Inception V3 expects images with dimensions 299 x 299 and normalized pixel ranges between -1 and 1.Let us preprocess our beautiful dog image:and check that the model correctly classifies this image.Good. The model works as expected and the redbone dog is classified as a redbone dog :-).Let’s move to the fun part and generate adversarial examples using the Fast Gradient Sign method.I have created an auxiliary function to visualize both the original and the adversarial image. You can see the full implementation in this GitHub repository.Well. It is interesting how the model prediction changed for the new image, which is almost indistinguishable from the original one. The new prediction is a bloodhound, which is another dog breed with very similar skin color and big ears. As the puppy in question could be a mixed breed, the model mistake seems to be small, so we want to work further to really break this model.One possibility is to play with different values of epsilon and try to find one that clearly gives a wrong prediction. Let’s try this.As epsilon increases the change in the image becomes visible. However, the model predictions are still other dog breeds: bloodhound and basset. We need to be smarter than this to break the model.Remember the intuition behind the Fast gradient sign method, i.e. imagine yourself inside the space of all possible images (with dimension 299 x 299 x 3), right where the original image X is. The gradient of the loss function tells you the direction you need to step to increase its value and make the model less certain about the right prediction. The size of the step is epsilon. You step and you check if you have are now sitting on an adversarial example.An extension of this would be to take many smaller steps, instead of a single one. After each step, you re-evaluate the gradient and decide the new direction you are going to walk towardsThis method is called the Iterative Fast Gradient method. What an original name!More formally:Where X0 = X, and Clip X,ϵ denotes clipping of the input in the range of [X−ϵ, X+ϵ].An implementation in PyTorch is the following:Now, let us try again to generate a good adversarial example starting from our innocent puppy.Step 1: bloodhound againStep 2: beagle againStep 3: mousetrap? Interesting. However, the model confidence on this prediction is only 16%. Let’s go further.Step 4: One more dog breed, boring…Step 5: beagle again..Step 6:Step 7: redbone again. Keep calm and continue walking in the image space.Step 8:Step 9: BINGO! 🔥🔥🔥What an exotic paper towel. And the model is pretty confident about its prediction, almost 99%.If you compare the initial and the final image we found in step 9you see they are essentially the same, a puppy, but for the Inception V3 model they are two very different things.I was very surprised the first time I saw such a thing. How small modifications to an image can cause such model misbehavior. This example is funny, but if you think about the implications for a self-driving car vision you might start to worry a bit.Deep learning models deployed in critical tasks need to properly handle adversarial examples, but how?As of November 2021, there is no universal defense strategy you can follow to protect against adversarial examples. In other words, attacking a model with adversarial examples is easier than protecting it.The best universal strategy, that can protect against any known attack, is to use adversarial examples when training the model.If the model sees adversarial examples during training, its performance at prediction time will be better for adversarial examples generated in the same way. This technique is called adversarial training.You generate them on the fly, as you train the model, and adjust the loss function to look both at clean and adversarial inputs.For example, we could eliminate the adversarial examples we found in the previous section if we added the 10+ examples to the training set and labeled all of them as redbone.This defense is very effective against attacks that use the Fast Gradient Sign method. However, there exist more powerful attacks, that we did not cover in this post, that can bypass this defense. If you wanna know more I encourage you to read this amazing paper by Nicholas Carlini and David Wagner.This is the reason why adversarial defense is an open problem in Machine Learning and cybersecurity.Adversarial examples are a fascinating topic at the intersection of cybersecurity and machine learning. However, it is still an open problem waiting to be solved.In this post, I gave a practical introduction to the topic with code examples. If you would like to work further I suggest cleverhans, a Python library for adversarial machine learning developed by the great Ian Goodfellow and Nicolas Papernot, and currently maintained by the University of Toronto.All the source code I shown in this article is in this Github repo.github.comDo you want to become an (even) better data scientist, and access top courses on machine learning and data science?👉🏽 Subscribe to the datamachines newsletter.👉🏽 Give me lots of claps 👏🏿👏🏽👏 below👉🏽 Follow me on Medium.Have a great day 🧡Pau",16/11/2021,0,27.0,31.0,973.0,473.0,23.0,4.0,0.0,18.0,en
4137,Understanding the Backbone of Video Classification: The I3D Architecture,Towards Data Science,Madeline Schiappa,247.0,4.0,807.0,"One of the distinctive differences between information in a single image and information in a video is the temporal element. This has led to improvements of deep learning model architectures to incorporate 3D processing in order to additionally process temporal information. This article summarizes the architectural changes from images to video through the I3D model.The I3D model was presented by researchers from DeepMind and the University of Oxford in a paper called “Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset” [1]. The paper compares previous approaches to the problem of action detection in videos while additionally presenting a new architecture, the focus here. Their approach starts with a 2D architecture and inflates all the filters and pooling kernels. By inflating them, they add an additional dimension to be taking into consideration, which in our case is time. While filters in 2D models are square N x N, by inflating them the filters become cubic N x N x N.In this paper the researchers bootstrap 3D filters from 2D filters on already trained models. In other words, they use the parameters from a 2D model trained on a very large dataset, like ImageNet, on their 3D implementation. To think about this intuitively, they repeat a given image T amount of times. What they actually do is repeat the weights of the 2D filters N times along the time dimension and then re-scaling them by dividing by N. To ensure this is properly done, the average and max-pooling layers should be the same for the 2D case as the 3D case for ImageNet.Another modification to consider is the receptive field of pooling and convolutional layers. To refresh, a receptive field in a convolutional neural network is the part of the image that is visible to one filter at a time, and it increases as we stack more layers. 2D convolutions and pooling focus on the height and width of the image and therefore are symmetrical(e.g. a kernel of 7x7 is symmetrical while a kernel 7x3 is not). However, when a temporal dimension is included, it is important to find the optimal receptive field, which is dependent on the frame rate and image dimensions. According to the researchers in [1], if the receptive field grows too quickly in time relative to space, it may conflate edges from different objects, breaking early feature detection. If the receptive field grows too slowly, it may not capture scene dynamics as well. In summary, the kernels in I3D are not symmetrical because of the additional time dimension.With these in mind, the authors visualize their architecture with the following diagram:As can be seen by the diagram, the beginning of the network uses asymmetrical filters for max-pooling, maintaining time while pooling over the spatial dimension. It is not until later in the network that they run convolutions and pooling that includes the time dimension. The inception module is commonly used in 2D networks and is out of the scope of this article. In summary however, it is an approximation of an optimal local sparse structure. It also processes spatial (and time in this case) information at various scales and then aggregates the results. This module was motivated to allow the network to grow “wider” instead of “deeper”. The 1x1x1 convolution is used to reduce the number of input channels before the larger 3x3x3 convolutions, also making it less computationally expensive than the alternative.Although the formal introduction of the architecture is a major contribution of the paper, the main contribution is the transfer learning from a Kinetics dataset to other video tasks. The Kinetics Human Action Dataset contains annotated videos of human actions. This is the reason behind the use of the pre-trained I3D network as a feature extraction network in a variety of action related deep learning tasks. Features are commonly extracted from the ‘mixed-5c’ module and passed into new architectures or a fine-tuned version of I3D.I3D is one of the most common feature extraction methods for video processing. Although there are other methods like the S3D model [2] that are also implemented, they are built off the I3D architecture with some modification to the modules used. If you want to classify video or actions in a video, I3D is the place to start. If you want features from a pre-trained model for your video related experiments, I3D is also recommended. I hope you enjoyed this summary![1] Carreira, J., & Zisserman, A. (2017). Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6299–6308).[2] Xie, S., Sun, C., Huang, J., Tu, Z., & Murphy, K. (2018). Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 305–321).*The images in this article are made to resemble original images from [1]",07/06/2020,0,2.0,13.0,986.0,476.0,2.0,0.0,0.0,1.0,en
4138,"Random Forests Classification: MATLAB, R and Python codes — All you have to do is just preparing data set (very simple, easy and practical)",Medium,DataAnalysis For Beginneｒ,279.0,3.0,287.0,"I release MATLAB, R and Python codes of Random Forests Classification (RFC). They are very easy to use. You prepare data set, and just run the code! Then, RFC and prediction results for new samples can be obtained. Very simple and easy!You can buy each code from the URLs below.https://gum.co/RciDk Please download the supplemental zip file (this is free) from the URL below to run the RFC code. http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.ziphttps://gum.co/gdJgy Please download the supplemental zip file (this is free) from the URL below to run the RFC code. http://univprofblog.html.xdomain.jp/code/R_scripts_functions.ziphttps://gum.co/nDrmZ Please download the supplemental zip file (this is free) from the URL below to run the RFC code. http://univprofblog.html.xdomain.jp/code/supportingfunctions.zipTo perform appropriate RFC, the MATLAB, R and Python codes follow the procedure below, after data set is loaded.1. Decide the number of decision trees For example, it is 500.2. Decide candidates of the ratio of the number of explanatory variables (X) for decision trees For example, they are 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8.3. Run RFC for every candidate of X-ratio and estimate values of objective variable (Y) for Out Of Bag (OOB) samples4. Calculate misclassification rate between actual Y and estimated Y for each candidate of X-ratio5. Decide the optimal X-ratio with the minimum misclassification rate value6. Construct RFC model with the optimal X-ratio7. Calculate confusion matrix between actual Y and calculated Y for the optimal X-ratio8. Calculate confusion matrix between actual Y and estimated Y of OOB samples for the optimal X-ratio9. Estimate Y based on the RFC model in 6.If it takes too much time to train RFC, please decrease the number of decision trees.MATLAB: https://gum.co/RciDkR: https://gum.co/gdJgyPython: https://gum.co/nDrmZMATLAB: http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.zipR: http://univprofblog.html.xdomain.jp/code/R_scripts_functions.zipPython: http://univprofblog.html.xdomain.jp/code/supportingfunctions.ziphttps://medium.com/@univprofblog1/data-format-for-matlab-r-and-python-codes-of-data-analysis-and-sample-data-set-9b0f845b565a#.3ibrphs4hEstimated values of Y for “data_prediction2.csv” are saved in ”PredictedY2.csv”.Please see the article below. https://medium.com/@univprofblog1/settings-for-running-my-matlab-r-and-python-codes-136b9e5637a1#.paer8scqy",23/08/2016,0,15.0,0.0,654.0,525.0,3.0,0.0,0.0,14.0,en
4139,Why Transformers are Slowly Replacing CNNs in Computer Vision?,Becoming Human: Artificial Intelligence Magazine,Pranoy Radhakrishnan,870.0,8.0,1457.0,"Before getting into Transformers, let’s understand why researchers were interested in building something like Transformers inspite of having MLPs , CNNs and RNNs.Everyone wants a universal model to solve different tasks with accuracy and speed. Just like MLPs which are universal function approximators, Transformer models are universal approximators of sequence-to-sequence functions.Transformers use the concept of Attention mechanism. Let’s look what is attention and briefly go through self attention mechanisms.Attention mechanism enhances the important parts of the input data and fades out the rest. Take the example of you captioning an image. You will have to focus on the relevant part of the image to generate meaningful captions. This is what Attention mechanisms do.But why we need attention and all, CNNs are pretty good at feature extraction, Right?For a CNN, both of these pictures are almost same. CNN does not encode the relative position of different features. Large filters are required to encode the combination of these features. For examples:- to encode the information “eyes above nose and mouth” require large filters.Large receptive fields are required in order to track long-range dependencies within an image. Increasing the size of the convolution kernels can increase the representational capacity of the network but doing so also loses the computational and statistical efficiency obtained by using local convolutional structure.Self-Attention modules, a type of Attention Mechanism, along with CNN helps to model long-range dependencies without compromising on computational and statistical efficiency. The self-attention module is complementary to convolutions and helps with modeling long range, multi-level dependencies across image regions.Here you can see a self attention module replaces the convolutional layer, so that now the model gets the ability to interact with pixels far away from its location.More recently, researchers ran a series of experiments replacing some or all convolutional layers in ResNets with attention, and found the best performing models used convolutions in early layers and attention in later layers.The self-attention mechanism is a type of attention mechanism which allows every element of a sequence to interact with every others and find out who they should pay more attention to.It allows capturing ‘long-term’ information and dependencies between sequence elements.As you can see from the image above, “it” refers to the “street” and not “animal”. Self-attention is a weighted combination of all other word embeddings. Here the embedding of “it” is a weighted combination of all other embedding vectors, with more weightage on the word “street”.To understand how the weighting is done, watch — https://www.youtube.com/watch?v=tIvKXrEDMhkBasically, a self-attention layer updates each component of a sequence by aggregating global information from the complete input sequence.So here we just learned how an attention mechanism like self attention can effectively solve some of the limitations of the Convolutional Networks. Now is it possible to entirely replace the CNN’s with an attention based model like the Transformers?Transformers have already replaced the LSTMs in the NLP domain. What is the possibility of the transformers replacing the CNN’s in computer vision. What are the approaches built with attention which have outperformed CNNs. Let’s look into that!The Transformer model was first proposed for solving NLP tasks mainly Language Translation. The proposed Transformer model has an encoder-decoder structure. Here, the picture on the left shows an encoder and the right is a decoder. Both the encoder and the decoder has self attention layers , linear layers and residual connection as shown.Note : In convolutional networks, feature aggregation and feature transformation are simultaneously performed (e.g., with a convolution layer followed by a non-linearity), these two steps are done separately in the Transformer model i.e., self-attention layer only performs aggregation while the feed-forward layer performs transformation.Take the case of language translation itself. The self attention mechanism in encoder helps the inputs(words) to interact with each other, thereby creating a representation for each token(word) that has semantic similarity between other tokens(words).The decoder’s job is to output the translated word one at a time attending on both input embeddings and outputs generated so far.The Decoder outputs one word at time by concentrating on specific parts of the encoder outputs and by looking at the previous outputs generated so far. To ensure that the decoder looks only at the previous outputs generated and not at the future outputs during training, Mask Self Attention mechanism is used in the decoder. It just masks the future words given at the input of the decoder during training.Self attention doesn’t care about position. For example in the sentence “Deepak is the son of Pradeep”. Consider that the SA gives more weightage to the word “Deepak” for the word “son” . If we shuffle the sentence, say “Pradeep is the son of Deepak”, the SA still weigh “Deepak” more for the word “son”. But here we want SA to weigh “Pradeep” more for the word “son”.This happens because the Self attention module produces the encoding for the word “son” by a weighted combination of all word embeddings without knowing its position. So shuffling of the tokens does not create any difference. Or in other words SA is permutation invariant. Self-attention is permutation-invariant unless you add positional information to each token before passing the embeddings to the SA module.Vision Transformer is an approach to replace convolutions entirely with a Transformer model.Transformer always operates on sequences, thats why we split the images to patches and and flattening each such “patch” to a vector. From now i would call a “patch” as a token. So now have a sequence of tokens.The self attention is by design is permutation invariant. Self-attention “generalises” the summation operation as it performs a weighted summation of different attention vectors.Invariance under permutation means that if you present a vector as an input — say [A, B, C, D] — the network should output the same result as if you had input [A, C, B, D], for instance, or any other permutation of vector elements.So positional information is added to each token before passing the whole sequence to the self attention module. Adding positional information will help the transformer understand the relative position of each tokens in the sequence. Here positional encodings are learned instead of using standard encodings.Finally output of the transformer from the first position is used for further classification by an MLP.Training the transformers from scratch requires a lot much data than CNN. This is because CNNs encode prior knowledge about the image domain like translational equivariance. Transformers on the other hand has to learn these properties from the data given.Translational equivariance is a property of convolutional layers where the feature layer will have an activation moved to the right, if we shift the object to the right in the image. But they are “really the same representation”.Vision Transformer , entirely provides the convolutional inductive bias(eg: equivariance) by performing self attention across of patches of pixels. The drawback is that, they require large amount data to learn everything from scratch.CNNs performs better in the low data data regimes due to its hard inductive bias. Whereas when a lot of a data is available, hard inductive biases(provided by CNNs) is restricting the overall capability of the model.So is it possible to obtain the benefits of the hard inductive bias of CNN’s in low data regimes without suffering from its limitations in large data regimes?The idea is to introduce a Positional Self Attention Mechanism(PSA) which allows the model to act as convolutional layers if needed. We just replace some of the Self attention layers with PSA Players.We know positional information is always added to the patches since SA are permutation invariant otherwise. Instead of adding some positional information to the input at embedding time before propagating it through the Self Attention layers(like in Vit), we replace the vanilla Self Attention with positional self-attention (PSA).In PSA, you can see Attention weights are calculated using relative positional encodings(r) and a trainable embedding(v) also. Relative positional encodings(r) depend only on the distance between the pixelsThese multi-head PSA Layers with learnable relative positional encodings can express any convolutional layer.So here we are not combining CNNs and attention mechanisms. Instead we use PSA Layers which have the capability to act as Convolutional Layers by adjusting some parameters. We are utilising this power of the PSA layers at initialisation. In small data regimes, this can help the model to generalise well. Whereas in large data regimes, the PSA layers can leave its convolutional nature if needed.Here we show that, Transformers originally developed to solve Machine Translation is showing good results in computer vision. ViTs outperforming CNNs for image classification was a major breakthrough. However, they require costly pre-training on large external datasets. ConViT, outperforms the ViTs on ImageNet, while offering a much improved sample efficiency. These results show that Transformers have the capability to overtake CNNs in many computer vision tasks.Transformers in Vision — https://arxiv.org/pdf/2101.01169.pdfConViT — https://arxiv.org/pdf/2103.10697.pdf",31/08/2021,0,24.0,0.0,555.0,301.0,12.0,1.0,0.0,6.0,da
4140,어텐션 메커니즘과 transfomer(self-attention),mojitok,platfarm tech team,29.0,14.0,918.0,"어텐션 메커니즘은 자연어 기계 번역을 위한 Seq2Seq 모델에 처음 도입되었습니다. 어텐션 메커니즘은 NLP 태스크 뿐만 아니라, 도메인에 관계 없이 다양하게 쓰이고 있습니다. 현재의 SOTA NLP모델들은 대부분 어텐션 메커니즘을 적용하고 있으니 최근 논문을 이해함에 있어 이해하고 넘어가야 하는 부분입니다.코드는 이곳(https://github.com/graykode/nlp-tutorial)을 참고해주세요 .1. Seq2Seq (링크)2. Seq2Seq with Attention (링크)3. Bi-LSTM with Attention (링크)4. Transformer (링크)Seq2Seq 모델은 대중적이므로 가볍게 짚고만 넘어가겠습니다. Seq2Seq 모델에 대한 자세한 설명들은 ratsgo님의 블로그를 참조하면 볼 수 있습니다. 더불어 자세한 내용은 원작 논문인 Neural Machine Translation by Jointly Learning to Align and Translate을 참고하셔도 좋습니다. 위 논문은 Attention 메커니즘이 처음 등장한 논문이자 조경현 교수님이 쓰신 논문입니다.번역모델(Seq2Seq) 어텐션 메커니즘의 핵심은 디코더의 특정 time-step의 output이 인코더의 모든 time-step의 output 중 어떤 time-step과 가장 연관이 있는가입니다. 이를 수식으로 다음과 같이 나타냅니다.Seq2Seq는 encoder의 output과 decoder output의 관계로 attention을 봤다면 LSTM에서는 LSTM를 거친 모든 outputs(contextual matrix)와 LSTM의 최종 state(query)간의 Attention을 본다는 차이가 있습니다.위의 그림은 1층 layer로 이루어진 Classification을 위한 Bi-LSTM Attention을 나타냅니다. Classification에서의 Attention은 번역 모델(Seq2Seq)와 다르게 LSTM hidden cell의 마지막 Hidden State이 어떤 time에 영향을 많이 받았는지가 포인트입니다.Transformer는 RNN, LSTM없이 time 시퀀스 역할을 하는 모델입니다. RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time 시퀀스 역할을 해줄 수 있는 굉장히 novel한 논문입니다. 원본 논문은 Attention Is All You Need(2017)이고, 이 논문 이후 현재의 모든 SOTA 모델들이 Transformer 구조에 기반하여 구현되었습니다. (BERT, openAI-GPT 등등) 원본 논문은 꼭 한번 읽어보시기 바랍니다. 해당 논문에 대한 요약은 (링크)에 잘 나와있습니다. 또한 일부 내용도 해당 블로그에서 참조하였음을 밝힙니다. 전체적인 Transformer 모델은 아래와 같습니다.일반적인 Seq2Seq-Attention 모델에서의 번역 태스크의 문제는 원본 언어(Source Language), 번역된 언어(Target Language)간의 어느정도 대응 여부는 어텐션을 통해 찾을 수 있었으나, 각 자신의 언어만에 대해서는 관계를 나타낼수 없었습니다. 예를 들면 I love tiger but it is scare와 나는 호랑이를 좋아하지만 그것(호랑이)는 무섭다 사이의 관계는 어텐션을 통해 매칭이 가능했지만 it이 무엇을 나타내는지?와 같은 문제는 기존 Encoder-Decoder 기반의 어텐션 메커니즘에서는 찾을 수 없었습니다.Transformer 모델을 세분화 하면 다음과 같은 구조를 가집니다.위 그림에서 빨간색이 인코더, 파란색이 디코더를 가르키고, 자세한 설명은 다음과 같습니다.Transformer가 학습하는 전반적인 그림은 다음과 같습니다.문장은 일반적인 임베딩과 Positional Encoding을 더하여 Encoder의 Layer로 들어갑니다.예를 들어 I love you but not love him이라는 문장이라면 앞의 love와 뒤의 love은 일반적인 임베딩만을 거쳤을 때 동일한 값을 가지게 됩니다.(이는 일반적인 Word2Vec과 같은 임베딩의 문제이기도 합니다.) 하지만 Positional Encoding 이라는 주기함수에 의한 위치에 따른 다른 임베딩을 거치면 같은 단어여도 문장에서 쓰인 위치에 따라 다른 임베딩 값을 가지게 됩니다.Positional Encoding은 get_sinusoid_encoding_table이라는 주기 함수로 구현됩니다.이 값이 동일하게 매 Layer마다 Q, K, V로 복사되어 Encoder_Layer로 들어가 MultiHeadAttention를 거치게 됩니다.d_model은 임베딩을 하기 위한 차원으로 보통 512를 사용하고, d_k와 d_v는 64를 사용합니다. 그리고 위 논문의 multi-head attention 그림에서의 h는 n_heads(number of head)를 나타내며 보통 8을 사용합니다. 이는 64 * 8 = 512이기 때문입니다.동일한 [batch_size x len_q x d_model] shape의 동일한 Q, K, V를 만들어 [d_model, d_model] linear를 곱한 후 임베딩 차원을 8등분하여 Scaled Dot Product Atttention으로 넘겨줍니다.Multi-Head Attention의 역할은 1문장을 여러 head로 Self-Attention 시킴에 있습니다.“Je suis étudiant”라는 문장의 임베딩 벡터가 512차원이라면 8개 head로 나눠 64개의 벡터를 한 Scaled Dot Attention이 맡아 처리하는 것입니다. 이는 동일한 문장도 8명(8 heads)이 각각의 관점에서 보고 추후에 합치는 과정이라고도 볼 수 있습니다.Scaled Dot Product Attention은 Self-Attention이 일어나는 부분입니다. 위에서 한 head당 Q(64), K(64), V(64)씩 가져가게 되는데 Self-Attention은 다음과 같습니다.여기서 Mask(opt.) 부분은 Decoder의 Masked MultiHead Attention과 다른 Masking이고 Optional 한 부분입니다. 단순히 아래와 같은 코드를 통해 입력 dimension(=512) 중 word이 아닌 경우를 구분하는 역할을 합니다(word 입력이 끝난 후 padding 처리와 동일)Q와 transposed K를 내적한후 Scaled with Softmax 함으로써 Self-Attention 시킵니다. 후 동일한 encoder에서 나온 V를 곱합니다. 위 Mask에 (Opt.)는 Optional의 약자로 구현 디테일을 의미합니다. 하지만 decoder에서의 self-attention 시에는 time 시퀀스와 같이 적용되므로 반드시 masking을 해야합니다.(Masked Multi-Head Attention) 이는 decoder시 설명하도록 하겠습니다.Seq2Seq attention에서 F(enc_output)와 alpha_vector를 내적해서 attention의 결과인 context vector로 사용했던것 처럼, 여기서는 V와 Q와 K_T의 내적의 softmax를 내적하여 context vector로 사용합니다.Scaled Dot Product라는 의미는 Q와 K 사이를 내적하여 어텐션을 Softmax를 통해 구하고, 그 후에 V를 내적하여 중요한 부분(Attention)을 더 살린다는 메커니즘이 내포되어있습니다.이렇게 8개의 head(여러 관점)으로 본 것을 다시 concate하고 PoswiseFeedForwardNet시킵니다.더불어 처음의 Q를 ResNet의 Residual Shortcut와 같은 컨셉으로 더해줍니다. 이는 층이 깊어지는 것을 더 잘 학습 시키기 위함입니다.Multi Head Attention에서 각 head가 자신의 관점으로만 문장을 Self-Attention 하게 된다면 각 head에 따라 Attention이 치우쳐질 것입니다. PoswiseFeedForwardNet은 각 head가 만들어낸 Self-Attention을 치우치지 않게 균등하게 섞는 역할을 합니다. BERT 톺아보기라는 글을 보면 PoswiseFeedForwardNet의 논문 구현 방식과 최근 트렌드의 구현 방식에 대해 설명하고 있습니다.While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.저도 conv 2개를 사용해서 kernel size 1로 구현했으며 이부분은 구현 디테일 이라고 생각합니다.디코더는 인코더와 동일하지만, Self-Attention시 Masked-Multi-Head Attention을 쓴다는 점이 다릅니다. Masked를 쓰는 이유는 Self-Attention시 자신의 time step 이후 word는 가려 Self-Attention 되는 것을 막는 역할을 합니다. 아래 코드와 같이 `np.triu`를 통해 한번 삼각형 행렬을 만들어 줍니다. 1이 Mask 역할을 합니다.마지막으로 Encoder의 K와 V, Decoder의 Q를 서로 Attention 시켜 위의 Seq2Seq 모델과 동일하게 Encoder와 Decoder 사이 관계도 Attention 시켜 줍니다.노란색 Box에서 왼쪽 2개 화살표가 Encoder의 K,V 오른쪽 화살표가 Self-Attention을 거친 Decoder의 Q입니다.이렇게 나온 logistic을 일반적인Teacher Forcing을 통해 학습을합니다.일반적인 Seq2Seq 모델과 동일하게 Inference시 Encoder의 들어오는 문장(번역할 문장)은 정확히 알지만, Decoder에 들어오는 문장(번역되어지는 문장)은 알지 못합니다. 따라서 시작 표시를 나타내는 <S>를 사용해서 Seq2Seq와 동일하게 Inference 합니다.이때 Encoder은 일반 학습시의 Encoder와 동일하고, Decoder만 순차적으로 진행하기 위해 Greedy Decoder 혹은 Beam Search를 사용합니다. Greedy Decoder는 위의 그림과 같이 한 word를 넣으면 그 word와 inference를 거친 다음 word를 붙여나가는 식으로 inference를 거칩니다.위의 그림과 같이 inference word가 <E>(end point)이면 inference를 멈춥니다.감사합니다.ratsgo’s blog: Sequence-to-Sequence 모델로 뉴스 제목 추출하기Neural Machine Translation by Jointly Learning to Align and TranslateAttention Is All You NeedThe Illustrated TransformerBERT 톺아보기안녕하세요! 플랫팜 인턴 Jeff(정태환/Tae Hwan Jung, nlkey2022@gmail.com)입니다. NLP (Sentiment Analysis) task를 진행하고 있습니다😀 NLP에 관련된 여러 논문들을 읽어보고 그 중 최근 SOTA인 Attention에 대해 글을 작성해볼 기회가 생겨 되도록 쉽게 설명해보았습니다.",10/03/2019,0,21.0,1.0,631.0,479.0,19.0,6.0,0.0,15.0,ko
4141,Understanding the Mathematics behind Gradient Descent.,Towards Data Science,Parul Pandey,19600.0,10.0,1584.0,"“Premature optimization is the root of all evil.” ― Donald Ervin KnuthAgile is a pretty well-known term in the software development process. The basic idea behind it is simple: build something quickly, ➡️ get it out there, ➡️ get some feedback ➡️ make changes depending upon the feedback ➡️ repeat the process. The goal is to get the product near the user and guide you with feedback to obtain the best possible product with the least error. Also, the steps taken for improvement need to be small and should constantly involve the user. In a way, an Agile software development process involves rapid iterations. The idea of — start with a solution as soon as possible, measure and iterate as frequently as possible, is Gradient descent under the hood.Gradient descent algorithm is an iterative process that takes us to the minimum of a function(barring some caveats). The formula below sums up the entire Gradient Descent algorithm in a single line.But how do we arrive at this formula? Well, It is straightforward and includes some high school maths. Through this article, we shall try to understand and recreate this formula in the context of a Linear Regression model.This article is an adaption of the video titled Mathematics of Gradient Descent — Intelligence and Learning by The Coding Train. This article was initially created as notes to supplement my understanding. I’ll highly recommend to see the video too.Given a known set of inputs and their corresponding outputs, A machine learning model tries to make some predictions for a new set of inputs.The Error would be the difference between the two predictions.This relates to the idea of a Cost function or Loss function.A Cost Function/Loss Function evaluates the performance of our Machine Learning Algorithm. The Loss function computes the error for a single training example, while the Cost function is the average of the loss functions for all the training examples. Henceforth, I shall be using both the terms interchangeably.A Cost function basically tells us ‘ how good’ our model is at making predictions for a given value of m and b.Let’s say there are a total of ’N’ points in the dataset, and for all those ’N’ data points, we want to minimize the error. So the Cost function would be the total squared error i.eWhy do we take the squared differences and simply not the absolute differences? Because the squared differences make it easier to derive a regression line. Indeed, to find that line we need to compute the first derivative of the Cost function, and it is much harder to compute the derivative of absolute values than squared values. Also, the squared differences increase the error distance, thus, making the bad predictions more pronounced than the good ones.The goal of any Machine Learning Algorithm is to minimize the Cost Function.This is because a lower error between the actual and predicted values signifies that the algorithm has done an excellent job learning. Since we want the lowest error value, we want those‘ m’ and ‘b’ values that give the smallest possible error.If we look carefully, our Cost function is of the form Y = X² . In a Cartesian coordinate system, this is an equation for a parabola and can be graphically represented as :To minimize the function above, we need to find that value of X that produces the lowest value of Y which is the red dot. It is pretty easy to locate the minima here since it is a 2D graph, but this may not always be the case, especially in higher dimensions. For those cases, we need to devise an algorithm to locate the minima, and that algorithm is called Gradient Descent.Gradient descent is one of the most popular algorithms to perform optimization and is the most common way to optimize neural networks. It is an iterative optimization algorithm used to find the minimum value for a function.Consider that you are walking along with the graph below, and you are currently at the ‘green’ dot. You aim to reach the minimum, i.e., the ‘red’ dot, but from your position, you are unable to view it.Possible actions would be:Essentially, there are two things that you should know to reach the minima, i.e. which way to go and how big a step to take.Gradient Descent Algorithm helps us to make these decisions efficiently and effectively with the use of derivatives. A derivative is a term that comes from calculus and is calculated as the slope of the graph at a particular point. The slope is described by drawing a tangent line to the graph at the point. So, if we can compute this tangent line, we might compute the desired direction to reach the minima. We will talk about this in more detail in the latter part of the article.In the same figure, if we draw a tangent at the green point, we know that if we are moving upwards, we are moving away from the minima and vice versa. Also, the tangent gives us a sense of the steepness of the slope.The slope at the blue point is less steep than that at the green point, which means it will take much smaller steps to reach the minimum from the blue point than from the green point.Let us now put all these learnings into a mathematical formula. In the equation, y = mX+b‘m’ and ‘b’ are its parameters. During the training process, there will be a small change in their values. Let that small change be denoted by δ. The value of parameters will be updated as m=m-δm and b=b-δb, respectively. Our aim here is to find those values of m and b iny = mx+b For which the error is minimum, i.e., values that minimize the cost function.Rewriting the cost function:The idea is that by being able to compute the derivative/slope of the function, we can find the minimum of a function.This size of steps taken to reach the minimum or bottom is called Learning Rate. We can cover more area with larger steps/higher learning rate but are at the risk of overshooting the minima. On the other hand, small steps/smaller learning rates will consume a lot of time to reach the lowest point.The visualizations below give an idea about the Learning Rate concept. See how in the third figure, we reach the minimum point with the minimum number of steps. This is the optimum learning rate for this problem.We saw that when the learning rate is too low, it takes many steps to converge. On the other hand, when the learning rate is too high, Gradient Descent fails to reach the minimum, as seen in the visualization below.Experiment with different learning rates by visiting the link below.developers.google.comMachine learning uses derivatives in optimization problems. Optimization algorithms like gradient descent use derivates to decide whether to increase or decrease the weights to increase or decrease any objective function.If we are able to compute the derivative of a function, we know in which direction to proceed to minimize it.Primarily we shall be dealing with two concepts from calculus :Power rule calculates the derivative of a variable raised to a power.The chain rule is used for calculating the derivative of composite functions. The chain rule can also be expressed in Leibniz’s notation as follows:If a variable z depends on the variable y, which itself depends on the variable x, so that y and z are dependent variables, then z, via the intermediate variable of y, depends on x as well. This is called the chain rule and is mathematically written as,Let us understand it through an example:Let us now apply the knowledge of these rules of calculus in our original equation and find the derivative of the Cost Function w.r.t to both ‘m’ and ‘b’. Revising the Cost Function equation :For simplicity, let us get rid of the summation sign. The summation part is important, especially with the concept of Stochastic gradient descent (SGD ) vs. batch gradient descent. During the batch gradient descent, we look at the error of all the training examples at once, while in the SGD, we look at each error at a time. However, to keep things simple, we will assume that we are looking at each error one at a time.Now let’s calculate the gradient of Error w.r.t to both m and b :Plugging the values back in the cost function and multiplying it with the learning rate:This 2 in this equation isn’t that significant since it just says that we have a learning rate twice as big or half as big. So let’s get rid of it too. So, Ultimately, this entire article boils down to two simple equations representing the equations for Gradient Descent.m¹,b¹ = next position parameters; m⁰,b⁰ = current position parametersHence,to solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.The point of this article was to demonstrate the concept of gradient descent. We used gradient descent as our optimization strategy for linear regression. by drawing the line of best fit to measure the relationship between student heights and weights. However, it is important to note here that the linear regression example has been chosen for simplicity but can be used with other Machine Learning techniques.",18/03/2019,0,51.0,9.0,522.0,323.0,24.0,5.0,0.0,9.0,en
4142,Logistic Regression from scratch in Python,Medium,Martín Pellarolo,366.0,4.0,464.0,"While Python’s scikit-learn library provides the easy-to-use and efficient LogisticRegression class, the objective of this post is to create an own implementation using NumPy. Implementing basic models is a great idea to improve your comprehension about how they work.We will use the well known Iris data set. It contains 3 classes of 50 instances each, where each class refers to a type of iris plant. To simplify things, we take just the first two feature columns. Also, the two non-linearly separable classes are labeled with the same category, ending up with a binary classification problem.Given a set of inputs X, we want to assign them to one of two possible categories (0 or 1). Logistic regression models the probability that each input belongs to a particular category.A function takes inputs and returns outputs. To generate probabilities, logistic regression uses a function that gives outputs between 0 and 1 for all values of X. There are many functions that meet this description, but the used in this case is the logistic function. From here we will refer to it as sigmoid.Functions have parameters/weights (represented by theta in our notation) and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function, defined as:Our goal is to minimize the loss function and the way we have to achive it is by increasing/decreasing the weights, i.e. fitting them. The question is, how do we know what parameters should be biggers and what parameters should be smallers? The answer is given by the derivative of the loss function with respect to each weight. It tells us how loss would change if we modified the parameters.Then we update the weights by substracting to them the derivative times the learning rate.We should repeat this steps several times until we reach the optimal solution.By calling the sigmoid function we get the probability that some input x belongs to class 1. Let’s take all probabilities ≥ 0.5 = class 1 and all probabilities < 0 = class 0. This threshold should be defined depending on the business problem we were working.Picking a learning rate = 0.1 and number of iterations = 300000 the algorithm classified all instances successfully. 13.8 seconds were needed. These are the resulting weights:LogisticRegression from sklearn:If we trained our implementation with smaller learning rate and more iterations we would find approximately equal weights. But the more remarkably difference is about training time, sklearn is order of magnitude faster. Anyway, is not the intention to put this code on production, this is just a toy exercice with teaching objectives.Further steps could be the addition of l2 regularization and multiclass classification.Code available here.",23/02/2018,10,0.0,2.0,690.0,339.0,6.0,0.0,0.0,2.0,en
4143,Beginner Guide to Variational Autoencoders (VAE) with PyTorch Lightning (Part 3),Towards Data Science,Reo Neo,143.0,8.0,1561.0,"This blog post is part of a mini-series that talks about the different aspects of building a PyTorch Deep Learning project using Variational Autoencoders.Part 1: Mathematical Foundations and ImplementationPart 2: Supercharge with PyTorch LightningPart 3: Convolutional VAE, Inheritance and Unit TestingPart 4: Streamlit Web App and DeploymentIn this section, we will look at how we can use the code we wrote in the previous section and use it to build a convolutional VAE. This VAE would be better at identifying important features in the images and thus generate even better images.The best part is that this new model can be built with minimal additional code thanks to PyTorch modules and class inheritance.Convolution is an operation commonly used in image processing to extract features of a particular image. Images are usually full of unnecessary information and zooming into any pixel, the surrounding pixels likely have a very similar colour. In Convolutional Neural Networks (CNNs), many convolution filters are automatically learnt to obtain features that are useful at classifying and identifying images. We simple borrow these principles to use Convolutional Layers to build the VAE.By building the convolutional VAE, we aim to get a better feature extraction process. Even though we are not performing any classification/regression task, we want the latent representation to be as information-rich as possible. With more powerful feature extraction, the decoder can then generate more convincing data points.Even though this new model uses a new architecture we want to write code efficiently. Good and efficient code uses the DRY (Don’t Repeat Yourself) principle. To avoid repeating code unnecessarily we will use Inheritance a powerful concept to build our model.Inheritance is a very powerful concept that is present in Object-Oriented Programming (OOP) languages. It allows users to define objects and then build new objects while retaining some of the functionality of the original object. Inheritance is a pretty extensive topic and there are things like multiple inheritance which I will not go into detail about. check out this for more information about OOP programming in Python and inheritance.realpython.comActually, inheritance is so common that we have already used inheritance in Part 1. Even without knowing it, inheritance is used extensively in PyTorch where every neural network inherits from the base class nn.Module.Because of this we only have to define the __init__ and forward methods and the base class will do the rest. The model that we are about to build will take this step further and build upon the VAE that was built in the previous section.Inheritance allows us to build complicated models in different stages. The previous VAE model we built acts as a skeleton. It performs the reparameterization and implements the KL-Divergence loss.We can then inherit this class and create a better model that uses an architecture that is more suited to the task.This model can then be adapted by changing the encoder and decoder. The encoder simply does representation learning and the decoder does generation. These subnetworks can be simple linear layers or complicated networks.In our convolutional VAE, we want to change these components while keeping everything else identical. This can be easily done using inheritanceThis allows us to avoid repeating a lot of the code. Class methods such as forward, training_step, train_loader will be kept exactly the same and inheritance allows us to copy that over automatically.If you notice carefully, in the previous model. The forward step included the flattening of the vector before feeding it into the encoder. For a Convolution VAE, we don’t want to do this flattening as it prevents us from 2D Convolutions.It seems like to make inheritance work we need to do some code refactoring!Essentially, code refactoring is making some changes to the code while maintaining the same outward functionality. This means that the code will still have the same behaviour in terms of the inputs and outputs. Refactoring can be done to make the code faster or in our case streamline the code so that we can re-use it somewhere else.Instead of rewriting the whole forward step, we can refactor our code such that the flattening of the input tensor and the reshaping of it back to 28 x 28 occurs inside self.encoder and self.decoder instead of being inside the forward function.This allows the model to be more versatile as it can accommodate for different encoders such as convolution where we do not want to flatten the input vector.But hold on! Let’s not do the refactoring just yet. The worst thing you want to happen is for your model to break when you make code changes. We want to ensure that the VAE model still does the exact same thing after refactoring.One good way to ensure that your project is still functional after the changes are to write unit tests.Unit tests are simple scripts that you can run to ensure that your code is working correctly. In the context of our model, it's to ensure that the model we built is still able to train and the gradients are still able to backpropagate well.For this, we will use pytest which is a powerful library for writing unit tests that also contains useful debugging tools to find out why tests fail.First, we create a folder in our directory called tests. Inside this folder, we create a file named test_model.py. This will store all the unit tests required.Let’s define a simple test:Another cool feature of pytest is that it will automatically search for test functions in the package. As long as the function name starts with test, pytest will run the test accordingly.By running pytest in the command line, we can confirm that the test passes.Now with proper testing systems in place, we can start to make our code changes.One important thing to understand about PyTorch modules is that they are basically functions. When an input is passed into any PyTorch module, it simply runs some operations and backpropagates the gradients back.This means that even simple reshape operations can be initialized as a PyTorch object.We simply take the first line of the old forward function and add that as a module. This way, placing Flatten() as a module in the encoder accomplishes the same thing.Now let's write the code for the Stack Module. For the MNIST dataset, this module reshapes the tensor back into its original shape which is (1,28,28)In our case, because the images are black and white there is only one channel but let's build a stack module that can work with coloured images too.To do this, we have to store information about the dataset into the model itself. This is done by passing the original shape of data as parameters into the module. These parameters are the channels, height and width. The forward operation will then be a view operation similar to the Flatten Module.To store these parameters, we need to use the __init__ function. This allows us to store these parameters as class variables. To do this, we first initialize it as a PyTorch module and this is done by calling super(self,Stack).__init__() in the __init__ function.Now that we have abstracted these reshaping functions into their own objects, we can use nn.Sequential to define these operations as part of the encoder and decoder modules.And just like that reshaping operations are part of self.encoder and self.decoderLet's run our unit tests to check that the code works.Nice! The test passes and the code runs as expected.docs.pytest.orgBuilding the Convolutional EncoderWith these changes, we can start to build the Conv_VAE. Let’s start with the encoder.The first part of the encoder is sequential steps of Conv2d layers together with ReLU activations and BatchNorm2d to help speed up training. This step performs the feature extraction while reducing the dimension.The next part is the flatten step to convert the vector back to a single dimension. The latent representation in the VAE is a single vector we need to get the input to the same shape. This can be done using the Flatten() module that we defined earlier. Just import it from the VAE file and we can use it in the encoder.Moving on to the decoderThe architecture for the decoder is really similar. It's basically the same thing but in the opposite direction.1. Feed Forward Layer (nn.Linear)2. Stack module to convert the linear layer into 2d shapes with channels3. ConvTranspose2d layers that upsample the images and generate images that have larger heights and widths. (Opposite of Conv2d)4. Conv2d layer to clean up the final outputThe final layer should output something which has the same dimension as the original shape and the MSE loss can be applied easily.And believe it or not, we are done here! Python classes will inherit all methods by default so all other functions outside of __init__ do not have to be defined again. This means everything from training, validation and even save_images will be automatically present for use in the new Conv VAE.The images produced by the Convolutional VAE seem to be more defined and there is more variability in the images. Despite using the same latent space dimension, the new model is able to capture and recreate images that show more variation. This shows the power of the improved encoder-decoder network and this difference will be even more prominent when applied to coloured images.In the next (and final) section, I will be looking at the steps needed to fully deploy the model onto Heroku and create a playground to interact with them!",10/06/2021,0,8.0,0.0,918.0,569.0,6.0,0.0,0.0,11.0,en
4144,60 Python Projects with Source Code,Coders Camp,Aman Kharwal,8500.0,2.0,181.0,"Python has been in the top 10 popular programming languages for a long time, as the community of Python programmers has grown a lot due to its easy syntax and library support. In this article, I will introduce you to 60 amazing Python projects with source code solved and explained for free.If you’re a newbie to Python where you’ve just learned lists, tuples, dictionaries, and some basic Python modules like the random module, here are some Python projects with source code for beginners for you:If you have learned the fundamental Python libraries and some of the external libraries, you should now know how to install external libraries and work with them. So if you are at that level now, you can work on all the advanced Python projects with source code mentioned below:So these were some very useful Python projects with source code for both a beginner and someone in advance level of Python. I hope you liked this article on Python Projects with source code solved and explained. Feel free to ask your valuable questions in the comments section below.",14/01/2021,0,60.0,0.0,1280.0,720.0,1.0,2.0,0.0,61.0,en
4145,"The Softmax Function, Neural Net Outputs as Probabilities, and Ensemble Classifiers",Towards Data Science,Haihan Lan,358.0,7.0,1337.0,"In this article, we’ll look at:Links to my other articles:In many cases when using neural network models such as regular deep feedforward nets and convolutional nets for classification tasks over some set of class labels, one wonders whether it is possible to interpret the output, for example y = [0.02, 0, 0.005, 0.975], as the probability of some input being in a class equal to the respective component values yᵢ in the output vector. Skipping straight to the long answer: no, unless you have a softmax layer as your output layer and train the net with the cross-entropy loss function. This point is important because it is sometimes omitted in online sources and even in some textbooks regarding classification with neural networks. We’ll take a look at how the softmax function is derived in the context of multinomial logistic regression and how to apply it to ensemble deep neural network models for robust classification.Briefly, the Categorical distribution is the multi-class generalization of the Bernoulli distribution. The Bernoulli distribution is a discrete probability distribution that models the outcome of a single experiment, or single observation of a random variable with two outcomes (e.g. the outcome of a single coin flip). The categorical distribution naturally extends the Bernoulli distribution to experiments with more than two outcomes.Now, simple logistic regression classification (i.e. logistic regression on only two classes or outcomes) assumes that the output Yᵢ (i being the data sample index) conditioned on inputs xᵢ is Bernoulli distributed:The link function relating the log odds of the Bernoulli outcomes to the linear predictor is the logit function:If we exponentiate both sides of the equation above and do a little rearranging, on the right-hand-side (RHS) we get the familiar logistic function:One way to approach deriving the generalized logistic or softmax function for multinomial logistic regression is to start by having one logit linked linear predictor for each class K, plus some normalization factor to ensure that the total sum of the probabilities over all classes equals to one. This resulting system of K equations is a system of log-linear probabilistic models:The ln(Z) term in the above system of equations is the (log of the) normalization factor, and Z is known as the partition function. As we are dealing with multinomial regression, this system of equations gives probabilities which are categorically distributed: Yᵢ | xᵢ ~ Categorical(pᵢ).Exponentiating both sides and imposing the constraint:givesThe RHS of the equation above is called the Gibbs measure and connects the softmax function to statistical mechanics. Next, solving for Z gives:And finally the system of equations becomes:The ratio on the RHS of each equation is the softmax function. In general, the softmax function is defined as:for j = 1 … K. We can see that the softmax function normalizes a K dimensional vector z of arbitrary real values into a K dimensional vector σ(z) whose components sum to 1 (in other words, a probability vector), and it also provides a weighted average of each zⱼ relative to the aggregate of zⱼ’s in a way that exaggerates differences (returns a value close to 0 or 1) if the zⱼ’s are very different from each other in terms of scale, but returns a moderate value if zⱼ’s are relatively the same scale. It is desirable for a classifier model to learn parameters which give the former condition rather than the latter (i.e decisive vs indecisive).Finally, just as the logit function is the link function for simple logistic regression and the logistic function is the inverse of the logit function, the multinomial logit function is the link function for multinomial logistic regression and the softmax can be thought of as the inverse of the multinomial logit function. Typically in multinomial logistic regression, maximum a-posterior (MAP) estimation is used to find the parameters β for each class k.Now that we have seen where the softmax function comes from, it’s time for us to use them in our neural net classifier models. The loss function to be minimized on softmax output layer equipped neural nets is the cross-entropy loss:where y is the true label for some iteration i and ŷ is the neural network output at iteration i. This loss function is in fact the same one used for simple and multinomial logistic regression. The general definition of the cross-entropy function is:The cross-entropy between p and q is defined as the sum of the information entropy of distribution p, where p is some underlying true distribution (in this case would be the categorical distribution of true class labels) and the Kullback–Leibler divergence of the distribution q which is our attempt at approximating p and p itself. Optimizing over this function minimizes the information entropy of p (giving more certain outcomes in p) while at the same time minimizes the ‘distance’ between p and q.A theoretical treatment of using the softmax in neural nets as the output layer activation is given in Bridle’s article. The gist of the article is that using the softmax output layer with the neural network hidden layer output as each zⱼ, trained with the cross-entropy loss gives the posterior distribution (the categorical distribution) over the class labels. In general deep neural nets can vastly outperform simple and multinomial logistic regression at the expense of not being able to provide statistical significance of the features/parameters, which is a very important aspect of inference or finding out which features affect the outcome of the classification. The complete neural network is optimized using a robust optimizer of choice; RMSprop is usually a good start.So now we’ll whip up a deep feedforward neural net classifier using the Keras functional API and do some wine classification. We’ll use an ensemble model of several neural nets to give us a robust classification (in practice this is what you should do, the variances in predictions of individual neural nets due to random initialization and stochastic gradient training must be averaged out for good results). The output of the ensemble model should give a vector of probabilities that some test example will belong to each class, i.e. a categorical distribution over the class labels.One way to aggregate the results of each individual neural net model is to use a softmax at the ensemble output to give a final probability. In order to automatically determine the optimal weighting of the final softmax averaging, we’ll tack on another layer ‘gluing together’ the outputs of each individual neural net in the ensemble. A diagram of the architecture is below.Each learning model will be differentiable from the final softmax aggregate output backwards. We can merge each of the sub-networks together using the Keras concatenate-merge layer. The concatenate layer concatenates the output tensors from each sub-network and allows the optimizer to optimize over the merged model. To simplify our training, each learning model will be trained on the same dataset. Bootstrapped sub-sets can be used but this makes it more complicated to train, as we would have to train each sub-network individually on its own input and target pair while freezing training updates on the rest of the learning models.It can be seen from the results of training that the fancy wines are no match for our ensemble classifier. As well, feature selection is not terribly important for our model, as it learns the dataset quite well using all features. The training and validation losses become small to the order of 10^-5 and 10^-3 respectively after 200 epochs, and this indicates our ensemble neural net model is doing a good job of fitting the data and predicting on the test set. The output probabilities are nearly 100% for the correct class and 0% for the others.In this article, we derived the softmax activation for multinomial logistic regression and saw how to apply it to neural network classifiers. It is important to remember to be careful when interpreting neural network outputs are probabilities. We then built an ensemble neural net classifier using the Keras functional API. Anyways, have fun running the code and as always please don’t hesitate to ask me about anything.",13/11/2017,0,17.0,50.0,387.0,176.0,13.0,2.0,0.0,17.0,en
4146,A beginner’s guide to deriving and implementing backpropagation,binaryandmore,Pranav Budhwant,46.0,10.0,1782.0,"This article is divided into two sections:1. Derivation — In this section, we will be deriving all the required formulae for performing backpropagation. I strongly recommend that you derive the equations on paper as you read through the article.2. Implementation — In this part, we will use the derived formulae to implement backpropagation from scratch. We will be solving a binary classification problem in python using numpy.DisclaimerThis article assumes a basic understanding of neural networks and how they work. If you are not familiar with neural networks, or think your concepts are a little rusty, you may want to review chapter 1 of the amazing book, Neural Networks and Deep Learning, by Michael Nielsen, or if you prefer video lectures, you might want to go through this course on Coursera.Consider the following network architecture:NotationsFor every layer in the network, there are certain parameters associated with it. Let us define some notations.Now that we have the notations in place, let’s dive into the math.We know that training a network requires three steps.Forward PropagationLet X be the input vector to the neural network, i.e. a[0] = X.Now, we need to calculate a[l] for every layer l in the network.Before calculating the activation, a[l], we will calculate an intermediate value z[l]. Each element k in z[l] is just the sum of bias for the neuron k in the layer l with the weighted sum of the activation of the previous layer, l-1.We can calculate z[l] from the following equation:Where ‘.’ refers to the matrix multiplication operation, and + refers to the matrix addition. Now that we have z[l], we can compute a[l] easily by applying the activation function g[l] element-wise to the vector z[l].With the above two equations, we can calculate the activation of each layer in the network. The activation of the last layer gives us the predicted output of the network, y’.Selecting the Cost Function C, and computing the costOkay, now we know how we will forward propagate the input X through the network and obtain the predictions. But now that we have the predictions, we need a way to quantitatively determine how good/bad these predictions are.So, how do we go about this?To do this, we define some function, known as the cost function, which will be a function of the predicted values and the ground truth.This function will estimate how badly the model is performing. Put simply, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between the input and the output. This cost function (also referred to as loss or error) can be estimated by iteratively running the model to compare estimated predictions, y’ against “ground truth” — the known values of y.The objective of a the model, therefore, is to find parameters, weights and biases, that minimize the cost function.That is why choosing the right cost function for achieving the desired result is a critical point of machine learning problems. The choice of the cost function depends on the application. Here is a good discussion about choosing the right cost function.In this article we will be using the cross-entropy cost function, which is calculated as follows:Here y is the actual output, the ground truth, and y’ is the predicted output, or, a[3] in this case. *Note: Here log refers to the natural logarithm.BackpropagationThe goal of backpropagation is to compute the partial derivatives of the cost function C with respect to any weight w or bias b in the network. Once we have these partial derivatives, we will update the weights and biases in the network by the product of some constant alpha and the partial derivative of that quantity with respect to the cost function. This is the popular gradient descent algorithm. The partial derivatives give us the direction of greatest ascent. So, we take a small step in the opposite direction — the direction of the greatest descent, i.e. the direction which will take us to the local minima of the cost function.The update rules looks like this:Here alpha is known as the learning rate of the network, because it decides how big updates we perform, in other words, it tells us how big steps are we taking in the direction of the local minima, i.e. what is the rate at which we are learning.To better understand the flow of computations, let us draw a computation graph.From the above computation graph, we can make a few observations which will help us calculate the partial derivatives. C is a function of a[3], a[3] is in turn a function of z[3], and z[3] is a function of w[3], b[3] and a[2]. So to calculate the partial derivative of the cost function C with respect to w[3], b[3], we will have to use the chain rule.This gives us:Similarly, we need the derivative of C with respect to w[2], b[2]. From the above computation graph, we can see that z[3] is a function of a[2], which(a[2]) is a function of z[2], and in turn z[2] is a function of w[2], b[2] and a[1]. So applying the chain rule again, we get:Likewise, for calculating the partial derivatives with respect to w[1], b[1], we would use:This can be better understood from the following figure:This gives us a better picture of how backpropagation actually works.So now that we have the understanding of what backpropagation is actually doing, let us generalize the above equations for the partial derivatives, which would make it easier for us to convert them into code.We have:So for calculating the partial derivatives of C with respect to w[l], b[l], we need to calculatewhere L is the last layer.*We need to calculate dC/dz[L] separately, and then dC/dz[l] will be a general formula to calculate the partial in terms of the partial derivative of the l + 1 layer, i.e. dC/dz[l+1].For our convenience we will assume the activation function for each layer g[l] to be the sigmoid activation function. As the activation function remains the same for all the layers, we will drop the superscript [l].The sigmoid function is defined as:Partial derivative of C with respect to z[L]The partial derivative of C with respect to z[L] can be calculated as follows:Where .* represents element-wise multiplication of the matrices, also known as the Hadamard product. We multiply element-wise to make sure that all the dimensions of our matrix multiplications match up as expected.Derivative of the activation function:Putting it all togetherPartial derivative of C with respect to z[l]We want the partial derivative of C with respect to z[l] in terms of the partial derivative of C with respect to the layer l+1, so that once we have z[L], we can calculate z[L-1], z[L-2].. and so on.We can express C as a function of z[l + 1] for any given l. Therefore, we can write:As we will first be calculating the dC/dz[L] term, we can then calculate dC/dz[l] for every l = L-1, L-2 .. and so on by substituting the values in the above equation. We still have to calculate dz[l+1]/da[l] and da[l]/dz[l]. Let us see how we can do that.Putting it all together we get:Note that we have adjusted the terms to make sure our matrix multiplication dimensions match as expected. Here ‘.’ represents the matrix multiplication operation and .* represents the element-wise product as above.Partial derivative of z[l] with respect to w[l]Now let us calculate dz[l]/dw[l]Partial derivative of C with respect to w[l]Using the derived equations,Partial derivative of z[l] with respect to b[l]Partial derivative of C with respect to b[l]Using the derived equations,Note that these derivations consider input X as only one training example, for a batch of m examples, we have to average the derivatives over the m examples.That completes the derivation of the fundamental equations of backpropagation. It’s really just the outcome of carefully applying the chain rule. A little less succinctly, we can think of backpropagation as a way of computing the gradient of the cost function by systematically applying the chain rule from multi-variable calculus. That’s all there really is to backpropagation — the rest is details.While deriving the above equations we have considered the activation function g[l] for all the layers to be the sigmoid activtion function. But that is hardly the case when we implement neural networks for solving real problems.Here is a list of activation functions along with their equations and the derivatives.Let us now look at the implementation of the backpropagation algorithm. You need to have a little experience with numpy to be able to understand the code.We will be implementing a very basic seed classification problem. The original dataset can be downloaded from the UCI Machine Learning Repository. In this implementation, we will be using only a subset of the original dataset, as we will be implementing a binary classification problem and the original dataset contains 3 classes. The modified dataset used in this code, along with the code can be downloaded from this GitHub repository.Let us start by importing the required libraries:We have assumed the activation function to be the sigmoid activation function, so as per the formulae derived above, we will need to implement the sigmoid function as well as sigmoid_prime, the derivative of the sigmoid function.Now, we will define a class NeuralNetwork which will have the implementation of all the required functions for creating and training the network.Let us understand the use of the functions defined above.1. __init__ : This is the constructor of the class, which is responsible for creating and initializing several class variables which will define the neural network, such as the network architecture, various weight matrices, bias vectors, activation vectors etc. 2. forward_propagate : This function performs the forward propagation step. It accepts the input for the network and calculates the activation layer by layer.3. compute_cost : This function calculates the cross entropy cost for the given y and the current activation of the final layer, a[L].Now, the backpropagation is performed by the combination of two functions, compute_derivatives and update_parameters.4. compute_derivatives : This function is responsible for calculating all the partial derivatives with respect to the cost function.5. update_parameters : This function performs the updates for the weights and the biases of the network. 6. predict : This function just calculates the predictions for the given input.7. fit : This function contains the training part of the network. It implements the stochastic gradient descent algorithm.Finally, let us import the dataset, perform a little preprocessing and train our model.Putting it all together:The content in this article is free to use as long as the article is cited.neuralnetworksanddeeplearning.comIf you learned something new from this article, please hit the 👏 icon to support it. This will help other Medium users find it. Share it, so that others can read it.",16/07/2018,0,25.0,102.0,995.0,601.0,30.0,1.0,0.0,19.0,en
4147,mAP (mean Average Precision) might confuse you!,Towards Data Science,Shivy Yohanandan,37.0,5.0,757.0,"One can be forgiven for taking mAP (mean average precision) to literally mean the average of precisions. Nevertheless, you couldn’t be further from the truth!Let me explain.In computer vision, mAP is a popular evaluation metric used for object detection (i.e. localisation and classification). Localization determines the location of an instance (e.g. bounding box coordinates) and classification tells you what it is (e.g. a dog or cat).Many object detection algorithms, such as Faster R-CNN, MobileNet SSD, and YOLO, use mAP to evaluate their models for publishing their research.You might ask, if it’s such a popular metric, why is it still confusing?Fair enough!mAP stands for Mean Average Precision (as you might already have guessed looking at the title).You might think it is the average of the Precision.If you do not know already:Precision measures how accurate your predictions are. i.e. the percentage of your predictions are correct.It measures how many of the predictions that your model made were actually correct.TP = True Positives (Predicted as positive as was correct)FP = False Positives (Predicted as positive but was incorrect)If that was the case, let’s calculate mAP for the following image:From the image, we get:True Positives (TP) = 1Fasle Positives (FP) = 0Because we only have one value the average of precision will be 1.Looking at the mAP score, you might end-up using this model in your application. That would be a disaster.AND THAT’S THE CATCH! DON’T LET THE TERM MISLEAD YOU.mAP is not calculated by taking the average of precision values.Object detection systems make predictions in terms of a bounding box and a class label.For each bounding box, we measure an overlap between the predicted bounding box and the ground truth bounding box. This is measured by IoU (intersection over union).For object detection tasks, we calculate Precision and Recall using IoU value for a given IoU threshold.For example, if IoU threshold is 0.5, and the IoU value for a prediction is 0.7, then we classify the prediction as True Positive (TF). On the other hand, if IoU is 0.3, we classify it as False Positive (FP).That also means that for a prediction, we may get different binary TRUE or FALSE positives, by changing the IoU threshold.Another important term to understand is Recall.Recall measures how well you find all the positives. For example, we can find 80% of the possible positive cases in our top K predictions.TP = True Positives (Predicted as positive as was correct)FN = False Negatives (Failed to predict an object that was there)The general definition for the Average Precision (AP) is finding the area under the precision-recall curve above.mAP (mean average precision) is the average of AP.In some contexts, AP is calculated for each class and averaged to get the mAP. But in others, they mean the same thing. For example, for COCO challenge evaluation, there is no difference between AP and mAP.AP is averaged over all categories. Traditionally, this is called “mean average precision” (mAP). We make no distinction between AP and mAP (and likewise AR and mAR) and assume the difference is clear from context. COCO EvaluationThe mean Average Precision or mAP score is calculated by taking the mean AP over all classes and/or overall IoU thresholds, depending on different detection challenges that exist.In PASCAL VOC2007 challenge, AP for one object class is calculated for an IoU threshold of 0.5. So the mAP is averaged over all object classes.For the COCO 2017 challenge, the mAP is averaged over all object categories and 10 IoU thresholds.So for the above ADAS image, let’s calculate the mAP using the actual formula:Here we assume that the confidence score threshold is 0.5 and the IoU threshold is also 0.5.So we calculate the AP at IoU threshold o.5.For simplicity, we will calculate an average for the 11-point interpolated AP. In the latest research, more advanced techniques have been introduced to calculate the AP.True Positives (TP) = 1Fasle Positives (FP) = 0False Negatives (FN) = 1We plot the 11 points interpolated Precision-Recall curve.We now calculate AP by taking the area under the PR curve. This is done by segmenting the recalls evenly to 11 parts: {0,0.1,0.2,…,0.9,1}.So mAP@0.5 for the image is 0.545, not 1.Hope this clarifies your misunderstanding on mAP.Originally published in www.xailient.com/blogsWant to train and evaluate a computer vision model? Click here.Looking for a pre-trained face detection model. Click here to download.Check-out this post for more details on creating a robust object detection model.References:Van Etten, A. (2019, January). Satellite imagery multiscale rapid detection with windowed networks. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 735–743). IEEE.cocodataset.orghttp://host.robots.ox.ac.uk/pascal/VOC/medium.comtowardsdatascience.comAuthorsShivy YohanandanSabina Pokhrel",09/06/2020,0,6.0,15.0,1317.0,576.0,15.0,0.0,0.0,14.0,da
4148,How to recognize fake AI-generated images,Medium,Kyle McDonald,7300.0,7.0,1070.0,"In 2014 machine learning researcher Ian Goodfellow introduced the idea of generative adversarial networks or GANs. “Generative” because they output things like images rather than predictions about input (like “hotdog or not”); “adversarial networks” because they use two neural networks competing with each other in a “cat-and-mouse game”, like a cashier and a counterfeiter: one trying to fool the other into thinking it can generate real examples, the other trying to distinguish real from fake.The first GAN images were easy for humans to identify. Consider these faces from 2014.But the latest examples of GAN-generated faces, published in October 2017, are more difficult to identify.Here are some things you can look for when trying to recognize an image produced by a GAN. We’ll focus on faces because they are a common testing ground for researchers, and many of the artifacts most visible in faces also appear in other kinds of images.It’s common for long hair to take this hyper-straight look where a small patch seems good, but a long strand looks like someone smudged a bunch of acrylic with a palette knife or a huge brush.GANs trained on faces have a hard time capturing rare things in the background with lots of structure. Also, GANs are shown both original and mirrored versions of the training data, which means they have trouble modeling writing because it typically only appears in one orientation.One reason the faces from a GAN look believable is because all the training data has been centered. This means that there is less variability for the GAN to model when it comes to, for example, the placement and rendering of eyes and ears. The background, on the other hand, can contain anything. This is too much for the GAN to model and it ends up replicating general background-like-textures rather than “real” background scenes.It can be difficult for a GAN to manage long-distance dependencies in images. While paired accessories like earrings usually match in the dataset, they don’t in the generated images. Or: eyes tend to point in the same direction and they are usually the same color, but the generated images are very frequently crosseyed and heterochromatic. Asymmetry is also commonly visible in ears being at very mismatched heights or sizes.GANs can assemble a general scene, but currently have difficulty with semi-regular repeating details like teeth. Sometimes a GAN will generate misaligned teeth, or it will stretch or shrink each tooth in unusual ways. Historically this problem has shown up in other domains like texture synthesis with images like bricks.This is one of the quickest ways to identify a GAN-generated image. Typically a GAN will bunch hair in clumps, create random wisps around the shoulders, and throw thick stray hairs on foreheads. Hair styles have a lot of variability, but also a lot of detail, making it one of the most difficult things for a GAN to capture. Things that aren’t hair can sometimes turn into hair-like textures, too.This GAN was trained on a subset of CelebA, which contains 200k images of 10k celebrity faces. In this dataset, I haven’t seen an example of someone with facial hair, earrings, and makeup; but the GAN regularly mixes different attributes from stereotypical gender presentations. More generally, I think this is because GANs don’t always learn the same categories or binaries that humans socially reinforce (in this case “male vs female”). It’s important to be clear here: like asymmetry, non-stereotypical gender presentation is not inherently an indicator that an image isn’t “real”. Unlike messy hair, it is less of a visual artifact present in individual images, and more of a disparity in matching statistics across a large collection of images.Some areas that are otherwise monochrome may exhibit semi-regular noise with horizontal or vertical banding. In the cases above, this is probably the network trying to imitate the texture of cloth. Older GANs have a much more prominent noise pattern that is usually described as checkerboard artifacts.Some areas with lighter solid colors have a multi-hued cast, including collars, necks, and eye whites (not shown).Check out that clear background text, those matching earrings, those equally sized teeth, detailed hairstyles. With all these tricks in mind, try playing this game that tests your ability to distinguish real from fake and see how many you get right. Note: some people have had problems clicking “start”.Update (December 13, 2018)One year after “Progressive Growing of GANs” which produce the above images, the same researchers have published “A Style-Based Generator Architecture for GANs”. Check out the video. This new work improves on many of the issues above.At low resolutions, almost all the images in the paper are indistinguishable from photographs. There are only a few artifacts that stand out to me that I will try to address.This glitch shows up in a few images in the exact same spot. This might have to do with the neural net trying to generate earrings and failing, because they all come from the same “source” image and in one case when mixed with a “middle style” showing a feminine face an earring appears in this spot. It could also be unrelated, because another example shows a similar glitch across multiple images in a totally different location.In the center is the “average face” from the dataset, based on 70k photos from Flickr users all over the world. There appears to be an earring in the right ear (left side of image), but not in the left ear. This is not a judgement about whether having an earring in one ear is “right” or “wrong”, but about whether this kind of asymmetry is equally common in the dataset. The mismatched ear sizes in the right image is another example of asymmetry. Another example of overly frequent asymmetry might be this face that appears to have some strabismus: one eye seems to point in a different direction than the other.They’re still there, but you might have to look a little closer. In this example one tooth has a space in the middle. In other images they show all the teeth sliding to one side.Also still there, but usually blending in a little better.This one image has an unusual watercolor aesthetic. It’s not clear why this might appear. In their previous work, they used a super-resolution network to preprocess the training images. If they used the same system here. In the other “coarse styles copied” image, this region appears as some variant of a brightly colored shirt.",06/12/2018,0,2.0,3.0,1075.0,692.0,19.0,0.0,0.0,13.0,en
4149,A brief introduction to artificial neural networks,Medium,Peter Bulyaki,21.0,5.0,913.0,"This post will try to give you a brief introduction to artificial neural networks or at least to some types of them. I will skip the introduction to biological neural networks as I am neither a biologist nor a doctor, I prefer not to write about what I do not fully understand.Overview of artificial neural networks and supervised learningI think it is very important to note that artificial neural networks are neither magical AI circuits nor oracles with the ability of predicting stock market movements. You can save one as a file on the disk and you can name it skynet if you like but it will not get more intelligent from that. In reality they are simply mathematical tools that can come very handy in solving certain problems (and can prove to be completely useless for others).It is best to imagine a neural network as a black box. This black box has inputs and outputs. In an uninitialized state this box will have a completely undefined behaviour: it will give arbitrary answers to any inputs (although it will always give the same answers to the same inputs). Imagine this inital state of the box as one with the highest disorder (or entropy), no effort has yet been made to make it do what we expect it to do. This state can be analogous with the brain of a newborn baby — he or she is just about to start experiencing the whole world.There is a process called supervised learning during which the box is fed with input-output patterns and its internal entropy is reduced by making it learn the relationships between these inputs and outputs. There is another word for this process in mathematics: regression analysis, and neural networks are not trying to be anything more than that either. The only thing that makes artificial neural networks unique is that the applied mathematical function used for modelling is based on how we think the brain works.The learning process starts by feeding our inputs to the uninitialized box and comparing the desired outputs with the actual ones. At first we will most likely receive a big difference as the box have no idea about the desired IO relationships. This difference is called the error of the neural network. During the training process our aim is to minimize this error as much as possible by iteratively feeding the same IO patterns to it again and again, and making such adjustments to the internal state of the box so that its overall error (or entropy) will decrease.Inside the box — the neuronThe atomic mathematical building blocks of neural networks are called neurons. They are similar to biological neurons in that they give outputs to inputs and they have persisting properties (weights), which means that they are able to adjust and store internal states that influence their outputs. (I personally prefer the analogy with transistors more than with braincells. Human braincells are far more complex than the mathematical neuron, they are almost little computers themselves.)The inputs arriving to a neuron are all normalized by being multiplied by a weight value. This weight can be imagined as priority or importance levels added to each input. Then the weighted input values are summarized. After this an activation function f is applied to the weighted sum.Activation functionsIn order to train the neural network (and its neurons of course) we need to determine the difference between the actual output values of the neuron and the desired output values. This difference also forms a function that is called error function. The error function is a surface in n dimensional space and it has a global minimum point. The training process aims to find this minimum point so first it needs to find the steepness of the surface. The steepness is the best direction (or at least the best guess) towards the global minumum.This error surface will depend on the linear combination of the activation functions. The gradient of this surface can only be determined at any point in an interval if the activation function itself is continuous and differentiable at least once in that interval. If we want to model the behavior of the biological neuron we could use the step or signum activation functions. The behaviour we get from these functions is that the neuron will only output values (or in other words it will only fire) if the summarized inputs reach a certain value.The step activation functionThe signum activation functionAlthough the above activation functions are continuous unfortunately their linear combination is not. By reducing the 90 degree steepness of this function it can be made continuous though. A very popular activation function does just this: the sigmoid or logistic distribution function. This is a real function on the (0,1) interval:The sigmoid activation function (shapes are for c=1, c=2 and c=3 values)The sigmoid function uses a c constant that determines the steepness of the activation function. This c value is chosen when preconditioning the network and it is not updated during the training process. It is a global property for the whole neural network (all neurons use the same steepness). By increasing the c value the sigmoid function converges to the step function.We need the derivative of the sigmoid function to determine the gradient of the error surface:An alternative to the sigmoid function is the symmetrical sigmoid, which is the hyperbolic tangent of the x/2 value.The symmetrical sigmoid activation function and its derivative.Originally published at bulyaki.com on November 2, 2012.",02/11/2012,0,11.0,13.0,317.0,155.0,12.0,0.0,0.0,1.0,en
4150,BigGAN: A New State of the Art in Image Synthesis,SyncedReview,Synced,24000.0,3.0,55.0,"“Best GAN samples ever yet? Very impressive ICLR submission! BigGAN improves Inception Scores by >100.”The above Tweet is from renowned Google DeepMind research scientist Oriol Vinyals. It was retweeted last week by Google Brain researcher and “Father of Generative Adversarial Networks” Ian Goodfellow, and picked up momentum and praise from AI researchers on social media.",02/10/2018,0,0.0,1.0,1234.0,621.0,1.0,0.0,0.0,0.0,en
4151,A 2022-Ready Deep Learning Hardware Guide,Towards Data Science,Nir Ben-Zvi,87.0,22.0,4760.0,"This is as up to date as: 3/1/2022This is a vastly revised version of the older version you all know and love.Almost every part of this guide has been thoroughly rewritten. The original guide has been getting updated over the course of 6 years, so I decided it’s time to basically (almost) write it from scratch.This time I tried to make this a bit more thorough and general. I’ll keep updating this, but I also want to make sure my readers can understand the topic even if I stop doing so one day.So, you’ve decided you want to purchase a machine dedicated to training machine learning models. Or, rather, you work in an organization where the buzzwords of this guide are constantly thrown around and you simply want to know a bit more about what they mean. This isn’t a terribly simple topic, so I’ve decided to write this guide. You can discuss those terms from various angles, and this guide will tackle one of them.A note on the various tables in this article; I made them, and getting all the required information took some time. Please don’t use them without my consent.I’m Nir Ben-Zvi, a Deep Learning researcher and a hardware enthusiast from early middle school days, where I would tear computers apart while friends were playing basketball (tried that too, went back to hardware pretty quickly).In the past few years I’ve advised organizations on building deep learning machines, and ultimately decided to put that knowledge into a guide.Today I’m a computer vision consultant, working with various companies and startups developing image-based products. A lot of the knowledge for this guide came from the decisions made towards building deep learning machines for my various clients.I originally wrote this guide in Hebrew around 5–6 years ago (but who’s counting) and ever since then I make sure it stays up to date. This time I actually decided to re-write most of it. Note that some parts almost haven’t changed a bit and the reason is that I felt they are still relevant.It’s pretty amazing how little hardware has changed over the past 4–5 years. For example, between November 2018 to April 2020, NVIDIA hasn’t updated its line of consumer graphics (GeForce) cards at all. Intel on the other hand has updated its desktop lines twice. Another thing that ended up being pretty anti climatic was AMDs new line of consumer and server-level processors (I’ve seen this is a pretty delicate topic so more on this later).So why is this guide still relevant and what’ll keep it relevant in one year’s time? Well, for one, I try to update it from time to time and I actually do it when something special affects the market. Additionally, I have also taken out parts which I felt were too generation-specific. For example, Intel has just announced its 12th generation silicone, but I’m not too sure that’s going to make building a DL machine too different from building one based on the current 11th generation, so I tried to make the CPU discussion more generic. If something drastic changes between generations I’ll naturally make the required changes.This guide is not for choosing a laptop. In my opinion the deep learning laptop doesn’t exist anymore, at least not for computer vision tasks. Modern DL models are simply too large to fit on such laptops, which usually have graphics cards which are meant for gamers (or, occasionally, for rendering tasks or for giving Photoshop some extra juice). Even the most powerful gaming laptops, those often called “Desktop Replacements” (or DTRs) are probably not strong enough to actually train (even fine-tune) a ResNet-101 based model in a reasonable time frame.In days where Google supplies T4 and P100 based Colab environments, I don’t see a reason to buy a strong laptop for DL purposes.You still want your laptop to be strong; at least 16GB of memory and 4+ cores. But it will mostly be a machine running a terminal to a remote instance. Those 16GBs are for Chrome. I use a Mac, by the way.If you still want a GPU Laptop that’s also portable, I’d buy the Razor Blade. End of discussion. You can look for other gaming oriented rigs but that’d be my favorite pick.I’ll admit I’m not too familiar with this field, and I haven’t seen those being used for non-gaming purposes. It’s still a single graphics card which probably wouldn’t suffice in the long run.I’ll begin by splitting the options into four “categories”:Machines with 8+ GPUs are probably best purchased pre-assembled from some OEM (Lambda Labs, Supermicro, HP, Gigabyte etc.) because building those quickly becomes expensive and complicated, as does their maintenance. Note that they also probably require setting up a modest server room in your office with proper cooling (and a fail safe for said cooling). These are very, very noisy and aren’t meant to be placed near human beings.You theoretically could build one yourselves — I’ve seen that done — but the monetary gain here will be too modest to justify in my opinion. I will note that we are situated in Israel, where access to parts (notably PSUs and enclosures) for such machines is difficult and required having them shipped from the US. Since this is the English guide I’ll note that perhaps building such a machine will be easier and cheaper where you live (and if you have regular access to Newegg).One extra reason not to build one is that OEMs will usually give you a much needed on-site support package.Since ultimately these are rarely self-built, I’m not going to talk about them further.TL;DR: you want a 3080/3080ti/3090. They belong to NVIDIA’s series (or generation) 30 of cards.Not so fast, though; it’s actually ridiculously hard to get your hands on cards due to the ongoing chip shortage. For the sake of this guide I’ll assume you can, but in reality you might have to build your rig with what’s available rather than what’s best.NVIDIA’s Ampere-based series 30 has now been available for around one year, with the 3060ti and 3080ti appearing later on. Let’s first go through those parts and compare the things that I believe matter:So what do we have here? The lowest-end part, the 3070 is supposed to bring 2080ti-equivalent performance at half the older model’s street price. The lower memory size can be an issue for some training tasks. For a pretty small price upgrade the 3080 will offer significantly higher performance — NVIDIA claims twice for some tasks.When poised against the 3090 after NVIDIA’s initial announcement, the 3080 seemed like an easy value-for-money choice (at MSRP). For 700$ you get close to double the performance of the older king, the 2080ti. It does provide less memory but higher speed should still make it great. My older opinion hasn’t changed.When initially announcing Series-30, most DL enthusiasts had to choose between the 3080 and the 3090. Both had a lot going for them and differed by a significant amount of money, making them ideal for different users. Since then, NVIDIA announced the in-between and much expected 3080ti. Does the “ti” part make the no-brainer choice it has in previous generations? I’ll get to that soon.Compared to the 3090, the 3080ti basically gives almost identical performance at a slightly cheaper price, with half the memory. The official 3090 spec requires three slots in your motherboard but this has changed a bit since announcement, making this limitation somewhat less relevant.The 3090 should be the most interesting card of the bunch in terms of price-performance, but at 350W it’s a bit of a challenge to power. When originally writing about it I mentioned that 2-slot solutions will be mandatory for it to see major adoption. Since then GIGABYTE, Asus and EVGA has done just that. I’m still against buying the (probably cheaper) three slot solutions. The 350 watts of power still make using this part difficult but the hardware support has improved in the past year.Anachronism is awesome; a year ago I wrote that:“The 24GB memory is interesting but if in it becomes a choice of fitting 5 3090s rather than 8 3080s in the same chassis, I see system builders opting for the latter, cheaper card. “Well, with two slot blower solutions for the 3090, if money is no issue, you can now put 8 3090 in the same chassis for an incredible 192GB of GPU memory in a single machine.Another thing to get excited about is the extremely fast GDDR6X chosen for the more expensive variants (up from GDDR6). Those are the 3080, 3080ti and 3090.I couldn’t get memory bandwidth figures from the press release but those cards will have insanely fast memory interfaces, which can potentially outweigh their lower memory sizes compared to Series 20 cards. This might sound counterintuitive, but if the card can get the data from its memory and process it fast enough, it can require less memory for similar throughputs (if we’re measuring images/second for training or inference).First things first; if you want to get something cheap for the purpose of learning how to deep learn — I’d get a 3070. But that’s not what this guide is about.For a startup (or a larger firm) building serious deep learning machines for its power-hungry researchers, I’d cram as much 3090s as possible. The double memory figure literally means you can train models at half the time, which is simply worth every penny. This goes for someone building a single or double GPU machine as well. If you do this for a living, the 3090 is an amazing value for money. For “only” 300$ less, I fail to see how the 3080ti fits there. Double memory is a big thing.If, however, the 3080ti is already stretching your budget, then it’s also a terrific value for money.Finally, the original 3080 at an MSRP of 699$ is still a ridiculously strong value for money and for someone maximizing her budget for a single GPU rig, adding a second one later on — get it. It’ll be amazing.Another way I’d look at this; if the choice is 3080ti vs 3080 and budget it tight — the 3080 is an amazing value for money that will serve you well.All of my recommendations are based on MSRPs, which sometimes vary a lot based on supply and demand, especially while the worldwide chip shortage continues. If, for instance, a 3090 runs closer to 2000$ — the 3080ti immediately becomes an insane value for money.No. Unlike the jump from Pascal to Turing (1080ti to 2080ti) — this generation, at least at the moment, provides a modest speed bump. Moving from 1080tis to 2080tis three years ago netted a very nice performance boost due to using mixed precision training or FP16 inference — thanks to their novel TensorCores. This time around we are getting the usual ~30% performance jump (task dependent of course) but nothing else.At this point in time I don’t see a reason to buy a 2080ti-based system unless you get a very good bargain on them.NVIDIA usually makes a distinction between consumer level cards (termed GeForce) and professional cards aimed at professional users.In the past, NVIDIA has another distinction for pro-grade cards; Quadro for computer graphics tasks and Tesla for deep learning. With generation 30 this changed, with NVIDIA simply using the prefix “A” to indicate we are dealing with a pro-grade card (like the A100). What used to be Quadro is now simply called a “Nvidia Workstation GPU” and Teslas are “Nvidia Data Center GPUs”. GCI-targeted GPUs use Axxxx designations such as A6000 while the deep learning ones use Axx and Axxx.So why aren’t we only discussing those “professional grade” GPUs? Well, they are hella expensive and for development (and learning) purposes aren’t really justified. Data center targeted cards are rated to run 24/7 for years, have global on-premise support and sometimes offer passive cooling solutions — all things that you definitely want for your final product, and are the reason you don’t see GCP or AWS using GeForce hardware. Well, that and the fact that NVIDIA’s EULA explicitly forbids that.The current performance leaders for both lines are the GeForce 3090 and the A100. There are also an A40 and recently announced A10, A16 and A30. While the A16 and A40 aren’t meant for DL machines, the A10 and A30 are interesting and I’ll discuss them next.A brave new world that didn’t exist until recently. Currently (and unsurprisingly) dominated by NVIDIA with their T4 (“T” for Turing, meaning the older generation) card, with crippled training capabilities but completely reasonable inference speeds. It’s still more expensive than a GeForce 2080ti/3080/3080ti/3090, but if you need fast inference with professional-level perks (passive cooling, great international support, robustness etc) it’s the only game in town. If that’s not something you need you probably know this by now. For series 30, NVIDIA has replaced this part with the A30, but it’s not an exact replacement, as it’s priced higher and is comparatively much more powerful. See table below, also comparing them to their relevant top of the line data-center counterparts.As mentioned above, “Nvidia Workstation GPUs” (previously Quadro) are NVIDIA’s rendering-targeted level of cards. On paper they shouldn’t belong here, but they can sometimes offer a sweet-spot of value for money between the even more expensive data-center cards (previously Tesla) and consumer cards. They are much more resilient to intense loads (compared to consumer-level GeForce cards) but still don’t cost as much as data center parts. I’m not too familiar with them, but I will note that for training purposes I don’t see the appeal. For inference tasks they can offer a viable alternative, but again, you need to fully understand what you are doing if you go there since they are “tweaked” to reach top performance when rendering graphics rather than running deep learning models.For this generation (Ampere), NVIDIA has several parts announced — the A40 and A6000 initially, followed by an A4000 and an A5000. It seems that the A40 and A6000 are going to be almost identical in terms of specs, with the big difference being that the A40 will be passively cooled.Not much information was released otherwise, but they are targeted towards data centers requiring strong GPU processing for rendering tasks rather than for DL. If they end up being cheaper than an equivalent A100 there might be more reason to discuss them in the future. This table compares them to previous generation similar cards and to their DL-oriented counterparts — the V100 and A100.Since the previous (20) series of cards, all of NVIDIA’s parts have TensorCores as well as CUDA cores. Those are cores meant to run single precision inference. When initially introduced this simply exploded, since performance doesn’t seem to be affected while inference speed sometimes doubles (and training becomes much faster as well).Starting generation 30 this is somewhat the norm and NVIDIA doesn’t even mention the number of distinct Tensor cores in it’s marketing material.Following generations (Turing, which was somewhat of a “generation 20 and a half”) added support for 8-bit and 4-bit inference as well, but those require careful calibration.INT8 is a complicated matter, which should be very well understood inside your organization. Unlike the move to half precision/FP16, which is almost hassle-free, moving to INT8 can significantly hurt performance and should be thought of as a new research task rather than a quick engineering solution for speedup.NVLINK is a hardware protocol by NVIDIA that allows connecting their cards (two or more) and forming a unified memory pool — which allows them to communicate without having to “pass through” the CPU memory. This actually speeds up many computations, hence is pretty cool, but unfortunately it is also disabled for GeForce (ala consumer-level) cards for connecting more than two cards. You can still use it (in the consumer-level variant called SLI) for two cards if that’s all that you have — but for more than two I wouldn’t really bother. How awful is the fact that your 2080ti/30XX-based, 8-GPU machine won’t have NVLINK support? Not too awful. For uses that require fast connection between GPUs (think synchronized batch-normalization) this might offer a nice speed boost but for general purposes you can do without it.Google and Amazon both offer A100 and V100 parts in their cloud. Microsoft also added V100 instances in some locations but is extremely restrictive in actually letting people use them in 8-GPU setups (I am no longer using Azure, if this changed let me know). Google also offers P100 basd instances which present very good value for money. Google also offers their own TPUs which are blazing fast and are a good solution if you use TensorFlow.Despite all of those, quick maths will show that if you are going to train models on a large number of GPUs and do so frequently (meaning such a machine will be fully utilized almost all the time) — purchasing a top-of-the-line training machine will be much more cost effective in the long run. If instead you are likely to train models occasionally but not all the time, a cloud instance can definitely be the wiser choice.It’s also probably best to try using a cloud instance for a few weeks and make the decision afterwards. Also note that even organizations with a wide array of on-prem instances occasionally use cloud based instances during peak times. It’s also important to note that on-prem instances mean you need someone extremely well versed in handling their maintenance. Losing a day’s work for 2–3 data scientists can cost a lot more than the money saved from buying a deep learning machine.Now that we’re done with the topic of graphics card, we can move over to the next part of training-machine-in-the-making — the Central Processing Unit, or, the CPU.A GPU generally requires 16 PCI-Express lanes. The PCI-Express the main connection between the CPU and GPU. Thankfully, most off the shelf parts from Intel support that. It’s connecting two cards where problems usually arise, since that will require 32 lanes — something most cheap consumer cards lack. Moving over to 4 cards we’ll be completely dependent on expensive Xeon-series (server-targeted) cards.Well, short answer? Not really. Despite what I just said, we can actually generally do just as well with 8 lanes per GPU for training and inference. It’s said that extremely speed constrained tasks (such as low-latency trading) will definitely need all 16 supported lanes, though. Also note that aforementioned NVLINK also removes some of this requirement, if your cards support it.As a general rule-of-thumb, don’t go lower than 8 lanes per GPU. Another important thing to note is that other parts which require PCI-E lanes, such as super-fast NMVe-based SSD drives.During the first iteration of this guide, Intel had some parts with 28, 40 and 44 PCI-E lanes at very competitive prices (this was generation 8). Since then more generations came into the market (12, Alder Lake, was just announced) and those parts have been replaced with the more expensive enthusiast oriented “series X” parts. In turn, those parts are now the reigning champions of deep learning hardware due to both their speed and PCI-E lane abundance.If you plan on building a machine with a single GPU, most i7/i9 parts of generation 11 have 20 lanes and will suit you perfectly. Generation 10 is still a terrific value for money and they all have 16 lanes.If you need more cowbell (or lanes), the X-designated parts have 48 lanes — enough for 4 GPUs with 8 lanes each, and even some juice left for a pair of NVMe-based SSD drives. So far they only exist for generation 10 (Icy Lake), though. The cheapest part, the 10900X, sports 48 lanes and 10 cores, with 12, 16 and 18 core parts following suite. It seems that Intel has no intention of introducing the X-series for it’s current generation 11 (Rocket Lake) of CPUs.Intel Ice Lake CPUs also have extensive hardware support for running deep models on the CPU. If you are into that kind of stuff I suggest reading more and understanding if it can be beneficial.Before moving on, make sure you fully understand how CPU sockets work. I assume the reader knows what this means at this stage. X-series part use a more complex socket arrangement which in turn requires more expensive motherboards. Since we are discussing very expensive machines to begin with, this shouldn’t make a huge difference in practice.Xeon CPUs, mentioned briefly earlier, are Intel’s series of CPUs for servers and data-centers (unlike Core for desktops). They are more expensive for various reasons outside of this discussion. Among other things they support multi-CPU setups, which can in turn provide the full 16 PCI-E lanes per GPU you might want and need (since you can combine a couple of CPUs and their respective lanes). Also important is that those CPUs are much more resilient — similarly to NVIDIA’s Data-Center series of GPUs vs consumer-level GeForce GPUs — and should withstand much higher computational loads over long periods of time.It’s also important to note that Xeon CPUs will increase overall system price for a few different reasons:Despite the above, if you care about multi-CPU systems running under extreme loads 24/7, you probably want a Xeon based machine. Xeon based machines were actually more popular in the past, but Intel since then added the aforementioned i9-X CPUs with plenty of PCI-E lanes for most consumer/enthusiast needs. Before moving on I’ll admit that I’m not as knowledgeable in building Xeon based machines and can’t recommend a specific CPU that is a good value for money among them. On top of that, there are a lot of different options for Xeon CPUs. Much more than for consumer level CPUs. One of the things that bump up their prices considerably is the number of supported CPU connections (some support 2-CPU systems while others support 4 and 8 CPU configurations and cost significantly more). They also come with much larger cache sizes, which can actually help a lot for various tasks but this is again outside the scope of this article.If you are building an 8-GPU machine, you are 100% dependent on Xeon CPUs for enabling 8x8 PCI-E lanes (=64 total). Unless you go AMD, that is.This hasn’t been updated since November 2018 and will be left as-is. Please don’t kill me, AMD Fans!Update: It does seem that people are now building DL machines with AMD CPUs; I might actually have to rewrite this if demand rises.Well, AMD is offering something that — at least on paper — should’ve turned the entire industry on its face but in turn hasn’t really. That thing is its Epyc-named series of server-targeted CPUs, and the accompanying consumer-level part named Threadripper. Both of those offer 128 and 64 PCI-E lanes, respectively — potentially blowing Intel out of the water. Those also have plenty of cores (as high as 32), much more than an equivalently priced Intel CPU. So, why haven’t those taken the market by a storm?So, unlike Intel which has its own proprietary connection between CPU and motherboard, AMD CPUs actually use PCI-Express for that. This means that a 64 lane CPU actually has only 56 usable lanes. This in turn means that for 4 cards, you are again left with a 16x/16x/8x/8x configuration. This is still awesome, and more than Intel can provide, but compared to Intel’s i9 CPUs this is less of an issue. Also, when building a machine costing thousands of dollars, saving 400–500$ for a cheaper CPU is perhaps not that important.So it turns out that AMD is still mainly competitive with Intel for gaming machines, or people that are heavily into saving a few hundred dollars. Also, for building a twin-GPU machine I will probably go with them, due to enabling of 2x16 PCI-E and CPU savings vs. overall system price being larger.Even if a certain CPU fits (physically, socket-wise) to a specific motherboard which in turn supports a lot of memory (again, physically, via large number of slots) — it’s still possible that the CPU itself doesn’t. One of the differentiating factors (in terms of price) between CPUs is their ability to support a lot of memory. 64+ GB memory is usually only supported by expensive CPUs.It’s a bit difficult to remain up to date with respect to memory and motherboards since those update constantly with hardware refreshes. They are also (in my opinion) much less exciting and interesting to care for.My advice to you? Following a choice of CPU/GPUs, understand the rest of your needs and pick a good motherboard that supports those. Read the fine print carefully and check that it supports what you need it to support. Some common mistakes are:Motherboards supporting the latest and greatest tend to be similar and similarly-priced. Go with one of the leading companies; MSI, Asus, GIGABYTE to name a few. As mentioned several times, make sure it supports the CPU socket, memory amount and type, and required amount of PCI-E slots, spacing and number of lanes. Regarding the number of lanes, note that some motherboards have GPU-compatible slots that don’t actually allow the full 8/16 PCI-E lanes through (they are x4 lanes underneath). Avoid those. As for specific makes — look next in chipsets. Note that most motherboards, save for the most expensive variant, usually only support 3 GPUs.TLDR: The relevant chipsets you want to buy are series X299 for the latest Intel X-models and (coincidentally) series X399 for AMD CPUs.Like the earlier part on CPUs and their motherboards, chipsets are a crucial part of motherboards and are what actually makes it support various CPU features. Flagship motherboards from leading brands will usually include the latest and greatest chipset which in turn supports everything you need.CPU makers use different “sockets” for connecting its CPUs to motherboards. These are generally represented by terms such as “LGA2066”, which mean the number of physical connections (pins) between the processor and the socket. LGA means “land grid array”, implying that the pins are actually part of the motherboard rather than the CPU (this used to be the other way around).Intel Generation 10 and 11 use socket LGA1200, replacing the older socket LGA1151 used until gen 9. For X-parts (also called HEDT for high-end desktop) it uses a different socket configuration due to their high power requirement — LGA2066, which still persists but should be replaced in the near future. Know those numbers when choosing your motherboard.The general rule of thumb is having the CPU memory twice as lage as GPU memory. So for four 2080tis I’d get 2x11x4 = 88GB of memory. Since memories tend to come in 16GB increments, we will probably have a 96GB memory machine for such a configuration. Reigning memory makers off the top off my head; Corsair (have been up there for years!), HyperX, Patriot, G.Skill and also A-Data at the budget end of things.Your power supply should be able to carry the insane power requirements of the machine you are building. Machines with more than two GPUs will generally have two separate power supplies. So, if you are planning on adding more cards in the future, make sure your enclosure supports a second PSU for later on. Two GPU machines can settle for a single 1000W PSU. Good makers of PSUs are Antec, CoolerMaster, Corsair, SeaSonic, EVGA etc.Assuming this whole machine will be in its own separate, well-chilled room, we don’t really need specialty cooling solutions. If it won’t, you’ll want one that is quiet enough to work next to.As for the enclosure, you’ll want one that is convenient to work with for installing everything and occasionally opening up for fixes/hardware swaps. Also note that PSUs that are given as a bundle with enclosures are usually junk — unless they come from real companies.This boils down to budget. The cheap option is a good, fast 1TB SSD for the currently-used dataset, with 4+ TBs for regular, slower and cheaper storage (model checkpoints etc).If budget is no concern it’s of course nicer to only purchase fast SSDs. Since prices tend to go down all the time, if you feel like splurging you can definitely buy more SSDs. Also worth mentioning is backups; if you absolutely can’t afford to lose your data, consider a RAID solution (outside scope of this article).A lot of the hardware described above has a lot in common with popular high-end gaming machines. Save for the pretty over-the-top GPU setups, most components (motherboard, RAM, PSU etc) will feel right at home in a high-end gaming setup. If you are having some trouble Googling all the information here, change “deep learning” to “gaming” and you’ll be right at home",15/11/2020,0,7.0,33.0,1155.0,755.0,10.0,4.0,0.0,14.0,en
4152,These Bored Apes Do Not Exist,Medium,Nathan Cooper Jones,57.0,12.0,2281.0,"TL;DR — I complain about NFTs, then attempt to train both a GAN and super-resolution model to generate Bored Apes that do not exist. You can check out all the generated images on thisboredapedoesnotexist.nathancooperjones.com.Friday, November 12th, 2021 started out as a normal day for me. Before starting my day at work, I decided to open Twitter to see if I missed anything since the night before.Then, it happened. I saw this Tweet:Say what you will about how Jimmy Fallon tells and reacts to jokes, but if this was a joke, I did not understand it. An animated monkey dressed in a sailor cap? I soon learned that this wasn’t just any animated ape, but one of exactly 10,000 unique images produced in a collection called Bored Ape Yacht Club. After ten minutes down a Twitter rabbit hole, I thought nothing of it and continued on with my day.But by this point, it was too late. Twitter, too, had gone down the NFT rabbit hole with me. To my dismay, throughout the next few weeks, my entire feed was plastered with suggested tweets about NFTs.In case you are lucky enough to have missed it, NFT stands for non-fungible token. The simplest way to understand an NFT is like this: an artist creates a piece of digital art (the NFT), then sells the ownership of the art to a buyer, who pays using cryptocurrency. In an effort to turn a profit, the buyer may then hold or trade the NFT, like a commodity.If this is unclear, think of this like the concept of purchasing a star from one of those sites that generates a printable certificate attesting that you are now the owner of that star. You might technically own it (I suppose), but I can still look at it, NASA can still visit it, and an alien can decide to blow it up at any time — and you have no jurisdiction over any of that. NFTs are like that, but even more lucrative and incredibly damaging to the environment (see here and here for more info on that).And of all the NFTs out there today, a collection of 10,000 Bored Apes stands out as royalty.Apparently, Jimmy Fallon, Post Malone, Steph Curry, Mark Cuban, and Logan Paul all spent hundred of thousands of dollars purchasing Bored Apes. Someone purchases a Bored Ape for $2.7 million. Not to be outdone, someone else purchases a Bored Ape for $3.4 million. Suddenly, the Bored Ape ecosystem is valued at over one billion dollars. What is going on?!Just a few weeks after my initial descent into the Twitter NFT rabbit hole, I see the Tweet that pushes me over the edge: someone suggests creating a GAN model to learn to reproduce the apes, generate a ton of these fake apes, release them for free, and flood the market — a virtual middle finger to the NFT industry. In fact, someone does create a GAN to generate the apes…and then ends up selling these images as NFTs…The description of this collection, called GAN APES on OpenSea, reads:A GAN, or a Generative Adversarial Network, is a type of machine learning framework that “learns” in an unsupervised manner. This collection was created by training our GAN for over 300 hours based on 9940 HQ Original Bored Ape Yacht Club (BAYC) images as well as our custom PKL file.The PKL file is specially trained on psychedelic / abstract images. The PKL file used to create this collection has been destroyed to ensure that the collection can never be restored and will remain unique forever. Everything is stored on IPFS — you own it forever.Oof.At this point, I had had enough — I was too invested in this not to do anything. Someone needed to do something. Today, I hope to be that someone.I think the GAN APES NFT collection had the right idea in a lot of respects, but missed a couple of things that may have affected the final results. It’s hard to say for sure (since the PKL file has been destroyed), but my hunch is that the GAN collapsed at some point and could only produce images that resembled apes, but without any solid features.My ‘data science spidey-sense’ also tingles a bit with a couple of additional observations:One advantage for my project is that we can use this GAN as a great baseline, and see if we can’t improve on these results. With this, I set out on a quest to see if I could generate 100,000 Bored Apes (10x more than the number of images currently in the collection) that didn’t exist in the original, yet matched this Bored Ape Yacht Club style as closely as possible.We can’t get a model without first getting the data, and in this case, that meant somehow collecting all 10,000 high-quality images of the Bored Ape Yacht Club collection. My first instinct was to write a complicated Python script to scrape all the images in the OpenSea collection, but, luckily for me, it turns out someone already did! In fact, they made a whole GitHub repo with the images. I cloned it locally, went into the images directory, wrote a quick script to convert the PNGs to JPEGs (a format that’s easier to use for training models), and… that’s it. Within seconds, I had 10,000 Bored Apes sitting on my desktop, waiting to be used. Nice!I think the GAN APES project had it right by using a GAN (generative adversarial network) here, but for my project, I initially wanted to use an evolution of the GAN made by NVIDIA called StyleGAN, whose architecture and results are so impressive, it’s one of those “even data scientists think this is magic” type of models. If you have never heard of this model before, check out thispersondoesnotexist.com — a collection of faces generated from a StyleGAN model where every face is from a person that does not exist.I promise not to dwell on the technical details too much here, but, in short, a GAN (and StyleGAN, for that manner) consists of two major components: a generator that tries to make new images that look like the images you show it, and a discriminator that tries to differentiate between the real images and the images made by the generator (yes, I am super duper oversimplifying this). Initially, both the generator and discriminator start out with no idea what they are doing, but eventually, they begin to learn that they are in direct competition with each other, as these are opposing tasks. The beauty of the GAN is that as the generator gets better, the discriminator has to get better in order to compete, and vice versa. Both components want to win in their own respective game. This competition is what makes these results so incredible.For nearly ten days, I tried countless attempts at training both a StyleGAN3 and StyleGAN2 model on the Bored Apes dataset, but despite my best efforts, my model completely collapsed before getting any decent results.“Okay,” I thought, “surely I just need to supe up some of these hyperparameters!” For the next week, I tried six more variations of the model — some generating smaller images, some larger; some with a massive batch size, some with smaller; some with lots of attention, some with none. I tried them all, even getting my partner in on the drama while I was out of town, which she did not appreciate (but nonetheless complied with).After days of attempts, I had just about admitted defeat to the NFTs. It seemed I had overestimated how simple it would be for a model to generate these Bored Apes.But then, I had an epiphany — a StyleGAN (and even a GAN for that matter) is an incredibly complicated architecture, one meant to generate complex images resembling a complex subject matter, such as the human face. But the Bored Apes were simple — they were all the same size and shape, all faced the right, all wore some top-piece of clothing, had some variant of hair or hats, etc. Using a vanilla GAN or StyleGAN might actually be overkill for such a simple, invariable task like this.I needed to reconsider my architecture to use something simpler, something lighter… something like a Lightweight GAN!To the rescue came Phil Wang. Phil recently created a repo called “Lightweight GAN” based on a 2020 paper proposing a simpler version of a GAN by Liu et al., able to quickly and accurately converge on a variety of simpler image tasks with very few images in the training dataset. By introducing skip-connections into the generator (similar to what exists in a ResNet) and treating the discriminator as a self-supervised auto-encoder, the training is not only incredibly quick, but surprisingly stable. I know I didn’t do this paper justice with that single-sentence description, so I highly recommend reading the full paper! It’s a great read.I cloned the repo, kicked off a training run (the command I used for that can be found here), and within hours, was seeing Bored Apes that did not exist. I couldn’t believe my eyes!I let my model train for about 120 hours before the discriminator finally seemed like it had won. And, just like that, I had me some fake apes!At this point, I felt like I had achieved my goal, but my apes were 1) a different size than the images in the Bored Ape Yacht Club collection (512 px vs 631 px), and 2) a different image format (mine were JPEGs, while the original were PNGs with transparent rounded corners).A normal person would likely say to themselves, “Well, I did what I set out to do. No one is going to care about an extra 59.5 pixels on each side. Time to go take my free time and live my life again!”But as many therapists have told me in my life so far, I am not a “normal person.” I, instead, decided to spend the next few days over-engineering a solution.At this point, I’m sure there is a flock (or shrewdness, if you will) of data scientists upset that I am going down this road, when a simpler solution exists. Why not instead just train up a Lightweight GAN to produce 1024x1024 px images, then just easily scale them down to 631x631 px? To this, I have two responses:After doing some research, it seemed like the answer was to duct-tape two completely separate models into one by also training up a super-resolution model. Ideally, this model can enhance (and clean up, if need be) an image to be bigger in size while not getting blurrier and more pixelated. You can see the difference in the image below — one is of a 256 px ape manually upscaled to 512 px, and the next is a 256 px ape upscaled to 512 px using super-resolution.I settled on a repo called Waifu2x written by user yu45020, who left a nice base training script to start with, as well as some pre-trained weights. The repo contains a whole bunch of models to do super-resolution, so I (completely randomly and arbitrarily) picked a Cascading Residual Network (paper here). To oversimplify another model architecture (three in one blog post must be a world record of some kind), this architecture mimics a ResNet, but with residual blocks replaced with cascading ones that allow for both local and global connections. The result allows us to upscale an image in an efficient manner without losing quality.I spent a couple hours modifying the training script and data loading methods to specifically add more noise to the training images (in the hopes the model would pick up exactly how to get rid of these smaller noise artifacts in the final image), and within a couple of hours, I had trained up a custom Bored Ape super-resolution model (that training and data augmentation code can be found here)! Best of all, it looked like the modifications to the augmentation I made to remove noise seemed to be working as expected!Finally, I wrote a small script to extract the alpha channel from an original Bored Ape image, copy it to a generated ape image, and save it in PNG format. Thanks to the efforts made by some brave souls who wrote the Python Pillow library, it only took me a couple of minutes (hours) to do.And with that, I sat back and decided to generate, super-resolution, and format 100,000 Bored Apes that did not exist.Finally, the fun part: a quiz!I’m sure you saw this twist coming, but yes, they are all generated from the GAN. Surprise!I think there is a bit of quality degradation when converting from a JPEG to a PNG, so future work should explore keeping the image format as a PNG throughout the entire generation process (something I can try the next time I have a free 120 hours). It’s certainly not perfect, but regardless, I think these results are incredible for a V1.I solemnly swear I will not sell these generated apes as NFTs. And, to hold me to that promise, I am open sourcing the code and model weights I used to generate these images, as well as the 100,000 fake apes. All the code used to train and evaluate these models can be found on GitHub here.github.comAnd, just to make it even more fun, I took some inspiration from NVIDIA and hosted all the images on the website thisboredapedoesnotexist.nathancooperjones.com. Feel free to check it out and refresh a couple times to see some fun Bored Apes that do not exist!This Bored Ape Does Not Existthisboredapedoesnotexist.nathancooperjones.comFeel free to use these images in any way you want.Except if you are Jimmy Fallon — you’ve done enough already.",06/12/2021,0,7.0,29.0,1091.0,625.0,17.0,2.0,0.0,35.0,en
4153,Detecting Empty Parking Lots With Mask RCNN Model,Medium,Ahmet Genç,49.0,5.0,414.0,"Nowadays, there are big problems about parking areas. The large number of vehicles in the cities and the scarcity of parking spaces lead to parking problems in the cities. The biggest solution that can be brought to this problem is to provide people with the information whether the parking spaces are automatically empty or full. Using a mask model, we can detect cars and measure the availability of parking spaces.Firstly we have an image which has a lot of parking lots. So how can you identify the parking spaces here. This can be done by different techniques. For example, white border lines on the sides of the parking spaces can be used or can be made using parking meters in the parking spaces. However, it is simply possible to identify the correct parking spaces by manually labeling the visible parking spaces.Using any annotation tool, parking lots are tagged from a frame in the video. I did this using the YOLO annotation tool. Here, the pinned areas will always remain constant. Thanks to these areas, virtual parking spaces are determined.Lets look some code..Firstly, the necessary libraries should be installed and the ‘mask_rcnn_coco.h5’ model should be downloaded to detect the cars. (or you can train your own model if you like)First, the necessary libraries should be installed and the ‘mask_rcnn_coco.h5’ model should be downloaded to detect the cars. (or you can train your own model if you like) In this project we will benefit from the opencv, tensorflow and mrcnn libraries.Mrcnn Model ConfigsThe mrcnn trained model to be used here has been trained to detect 80 kinds of class objects. But we will only filter classes to detect cars.The annotation tool gives the parking lot coordinates in a text file. The cordinates are divided and separated and assigned to 4 different variables and stored in an array. In this way, the boxes are drawn on the picture.The video taking via open-cv library. Here, the frame range we want to process can be changed with the help of fps. Then, the frame is processed with the help of the model and the cars are identified. Once the cars are detected, the boxes of the cars and the intersections of the parked lot have been found. If the intersection rate exceeds a certain level, the parking lot box to be drawn is drawn in red (ie not available) and green (ie available) boxes are drawn.As a result, all parking spaces that are empty or occupied are automatically found.",20/12/2019,4,6.0,0.0,1167.0,638.0,5.0,0.0,0.0,0.0,en
4154,What is Online Machine Learning?,The Hands-on Advisors,Max Pagels,279.0,7.0,1652.0,"During the start of my career, I was fortunate enough to work on a subfield of machine learning known as online learning (also known as incremental or out-of-core learning). Compared to “traditional” ML solutions, online learning is a fundamentally different approach, one that embraces the fact that learning environments can (and do) change from second to second. It’s tricky to get right, but when applied correctly, the results you can achieve with online learning are nothing short of remarkable. In this post, I’ll give a quick introduction to the technique.Update 27/09/2019: lots of people have asked if there exist any purpose-built incremental learning libraries. Yes! Vowpal Wabbit is extremely powerful, and has been around for quite a while. For those that prefer scikit-inspired APIs, take a look at creme.Our <insert name of AI product> gets better the more you use it!Ever read about about some AI product or platform that claims the above, promising to get better over time? In the vast majority of cases, what’s actually under the bonnet is, well, a bit underwhelming.In a general sense, you need two things for machine learning: data and a suitable learning algorithm. The learning algorithm learns from/trains on your data and produces a (hopefully) accurate model, typically used for prediction on new data. I’m oversimplifying things, but that’s the core idea.ML models, save for some exceptions, are static things. They are essentially collections of parameters. After you’ve trained a model, its parameters don’t change. From a technical perspective, that’s good news: if you want to serve predictions over an API, you can instantiate several instances of a model, place a load balancer on top of them, and pop round the pub for a pint to congratulate yourself on a job well done. Since model parameters don’t change, you don’t need to synchronise between model instances. It’s horizontally scalable, almost trivially so. And if we’re being honest, horizontally scalable is the best type of scalable.But what about new data? If we just train a model once and never touch it again, we’re missing out the information more data could provide us. This is especially important in environments where behaviours change quickly. Online shopping is one such environment: a product that is popular today may be all but forgotten tomorrow.In order to react to new data and make an AI that learns over time, ML practitioners typically do one of two things:99,99 per cent of the time, when someone claims “our AI gets better the more you use it”, what they really mean is that they’ve gone for approach 2), scheduling the training of new models. In its most simple form, this could very literally be one line in a crontab file.All this is well and good, apart from one glaring problem: even if you train new models each week, or even each day, you’re still lagging behind. Your model is never fully up-to-date with current events, because it’s trained on stale data. Ideally, what you want is a model that can learn from new examples in something close to real time. Not only predict in real time, but learn in real time, too.I love using scikit-learn to play around with ML. All of the different algorithms in scikit-learn implement the same simple API, making it very easy to get up and running. For regression problems, I usually start with the SGDRegressor class. Here’s how to train a simple model on dummy data (taken straight from the scikit documentation):The fit() method does all the training magic, resulting in a model we can use for prediction (in this case, predicting on one example):In addition to the fit()method, the SGDRegressor also provides a partial_fit() method, so that you can incrementally train on small batches of data. In fact, all learning algorithms that are compatible with standard optimisation algorithms like (stochastic) gradient decent, adam, RMSprop, and so on have this capability.Out of curiosity, let’s see how long it takes to train on a single example using partial_fit():0.0009 seconds on my machine. That’s quite fast. In fact, if we were to put our SGDRegressor behind a REST API and train on an example each time an HTTP request was made, factoring in, say, 10ms for request processing, we could handle about 520 requests a second, or about 45 million requests a day.This prompts an interesting question: given these numbers, would it be possible to learn from new examples in something close to real time? Therein lies the potential of online learning: the second we see a new example, let’s learn from it as fast as we can. The faster, the better. In fact, because speed trumps everything else in online learning, we typically use simple learning algorithms over complex ones like neural networks. We strive for millisecond-level learning; everything else comes second.ML purists might scoff at the idea of online algorithms for real time learning. Training a model can go wrong in lots of different ways: the algorithm itself might not be suitable, the model might fail to generalise well, the learning rate might be wrong, the regularisation might be too low or too high…the list goes on. Why on earth would we even attempt to learn immediately when there are no guarantees on what might happen?The answer is simple: no matter how good a model is, or how much data you feed it, a model is still an imperfect representation of an environment. To make the best possible decisions right now, we can’t afford to have a model that only knows about things that happened yesterday.Consider the following example. Let’s say we run a news website. We personalise our news by collecting data on what was clicked or not clicked, and by whom. Based on this information, we predict the types of news different visitors might like, and serve them relevant items.One day, out of the blue, word gets out that the government is issuing a state of emergency, and will hold a press conference in an hour. Suddenly, everyone is interested in domestic affairs — even those who typically only read about sports or look at the funnies. When presented with a news piece about the conference, a huge percentage of the audience clicks it to learn more.If you had gone the traditional route and batch trained your recommendation engine once a day, it would still be stuck offering the same type of content, even though the underlying world changed dramatically¹. You should be serving up domestic news right now, but aren’t because your system is too slow.It gets worse: the following day, after the press conference and following a new training cycle, your engine would start actively recommending domestic news which, after 24 hours, isn’t necessarily interesting any more. It’s made two mistakes, both because it can’t react fast enough.That’s the power of online learning: done properly, it can react in minutes or even seconds. With it, there is no such thing as “yesterday’s news”.Bolding for emphasis, implementing real time learning isn’t easy. If you place some learning algorithm behind an API and, god forbid, open it up to the Internet, there’s an almost limitless number of ways it can go wrong. You might get lots of feedback (examples) from one thing but not another, leading to a skewed classes problem. You might’ve set your learning rate too high, causing your model to forget everything that happened more than a second ago. You might overfit, or underfit. Someone might DDoS your system, screwing up learning in the process. Online learning is prone to catastrophic interference — more so than most other techniques.Online learning also requires an entirely different approach in terms of technical architecture. Since a model can, and will, change from second to second, you can’t just instantiate several instances like you can with traditional techniques. It’s not horizontally scalable. Instead, you are forced to have a single model instance that eats new data as fast as it can, spitting out sets of learned parameters behind an API. And the second that one set in one process gets replaced by a new one, all other processes must follow suit immediately. It’s an engineering challenge, because the most important part (the model) is only vertically scalable. It may not even be feasible to distribute between threads.Learning immediately also requires fast access to new data. If you’re lucky enough to get all the data you need for a single training example as part of an API call, you’re good to go. But if something is not available client-side, you need to be able to grab that data from somewhere in milliseconds. Typically, that means using an in-memory store like Redis. “Big data” processing frameworks aren’t of much help. If you want to to both batch and online learning, Spark isn’t enough. If you do only online learning, Spark is useless.I could go on for hours about the technical aspects, but the bottom line is this: online learning is an ML solution unlike any other, and it requires a technical approach unlike any other.Summing up online learning isn’t easy. It’s not a single learning algorithm: in fact, lots of algorithms can learn online. It’s also not fundamentally different in terms of how learning happens: you can use the same optimisation steps you always do. It doesn’t even have a bombastic, sci-fi sounding name.What online learning is is a fundamentally different way of approaching machine learning. It’s an approach that embraces change, no matter how drastic. Its existence is predicated on the belief that since everything is in flux, we should stop seeking stationarity and instead start living in the moment.A special thanks to Data Scientist Jarno Kiviaho, who I consider to be one of Finland’s top authorities on this topic.If you fancy playing around with the (admittedly simple) code in this post, it’s available as a GitHub gist: https://gist.github.com/maxpagels/b9c9001f7e5b28a5742b81b02f7704e2[1]: You could, theoretically, have a feature indicating a drastic event like this, but it’s impossible to account for everything.",20/04/2018,3,6.0,16.0,0.0,0.0,0.0,1.0,0.0,4.0,en
4155,Image analysis intro using python & opencv,Product Development Notebook,Chris Loughnane,247.0,7.0,1039.0,"It’s been a while since I first wrote about how useful computer vision can be in product development, and I recently put together a quick demo for other engineers at my new gig (@ Continuum) that is cleaner and more thorough than previous versions (plus it uses the cv2 library instead of the deprecated cv library I used before).(and of course it’s written in python)………Image analysis is hugely powerful, particularly in the context of product development. Most of the challenges in computer vision (and AI in general) comes from trying to process unstructured and/or uncontrolled information.Fortunately, we product development engineers spend a bunch of time setting up experiments in the lab. In those cases we have much more control over lighting and what we’re tracking than we would if we were trying to track something in the “real” world.So to kick off with the basics we’re going to take a look at an image and see if we can identify an object in it!Python’s most interesting functionality comes from it’s wide range of libraries. These are typically placed at the top of a file using the syntax shown belowIn [1]:First create a variable that has the location of folder. Then list all the files in that folder using the listdir function from the os libraryIn [2]:Next create a variable that has the full path of the file we wantIn [3]:Now that we’ve got the file location in we can read the image in using the imread function from the cv2 library and assign it to the variable img. Once we've read it in we'll print the shape property of the img array. We can also query the object using the ? syntaxIn [4]:Note the shape of the image, (1080,1920,3) indicates that the image gas 1080 rows (y dimension), 1920 columns (x dimension), and 3 channels (Blue, Green, and Red).Let’s take a look at the image itself. To do that we’ll use the imshow function from matplotlib.pyplot (which we've shortened to plt)In [5]:Out[5]:Much better, although the colors seem a bit off. This is a quirk where the cv2 library reads images in as BGR (Blue Green Red), but the plt.imshow function assumes RGB.This is a little annoying for displaying images, but doesn’t really matter for analysis as the RGB color space is pretty useless for analyzing images (as we will see), it’s much more useful for generating them.For now, let’s overwrite the img variable with an RGB image, using the cv2.cvtColor function, and then display it againIn [6]:Out[6]:To show what I mean about the RGB color space being useless for analyzing images, let’s look at each channel individually. We do this by slicing our img array (now in RGB color space) into three individual arrays.The syntax for slicing is [rows,columns,channels]. A (:) indicates that you want to keep everything.In our case we want to keep all the rows and columns, but only keep one channel per img. This results in a grayscale image (i.e each pixel has a single value from 0–255, as opposed to 3 for our original RGB image)We then plot the image with our plt.imshow function. The plt.subplot function lets us plot multiple images at onceIn [7]:Out[7]:Looking at those images, none of them seem to have the ball be either the darkest (pixels are all low) or brightest (pixels are all high) things in the image. That makes it very difficult to identify.Instead we are going to use the Lab color space. We do this because The nonlinear relations for L, a, and b* are intended to mimic the nonlinear response of the eye.To convert to Lab we change we use the same cv2.cvtColor function as before, but pass it the cv2.COLOR_RGB2LAB parameter.We thin show all the images as we did witht the RGB image, though the code is a little more pythonic.In [8]:Much better!. Though the L channel (which stands for brightness) is pretty useless, a and b look promising.The next step is to threshold the image to make everything below a certain threshold go to 0, and everything above or equal to it go to 1.The way this is done is using the cv2.threshold function. To better understand what information you need to give the function we can again use the ? syntaxIn [9]:So it takes the following parameters:The annoying thing about thresholding is that it’s kind of a guess-and-check to find the best value (ie one that keeps most everything you want and eliminates most everything you don’t). In cases like this I like to use the iPython Notebook’s interact function.interact takes a function and gives you a way to vary that function's input live. In our case, our function will take the input thresh_val, calculate the threshold using the cv2.threshold function, and use plt.imshow to show the image.the interact function will only work in an active ipython notebookIn [10]:Out[10]:In [11]:Out[11]:Out[11]:In trying to figure out how to find the center of that point I searched for How to find the center of anrea opencv python and came up with this tutorial.It uses the cv2.moments function to ""calculate all the moments up to the third order of a polygon or rasterized shape""... or something.Anyway, some of those moments can be used to calculate the center. So let’s calculate the moments and give it the variable M.In [12]:So it looks like it returns a dictionary (a series of key:value pairs). It’s actually tough to read so let’s print it out more nicely using the iteritems function to iterate through the dictionaryIn [13]:Much cleaner to read, but still greek to me. Following along with the tutorial…In [14]:With that we can use cv2.circle to draw a red (255,0,0). 20px circle at the center.We need to use integers because there are no decimals in pixels.In [15]:Now we just so the image and…In [16]:Out[16]:Looks good!Now what’s even better is that once you have something like this automated, you can easily do video analysis… as videos are just a series of images (usually at 30 frames per second). If you combine that power with a well-positioned high-speed camera… you can easily capture data on really neat phenomena.Here are a few things I’ve done in the past that show how the techniques shown above can be applied to video.In [17]:In [18]:In [19]:",26/10/2015,24,4.0,7.0,962.0,440.0,7.0,1.0,0.0,7.0,en
4156,Multi Class Text Classification with LSTM using TensorFlow 2.0,Towards Data Science,Susan Li,25000.0,7.0,707.0,"A lot of innovations on NLP have been how to add context into word vectors. One of the common ways of doing it is using Recurrent Neural Networks. The following are the concepts of Recurrent Neural Networks:The above is the architecture of Recurrent Neural Networks.Assuming we are solving document classification problem for a news article data set.Therefore, we generally do not use vanilla RNNs, and we use Long Short Term Memory instead. LSTM is a type of RNNs that can solve this long term dependency problem.In our document classification for news article example, we have this many-to- one relationship. The input are sequences of words, output is one single class or label.Now we are going to solve a BBC news document classification problem with LSTM using TensorFlow 2.0 & Keras. The data set can be found here.There are 2,225 news articles in the data, we split them into training set and validation set, according to the parameter we set earlier, 80% for training, 20% for validation.Tokenizer does all the heavy lifting for us. In our articles that it was tokenizing, it will take 5,000 most common words. oov_token is to put a special value in when an unseen word is encountered. This means we want <OOV> to be used for words that are not in the word_index. fit_on_text will go through all the text and create dictionary like this:We can see that “<OOV>” is the most common token in our corpus, followed by “said”, followed by “mr” and so on.After tokenization, the next step is to turn those tokens into lists of sequence. The following is the 11th article in the training data that has been turned into sequences.When we train neural networks for NLP, we need sequences to be in the same size, that’s why we use padding. If you look up, our max_length is 200, so we use pad_sequences to make all of our articles the same length which is 200. As a result, you will see that the 1st article was 426 in length, it becomes 200, the 2nd article was 192 in length, it becomes 200, and so on.In addition, there is padding_type and truncating_type, there are all post, means for example, for the 11th article, it was 186 in length, we padded to 200, and we padded at the end, that is adding 14 zeros.And for the 1st article, it was 426 in length, we truncated to 200, and we truncated at the end as well.Then we do the same for the validation sequences.Now we are going to look at the labels. Because our labels are text, so we will tokenize them, when training, labels are expected to be numpy arrays. So we will turn list of labels into numpy arrays like so:Before training deep neural network, we should explore what our original article and article after padding look like. Running the following code, we explore the 11th article, we can see that some words become “<OOV>”, because they did not make to the top 5,000.Now its the time to implement LSTM.In our model summary, we have our embeddings, our Bidirectional contains LSTM, followed by two dense layers. The output from Bidirectional is 128, because it doubled what we put in LSTM. We can also stack LSTM layer but I found the results worse.We have 5 labels in total, but because we did not one-hot encode labels, we have to use sparse_categorical_crossentropy as loss function, it seems to think 0 is a possible label as well, while the tokenizer object which tokenizes starting with integer 1, instead of integer 0. As a result, the last Dense layer needs outputs for labels 0, 1, 2, 3, 4, 5 although 0 has never been used.If you want the last Dense layer to be 5, you will need to subtract 1 from the training and validation labels. I decided to leave it as it is.I decided to train 10 epochs, and it is plenty of epochs as you will see.We probably only need 3 or 4 epochs. At the end of the training, we can see that there is a little bit overfitting.In the future posts, we will work on improving the model.Jupyter notebook can be found on Github. Enjoy the rest of the weekend!References:www.coursera.orglearning.oreilly.co",08/12/2019,8,6.0,0.0,646.0,302.0,19.0,8.0,0.0,16.0,en
4157,Using OOB Tags in AIML: Part I,pandorabots-blog,Pandorabots,257.0,3.0,690.0,"Suppose you are building an Intelligent Virtual Agent or Virtual Personal Assistant (VPA) that uses a Pandorabot as the natural language processing engine. You might want this VPA to be able to perform tasks such as sending a text message, adding an event to a calendar, or even just initiating a phone call. OOB tags allow you to do just that!OOB stands for “out of band,” which is an engineering term used to refer to activity performed on a separate, hidden channel. For a Pandorabot VPA, this translates to activities which fall outside of the scope of an ordinary conversation, such as placing a phone call, checking dynamic information like the weather, or searching wikipedia for the answer to some question. The task is executed, but does not necessarily always produce an effect on the conversation between the Pandorabot and the user.OOB tags are used in AIML templates and are written in the following format: <oob>command</oob>. The command that is to be executed is specified by a set of tags which occur within the <oob> tags. These inner OOB tags can be whatever you like, and the phone-related actions they initiate are defined in your applications code.To place a call you might see something like this: <oob><dial>some phone number</dial></oob>. The <dial> tag within the <oob> tag sends a message to the phone to dial the number specified. When your client indicates they want to dial a number, your application will receive a template containing the command specified inside the OOB tag. Within your application, this inner command will be interpreted and the appropriate actions will be executed.It is useful to think of the activities initiated by oob tags as falling into one of two categories, based on whether they return information to the user via the chat interface or not.The first category, those that do not return information, typically involve activities that interrupt the conversation. If you ask your VPA to look up restuarants on a map, it will open up your map application and perform a search. Similarly, if you ask your bot to make a phone call, it will open the dialer application and make a call. In both of these examples, the activity performed interrupts the conversation and displays some other screen.The second category, those that do return information to the user via the chat interface, are generally actions that are executed in the background of the conversation. If you ask your Pandorabot to look up the “Population of the United States” on Wikipedia, it will perform the search, and then return the results of the search to the user via the chat window. Similarly, if you ask your Pandorabot to send a text message to the friend, it will send the text, and then return a message to the user via the chat window indicating the success of the action, i.e. “Your text message was delivered!”In this second set of examples, it is useful to distinguish between those activities whose results will be returned directly to the user, like the Wikipedia example, and those activities whose successful completion will simply be indicated to the user through the chat interface, as with the texting example.Here is an example of a category that uses the phone dialer on android.Here is an example interaction this category would lead to:Human: Dial 1234567.Robot: Calling 1234567.Here is a slightly more complicated example involving the oob tag, which launches a browser and performs a google search:Human: Look up Pandorabots.Robot: Searching…Searching… Please stand by.Note: not shown in the previous example is the category RANDOM SEARCH PHRASE, which delivers a random selection from a short list of possible replies, each indicating to the user that the bot correctly interpreted their search request.For a complete list of oob tags as implemented in the CallMom Virtual Personal Assistant App for Android, as well as usage examples, click here.Be sure to look out for the upcoming post “Using OOB Tags in AIML: Part II”, which will go over a basic example of how to intrepret the OOB tags received from the Pandorabots server within the framework of your own VPA application.Originally published at blog.pandorabots.com on October 9, 2014.",09/10/2014,2,5.0,3.0,0.0,0.0,0.0,0.0,0.0,3.0,en
4158,Understanding Encoders-Decoders with an Attention-based mechanism,DataX Journal,Harsh Sharma,39.0,9.0,1379.0,"How attention-based mechanism completely transformed the working of neural machine translations while exploring contextual relations in sequences!When it comes to applying deep learning principles to natural language processing, contextual information weighs in a lot! In the past few years, it has been shown that various improvement in existing neural network architectures concerned with NLP has shown an amazing performance in extracting featured information from textual data and performing various operations for a day to day life. One of the models which we will be discussing in this article is encoder-decoder architecture along with the attention model.The encoder-decoder architecture for recurrent neural networks is actually proving to be powerful for sequence-to-sequence-based prediction problems in the field of natural language processing such as neural machine translation and image caption generation.We will try to discuss the drawbacks of the existing encoder-decoder model and try to develop a small version of the encoder-decoder with an attention model to understand why it signifies so much for modern-day NLP applications!Encoder-decoder — a brief introductionThe encoder-decoder architecture with recurrent neural networks has become an effective and standard approach these days for solving innumerable NLP based tasks. The encoder-decoder model is a way of organizing recurrent neural networks for sequence-to-sequence prediction problems or challenging sequence-based inputs like texts [ sequence of words ], images [ sequence of images or images within images] to provide many detailed predictions.Encoder:The encoder is a kind of network that ‘encodes’, that is obtained or extracts features from given input data. It reads the input sequence and summarizes the information in something called the internal state vectors or context vector (in the case of the LSTM network, these are called the hidden state and cell state vectors). We usually discard the outputs of the encoder and only preserve the internal states. This context vector aims to contain all the information for all input elements to help the decoder make accurate predictions. The hidden and cell state of the network is passed along to the decoder as input.Decoder:A decoder is something that ‘decodes’, interpret the context vector obtained from the encoder. The context vector of the encoder’s final cell is input to the first cell of the decoder network. Using these initial states, the decoder starts generating the output sequence, and these outputs are also taken into consideration for future predictions. A stack of several LSTM units where each predicts an output (say y_hat) at a time step t.each recurrent unit accepts a hidden state from the previous unit and produces an output as well as its own hidden state to pass along the further network.One of the main drawbacks of this network is its inability to extract strong contextual relations from long semantic sentences, that is if a particular piece of long text has some context or relations within its substrings, then a basic seq2seq model[ short form for sequence to sequence] cannot identify those contexts and therefore, somewhat decreases the performance of our model and eventually, decreasing accuracy.Keeping this in mind, a further upgrade to this existing network was required so that important contextual relations can be analyzed and our model could generate and provide better predictions.Attention mechanism- basic workingAttention is an upgrade to the existing network of sequence to sequence models that address this limitation. The simple reason why it is called ‘attention ’ is because of its ability to obtain significance in sequences.First, it works by providing a more weighted or more signified context from the encoder to the decoder and a learning mechanism where the decoder can interpret were to actually give more ‘attention’ to the subsequent encoding network when predicting outputs at each time step in the output sequence.We can consider that by using the attention mechanism, there is this idea of freeing the existing encoder-decoder architecture from the fixed-short-length internal representation of text. This is achieved by keeping the intermediate outputs from the encoder LSTM network which correspond to a certain level of significance, from each step of the input sequence and at the same time training the model to learn and give selective attention to these intermediate elements and then relate them to elements in the output sequence.In simple words, due to few selective items in the input sequence, the output sequence becomes conditional,i.e., it is accompanied by a few weighted constraints. These conditions are those contexts, which are getting attention and therefore, being trained on eventually and predicting the desired results.Attention-based sequence to sequence model demands a good power of computational resources, but results are quite good as compared to the traditional sequence to sequence model. Besides, the model is also able to show how attention is paid to the input sequence when predicting the output sequence. This can help in understanding and diagnosing exactly what the model is considering and to what degree for specific input-output pairs.Rather than just encoding the input sequence into a single fixed context vector to pass further, the attention model tries a different approach. This model tries to develop a context vector that is selectively filtered specifically for each output time step, so that it could focus and generate scores specific to those relevant filtered words and accordingly, train our decoder model with full sequences and especially those filtered words to obtain predictions.Let us consider the following to make this assumption clearer.Attention is proposed as a method to both align and translate for a certain long piece of sequence information, which need not be of fixed length. Here, alignment is the problem in machine translation that identifies which parts of the input sequence are relevant to each word in the output, whereas translation is the process of using the relevant information to select the appropriate output. With help of attention models, these problems can be easily overcome and provides flexibility to translate long sequences of information.Let us try to observe the sequence of this process in the following steps:That being said, let’s try to consider a very simple comparison of the model’s performance between seq2seq with attention and seq2seq without attention model architecture.Comparing attention and without attention-based seq2seq models2. Thanks to attention-based models, contextual relations are being much more exploited in attention-based models, the performance of the model seems very good as compared to the basic seq2seq model, given the usage of quite high computational power.3. Using word embeddings might help the seq2seq model to gain some improvement with limited computational power, but long sequences with heavy contextual information might not get trained properly.All this being given, we have a certain metric, apart from normal metrics, that help us understand the performance of our model — the BLEU score.The bilingual evaluation understudy score, or BLEUfor short, is an important metric for evaluating these types of sequence-based models. It helps to provide a metric for a generated sentence to an input sentence being passed through a feed-forward model.This score scales all the way from 0, being totally different sentence, to 1.0, being perfectly the same sentence.BELU score was actually developed for evaluating the predictions made by neural machine translation systems. Though is not totally perfect, but does offer certain benefits:The python’s own natural language toolkit library, or nltk, consists of the bleu score that you can use to evaluate your generated text against a given input text.nltk provides the sentence_bleu() function for evaluating a candidate sentence against one or more reference sentences.Given below is a comparison for the seq2seq model and attention model’s bleu score:After diving through every aspect, it can be therefore concluded that sequence to sequence-based models with the attention mechanism does work quite well when compared with basic seq2seq models. Exploring contextual relations with high semantic meaning and generating attention-based scores to filter certain words actually help to extract the main weighted features and therefore helps in a variety of applications like neural machine translation, text summarization, and much more. Implementing attention models with bidirectional layer and word embedding can actually help to increase our model’s performance but at the cost of high computational power.Though with limited computational power, one can use the normal sequence to sequence model with additions of word embeddings like trained google news or wikinews or ones with glove algorithm to explore contextual relationships to some extent, dynamic length of sentences might decrease its performance after some time, if being trained on extensively.",01/02/2021,0,17.0,0.0,615.0,361.0,8.0,3.0,0.0,2.0,en
4159,Should AI explain itself? or should we design Explainable AI so that it doesn’t have to,Towards Data Science,Prajwal Paudyal,55.0,9.0,1578.0,"In this article, I’ll go over:This article got longer that what I originally intended, so for the busy souls, here is a synopsis.Explanations for AI behavior that are generated Ad-hoc or post-hoc are more like justifications and may not be capture the truth of the decision process. If trust and accountability is needed, that has to be taken into account early on in the design process. Explainable AI (XAI )is NOT an AI that can explain itself, it is a design decision by developers. It is AI that is transparent enough so that the explanations that are needed are part of the design process.Now, the full story.A self driving car knocked down and killed a pedestrian in Tempe, AZ in 2018. Issues like who is to blame (accountability), who to prevent this (safety) and whether to ban self-driving cars (liability and policy evaluation) all require AI models used to make those decisions to be interpretable.This research paper shows that a classifier that could recognize wolves from husky dogs was basing its decision solely on the presence of snow in the background.This article on Propublica show hows a predictive model for risk of repeat criminal offense can have a strong ethnic bias. Similar ethnic and gender bias has been exposed in candidate-screening algorithms for hiring, loan approval applications among others.So, if we measure the aptitude of an AI system solely based on one performance metric (i.e.) accuracy, we might be in for a surprise later. (Article: Do AI take shortcuts?)Adversarial examplesThen, there are adversarial examples where some attackers can selectively add distortions to images to make an otherwise robust classifier always choose a certain class. These distorted images look normal to humans and thus undetectable. This has big implications for security algorithms.As AI systems are involved in more and more decisions we make, the want and need to make AI explainable will only grow, however the decisions from most of the AI systems we use now, are not explainable.What kind of explanations do we need?Now that we have established why we need XAI, let’s talk about what kind of explanations are desirable.The first type are explanations that those that help to understand the data better. An example would be a system that tell you that a particular picture is a cat because it looks like another example of a cat (nearest neighbor). Another XAI can tell you its a cat because it has whiskers and fur (feature).The second type of explanations are the ones that give understand the model better. Approaches to visualize the various neuron activations in neural networks are primarily in this category.XAI explanations will usually cover both of these areas, but will usually have a stronger inclination.Not all applications require explanations. So the first step would be to determine if explanations are needed. If the application in mind has no critical behavior that can give rise to some liability then a blackbox model will work just fine. However, as we discussed above, there are many applications that absolutely need XAI.Now, to make sure our understanding is well-rounded let’s go over some of the criticisms for XAI1. Too complex to explainSkeptics point out (and correctly so) that most popular AI models with good performance have around 100 million parameters. This means there were 100 million numbers that were learned during training that contribute to a decision. With that complexity, how can we begin to think about which factors affect the explanations?2. Performance vs. Explainability TradeoffMachine learning in classification works by: 1) transforming the input feature space into a different representation (feature engineering) and 2) searching for a decision boundary to separate the classes in that representation space. (optimization). Modern deep learning approaches perform 1 and 2 jointly by via. a hierarchical representation learning.Simple models like linear regression are also inherently explainable since the weights and signs on them are indicators of importance of features and decisions can be explained in terms of those weights.Decision trees also are explainable, because the various attributes are used to form splits. These splitting decisions can be used as a basis for mining rules that can suffice to some extent as explanations. (However there are issues with ambiguities)A deep neural network(DNN) that learns millions of parameters and may be regularized by techniques like batch normalization and dropouts is quite incomprehensible. Many other Machine Learning techniques also face this problem.However with DNNs gaining state-of-art performances in many domains and with a big margin, it is hard to make a case against using them.3. Humans cannot explain their decisions eitherIf we adhere to humans as the golden standard, then the argument is that we should either distrust all humans, or we should be wholly content with our unexplainable AI partners.There is evidence that most of the thinking and decision making mechanism in humans happens unconsciously. There is also evidence that we have evolved to invent explanations, when in truth we are at times clueless of the decision process. [1] We also generate naive explanations for things we do for reasons we don’t want to admit, if only to avoid cognitive dissonance.In the face of all this, how do we expect AI to be able to explain itself?There are ad-hoc and post-hoc XAI systems suggested that use two separate models: a learning model to make the decisions and an explanation model to provide explanations.This is perhaps similar to our human neurology, and these explanations are more like justifications given by the PR team for a bad executive decision.If the explanation model knew what was going on AND could explain it, why not use it instead of the decision model in the first place?To want a separate AI that can explain an AI might not be the way to go, specially for reasons of accountability, trust or liability. This is any time a higher order decision has to be made by taking the explanations into consideration.So, the ad-hoc and post-hoc explanations can be used to better understand the modeling process, or research into how AI works behind the scenes, but usually is not enough to support business level decisions.Let us recap what we covered so far.So where do we go from here?We believe that we’re seeing the world just fine until it’s called to our attention that we’re not. Then we have to go through a process of learning — David EaglemanIf the application being design requires explanations, then the expected explanations have to be brought to the table much earlier on- during finalizing the requirements, so the architectural design can incorporate it.Rather than making the very tough decision whether to use simpler but explainable models with low performance or to use complex black-box models with no explanations, the better option might be toNow, easier said than done. Knowing what explanations are required of an application will need close collaboration of all stake-holders. For instance a loan approving AI application will need explanations that are radically different than a face identification algorithm. In other applications, learning to make decisions by first learning high level attributes may still be impossible.To make this understanding more concrete, let us look at a practical example of an AI application from this research that teaches people sign language words. If the student executes the sign correctly, everything is fine. But if the student hasn’t quite gotten it yet, like any good tutor, the AI application is expected to give some feedback to the students. This AI lives in a computer and can use the cameras to look a learner.Now, if black-box Computer Vision (CV) algorithms are utilized, the AI can perhaps determine if a student did a sign incorrectly, but what feedback can it provide? It can only say ‘try again’, perhaps with some tonal adjustments.However, let us consider another AI that is designed with the possible explanations in mind.Sign Language Linguists have postulated that they way signs differ from each other either in the location of signing, the movement or the hand-shape. (In other words, you can mess up in one of these three ways) *.Now, with that in mind, separate AI models can be trained to detect the correctness for each of these attributes. Then the final decision can be a combination of the decisions of these three models.So, when a new learner makes a mistake in one of these, an appropriate feedback like ‘your hand-shape wasn’t correct’ can be given with examples of the correct one.The key insight here is that, recognizing the shape of the hand, or if it matches with another hand-shape is a problem perhaps best solved by a CNN which itself doesn’t need to be explained. However, the bigger problem of recognizing a sign is broken down in terms of sub-problems which forms the basis for explanations and this knowledge is domain specific (i.e. this breakdown will only work for Sign Languages)Can all problems that need explanations be broken down like this? If so, can we develop a theoretical framework for doing so? Can we quantify the explanation-performance trade-off and minimize it? How granular should the explanations be? What are best ways to provide explanations?These questions will have to be answered in collaboration with the field experts where the AI algorithms are designed. One thing is for sure, going forward, as people and governments demand more explanation (GDPR 2018), the ‘one model fits all problems’ approach to AI will not be the answer.Comments? Suggestions?Here are some of my other posts:bit.lytowardsdatascience.comtowardsdatascience.com*Facial expressions isn’t considered in this work, and orientation is learnt jointly with hand shape.",04/03/2019,0,13.0,6.0,862.0,554.0,10.0,4.0,0.0,13.0,en
4160,An Introduction to Perceptron Algorithm,Towards Data Science,Yang S,163.0,6.0,683.0,"This blog will cover following questions and topics1. What is Perceptron?2. Stochastic Gradient Descent for Perceptron3. Implementation in Python1. What is Perceptron?Perceptron set the foundations for Neural Network models in 1980s. The algorithm was developed by Frank Rosenblatt and was encapsulated in the paper “Principles of Neuro-dynamics: Perceptrons and the Theory of Brain Mechanisms” published in 1962. At that time, Rosenblatt’s work was criticized by Marvin Minksy and Seymour Papert, arguing that neural networks were flawed and could only solve linear separation problem. However, such limitation only occurs in the single layer neural network.Perceptron can be used to solve two-class classification problem. The generalized form of algorithm can be written as:Nonlinear activation sign function is:While logistic regression is targeting on the probability of events happen or not, so the range of target value is [0, 1]. Perceptron uses more convenient target values t=+1 for first class and t=-1 for second class. Therefore, the algorithm does not provide probabilistic outputs, nor does it handle K>2 classification problem. Another limitation arises from the fact that the algorithm can only handle linear combinations of fixed basis function.2. Stochastic Gradient Descent for PerceptronAccording to previous two formulas, if a record is classified correctly, then:Otherwise,Therefore, to minimize cost function for Perceptron, we can write:M means the set of misclassified records.By taking partial derivative, we can get gradient of cost function:Unlike logistic regression, which can apply Batch Gradient Descent, Mini-Batch Gradient Descent and Stochastic Gradient Descent to calculate parameters, Perceptron can only use Stochastic Gradient Descent. We need to initialize parameters w and b, and then randomly select one misclassified record and use Stochastic Gradient Descent to iteratively update parameters w and b until all records are classified correctly:Note that learning rate a ranges from 0 to 1.For example, we have 3 records, Y1 = (3, 3), Y2 = (4, 3), Y3 = (1, 1). Y1 and Y2 are labeled as +1 and Y3 is labeled as -1. Given that initial parameters are all 0. Therefore, all points will be classified as class 1.Stochastic Gradient Descent cycles through all training data. In the initial round, by applying first two formulas, Y1 and Y2 can be classified correctly. However, Y3 will be misclassified. Assuming learning rate equals to 1, by applying gradient descent shown above, we can get:Then linear classifier can be written as:That is 1 round of gradient descent iteration.Table above shows the whole procedure of Stochastic Gradient Descent for Perceptron. If a record is classified correctly, then weight vector w and b remain unchanged; otherwise, we add vector x onto current weight vector when y=1 and minus vector x from current weight vector w when y=-1. Note that last 3 columns are predicted value and misclassified records are highlighted in red. If we carry out gradient descent over and over, in round 7, all 3 records are labeled correctly. Then the algorithm will stop. Final formula for linear classifier is:Note that there is always converge issue with this algorithm. When the data is separable, there are many solutions, and which solution is chosen depends on the starting values. When the data is not separable, the algorithm will not converge. For details, please see corresponding paragraph in reference below.3. Implementation in PythonFirst, load the Iris data.Then visualize dataUpdate y=0 to y=-1After applying Stochastic Gradient Descent, we get w=(7.9, -10.07) and b=-12.39Figure above shows the final result of Perceptron. We can see that the linear classifier (blue line) can classify all training dataset correctly. In this case, the iris dataset only contains 2 dimensions, so the decision boundary is a line. In the case when the dataset contains 3 or more dimensions, the decision boundary will be a hyperplane.ConclusionIn this blog, I explain the theory and mathematics behind Perceptron, compare this algorithm with logistic regression, and finally implement the algorithm in Python. Hope after reading this blog, you can have a better understanding of this algorithm. If you have interests in other blogs, please click on the following link:medium.comReference[1] Christopher M. Bishop, (2009), Pattern Recognition and Machine Leaning[2] Trevor Hastie, Robert Tibshirani, Jerome Friedman, (2008), The Elements of Statistical Learning",19/06/2019,5,29.0,26.0,373.0,114.0,16.0,0.0,0.0,1.0,en
4161,Taking Keras to the Zoo,Gab41,Karl N.,398.0,7.0,1061.0,"If you follow any of the popular blogs like Google’s research, FastML, Smola’s Adventures in Data Land, or one of the indie-pop ones like Edwin Chen’s blog, you’ve probably also used ModelZoo. Actually, if you’re like our boss, you affectionately call it “The Zoo”. (Actually x 2, if you have interesting blogs that you read, feel free to let us know!)Unfortunately, ModelZoo is only supported in Caffe. Fortunately, we’ve taken a look at the difference between the kernels in Keras, Theano, and Caffe for you, and after reading this blog, you’ll be able to load models from ModelZoo into any of your favorite Python tools.Why this post? Why not just download our Github code?In short, it’s better you figure out how these things work before you use them. That way, you’re better armed to use the latest TensorFlow and Neon toolboxes if you’re prototyping and transitioning your code to Caffe.So, there’s Hinton’s Dropout and then there’s Caffe’s Dropout…and they’re different. You might be wondering, “What’s the big deal?” Well sir, I have a name of a guy for you, and it’s Willy…Mr. Willy Nilly. One thing Willy Nilly likes is the number 4096. Another thing he likes is to introduce regularization (which includes Dropout) arbitrarily, and Bayesian theorists aren’t a fan. Those people try to fit their work into the probabilistic framework, and they’re trying to hold onto what semblance of theoretical bounds exist for neural networks. However, for you as a practitioner, understanding who’s doing what will save you hours of debugging code.We singled out Dropout because the way people have implemented it spans the gamut. There’s actually some history as to this variation, but no one really cared, because optimizing for it has almost universally produced similar results. Much of the discussion stems from how the chain rule is implemented since randomly throwing stuff away is apparently not really a differentiable operation. Passing gradients back (i.e., backpropagation) is a fun thing to do; there’s a “technically right” way to do it, and then there’s what’s works.Back to ModelZoo, where we’d recommend you note the only sentence of any substance in this section, and the sentence is as follows. While Keras and perhaps other packages multiply the gradients by the retention probability at inference time, Caffe does not. That is to say, if you have a dropout level of 0.2, your retention probability is 0.8, and at inference time, Keras will scale the output of your prediction by 0.8. So, download the ModelZoo *.caffemodels, but know that deploying them on Caffe will produce non-scaled results, whereas Keras will.Hinton explains the reason why you need to scale, and the intuition is as follows. If you’ve only got a portion of your signal seeping through to the next layer during training, you should scale the expectation of what the energy of your final result should be. Seems like a weird thing to care about, right? The argument that minimizes x is still the same as the argument that minimizes 2x. This turns out to be a problem when you’re passing multiple gradients back and don’t implement your layers uniformly. Caffe works in instances like Siamese Networks or Bilinear Networks, but should you scale your networks on two sides differently, don’t be surprised if you’re getting unexpected results.What does this look like in Francois’s code? Look at the “Dropout” code on Github, or in your installation folder under keras/layers/core.py. If you want to make your own layer for loading in the Dropout module, just comment out the part of the code that does this scaling:You can modify the original code, or you can create your own custom layer. (We’ve opted to keep our installation of Keras clean and just implemented a new class that extended MaskedLayer.) BTW, you should be careful in your use of Dropout. Our experience with them is that they regularize okay, but could contribute to vanishing gradients really quickly.Everyday except for Sunday and some holidays, a select few machine learning professors and some signal processing leaders meet in an undisclosed location in the early hours of the morning. The topic of their discussion is almost universally, “How do we get researchers and deep learning practitioners to code bugs into their programs?” One of the conclusions a while back was that the definition of convolution and dense matrix multiplication (or cross-correlation) should be exactly opposite of each other. That way, when people are building algorithms that call themselves “Convolutional Neural Networks”, no one will know which implementation is actually being used for the convolution portion itself.For those who don’t know, convolutions and sweeping matrix multiplication across an array of data, differ in that convolutions will be flipped before being slid across the array. From Wikipedia, the definition is:On the other hand, if you’re sweeping matrix multiplications across the array of data, you’re essentially doing cross-correlation, which on Wikipedia, looks like:Like we said, the only difference is that darned minus/plus sign, which caused us some headache.We happen to know that Theano and Caffe follow different philosophies. Once again, Caffe doesn’t bother with pleasantries and straight up codes efficient matrix multiplies. To load models from ModelZoo into either Keras and Theano will require the transformation because they strictly follow the definition of convolution. The easy fix is to flip it yourself when you’re loading the weights into your model. For 2D convolution, this looks like:weights=weights[:,:,::-1,::-1]Here, the variable “weights” will be inserted into your model’s parameters. You can set weights by indexing into the model. For example, say I want to set the 9th layer’s weights. I would type:model.layers[9].set_weights(weights)Incidentally, and this is important, when loading any *.caffemodel into Python, you may have to transpose it in order to use it. You can quickly find this out by loading it if you get an error, but we thought it worth noting.Alright, alright, we know what you’re really here for; just getting the code and running with it. So, we’ve got some example code that classifies using Keras and the VGG net from the web at our Git (see the link below). But, let’s go through it just a bit. Here’s a step by step account of what you need to do to use the VGG caffe model.And now you have the basics! Go ahead and take a look at our Github for some goodies. Let us know!Originally published at www.lab41.org on December 13, 2015.",13/12/2015,0,0.0,4.0,756.0,445.0,6.0,5.0,0.0,13.0,en
4162,Introduction: Reinforcement Learning with OpenAI Gym,Towards Data Science,ASHISH RANA,291.0,12.0,2114.0,"Understand the basic goto concepts to get a quick start on reinforcement learning and learn to test your algorithms with OpenAI gym to achieve research centric reproducible results.This article first walks you through the basics of reinforcement learning, its current advancements and a somewhat detailed practical use-case of autonomous driving. After that we get dirty with code and learn about OpenAI Gym, a tool often used by researchers for standardization and benchmarking results. When the coding section comes please open your terminal and get ready for some hands on.A time saver tip: You can directly skip to ‘Conceptual Understanding’ section if you want to skip basics and only want try out Open AI gym directly.Mainly three categories of learning are supervised, unsupervised and reinforcement. Let’s see the basic differences between them. In supervised learning we try to predict a target value or class where the input data for training is already having labels assigned to it. Whereas unsupervised learning uses unlabelled data for looking at patterns to make clusters, or doing PCA/anomaly detection. RL algorithms are like optimization procedures to find best methods to earn maximum reward i.e. give winning strategy to attain objective.Within Reinforcement Learning, there are multiple paradigms that attain this winning strategy in their own way. In complex situations calculating exact winning strategy or exact reward-value function becomes really hard, especially where our agents start learning from interactions rather than prior-gained experience. For example, a dog finding its favorite toy hidden in a new house. The dog will search the house and get an approximate idea about house schematics based on its interactions. But, still there might be a very good chance that the dog will find it incredibly hard to find the hidden toy again if it’s located in a new unexplored location.This interaction based learning is also popularly known as model-free learning where the agent doesn’t have exact knowledge of the environment i.e. agent doesn’t store the knowledge of state-action probability transition function. And it tries to approximate either winning strategy (i.e. policy) or reward-value gains (i.e. value function). The same dog searching for his hidden toy in his owner’s house would have previous experience i.e. prior environment knowledge and would be comfortable searching for the toy in any location within the house. And this type of learning for the hidden toy finding is known as model-based learning in RL.By very definition in reinforcement learning an agent takes action in the given environment either in continuous or discrete manner to maximize some notion of reward that is coded into it. Sounds too profound, well it is with a research base dating way back to classical behaviorist psychology, game theory, optimization algorithms etc. But, good for us, a lot of ‘setting the stage’ work has already been done for us to kick-start us directly into the problem formulation world and discover new things.Essentially, the most important of them all is that reinforcement learning scenarios for an agent in a completely known/observable deterministic environment can be formulated as a dynamic programming problem. Fundamentally meaning that the agent has to perform a series of steps in a systematic manner so that it can learn the ideal solution and it will receive guidance from reward values. The equation that expresses such a scenario in mathematical terms is known as the Bellman equation which we will see in action in some time.Let’s first qualitatively define the concept of agent and environment formally before proceeding further for understanding technical details about RL. Environment is the universe of agents which changes the state of agent with given action performed on it. Agent is the system that perceives the environment via sensors and performs actions with actuators. In the below situations Homer(Left) and Bart(right) are our agents and World is their environment. They performs actions within it and improve their state of being by getting happiness or contentment as reward.Starting with the most popular game series since IBM’s Deep Blue v/s Kasparov which created huge hype for AI. And the inhuman-like awareness for the deep reinforcement learning agent in AlpaGo v/s Lee Sedol in Go competition series. Mastering a game with more board configuration than atoms in the Universe against a den 9 master shows the power such smart systems hold. Also recent RL research breakthroughs and wins against World Pros with Dota bots from OpenAI is also commendable. With agents getting trained to handle such complex and dynamic environments, mastering these games are an example of testing the limits of AI agents for handling very complex hierarchical-like situations. Application wise, already complex applications like driverless cars, smart drones are operating in the real world.Let’s understand some more fundamentals of reinforcement learning and then start with OpenAI gym to make our own agent. After that I’ll recommend you to move towards Deep RL and tackle more complex situations. Scope of all the RL applications is beyond imagination and can be applied to so many domains like time-series predictions, healthcare, supply-chain automation and so on. But, here we’ll discuss one of the most popular application use-case of self driving autonomous vehicles and navigation tasks in general.The unique ability to run algorithms on the same state over and over which helps it to learn best action for that particular state to progress to the ideal next state, which essentially is equivalent to breaking the construct of time for humans to gain infinite learning experience at almost no time.With RL as a framework agent acts with certain actions which transform the state of the agent, each action is associated with reward value. It also uses a policy to determine its next action, which is constituted of a sequence of steps that maps states-action pairs to calculated reward values. A policy can be qualitatively defined as an agent’s way of behaving at a given time. Now, policies can be deterministic and stochastic, finding an optimal policy is the key for solving a given task.Also, Different actions in different states will have different associated reward values. Like the ‘Fire’ command in a game of Pocket Tanks can’t always have the same reward value associated with it, as sometimes it’s better to retain a position which is strategically good. To handle this complex dynamic problem with such huge combinations in a planned manner we need a Q-value( or action-value ) table which stores a map of state-action pairs to rewards.Now, defining the environment in RL’s context as a functional component, it simply takes action at a given state as input and returns a new state and reward value associated with action-state pair.Interestingly enough though, neural nets enter the picture with their ability to learn state-action pairs rewards with ease when the environment becomes highly complex to handle with computationally restrictive Iterative algorithms and this is known as Deep RL. Like playing those earlier games like Mario, Atari, PAC-MAN etc.Here, we will limit to simple Q-Learning only i.e. w/o neural networks, where Q function maps state-action pairs to a maximum with combination of immediate reward plus future rewards i.e. for new states learned value is current reward plus future estimate of rewards. Quantifying it into an equation with different parameters like learning rate and discount factor to guide agent’s choice of action. We arrive at the following equation: Structurally, it holds much similarity to Bellman’s equation.A 2016 Nature survey indicated that more than 70 percent of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments.OpenAI is created for removing this problem of lack of standardization in papers along with an aim to create better benchmarks by giving versatile numbers of environments with great ease of setting up. Aim of this tool is to increase reproducibility in the field of AI and provide tools with which everyone can learn about the basics of AI.Open your terminal and get ready for some CTRL+C and CTRL+V work !! But, of course I’ll recommend against it.What is OpenAI gym ? This python library gives us a huge number of test environments to work on our RL agent’s algorithms with shared interfaces for writing general algorithms and testing them. Let’s get started, just type pip install gym on the terminal for easy install, you’ll get some classic environment to start working on your agent. Copy the code below and run it, your environment will get loaded. You can check out other available environments like Algorithmic, Atari, Box2D and Robotics here and use the second listed code snippet component below for listing all the available environments.When object interacts with environment with an action then step() function returns observation which generally represents environments next state, reward a float of reward in previous action, done when it’s time to reset the environment or goal achieved and info a dict for debugging, it can be used for learning if it contains raw probabilities of environment’s last state. See how it works from the code snippet below. Also, observe how observation of type Space is different for different environments.What is action_space in above code? action-space & observation-space describes what is the valid format of action & state parameters for that particular env to work on with. Just take a look at values returned.Discrete is non-negative possible values, above 0 or 1 are equivalent to left and right movement for CartPole balancing. Box represent n-dim array. These standard interfaces can help in writing general codes for different environments. As we can simply check the bounds env.observation_space.high/[low] and code them into our general algorithm.Here, we are using Python3.x for the highlighted code sample of Q-Learning algorithm below.Let’s start building our Q-table algorithm, which will try to solve the FrozenLake navigation environment. In this environment the aim is to reach the goal, on a frozen lake that might have some holes in it. Here is how the surface is the depicted by this Toy-Text environment.Q-table contains state-action pairs mapping to reward. So, we will construct an array which maps different states and actions to reward values during execution of algorithm. Its dimension will clearly |states|x|actions|. Let’s write it in code for the Q-learning Algorithm.If you are interested in working with other environments, you can opt to work along the lines of a cleaner code snippet highlighted below.But do remember even with a common interface the code complexity will be different for different environments. In the above environment we only had a simple 64 state environment with few actions only to handle. We were able to store them in a two dimensional array for reward mapping very easily. Now, Let’s consider more complicated environment cases like Atari envs and look at the approach that is needed.observation_space is needed to be represented by a 210x160x3 tensor which makes our Q-table even more complicated. Also, each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from {2,3,4}. With 33,600 pixels in RGB channels with values ranging from 0–255 the environment clearly has become overly complicated. A simple Q-learning approach can’t be used here. Deep learning with its CNN architecture is the solution for this problem and a topic you should focus on for follow up of this introductory article.Now, with the above tutorial you have the basic knowledge about the gym and all you need to get started with it. Gym is also TensorFlow & PyTorch compatible but I haven’t used them here to keep the tutorial simple. After trying out the gym package you must get started with stable-baselines3 for learning the good implementations of RL algorithms to compare your implementations. To see all the OpenAI tools check out their github page. RL is an expanding field with applications in a huge number of domains and it will play an important role in future AI breakthroughs. Hope you continue with your RL journey & thanks for reading!!Yes, sharing with you shameful yet glorious starter project plugs, in case you are just getting started with reinforcement learning. The additional advantage of picking up these practical projects is getting complete mentor support & wonderful free reinforcement learning resources like books, videos and so on. Interestingly enough though, there are two liveProjects that I’ll recommend to you pursuing further:www.manning.comAdditionally, the first milestone is free practice for this liveProject. Even if you don’t move forward with the liveProject, I promise that you’ll learn a great deal about the autonomous driving RL task just by practically following along the first milestone. Cheers!Important Note: If you are here before July-26-2021 and are interested in carrying forward with the first liveProject. Please, use the code lprana and you’ll get 45% off on the project. Kudos!!www.manning.comChao for now! In case you select the first project, be seeing you there…",21/09/2018,8,29.0,32.0,972.0,490.0,11.0,2.0,0.0,11.0,en
4163,Reinforcement Learning (Pekiştirmeli Öğrenme) — İnsan Beyniyle Aradaki Fark Kapanırken,Medium,Volkan Levent Soylu,44.0,4.0,785.0,"Hatırlarsınız, önce 1997’de DeepMind’ın bilgisayarı Deep Blue Kasparov’u satrançta yenmişti. Bir sonraki adımdaysa AlphaGo önce dünya Go şampiyonu Ke Jie’yi, sonrasında da bir üst model AlphaGo Zero en iyi Go oyuncularından biri sayılan Lee Sedol’u 2016’da 3 kez yendi. Şimdi yeni adımın StarCraft olacağı söyleniyor; ki bilen bilir StarCraft koordinasyon, hızlı karar alma, dikkat olarak epey zorlayıcı bir oyundur. AlphaGo bütün bunları Reinforcement Learning (Pekiştirmeli Öğrenme) ile yaptı.Pekiştirmeli Öğrenme, Makine Öğrenmesi’nin alt kollarından biri. Makine Öğrenmesi’nde genellikle Markov Karar Süreci adı verilen bir model kullanılıyor. Bu model yapay zekânın önceden bilgilendirilmesine ve yönlendirilmesine dayalı. Kesin bir neden sonuç ilişkisi var. Pekiştirmeli Öğrenme süreciyse ön bilgi yerine gözlem ve seçimde bulunmak üzerine kurulu. Neden sonuç ilişkisinin nasıl kurulduğu bilgisi yapay zekâya verilmiyor (ya da çok az veriliyor), yapay zekâ kendisi öğreniyor.Bu sistemin özelliği, yapay zekayı bir sonuca koşullandırması. Bu sonuç her zaman en yüksek ödül değerinde. Daha az, daha farklı bir ödül ya da hiç ödül alamaması yapay zekâ için bir ceza olarak görülüyor. Dolayısıyla yapay zekanın motivasyonun her zaman tek yönde işlediği söylenebilir.Sistem nasıl işliyor? Adımlar kabaca şöyle:1-Yapay zekâ içinde bulunduğu sistem hakkındaki bilgisi yetersiz olduğu için neden sonuç arasında bağ kurmak amacıyla gözlem yapıyor.2-Gözlemin ardından yapay zekâ önündeki seçenekler arasında bir tercihte bulunmaya zorlanıyor. En yüksek ödüle güdülü olduğu için, o ödüle en uygun olacak şekilde bir karar vererek harekete geçiyor.3-İlk adımından sonra yapay zekâ tekrar gözlemde bulunarak önüne çıkan yeni seçeneklere (ortama) bakıyor. Burada önemli olan nokta, yapay zekanın ilk adımdan sonra hedefine ulaşma konusunda nasıl doğru karar alabileceğini öğrenmiş olması. Yani yapay zekanın sistem içindeki her adımı bir tür birikim demek.Şunu da vurgulayayım: Pekiştirmeli Öğrenme’de hedeflerden birisi, yapay zekâ için varılacak hedefin mesafesinin sonsuza yakın olması. Yani bir nevi ufkun ucunda görülen hedefe sürekli ilerlese de hedefin hiç yaklaşmaması gibi bir koşulun yaratılmak istenmesi söz konusu. Bu modellemede her şey yukarıdaki üç adımdan ibaret gibi gözükse de sistemi gereği yapay zekânın sonsuza yakın bir hedef-gözlem-harekete geçme-yeniden gözlem döngüsüne girmesi önemli.Makine Öğrenmesi’nin gözlemleyerek ve tekrar ederek daha çok şey modellemek üzerine kurulu olduğunu biliyoruz. Pekiştirmeli Öğrenme’deyse bu süreç çok farklı bir boyuta erişmiş durumda. İlk baştaki Go oyunundaki gelişim üzerinden gideyim: İlk AlphaGo modelleri gerçek Go oyuncularıyla 100 binin üzerinde maça çıkmışken, geliştirilmiş model AlphaGo Zero sadece kendine karşı oynayarak 30 milyon maç yapmış. Ve şu an kendisinden alt modellerin hepsini yenebilecek seviyeye ulaştığı açıklandı. 100 bin ile 30 milyon biraz ters bir orantı gibi gelebilir. Ancak buradaki önemli nokta, Zero’nun sadece kendi kendisiyle maç yaparak bu seviyeye çıkmış olması. Yani kendisinden farklı şekilde düşünen, hareket eden bir yapay zekâ ile hiç karşılaşmadı (siz sadece kendinizle satranç, Go ya da başka bir şey oynasaydınız, ne kadar yeni bir şey öğrenebilirdiniz?). Tek başına bu bile sabit bir şekilde duran bir yapay zekânın gözlemden ve deneme yanılmadan başka hiçbir şeye başvurmadan inanılmaz bir eşiğe varması demek oluyor. Yapay zekanın edindiği işlem hacmi ve parametrelerin bu şekilde artışı beynimizin hücre sayısı ve işlem kapasitesinin evrimsel gelişiminin bir süre sonra kümülatif oranla artması ile hayli benzerlik gösteriyor; ki bu düşünülmeyi hak eden apayrı bir konu.Go ya da satranç oyununda yapay zekanın rakibine karşı yaşadığı zorlukların, gerçek hayatta kullanıldığı alanda karşısına çıkan sorun, beklenmedik bir şey gibi koşulları sürekli değiştiren etmenlere denk düştüğü söylenebilir. Yani yüzlerce değişkenin her seferinde kaotik bir şekilde olasılıkları değiştirdiği bir ortamda, her seferinde hedefi sürekli doğru yönde tutma ihtimali demek oluyor. Bu açıdan bakarsak Pekiştirmeli Öğrenme büyük bir sıçrama noktası olarak görülebilir. Son derece önemli bir şey bu, zira yapay zekânın kullanıldığı alanların giderek artmasıyla bu zorlayıcı koşulların daha çok ortaya çıkacağı kesin. Örneğin değişim gösteren hava durumu, sıcaklık, trafik, saat gibi faktörlere bakarak lojistik konusunda an be an yeni çözümler veya önlemler üretebilir. Kişinin genetik haritası, sürekli gelişen yeni vücut değerleri üzerinden birtakım öngörülerde ve teşhislerde bulunabilir. Bir basketbol takımında, her oyuncunun performansına, sakatlıklara, rakibin form durumuna, hatta oynadığı sahaya göre oluşan sayısız varyasyon arasında maçın sonucuna dair bir değer aralığı yaratabilir. En önemli değişkenleri bularak “bu oyuncuyu bu akşam oynatmazsan, bu taktiğe başvurmazsan daha iyi olur” gibi öneriler sunabilir. Ve en bilinen örnek üzerinden gidersem, bir robota karşısına çıkan engellere göre zıplaması, yükseğe hatta daha yükseğe zıplaması öğretilebilir. Yerine göre eğilmeye, koşmaya hatta yolunu değiştirmeye zorlanabilir, oldukça karmaşık da olsa bir labirentten çıkacak şekilde tasarlanabilir.Peki bu sistemin handikapları yok mu? En büyük eksiklerden biri, bu şekilde çalışan sistemin çok fazla işlemci gücü ve bellek kullanmasının gerekiyor oluşu. Bu da bilgisayar kapasitesi ve elektrik tüketimi demek oluyor. Bunun dışında işin mühendisliğiyle ilgili teorik engeller de mevcut. Ama şu an gelinen noktada, sadece kendini gözlemleyerek Go’yu çok iyi seviyede öğrenen ve rakiplerini geride bırakan bir sistemin önüne fazla bir şey engel çıkamaz diye düşünüyorum.Yukarıda da dediğim gibi, yapay zekânın insan beyni ile arasındaki farkın pratikte giderek kapanması ise ayrıca düşünülmesi gereken bir konu.",23/11/2017,0,31.0,1.0,658.0,397.0,4.0,0.0,0.0,0.0,tr
4164,Sugerencias para definir un menú de navegación,Capire.info,editorCapire.info,29.0,5.0,903.0,"Escribe Jorge Garrido G.Algunas sugerencias sobre cómo construir un menú que entregue orientación y control al usuario, sin perder claridad en su forma de presentarse.¿Qué es un menú? ¿Qué representa? ¿Qué comunica? ¿Para qué sirve? ¿Todo sitio web o aplicación debe tener un menú?Las respuestas no son tan sencillas ni están tan claras; mucho menos se puede considerar este como un tema superado. No lo creo por lo que percibo cuando navego, periódicamente. Veo menús poco cuidados, incomprendidos, mal diseñados, desenfocados.Hay muchos otros recursos, fuera de los menús de navegación, para destacar los contenidos más importantes: Accesos directos con características gráficas sobresalientes, listados de hotlinks, nubes de tags y un largo etcétera. ¿Por qué darle toda la responsabilidad al menú?Para empezar: un menú debe ser el fiel reflejo de la estructura lógica que organiza la información. Un menú -y he aquí el error más típico en este sentido- no es un conjunto de accesos directos o hotlinks a lo más usado.Para empezar: un menú debe ser el fiel reflejo de la estructura lógica que organiza la información. Un menú -y he aquí el error más típico en este sentido- no es un conjunto de accesos directos o hotlinks a lo más usado.¿Debe el menú hacerse cargo de poner de inmediato a la vista lo más importante?Veámoslo con un ejemplo: un hipotético sitio web de literatura chilena, el cual parte con un menú de navegación temático -bastante lógico- que identifica géneros literarios:Novela / Cuento / Poesía / DramaturgiaImaginemos que en este sitio “Pablo Neruda” -premio Nobel y posiblemente el escritor chileno más conocido en el mundo- es el contenido más importante. ¿Debe por eso “Pablo Neruda” ser una de las opciones del menú principal? Es una jugada por la que se suele optar: romper el esquema lógico de organización de la información en beneficio de un contenido particular por el cual, se cree, vale la pena la excepción.Siguiendo con el ejemplo. El menú quedaría así:Novela / Cuento / Poesía / Dramaturgia / Pablo Neruda…El escenario no parece complejo. Sigue siendo un menú abordable. El punto es que un menú no se puede quedar sólo en la información actual, debe estar preparado para albergar a la información potencial. Si una regla se rompe, hay que considerar las posibles consecuencias. ¿Qué pasa si luego se resuelve destacar el tema de las guerrillas literarias y otros dos poetas, Pablo de Rokha y Vicente Huidobro, adquieren el mismo valor que Neruda? ¿Y por qué no sumar a Gabriela Mistral, que también ganó el Nobel? ¿Y si el próximo año lo gana Nicanor Parra?Entonces el menú ya se nos empieza a escapar de las manos:Novela / Cuento /Poesía / Dramaturgia / Pablo Neruda / Pablo de Rokha / Vicente Huidobro / Gabriela Mistral / Nicanor Parra…Como se aprecia, un menú que comenzó como una estructura lógica para agrupar géneros literarios, termina convertido en un híbrido que mezcla indistintamente dos criterios de clasificación de la información: la mitad mantiene la lógica de géneros y la otra mitad es un listado de nombres y seudónimos de escritores.Alguien podrá contraargumentar: “¿Entonces cómo destaco a Neruda si es el contenido que más interés despierta en el sitio?”. Hay muchos otros recursos, fuera de los menús de navegación, para destacar los contenidos más importantes: Accesos directos con características gráficas sobresalientes, listados de hotlinks, nubes de tags y un largo etcétera. ¿Por qué darle toda la responsabilidad al menú?Pensemos en un supermercado. El equivalente al menú es la estructura lógica de distribución de productos emparentados en distintos pasillos; y el equivalente a los accesos directos destacados son las promociones ubicadas en las esquinas y las mesas con promotoras. Si aparece un nuevo producto interesante -digamos, una nueva cerveza artesanal- no se redefine la lógica de pasillos para darle cabida; el producto se ubica donde le corresponde, junto a las otras cervezas y además, probablemente, tendrá un espacio promocional extra en otro lugar (el equivalente al hotlink).Lo interesante de citar a un supermercado es que se trata de una estructura pensada para vender productos. En ningún caso es neutra. Pero aun así, dentro de muchas lógicas orientadas al negocio, opera una que tiene que ver simplemente con facilitar la búsqueda exploratoria al cliente; por eso las etiquetas que encabezan los pasillos son de productos genéricos, lo suficientemente amplios como para abarcar a una gama relevante de subproductos, algunos de ellos, indudablemente, más rentables y más apetecidos que otros.Dos conceptos que operan detrás del ejemplo de los pasillos de los supermercados y que sustentan a los menús de navegación son la Orientación y el Control. Mientras un menú permita a los usuarios “orientarse”, es decir, saber dónde están, hacia qué otros lugares pueden dirigirse y en qué sección(es) tiene sentido buscar lo que necesitan; y a la vez “Tener el control”, gracias a la identificación de un lugar que constituye la columna vertebral, permanente, para moverse desde los lugares más generales hasta los más recónditos, donde haya clasificaciones que responden a criterios identificables claramente, donde todo el contenido cabe de alguna manera, donde existen categorías significativas y en lo posible excluyentes; mientras el menú garantice todo esto, no es necesario sobrecargarlo y lo recomendable es pensar en otros recursos para mostrar la jerarquía de los contenidos.Algunas sugerencias a la hora de definir un menú de navegación:*************Artículo publicado en Capire.info bajo licencia Creative Commons. Toda reproducción total o parcial debe incluir la fuente www.capire.info y a su autor.",18/06/2008,0,14.0,0.0,672.0,223.0,1.0,1.0,0.0,2.0,es
4165,How to implement an Adam Optimizer from Scratch,Towards Data Science,Enoch Kan,533.0,3.0,430.0,"It’s not as hard as you think!Tl;dr if you want to skip the tutorial. Here is the notebook I created.Adam is algorithm the optimizes stochastic objective functions based on adaptive estimates of moments. The update rule of Adam is a combination of momentum and the RMSProp optimizer.The rules are simple. Code Adam from scratch without the help of any external ML libraries such as PyTorch, Keras, Chainer or Tensorflow. Only libraries we are allowed to use arenumpy and math .(っ^‿^)っ(っ^‿^)っ(っ^‿^)っ(っ^‿^)っ(っ^‿^)っ(っ^‿^)っThe easiest way to learn how Adam’s works is to watch Andrew Ng’s video. Alternatively, you can read Adam’s original paper to get a better understanding of the motivation and intuition behind it.Two values that Adam depend on are β₁ and β₂. β₁ is the exponential decay of the rate for the first moment estimates, and its literature value is 0.9. β₂ is the exponential decay rate for the second-moment estimates, and its literature value is 0.999. Both literature values work well with most datasets.On a given iteration t, we can calculate the moving averages based on parameters β₁, β₂, and gradient gt. Since most algorithms that depend on moving averages such as SGD and RMSProp are biased, we need an extra step to correct the bias. This is known as the bias correction step:Finally, we can update the parameters (weights and biases) based on the calculated moving averages with a step size η:To summarize, we need to define several variables: 1st-order exponential decay β₁, 2nd-order exponential decay β₂, step size η and a small value ε to prevent zero-division. Additionally, we define m_dw , v_dw , m_db and v_db as the mean and uncentered variance from the previous time step of the gradients of weights and biases dw and db.Recall that Adam relies on two important moments: a 1st-order moment estimate of the mean and the 2nd-order moment estimate of the variance. Using these moment estimates we can update the weights and biases given an appropriate step size.To test our implementation, we will first need to define a loss function and its respective gradient function. A gradient function can be obtained by simply taking the derivative of the loss function. For example:Note that we also define an additional function to check the convergence based on the fact that weights will not change when convergence is reached. Finally, we can iteratively update the weights and biases using our constructed Adam optimizer and see if they converge:Looking at the results, convergence is reached under 750 iterations. Great success!Feel free to check out my other stories and github projects. Have a good day!",06/11/2020,0,3.0,13.0,555.0,335.0,5.0,0.0,0.0,5.0,en
4166,Machine Learning Algorithms For Beginners with Code Examples in Python,Towards AI,Towards AI Editorial Team,24000.0,26.0,3547.0,"Author(s): Pratik Shukla, Roberto Iriondo, Sherwin ChenLast updated April 14, 2021members.towardsai.netMachine learning (ML) is rapidly changing the world, from diverse types of applications and research pursued in industry and academia. Machine learning is affecting every part of our daily lives. From voice assistants using NLP and machine learning to make appointments, check our calendar, and play music, to programmatic advertisements — that are so accurate that they can predict what we will need before we even think of it.More often than not, the complexity of the scientific field of machine learning can be overwhelming, making keeping up with “what is important” a very challenging task. However, to make sure that we provide a learning path to those who seek to learn machine learning, but are new to these concepts. In this article, we look at the most critical basic algorithms that hopefully make your machine learning journey less challenging.Any suggestions or feedback is crucial to continue to improve. Please let us know in the comments if you have any.📚 Check out our tutorial diving into simple linear regression with math and Python. 📚A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. ~ Tom M. Mitchell [1]Machine learning behaves similarly to the growth of a child. As a child grows, her experience E in performing task T increases, which results in higher performance measure (P).For instance, we give a “shape sorting block” toy to a child. (Now we all know that in this toy, we have different shapes and shape holes). In this case, our task T is to find an appropriate shape hole for a shape. Afterward, the child observes the shape and tries to fit it in a shaped hole. Let us say that this toy has three shapes: a circle, a triangle, and a square. In her first attempt at finding a shaped hole, her performance measure(P) is 1/3, which means that the child found 1 out of 3 correct shape holes.Second, the child tries it another time and notices that she is a little experienced in this task. Considering the experience gained (E), the child tries this task another time, and when measuring the performance(P), it turns out to be 2/3. After repeating this task (T) 100 times, the baby now figured out which shape goes into which shape hole.So her experience (E) increased, her performance(P) also increased, and then we notice that as the number of attempts at this toy increases. The performance also increases, which results in higher accuracy.Such execution is similar to machine learning. What a machine does is, it takes a task (T), executes it, and measures its performance (P). Now a machine has a large number of data, so as it processes that data, its experience (E) increases over time, resulting in a higher performance measure (P). So after going through all the data, our machine learning model’s accuracy increases, which means that the predictions made by our model will be very accurate.Another definition of machine learning by Arthur Samuel:Machine Learning is the subfield of computer science that gives “computers the ability to learn without being explicitly programmed.” ~ Arthur Samuel [2]Let us try to understand this definition: It states “learn without being explicitly programmed” — which means that we are not going to teach the computer with a specific set of rules, but instead, what we are going to do is feed the computer with enough data and give it time to learn from it, by making its own mistakes and improve upon those. For example, We did not teach the child how to fit in the shapes, but by performing the same task several times, the child learned to fit the shapes in the toy by herself.Therefore, we can say that we did not explicitly teach the child how to fit the shapes. We do the same thing with machines. We give it enough data to work on and feed it with the information we want from it. So it processes the data and predicts the data accurately.For instance, we have a set of images of cats and dogs. What we want to do is classify them into a group of cats and dogs. To do that we need to find out different animal features, such as:We form a vector on each of these questions’ answers. Next, we apply a set of rules such as:If height > 1 feet and weight > 15 lbs, then it could be a cat.Now, we have to make such a set of rules for every data point. Furthermore, we place a decision tree of if, else if, else statements and check whether it falls into one of the categories.Let us assume that the result of this experiment was not fruitful as it misclassified many of the animals, which gives us an excellent opportunity to use machine learning.What machine learning does is process the data with different kinds of algorithms and tells us which feature is more important to determine whether it is a cat or a dog. So instead of applying many sets of rules, we can simplify it based on two or three features, and as a result, it gives us a higher accuracy. The previous method was not generalized enough to make predictions.Machine learning models helps us in many tasks, such as:A machine learning model is a question/answering system that takes care of processing machine-learning related tasks. Think of it as an algorithm system that represents data when solving problems. The methods we will tackle below are beneficial for industry-related purposes to tackle business problems.For instance, let us imagine that we are working on Google Adwords’ ML system, and our task is to implementing an ML algorithm to convey a particular demographic or area using data. Such a task aims to go from using data to gather valuable insights to improve business outcomes.We use regression algorithms for predicting continuous values.Regression algorithms:We use classification algorithms for predicting a set of items’ class or category.Classification algorithms:We use clustering algorithms for summarization or to structure data.Clustering algorithms:We use association algorithms for associating co-occurring items or events.Association algorithms:We use anomaly detection for discovering abnormal activities and unusual cases like fraud detection.We use sequential pattern mining for predicting the next data events between data examples in a sequence.We use dimensionality reduction for reducing the size of data to extract only useful features from a dataset.We use recommenders algorithms to build recommendation engines.Examples:Nowadays, we hear many buzz words like artificial intelligence, machine learning, deep learning, and others.📚 Check out our editorial recommendations on the best machine learning books. 📚Artificial intelligence (AI), as defined by Professor Andrew Moore, is the science and engineering of making computers behave in ways that, until recently, we thought required human intelligence [4].These include:As defined by Professor Tom Mitchell, machine learning refers to a scientific branch of AI, which focuses on the study of computer algorithms that allow computer programs to automatically improve through experience [3].These include:Deep learning is a subset of machine learning in which layered neural networks, combined with high computing power and large datasets, can create powerful machine learning models. [3]Python is a popular and general-purpose programming language. We can write machine learning algorithms using Python, and it works well. The reason why Python is so popular among data scientists is that Python has a diverse variety of modules and libraries already implemented that make our life more comfortable.Let us have a brief look at some exciting Python libraries.Machine learning algorithms classify into two groups :Goal: Predict class or value label.Supervised learning is a branch of machine learning(perhaps it is the mainstream of machine/deep learning for now) related to inferring a function from labeled training data. Training data consists of a set of *(input, target)* pairs, where the input could be a vector of features, and the target instructs what we desire for the function to output. Depending on the type of the *target*, we can roughly divide supervised learning into two categories: classification and regression. Classification involves categorical targets; examples ranging from some simple cases, such as image classification, to some advanced topics, such as machine translations and image caption. Regression involves continuous targets. Its applications include stock prediction, image masking, and others- which all fall in this category.To understand what supervised learning is, we will use an example. For instance, we give a child 100 stuffed animals in which there are ten animals of each kind like ten lions, ten monkeys, ten elephants, and others. Next, we teach the kid to recognize the different types of animals based on different characteristics (features) of an animal. Such as if its color is orange, then it might be a lion. If it is a big animal with a trunk, then it may be an elephant.We teach the kid how to differentiate animals, this can be an example of supervised learning. Now when we give the kid different animals, he should be able to classify them into an appropriate animal group.For the sake of this example, we notice that 8/10 of his classifications were correct. So we can say that the kid has done a pretty good job. The same applies to computers. We provide them with thousands of data points with its actual labeled values (Labeled data is classified data into different groups along with its feature values). Then it learns from its different characteristics in its training period. After the training period is over, we can use our trained model to make predictions. Keep in mind that we already fed the machine with labeled data, so its prediction algorithm is based on supervised learning. In short, we can say that the predictions by this example are based on labeled data.Example of supervised learning algorithms :Goal: Determine data patterns/groupings.In contrast to supervised learning. Unsupervised learning infers from unlabeled data, a function that describes hidden structures in data.Perhaps the most basic type of unsupervised learning is dimension reduction methods, such as PCA, t-SNE, while PCA is generally used in data preprocessing, and t-SNE usually used in data visualization.A more advanced branch is clustering, which explores the hidden patterns in data and then makes predictions on them; examples include K-mean clustering, Gaussian mixture models, hidden Markov models, and others.Along with the renaissance of deep learning, unsupervised learning gains more and more attention because it frees us from manually labeling data. In light of deep learning, we consider two kinds of unsupervised learning: representation learning and generative models.Representation learning aims to distill a high-level representative feature that is useful for some downstream tasks, while generative models intend to reproduce the input data from some hidden parameters.Unsupervised learning works as it sounds. In this type of algorithms, we do not have labeled data. So the machine has to process the input data and try to make conclusions about the output. For example, remember the kid whom we gave a shape toy? In this case, he would learn from its own mistakes to find the perfect shape hole for different shapes.But the catch is that we are not feeding the child by teaching the methods to fit the shapes (for machine learning purposes called labeled data). However, the child learns from the toy’s different characteristics and tries to make conclusions about them. In short, the predictions are based on unlabeled data.Examples of unsupervised learning algorithms:For this article, we will use a few types of regression algorithms with coding samples in Python.Linear regression is a statistical approach that models the relationship between input features and output. The input features are called the independent variables, and the output is called a dependent variable. Our goal here is to predict the value of the output based on the input features by multiplying it with its optimal coefficients.(1) To predict sales of products.(2) To predict economic growth.(3) To predict petroleum prices.(4) To predict the emission of a new car.(5) Impact of GPA on college admissions.There are two types of linear regression :In simple linear regression, we predict the output/dependent variable based on only one input feature. The simple linear regression is given by:Below we are going to implement simple linear regression using the sklearn library in Python.a. Import required libraries:Since we are going to use various libraries for calculations, we need to import them.b. Read the CSV file:We check the first five rows of our dataset. In this case, we are using a vehicle model dataset — please check out the dataset on Softlayer IBM.c. Select the features we want to consider in predicting values:Here our goal is to predict the value of “co2 emissions” from the value of “engine size” in our dataset.d. Plot the data:We can visualize our data on a scatter plot.e. Divide the data into training and testing data:To check the accuracy of a model, we are going to divide our data into training and testing datasets. We will use training data to train our model, and then we will check the accuracy of our model using the testing dataset.f. Training our model:Here is how we can train our model and find the coefficients for our best-fit regression line.g. Plot the best fit line:Based on the coefficients, we can plot the best fit line for our dataset.h. Prediction function:We are going to use a prediction function for our testing dataset.i. Predicting co2 emissions:Predicting the values of co2 emissions based on the regression line.j. Checking accuracy for test data :We can check the accuracy of a model by comparing the actual values with the predicted values in our dataset.Putting it all together:Launch it on Google Colab:colab.research.google.comIn simple linear regression, we were only able to consider one input feature for predicting the value of the output feature. However, in Multivariable Linear Regression, we can predict the output based on more than one input feature. Here is the formula for multivariable linear regression.a. Import the required libraries:b. Read the CSV file :c. Define X and Y:X stores the input features we want to consider, and Y stores the value of output.d. Divide data into a testing and training dataset:Here we are going to use 80% data in training and 20% data in testing.e. Train our model :Here we are going to train our model with 80% of the data.f. Find the coefficients of input features :Now we need to know which feature has a more significant effect on the output variable. For that, we are going to print the coefficient values. Note that the negative coefficient means it has an inverse effect on the output. i.e., if the value of that features increases, then the output value decreases.g. Predict the values:h. Accuracy of the model:Now notice that here we used the same dataset for simple and multivariable linear regression. We can notice that the accuracy of multivariable linear regression is far better than the accuracy of simple linear regression.Putting it all together:Launch it on Google Colab:colab.research.google.comSometimes we have data that does not merely follow a linear trend. We sometimes have data that follows a polynomial trend. Therefore, we are going to use polynomial regression.Before digging into its implementation, we need to know how the graphs of some primary polynomial data look.a. Graph for Y=X:b. Graph for Y = X²:c. Graph for Y = X³:d. Graph with more than one polynomials: Y = X³+X²+X:In the graph above, we can see that the red dots show the graph for Y=X³+X²+X and the blue dots shows the graph for Y = X³. Here we can see that the most prominent power influences the shape of our graph.Below is the formula for polynomial regression:Now in the previous regression models, we used sci-kit learn library for implementation. Now in this, we are going to use Normal Equation to implement it. Here notice that we can use scikit-learn for implementing polynomial regression also, but another method will give us an insight into how it works.The equation goes as follows:In the equation above:θ: hypothesis parameters that define it the best.X: input feature value of each instance.Y: Output value of each instance.The main matrix in the standard equation:a. Import the required libraries:b. Generate the data points:We are going to generate a dataset for implementing our polynomial regression.c. Initialize x,x²,x³ vectors:We are taking the maximum power of x as 3. So our X matrix will have X, X², X³.d. Column-1 of X matrix:The 1st column of the main matrix X will always be 1 because it holds the coefficient of beta_0.e. Form the complete x matrix:Look at the matrix X at the start of this implementation. We are going to create it by appending vectors.f. Transpose of the matrix:We are going to calculate the value of theta step-by-step. First, we need to find the transpose of the matrix.g. Matrix multiplication:After finding the transpose, we need to multiply it with the original matrix. Keep in mind that we are going to implement it with a normal equation, so we have to follow its rules.h. The inverse of a matrix:Finding the inverse of the matrix and storing it in temp1.i. Matrix multiplication:Finding the multiplication of transposed X and the Y vector and storing it in the temp2 variable.j. Coefficient values:To find the coefficient values, we need to multiply temp1 and temp2. See the Normal Equation formula.k. Store the coefficients in variables:Storing those coefficient values in different variables.l. Plot the data with curve:Plotting the data with the regression curve.m. Prediction function:Now we are going to predict the output using the regression curve.n. Error function:Calculate the error using mean squared error function.o. Calculate the error:Putting it all together:Launch it on Google Colab:colab.research.google.comSome real-life examples of exponential growth:1. Microorganisms in cultures.2. Spoilage of food.3. Human Population.4. Compound Interest.5. Pandemics (Such as Covid-19).6. Ebola Epidemic.7. Invasive Species.8. Fire.9. Cancer Cells.10. Smartphone Uptake and Sale.The formula for exponential regression is as follow:In this case, we are going to use the scikit-learn library to find the coefficient values such as a, b, c.a. Import the required libraries:b. Insert the data points:c. Implement the exponential function algorithm:d. Apply optimal parameters and covariance:Here we use curve_fit to find the optimal parameter values. It returns two variables, called popt, pcov.popt stores the value of optimal parameters, and pcov stores the values of its covariances. We can see that popt variable has two values. Those values are our optimal parameters. We are going to use those parameters and plot our best fit curve, as shown below.e. Plot the data:Plotting the data with the coefficients found.f. Check the accuracy of the model:Check the accuracy of the model with r2_score.Putting it all together:Launch it on Google Colab:colab.research.google.comSome real-life examples of sinusoidal regression:Sometimes we have data that shows patterns like a sine wave. Therefore, in such case scenarios, we use a sinusoidal regression. Below we can show the formula for the algorithm:a. Generating the dataset:b. Applying a sine function:Here we have created a function called “calc_sine” to calculate the value of output based on optimal coefficients. Here we will use the scikit-learn library to find the optimal parameters.c. Why does a sinusoidal regression perform better than linear regression?If we check the accuracy of the model after fitting our data with a straight line, we can see that the accuracy in prediction is less than that of sine wave regression. That is why we use sinusoidal regression.Putting it all together:Launch it on Google Colab:colab.research.google.comSome real-life examples of logarithmic growth:📚 Check out our editorial recommendations on the best machine learning books. 📚Sometimes we have data that grows exponentially in the statement, but after a certain point, it goes flat. In such a case, we can use a logarithmic regression.a. Import required libraries:b. Generating the dataset:c. The first column of our matrix X :Here we will use our normal equation to find the coefficient values.d. Reshaping X:e. Going with the Normal Equation formula:f. Forming the main matrix X:g. Finding the transpose matrix:h. Performing matrix multiplication:i. Finding the inverse:j. Matrix multiplication:k. Finding the coefficient values:l. Plot the data with the regression curve:m. Accuracy:Putting it all together:Launch it on Google Colab:colab.research.google.comDISCLAIMER: The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University, nor other companies (directly or indirectly) associated with the author(s). These writings do not intend to be final products, yet rather a reflection of current thinking, along with being a catalyst for discussion and improvement.For attribution in academic contexts, please cite this work as:[1] Mitchell, Tom. (1997). Machine Learning. McGraw Hill. p. 2. ISBN 0–07–042807–7[2] Machine Learning, Arthur Samuel, Carnegie Mellon, http://www.contrib.andrew.cmu.edu/~mndarwis/ML.html[3] Machine Learning (ML) vs. AI, Towards AI, https://towardsai.net/ai-vs-ml[4] Key Machine Learning Definitions, Towards AI, https://towardsai.net/machine-learning-definitionsPublished via Towards AII. Best Datasets for Machine Learning and Data ScienceII. AI Salaries Heading SkywardIII. What is Machine Learning?IV. Best Masters Programs in Machine Learning (ML) for 2020V. Best Ph.D. Programs in Machine Learning (ML) for 2020VI. Best Machine Learning BlogsVII. Key Machine Learning DefinitionsVIII. Breaking Captcha with Machine Learning in 0.05 SecondsIX. Machine Learning vs. AI and their Important DifferencesX. Ensuring Success Starting a Career in Machine Learning (ML)XI. Machine Learning Algorithms for BeginnersXII. Neural Networks from Scratch with Python Code and Math in DetailXIII. Building Neural Networks with PythonXIV. Main Types of Neural NetworksXV. Monte Carlo Simulation Tutorial with PythonXVI. Natural Language Processing Tutorial with Python",04/06/2020,8,97.0,0.0,508.0,270.0,82.0,17.0,0.0,43.0,en
4167,Gradient descent vs coordinate descent,HackerNoon.com,Francesco Gadaleta,758.0,4.0,438.0,"When it comes to function minimization, it’s time to open a book of optimization and linear algebra. I am currently working on variable selection and lasso-based solutions in genetics. What lasso does is basically minimizing the loss function and an penalty in order to set to zero some regression coefficients and select only those covariates that are really associated with the response. Pheew, the shortest summary of lasso ever!We all know that, provided the function to be minimized is convex, a good direction to follow, in order to find a local minimum, is towards the negative gradient of the function. Now, my question is how good or bad is following the negative gradient with respect to a coordinate descent approach that loops across all dimensions and minimizes along each?There is no better way to try this with real code and start measuring. Hence, I wrote some code that implements both gradient descent and coordinate descent.The comparison might not be completely fair because the learning rate in the gradient descent procedure is fixed at 0.1 (which in some cases might be slower indeed). But even with some tuning (maybe with some linear search) or adaptive learning rates, it’s quite common to see that coordinate descent overcomes its brother gradient descent many times.This occurs much more often when the number of covariates becomes very high, as in many computational biology problems. In the figure below, I plot the analytical solution in red, the gradient descent minimisation in blue and the coordinate descent in green, across a number of iterations.A small explanation is probably necessary to read the function that performs coordinate descent. For a more mathematical explanation refer to the original post.It’s quite common to see that coordinate descentovercomes its brother gradient descentCoordinate descent will update each variable in a Round Robin fashion. Despite the learning rate of the gradient descent procedure (which could indeed speed up convergence), the comparison between the two is fair at least in terms of complexity.Coordinate descent needs to perform operations for each coordinate update. Gradient descent performs the same number of operations . The R code that performs this comparison and generates the plot above is given belowFeel free to download this code (remember to cite me and send me some cookies).Happy descent!Originally published at worldofpiggy.com on May 31, 2014.Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!",31/05/2014,1,3.0,5.0,1201.0,536.0,5.0,0.0,0.0,14.0,en
4168,How I deployed my spark document classification(Logistic Regression) model/s as a standalone app for real-time prediction,HackerNoon.com,surendranath bobba,9.0,5.0,673.0,"TLDR — Use pipelines to save TF-IDF model generated from the training set, and SVM model for prediction. So essentially save two models, one for feature extraction and transformation of input, the other for prediction.One of the big challenges when you develop a text classification model, the trained model which you get is not enough for prediction if your plan was to train offline and deploy only the model for prediction in some cases. Especially in the case where we are extracting features from the training set using `Hashing Trick` and to normalise the importance of a feature/term to the document using `Inverse Document Frequency`, the most frequent terms in documents actually have lesser importance to the whole corpus. This is all commonly labelled according to spark website as `Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.`If we are using TF-IDF for feature vectorization using spark, we typically implement it like thisSo as you can see, just the trained model won’t enough for a standalone prediction as we have to extract the features from our input document and normalise their term frequencies, which is all dependent on the training set, which we don’t want to include in real-time prediction(time consuming as well as increase in memory consumption of the app).So from spark 1.3 onwards, pipelines were introduced, where we can automate our workflow of extraction, transformation and prediction using pipelines. And from spark 1.6, we had the ability to save the pipeline models which included all the workflows. So if we wanted to use pipelines to train models offline and predict somewhere they are the goto solution. So if we wanted to use logistic regression to train and predict, this is how we can do(picked from http://spark.apache.org/docs/latest/ml-pipeline.html).This was the perfect solution, but not all logistic regression algorithms were supported, only logistic regression and naive bayes were supported.So if we wanted to use SVM or LogisticRegressionwithBFGS, we are out of luck. For these algorithm libraries to support pipelines, they have to implement a method called `fit`. To put it more precisely, pipelines work on the concept of transformers and estimators, whatever we put inside the pipeline workflow has to be one of those. Our algorithm models are estimators since they train or fit data. The fit() method accepts a dataframe and returns a `pipelineModel`. SVM doesn’t support this method.So I tried to make the existing SVM an estimator without much success, as there seems to be a complete lack of documentation on how to create our own estimators and transformers. I would love to hear from anyone who was able to accomplish this.So I was searching for alternate ways to make this work, a thought struck after a week of cogitating — use pipelines. Sounds confusing — I know. Let me elaborate it.What if we set pipelines staging upto IDF Model generation only?. This outputs a pipeline model, which can be saved along with the trained SVM model.So instead of saving only one SVM model for prediction, I used pipelines to generate a extract and transform model including stages of ‘tokenizing, extracting and transform’ to generate a pipelineModel and save it as well.Here’s my final code snippets which I used to save both the SVM and pipeline Models.For standalone prediction without trained data, load the two models and transform your input text to a Dataframe of extracted features, and pass your transformed input DF to the SVM model.Note:- First two code snippets are referenced from http://spark.apache.org/docs/latest/ml-pipeline.htmlPS:- I would love to hear alternate solutions and corrections if any. ThanksHacker Noon is how hackers start their afternoons. We’re a part of the @AMIfamily. We are now accepting submissions and happy to discuss advertising &sponsorship opportunities.To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!",04/10/2016,3,83.0,11.0,0.0,0.0,0.0,0.0,0.0,13.0,it
4169,Demonstrating Customers Segmentation with DBSCAN Clustering Using Python,MLearning.ai,Hshan.T,36.0,7.0,1151.0,"IntroWe always try to focus on a subset rather than broad coverage of potential customers to optimize our business objectives by shrinking our target customers through customers segmentation effort. Recall customer segmentation via centroid-based clustering — K-Means, discussed in previous post, there are some drawbacks associated with model applied that we may wish to minimize or even eliminate with goal of proposing more flexible or robust model as an alternative. In this piece of writing, we will be going through a density-based solution — DBSCAN, that overcome the issues.Why DBSCAN?Looking at clustering result from K-Means, every data point is assigned to a cluster and mean of points is centroid of each cluster. Thus, each point played their role in determining the cluster. In reality, it is not unusual to expect some data points to be deviated from ‘normal’, so called anomalies or outliers. Presence of a single outlier or minor change in a single point might distort entire clustering result. Besides, clusters form by K-Means is rather more spherical, problem arises if clusters are of other odd or elliptical shapes. Scree plot and silhouette plot serve as a guidance in deciding parameter k, but they are highly influential by model sensitivity to anomalies. Creation of a new approach intends to overcome complications of existing methods. DBSCAN deals or handles most downsides of K-Means.DBSCANDBSCAN is density-based non-parametric unsupervised learning as well, we do not prescribe any model where data is from. Fewer assumptions, more flexible the model built, wider application. Similar to K-Means, DBSCAN is grouping data by their similarities based on distance functions and density. Density here means number of points in region defined by model parameters. Let us define terms used in explaining the process happening behind the scenes:1. eps — define neighborhood of a point, x, or radius of a circle (or hypersphere in N-D space) with x as centre2. minPts — minimum number of points in neighborhood to form a dense region3. Core point — a point that has at least minPts data points within eps.4. Border point — a point that is within a dense region or a cluster but it is not a core point5. Noise — not belong to any cluster6. Directly density reachable — point x is directly density reachable from y if x is within eps-neighborhood of y7. Indirectly density reachable — point x(1) is said to be indirectly density-reachable(eps, minPts) from x(n) if there is a chain of points {x(1), …, x(n)} such that x(i+1) is directly density reachable from x(i).8. Density connectivity — point x is density-connected(eps, minPts) from y if there is a point z such that both x and y are directly reachable from z.DBSCAN partitions data such that dense region is separated by sparse region. It works with data of following characteristics:· irregular clusters shapes· different clusters sizes· presence of outliersAlgorithm:1. Randomly picking a point and check if it is a core point with minPts points in eps-neighborhood, form a cluster.2. Run recursively on the points in neighborhood of a core point until all are visited, assigning density-reachable points as a cluster.3. Iterate the process on remaining unvisited points.Implementing DBSCANData is only useful if we try to dive into meaning behind the numbers. Data will only be a record if we do not analyze, digest and make it visible. After some brief introduction about DBSCAN, text below is demonstrating application of DBSCAN by adopting preprocessed real-world data from previous post. Overall planning will be same, but this round DBSCAN will take over role of K-Means and be in-charge of doing partitioning task.First, we need to estimate and specify parameters for DBSCAN, start by minPts and follow by eps using the former. If minPts=1, every data point will form a cluster by itself, means if we have n data, we gonna have n clusters. This does not make any sense for clustering to be done. If minPts=2, it will be same as single-linked hierarchy clustering. Hence, minPts should be set as minPts ≥ D+1. Nevertheless, by rule of thumb, it is set to be minPts=2*D below for simplicity. Moving forward, eps is determined from k distance graph. Mean distance from k nearest neighbors is computed and plotted against range from 1:n. The y value of point where ‘’elbow’ is spotted on the curve, eps=0.7 for this case.Python code:Applying parameters estimated, a DBSCAN model is fitted and result is visualized in 3D plot. Note that black dots are anomalies detected, DBSCAN model in sklearn library will label as ‘-1’. Anomalies are spotted loosely distributed away from denser regions. Number of clusters and set of outliers are largely affected by parameters settings. Spotting ‘elbow’ might be subjective in some confusing cases, where curve does not show aggressive change of distance between successive points. So, it might be importance to note the possible or logic number of clusters to look for. Try to change parameters in this case, a large number of clusters might be obtained which is not too reasonable and digress from what we are interested in. One of the reason to explain the scenario is due to small sample size of processed data. Important characteristics or aggregated features for each cluster is then analyzed and summarized in snake plot below to aid business business decision making.Python code:Python code:Overall shapes in this snake plot is similar to Figure 6 here, but look carefully into numbers recorded or plotted above, there are some differences. Slightly more obvious difference in ‘Monetary’. Cluster 2 is most economically valuable to business with the lowest ‘Recency’. highest ‘Monetary’ and ‘Frequency’. When goal of this task changed, the way how we look at this result should be updated to make sound business decision. For example, if business wish to retain potential churning customers or seeking targets for some new products or promotions, cluster with high ‘Recency’ grabs our attention, subsequently other cluster is selected instead of 2. A carefully and thoroughly planned analysis goal is crucial because it affects how data is processed and hence result will be highly impacted.DBSCAN vs K-MeansConclusionDistance function is used to compute similarities between samples. Choices on parameters is made based on function selected. In simpler case, we manage to determine dense regions in scatter plots visually and intuitively. Such a phenomenon is pretty rare, especially as we are exposed to higher dimensional data or data without clear boundaries between dense regions. Although advantages of DBSCAN over K-Means is outlined, its result expected to deteriorate as data dimension is too big, problem of ‘curse of dimensionality’ comes in. Clustering result with data of different densities or loosely distributed will not be too decent or even departure from our goal. There are times the points fall in confusing regions of unclear boundaries, they may be grouped into either cluster. Understand data we have, choose model that is able to handle the data structures to get the most veracious result telling true story.",04/03/2021,0,7.0,40.0,830.0,345.0,11.0,0.0,0.0,4.0,en
4170,NLP: Spam Detection in SMS (text) data using Deep Learning,Towards Data Science,"Sudip Shrestha, PhD",46.0,17.0,1954.0,"Today, internet and social media have become the fastest and easiest ways to get information. In this age, reviews, opinions, feedbacks, messages and recommendations have become significant source of information. Thanks to advancement in technologies, we are now able to extract meaningful information from such data using various Natural Language Processing (NLP) techniques. NLP , a branch of Artificial Intelligence (AI), makes use of computers and human natural language to output valuable information. NLP is commonly used in text classification task such as spam detection and sentiment analysis, text generation, language translations and document classification.The purpose of this article is to understand how we can use TensorFlow2 to build SMS spam detection model. Particularly, we will build a binary classification model to detect whether a text message is spam or not (aka Ham). Moreover, we’ll learn how to implement Dense, Long Short Term Memory (LSTM) and Bidirectional-LSTM (Bi-LSTM) deep learning models in TensorFlow2 Keras API.The SMS (text) data was downloaded from UCI datasets. It contains 5,574 SMS phone messages. The data were collected for the purpose of mobile phone spam research and have already been labeled as either spam or ham.We will use Dense text classifier , LSTM and Bi-LSTM and compare these methods in terms of performance and select a final one.Below are the sections that we cover in this article:Let’s get started, first making sure the version of TensrFlow.It’s a good practice to keep all the imports at the beginning of the code.The data can be dowloaded from UCI datasets and saved in a local folder. The text file (and a complete Jupyter notebook) is also provided in my github location hence can be read using below syntax. The text file is a tab separated (\t) file hence, we can use pandas to read data as a dataframe. We can also provide the columns name by passing names, call it label and message.Let’s get the summary statistics and visualize the data. The describe() method from pandas provide a summary statistics. Such as, there are 5,572 labels and messages. There are two unique labels indicating for “ham” and “spam”. We can also observe that there are less unique messages (5,169) than total message count(5,572) indicating some repeated messages. The top label is “ham” and the top message in the data is “Sorry, I’ll call later”. The duplicatedRow below shows, there are 403 duplicated messages.There are 4,825 ham compared to 747 spam messages. This indicates the imbalanced data which we will fix later. The most popular ham message is “Sorry, I’ll call later”, whereas the most popular spam message is “Please call our customer service…” which occurred 30 and 4 times, respectively.Below, we further explore the data by label groups by creating a WordCloud and a bar chart. First, let’s create a separate dataframe for ham and spam message and convert it to numpy array to generate WordCloud.To visualize using WordCloud(), we extract words most commonly found in ham and spam messages, remove meaningless stop words such as “the”, “a” , “is” etc, and plot it. The WordCloud visualizes the most frequent words in the given text.The ham message WordCloud below shows that “now”, “work”, “How”, “Ok” and “Sorry” are the most commonly appeared word in ham messages.The spam message WordCloud below shows that “Free”, “call”, “text”, “claim” and“reply” are most commonly appeared words in spam messages.Now, let’s further explore the imbalanced data. Below, the bar chart shows that the classes are imbalanced. There are most frequent ham messages (85%) than spam (15%).There are several ways to handle the imbalance data, for instanceHowever, for our problem, we use downsampling just to illustrate how it is implemented. You can of course, try other techniques and compare the results.Downsampling is a process where you randomly delete some of the observations from the majority class so that the numbers in majority and minority classes are matched. Below, we have downsampled the ham messages (majority class). There are now 747 messages in each class.Below chart shows a similar distribution across message types after accounting for the imbalanced data.Furthermore, on average, the ham message has length of 73 words whereas spam message has 138. The length information may be useful when we set maxlen parameter later.After exploring and accounting for imbalanced data, next let’s convert the text label to numeric and split the data into training set and testing set. Also, convert label to numpy arrays to fit deep learning models. 80% of data were used for training and 20% for testing purposes.Now, let’s use text pre-processing which includes Tokenization, Sequencing and Padding.The figure below depicts an example of a process flow in text pre-processing along with the expected output.For basic understanding on pre-processing text data using TensorFlow2, please refer to my previous article on pre-processing data for deep learning model.TokenizationAs deep learning models do not understand text, let’s convert text into numerical representation. For this purpose, a first step is Tokenization. The Tokenizer API from TensorFlow Keras splits sentences into words and encodes these into integers. Tokenizer() does all the required pre-processing such asFirst, define hyper-parameters used for pre-processing. We’ll describe these hyper-parameters later.Below, we used Tokenizer() to tokenize the words.Hyper-parameters used in Tokenizer object are: num_words and oov_token, char_level.We can get the word_index using tokenizer.word_index. A snapshot of word_index is printed below.Sequencing and PaddingAfter tokenization, we represent each sentence by sequences of numbers using texts_to_sequences() from tokenizer object. Subsequently, we use pad_sequences() so that each sequence will have same length. Sequencing and padding are done for both training and testing data.Before padding, first sequence is 27 word long where as second one is 24. Once the padding was applied, both sequences have length of 50.As depicted below, the padded sequence has length of 50.With our data loaded and preprocessed, we’re now well prepared to use neural network architecture to classify the text message. Let’s train the model using a Dense architecture followed by LSTM and Bi-LSTM.Define hyper-parameters:Below is a model architecture of dense spam detection model.Here, we use fairly a shallow neural network architecture, however, you can make it more dense adding more layers.The model.summary() below, provides the layer, shape and number of parameters used in each layer. In the embedding layer, the 8000 parameter comes from 500 words (vocab_size), each one with a 16 dimensional word-vector space (500 X 16 =8000). The embedding layer is passed through GlobalAveragePooling1D and into dense layers have shape of 16 (due to the average pooling along 16 embedding dimension). We selected 24 neurons for the dense hidden layer. Each of the 24 neurons in the dense layer gets input from each of the 16 values coming from the GlobalAveragePooling1D layer, for a total of 384 (16 X 24) weights and 24 biases (one for each 24 neurons). Hence the total parameter is 408. Finally, the output layer has 24 weights (one for each neuron) and its one bias term resulting 25 parameters in total.Compiling the Dense modelLet’s compile our dense spam classifier model. We use ‘binary_crossentropy’ as a loss function because of binary output, ‘adam’ as an optimiser which makes use of momentum to avoid local minima and ‘accuracy’ as a measure of model performance.Training and evaluating the Dense modelNext, let’s fit our dense classifier using model.fit() argument. It uses padded training data and training labels for training the model and validation data for validating.The model resulted , training loss: 0.07, training accuracy: 97%, validation loss: 0.13 and validation accuracy: 94%.We can further visualize the history results by plotting loss and accuracy by number of epochs.Figure below shows, loss over number of epochs for training and validation data sets. As expected, the loss is decreasing as the number of epochs increases. The validation loss is higher than training loss after around 5 epochs and the difference is more pronounced with increase in epochs.The accuracy plot below shows, the accuracy is increasing over epochs. As expected, the model is performing better in training set than the validation set. If the model performs very well on training data however, its performance is worse in testing data, then it is an indication of overfitting. In our model, we don’t see a significant issue with over-fitting. Moreover, we have accounted for over-fitting problem by using dropout layer and callback earlier.Below, we fit the spam detection model using LSTM. Some new hyper-parameters used in LSTM below are n_lstm and return_sequences.Below are hyper-parameters used for LSTM model.Let’s define LSTM spam detection model architecture. In Keras, by simply adding LSTM we can fit the LSTM model.Compiling the LSTM modelTraining and evaluation LSTM modelThe training and evaluations are same as we did for Dense model above.The validation loss and accuracy from LSTM are 0.31 and 91%, respectively.Unlike in LSTM, the Bi-LSTM learns patterns from both before and after a given token within a document. The Bi-LSTM back-propagates in both backward and forward directions in time. Due to this, the computational time is increased compared to LSTM. However, in most of the cases Bi-LSTM results in better accuracy.Below, we can see the Bi-directional LSTM architecture, where only difference than LSTM is that we use Bidirectional wrapper to LSTM.Compiling the BiLSTM modelTraining and evaluation BiLSTM modelThe validation loss and accuracy from BiLSTM are 0.18 and 95%, respectively.All Dense, LSTM and Bi-LSTM models are comparable in terms of loss and accuracy. The validation loss for these three models are 0.13, 0.31 and 0.18, respectively. And, the validation accuracy are 94%, 91% and 95%, respectively.Among all, both Dense and BiLSTM outperformed the LSTM. Based on loss, accuracy and the plots above, we select Dense architecture as a final model for classifying the text messages for spam or ham. The dense classifier has simple structure and the loss and accuracy over epochs are more stable than in BiLSTM.Scenario 1: Using raw text from our data:Let’s evaluate how our Dense spam detection model predicts/classifies whether its spam or ham given the text from our original data. First and second messages below are ham whereas the third one is a spam message. We’ve used the same tokenizer that we created earlier in the code to convert them into the sequences. This makes sure the new words will have the same token as in the training set. Once tokenized, we use padding as we did earlier and provide the same dimension as in training set.predict_spam function is defined as below.As shown below, the model correctly predicts first two sentences as not spam where as the third one as spam. There is 99% chance that the third sentence is spam.Scenario 2: Using newly created text message and see how the model classifies them.  Below, first sentence is more like a spam whereas the rest of the two sentences are more like ham.Our model correctly classifies the first message as spam (94% chance to be spam) were as the rest as ham.We used the text messages from UCI datasets and fit deep learning models such as Dense architecture, LSTM and Bi-LSTM and compared the accuracy and loss on validation set across these models. Finally, we selected the Dense architectural deep learning model to classify text messages as spam or ham and used it to classify new text messages. This article, provides an overview of using different architectural deep learning models to NLP problem using TensorFlow2 Keras.Next, we can explore more sampling approaches such as upsampling, SMOTE, overall sample. We can also try using different hyper-parameters, increase the sample size to improve the model further.I would like to thank Jon Krohn for his book “Deep Learning Illustrated”, Lazy programmer Inc for their excellent course “Tensorflow 2.0: Deep Learning and Artificial Intelligence” and Jose Portilla for an awesome course “Complete Tensorflow 2 and Keras Deep Learning Bootcamp” on Udemy.Thank you all for reading and please feel free to leave comments/suggestions.",27/07/2020,45,19.0,21.0,533.0,260.0,27.0,8.0,0.0,6.0,en
4171,POS Tagging Using RNN,Towards Data Science,Tanya Dayanand,46.0,15.0,2042.0,"The classical way of doing POS tagging is using some variant of Hidden Markov Model. Here we'll see how we could do that using Recurrent neural networks. The original RNN architecture has some variants too. It has a novel RNN architecture — the Bidirectional RNN which is capable of reading sequences in the ‘reverse order’ as well and has proven to boost performance significantly.Then two important cutting-edge variants of the RNN which have made it possible to train large networks on real datasets. Although RNNs are capable of solving a variety of sequence problems, their architecture itself is their biggest enemy due to the problems of exploding and vanishing gradients that occur during the training of RNNs. This problem is solved by two popular gated RNN architectures — the Long, Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU). We’ll look into all these models here with respect to POS tagging.The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, or simply POS-tagging. The NLTK library has a number of corpora that contain words and their POS tag. I will be using the POS tagged corpora i.e treebank, conll2000, and brown from NLTK to demonstrate the key concepts. To get into the codes directly, an accompanying notebook is published on Kaggle.www.kaggle.comThe following table provides information about some of the major tags:Let’s begin with importing the necessary libraries and loading the dataset. This is a requisite step in every data analysis process(The complete code can be viewed here). We’ll be loading the data first using three well-known text corpora and taking the union of those.As a part of preprocessing, we’ll be performing various steps such as dividing data into words and tags, Vectorise X and Y, and Pad sequences.Let's look at the data first. For each of the words below, there is a tag associated with it.Since this is a many-to-many problem, each data point will be a different sentence of the corpora. Each data point will have multiple words in the input sequence. This is what we will refer to as X. Each word will have its corresponding tag in the output sequence. This what we will refer to as Y. Sample dataset:The next thing we need to figure out is how are we going to feed these inputs to an RNN. If we have to give the words as input to any neural networks then we essentially have to convert them into numbers. We need to create a word embedding or one-hot vectors i.e. a vector of numbers form of each word. To start with this we'll first encode the input and output which will give a blind unique id to each word in the entire corpus for input data. On the other hand, we have the Y matrix(tags/output data). We have twelve POS tags here, treating each of them as a class and each pos tag is converted into one-hot encoding of length twelve. We’ll use the Tokenizer() function from Keras library to encode text sequence to integer sequence.Make sure that each sequence of input and output is of the same length.The sentences in the corpus are not of the same length. Before we feed the input in the RNN model we need to fix the length of the sentences. We cannot dynamically allocate memory required to process each sentence in the corpus as they are of different lengths. Therefore the next step after encoding the data is to define the sequence lengths. We need to either pad short sentences or truncate long sentences to a fixed length. This fixed length, however, is a hyperparameter.You know that a better way (than one-hot vectors) to represent text is word embeddings. Currently, each word and each tag is encoded as an integer. We’ll use a more sophisticated technique to represent the input words (X) using what’s known as word embeddings.However, to represent each tag in Y, we’ll simply use one-hot encoding scheme since there are only 12 tags in the dataset and the LSTM will have no problems in learning its own representation of these tags.To use word embeddings, you can go for either of the following models:We’re using the word2vec model for no particular reason. Both of these are very efficient in representing words. You can try both and see which one works better.The dimension of a word embedding is: (VOCABULARY_SIZE, EMBEDDING_DIMENSION)All the data preprocessing is now complete. Let’s now jump to the modeling part by splitting the data to train, validation, and test sets.Before using RNN, we must make sure the dimensions of the data are what an RNN expects. In general, an RNN expects the following shapeShape of X: (#samples, #timesteps, #features)Shape of Y: (#samples, #timesteps, #features)Now, there can be various variations in the shape that you use to feed an RNN depending on the type of architecture. Since the problem we’re working on has a many-to-many architecture, the input and the output both include number of timesteps which is nothing but the sequence length. But notice that the tensor X doesn’t have the third dimension, that is, number of features. That’s because we’re going to use word embeddings before feeding in the data to an RNN, and hence there is no need to explicitly mention the third dimension. That’s because when you use the Embedding() layer in Keras, the training data will automatically be converted to (#samples, #timesteps, #features) where #features will be the embedding dimension (and note that the Embedding layer is always the very first layer of an RNN). While using the embedding layer we only need to reshape the data to (#samples, #timesteps) which is what we have done. However, note that you’ll need to shape it to (#samples, #timesteps, #features) in case you don’t use the Embedding() layer in Keras.Next, let’s build the RNN model. We’re going to use word embeddings to represent the words. Now, while training the model, you can also train the word embeddings along with the network weights. These are often called the embedding weights. While training, the embedding weights will be treated as normal weights of the network which are updated in each iteration.In the next few sections, we will try the following three RNN models:Let’s start with the first experiment: a vanilla RNN with arbitrarily initialized, untrainable embedding. For this RNN we won’t use the pre-trained word embeddings. We’ll use randomly initialized embeddings. Moreover, we won’t update the embeddings weights.We can see here, after ten epoch, it is giving fairly decent accuracy of approx 95%. Also, we are getting a healthy growth curve below.Next, try the second model — RNN with arbitrarily initialized, trainable embeddings. Here, we’ll allow the embeddings to get trained with the network. All I am doing is changing the parameter trainable to true i.e trainable = True. Rest all remains the same as above. On checking the model summary we can see that all the parameters have become trainable. i.e trainable params are equal to total params.On fitting the model the accuracy has grown significantly. It has gone up to approx 98.95% by allowing the embedding weights to train. Therefore embedding has a significant effect on how the network is going to perform.we’ll now try the word2vec embeddings and see if that improves our model or not.Let’s now try the third experiment — RNN with trainable word2vec embeddings. Recall that we had loaded the word2vec embeddings in a matrix called ‘embedding_weights’. Using word2vec embeddings is just as easy as including this matrix in the model architecture.The network architecture is the same as above but instead of starting with an arbitrary embedding matrix, we’ll use pre-trained embedding weights (weights = [embedding_weights]) coming from word2vec. The accuracy, in this case, has gone even further to approx 99.04%.The results improved marginally in this case. That’s because the model was already performing very well. You’ll see much more improvements by using pre-trained embeddings in cases where you don’t have such a good model performance. Pre-trained embeddings provide a real boost in many applications.To solve the vanishing gradients problem, many attempts have been made to tweak the vanilla RNNs such that the gradients don’t die when sequences get long. The most popular and successful of these attempts has been the long, short-term memory network, or the LSTM. LSTMs have proven to be so effective that they have almost replaced vanilla RNNs.Thus, one of the fundamental differences between an RNN and an LSTM is that an LSTM has an explicit memory unit which stores information relevant for learning some task. In the standard RNN, the only way the network remembers past information is through updating the hidden states over time, but it does not have an explicit memory to store information.On the other hand, in LSTMs, the memory units retain pieces of information even when the sequences get really long.Next, we’ll build an LSTM model instead of an RNN. We just need to replace the RNN layer with LSTM layer.The LSTM model also provided some marginal improvement. However, if we use an LSTM model in other tasks such as language translation, image captioning, time series forecasting, etc. then you may see a significant boost in the performance.Keeping in mind the computational expenses and the problem of overfitting, researchers have tried to come up with alternate structures of the LSTM cell. The most popular one of these alternatives is the gated recurrent unit (GRU). GRU being a simpler model than LSTM, it's always easier to train. LSTMs and GRUs have almost completely replaced the standard RNNs in practice because they’re more effective and faster to train than vanilla RNNs (despite the larger number of parameters).Let’s now build a GRU model. We’ll then also compare the performance of the RNN, LSTM, and the GRU model.There is a reduction in params in GRU as compared to LSTM. Therefore we do get a significant boost in terms of computational efficiency with hardly any decremental effect in the performance of the model.The accuracy of the model remains the same as the LSTM. But we saw that the time taken by an LSTM is greater than a GRU and an RNN. This was expected since the parameters in an LSTM and GRU are 4x and 3x of a normal RNN, respectively.For example, when you want to assign a sentiment score to a piece of text (say a customer review), the network can see the entire review text before assigning them a score. On the other hand, in a task such as predicting the next word given previous few typed words, the network does not have access to the words in the future time steps while predicting the next word.These two types of tasks are called offline and online sequence processing respectively.Now, there is a neat trick you can use with offline tasks — since the network has access to the entire sequence before making predictions, why not use this task to make the network ‘look at the future elements in the sequence’ while training, hoping that this will make the network learn better?This is the idea exploited by what is called bidirectional RNNs.By using bidirectional RNNs, it is almost certain that you’ll get better results. However, bidirectional RNNs take almost double the time to train since the number of parameters of the network increase. Therefore, you have a tradeoff between training time and performance. The decision to use a bidirectional RNN depends on the computing resources that you have and the performance you are aiming for.Finally, let’s build one more model — a bidirectional LSTM and compare its performance in terms of accuracy and training time as compared to the previous models.You can see the no of parameters has gone up. It does significantly shoot up the no of params.The bidirectional LSTM did increase the accuracy substantially (considering that the accuracy was already hitting the roof). This shows the power of bidirectional LSTMs. However, this increased accuracy comes at a cost. The time taken was almost double than of a normal LSTM network.Below is the quick summary of each of the four models we tried. We can see a trend here as we move from one model to another.If you have any questions, recommendations, or critiques, I can be reached via LinkedIn or the comment section.",03/09/2020,22,116.0,53.0,1039.0,332.0,25.0,3.0,0.0,8.0,en
4172,Deep Convolutional Generative Adversarial Network using PyTorch,Geek Culture,Renu Khandelwal,3900.0,8.0,39.0,This post will learn to create a DCGAN using PyTorch on the MNIST dataset.A basic understanding of CNNA sample implementation using CNNUnderstanding Deep Convolutional GANGANs were invented by Ian Goodfellow in 2014 and first described in the paper Generative…,06/05/2021,0,3.0,1.0,0.0,0.0,0.0,0.0,0.0,4.0,en
4173,A Basic Introduction to Separable Convolutions,Towards Data Science,Chi-Feng Wang,1200.0,8.0,1585.0,"Anyone who takes a look at the architecture of MobileNet will undoubtedly come across the concept of separable convolutions. But what is that, and how is it different from a normal convolution?There are two main types of separable convolutions: spatial separable convolutions, and depthwise separable convolutions.Conceptually, this is the easier one out of the two, and illustrates the idea of separating one convolution into two well, so I’ll start with this. Unfortunately, spatial separable convolutions have some significant limitations, meaning that it is not heavily used in deep learning.The spatial separable convolution is so named because it deals primarily with the spatial dimensions of an image and kernel: the width and the height. (The other dimension, the “depth” dimension, is the number of channels of each image).A spatial separable convolution simply divides a kernel into two, smaller kernels. The most common case would be to divide a 3x3 kernel into a 3x1 and 1x3 kernel, like so:Now, instead of doing one convolution with 9 multiplications, we do two convolutions with 3 multiplications each (6 in total) to achieve the same effect. With less multiplications, computational complexity goes down, and the network is able to run faster.One of the most famous convolutions that can be separated spatially is the Sobel kernel, used to detect edges:The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels.Unlike spatial separable convolutions, depthwise separable convolutions work with kernels that cannot be “factored” into two smaller kernels. Hence, it is more commonly used. This is the type of separable convolution seen in keras.layers.SeparableConv2D or tf.layers.separable_conv2d.The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can image each channel as a particular interpretation of that image; in for example, the “red” channel interprets the “redness” of each pixel, the “blue” channel interprets the “blueness” of each pixel, and the “green” channel interprets the “greenness” of each pixel. An image with 64 channels has 64 different interpretations of that image.Similar to the spatial separable convolution, a depthwise separable convolution splits a kernel into 2 separate kernels that do two convolutions: the depthwise convolution and the pointwise convolution. But first of all, let’s see how a normal convolution works.If you don’t know how a convolution works from a 2-D perspective, read this article or check out this site.A typical image, however, is not 2-D; it also has depth as well as width and height. Let us assume that we have an input image of 12x12x3 pixels, an RGB image of size 12x12.Let’s do a 5x5 convolution on the image with no padding and a stride of 1. If we only consider the width and height of the image, the convolution process is kind of like this: 12x12 — (5x5) — >8x8. The 5x5 kernel undergoes scalar multiplication with every 25 pixels, giving out1 number every time. We end up with a 8x8 pixel image, since there is no padding (12–5+1 = 8).However, because the image has 3 channels, our convolutional kernel needs to have 3 channels as well. This means, instead of doing 5x5=25 multiplications, we actually do 5x5x3=75 multiplications every time the kernel moves.Just like the 2-D interpretation, we do scalar matrix multiplication on every 25 pixels, outputting 1 number. After going through a 5x5x3 kernel, the 12x12x3 image will become a 8x8x1 image.What if we want to increase the number of channels in our output image? What if we want an output of size 8x8x256?Well, we can create 256 kernels to create 256 8x8x1 images, then stack them up together to create a 8x8x256 image output.This is how a normal convolution works. I like to think of it like a function: 12x12x3 — (5x5x3x256) — >12x12x256 (Where 5x5x3x256 represents the height, width, number of input channels, and number of output channels of the kernel). Not that this is not matrix multiplication; we’re not multiplying the whole image by the kernel, but moving the kernel through every part of the image and multiplying small parts of it separately.A depthwise separable convolution separates this process into 2 parts: a depthwise convolution and a pointwise convolution.In the first part, depthwise convolution, we give the input image a convolution without changing the depth. We do so by using 3 kernels of shape 5x5x1.Each 5x5x1 kernel iterates 1 channel of the image (note: 1 channel, not all channels), getting the scalar products of every 25 pixel group, giving out a 8x8x1 image. Stacking these images together creates a 8x8x3 image.Remember, the original convolution transformed a 12x12x3 image to a 8x8x256 image. Currently, the depthwise convolution has transformed the 12x12x3 image to a 8x8x3 image. Now, we need to increase the number of channels of each image.The pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; in our case, 3. Therefore, we iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.We can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.And that’s it! We’ve separated the convolution into 2: a depthwise convolution and a pointwise convolution. In a more abstract way, if the original convolution function is 12x12x3 — (5x5x3x256) →12x12x256, we can illustrate this new convolution as 12x12x3 — (5x5x1x1) — > (1x1x3x256) — >12x12x256.Let’s calculate the number of multiplications the computer has to do in the original convolution. There are 256 5x5x3 kernels that move 8x8 times. That’s 256x3x5x5x8x8=1,228,800 multiplications.What about the separable convolution? In the depthwise convolution, we have 3 5x5x1 kernels that move 8x8 times. That’s 3x5x5x8x8 = 4,800 multiplications. In the pointwise convolution, we have 256 1x1x3 kernels that move 8x8 times. That’s 256x1x1x3x8x8=49,152 multiplications. Adding them up together, that’s 53,952 multiplications.52,952 is a lot less than 1,228,800. With less computations, the network is able to process more in a shorter amount of time.How does that work, though? The first time I came across this explanation, it didn’t really make sense to me intuitively. Aren’t the two convolutions doing the same thing? In both cases, we pass the image through a 5x5 kernel, shrink it down to one channel, then expand it to 256 channels. How come one is more than twice as fast as the other?After pondering about it for some time, I realized that the main difference is this: in the normal convolution, we are transforming the image 256 times. And every transformation uses up 5x5x3x8x8=4800 multiplications. In the separable convolution, we only really transform the image once — in the depthwise convolution. Then, we take the transformed image and simply elongate it to 256 channels. Without having to transform the image over and over again, we can save up on computational power.It’s worth noting that in both Keras and Tensorflow, there is a argument called the “depth multiplier”. It is set to 1 at default. By changing this argument, we can change the number of output channels in the depthwise convolution. For example, if we set the depth multiplier to 2, each 5x5x1 kernel will give out an output image of 8x8x2, making the total (stacked) output of the depthwise convolution 8x8x6 instead of 8x8x3. Some may choose to manually set the depth multiplier to increase the number of parameters in their neural net for it to better learn more traits.Are the disadvantages to a depthwise separable convolution? Definitely! Because it reduces the number of parameters in a convolution, if your network is already small, you might end up with too few parameters and your network might fail to properly learn during training. If used properly, however, it manages to enhance efficiency without significantly reducing effectiveness, which makes it a quite popular choice.Finally, because pointwise convolutions use the concept, I’d like to touch upon the usages of a 1x1 kernel.A 1x1 kernel — or rather, n 1x1xm kernels where n is the number of output channels and m is the number of input channels — can be used outside of separable convolutions. One obvious purpose of a 1x1 kernel is to increase or reduce the depth of an image. If you find that your convolution has too many or too little channels, a 1x1 kernel can help balance it out.For me, however, the main purpose of a 1x1 kernel is to apply non-linearlity. After every layer of a neural network, we can apply an activation layer. Whether it be ReLU, PReLU, Softmax, or another, activation layers are non-linear, unlike convolution layers. “A linear combination of lines is still a line.” Non-linear layers expand the possibilities for the model, as is what generally makes a “deep” network better than a “wide” network. In order to increase the number of non-linear layers without significantly increasing the number of parameters and computations, we can apply a 1x1 kernel and add an activation layer after it. This helps give the network an added layer of depth.Leave a comment below if you have any further questions! And don’t forget to give this story some claps!",14/08/2018,0,5.0,0.0,1258.0,441.0,8.0,0.0,0.0,2.0,en
4174,"Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3",Medium,Jonathan Hui,27000.0,18.0,3190.0,"You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in this article. For those only interested in YOLOv3, please forward to the bottom of the article. Here is the accuracy and speed comparison provided by the YOLO web site.A demonstration from the YOLOv2.Let’s start with our own testing image below.The objects detected by YOLO:Grid cellFor our discussion, we crop our original photo. YOLO divides the input image into an S×S grid. Each grid cell predicts only one object. For example, the yellow grid cell below tries to predict the “person” object whose center (the blue dot) falls inside the grid cell.Each grid cell predicts a fixed number of boundary boxes. In this example, the yellow grid cell makes two boundary box predictions (blue boxes) to locate where the person is.However, the one-object rule limits how close detected objects can be. For that, YOLO does have some limitations on how close objects can be. For the picture below, there are 9 Santas in the lower left corner but YOLO can detect 5 only.For each grid cell,To evaluate PASCAL VOC, YOLO uses 7×7 grids (S×S), 2 boundary boxes (B) and 20 classes (C).Let’s get into more details. Each boundary box contains 5 elements: (x, y, w, h) and a box confidence score. The confidence score reflects how likely the box contains an object (objectness) and how accurate is the boundary box. We normalize the bounding box width w and height h by the image width and height. x and y are offsets to the corresponding cell. Hence, x, y, w and h are all between 0 and 1. Each cell has 20 conditional class probabilities. The conditional class probability is the probability that the detected object belongs to a particular class (one probability per category for each cell). So, YOLO’s prediction has a shape of (S, S, B×5 + C) = (7, 7, 2×5 + 20) = (7, 7, 30).The major concept of YOLO is to build a CNN network to predict a (7, 7, 30) tensor. It uses a CNN network to reduce the spatial dimension to 7×7 with 1024 output channels at each location. YOLO performs a linear regression using two fully connected layers to make 7×7×2 boundary box predictions (the middle picture below). To make a final prediction, we keep those with high box confidence scores (greater than 0.25) as our final predictions (the right picture).The class confidence score for each prediction box is computed as:It measures the confidence on both the classification and the localization (where an object is located).We may mix up those scoring and probability terms easily. Here are the mathematical definitions for your future reference.YOLO has 24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use 1 × 1 reduction layers alternatively to reduce the depth of the features maps. For the last convolution layer, it outputs a tensor with shape (7, 7, 1024). The tensor is then flattened. Using 2 fully connected layers as a form of linear regression, it outputs 7×7×30 parameters and then reshapes to (7, 7, 30), i.e. 2 boundary box predictions per location.A faster but less accurate version of YOLO, called Fast YOLO, uses only 9 convolutional layers with shallower feature maps.YOLO predicts multiple bounding boxes per grid cell. To compute the loss for the true positive, we only want one of them to be responsible for the object. For this purpose, we select the one with the highest IoU (intersection over union) with the ground truth. This strategy leads to specialization among the bounding box predictions. Each prediction gets better at predicting certain sizes and aspect ratios.YOLO uses sum-squared error between the predictions and the ground truth to calculate loss. The loss function composes of:Classification lossIf an object is detected, the classification loss at each cell is the squared error of the class conditional probabilities for each class:Localization lossThe localization loss measures the errors in the predicted boundary box locations and sizes. We only count the box responsible for detecting the object.We do not want to weight absolute errors in large boxes and small boxes equally. i.e. a 2-pixel error in a large box is the same for a small box. To partially address this, YOLO predicts the square root of the bounding box width and height instead of the width and height. In addition, to put more emphasis on the boundary box accuracy, we multiply the loss by λcoord (default: 5).Confidence lossIf an object is detected in the box, the confidence loss (measuring the objectness of the box) is:If an object is not detected in the box, the confidence loss is:Most boxes do not contain any objects. This causes a class imbalance problem, i.e. we train the model to detect background more frequently than detecting objects. To remedy this, we weight this loss down by a factor λnoobj (default: 0.5).LossThe final loss adds localization, confidence and classification losses together.YOLO can make duplicate detections for the same object. To fix this, YOLO applies non-maximal suppression to remove duplications with lower confidence. Non-maximal suppression adds 2- 3% in mAP.Here is one of the possible non-maximal suppression implementation:SSD is a strong competitor for YOLO which at one point demonstrates higher accuracy for real-time processing. Comparing with region based detectors, YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster.Batch normalizationAdd batch normalization in convolution layers. This removes the need for dropouts and pushes mAP up 2%.High-resolution classifierThe YOLO training composes of 2 phases. First, we train a classifier network like VGG16. Then we replace the fully connected layers with a convolution layer and retrain it end-to-end for the object detection. YOLO trains the classifier with 224 × 224 pictures followed by 448 × 448 pictures for the object detection. YOLOv2 starts with 224 × 224 pictures for the classifier training but then retune the classifier again with 448 × 448 pictures using much fewer epochs. This makes the detector training easier and moves mAP up by 4%.Convolutional with Anchor BoxesAs indicated in the YOLO paper, the early training is susceptible to unstable gradients. Initially, YOLO makes arbitrary guesses on the boundary boxes. These guesses may work well for some objects but badly for others resulting in steep gradient changes. In early training, predictions are fighting with each other on what shapes to specialize on.In the real-life domain, the boundary boxes are not arbitrary. Cars have very similar shapes and pedestrians have an approximate aspect ratio of 0.41.Since we only need one guess to be right, the initial training will be more stable if we start with diverse guesses that are common for real-life objects.For example, we can create 5 anchor boxes with the following shapes.Instead of predicting 5 arbitrary boundary boxes, we predict offsets to each of the anchor boxes above. If we constrain the offset values, we can maintain the diversity of the predictions and have each prediction focuses on a specific shape. So the initial training will be more stable.In the paper, anchors are also called priors.Here are the changes we make to the network:Anchor boxes decrease mAP slightly from 69.5 to 69.2 but the recall improves from 81% to 88%. i.e. even the accuracy is slightly decreased but it increases the chances of detecting all the ground truth objects.Dimension ClustersIn many problem domains, the boundary boxes have strong patterns. For example, in the autonomous driving, the 2 most common boundary boxes will be cars and pedestrians at different distances. To identify the top-K boundary boxes that have the best coverage for the training data, we run K-means clustering on the training data to locate the centroids of the top-K clusters.Since we are dealing with boundary boxes rather than points, we cannot use the regular spatial distance to measure datapoint distances. No surprise, we use IoU.On the left, we plot the average IoU between the anchors and the ground truth boxes using different numbers of clusters (anchors). As the number of anchors increases, the accuracy improvement plateaus. For the best return, YOLO settles down with 5 anchors. On the right, it displays the 5 anchors’ shapes. The purplish-blue rectangles are selected from the COCO dataset while the black border rectangles are selected from the VOC2007. In both cases, we have more thin and tall anchors indicating that real-life boundary boxes are not arbitrary.Unless we are comparing YOLO and YOLOv2, we will reference YOLOv2 as YOLO for now.Direct location predictionWe make predictions on the offsets to the anchors. Nevertheless, if it is unconstrained, our guesses will be randomized again. YOLO predicts 5 parameters (tx, ty, tw, th, and to) and applies the sigma function to constraint its possible offset range.Here is the visualization. The blue box below is the predicted boundary box and the dotted rectangle is the anchor.With the use of k-means clustering (dimension clusters) and the improvement mentioned in this section, mAP increases 5%.Fine-Grained FeaturesConvolution layers decrease the spatial dimension gradually. As the corresponding resolution decreases, it is harder to detect small objects. Other object detectors like SSD locate objects from different layers of feature maps. So each layer specializes at a different scale. YOLO adopts a different approach called passthrough. It reshapes the 26 × 26 × 512 layer to 13 × 13 × 2048. Then it concatenates with the original 13 × 13 ×1024 output layer. Now we apply convolution filters on the new 13 × 13 × 3072 layer to make predictions.Multi-Scale TrainingAfter removing the fully connected layers, YOLO can take images of different sizes. If the width and height are doubled, we are just making 4x output grid cells and therefore 4x predictions. Since the YOLO network downsamples the input by 32, we just need to make sure the width and height is a multiple of 32. During training, YOLO takes images of size 320×320, 352×352, … and 608×608 (with a step of 32). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale. In additional, we can use lower resolution images for object detection at the cost of accuracy. This can be a good tradeoff for speed on low GPU power devices. At 288 × 288 YOLO runs at more than 90 FPS with mAP almost as good as Fast R-CNN. At high-resolution YOLO achieves 78.6 mAP on VOC 2007.Here is the accuracy improvements after applying the techniques discussed so far:Accuracy comparison for different detectors:GoogLeNetVGG16 requires 30.69 billion floating point operations for a single pass over a 224 × 224 image versus 8.52 billion operations for a customized GoogLeNet. We can replace the VGG16 with the customized GoogLeNet. However, YOLO pays a price on the top-5 accuracy for ImageNet: accuracy drops from 90.0% to 88.0%.DarkNetWe can further simplify the backbone CNN used. Darknet requires 5.58 billion operations only. With DarkNet, YOLO achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet. Darknet uses mostly 3 × 3 filters to extract features and 1 × 1 filters to reduce output channels. It also uses global average pooling to make predictions. Here is the detail network description:We replace the last convolution layer (the cross-out section) with three 3 × 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 × 1 convolutional layer to convert the 7 × 7 × 1024 output into 7 × 7 × 125. (5 boundary boxes each with 4 parameters for the box, 1 objectness score and 20 conditional class probabilities)YOLO is trained with the ImageNet 1000 class classification dataset in 160 epochs: using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9. In the initial training, YOLO uses 224 × 224 images, and then retune it with 448× 448 images for 10 epochs at a 10−3 learning rate. After the training, the classifier achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%.Then the fully connected layers and the last convolution layer is removed for a detector. YOLO adds three 3 × 3 convolutional layers with 1024 filters each followed by a final 1 × 1 convolutional layer with 125 output channels. (5 box predictions each with 25 parameters) YOLO also add a passthrough layer. YOLO trains the network for 160 epochs with a starting learning rate of 10−3 , dividing it by 10 at 60 and 90 epochs. YOLO uses a weight decay of 0.0005 and momentum of 0.9.Datasets for object detection have far fewer class categories than those for classification. To expand the classes that YOLO can detect, YOLO proposes a method to mix images from both detection and classification datasets during training. It trains the end-to-end network with the object detection samples while backpropagates the classification loss from the classification samples to train the classifier path. This approach encounters a few challenges:Hierarchical classificationWithout going into details, YOLO combines labels in different datasets to form a tree-like structure WordTree. The children form an is-a relationship with its parent like biplane is a plane. But the merged labels are now not mutually exclusive.Let’s simplify the discussion using the 1000 class ImageNet. Instead of predicting 1000 labels in a flat structure, we create the corresponding WordTree which has 1000 leave nodes for the original labels and 369 nodes for their parent classes. Originally, YOLO predicts the class score for the biplane. But with the WordTree, it now predicts the score for the biplane given it is an airplane.Sincewe can apply a softmax function to compute the probabilityfrom the scores of its own and the siblings. The difference is, instead of one softmax operations, YOLO performs multiple softmax operations for each parent’s children.The class probability is then computed from the YOLO predictions by going up the WordTree.For classification, we assume an object is already detected and therefore Pr(physical object)=1.One benefit of the hierarchy classification is that when YOLO cannot distinguish the type of airplane, it gives a high score to the airplane instead of forcing it into one of the sub-categories.When YOLO sees a classification image, it only backpropagates classification loss to train the classifier. YOLO finds the bounding box that predicts the highest probability for that class and it computes the classification loss as well as those from the parents. (If an object is labeled as a biplane, it is also considered to be labeled as airplane, air, vehicle… ) This encourages the model to extract features common to them. So even we have never trained a specific class of objects for object detection, we can still make such predictions by generalizing predictions from related objects.In object detection, we set Pr(physical object) equals to the box confidence score which measures whether the box has an object. YOLO traverses down the tree, taking the highest confidence path at every split until it reaches some threshold and YOLO predicts that object class.YOLO9000YOLO9000 extends YOLO to detect objects over 9000 classes using hierarchical classification with a 9418 node WordTree. It combines samples from COCO and the top 9000 classes from the ImageNet. YOLO samples four ImageNet data for every COCO data. It learns to find objects using the detection data in COCO and to classify these objects with ImageNet samples.During the evaluation, YOLO test images on categories that it knows how to classify but not trained directly to locate them, i.e. categories that do not exist in COCO. YOLO9000 evaluates its result from the ImageNet object detection dataset which has 200 categories. It shares about 44 categories with COCO. Therefore, the dataset contains 156 categories that have never been trained directly on how to locate them. YOLO extracts similar features for related object types. Hence, we can detect those 156 categories by simply from the feature values.YOLO9000 gets 19.7 mAP overall with 16.0 mAP on those 156 categories. YOLO9000 performs well with new species of animals not found in COCO because their shapes can be generalized easily from their parent classes. However, COCO does not have bounding box labels for any type of clothing so the test struggles with categories like “sunglasses”.A quote from the YOLO web site on YOLOv3:On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev.Class PredictionMost classifiers assume output labels are mutually exclusive. It is true if the output are mutually exclusive object classes. Therefore, YOLO applies a softmax function to convert scores into probabilities that sum up to one. YOLOv3 uses multi-label classification. For example, the output labels may be “pedestrian” and “child” which are not non-exclusive. (the sum of output can be greater than 1 now.) YOLOv3 replaces the softmax function with independent logistic classifiers to calculate the likeliness of the input belongs to a specific label. Instead of using mean square error in calculating the classification loss, YOLOv3 uses binary cross-entropy loss for each label. This also reduces the computation complexity by avoiding the softmax function.Bounding box prediction & cost function calculationYOLOv3 predicts an objectness score for each bounding box using logistic regression. YOLOv3 changes the way in calculating the cost function. If the bounding box prior (anchor) overlaps a ground truth object more than others, the corresponding objectness score should be 1. For other priors with overlap greater than a predefined threshold (default 0.5), they incur no cost. Each ground truth object is associated with one boundary box prior only. If a bounding box prior is not assigned, it incurs no classification and localization lost, just confidence loss on objectness. We use tx and ty (instead of bx and by) to compute the loss.Feature Pyramid Networks (FPN) like Feature PyramidYOLOv3 makes 3 predictions per location. Each prediction composes of a boundary box, a objectness and 80 class scores, i.e. N × N × [3 × (4 + 1 + 80) ] predictions.YOLOv3 makes predictions at 3 different scales (similar to the FPN):To determine the priors, YOLOv3 applies k-means cluster. Then it pre-select 9 clusters. For COCO, the width and height of the anchors are (10×13),(16×30),(33×23),(30×61),(62×45),(59× 119),(116 × 90),(156 × 198),(373 × 326). These 9 priors are grouped into 3 different groups according to their scale. Each group is assigned to a specific feature map above in detecting objects.Feature extractorA new 53-layer Darknet-53 is used to replace the Darknet-19 as the feature extractor. Darknet-53 mainly compose of 3 × 3 and 1× 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster.YOLOv3 performanceYOLOv3's COCO AP metric is on par with SSD but 3x faster. But YOLOv3’s AP is still behind RetinaNet. In particular, AP@IoU=.75 drops significantly comparing with RetinaNet which suggests YOLOv3 has higher localization error. YOLOv3 also shows significant improvement in detecting small objects.YOLOv3 performs very well in the fast detector category when speed is important.The original paper on YOLO.Paper on YOLOv2 and YOLO9000.DarkNet implementation.",18/03/2018,0,50.0,19.0,1122.0,508.0,45.0,12.0,0.0,21.0,en
4175,Price Prediction using Machine Learning Regression — a case study,Towards Data Science,Arun Kumar,29.0,17.0,2451.0,"This article is a detailed account of my approach to solving a regression problem, which is also a popular Kaggle competition. Hope you find it useful and enjoy reading it :)Artificial Intelligence is an integral part of all major e-commerce companies today. With the evolution of the information industry and extensive research in the field of AI in the past two decades, businesses have started to explore the ways to automate various activities using state of the art Machine Learning algorithms and Deep Neural Networks. Many IT giants and start-ups have already taken a big leap in this field and have dedicated teams and resources for research and development of cutting edge AI applications. Online retail platforms today are extensively driven by AI-powered algorithms and applications. Activities ranging from inventory management and quality checking at the warehouse to product recommendation and sales demographics on the website, all employ machine learning at various scales.Mercari is Japan’s biggest community-powered shopping website. With the aim of realizing a society where global resources are used carefully and where everyone can live richly, the company has developed a flea market application ‘Mercari’ in Japan and the United States that allows individuals to easily and safely buy and sell goods. Mercari’s challenge is to build an algorithm that automatically suggests the right product prices to sellers on its app.Predicting the price of a product is a tough challenge since very similar products having minute differences such as different brand names, additional specifications, quality, demand of the product, etc. can have very different prices. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?Price prediction gets even more difficult when there is a huge range of products, which is common with most of the online shopping platforms. Mercari’s sellers are allowed to list almost anything on the app. It’s highly challenging to predict the price of almost anything that is listed on online platforms.Let’s look at this problem from Machine Learning perspective.Mercari has provided user-inputted text descriptions of its products, including details like product category name, brand name, and item condition. Using this data, we have to come up with a model that predicts the price of a product listed on Mercari as accurately as possible. This looks like a standard regression problem.We must have a yardstick to measure how good or bad our model’s performance is. In machine learning terminology, we call this yardstick performance metric or simply metric. There are various metrics to measure the performance of a regression model, e.g. mean absolute error, mean squared error, mean squared logarithmic error, maximum residual error, median absolute error, coefficient of determination(R²), etc.For this problem, Kaggle uses Root Mean Squared Logarithmic Error(RMSLE). Lesser the RMSLE, better is our model. RMSLE is calculated asHaving understood the constraints, business objectives and the problem we need to solve , it’s time to get our hands dirty.The data can be downloaded from the Kaggle competition page. There are two files train.tsv and test.tsv and a Kaggle submission template sample_submission.csv. The total size of the data is 1.03 GB after decompression.The files consist of product listings. These files are tab-delimited. train.tsv has 1,482,535 rows and test.tsv has 3,460,725 rows. Both train and test files have the following data fieldsA row in a data table is called a data point and a column is called a feature/variable. Going forward in this blog, I will use the words row and data point interchangeably. Same follows with column and feature/variable.Below is a sample from the train data.We see that there are some null values (NaN) in the data. We would replace these with ‘missing’. Also, we split the list of three values in the column category_name into three new columnsgencat_name, subcat1_name, subcat2_name. For example, a data point with category_name=[Men, Tops, T-shirts] will have gencat_name=Men, subcat1_name=Tops, and subcat2_name=T-shirts.Let’s look at the training data from a machine learning engineer’s perspective and see if we can draw some useful inferences. This will give us a fair idea about what our approaches to solving the problem should be.nameThere are 1,224,596 unique product names in the raw data(before pre-processing). The most common product name is “Bundle”, which occurs in 2,232 data points.item_condition_idThe majority of the items are in condition 1. There are very few items which are in condition 5.brand_namebrand_name is missing for a large number (42.68 %) of data points.gencat_namesubcat1_name, subcat2_nameFurther to 10 general categories, there are 114 subcategories of products, which in turn may belong to 871 further subcategories. To put it simply, subcat1_name tells about the category of the item in a broader sense whereas subcat2_name goes one level deeper and gives more detail about what exactly the item is. Recall the example [Men, Tops, T-shirts]The below figures show the top 15 subcategories by number of items.priceThis is the target variable or ‘y’ in our train data. We need to predict this for test data. It has a wide range of values ranging from $3 to $2009. However, most of the items have a price lower than $200, as can be seen in the below graphs and percentile values.Let us explore if there are any trends in the price of the items depending on the values of different columns of data. This will help us decide which columns will be more useful than others in determining the price of an item.Variation of price with the item conditionI have used simple box plots to see how the price of an item varies with the condition of the item.Note that in a box plot the lower boundary of the box denotes the 25ᵗʰ percentile, upper boundary denotes the 75ᵗʰ percentile, and the line inside the box denotes the 50ᵗʰ percentile or the median.Variation of price with item category(gencat_name)Looking at the box plots and mean price of the various categories in the below figure, we can say that there is some variation in the price of items belonging to different product categories.Variation of price with item subcategory(subcat1_name)There is a strong variation in the prices of some of the categories of products. For example, items belonging to Computers Tablets, Cameras Photography, Strollers, Musical Instruments, etc. are expensive when compared to the items belonging to Paper Goods, Children, Office Supplies, Trading Cards, etc.This indicates that categories and subcategories are going to be important features in determining the price of an item.I have done some basic text pre-processing like removal of non-alphanumeric characters, regular expressions, stopwords, etc. from name and item_description. The code is given below.This is similar to text pre-processing. I have removed blank spaces and replaced symbol ‘&’(and) with ‘_’(underscore). I have done this cleaning in order to get precise one-hot encoding of categories, which is explained in the featurization section.A machine understands only numbers, it does not directly understand letters or text that we as humans can read. That means we need to convert our text and categorical data to numbers. This process is called feature extraction or featurization. There are several feature extraction techniques related to different kinds of data, we will see some of these in this section.Based on my intuition and research on existing approaches, I came up with the following features that I thought would be useful in determining the price of an item. There is no standard rule for using these features, these are purely intuition-based ideas which may vary from problem to problem. Therefore, these are called hand-made features or engineered features. The below table provides the names of the features. description of what each of these features means and how it is calculated.Let’s check if the above features are really useful.Uni-variate analysis on some of the engineered featuresThe below plots have been achieved using seaborn regplot which tries to fit a simple linear model and plots the equation of fitted line/plane over a uni-variate (feature vs target) scatter plot.Features such as brand_mean_price, brand_median price, subcat2_mean_price, subcat2_median_price show strong linear trends. Thus they seem to be useful in determining the price of items.For the purpose of cross-validation(checking if the trained model is working well on unseen data), I have split our data into train and cv in the ratio of 90:10. I will train our models on train and validate them on cv.Note that the target variable price has been converted to logarithmic scale by using NumPy’s log1p() function. This has been done so that we can use root mean square error as the metric instead of explicitly defining a complex metric RMSLE. (since RMSLE is nothing but RMSE of log values)One-hot encoding is a scheme of vectorization where each category in a categorical variable is converted into a vector of length equal to the number of data points. The vector contains a value of 1 against each data point that belongs to the category corresponding to the vector and contains 0 otherwise. To understand better, look at the following example.I have converted all the categorical variables (brand_name, gencat_name, subcat1_name, subcat2_name) to their one-hot encoded vectors. Example code is shown below:Note that categorical variables item_condition_id and shipping already contain numerical values and there is no need to convert them to vectors.TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents. You can read more about TF-IDF and its mathematical details here.uni-grams, bi-grams and n-grams: In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be syllables, letters, words, etc. depending on the application (words in our case). An n-gram of size 1 is referred to as a ‘uni-gram’, size 2 is a ‘bi-gram’ and so on.I have encoded name and item_description into TF-IDF vectors of uni-grams, bi-grams and tri-grams. Note that using 1,2,3-grams together would result in a huge number of words in the dictionary of TF-IDF vectorizer and using all of them would result in very high dimensional vectors. To avoid this, I have limited the number of dimensions to 250k for name and 500k for item_description vectors.The primary purpose of normalization is to scale numeric data from different columns down to an equivalent scale so that the model doesn’t get skewed due to huge variance in a few columns. I have used min-max normalization here(code given below).We would be feeding our models an input matrix X_train, which contains all the features that we have extracted in the previous section, and an array of corresponding target values, y_train. Therefore, we need to first build X_train by concatenating all the feature vectors side by side.Now our data is ready to be fed to models. Let’s start modeling.I will train the following regression models one by one and evaluate their performance on the validation data:To know more about these models and read the documentation click on the model name.Ridge is a linear least squares model with l2 regularization. In other words, it is linear regression with l2 regularizer.Over-fitting or under-fitting of the Ridge model depends on the parameter alpha, which can be tuned to the right value by doing hyper-parameter tuning as shown below.Training the model using best hyper-parameter and testingThe models that we would try going further take tremendous amount of time to train when the data is high dimensional. Therefore, I am selecting only top 48,000 features from text TF-IDF vectors and categorical one-hot encoded vectors.I concatenate these with the numerical features, ridge model’s predictions(y_pred being used as feature)and use them with my models.SVR is an advanced version of linear regression. It finds a hyperplane in a d-dimensional space that distinctly classifies the data points.Similar to alpha in linear regression, C in SVR is found by hyper-parameter tuning.Training RandomForest Regressor with higher values of n_estimators (N) was taking tremendous amount of time without giving any results. Due to this reason, we have trained it with less number of estimators. As you can guess, the results were not satisfactory.Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.Since it is based on decision tree algorithms, it splits the tree leaf-wise with the best fit whereas other boosting algorithms split the tree depth-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the depth-wise algorithm and hence results in much better accuracy. Also, it is surprisingly very fast, hence the word ‘Light’.Hyperparameter tuning for LightGBM has been done using 3-fold cross-validation using RandomizedSearchCV.The classical machine learning models seem to work decently well but we ought to have much better performance to get a good Kaggle score. Most of the existing approaches have employed some or the other deep learning models such as Convolutional Neural Networks(CNNs), Recurrent Neural Networks(RNNs) or a combination of both. The performance of deep learning models seems to be significantly better than classical ML models, which encouraged me to try a basic deep learning model, MLP.I have done the following processing on train and test data:The reasons for choosing MLP over CNN or RNN are:I have trained 4 high variance models of exactly the same architecture and finally taken ensemble of these to get final predictions. This is similar to Bagging in RandomForest. For 2 out of 4 models I have binarized the input data by setting all non-zero values to 1. This is like getting an extra dataset with a binary CountVectorizer instead of TF-IDF.I used Adam optimizer with learning rate 0.003 and an initial batch size of 512 and trained the model for 2 epochs, doubling the batch size at each epoch. For binarized input, I trained 3 epochs in the same fashion.With the above model, I got a validation RMSLE=0.3848, which is a great improvement compared to all my previous models.The final submission score on Kaggle with this model was 0.39446 in the private leaderboard. Although the competition was closed long ago, placing this score on the leaderboard puts me at the 5th position (top 0.2%) in both private and public LB.The validation RMSLE I got was 0.3848 as compared to 0.3875 in the source kernel.It was fun as well as a great learning experience doing this case study. I thank you for reading this blog and I hope this has added some value to you. I have included minimal code in this blog. For full code, please refer to the ipython notebooks in my GitHub repository Mercari-Price-Suggestion-Challenge.I would love to hear your reactions, suggestions or queries.You can connect with me on LinkedIn. Here’s my profile.",30/03/2020,16,83.0,50.0,742.0,405.0,25.0,15.0,0.0,28.0,en
4176,Embeddings in Machine Learning,The Startup,𝐁𝐚𝐲𝐚𝐧 𝐁𝐞𝐧𝐧𝐞𝐭𝐭,13.0,3.0,616.0,"I first came across the concept of embeddings while developing the RNN typing practice app.Even though I am just beginning to understand the range of uses for embeddings, I thought it would be useful to write down some of the basics.First, let’s look at what I knew before embeddings, one-hot vectors.Remember one-hot vectors? No? Well do you remember unit vectors from math class? Still no? Okay — assume that we have three labels, [🍎, 🍊, 🍌]. We want to represent these values in a way that machines can understand. Initially, we might be tempted to assign the values [1, 2, 3], but the issue here is we don’t necessarily want a 🍌 to equal three 🍎.We could instead assign vectors to each label, where the dimension of each vector is equal to the number of labels. In this case we have three labels, so three dimensions.Think about a company that has a million books in that company’s catalogue, which they would like to use as input data. It is not practical to create a system that needs a million-dimension vector to represent each book. Unless they are specifically looking for something unique to each book, there must be some aspect that some books will share. How can we manage that?With one-hot vectors, each vector is unique, and the dot product of any two vectors is always 0. However, what if this is not desired and we want there to be some similarity between vectors (a non-zero dot product)? Going back to our example above, what if in our application we are looking at the shapes of fruit? It is logical that there would be some overlap of values.An 🍎 and an 🍊 share a similar shape, so the values are similar. A 🍌 is quite a unique shape, but slightly more like an 🍎 than an 🍊. I also snuck a 🥭 in there. A 🥭 is a bit more oblong, but has a round top, so the numbers are a mix. One of the main benefits of using embeddings is that we can have the number of labels exceed the dimension of the embedding. In this case, we have 4 labels, but a dimension of 3.However, these numbers are just me, a human, squinting my eyes and pressing numbers on my keypad. These numbers are trainable and we can let our model adjust these values during training. If our data was the ratio of length to the width of the fruit, after training, the values may vaguely look like what I put above.Here’s where the alchemy begins, the embedding dimension hyperparameter. Google’s Machine Learning Crash Course on Embeddings mentions the following as a good starting point.For the RNN typing practice app, there are 28 values (a-z, space, and a null character for masking). If I followed the above formula, would have resulted in a dimension of 2 or 3. After training, I didn’t get acceptable results. I thought some more about the problem and I theorized that setting the dimension to 28 might work. This way there would be a high enough dimension to have each character compared to all characters. I ran tests with dimensions of [2, 28, 28² ➡ 784], and 28 performed the best. Is there a better value? Possibly, but I was happy with the results.Embeddings are a basic method to encode label information into a vector. This information can begin as the representation of the label and can be trained so that similar labels have similar vectors. The possibility of representing many labels with far fewer dimensions is one of the main benefits. Additional information can also be encoded, such as positional information, which is used prominently in transformers.Originally from:www.bayanbennett.com",13/08/2020,0,1.0,4.0,1400.0,933.0,1.0,0.0,0.0,8.0,en
4177,The 5 Clustering Algorithms Data Scientists Need to Know,Towards Data Science,George Seif,20000.0,11.0,1319.0,"Want to be inspired? Come join my Super Quotes newsletter. 😎Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.In Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Today, we’re going to look at 5 popular clustering algorithms that data scientists need to know and their pros and cons!K-Means is probably the most well-known clustering algorithm. It’s taught in a lot of introductory data science and machine learning classes. It’s easy to understand and implement in code! Check out the graphic below for an illustration.K-Means has the advantage that it’s pretty fast, as all we’re really doing is computing the distances between points and group centers; very few computations! It thus has a linear complexity O(n).On the other hand, K-Means has a couple of disadvantages. Firstly, you have to select how many groups/classes there are. This isn’t always trivial and ideally with a clustering algorithm we’d want it to figure those out for us because the point of it is to gain some insight from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.Mean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration.An illustration of the entire process from end-to-end with all of the sliding windows is shown below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.In contrast to K-means clustering, there is no need to select the number of clusters as mean-shift automatically discovers this. That’s a massive advantage. The fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data-driven sense. The drawback is that the selection of the window size/radius “r” can be non-trivial.DBSCAN is a density-based clustered algorithm similar to mean-shift, but with a couple of notable advantages. Check out another fancy graphic below and let’s get started!DBSCAN poses some great advantages over other clustering algorithms. Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises, unlike mean-shift which simply throws them into a cluster even if the data point is very different. Additionally, it can find arbitrarily sized and arbitrarily shaped clusters quite well.The main drawback of DBSCAN is that it doesn’t perform as well as others when the clusters are of varying density. This is because the setting of the distance threshold ε and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold ε becomes challenging to estimate.One of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. We can see why this isn’t the best way of doing things by looking at the image below. On the left-hand side, it looks quite obvious to the human eye that there are two circular clusters with different radius’ centered at the same mean. K-Means can’t handle this because the mean values of the clusters are very close together. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center.Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! Taking an example in two dimensions, this means that the clusters can take any kind of elliptical shape (since we have a standard deviation in both the x and y directions). Thus, each Gaussian distribution is assigned to a single cluster.To find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation), we will use an optimization algorithm called Expectation–Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. Then we can proceed with the process of Expectation–Maximization clustering using GMMs.There are 2 key advantages to using GMMs. Firstly GMMs are a lot more flexible in terms of cluster covariance than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster’s covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support mixed membership.Hierarchical clustering algorithms fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm stepsHierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can’t do this. These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of O(n³), unlike the linear complexity of K-Means and GMM.There are your top 5 clustering algorithms that a data scientist should know! We’ll end off with an awesome visualization of how well these algorithms and a few others perform, courtesy of Scikit Learn! Very cool to see how the different algorithms compare and contrast with different data!",05/02/2018,0,5.0,7.0,593.0,413.0,8.0,5.0,0.0,1.0,en
4178,Understanding Attention Mechanism,Medium,Shashank Yadav,85.0,5.0,800.0,"Attention mechanism for sequence modelling was first introduced in the paper: Neural Machine Translation by jointly learning to align and translate, Bengio et. al. ICLR 2015. Even though the paper itself mentions the word “attention” scarcely (3 times total in 2 consecutive lines!!) the term has caught on. A lot of prominent work that came later on uses the same naming convention (Well, I for one think it’s more of a “soft memory” rather than “attention”).This post focuses on Bengio et. al. 2015 and tries to give a step by step explanation of the (attention) model explained in their paper. Probably it’s just me but the explanation given in the paper and the diagrams that came with it left a lot to the imagination. This post tries to make understanding their great work a little easier. So here we go:The main assumption in sequence modelling networks such as RNNs, LSTMs and GRUs is that the current state holds information for the whole of input seen so far. Hence the final state of a RNN after reading the whole input sequence should contain complete information about that sequence. This seems to be too strong a condition and too much to ask.Attention mechanism relax this assumption and proposes that we should look at the hidden states corresponding to the whole input sequence in order to make any prediction. But how do we decide which states to look at? Well, try learning that!!As introduced in the previous section, the task at hand is:To “learn” how much we need to “attend” to each hidden state of the encoder.The complete architecture is as follows:The network is shown in a state when the encoder (lower part of the Fig 2) has computed the hidden states hⱼ corresponding to each input Xⱼ and the decoder (top part of Fig 2) has run for t-1 steps and is now going to produce output for time step t.Don’t get too nervous looking at this seemingly difficult figure. We’ll be going through each component one by one. Broadly, the whole process can be divided into four steps:Let’s have a look!!Hence the hidden state for the jᵗʰ input hⱼ is the concatenation of jᵗʰ hidden states of forward and backward RNNs. We’ll be using a weighted linear combination of all of these hⱼs to make predictions at each step of the decoder. The decoder output length might be same or different than that of encoder.At each time step t of the decoder the amount of attention to be paid to the hidden encoder unit hⱼ is denoted by αₜⱼ and calculated as a function of both hⱼ and previous hidden state of decoder s ₜ-₁:In the paper a is parametrized as a feedforward neural network that runs for all j at the decoding time step t. Note the 0 ≤ αₜⱼ ≤ 1 and that all ∑ⱼ αₜⱼ = 1 because of the softmax on eₜⱼ. These αₜⱼ can be visualized as the attention paid by decoder at time step t to hidden encoder unit hⱼ.Time to make use of the attention weights we’ve computed in the preceding step!! Here’s another figure to help understand:The context vector is simply a linear combination of the hidden weights hⱼ weighted by the attention values αₜⱼ that we computed:From the equation we can see that αₜⱼ determines how much hⱼ affects the context cₜ. Higher the value, higher the impact of hⱼ on the context for time t.We are nearly there! All there remains is to use the context vector cₜ we worked so hard to compute, along with the previous hidden state of the decoder s ₜ-₁ and the previous output yₜ-₁and use all of them to compute the new hidden state and output of the decoder: sₜ and yₜ respectively.In the paper, authors have used a GRU cell for f and a similar function for g. These are higher level details and if you’re interested I’d suggest you have a look into Appendix A of the paper. For details about training look into Appendix B of the paper.So yeah, it’s over. This is it. Attention mastered! (Probably at some level of understanding we did). Thanks a lot for sticking by till now.This was the first paper that introduced that concept of attention and several works have come since then that have built on top of this. This idea has been the most useful in NLP where the state of the art transformer networks that utilize self attention have taken the field by storm. This tutorial would be the first step for taking deep dive into the field. I hope this was helpful. Looking forward to your comments and suggestions.Note: All the diagrams used here have been created by the author. Feel free to use them along with a note of acknowledgement :-)",06/02/2019,0,28.0,8.0,1400.0,625.0,10.0,1.0,0.0,1.0,en
4179,"Ensemble methods: bagging, boosting and stacking",Towards Data Science,Joseph Rocca,3700.0,20.0,3890.0,"This post was co-written with “Unity is strength”. This old saying expresses pretty well the underlying idea that rules the very powerful “ensemble methods” in machine learning. Roughly, ensemble learning methods, that often trust the top rankings of many machine learning competitions (including Kaggle’s competitions), are based on the hypothesis that combining multiple models together can often produce a much more powerful model.The purpose of this post is to introduce various notions of ensemble learning. We will give the reader some necessary keys to well understand and use related methods and be able to design adapted solutions when needed. We will discuss some well known notions such as boostrapping, bagging, random forest, boosting, stacking and many others that are the basis of ensemble learning. In order to make the link between all these methods as clear as possible, we will try to present them in a much broader and logical framework that, we hope, will be easier to understand and remember.In the first section of this post we will present the notions of weak and strong learners and we will introduce three main ensemble learning methods: bagging, boosting and stacking. Then, in the second section we will be focused on bagging and we will discuss notions such that bootstrapping, bagging and random forests. In the third section, we will present boosting and, in particular, its two most popular variants: adaptative boosting (adaboost) and gradient boosting. Finally in the fourth section we will give an overview of stacking.Ensemble learning is a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.In machine learning, no matter if we are facing a classification or a regression problem, the choice of the model is extremely important to have any chance to obtain good results. This choice can depend on many variables of the problem: quantity of data, dimensionality of the space, distribution hypothesis…A low bias and a low variance, although they most often vary in opposite directions, are the two most fundamental features expected for a model. Indeed, to be able to “solve” a problem, we want our model to have enough degrees of freedom to resolve the underlying complexity of the data we are working with, but we also want it to have not too much degrees of freedom to avoid high variance and be more robust. This is the well known bias-variance tradeoff.In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for designing more complex models by combining several of them. Most of the time, these basics models perform not so well by themselves either because they have a high bias (low degree of freedom models, for example) or because they have too much variance to be robust (high degree of freedom models, for example). Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a strong learner (or ensemble model) that achieves better performances.In order to set up an ensemble learning method, we first need to select our base models to be aggregated. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways. The ensemble model we obtain is then said to be “homogeneous”. However, there also exist some methods that use different type of base learning algorithms: some heterogeneous weak learners are then combined into an “heterogeneous ensembles model”.One important point is that our choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it should be with an aggregating method that tends to reduce variance whereas if we choose base models with low variance but high bias, it should be with an aggregating method that tends to reduce bias.This brings us to the question of how to combine these models. We can mention three major kinds of meta-algorithms that aims at combining weak learners:Very roughly, we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).In the following sections, we will present in details bagging and boosting (that are a bit more widely used than stacking and will allow us to discuss some key notions of ensemble learning) before giving a brief overview of stacking.In parallel methods we fit the different considered learners independently from each others and, so, it is possible to train them concurrently. The most famous such approach is “bagging” (standing for “bootstrap aggregating”) that aims at producing an ensemble model that is more robust than the individual models composing it.Let’s begin by defining bootstrapping. This statistical technique consists in generating samples of size B (called bootstrap samples) from an initial dataset of size N by randomly drawing with replacement B observations.Under some assumptions, these samples have pretty good statistical properties: in first approximation, they can be seen as being drawn both directly from the true underlying (and often unknown) data distribution and independently from each others. So, they can be considered as representative and independent samples of the true data distribution (almost i.i.d. samples). The hypothesis that have to be verified to make this approximation valid are twofold. First, the size N of the initial dataset should be large enough to capture most of the complexity of the underlying distribution so that sampling from the dataset is a good approximation of sampling from the real distribution (representativity). Second, the size N of the dataset should be large enough compared to the size B of the bootstrap samples so that samples are not too much correlated (independence). Notice that in the following, we will sometimes make reference to these properties (representativity and independence) of bootstrap samples: the reader should always keep in mind that this is only an approximation.Bootstrap samples are often used, for example, to evaluate variance or confidence intervals of a statistical estimators. By definition, a statistical estimator is a function of some observations and, so, a random variable with variance coming from these observations. In order to estimate the variance of such an estimator, we need to evaluate it on several independent samples drawn from the distribution of interest. In most of the cases, considering truly independent samples would require too much data compared to the amount really available. We can then use bootstrapping to generate several bootstrap samples that can be considered as being “almost-representative” and “almost-independent” (almost i.i.d. samples). These bootstrap samples will allow us to approximate the variance of the estimator, by evaluating its value for each of them.When training a model, no matter if we are dealing with a classification or a regression problem, we obtain a function that takes an input, returns an output and that is defined with respect to the training dataset. Due to the theoretical variance of the training dataset (we remind that a dataset is an observed sample coming from a true unknown underlying distribution), the fitted model is also subject to variability: if another dataset had been observed, we would have obtained a different model.The idea of bagging is then simple: we want to fit several independent models and “average” their predictions in order to obtain a model with a lower variance. However, we can’t, in practice, fit fully independent models because it would require too much data. So, we rely on the good “approximate properties” of bootstrap samples (representativity and independence) to fit models that are almost independent.First, we create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution. Then, we can fit a weak learner for each of these samples and finally aggregate them such that we kind of “average” their outputs and, so, obtain an ensemble model with less variance that its components. Roughly speaking, as the bootstrap samples are approximatively independent and identically distributed (i.i.d.), so are the learned base models. Then, “averaging” weak learners outputs do not change the expected answer but reduce its variance (just like averaging i.i.d. random variables preserve expected value but reduce variance).So, assuming that we have L bootstrap samples (approximations of L independent datasets) of size B denotedwe can fit L almost independent weak learners (one on each dataset)and then aggregate them into some kind of averaging process in order to get an ensemble model with a lower variance. For example, we can define our strong model such thatThere are several possible ways to aggregate the multiple models fitted in parallel. For a regression problem, the outputs of individual models can literally be averaged to obtain the output of the ensemble model. For classification problem the class outputted by each model can be seen as a vote and the class that receives the majority of the votes is returned by the ensemble model (this is called hard-voting). Still for a classification problem, we can also consider the probabilities of each classes returned by all the models, average these probabilities and keep the class with the highest average probability (this is called soft-voting). Averages or votes can either be simple or weighted if any relevant weights can be used.Finally, we can mention that one of the big advantages of bagging is that it can be parallelised. As the different models are fitted independently from each others, intensive parallelisation techniques can be used if required.Learning trees are very popular base models for ensemble methods. Strong learners composed of multiple trees can be called “forests”. Trees that compose a forest can be chosen to be either shallow (few depths) or deep (lot of depths, if not fully grown). Shallow trees have less variance but higher bias and then will be better choice for sequential methods that we will described thereafter. Deep trees, on the other side, have low bias but high variance and, so, are relevant choices for bagging method that is mainly focused at reducing variance.The random forest approach is a bagging method where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance. However, random forests also use another trick to make the multiple fitted trees a bit less correlated with each others: when growing each tree, instead of only sampling over the observations in the dataset to generate a bootstrap sample, we also sample over features and keep only a random subset of them to build the tree.Sampling over features has indeed the effect that all trees do not look at the exact same information to make their decisions and, so, it reduces the correlation between the different returned outputs. Another advantage of sampling over the features is that it makes the decision making process more robust to missing data: observations (from the training dataset or not) with missing data can still be regressed or classified based on the trees that take into account only features where data are not missing. Thus, random forest algorithm combines the concepts of bagging and random feature subspace selection to create more robust models.In sequential methods the different combined weak models are no longer fitted independently from each others. The idea is to fit models iteratively such that the training of model at a given step depends on the models fitted at the previous steps. “Boosting” is the most famous of these approaches and it produces an ensemble model that is in general less biased than the weak learners that compose it.Boosting methods work in the same spirit as bagging methods: we build a family of models that are aggregated to obtain a strong learner that performs better. However, unlike bagging that mainly aims at reducing variance, boosting is a technique that consists in fitting sequentially multiple weak learners in a very adaptative way: each model in the sequence is fitted giving more importance to observations in the dataset that were badly handled by the previous models in the sequence. Intuitively, each new model focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias (even if we can notice that boosting can also have the effect of reducing variance). Boosting, like bagging, can be used for regression as well as for classification problems.Being mainly focused at reducing bias, the base models that are often considered for boosting are models with low variance but high bias. For example, if we want to use trees as our base models, we will choose most of the time shallow decision trees with only a few depths. Another important reason that motivates the use of low variance but high bias models as weak learners for boosting is that these models are in general less computationally expensive to fit (few degrees of freedom when parametrised). Indeed, as computations to fit the different models can’t be done in parallel (unlike bagging), it could become too expensive to fit sequentially several complex models.Once the weak learners have been chosen, we still need to define how they will be sequentially fitted (what information from previous models do we take into account when fitting current model?) and how they will be aggregated (how do we aggregate the current model to the previous ones?). We will discuss these questions in the two following subsections, describing more especially two important boosting algorithms: adaboost and gradient boosting.In a nutshell, these two meta-algorithms differ on how they create and aggregate the weak learners during the sequential process. Adaptive boosting updates the weights attached to each of the training dataset observations whereas gradient boosting updates the value of these observations. This main difference comes from the way both methods try to solve the optimisation problem of finding the best model that can be written as a weighted sum of weak learners.In adaptative boosting (often called “adaboost”), we try to define our ensemble model as a weighted sum of L weak learnersFinding the best ensemble model with this form is a difficult optimisation problem. Then, instead of trying to solve it in one single shot (finding all the coefficients and weak learners that give the best overall additive model), we make use of an iterative optimisation process that is much more tractable, even if it can lead to a sub-optimal solution. More especially, we add the weak learners one by one, looking at each iteration for the best possible pair (coefficient, weak learner) to add to the current ensemble model. In other words, we define recurrently the (s_l)’s such thatwhere c_l and w_l are chosen such that s_l is the model that fit the best the training data and, so, that is the best possible improvement over s_(l-1). We can then denotewhere E(.) is the fitting error of the given model and e(.,.) is the loss/error function. Thus, instead of optimising “globally” over all the L models in the sum, we approximate the optimum by optimising “locally” building and adding the weak learners to the strong model one by one.More especially, when considering a binary classification, we can show that the adaboost algorithm can be re-written into a process that proceeds as follow. First, it updates the observations weights in the dataset and train a new weak learner with a special focus given to the observations misclassified by the current ensemble model. Second, it adds the weak learner to the weighted sum according to an update coefficient that expresse the performances of this weak model: the better a weak learner performs, the more it contributes to the strong learner.So, assume that we are facing a binary classification problem, with N observations in our dataset and we want to use adaboost algorithm with a given family of weak models. At the very beginning of the algorithm (first model of the sequence), all the observations have the same weights 1/N. Then, we repeat L times (for the L learners in the sequence) the following steps:Repeating these steps, we have then build sequentially our L models and aggregate them into a simple linear combination weighted by coefficients expressing the performance of each learner. Notice that there exists variants of the initial adaboost algorithm such that LogitBoost (classification) or L2Boost (regression) that mainly differ by their choice of loss function.In gradient boosting, the ensemble model we try to build is also a weighted sum of weak learnersJust as we mentioned for adaboost, finding the optimal model under this form is too difficult and an iterative approach is required. The main difference with adaptative boosting is in the definition of the sequential optimisation process. Indeed, gradient boosting casts the problem into a gradient descent one: at each iteration we fit a weak learner to the opposite of the gradient of the current fitting error with respect to the current ensemble model. Let’s try to clarify this last point. First, theoretical gradient descent process over the ensemble model can be writtenwhere E(.) is the fitting error of the given model, c_l is a coefficient corresponding to the step size andis the opposite of the gradient of the fitting error with respect to the ensemble model at step l-1. This (pretty abstract) opposite of the gradient is a function that can, in practice, only be evaluated for observations in the training dataset (for which we know inputs and outputs): these evaluations are called pseudo-residuals attached to each observations. Moreover, even if we know for the observations the values of these pseudo-residuals, we don’t want to add to our ensemble model any kind of function: we only want to add a new instance of weak model. So, the natural thing to do is to fit a weak learner to the pseudo-residuals computed for each observation. Finally, the coefficient c_l is computed following a one dimensional optimisation process (line-search to obtain the best step size c_l).So, assume that we want to use gradient boosting technique with a given family of weak models. At the very beginning of the algorithm (first model of the sequence), the pseudo-residuals are set equal to the observation values. Then, we repeat L times (for the L models of the sequence) the following steps:Repeating these steps, we have then build sequentially our L models and aggregate them following a gradient descent approach. Notice that, while adaptative boosting tries to solve at each iteration exactly the “local” optimisation problem (find the best weak learner and its coefficient to add to the strong model), gradient boosting uses instead a gradient descent approach and can more easily be adapted to large number of loss functions. Thus, gradient boosting can be considered as a generalization of adaboost to arbitrary differentiable loss functions.Stacking mainly differ from bagging and boosting on two points. First stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners. Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms.As we already mentioned, the idea of stacking is to learn several different weak learners and combine them by training a meta-model to output predictions based on the multiple predictions returned by these weak models. So, we need to define two things in order to build our stacking model: the L learners we want to fit and the meta-model that combines them.For example, for a classification problem, we can choose as weak learners a KNN classifier, a logistic regression and a SVM, and decide to learn a neural network as meta-model. Then, the neural network will take as inputs the outputs of our three weak learners and will learn to return final predictions based on it.So, assume that we want to fit a stacking ensemble composed of L weak learners. Then we have to follow the steps thereafter:In the previous steps, we split the dataset in two folds because predictions on data that have been used for the training of the weak learners are not relevant for the training of the meta-model. Thus, an obvious drawback of this split of our dataset in two parts is that we only have half of the data to train the base models and half of the data to train the meta-model. In order to overcome this limitation, we can however follow some kind of “k-fold cross-training” approach (similar to what is done in k-fold cross-validation) such that all the observations can be used to train the meta-model: for any observation, the prediction of the weak learners are done with instances of these weak learners trained on the k-1 folds that do not contain the considered observation. In other words, it consists in training on k-1 fold in order to make predictions on the remaining fold and that iteratively so that to obtain predictions for observations in any folds. Doing so, we can produce relevant predictions for each observation of our dataset and then train our meta-model on all these predictions.A possible extension of stacking is multi-level stacking. It consists in doing stacking with multiple layers. As an example, let’s consider a 3-levels stacking. In the first level (layer), we fit the L weak learners that have been chosen. Then, in the second level, instead of fitting a single meta-model on the weak models predictions (as it was described in the previous subsection) we fit M such meta-models. Finally, in the third level we fit a last meta-model that takes as inputs the predictions returned by the M meta-models of the previous level.From a practical point of view, notice that for each meta-model of the different levels of a multi-levels stacking ensemble model, we have to choose a learning algorithm that can be almost whatever we want (even algorithms already used at lower levels). We can also mention that adding levels can either be data expensive (if k-folds like technique is not used and, then, more data are needed) or time expensive (if k-folds like technique is used and, then, lot of models need to be fitted).The main takeaways of this post are the following:In this post we have given a basic overview of ensemble learning and, more especially, of some of the main notions of this field: bootstrapping, bagging, random forest, boosting (adaboost, gradient boosting) and stacking. Among the notions that were left aside we can mention for example the Out-Of-Bag evaluation technique for bagging or also the very popular “XGBoost” (that stands for eXtrem Gradient Boosting) that is a library that implements Gradient Boosting methods along with a great number of additional tricks that make learning much more efficient (and tractable for big dataset).Finally, we would like to conclude by reminding that ensemble learning is about combining some base models in order to obtain an ensemble model with better performances/properties. Thus, even if bagging, boosting and stacking are the most commonly used ensemble methods, variants are possible and can be designed to better adapt to some specific problems. This mainly requires two things: fully understand the problem we are facing… and be creative!Thanks for reading!Our last articles with towardsdatascience.comtowardsdatascience.com",23/04/2019,0,43.0,3.0,1196.0,446.0,21.0,5.0,0.0,6.0,en
4180,How to Improve Naive Bayes?,Analytics Vidhya,Kopal Jain,64.0,4.0,227.0,"Reference How to Implement Naive Bayes? Section 2: Building the Model in Python, prior to continuing…[10] Define Grid Search ParametersWhy this step: To set the selected parameters used to find the optimal combination. By referencing the sklearn.naive_bayes.GaussianNB documentation, you can find a completed list of parameters with descriptions that can be used in grid search functionalities.[11] Hyperparameter Tune using Training DataNote: Total number of fits is 1000 since the cv is defined as 10 and there are 100 candidates (var_smoothing has 100 defined parameters). Therefore, the calculation for a total number of fits → 10 x [100] = 1000.Why this step: To find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results.[12] Predict on Testing DataWhy this step: To obtain model prediction on testing data to evaluate the model’s accuracy and efficiency.[13] Numeric AnalysisNote: Using the confusion matrix, the True Positive, False Positive, False Negative, and True Negative values can be extracted which will aid in the calculation of the accuracy score, precision score, recall score, and f1 score:Why this step: To evaluate the performance of the tuned classification model. As you can see, the accuracy, precision, recall, and F1 scores all have improved by tuning the model from the basic Gaussian Naive Bayes model created in Section 2.github.comNext Up — Why use Naive Bayes? Section 4: Evaluating the Model Tradeoffs…",02/04/2021,4,10.0,14.0,1062.0,495.0,2.0,3.0,0.0,5.0,en
4181,Object detection in Deep learning (Part2),"AI³ | Theory, Practice, Business",Amin Ag,4.0,4.0,496.0,"R-CNN & Fast R-CNNFollowing part1, an object-detection-algorithm has to draw up to several bounding boxes representing different objects of interest within the image and you would not know how many beforehand.A direct approach (brut force) to solve this issue would be to take different regions of interest from the image and use a CNN to classify the presence of the object within that region. The problem here, the objects of interest might have different spatial locations within the image and different aspect ratios. Hence, you would have to select a huge number of regions and this could computationally hard (increasingly hard). Therefore, algorithms like R-CNN, YOLO, etc have been developed to find these occurrences and find them fast.The proposed idea [1], instead of trying to classify a huge number of regions, you can just work with 2000 regions. These 2000 region proposals are generated using the selective search algorithm.These 2000 candidate region proposals are fed into a convolutional neural network that produces a 4096-dimensional feature vector as output. The CNN acts as a feature extractor and the output dense layer consists of the features extracted from the image and the extracted features are fed into a classifier to detect the presence of the object within that candidate region proposal. Also, it predicts the four offset values to increase the precision of the bounding box.A faster object detection algorithm called Fast R-CNNThe approach is similar to the R-CNN algorithm. But, instead of feeding the region proposals to the CNN, we feed the input image to the CNN to generate a convolutional feature map. From the convolutional feature map, we identify the region of proposals and warp them into squares and by using an RoI pooling layer we reshape them into a fixed size so that it can be fed into a fully connected layer.The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.Similar to Fast R-CNN, the image is provided as an input to a convolutional network which provides a convolutional feature map. Instead of using selective search algorithm on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.(You Only Look Once)The algorithm applies a neural network to an entire image. The method used to come up with these probabilities is logistic regression. The bounding boxes are weighted by the associated probabilities. For class prediction, independent logistic classifiers are used.YOLO divides the input image into an S×S grid. Each grid cell predicts only one object.For each grid cell,R-CNN; Ross Girshick et al.Faster-R-CNN ; Shaoqing Ren et al.The Original Paper on YOLOv3, https://pjreddie.com/media/files/papers/YOLOv3.pdf",22/09/2019,0,8.0,0.0,1116.0,601.0,3.0,4.0,0.0,5.0,en
4182,"A brief overview of R-CNN, Fast R-CNN and Faster R-CNN",MLearning.ai,Sema Zeynep Bulut,4.0,6.0,1129.0,"R-CNN architecture is used to detect the classes of objects in the images and the bounding boxes of these objects. RCNN architecture has been developed since classification cannot be made for more than one object with CNN in visuals containing more than one object.The general working principle of R-CNN takes place in two steps. First, the features where the object can be found in the visual are determined with selective search, then after the regions are determined, each region is given as an input to a CNN model and the prediction process is performed for classes and bounding boxes.Selective Search:It is used to determine the regions on the image that should be captured. Small areas are determined first. Then, similar regions are combined to create larger new regions. This process continues repeatedly, larger regions are created at each step during the process, however, the objects in the visual are actually clustered.At R-CNN, we use selective search to identify candidates for specific regions. Each region candidate is given as an input to different CNN networks. As a result of the operations performed on the region island, approximately 2000 different regions are obtained, 2000 CNN network is used for the 2000 regions obtained. Classes of objects are determined with SVM by using features from these networks and bounding boxes of objects are determined by regression.Intersection Over Union (IoU) score refers to the accuracy of the predicted bounding boxes. The obtained bounding box and the real bounding box are compared, with this comparison process, the IoU score is obtained.→ The intersection of the predicted area for the bounding box and the real bounding box area / The combination of the predicted area for the bounding box and the real area of the bounding boxNon-Max Supression:Not all regions obtained from the images are used. Non-Max Supression technique is used to obtain the correct areas. With this technique, the bounding boxes obtained with an Intersection Over Union (IoU) score greater than 0.5 are kept and other bounding boxes are suppressed. If more than 0.5 bounding boxes are obtained for an object, the bounding box with the highest IoU score is used.The cost of R-CNN models is quite high because nearly 2000 different candidate regions are extracted for each image, different CNN networks are used for each region. These process steps cause both a great cost and a long training time.For this reason, different CNN models created for each region in the R-CNN architecture were removed and Fast R-CNN architecture was developed using a single CNN for the regions. Unlike R-CNN, the use of CNN, SVM and Regressor has been developed. The architecture created with the combination of CNN, SVM and Regressor performed very well with the developed models.The whole image is processed with CNN and feature maps are obtained. Required features for region recommendations are collected (region proposal feature map). Then, max pooling is applied to the obtained feature maps, and the dimensions of the feature maps are reduced. The layer where the max pooling process is performed is called the RoI (Region of Interest) pooling layer. Feature maps of reduced dimensions are transformed into a one-dimensional vector and given as input to the CNN model. With Softmax, the class information of the object in the region is determined, while the bounding box regressor of the object is determined.It works about 10 times faster than R-CNN.Because selective search applied in R-CNN and Fast R-CNN is costly in terms of computations , Region Proporsal Network (RPN) is used in Faster R-CNN. How efficient the use of RPN is has been demonstrated by certain studies.Any size image is taken as input. The image is then given as input to a CNN model. If the models you use are models such as VGG16, AlexNet, reconstruct the model without using a fully connected layer in the model. Because we have to give feature map as input for RPN, feature maps are also produced by convolutional layers.An important structure we need to know when talking about RPN: anchor boxes. Anchors are boxes with different scales and aspect ratios. While the small network to be created is sliding on the feature map, an object search is made in the feature map in accordance with the anchor.The first convolutional layer in RPN applies a filter of 3x3 by default, the number of output channels is 512. This conv layer takes the feature map as input. The output of the convolutional layer is given as input to two different convolutional layers. Both have a filter size of 1x1. The first layer is cls, ie the classification layer. The cls layer informs us whether there is an object in the location of the sliding window or not. A binary classification process is applied here. In the case of an object 1, in case of no object 0, in the modeling stage, the sigmoid activation function is used with the convolutional layer in general. The number of output channels of this layer is expressed as 2 * 9, by default the value of k is 9 (therefore the filter size in the first layer is 3x3), the number of output channels of the Cls layer should be 2 * k. The Reg layer works parallel to the Cls layer. Reg layer is responsible for drawing bounding boxes of objects detected in Cls. Since it contains 4 coordinate information, the number of output channels of this layer is expressed as 4 * 9 (4 * k). Linear activation function is preferred with conv layer for Reg layer in model coding.In summary, as a result of the filtering processes, the output of RPN is anchor boxes marked as having an object. Bounding boxes with a value of 0 in the cls layer are put into the background.The RoI layer that comes after RPN receives the outputs of RPN’s cls and reg layers as input, along with the feature map that RPN takes as input. The RoI layer is responsible for making the size of each feature map the same before the fully connected layer, which is the last layer of Faster R-CNN. Since the region recommendations from RPN are of different sizes (because anchor boxes have different scales and aspect ratios), it is necessary to produce feature maps with fixed size. The RoI layer uses a 7x7 max pooling layer for size. Number of output layer is 512.The values obtained after the RoI are subjected to the classification process by giving them as input to the fully connected layer after flattening. In the classification process, the bounding box of the object whose class is predicted is created with the help of regressor.I hope I was able to convey the basics of R-CNN, Fast R-CNN and Faster R-CNN correctly. Thank you for reading!Resources:1- https://arxiv.org/abs/1506.014972- https://www.youtube.com/watch?v=iHf2xHQ2VYo3- https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e4- https://www.hackerearth.com/blog/developers/object-detection-for-self-driving-cars/",06/05/2021,0,0.0,2.0,703.0,445.0,7.0,0.0,0.0,4.0,en
4183,Evasion attacks on Machine Learning (or “Adversarial Examples”),Towards Data Science,ilmoi,240.0,12.0,2327.0,"Machine learning is exciting. However, just like any new technology or invention, not only does ML enable new amazing capabilities — but also, unfortunately, new vulnerabilities.Previously I’ve discussed how to think about these vulnerabilities in a structured way (or how to develop a “threat model” for your ML). This time I’d like to dive deep into how your ML system can be exploited during inference time through what is known as an evasion attack.With no time to waste, let’s get started.An evasion attack happens when the network is fed an “adversarial example” — a carefully perturbed input that looks and feels exactly the same as its untampered copy to a human — but that completely throws off the classifier.Despite all the hype around adversarial examples being a “new” phenomenon — they’re not actually that new. A paper by one of the leading names in Adversarial ML, Battista Biggio, pointed out that the field of attacking machine learning dates back as far as 2004. Back then adversarial examples were studied in the context of spam filtering, showing that linear classifiers could be easily tricked by few carefully-crafted changes in the content of spam emails.Full timeline:The “newness” of adversarial examples really came from them being discovered in the context of deep learning. In 2013 Christian Szegedy of Google AI was working on understanding how neural nets “think” but instead discovered an intriguing property that all of them seem to possess — being easily fooled by small perturbations. Given how big deep learning has become — adversarial example and the field of Adversarial ML have ended up right in the spotlight.When talking about adversarial examples I will mainly use computer vision papers — but really they’re equally applicable to any type of machine learning system, from complicated to not. Some of my personal favorites below:Image: fooling face detection with adversarial glasses.Video: stealth T-shirt at DEFCON.Audio: speech-to-text transcription.Language: sentiment analysis and textual entailment.Reinforcement learning: boat racing.Malware: evading a malware classifier using RL.It’s a good question. As of today there is no consensus in the community as to why that might be. A number of hypotheses exist.The first and original hypothesis trying to explain adversarial examples came from Szegedy’s own paper, where they argued that they exist due to the presence of low-probability “pockets” in the manifold (ie too much non-linearity) and poor regularization of networks.Later an opposing theory emerged, pioneered by Goodfellow, arguing that in fact adversarial examples occurred due to too much linearity in modern machine learning and especially deep learning systems. Goodfellow argued that activations functions like ReLU and Sigmoid are basically straight lines in the middle (where, it just so happens we prefer to keep our gradients to prevent them from exploding or vanishing). And so really inside a neural net you have a ton of linear functions, all perpetuating one another’s input, all in the same direction. If you then add tiny perturbations to some of the inputs (a few pixels here and there) that accumulates into massive difference on the other end of the network and it spits out gibberish.The third and perhaps most commonly adopted hypothesis today is the tilted boundary. In a nutshell, the authors argue that because the model never fits the data perfectly (otherwise test set accuracy would have always been 100%) — there will always be adversarial pockets of inputs that exist between the boundary of the classifier and the actual sub-manifold of sampled data. They also empirically debunk the previous two approaches, so if you are technical this paper is worth a read.Then there’s a few others, very mathy explanations:And finally there’s my personal favorite. A recent paper from MIT argues that adversarial examples are not a bug — they’re a feature of how neural networks see the world. Just because we humans are limited to 3 dimensions and can’t distinguish noise patterns from one another doesn’t mean those noise patterns are not good features. Our eyes are just bad sensors. So what we’re really dealing with here is a pattern recognition machine more sophisticated than ourselves — and instead of calling patterns that we don’t understand “adversarial examples” we should just accept there’s more to the world than meets the eye (get it?).If the entire paper is too long — I highly recommend this 11min blog summary.That paper, btw, uniquely explains another really interesting phenomenon related to adversarial examples — their transferability. Previous work pointed out that adversarial example easily transfer between both different models and different datasets. That’s right — you could have trained a model on dogs vs cats, but then can fool a completely different model trained on hotdogs vs not hotdogs with the same adversarial noise added to the input. Because the MIT paper points out that noise is actually a feature, it makes sense that other images in the image realm (be they cats, dogs, hotdogs or not hotdogs) all share it. Fascinating stuff.Attacker’s knowledge of the target system (or their “capability”) is important. The more they know about your model and how its built — the easier it is for them to mount an attack on it.At Calypso we’ve broken down evasion attacks into five separate classes:Gradient-based attacks by definition require access to the model’s gradients and are therefore a type of WhiteBox attacks. These are hands down the most powerful, because the attacker can use their detailed understanding of how the model thinks (its gradients) to mathematically optimize the attack.This is also the technique that works best for attacking hardened models. In fact, research has shown that if the attacker has access to the model’s gradients — they will always be able to craft a new set of adversarial examples to fool the model. This is why adversarial examples are such a big deal — security through obscurity aside it’s actually pretty hard to defend against them (more on this later).The three most powerful gradient-based attacks as of today are:Confidence score attacks use the outputted classification confidence to estimate the gradients of the model, and then perform similar smart optimization to gradient-based attacks above. This approach doesn’t require the attacker to know anything about the model and hence is of the BlackBox type.The three most powerful confidence-based attacks as of today are:Hard label attacks rely solely on the label outputted by the model (“cat”, “dog”, “hotdog”) and don’t require the confidence scores. This makes the attack more dumb — but arguably more realistic (how many public endpoints do you know of that output a classification score?). The most powerful attack in this category remains the Boundary Attack.Surrogate model attacks are very similar to gradient-based attacks, except they require an extra step. When the adversary doesn’t have access to the model’s internals but still wants to mount a WhiteBox attack, they can try to first rebuild the target’s model on their machine. They have a few options:Finally, there are brute-force attacks. These attacks use no optimization at all to generate adversarial examples and instead resort to simple things like:In short — it’s hard. We’re really between a rock and a hard place here. Let’s examine the two broad categories of defenses — empirical defenses and formal methods.Let’s start with formal methods because they are quicker to discuss. Those of you who worked in industries like chip design or aerospace & defense will be familiar with formal methods. For everyone else — formal methods are a mathematical technique used to guarantee the robustness of software / hardware systems. In most of software, if you design or build the thing wrong it might lead to some downtime and a few angry customer reviews — but it probably won’t kill anyone. In some industries, however, such flaws are unacceptable. You can’t manufacture 100m chips only to realize some part of the chip is flawed — and you can’t put a plane in the air unless you’ve mathematically verified every component works as intended.The way formal methods work, very simply, is by attempting every possible scenario and seeing how it plays out. In the world of evasion attacks that means trying to generate every possible adversarial example within a certain radius of perturbation. As an example, imagine you have an image with just two grayscale pixels — let’s say 180 and 80. Then let’s say you decide on a perturbation range of 3 in each direction. That gives you (3+1+3)² or 49 combinations to try out — and if you wanted to formally verify none of those are adversarial you would have to put each one through your model and see what comes out there other end.49 combinations for an image with 2 dimensions and no color. What about a 1000 x 1000px colored image with same perturbation range of 3 in each direction? That’s (3+1+3)^(3*1000*1000) combinations to check for (excel refused to produce a number)!Hopefully you can quickly see the problem with formal methods — they’re not cheap and often completely intractable from a computational perspective. In fact, as applied to neural networks, state of the art formal method techniques today can’t verify a network more than a few layers deep. So for now it’s a worthy — but elusive aspiration.If you did want to read more into formal methods for neural networks, here are some good papers:Otherwise let’s move on to empirical defenses. Empirical defenses are, as the name suggests, relying on experiments to demonstrate the effectiveness of a defense. Eg you might attack a model twice — first a normal, undefended version, then a hardened version — and observe how easy each is to fool (hopefully the hardened model performs better). So where formal methods are trying to compute every possible scenario and in doing so verify no adversarial examples exist — empirical methods take the approach of “you see it works, why do the math”.Let’s go through some of the more popular types:Adversarial training — hands down the most talked-about and arguably most effective defense today. During adversarial training the defender retrains the model with adversarial examples included in the training pool, but labelled with correct labels. This teaches the model to ignore the noise and only learn from “robust” features.The problem with adversarial training is that it only defends your model against the same attacks used to craft the examples originally included in the training pool. Thus if someone then mounts an attack optimized using a different algorithm, or mounts an adaptive attack (ie a whitebox attack on a defended model) — they are able to fool the classifier anew, as if no defense was in place.You might think that, well, you could keep retraining your model including newly forged adversarial examples over and over again — but at some point you’ve inserted so much fake data into the training set that the boundary the model learns basically becomes useless.That said, if the goal is to simply make it harder for the attacker to bypass the classifier — then adversarial training is a solid choice. There’s also a lot of creative approaches to actually doing it:Next, gradient masking. It’s basically a non-defense. For a while the Adversarial ML community thought that because gradients are needed to compute powerful attacks on models, hiding the gradients should solve the issue. Defensive distillation was very popular — but quickly proved to be false.The reason gradient masking doesn’t work as a defense tactic is because of adversarial examples’ transferability property. Even if you succeed at hiding the gradients of your model — the attacker can still build a surrogate, attack it, then transfer the examples.Input modification happens when an input, before being passed to the model, is in some way “cleaned” to get rid of adversarial noise. Examples include all sorts of denoising solutions (autoencoders, high level representational denoisers), color bit depth reduction, smoothing, reforming GANs, JPEG compression, foveation, pixel deflection, general basis function transformations, and many others.Next, detection. Some detection methods are closely related to input modification — as once an input has been cleaned its prediction can be compared with the original prediction, and if the two are far apart it’s likely that the input has been tampered with. Here are a few examples.Others are different. Eg here they actually train a separate detection network whose only job is to decide if an input is adversarial. Then there is a number of detection defenses that examine raw statistics computed at various points in the input feeding / prediction process:In general input modification & detection methods are great because they can be applied to an already trained model and don’t require the data scientist to go back to square 0. We are big fans of these kinds of defenses here at Calypso.Finally, there is the extra (NULL) class approach. The idea is simple. Classifiers are trained on a very particular data distribution and by definition are clueless when taken outside the bounds of that. So instead of forcing the classifier to guess the label when it clearly doesn’t know what it is — why not give it the option of abstaining. That’s what the NULL class enables (paper).All in all empirical defenses are imperfect — but they work, and sometimes only take a few lines of code. Yes, admittedly, they are a never ending cat-and-mouse game with the attacker and new ones are often quickly broken — but if you think about attackers like electricity (always take the path of least resistance) — our job is to just make their lives just hard enough.Phew! This has bee a long post — but hopefully you found it interesting and useful.Final note, if you’re an engineer rather than a mathematician and prefer to learn through doing — Ethical Institute for AI & Machine Learning has an excellent collection of open source tools related to adversarial robustness. Go ahead and play around.Otherwise, if you’re done with evasion attacks but want to know more about ML security — I recommend reading my other posts on the ML security threat model and poisoning and privacy attacks.Stay safe & secure everyone.",15/07/2019,0,15.0,5.0,739.0,454.0,14.0,10.0,0.0,70.0,en
4184,Fine-Tuned Named Entity Recognition with Hugging Face BERT,Medium,Andrew Marmon,12.0,5.0,983.0,"In many organizations, there is a unique vocabulary that maps names to known entities within that domain. At the United Nations, for instance, we have many specific entities which it is useful to identify in documents, including specific named committees and assemblies, important topics like the Sustainable Development Goals (SGDs), and many different countries and cultural groups that must be identified correctly. Exhaustively naming each and every important topic that may appear in a document, however, is not reasonable considering the shear number that may be important, especially considering the context of a document or sentence in which this entity is present. Instead, we want to be able to automatically identify and predict named entities using Named Entity Recognition (NER) to avoid creating a Regex parser with thousands or hundreds of thousands of possible named entities to match.In this article I will show you how to use the Hugging Face library to fine-tune a BERT model on a new dataset to achieve better results on a domain specific NER task. In this case, we want to predict United Nations named entities in transcripts of meetings of the General Assembly. Key phrases such as “the general assembly”, “second plenary meeting”, “the rohingya muslims”, and more will need to be identified and extracted as named entities within these transcripts in a consistent and automated fashion.First we need to retrieve a dataset that is set up with text and it’s associated entity labels. Because we want to fine-tune a BERT NER model on the United Nations domain, we will need labelled text from United Nations documents. We will use Leslie Huang’s generously open sourced UN NER dataset as training and test data.Notice how this data is structured in the tagged-training folder. Each text file is formatted with text from sentences and a label for each word which describes it as either not an entity (O) or as a specific type of entity (I-LOC, I-MISC, I-ORG, etc.). To get this into training format, we will read all of these files and, for each sentence, create a list of words and a list of these words entity labels. We will store this in a dataframe by reading the files in the UN NER dataset and reading each sentence as a row:which yields the dataframe:Now that we have the data in a workable format, we will use the Hugging Face library to fine-tune a BERT NER model to this new domain.A tokenizer is responsible for preparing the text inputs as inputs to the transformer model (in this case, BERT). Each of these tokens have a mapping to a number that will be used as the transformer input. In order to get value from the fine-tuning process, we will need to use the same tokenizer on our UN NER dataset that was used during the original training. This will ensure that what was learned during upstream training will be carried over into this downstream task.First, we need to get the Hugging Face transformer and datasets libraries.Next, we will tokenize our inputs and match the labels in the UN NER dataset (some of which have data quality issues which must be handled) to those labels used in upstream BERT training.Which yields us the datasets for both training and testing. Next, we need to tokenize these datasets to convert the tokens and labels into numeric form for our BERT model.Each token has now been transformed into a numeric representation and each label has been mapped to those used in upstream training, all in Hugging Face data format.Now that we have our dataset in the proper format, we will train our BERT model using the pre-trained model weights as our initial checkpoint. During this process, we will evaluate the model on the testing data by reporting F1 score and accuracy and finally save the model in the “un-ner.model” directory.After running this pipeline with these model parameters, we were able to achieve an F1 score of 0.85 and an accuracy of 0.98 on unseen data. Now we can load these saved model weights and predict on sentences in our current General Assembly documents to qualitatively assess the results!Now that we have our saved fine-tuned model weights, we can load them into a Hugging Face transformer object and predict entities on new sentences! We will run the entity recognition on the following paragraph,Before proceeding further, I should like to inform members that action on draft resolution iv, entitled situation of human rights of Rohingya Muslims and other minorities in Myanmar is postponed to a later date to allow time for the review of its programme budget implications by the fifth committee. The assembly will take action on draft resolution iv as soon as the report of the fifth committee on the programme budget implications is available. I now give the floor to delegations wishing to deliver explanations of vote or position before voting or adoption.and extract the associated entities.Using this fine-tuned model, we were able to extract many useful concepts related the the decisions made in this General Assembly meeting. We were able to correctly identify the Rohingya Muslims, the Fifth Committee, the General Assembly, Myanmar, and other entities within this paragraph. By fine-tuning our BERT model on this UN specific dataset we were able to achieve excellent quantitative and qualitative results.The full codebase can be found on GitHub, and the results for this prediction task on this paragraph can be found in un_ner.csv.This information can be critical in extracting useful information from the staggering number of documents created within the United Nations. For instance, by extracting these entities from documents we can create knowledge graphs connecting otherwise disparate documents together. This can be used for document topic inference, relating useful documents to eachother, and question and answering tasks. This NER work is just a small part of the work we are doing at the UN Emerging Technology Lab (ETL) to accelerate the UN’s mission.",05/08/2021,2,0.0,0.0,1137.0,675.0,2.0,0.0,0.0,8.0,en
4185,Gradient Boosting from scratch,ML Review,Prince Grover,3300.0,8.0,1358.0,"Simplifying a complex algorithmAlthough most of the Kaggle competition winners use stack/ensemble of various models, one particular model that is part of most of the ensembles is some variant of Gradient Boosting (GBM) algorithm. Take for an example the winner of latest Kaggle competition: Michael Jahrer’s solution with representation learning in Safe Driver Prediction. His solution was a blend of 6 models. 1 LightGBM (a variant of GBM) and 5 Neural Nets. Although his success is attributed to the semi-supervised learning that he used for the structured data, but gradient boosting model has done the useful part too.Even though GBM is being used widely, many practitioners still treat it as complex black-box algorithm and just run the models using pre-built libraries. The purpose of this post is to simplify a supposedly complex algorithm and to help the reader to understand the algorithm intuitively. I am going to explain the pure vanilla version of the gradient boosting algorithm and will share links for its different variants at the end. I have taken base DecisionTree code from fast.ai library (fastai/courses/ml1/lesson3-rf_foundations.ipynb) and on top of that, I have built my own simple version of basic gradient boosting model.When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error)An ensemble is just a collection of predictors which come together (e.g. mean of all predictions) to give a final prediction. The reason we use ensembles is that many different predictors trying to predict same target variable will perform a better job than any single predictor alone. Ensembling techniques are further classified into Bagging and Boosting.We typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other. Each observation is chosen with replacement to be used as input for each of the model. So, each model will have different observations based on the bootstrap process. Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. Example of bagging ensemble is Random Forest models.This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. (So the observations are not chosen based on the bootstrap process, but based on the error). The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of boosting algorithm.Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. (Wikipedia definition)The objective of any supervised learning algorithm is to define a loss function and minimize it. Let’s see how maths work out for Gradient Boosting algorithm. Say we have mean squared error (MSE) as loss defined as:We want our predictions, such that our loss function (MSE) is minimum. By using gradient descent and updating our predictions based on a learning rate, we can find the values where MSE is minimum.So, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values.The logic behind gradient boosting is simple, (can be understood intuitively, without using mathematical notation). I expect that whoever is reading this post might be familiar with simple linear regression modeling.A basic assumption of linear regression is that sum of its residuals is 0, i.e. the residuals should be spread randomly around zero.Now think of these residuals as mistakes committed by our predictor model. Although, tree-based models (considering decision tree as base models for our gradient boosting here) are not based on such assumptions, but if we think logically (not statistically) about this assumption, we might argue that, if we are able to see some pattern of residuals around 0, we can leverage that pattern to fit a model.So, the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). Algorithmically, we are minimizing our loss function, such that test loss reach its minima.In summary, • We first model data with simple models and analyze data for errors. • These errors signify data points that are difficult to fit by a simple model. • Then for later models, we particularly focus on those hard to fit data to get them right. • In the end, we combine all the predictors by giving some weights to each predictor.A more technical quotation of the same logic is written in Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World,“The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. … Note, however, it is not obvious at all how this can be done”Let’s consider simulated data as shown in scatter plot below with 1 input (x) and 1 output (y) variables.Data for above shown plot is generated using below python code:1. Fit a simple linear regressor or decision tree on data (I have chosen decision tree in my code) [call x as input and y as output]2. Calculate error residuals. Actual target value, minus predicted target value [e1= y - y_predicted1 ]3. Fit a new model on error residuals as target variable with same input variables [call it e1_predicted]4. Add the predicted residuals to the previous predictions[y_predicted2 = y_predicted1 + e1_predicted]5. Fit another model on residuals that is still left. i.e. [e2 = y - y_predicted2] and repeat steps 2 to 5 until it starts overfitting or the sum of residuals become constant. Overfitting can be controlled by consistently checking accuracy on validation data.To aid the understanding of the underlying concepts, here is the link with complete implementation of a simple gradient boosting model from scratch. [Link: Gradient Boosting from scratch]Shared code is a non-optimized vanilla implementation of gradient boosting. Most of the gradient boosting models available in libraries are well optimized and have many hyper-parameters.Blue dots (left) plots are input (x) vs. output (y) • Red line (left) shows values predicted by decision tree • Green dots (right) shows residuals vs. input (x) for ith iteration • Iteration represent sequential order of fitting gradient boosting treeWe observe that after 20th iteration , residuals are randomly distributed (I am not saying random normal here) around 0 and our predictions are very close to true values. (iterations are called n_estimators in sklearn implementation). This would be a good point to stop or our model will start overfitting.Let’s see how our model look like for 50th iteration.We see that even after 50th iteration, residuals vs. x plot look similar to what we see at 20th iteration. But the model is becoming more complex and predictions are overfitting on the training data and are trying to learn each training data. So, it would have been better to stop at 20th iteration.Python code snippet used for plotting all the above figures.I hope that this blog helped you to get basic intuition behind how gradient boosting works. To understand gradient boosting for regression in detail, I would strongly recommend you to read this amazing article by the faculty at University of San Francisco, Terence Parr (Creator of the ANTLR parser generator) and Jeremy Howard (Founding researcher at fast.ai) : How to explain gradient boosting.More useful resources4. A Kaggle Master Explains Gradient Boosting: Ben Gormanblog.kaggle.com5. Widely used GBM algorithms: XGBoost || Lightgbm || Catboost || sklearn.ensemble.GradientBoostingClassifier",09/12/2017,0,49.0,20.0,959.0,546.0,9.0,3.0,0.0,23.0,en
4186,Simple Reinforcement Learning: Q-learning,Towards Data Science,Andre Violante,439.0,5.0,817.0,"One of my favorite algorithms that I learned while taking a reinforcement learning course was q-learning. Probably because it was the easiest for me to understand and code, but also because it seemed to make sense. In this quick post I’ll discuss q-learning and provide the basic background to understanding the algorithm.Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It’s considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn’t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward.The ‘q’ in q-learning stands for quality. Quality in this case represents how useful a given action is in gaining some future reward.When q-learning is performed we create what’s called a q-table or matrix that follows the shape of [state, action] and we initialize our values to zero. We then update and store our q-values after an episode. This q-table becomes a reference table for our agent to select the best action based on the q-value.The next step is simply for the agent to interact with the environment and make updates to the state action pairs in our q-table Q[state, action].Taking Action: Explore or ExploitAn agent interacts with the environment in 1 of 2 ways. The first is to use the q-table as a reference and view all possible actions for a given state. The agent then selects the action based on the max value of those actions. This is known as exploiting since we use the information we have available to us to make a decision.The second way to take action is to act randomly. This is called exploring. Instead of selecting actions based on the max future reward we select an action at random. Acting randomly is important because it allows the agent to explore and discover new states that otherwise may not be selected during the exploitation process. You can balance exploration/exploitation using epsilon (ε) and setting the value of how often you want to explore vs exploit. Here’s some rough code that will depend on how the state and action space are setup.Updating the q-tableThe updates occur after each step or action and ends when an episode is done. Done in this case means reaching some terminal point by the agent. A terminal state for example can be anything like landing on a checkout page, reaching the end of some game, completing some desired objective, etc. The agent will not learn much after a single episode, but eventually with enough exploring (steps and episodes) it will converge and learn the optimal q-values or q-star (Q∗).Here are the 3 basic steps:Here is the basic update rule for q-learning:In the update above there are a couple variables that we haven’t mentioned yet. Whats happening here is we adjust our q-values based on the difference between the discounted new values and the old values. We discount the new values using gamma and we adjust our step size using learning rate (lr). Below are some references.Learning Rate: lr or learning rate, often referred to as alpha or α, can simply be defined as how much you accept the new value vs the old value. Above we are taking the difference between new and old and then multiplying that value by the learning rate. This value then gets added to our previous q-value which essentially moves it in the direction of our latest update.Gamma: gamma or γ is a discount factor. It’s used to balance immediate and future reward. From our update rule above you can see that we apply the discount to the future reward. Typically this value can range anywhere from 0.8 to 0.99.Reward: reward is the value received after completing a certain action at a given state. A reward can happen at any given time step or only at the terminal time step.Max: np.max() uses the numpy library and is taking the maximum of the future reward and applying it to the reward for the current state. What this does is impact the current action by the possible future reward. This is the beauty of q-learning. We’re allocating future reward to current actions to help the agent select the highest return action at any given state.ConclusionWell that’s it, short and sweet (hopefully). We discussed that q-learning is an off-policy reinforcement learning algorithm. We show the basic update rule for q-learning using some basic python syntax and we reviewed the required inputs to the algorithm. We learned that q-learning uses future rewards to influence the current action given a state and therefore helps the agent select best actions that maximize total reward.There is a lot more on q-learning but hopefully this is enough to get you started and interested in learning more. I added several resources below that I found helpful when learning about q-learning. Enjoy!Resources",18/03/2019,3,14.0,10.0,1400.0,1051.0,1.0,2.0,0.0,6.0,en
4187,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,Emergent // Future,Arthur Juliani,12000.0,6.0,1189.0,"For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1–3). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:Eq 1. Q(s,a) = r + γ(max(Q(s’,a’))This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:(Thanks to Praneet D for finding the optimal hyperparameters for this approach)Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.Eq2. Loss = ∑(Q-target - Q)²Below is the Tensorflow walkthrough of implementing our simple Q-Network:While the network learns to solve the FrozenLake problem, it turns out it doesn’t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!If you’d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on Twitter @awjliani.More from my Simple Reinforcement Learning with Tensorflow series:",26/08/2016,0,3.0,16.0,954.0,467.0,2.0,1.0,0.0,19.0,en
4188,Neural Networks without Backpropagation: Direct Feedback Alignment,Blog rilut,Rizky Luthfianto,183.0,4.0,503.0,"Here’s a quick summary on Arild Nøkland’s 2016 paper “Direct Feedback Alignment” which is not only written clearly but also interesting. Both Lillicrap et al. (2016) and Nøkland (2016) were able to train a Neural Network (NN) without Backpropagation.First, create a simple NN like this:with cross-entropy as its loss function.So here is our implementation so far:To train a NN, we have to get the loss function derivative w.r.t to softmax function. Let’s take a look at Equation 5 from Nøkland’s paper.which simply corresponds to:I am sorry as I am not going to explain the Calculus behind this. Should you refer to Sadowski’s Notes on Backpropagation if you want the explanation.This is our implementation of Backpropagation:Where d1, d2, and ewill be used later for updating weights.Lillicrap et al. proposed an algorithm named Feedback Alignment. They argued that symmetric weights on Backpropagation isn’t required for learning.Which means: Normally, with Backpropagation, you have to use the transpose of current weight matrices for updating their own weights. But turns out: you don’t have to. You can train your weights with fixed-random matrices just fine.And on 2016, Nøkland built on this idea further.We will take a look at four methods. They are Backpropagation (BP), Feedback Alignment (FA), and two methods proposed by Nøkland himself: Direct Feedback Alignment (DFA) and Indirect Feedback Alignment (IFA).You can see on the illustration below on how they works. Note that, the grey arrows is the forward propagation, while the black arrows is the backward propagation or the learning process. You can just focus on the black arrows.Below, you can see the detailed explanation of those learning methods:You can see that the paper was written clearly, it even explained Backpropagation in a simple way. Next, let’s continue the code, so it can corresponds to those equations.Yes, all of them use derivatives. But…While Feedback Alignment implementation looks almost similar to Backpropagation, it uses random matrix. It shows that Neural Networks can learn just fine using random matrices, without using the weight matrices.With DFA, you can just use the gradient from the last layer to train all layers in Neural Networks. Each of your layers do not need to depends on gradient from the layers behind of them. So, the training process doesn’t have to progress layer by layer anymore.IFA is even also interesting, you can train a layer from a feedback from a layer in front of it.Let me summarize both initializations for you:Implementation-wise, you might want to know that BP & FA’s matrices has the same shape. While DFA & IFA has different shape to BP & FA’s matrices. So, you shouldn’t be surprised when you met an error message on matrix shapes later.That’s all, I leave the rest of implementation as an exercise to readers :)I’ve tried FA/DFA/IFA on a small dataset such as this Kaggle competition dataset, and the NN did can learn.You should try it on a deeper architecture and a more serious dataset such as CIFAR, which results are available on Nøkland’s paper. Nøkland’s original implementation is available on https://github.com/anokland/dfa-torch",03/01/2017,0,8.0,0.0,438.0,234.0,7.0,1.0,0.0,5.0,en
4189,Clustering with Gaussian Mixture Model,Clustering with Gaussian Mixture Model,Azad Soni,55.0,3.0,520.0,"One of the popular problems in unsupervised learning is clustering. Clustering is the assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense.As in above diagram the result of clustering is colouring of the squares into three clusters.One of the basic approach to solve cluster analysis problem is K-means. K-means algorithm partitioned the data into K clusters .K means:In general, suppose we have n data points, that have to be partitioned in K clusters. The goal is to assign a cluster to each data point. K-means is a clustering method that aims to find the K positions of the clusters that minimize the distance(for example Euclidian distance) from the data points to the cluster by minimizing distance loss function. K means do hard assignment for the data points that means a point either totally belongs to a cluster or not at all. For detailed description about how K-means clustering algorithm work see the reference section.Issues with K means:There are some issues with K-means algorithm. K-means often doesn’t work when clusters are not round shaped because of it uses some kind of distance function and distance is measured from cluster center.Another major problem with K-Means clustering is that Data point is deterministically assigned to one and only one cluster, but in reality there may be overlapping between the cluster for example picture shown below:Here image b denotes unlabeled data that we are going to cluster and image c denotes the ambiguity that occur at overlapping region.While doing clustering we can not tell exactly about the cluster of data points that belongs to overlapping regions of picture c. For fixing this data points are assigned to clusters with certain probabilities and this is what gaussian mixture model do.Gaussian Mixture Models(GMM):For address these problems gaussian mixture model was introduced. A probabilistic approach to clustering addressing many of these problems. In this approach we describe each cluster by its centroid (mean), covariance , and the size of the cluster(Weight).Here rather than identifying clusters by “nearest” centroids, we fit a set of k gaussians to the data. And we estimate gaussian distribution parameters such as mean and Variance for each cluster and weight of a cluster. After learning the parameters for each data point we can calculate the probabilities of it belonging to each of the clusters.So mathematically we can define gaussian mixture model as mixture of K gaussian distribution that means it’s a weighted average of K gaussian distribution. So we can write data distribution asWhere N(x|mu_k,sigma_k) represents cluster in data with mean mu_k and co variance epsilon_k and weight pi_k.After learning the parameters mu_k, epsilon_k and pi_k we learnt k gaussian distribution from the data distribution that k gaussian distributions are clusters.Since we learnt K distributions(cluster) then for anydata point x depending upon its distance from every distribution we will get probability of it belong to each distribution.For example after getting component distribution in below image depending upon variance and mean of particular cluster we get probability of any data point x belongs to every cluster.(Picture taken from Stack Overflow)References:K means:GMM:",05/12/2017,0,5.0,1.0,668.0,367.0,4.0,2.0,0.0,5.0,en
4190,Training a spaCy NER Pipeline with Prodigy,Analytics Vidhya,JP Zamanillo,15.0,5.0,647.0,"The success of a custom Named Entity Recognition (NER) model is dependent on the quality of data passed to it. However, supplying a model with sufficient examples of training data is typically a time-consuming and exhaustive process. Using Prodigy, the task of labeling your data for building a custom NER pipeline to a spaCy model is much quicker and simpler.According to the official Prodigy site:Prodigy is a modern annotation tool for creating training and evaluation data for machine learning models. You can also use Prodigy to help you inspect and clean your data, do error analysis and develop rule-based systems to use in combination with your statistical models.Prodigy makes it easy to label your data to use in model training. For this overview, we’ll use a data set that contains seafood menu items ranging back to the 1840s. We’ll cover how to add a customized NER pipeline to a blank spaCy model using brand new entity labels, including:SEAFOOD: Dishes pertaining directly to seafood-related items such as halibut, cod, or clam chowder.SIDE: Dishes that are not seafood, helping us understand how people typically pair their seafood choices.METHOD: The way that a dish is prepared, such as broiled, roasted, or grilled.LOCATION: Where the seafood comes from, helping us determine sourcing patterns.Since we’re training an entirely new NER pipeline, we must manually annotate a few hundred examples. We can do this using the ner.manual recipe (a recipe is how Prodigy refers to the various methods available).There are few items to unpack in the ner.manual command. This command and the subsequent commands are run through the command line.Once we have a few hundred working examples, we can do an initial training cycle and define a new model. This process helps us with future labeling tasks since, following training, our model will help us annotate new examples.With the initial model trained and saved, we can use the ner.teach command to expedite the labeling process. The benefit of ner.teach is that the labeling process becomes binary, meaning we only need to confirm or deny the model’s labeling predictions.Before retraining our model, we have to convert the binary data set to a Gold data set. With Prodigy, there are two types of data sets, Silver and Gold. The Silver data set types refer to the binary labeling set, whereas the Gold sets refer to the complete ground-truth sets. Let’s merge the Silver data set we created with the Gold set we made during the manual labeling process using ner.silver-to-gold.With ner.silver-to-gold, the Prodigy interface is identical to the ner.manual step. This step combines manual annotation with automated labeling to confirm and fix the model’s predictions.We can now retrain our model with the Gold set to improve our results and further simplify labeling.The retraining script is nearly identical to the previous reference prodigy train script. The difference is that instead of referring to blank:en, we’ll use the seafood_model that we created during our first training iteration.The process of training custom pipelines is iterative and never complete. Throughout the process, we can observe that the model becomes more accurate as more training data is prepared for ingesting.Model ResultsBelow is an example of how the custom NER component performs with roughly 1,800 labeled examples. After 10 training iterations, the model received an F-Score of 77.329.A brief note: The F-score, also called the F1-score, is a measure of a model’s predictive power on a dataset. It evaluates binary classification systems, which classify examples into ‘positive’ or ‘negative.’ This metric is similar to accuracy, or the correctly labeled examples versus the incorrectly labeled examples. The F-score is more robust to class imbalances, which is outside the scope of this overview. (Source: DeepAI)Thank you very much for reading this brief overview on using Prodigy for your Natural Language Processing projects. The Prodigy documentation provides other use-cases for your models, and I hope to offer additional overviews of the various methods available.",26/04/2021,0,6.0,1.0,1679.0,925.0,4.0,4.0,0.0,5.0,en
4191,Solving the Multi-Armed Bandit Problem,Towards Data Science,Anson Wong,661.0,6.0,1138.0,"The multi-armed bandit problem is a classic reinforcement learning example where we are given a slot machine with n arms (bandits) with each arm having its own rigged probability distribution of success. Pulling any one of the arms gives you a stochastic reward of either R=+1 for success, or R=0 for failure. Our objective is to pull the arms one-by-one in sequence such that we maximize our total reward collected in the long run.The non-triviality of the multi-armed bandit problem lies in the fact that we (the agent) cannot access the true bandit probability distributions — all learning is carried out via the means of trial-and-error and value estimation. So the question is:How can we design a systematic strategy that adapts to these stochastic rewards?This is our goal for the multi-armed bandit problem, and having such a strategy would prove very useful in many real-world situations where one would like to select the “best” bandit out of a group of bandits i.e. A/B testing, line-up optimization, evaluating social media influence.In this article, we approach the multi-armed bandit problem with a classical reinforcement learning technique of an epsilon-greedy agent with a learning framework of reward-average sampling to compute the action-value Q(a) to help the agent improve its future action decisions for long-term reward maximization. The Python code implementation of this multi-armed bandit algorithm solution can be found at my Github at:https://github.com/ankonzoid/LearningX/tree/master/classical_RL/multiarmed_banditPlease refer to our Appendix for more details about the epsilon-greedy agent, and how the reward-average sampling method is used to iteratively update Q(a). In the next section, we explain the results of deploying such an agent in the multi-armed bandit environment.More of my blogs, tutorials, and projects on Deep Learning and Reinforcement Learning can be found at my Medium and at my Github.Consider our Python code example of 10 hard-coded bandits each with their own individual success probabilities (remember that our agent is blind to these numbers, it can only realize them via sampling the bandits individually):By inspection, we will be expecting our the agent in the long-term to pick out Bandit #4 as the strongest signal, with Bandit #9 following second, and Bandit #10 following third, etc.Now to the results. We performed 2,000 experiments for the agent to start from scratch with epsilon exploration probability of 10%, and trained the agent for 10,000 episodes per experiment. The average proportion of bandits chosen by the agent as a function of episode number is depicted in Fig 1.In Fig 1, we can see that that the selection choice of bandits is uniformly distributed at ~10% amongst all bandits near the beginning of training (< 10 episodes) as it is in its exploratory phase of not knowing which bandits to take advantage of yet. It is until we reach later episodes (> 100 episodes) do we see a clear greedy mechanism take precedence in deciding which bandits should get more priority because of the rewards sampled so far. As expected Bandits #4, #9, #10 at this mid-to-late training phase are the ones that get chosen by the agent. Lastly and almost inevitably, the agent tends to almost always choose Bandit #4 as the “best” bandit at the end of training with a plateau of ~90% (since ~10% should always remain because of the fixed epsilon exploration parameter).Although the optimal policy is to select Bandit #4 in this problem, you will notice that this does not mean that pulling Bandit #4 will always beat any other bandit on a given pull since the rewards are stochastic; it is in the long-term reward average that you will find Bandit #4 to dominate. Also, there is nothing particularly special about using our agent to approach this problem— it is just one of many methods that can adaptively maximize the collection of long-term rewards. There definitely exists situations where a completely exploratory (epsilon = 100%), or a completely greedy agent (epsilon = 0%), or anything in between, could end up collecting more rewards for a finite number of episodes than our epsilon=10%-greedy agent. The main appeal of deploying such an agent in my perspective is for the automation of minimizing re-choosing bandits that have already shown some evidence of failure. From a business and practical perspective, this can save a lot of time and resources that would be otherwise wasted in the optimization process of finding the “best” bandit.In a nutshell, the epsilon-greedy agent is a hybrid of a (1) completely-exploratory agent and a (2) completely-greedy agent. In the multi-armed bandit problem, a completely-exploratory agent will sample all the bandits at a uniform rate and acquire knowledge about every bandit over time; the caveat of such an agent is that this knowledge is never utilized to help itself to make better future decisions! On the other extreme, a completely-greedy agent will choose a bandit and stick with its choice for the rest of eternity; it will not make an effort to try out other bandits in the system to see whether they have better success rates to help it maximize its long-term rewards, thus it is very narrow-minded!To get a somewhat desirable agent that possesses the best of both worlds, the epsilon-greedy agent is designed to give an epsilon chance (say for example 10%) towards exploring bandits randomly at any state, and acts greedily on its current ‘best’ bandit value estimate for all other times. The intuition surrounding this is that the greedy-mechanism can help the agent focus on its currently most “successful” bandits, and the exploratory-mechanism gives the agent to explore for better bandits that might be out there.The only thing left is: how do we define a notion of “value” of a bandit to the agent so that it can choose greedily? Borrowing from reinforcement learning, we can define the action-value function Q(s, a) to represent the expected long-term reward of taking action a from state s. In our case of the multi-armed bandit, each action brings the agent to a terminal state so long-term rewards are exactly the immediate rewards and we simplify the notation of the definition of the action-value aswhere k is the counter for how many times action a (bandit) was chosen in the past, and r are the stochastic rewards for each time that bandit was chosen. With some extra arithmetic manipulation, this definition can be re-written recursively asAs we do not know start off knowing the “true” values of Q(a), we can use this recursive definition as an iterative tool for approximating Q(a) at the end of every episode.To pair up the epsilon-greedy agent with our action-values Q(a) estimates, we let the epsilon-greedy agent choose a bandit at random epsilon-probability of the time, and let the agent use greedily choose an action from our Q(a) estimates for the rest of the timesWith these two concepts down, we can now go about solving the multi-armed bandit problem!",25/09/2017,1,14.0,4.0,674.0,310.0,5.0,0.0,0.0,3.0,en
4192,Single Stage Instance Segmentation — A Review,Towards Data Science,Patrick Langechuan Liu,933.0,16.0,3054.0,"Update:Instance segmentation is a challenging computer vision task that requires the prediction of object instances and their per-pixel segmentation mask. This makes it a hybrid of semantic segmentation and object detection.Ever since Mask R-CNN was invented, the state-of-the-art method for instance segmentation has largely been Mask RCNN and its variants (PANet, Mask Score RCNN, etc). It adopts the detect-then-segment approach, first perform object detection to extract bounding boxes around each object instances, and then perform binary segmentation inside each bounding box to separate the foreground (object) and the background.There are some other instance segmentation methods other than the top-down approach of detect-then-segment (or segmentation-by-detection). One such example is focusing on pixel by formulating instance segmentaiton as a bottom-up pixel assignment problem, as is done in SpatialEmbedding (ICCV 2019). But such methods usually has inferier performance than detect-then-segment SOTA, and we will not go to details in this post.However, Mask RCNN is quite slow and precludes the use of many real-time applications. In addition, masks predicted by Mask RCNN have fixed resolution and thus are not refined enough for large objects with complex shapes. There has been a wave of studies on single-stage instance segmentation, fueled by the advances in anchor-free object detection methods (such as CenterNet and FCOS. See my slides for a quick intro into anchor-free object detection). Many of these methods are faster and more accurate than Mask RCNN, as shown in the image below.This blog will review the recent advances in single-stage instance segmentation, with a focus on mask representation — one key aspect of instance segmentation.One core question to ask in instance segmentation is the representation or parameterization of instance masks — 1) whether to use local masks or global masks and 2) how to represent/parameterize the mask.There are largely two ways to represent an instance mask: local masks and global masks. A global mask is what we ultimately want, which has the same spatial extent to the input image, although the resolution may be smaller such as 1/4 or 1/8 of the original image. It has the natural advantage of having the same resolution (and thus fixed-length features) for big and small objects. This will not sacrifice resolution for bigger objects and the fixed resolution lends itself to perform batching for optimization. A local mask is usually more compact in the sense that it does not have excessive boundaries as a global mask. It has to be used with mask location to be recovered to the global mask, and local mask size will depend on object size. But to perform effective batching, instance masks require a fixed-length parameterization. The simplest solution is by resizing instance masks to fixed image resolution, as adopted by Mask RCNN. There are, as we see below, more effective ways to parameterized local masks as well.Based on whether local or global masks are used, single-stage instance segmentation can be largely categorized into local-mask-based and global-mask-based approaches.Local-mask-based methods output instance masks on each local region directly.Bounding box in a sense is a rough mask, which approximates the contour of the mask with the minimum bounding rectangle. ExtremeNet (Bottom-up Object Detection by Grouping Extreme and Center Points, CVPR 2019) performs detection by using four extreme points (thus a bounding box with 8 degrees of freedom rather than the conventional 4 DoF), and this richer parameterization can be naturally extended to an octagonal mask by extending an extreme point in both directions on its corresponding edge to a segment of 1/4 of the entire edge length.Since then there are a series of work trying to encode/parameterize the contours of an instance mask into fixed-length coefficients, given different decomposition basis. These methods regress the center of each instance (not necessarily the bbox center) and the contour with respect to that center. ESE-Seg (Explicit Shape Encoding for Real-Time Instance Segmentation, ICCV 2019) designs an inner center radius shape signature for each instance and fits it with Chebyshev polynomials. PolarMask (PolarMask: Single Shot Instance Segmentation with Polar Representation, CVPR 2020) uses utilizes rays at constant angle intervals from the center to describe the contour. FourierNet (FourierNet: Compact mask representation for instance segmentation using differentiable shape decoders) introduces a contour shape decoder using Fourier transform and achieves smoother boundaries than PolarMask.These methods typically use 20 to 40 coefficients to parameterize the mask contours. They are fast to inference and easy to optimize. However, their drawbacks are also obvious. First of all, visually they all look — let’s be honest — quite awful. They can not depict the mask precisely and can not describe objects that have holes in the center.Personally I think this line of work is cute but has little future. Explicit encoding of complex topology of instance masks or their contours are intractable.TensorMask (TensorMask: A Foundation for Dense Object Segmentation, ICCV 2019) is one of the first works to demonstrate the idea of dense mask prediction, by predicting a mask at each feature map location. TensorMask still predicts a mask wrt a region of interest instead of a global mask, and it is able to run instance segmentation without running object detection.TensorMask utilizes structured 4D tensors to represent masks over a spatial domain (2D iterates over all possible locations in the input image and 2D representing a mask in each location), it also introduces aligned representation and tensor bipyramid to recover spatial details, but these align operations make the network even slower than the two-stage Mask R-CNN. In addition, in order to get good performance, it needs to be trained with a schedule that is six times longer than a standard COCO object detection pipeline (6x schedule).Natural object masks are not random and akin to natural images, instance masks reside in a much lower intrinsic dimension than that of pixel space. MEInst (Mask Encoding for Single Shot Instance Segmentation, CVPR 2020) distills the mask into a compact and fixed dimensional representation. With a simple linear transformation with PCA, MEInst is able to compress a 28x28 local mask into a 60-dim feature vector. The paper also tried to directly regress 28x28=784-dim feature vector on a one-stage object detector (FCOS), and also get reasonable results with 1 to 2 AP point drop. This means that directly predicting high dimensional masks (in natural representation per TensorMask) is not entirely impossible, but are hard to optimize. The compact representation of masks makes it easier to optimize and also faster to run at inference time. It is most similar to Mask RCNN and can be directly used with most of the other object detection algorithms.Global-mask-based methods first generate intermediate and shared feature maps based on the whole image, then assemble the extracted features to form the final masks for each instance. This is the mainstream methods among recent one-stage instance segmentation methods.YOLACT (YOLACT: Real-time Instance Segmentation, ICCV 2019) is one of the first methods attempting real-time instance segmentation. YOLACT breaks instance segmentation into two parallel tasks, generating a set of prototype masks and predicting per-instance mask coefficients. The prototype masks are generated with FCN and can directly benefit from advances in semantic segmentation. The coefficients are predicted as extra features of the bounding box. These two parallel steps are followed by an assembly step: a simple linear combination realized by matrix multiplication and a cropping operation with the predicted bounding boxes for each instance. The cropping operation reduces the network’s burden to suppress noise outside of the bounding box but still sees some leakage if the bounding box include part of another instance of the same class.The prediction of prototype masks are critical to ensure high resolution of the final instance masks, which is comparable with semantic segmentation. The prototype masks are only dependent on input images and are independent of categories and specific instances. This distributed representation is compact as the number of the prototype masks is independent of the number of instances, which makes YOLACT’s mask computation cost constant (unlike Mask RCNN which has a computation cost linear to the number of instances).Looking back at InstanceFCN (Instance-sensitive Fully Convolutional Networks, ECCV 2016) and the followup study FCIS (Fully Convolutional Instance-aware Semantic Segmentation, CVPR 2017) by MSRA, they seem to be a special case of YOLACT. Both InstanceFCN and FCIS utilize FCN to generate multiple instance-sensitive score maps that contain the relative positions to objects instances, then apply an assembling module to output object instances. The position-sensitive score maps can be seen as the prototype masks, but instead of learned linear coefficients, IntanceFCN and FCIS use a fixed set of spatial pooling operations to combine the position-sensitive prototype masks.BlendMask (BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation, CVPR 2020) builds on YOLACT, but instead of predicting one scalar coefficient for each prototype mask, BlendMask predicts a low-res (7x7) attention map to blend the masks within the bounding box. This attention map is predicted as a high dimensional feature (7x7=49-d) attached to each bounding box. Interestingly, the prototype masks used by BlendMask is 4, but it works by even only 1 prototype masks. CenterMask (CenterMask: single shot instance segmentation with point representation, CVPR 2020) works almost in exactly the same way and uses 1 prototype mask (named global saliency map) explicitly. CenterMask uses CenterNet as the backbone, while BlendMask uses a similar anchor-free and single-stage FCOS as the backbone.Note that both BlendMask and CenterMask developed further dependencies on the detected bounding box. The attention map or shape mask has to be scaled to the same size as the bounding box, before blending with the cropped prototype mask.CondInst (Conditional Convolutions for Instance Segmentation) takes one step further and completely removes any dependency on bounding boxes. Instead of assembling cropped prototype masks, it borrows the idea of dynamic filters and predicts the parameters of a lightweight FCN head. The FCN head has three layers and 169 parameters in total. What is amazing is that the authors showed that even when the prototype mask is a 2–ch CoordConv alone, the network is predicting good results with 31 AP on COCO. We will discuss this in the implicit representation section below.Both BlendMask /CenterMask and CondInst is an extension to YOLACT.The use of a branch to predict prototype masks allow these methods to benefit from using an auxiliary task of semantic segmentation (usually with 1 to 2 points boost in AP). It can also be naturally extended to perform panoptic segmentation as well.Regarding the parameters needed to represent each instance mask, some technical details are listed below. These methods with global masks and coefficients use 32, 196, 169 parameters per instance mask.SOLO is one of a kind and merits its own section. The papers are deeply insightful and very well written. They are a piece of art to me (like another one of my favorite CenterNet).The first author of the paper posted his reply on the motivation of SOLO on Zhihu (知乎), which I quote below:Semantic segmentation predict the semantic category for each pxiel in the image. In analogously, for instance segmentation, we propose to predict the “instance category” of each pixel. Now the key question is, how do we define Instance Category?If two object instances in the input image has exactly the same shape and position, they are the the same instance. Any two different instances either have different position or shape. And as shape is hard to describe in general we approximate shape with size.Thus “instance category” is defined by location and size. Location is classified by its center position. SOLO approximates the center position by dividing the input image into a grid of S x S cells and thus S² classes. Size is handled by assigning objects of different sizes to different levels of a feature pyramid (FPN). Thus for each pixel, SOLO only needs to decide which SxS grid cell and which FPN level to assign the pixel (and the corresponding instance category) to. So SOLO only needs to perform two pixel-level classification problems, analogous to semantic segmentation. Now another key question is how are the masks represented?The instance masks are represented directly by global masks stacked into S² channels. This is an ingenious design that solves many problems simultaneously. First, many previous studies store 2D masks as a flattened vector and this quickly gets intractable when the mask resolution increases leading to the explosion of the number of channels. A global mask naturally preserves the spatial relationships within the pixels of the mask. Second, the global mask generation can preserve high resolution of the mask. Third, the number of predicted masks are fixed, regardless of objects in the image. This is similar to the line of work of prototype masks, and we will see how these two streams merge in SOLOv2.SOLO formulates instance segmentation as a classification-only problem and removes any dependent on regression. This makes SOLO naturally independent of object detection. SOLO and CondInst are the two works that directly operate on global masks and are truly bounding box free methods.From the global masks predicted by SOLO, we can see that the masks are relatively insensitive to localization error as masks predicted by neighboring channels are quite similar. This brings up the tradeoff between the resolution (and thus precision) of object localization and instance masks.TensorMask’s idea of 4D structured tensor makes perfect sense in theory but is hard to realize in practice in the current framework of NHWC tensor format. Flattening a 2D tensor with spatial semantics into a 1D vector will inevitably lose some spatial details (similar to doing semantic segmentation with fully connected networks), and has its limitations in even representing a low-resolution image of 128x128. Either the 2D of location or the 2D of mask has to sacrifice resolution. Most previous studies took for granted that the location resolution is more important and downsamples/compresses the mask dimensions, hurting the expressiveness and quality of the masks. TensorMask tried to strike a balance but the tedious operations led to slow training and inference. SOLO realizes that we do not need high-resolution location information and borrows from YOLO by compressing location into a coarse S² grid. In this way, SOLO keeps the high resolution of global masks.I naively thought SOLO could perhaps work by predicting the S² x W x H global masks as an additional flattened WH-dimensional feature attached to each of the S² grids in YOLO. I was wrong — the formulation of global masks in full resolution instead of a flattened vector is actually the key to SOLO’s success.As mentioned above, the global masks predicted by SOLO in the S² channels are quite redundant and sparse. Even at a coarse resolution of S=20, there are 400 channels and it is unlikely that there are so many objects in the picture that each of the channels contains a valid instance mask.In decoupled SOLO, the original M tensor of shape H x W x S² is replaced by two tensors X and Y each of shape H x W x S. For an object located at grid location (i, j), M_ij is approximated by the element-wise multiplication X_i ⊗ Y_j. This reduces 400 channels to 40 channels and experiments show that there is no degradation in performance.Now it is natural to ask can we borrow from YOLACT’s prototype mask idea by predicting even fewer masks and predicting coefficients for each grid cell to combine them? SOLOv2 does exactly that.In SOLOv2, there are two branches, a feature branch and a kernel branch. The feature branch predicts E prototype masks, and the kernel branch predicts a kernel of size D at each of the S² grid cell locations. This dynamic filter approach is the most flexible as we saw in the YOLACT section above. When D=E, it is a simple linear combination of prototype masks (or 1x1 conv), the same as YOLACT. The paper also tried 3x3 conv kernels (D=9E). This can be taken a step further by predicting the weights and biases of a lightweight multi-layer FCN such as in CondInst.Now since the global mask branch is decoupled from its dedicated location, we can observe that the emerging prototype masks exhibit more complex patterns than that in SOLO. They are still position-sensitive and more similar to that of YOLACT.The idea of dynamic filters used in CondInst and SOLOv2 sounds glorious at first but are actually quite simple if you think of it as a natural extension to a list of coefficients used for linear combination.You can also think we parameterized the mask with the coefficients or attention maps or eventually, into dynamic filters for a small neural network head. The idea of using a neural network to dynamically encode a geometric entity is also explored in 3D learning recently. Traditionally, a 3D shape is either encoded with voxels, point clouds or mesh. Occupancy Networks (Occupancy Networks: Learning 3D Reconstruction in Function Space, CVPR 2019) proposed to encode the shape into a neural network, by considering the continuous decision boundary of a deep neural network as a 3D surface. The network takes in a point in 3D and tells whether it is on the boundary of the encoded 3D shape. This approach allows extracting 3D meshes at any resolution during inference.Can we learn a neural network consisting of dynamic filters per object instance so that the network takes in a point in 2D and output if the point belongs to that object mask or not? This naturally outputs a global mask and can have any desired resolution. Looking back at the ablation study of CondInst, it is demonstrated that even without the prototype mask but only with CoordConv input (which serves as performing uniform spatial sampling). As this operation is detached from the resolution of the prototype masks, it would be interesting to input CoordConv alone at a higher resolution to get higher resolution global masks to see if this improves performance. I strongly believe the implicit encoding of instance mask is the future.Most of the single-stage instance segmentation work are based on anchor-free object detection such as CenterNet and FCOS. Perhaps unsurprisingly, many of the above papers are from the same lab at the University of Adelaide that created FCOS. They have recently open-sourced their platform at https://github.com/aim-uofa/AdelaiDet/.Many of the recent methods are fast and achieves realtime or near-realtime performance (30+ FPS). NMS is usually the bottleneck for realtime instance segmentation. To achieve truly realtime performance, YOLACT uses Fast NMS and SOLOv2 uses Matrix NMS. I will talk about various NMS methods in another post.",29/04/2020,0,55.0,4.0,1321.0,664.0,17.0,5.0,0.0,48.0,en
4193,Difference between AutoEncoder (AE) and Variational AutoEncoder (VAE),Towards Data Science,Aqeel Anwar,2100.0,8.0,1523.0,"The ability to simplify means to eliminate the unnecessary so that the necessary may speak — Hans HofmannData compression is an essential phase in training a network. The idea is to compress the data so that the same amount of information can be represented by fewer bits. This also helps with the problem of the curse of dimensionality. A dataset with many attributes is different to train with because it tends to overfit the model. Hence dimensionality reduction techniques need to be applied before the dataset can be used for training.This is where the Autoencoder (AE) and Variational Autoencoder (VAE) come into play. They are end-to-end networks that are used to compress the input data. Both Autoencoder and Variational Autoencoder are used to transform the data from a higher to lower-dimensional space, essentially achieving compression.Autoencoder is used to learn efficient embeddings of unlabeled data for a given network configuration. The autoencoder consists of two parts, an encoder, and a decoder. The encoder compresses the data from a higher-dimensional space to a lower-dimensional space (also called the latent space), while the decoder does the opposite i.e., convert the latent space back to higher-dimensional space. The decoder is used to ensure that latent space can capture most of the information from the dataset space, by forcing it to output what was fed as input to the decoder.The block diagram can be seen below.During training, the input data x is fed to the encoder function e_theta(x). The input is passed through a series of layers (parameterized by the variable theta) reducing its dimensions to achieve a compressed latent vector z. The number of layers, type and size of the layers, and the latent space dimension are user-controlled parameters. Compression is achieved if the dimension of the latent space is less than that of the input space, essentially getting rid of redundant attributes.The decoder d_phi(z) usually (but not necessarily) consists of near-compliment layers of the layers used in the encoder but in reverse order. A near-complement layer of a layer is the one that can be used to undo the operations (to some extent) of the original layer such as transposed conv layer to conv layer, pooling to unpooling, fully connected to fully connected, etc.The entire encoder-decoder architecture is collectively trained on the loss function which encourages that the input is reconstructed at the output. Hence the loss function is the mean squared error between the encoder input and the decoder output.The idea is to have a very low dimensional latent space so that maximum compression is achieved, but at the same time, the error is small enough. Reducing the dimension of the latent space beyond a certain value will result in a significant loss of information.There are no constraints on the values/distribution of the latent space. It can be anything, as long as it can reconstruct the input when the decoder function is applied to it.Below is an example of the latent space generated by training the network on an MNIST dataset.It can be seen that the same digits tend to cluster themselves in the latent space. Another important thing to note is that there are parts of the latent space that doesn't correspond to any data point. Using those as inputs to the encoder will result in an output that doesn’t look like any digit from the MNIST data. This is what we mean by that the latent space is not regularized. Such a latent space only has a few regions/cluster that has the generative capability, which means that sampling any point in the latent space that belongs within a cluster will generate a variation of the data that the cluster belongs to. But the entire latent space does not have the generative capability. The regions which do not belong to any cluster will generate garbage output. Once the network is trained, and the training data is removed, we have no way of knowing if the output generated by the decoder from a randomly sampled latent vector is valid or not. Hence AE is mainly used for compression.For valid inputs, the AE is able to compress them to fewer bits essentially getting rid of the redundancy (Encoder) but due to non-regularized latent space AE, the decoder can not be used to generate valid input data from latent from vectors sampled from the latent space.The latent space of a linear autoencoder strongly resembles the eigenspace achieved during the principal component analysis of the data. A linear autoencoder with input space dimension n and latent space dimensions set to m<n result will span the same vector space as spanned by the first m eigenvectors of PCA. If AE is similar to PCA, why use AE? The power of AE comes with its non-linearity. Adding non-linearity (such as nonlinear activation functions, and more hidden layers) makes AE capable to learn rather powerful representations of the input data in lower dimensions with much less information loss.Variational autoencoder addresses the issue of non-regularized latent space in autoencoder and provides the generative capability to the entire space. The encoder in the AE outputs latent vectors. Instead of outputting the vectors in the latent space, the encoder of VAE outputs parameters of a pre-defined distribution in the latent space for every input. The VAE then imposes a constraint on this latent distribution forcing it to be a normal distribution. This constraint makes sure that the latent space is regularized.The block diagram of VAE can be seen below. During training, the input data x is fed to the encoder function e_theta(x). Just like AE, the input is passed through a series of layers (parameterized by the variable theta) reducing its dimensions to achieve a compressed latent vector z. However, the latent vector is not the output of the encoder. Instead, the encoder outputs the mean and the standard deviation for each latent variable. The latent vector is then sampled from this mean and standard deviation which is then fed to the decoder to reconstruct the input. The decoder in the VAE works similarly to the one in AE.The loss function is defined by the VAE objectives. VAE has two objectivesHence the training loss of VAE is defined as the sum of these the reconstruction loss and the similarity loss. The reconstruction error, just like in AE, is the mean squared loss of the input and reconstructed output. The similarity loss is the KL divergence between the latent space distribution and standard gaussian (zero mean and unit variance). The loss function is then the sum of these two losses.As mentioned before, the latent vector is sampled from the encoder-generated distribution before feeding it to the decoder. This random sampling makes it difficult for backpropagation to happen for the encoder since we can’t trace back errors due to this random sampling. Hence we use a reparameterization trick to model the sampling process which makes it possible for the errors to propagate through the network. The latent vector z is represented as a function of the encoder’s output.The training tries to find a balance between the two losses and ends up with a latent space distribution that looks like the unit norm with clusters grouping similar input data points. The unit norm condition makes sure that the latent space is evenly spread out and does not have significant gaps between clusters. In fact, the clusters of similar-looking data inputs usually overlap in some regions. Below is an example of the latent space generated by training the network on the same MNIST dataset, as was used to visualize the latent space of AE. Note how there are no gaps between clusters and the space resembles the distribution of unit norm.An important thing to note is that when the latent vector is sampled from the regions with overlapping clusters, we get morphed data. We get a smooth transition between the decoder's output when we sample the latent space moving from one cluster to the other.This article covered the understanding of Autoencoder (AE) and variational Autoencoder (VAE) which are mainly used for data compression and data generation respectively. VAE addresses the issue of non-regularized latent space of AE which makes it able to generate data from randomly sampled vectors from the latent space. The key summary points of AE and VAE are· Used to generate a compressed transformation of input in a latent space· The latent variable is not regularized· Picking a random latent variable will generate garbage output· The latent variable has a discontinuity· Latent variable is deterministic values· The latent space lacks the generative capability· Enforces conditions on the latent variable to be the unit norm· The latent variable in the compressed form is mean and variance· The latent variable is smooth and continuous· A random value of latent variable generates meaningful output at the decoder· The input of the decoder is stochastic and is sampled from a gaussian with mean and variance of the output of the encoder.· Regularized latent space· The latent space has generative capabilities.If this article was helpful to you or you want to learn more about Machine Learning and Data Science, follow Aqeel Anwar, or connect with me on LinkedIn or Twitter.aqeel-anwar.medium.com",03/11/2021,0,18.0,14.0,1622.0,819.0,9.0,1.0,0.0,4.0,en
4194,Predict Customer Churn with R,Towards Data Science,Susan Li,25000.0,10.0,718.0,"For any service company that bills on a recurring basis, a key variable is the rate of churn. Harvard Business Review, March 2016For just about any growing company in this “as-a-service” world, two of the most important metrics are customer churn and lifetime value. Entrepreneur, February 2016Customer churn occurs when customers or subscribers stop doing business with a company or service, also known as customer attrition. It is also referred as loss of clients or customers. One industry in which churn rates are particularly useful is the telecommunications industry, because most customers have multiple options from which to choose within a geographic location.Similar concept with predicting employee turnover, we are going to predict customer churn using telecom dataset. We will introduce Logistic Regression, Decision Tree, and Random Forest. But this time, we will do all of the above in R. Let’s get started!The data was downloaded from IBM Sample Data Sets. Each row represents a customer, each column contains that customer’s attributes:The raw data contains 7043 rows (customers) and 21 columns (features). The “Churn” column is our target.We use sapply to check the number if missing values in each columns. We found that there are 11 missing values in “TotalCharges” columns. So, let’s remove all rows with missing values.Look at the variables, we can see that we have some wrangling to do.2. We will change “No phone service” to “No” for column “MultipleLines”3. Since the minimum tenure is 1 month and maximum tenure is 72 months, we can group them into five tenure groups: “0–12 Month”, “12–24 Month”, “24–48 Months”, “48–60 Month”, “> 60 Month”[1] 1[1] 724. Change the values in column “SeniorCitizen” from 0 or 1 to “No” or “Yes”.5. Remove the columns we do not need for the analysis.Correlation between numeric variablesThe Monthly Charges and Total Charges are correlated. So one of them will be removed from the model. We remove Total Charges.Bar plots of categorical variablesAll of the categorical variables seem to have a reasonably broad distribution, therefore, all of them will be kept for the further analysis.First, we split the data into training and testing sets:Confirm the splitting is correct:[1] 4924 19[1] 2108 19Fitting the Logistic Regression Model:Feature Analysis:The top three most-relevant features include Contract, tenure_group and PaperlessBilling.Analyzing the deviance table we can see the drop in deviance when adding each variable one at a time. Adding InternetService, Contract and tenure_group significantly reduces the residual deviance. The other variables such as PaymentMethod and Dependents seem to improve the model less even though they all have low p-values.Assessing the predictive ability of the Logistic Regression model[1] Logistic Regression Accuracy 0.789373814041746Logistic Regression Confusion MatrixOdds RatioOne of the interesting performance measurements in logistic regression is Odds Ratio.Basically, Odds ratio is what the odds of an event is happening.Decision Tree visualizationFor illustration purpose, we are going to use only three variables for plotting Decision Trees, they are “Contract”, “tenure_group” and “PaperlessBilling”.Decision Tree Confusion MatrixWe are using all the variables to product confusion matrix table and make predictions.Decision Tree Accuracy[1] Decision Tree Accuracy 0.780834914611006The accuracy for Decision Tree has hardly improved. Let’s see if we can do better using Random Forest.Random Forest Initial ModelThe error rate is relatively low when predicting “No”, and the error rate is much higher when predicting “Yes”.Random Forest Prediction and Confusion MatrixRandom Forest Error RateWe use this plot to help us determine the number of trees. As the number of trees increases, the OOB error rate decreases, and then becomes almost constant. We are not able to decrease the OOB error rate after about 100 to 200 trees.Tune Random Forest ModelWe use this plot to give us some ideas on the number of mtry to choose. OOB error rate is at the lowest when mtry is 2. Therefore, we choose mtry=2.Fit the Random Forest Model After TuningOOB error rate decreased to 20.41% from 20.61% on Figure 14.Random Forest Predictions and Confusion Matrix After TuningBoth accuracy and sensitivity are improved, compare with Figure 15.Random Forest Feature ImportanceFrom the above example, we can see that Logistic Regression, Decision Tree and Random Forest can be used for customer churn analysis for this particular dataset equally fine.Throughout the analysis, I have learned several important things:Source code that created this post can be found here. I would be pleased to receive feedback or questions on any of the above.",16/11/2017,32,29.0,8.0,811.0,394.0,21.0,4.0,0.0,4.0,en
4195,"Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs",Towards Data Science,Boris Knyazev,579.0,16.0,2818.0,"I’m presenting an overview of important Graph Neural Network works, by distilling key ideas and explaining simple intuition behind milestone methods using Python and PyTorch. This post continues the first part of my tutorial.In the “Graph of Graph Neural Network (GNN) and related works” above, I added papers on graphs that I have come across in the last year. In this graph, a directed edge between two works denotes that one paper is based on the other (while not necessary citing it) and a color of the work denotes:Note, that some other important works and edges are not shown to avoid further clutter, and only a tiny fraction of works, highlighted in bold boxes, will be covered in this post. Disclaimer: I still found room to squeeze our own recent works there 😊.Most of the important methods are covered in this non-exhaustive list of works:The first work where graphs were classified using a neural network seems to be a 1997 paper by Alessandro Sperduti and Antonina Starita on “Supervised Neural Networks for the Classification of Structures”.Sperduti & Starita, 1997: “Until now neural networks have been used for classifying unstructured patterns and sequences. However, standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach.”From 1997, the body of works on learning from graphs has grown so much and in so many diverse directions that it is very hard to keep track without some smart automated system. I believe we are converging to using methods based on neural networks (based on our formula (2) explained in the first part of my tutorial), or some combination of neural networks and other methods.To recap the notation we used in the first part, we have some undirected graph G with N nodes. Each node in this graph has a C-dimensional feature vector, and features of all nodes are represented as an N×C dimensional matrix X⁽ˡ⁾. In a typical graph network, such as GCN (Kipf & Welling, ICLR, 2017), we feed these features X⁽ˡ⁾ to a graph neural layer with C×F dimensional trainable weights W⁽ˡ⁾ , so that the output of this layer is an N×F matrix X⁽ˡ⁺¹⁾ encoding updated (and hopefully better in some sense) node features. 𝓐 is an N×N matrix, where the entry 𝓐ᵢⱼ indicates if node i is connected (adjacent) to node j. This matrix is called an adjacency matrix. I use 𝓐 instead of plain A to highlight that this matrix can be normalized in a way to facilitate feature propagation in a deep network. For the purpose of this tutorial, we can assume that 𝓐=A, i.e. each i-th row of the matrix product 𝓐X⁽ˡ⁾ will contain a sum of features of node i neighbors.In the rest of this part of the tutorial, I’ll briefly explain works of my choice showed in bold boxes in the overview graph. I recommend Bronstein et al.’s review for a more comprehensive and formal analysis.Note that even though I dive into some technical details of spectral graph convolution below, many recent works (e.g., GIN in Xu et al., ICLR, 2019) are built without spectral convolution and show great results in some tasks. However, knowing how spectral convolution works is still helpful to understand and avoid potential problems with other methods.Bruna et al., 2014, ICLR 2014I explain spectral graph convolution in detail in my another post.I’ll briefly summarize it here for the purpose of this part of the tutorial. A formal definition of spectral graph convolution, which is very similar to the convolution theorem in signal/image processing, can be written as:where V are eigenvectors and Λ are eigenvalues of the graph Laplacian L, which can be found by eigen-decomposition: L=VΛVᵀ; W_spectral are filters. Throughout this tutorial I’m going to assume “symmetric normalized Laplacian”. It is computed based only on an adjacency matrix A of a graph, which can be done in a few lines of Python code as follows:Here, we assume that A is symmetric, i.e. A = Aᵀ and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers in the form of formula (2).So, given graph Laplacian L, node features X and filters W_spectral, in Python spectral convolution on graphs looks very simple:where we assume that our node features X⁽ˡ⁾ are 1-dimensional, e.g. MNIST pixels, but it can be extended to a C-dimensional case: we will just need to repeat this convolution for each channel and then sum over C as in signal/image convolution.Formula (3) is essentially the same as spectral convolution of signals on regular grids using the Fourier Transform, and so creates a few problems for machine learning:These issues prevent scaling to datasets with large graphs of variable structure.To solve the first issue, Bruna et al. proposed to smooth filters in the spectral domain, which makes them more local in the spatial domain according to the spectral theory. The idea is that you can represent our filter W_spectral from formula (3) as a sum of 𝐾 predefined functions, such as splines, and instead of learning N values of W, we learn K coefficients α of this sum:While the dimensionality of fk does depend on the number of nodes N, these functions are fixed, so we don’t learn them. The only thing we learn are coefficients α, and so W_spectral is no longer dependent on N. To make our approximation in formula (4) reasonable, we want K<<N to reduce the number of trainable parameters from N to K and, more importantly, make it independent of N, so that our GNN can digest graphs of any size.While solves the first issue, this smoothing method does not address the second issue.Defferrard et al., NeurIPS, 2016The main drawback of spectral convolution and its smooth version above is that it still requires eigen-decomposition of an N×N dimensional graph Laplacian L, which creates two main problems:Now, what does Chebyshev graph convolution have to do with all that?It turns out that it solves both problems at the same time! 😃That is, it avoids computing costly eigen-decomposition and the filters are no longer “attached” to eigenvectors (yet they still are functions of eigenvalues Λ). Moreover, it has a very useful parameter, usually denoted as K having a similar intuition as K in our formula (4) above, determining the locality of filters. Informally: for K=1, we feed just node features X⁽ˡ⁾ to our GNN; for K=2, we feed X⁽ˡ⁾ and 𝓐X⁽ˡ⁾; for K=3, we feed X⁽ˡ⁾, 𝓐X⁽ˡ⁾ and 𝓐²X⁽ˡ⁾; and so forth for larger K (I hope you’ve noticed the pattern). See more accurate and formal definition in Defferrard et al. and my code below, plus additional analysis is given in (Knyazev et al., NeurIPS-W, 2018).Due to the power property of adjacency matrices, when we perform 𝓐²X⁽ˡ⁾ we actually average (or sum depending on how 𝓐 is normalized) over 2-hop neighbors, and analogously for any n in 𝓐ⁿX⁽ˡ⁾ as illustrated below, where we average over n-hop neighbors.Note that to satisfy the orthogonality of the Chebyshev basis, 𝓐 assumes no loops in the graph, so that in each i-th row of matrix product 𝓐X⁽ˡ⁾ we will have features of the neighbors of node i, but not the features of node i itself. Features of node i will be fed separately as a matrix X⁽ˡ⁾.If K equals the number of nodes N, the Chebyshev convolution closely approximates a spectral convolution, so that the receptive field of filters will be the entire graph. But, as in the case of convolutional networks, we don’t want our filters to be as big as the input images for a number of reasons that I already discussed, so in practice, K takes reasonably small values.In my experience, this is one of the most powerful GNNs, achieving great results in a very wide range of graph tasks. The main downside is the necessity to loop over K in the forward/backward pass (since Chebyshev polynomials are recursive, so it’s not possible to parallelize them), which slows down the model.Same as with Splines discussed above, instead of training filters, we train coefficients, but this time, of the Chebyshev polynomial.To generate the Chebyshev basis, you can use the following Python code:The full code to generate spline and Chebyshev bases is in my github repo.To illustrate how a Chebyshev filter can look on a irregular grid, I follow the experiment from Bruna et al. again and sample 400 random points from the MNIST grid in the same way as I did to show eigenvectors of the graph Laplacian. I trained a Chebyshev graph convolution model on the MNIST images sampled from these 400 locations (same irregular grid is used for all images) and one of the filter for K=1 and K=20 is visualized below.Kipf & Welling, ICLR, 2017As you may have noticed, if you increase K of the Chebyshev convolution, it increases the total number of trainable parameters. For example, for K=2, our weights W⁽ˡ⁾ will be 2C×F instead of just C×F. This is because we concatenate features X⁽ˡ⁾ and 𝓐X⁽ˡ⁾ into a single N×2C matrix. More training parameters means the model is more difficult to train and more data must be labeled for training. Graph datasets are often extremely small. Whereas in computer vision, MNIST is considered a tiny dataset, because images are just 28×28 dimensional and there are only 60k training images, in terms of graph networks MNIST is quite large, because each graph would have N=784 nodes and 60k is a large number of training graphs. In contrast to computer vision tasks, many graph datasets have only around 20–100 nodes and 200–1000 training examples. These graphs can represent certain small molecules and labeling chemical/biological data is usually more expensive than labeling images. Therefore, training Chebyshev convolution models can lead to severe overfitting of the training set (i.e. the model will have the training loss close to 0 yet will have a large validation or test error). So, GCN of Kipf & Welling essentially “merged” matrices of node features X⁽ˡ⁾ and 𝓐X⁽ˡ⁾ into a single N×C matrix. As a result, the model has two times fewer parameters to train compared to Chebyshev convolution with K=2, yet has the same receptive field of 1 hop. The main trick involves adding “self-loops” to your graph by adding an identity matrix I to 𝓐 and normalizing it in a particular way, so now in each i-th row of matrix product 𝓐X⁽ˡ⁾ we will have features of the neighbors of node i, as well as features of node i.This model seems to be a standard baseline choice well-suited for many application due to its lightweight, good performance and scalability to larger graphs.The difference between GCN and Chebyshev convolution is illustrated below.The code above follows the same structure as in the first part of my tutorial, where I compared classical NN and GNN. One of the main steps both in GCN and Chebyshev convolution is computation of the rescaled graph Laplacian L. This rescaling is done to make eigenvalues in the range [-1,1] to facilitate training (this might be not a very important step in practice as weights can adapt during training). In GCN, self-loops are added to the graph by adding an identity matrix before computing the Laplacian as discussed above. The main difference between the two methods is that in the Chebyshev convolution we recursively loop over K to capture features in the K-hop neighborhood. We can stack such GCN or Chebyshev layers interleaved with nonlinearities to build a Graph Neural Network.Now, let me politely interrupt 😃 our spectral discussion and give a general idea behind two other exciting methods: Edge-conditioned filters by Simonovsky & Komodakis, CVPR, 2017 and MoNet by Monti et al., CVPR, 2017, which share some similar concepts.Simonovsky & Komodakis, CVPR, 2017As you know, in ConvNets we learn the weights (filters) by optimizing some loss like Cross Entropy. In the same way, we learn our W⁽ˡ⁾ in GNNs. Imagine that instead of learning these weights, you have another network that predicts them. So during training, we learn the weights of that auxiliary network, which takes an image or a graph as an input and returns weights W⁽ˡ⁾ (Θ in their work) as the output. The idea is based on Dynamic Filter Networks (Brabandere et al., NIPS, 2016), where “dynamic” means that filters W⁽ˡ⁾ will be different depending on the input as opposed to standard models in which filters are fixed (or static) after training.This is a very general form of convolution that, besides images, can be easily applied to graphs or point clouds as they did in their CVPR paper and got excellent results. However, there is no “free lunch”, and training such models is quite challenging, because the regular grid constraint is now relaxed and the scope of solutions increases dramatically. This is especially true for larger graphs with many edges or for convolution in deeper layers, which often have hundreds of channels (number of features, C), so you might end up generating thousands of numbers in total for each input! In this regard, standard ConvNets are so good, because we don’t waste the model’s capacity on training to predict these weights, instead we directly enforce that the filters should be the same for all inputs. But this prior makes ConvNets limited and we cannot directly apply them to graphs or point clouds. So, as always, there’s some trade-off between flexibility and performance in a particular task.When applied to images, like MNIST, the Edge-conditioned model can learn to predict anisotropic filters — filters that are sensitive to orientation, such as edge detectors. Compared to Gaussian filters discussed in the first part of my tutorial, these filters are able to better capture certain patterns in images, such as strokes in digits.I want to highlight one more time that whenever we have a complicated model with auxiliary networks, it becomes a chicken-or-the-egg problem in some sense. To solve it, one of the networks — the auxiliary or the main one — should receive a very strong signal, so that it can implicitly supervise another network. In our BMVC paper, which is similar to Simonovsky & Komodakis’s work, we apply additional constraints on the edge-generating network to facilitate training. I will describe our work in detail in later posts.Monti et al., CVPR, 2017MoNet is different from other works discussed in this post, as it assumes to have the notion of node coordinates, and therefore is more suited for geometric tasks such as 3D mesh analysis or image/video reasoning. It is somewhat similar to edge-conditioned filters of Simonovsky & Komodakis, since they also introduce an auxiliary learnable function 𝐷(𝑤, 𝜃, ρ) that predicts weights. The difference is that these weights depend on the node polar coordinates (angle 𝜃 and radius ρ); and trainable parameters 𝑤 of that function are constrained to be means and variances of Gaussians, so that instead of learning N×N matrices, we only learn fixed-size vectors (means and variances) independently of the graph size N. In terms of standard ConvNets, it would be the same as learning only 2 values (the mean and variance of a Gaussian) for each filter instead of learning 9, 25 or 121 values for 3×3, 5×5 or 11×11 dimensional filters respectively. This parameterization would greatly reduce the number of parameters in a ConvNet, but the filters would be very limited in their power to capture image features.Monti et al. train 𝐽 means and variances of Gaussians and the process of transforming node coordinates is similar to fitting them into a Gaussian Mixture Model. The model is quite computationally intensive to train if we want our filters to be global enough, but it can be a good choice for visual tasks (see our BMVC paper for comparison), yet it is often worse than simple GCN on non-visual tasks (Knyazev et al., NeurIPS-W, 2018). Since function D depends on coordinates, the generated filters are also anisotropic and have a shape of oriented and elongated Gaussians as illustrated below.Despite a lengthy discussion, we have only scratched the surface. Applications of graph neural networks are expanding far beyond typical graph reasoning tasks, like molecule classification. The number of different graph neural layers is increasing very quickly, similar to how it was for convolutional networks a few years ago, so it’s hard to keep track of them. On that note, PyTorch Geometric (PyG) — a nice toolbox to learn from graphs — frequently populates its collection with novel layers and tricks.Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of Find me on Github, LinkedIn and Twitter. My homepage.If you want to cite this blog post in your paper, please use:@misc{knyazev2019tutorial, title={Tutorial on Graph Neural Networks for Computer Vision and Beyond}, author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R}, year={2019}}",12/08/2019,0,58.0,201.0,849.0,466.0,11.0,4.0,0.0,62.0,en
4196,Important Topics in Machine Learning You Need to Know,Towards Data Science,Sabina Pokhrel,2100.0,13.0,1656.0,"Machine learning is a hot topic right now and everyone is trying to get their hands on any information they can get about the topic. With the amount of information that is out there about machine learning, one can get overwhelmed. In this post, I have listed some of the most important topics in machine learning that you need to know, along with some resources which can help you in further reading about the topics which you are interested to know in-depth.AI is a branch of computer science that aims to create intelligent machines that mimic human behaviour such as knowledge, reasoning, problem-solving, perception, learning, planning, ability to manipulate and move objectsAI is an area of computer science that emphasizes the creation of intelligent machines that work and react like humans.en.wikipedia.orgsearchenterpriseai.techtarget.comMachine learning falls under the umbrella of AI, that provides systems with the ability to automatically learn and improve from experience without being explicitly programmed.The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples we provide.The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.en.wikipedia.orgwww.expertsystem.commedium.comSupervised learning is a machine learning task of learning a function that maps an input to an output based on example input-output pairs. A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.In supervised learning, we have labelled training data.en.wikipedia.orgwww.coursera.orgUnsupervised learning is a machine learning task that draws inferences from datasets consisting of input data without labelled responses. The goal of unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.Clustering and association are some of the unsupervised learning subcategories.en.wikipedia.orgwww.datarobot.comdataconomy.comA neural network is a biologically-inspired programming paradigm which enables a computer to learn from observational data. The design of an artificial neural network is inspired by the biological neural network of the human brain, leading to a process of learning that’s far more capable than that of standard machine learning models.Neural networks, also known as artificial neural networks, consists of input and output layers, as well as a hidden layer consisting of units that transform the input into something that the output layer can use. They perform very well in tasks that require to find patterns.towardsdatascience.comneuralnetworksanddeeplearning.comwww.techradar.comIt is a concept in neural networks, which allows networks to adjust their hidden layers of neurons in situations where the outcome doesn’t match what the creator is hoping for.en.wikipedia.orgtowardsdatascience.comDeep learning is a subset of machine learning where multiple layers of neural networks are stacked to create a huge network to map input into the output. It allows the network to extract different features until it can recognize what it is looking for.www.techopedia.comtowardsdatascience.comwww.coursera.orgLinear regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. One example of a task where linear regression can be used is forecasting housing price based on past values.The cost function of linear regression is Root Mean Squared Error (RMSE) between predicted y value (pred) and true y value (y).en.wikipedia.orgtowardsdatascience.commachinelearningmastery.comLogistic regression is a supervised machine learning algorithm which is used for the classification problem. It is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transaction fraud or not a fraud.Logistic regression transforms its output using the logistic sigmoid function to return a probability value.There are two types of logistic regression:towardsdatascience.comen.wikipedia.orgwww.statisticssolutions.comThe k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.Can be used on recommendation systems.KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then voting for the most frequent label (in the case of classification) or averages the labels (in the case of regression).medium.comblog.usejournal.comwww.saedsayad.comRandom forest is like a universal machine learning technique that can be used for both regression and classification purpose. It consists of a large number of individual decision trees that operate as an ensemble. Each individual decision tree in the random forest spits out a class prediction and the class with the most votes become our model’s prediction.In general, a random forest model does not overfit, and even if it does, it is easy to stop it from overfitting.There is no need for a separate validation set for a random forest model.It makes only a few statistical assumptions. Does not assume that your data is normally distributed, nor it assumes that the relationships are linear.It requires very few pieces of feature engineering.towardsdatascience.comtowardsdatascience.comEnsemble learning helps improve machine learning results by combining several models. This approach allows the production of better performance compared to a single model.Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve prediction (stacking).Examples are random forest, Gradient boosted decision trees, ADA boost.towardsdatascience.comtowardsdatascience.comBoosting is an ensemble technique in which the predictors are not made independently, but sequentially.It is a method of converting weak learners into strong learners. Gradient boosting is an example of boosting. It is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble or weak prediction models, typically decision trees.medium.comblog.kaggle.comOverfitting happens when a model that models the training data too well.Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. It negatively impacts the model's ability to generalize.It can be prevented by:elitedatascience.commachinelearningmastery.comUnderfitting refers to a model that can neither model the training data nor generalize to new data. It will have poor performance on the training data.chemicalstatistician.wordpress.commedium.comwww.datarobot.comRegularization is a technique to modify machine learning models to avoid the problem of overfitting. You can apply regularization to any machine learning model. Regularization simplifies overly complex models that are prone to be overfitted by adding penalty tern to the objective function. If a model is overfitted, it will have problem generalizing and thus will give inaccurate predictions when it is exposed to new data sets.towardsdatascience.comtowardsdatascience.comA regression model that uses the L1 regularization technique is called Lasso Regression. A model which uses the L2 regularization technique is called Rigid Regression.The key difference between the two is the penalty term which is added to the loss function.Rigid regression adds “squared magnitude” of coefficient as penalty term to the loss function. Lasso regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.medium.comdevelopers.google.comwww.chioka.inCross-validation is a technique for evaluating machine learning models by training several ML models on subsets of the available input data and evaluating them on a complementary subset of the data. It is used to prevent overfitting of the model.Different types of cross-validation techniques are:towardsdatascience.comtowardsdatascience.comMean Absolute Error (MAE): measures the average of the absolute difference between actual and predicted values.Root Mean Squared Error (RMSE): measures the square root of the average of the differences of the squares between the actual and the predicted values.becominghuman.aimedium.commedium.comConfusion matrix: It is one of the most intuitive and easiest metrics used for finding the correctness and accuracy of the model. It is used for classification problem where the output can be of two or more types of classes.True Positives (TP): are the cases when the actual class of the data point was 1 (True) and the predicted is also 1 (True).True Negatives (TN): are the cases when the actual class of the data point was 0 (false) and the predicted is also 0 (False).False Positives (FP): are the cases when the actual class of the data point was 0 (False) and the predicted is 1 (True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one.False Negatives (FN): are the cases when the actual class of the data point was 1 (True) and the predicted is 0 (False). False because the model has predicted incorrectly and negative because the class predicted was a negative one (0).Accuracy: Accuracy in classification problems is the number of correct predictions made by the model over all the predictions made.When to use accuracy: accuracy is a good measure when the target variable classes in the data are nearly balanced.When not to use accuracy: accuracy should never be used as a measure when the target variable classes in the data are a majority of one class.Precision (hits): Precision is a measure that tells us what proportion of predicted values as True is actually True.Recall or sensitivity (misses): Recall is a measure that tells of what proportion of patients are actually true were predicted as being true by the model.F1 score: Represents both precision and recall.Receiver Operating Characteristic (ROC) curve: An ROC curve is a graph showing the performance of a classification model at all classification thresholds.The curve plots two parameters:AUC (Area Under the ROC Curve): AUC measures the entire two-dimensional area underneath the entire ROC curve.It provides an aggregate measure of the performance across all possible classification thresholds.medium.comtowardsdatascience.comThe topics discussed above were the basics of machine learning. We discussed the basic terms such as AI, machine learning and deep learning, different types of machine learning: supervised and unsupervised learning, some machine learning algorithms such as linear regression, logistic regression, k-nn, and random forest, and performance evaluation matrices for different algorithms.Which topic do you think is the most important one? Leave your thoughts as comments below.",04/09/2019,0,34.0,1.0,867.0,625.0,16.0,4.0,0.0,66.0,en
4197,Deriving Backpropagation with Cross-Entropy Loss,Towards Data Science,Essam Wisam,15.0,6.0,747.0,"There is a myriad of loss functions that you can choose for your neural network. The choice of loss function is imperative for the network’s performance because eventually the parameters in the network are going to be set such that the loss is minimized.Cross-Entropy loss is a popular choice if the problem at hand is a classification problem, and in and of itself it can be classified into either categorical cross-entropy or multi-class cross-entropy (with binary cross-entropy being a special case of the former.) In case you’re scratching your head about how different are these, I’ll try to introduce each before delving into the derivation.Let’s start with categorical cross-entropy. For this loss function our y’s are one-hot encoded to denote the class our image (or whatever) belongs to. Thus for any x, y is of length equal to the number of classes and the last layer in our model has a neuron for each class. We use Softmax in our last layer to get the probability of x belonging to each of the classes. These probabilities sum to 1.We’ll lightly use this story as a checkpoint. There we considered quadratic loss and ended up with the equations below.Note that the output (activations vector) for the last layer is aᴴ and in index notation we would write aᴴₙ to denote the nth neuron in the last layer. The same applies for the pre-activations vector zᴴ.The only equation that we expect to change in the system above is that for δᴴ because we used the explicit formula for the loss function to find that. As a result, we’ll be only dealing with the last layer in the derivation so we might as well drop the superscript for now and keep in mind that we’re targeting the last layer whenever we write a, z or δ.By this, the loss function iswith the activation of the nth neuron in the last layer beingNotice that the activation of the nth neuron depends on the pre-activations of all other neurons in the layer. This would’ve not been the case if the last layer involved Sigmoid or ReLU activations. On this account, to find δ for some neuron in the last layer we use the chain-rule by writingIt will simplify things a bit if we write this asStarting with ∂J/∂aₙ we can writeand for ∂aₙ/∂zₙ we haveYou can arrive at the derivative using the quotient rule but there are other ways that might be worth checking out.Now multiplying both of our results we and plugging in the original equation we getFor the remaining sum, let’s first compute ∂J/∂aₘ by writingAnd then for ∂aₘ/∂zₙ (m≠n) we haveNow multiplying both results, we getand propagating that back to the original equation we getwhich can be simplified toprovided that y is a one-hot vector, we know that Σₘyₘ=1; thus, we can writeor in vector form after reimposing the superscript (H to denote the last layer)To conclude the proof, let’s update the backpropagation equations toWe’re not all there yet. Using categorical cross-entropy your model would do good classifying the image below as a dog; however, your dataset might include many images that have both cats and dogs in them. In this case, y is no longer a one-hot vector (and rather looks something like [0 1 0 0 1] if two classes are present within the image.) Thus, we use multi-class cross-entropy and refrain from using Softmax in the last layer; instead, we use Sigmoid. We conclude that a class is present in the image if its Sigmoid activation is greater than some threshold (0.5 for instance.)The multi-class cross-entropy loss function for on example is given byIf we go back to dropping the superscript we can writeBecause we’re using Sigmoid, we also haveUnlike Softmax aₙ is only a function in zₙ; thus, to find δ for the last layer, all we need to consider is thator more preciselywhich isby differentiating (like we did earlier) then adding the two fractions we getNow we need to consider ∂aₙ/∂zₙ before plugging in equation 2.0this is simply the derivative of the Sigmoid functionby substituting aₙ for it’s definition we getNow using both results in the original equationthereby,and now by reimposing the superscript and writing this in vector formwhich quite marvelously is the same result we got for categorical cross-entropy with Softmax as an activation. So we still useas our backpropagation equations.References:[1] Pixabay, 2021, https://pixabay.com/de/photos/haustiere-niedlich-katze-hund-3715733/. Accessed 2 Oct 2021.[2] Loroy, Pauline. “Photo By Pauline Loroy On Unsplash”. Unsplash.Com, 2021, https://unsplash.com/photos/U3aF7hgUSrk. Accessed 2 Oct 2021.",02/10/2021,0,2.0,30.0,1396.0,234.0,33.0,0.0,0.0,4.0,en
4198,The Ultimate Guide to Clustering Algorithms and Topic Modeling,Towards Data Science,Zijing Zhu,1100.0,8.0,1313.0,"Clustering is one of the most used unsupervised machine learning algorithms. You can think of clustering as putting unorganized data points into different categories so that you can learn more about the structures of your data. Clustering has a variety of applications in extracting information from data without labels. For example, companies cluster customers based on their characteristics, like purchasing behaviors, to make better market campaigns, to set pricing strategies to make more profit, etc. Clustering algorithms are also widely used in natural language processing (NLP) to extract information from unstructured textual data, and topic modeling is one example.The series of articles aims to provide readers with a thorough view of two common but very different clustering algorithms, K-Means and Latent Dirichlet Allocation (LDA), and their applications in topic modeling:Part 1: A beginner’s guide to K-Means (this article)Part 2: A beginner’s guide to LDAPart 3: Use K-Means and LDA for topic modeling (coming soon)In this article, I will introduce the details of applying the K-Means algorithm.K-Means is one of the simplest clustering algorithms to detect common patterns in unorganized data points. The algorithm classifies all data points into K clusters by identifying close data points based on their similarities. K-Means simply defines similarity as the Euclidean distance measured in the feature space. For illustration purposes, using the code below, I generate three clusters of normally distributed points in two dimensions:The graph below shows the manually generated three clusters of data. Since the sample data only have two features, we can clearly see which points are close to each other in a two-dimensional graph. The same intuition stands when feature space increase way over two dimensions in real applications.To apply K-Means, researchers first need to determine the number of clusters. Then the algorithm will assign each sample to the cluster where its distance from the center of the cluster is minimized. The code is straightforward:Here, since we generated the data, we know there are three clusters. What about in real applications when you know little about the data? This article will discuss the answer to this question after introducing the mathematical details of the K-Means algorithm.K-Means algorithm predetermines the number of clusters K, and then assigns a collection of clusters C = {C1, C2,…Ck} that minimize:where 𝜇𝑘 is the center of the points of cluster Ck:The algorithm works as follows:2. Initialize the center point 𝜇𝑘 (k ∈ K) of each cluster with a random value.3. Calculate the squared Euclidean distance of each data point X_j to the center point of each cluster.4. Assign X_j to the closest cluster k where the squared Euclidean distance is minimized:5. Update 𝜇𝑘 by taking the mean of sample points assigned to cluster k.6. Repeat Steps 3 to 5 until converge.Notice that the iterative steps lower the objective function, and there are only a finite number of possible partitions of the points, so the algorithm is guaranteed to converge. However, the converged solution may not be globally optimal. By default, K-Means runs the clustering algorithm ten times with different centroid seeds and takes the best result in metrics.How to determine the goodness-of-fit for clustering models like K-means? The answer to this question is important for finding the best fit model during the iteration steps and plays an important role in helping researchers decide the number of clusters. The most used metrics for clustering algorithms are inertia and silhouette.InertiaInertia measures the distance from each data points to its final cluster center. For each cluster, inertia is given by the mean squared distance between each data point X_j ∈ Ck and the center 𝜇𝑘:After summing up inertias from all clusters, the total inertia is used to compare the performance of different K-Means models:By the definition of inertia, it is clear that we need to choose the model that minimizes the total inertia.Silhouette CoefficientThe silhouette coefficient is another metric that can be used in clustering algorithms. For each data point X_j, it calculates the average distance between X_j and all other points in the same clusters. Let’s define it as a_j. Then it looks for X_j’s next nearest cluster (the second-best cluster to classify X_j) and calculates the average distance between X_j and all points in this cluster as b_j. The silhouette coefficient for point X_j is:From the equation above, we can see the silhouette coefficient is between -1 and 1 depending on how which one is larger, a_j or b_j:If b_j is larger than a_j, it means the model has clustered X_j into the best cluster. The larger the b_j compared to a_j, the better the cluster. Otherwise, when a_j is larger, it means this point should probably be in the other cluster. The larger the a_j compared to b_j, the closer the value to -1.The silhouette coefficient gives information about how tightly each cluster is when calculating its average for all data points in the clusters. It measures the model performance when calculating the average silhouette coefficient for all data points.Unfortunately, we cannot solve this question analytically, but it is useful to follow some general rules when deciding K. The number of K is determined both mathematically and practically. To deliver the best model, we can calculate the inertia from the different choices of K and choose the one that is the most efficient. This is when the Elbow curve comes in handy. The Elbow curve plots the inertia for different K. Note that inertia will always decrease as K increase. If K equals the number of total data points, inertia will be zero (each data point is a cluster). We can use the Elbow curve to check the decreasing speed and choose the K at the Elbow point when after this point, inertia decreases substantially slower.Using the data points generated above and the code below, we can plot the Elbow curve:and this is the plot:The plot shows inertia keeps decreasing as K increases. When K = 3, there is a turning point in terms of the inertia decreasing speed, and this is the Elbow point we should look for. Note that when we generate the data, we use three normal distributions. Thus it matches the result based on the Elbow curve.We also need to consider whether the number of clusters we decide using the Elbow curve is practical. We should choose the K that is easy to interpret and practically feasible. For example, if your company only has the resources (labor and capital) to cluster customers into three categories, then you should set K at three regardless of the Elbow curve’s suggestions.We need to preprocess the features with at least two steps for better model performance and more reliable results: scaling and dimensionality reduction.Scale FeaturesSince K-Means uses the Euclidean distance between data points to define clusters, all features should be scaled such that their units are comparable. Otherwise, certain features’ variance that comes from incomparable units may bias the result. We can scale the data with scikit-learn’s StandardScaler transformer. It centers each feature around its mean and scales it by dividing by that feature's standard deviation.Perform Dimensionality ReductionAs you may notice, clustering algorithms are computationally complex, and the complexity increases fast with the number of features. Thus, it is very common to reduce the dimensionality of the data before applying the K-Means clustering algorithm.Principal Components Analysis (PCA) is a widely used algorithm for reducing the number of features in data while retaining as much information as possible. You can refer to this document for more details.K-Means is the simplest and most popular clustering algorithm with a variety of use cases. This article focuses on introducing its mathematical details, the metrics it uses, and suggestions when applying it. In the next articles, I will introduce an alternative clustering algorithm, LDA, and the applications of both K-Means and LDA in topic modeling.Thank you for reading! Here is the list of all my blog posts. Check them out if you are interested.zzhu17.medium.comzzhu17.medium.com",23/07/2021,3,16.0,2.0,308.0,244.0,11.0,1.0,0.0,6.0,en
4199,Text Summarization from scratch using Encoder-Decoder network with Attention in Keras,Towards Data Science,Varun Saravanan,16.0,15.0,2309.0,"During our school days, most of us would have encountered the reading comprehension section of our English paper. We would be given a paragraph or Essay based on which we need to answer several questions.How do we as humans approach this task at hand? We go through the entire text, make sense of the context in which the question is asked and then we write answers. Is there a way we can use AI and deep learning techniques to mimic this behavior of us?Automatic text summarization is a common problem in machine learning and natural language processing (NLP). There are two approaches to this problem.2. Abstractive Summarization-Abstractive text summarization, on the other hand, is a technique in which the summary is generated by generating novel sentences by either rephrasing or using the new words, instead of simply extracting the important sentences. For example, some questions in the reading comprehension might not be straightforward in such cases we do rephrasing or use new words to answer such questions.We humans can easily do both kinds of text summarization. In this blog let us see how to implement abstractive text summarization using deep learning techniques.Problem StatementGiven a news article text, we are going to summarize it and generate appropriate headlines.Whenever any media account shares a news story on Twitter or in any social networking site, they provide a crisp headlines /clickbait to make users click the link and read the article.Often media houses provide sensational headlines that serve as a clickbait. This is a technique often employed to increase clicks to their site.Our problem statement is to generate headlines given article text. For this we are using the news_summary dataset. You can download the dataset hereBefore we go through the code, let us learn some concepts needed for building an abstractive text summarizer.Techniques like multi-layer perceptron(MLP) work well your input data is vector and convolutional neural networks(CNN) works very well if your input data is an image.What if my input x is a sequence? What if x is a sequence of words. In most languages sequence of words matters a lot. We need to somehow preserve the sequence of words.The core idea here is if output depends on a sequence of inputs then we need to build a new type of neural network which gives importance to sequence information, which somehow retains and leverages the sequence information.We can build a Seq2Seq model on any problem which involves sequential information. In our case, our objective is to build a text summarizer where the input is a long sequence of words(in a text body), and the output is a summary (which is a sequence as well). So, we can model this as a Many-to-Many Seq2Seq problem.A many to many seq2seq model has two building blocks- Encoder and Decoder. The Encoder-Decoder architecture is mainly used to solve the sequence-to-sequence (Seq2Seq) problems where the input and output sequences are of different lengths.Generally, variants of Recurrent Neural Networks (RNNs), i.e. Gated Recurrent Neural Network (GRU) or Long Short Term Memory (LSTM), are preferred as the encoder and decoder components. This is because they are capable of capturing long term dependencies by overcoming the problem of vanishing gradient.Let us see a high-level overview of Encoder-Decoder architecture and then see its detailed working in the training and inference phase.Intuitively this is what happens in our encoder-decoder network:1. We feed in our input (in our case text from news articles) to the Encoder unit. Encoder reads the input sequence and summarizes the information in something called the internal state vectors (in case of LSTM these are called the hidden state and cell state vectors).2. The encoder generates something called the context vector, which gets passed to the decoder unit as input. The outputs generated by the encoder are discarded and only the context vector is passed over to the decoder.3. The decoder unit generates an output sequence based on the context vector.We can set up the Encoder-Decoder in 2 phases:In the training phase at every time step, we feed in words from a sentence one by one in sequence to the encoder. For example, if there is a sentence “I am a good boy”, then at time step t=1, the word I is fed, then at time step t=2, the word am is fed, and so on.Say for example we have a sequence x comprising of words x1,x2,x3,x4 then the encoder in training phase looks like below:The initial state of the LSTM unit is zero vector or it is randomly initiated. Now h1,c1 is the state of LSTM unit at time step t=1 when the word x1 of the sequence x is fed as input.Similarly h2,c2 is the state of the LSTM unit at time step t=2 when the word x2 of the sequence x is fed as input and so on.The hidden state (hi) and cell state (ci) of the last time step are used to initialize the decoder.Now the initial states of the decoder are initialized to the final states of the encoder. This intuitively means that the decoder is trained to start generating the output sequence depending on the information encoded by the encoder.<start> and <end> are the special tokens that are added to the target sequence(in our case the headlines we want to predict)before feeding it into the decoder.The target sequence is unknown while decoding the test sequence. So, we start predicting the target sequence by sending the first word into the decoder which would be always the <start> token. And the <end> token signals the end of the sentence.Now at the inference phase, we want our decoder to predict our output sequence(in our case headlines). After training, the model is tested on new source sequences for which the target sequence is unknown. So, we need to set up the inference architecture to decode a test sequenceAt every time step, the LSTM unit in my decoder gives me outputs y¹,y²,y³…y^k. where k is the length of the output sequence. At time step t=1 output y¹ is generated, at time t=2 output y ^2 is generated and so on.But in the testing stage as mentioned earlier we do not know what the length of our target sequence would be. How do we tackle this problem? Or in other words, how do we decode the test sequence. We follow the below steps for the same :Researchers observed that the BLEU score deteriorates as the sentence length for the source and reference text increases. It does a reasonable job up-till a sentence length of 20, after that the score falls.For our task both the source and target sentence length are higher than 20, hence we need to overcome this shortcoming of the encoder-decoder network.There are 2 different classes of attention mechanism depending on the way the attended context vector is derived:We will be using global attention for our task at hand.Now that we have learned all the concepts lets dive deep into code. First, let us import all the necessary librariesKeras does not officially support the attention layer. So, we can either implement our attention layer or use a third-party implementation. We will go with the latter option for this blog. You can download the attention layer from here and copy it in a different file called attention.py and then we can import the same.Now let us read our dataset. Due to computational constraints we shall just load 20000 rows from our dataset.Now we need to clean our text, we perform the following steps for the text and headlines pair:We use the following function to expand contractionsWe preprocess the text and headline pairs using the below function:The above code snippet does preprocessing for article text. The same code can be used for the headlines' column also.Here the text is our Source and the headlines are our target. We need to add start and end tokens to our target sequences ie our headlines as we saw earlier.Now we will add a new feature word count. We will add this feature for text as well as headlines. Then let us see the percentile values of word count for text and headlines. This will help us to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequenceLet us get the percentile values of the word count of text.Let us see the percentile values from 90 to 100 for the word count of textWe take the 95th percentile value ie 62 to be our maximum length of text. Similarly we plot the percentile values of headlines and we take 15 to be the maximum length of headlines.Test Train splitBefore moving on to build our model, we need to do a test train split for our data. We use the sklearn to do the same. We will use 70 % of the data as training data and evaluate the performance on the remaining 30 %.We earlier saw that to the encoder we send the text sequences at every time step. The words from the input sequence at every time step is passed to the encoder network. Before it is passed as input to the encoder unit we add an embedding layer.An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.For instance, “coconut” and “polar bear” are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But “kitchen” and “dinner” are related words, so they should be embedded close to each other.Word embeddings are computed by applying dimensionality reduction techniques to datasets of co-occurrence statistics between words in a corpus of text. This can be done via neural networks (the “word2vec” technique), or matrix factorization.Glove embeddingsOne of the most popularly used embedding technique is GloVe embedding. GloVe stands for “Global Vectors for Word Representation”.We will be using GloVe embeddings for our input text sequence(you can read about GloVe here).Specifically, we will use the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. You can download them hereNow we need to check how many words from our input text sequence corpus are present in the Glove Corpus. This is very important as our model will be only learning the words present in the Glove corpus. We can check this by using the code below :We observe that the percentage of words present in both glove vectors and our corpus is nearly 73% which is very good.Any Machine learning or deep learning model cannot understand text directly. We need to convert it into numbers. We can do this through tokenization.Tokenization is the process of dividing the text into a set of meaningful pieces. These pieces are called tokens. For example, we can divide a chunk of text into words, or we can divide it into sentences. A tokenizer builds the vocabulary and converts a word sequence to an integer sequence.We tokenize the text as well as the headline, then we convert it to integers and then pad it to maximum length ( we arrived at this maximum length in the previous section)Next, let’s build the dictionary to convert the index to word for target and source vocabularyNow in the earlier section, we had loaded the glove model. Let us now get the weight matrix for the text sequences from the glove model. Words not found in the glove embedding would be initialized to zero vectors. We can use the following piece of code to get the weight matrix :We build our Encoder by stacking 3 LSTMs vertically with the horizontal axis being the time axis. Stacking helps in increasing the model complexity and also helps in better representation of our input text sequence.A stacked LSTM model would visually look like this :(we are using stacking only for our Encoder architecture and not for decoder as shown in the image below.)We can use the below code to build our model:Some important terminologiesLet us have a look at our model summary :Now we see that there are non-trainable parameters. These parameters are from weight vectors of our embedding matrix. We have set trainable as False and hence these weights will not be learned by our model.We shall now go ahead and compile our modelNow let us checkpoint our model and save the best weights. Also, let us define early stopping for our model with patience of 3 (which means the training stops if the validation loss does not reduce after 3 epochs)Now let us train our model with a batch size of 128 andWe plot the Epoch vs Val loss plotWe observe that the val loss does not reduce after the 18th Epoch so we stop the training.Now we build the inference for our model as discussed in the previous sectionsNow we are defining a function below which is the implementation of the inference process (decoding test sequence)Now let us define the functions to convert an integer sequence to a word sequence for text as well as headlines :Finally we have the prediction function, lets see how our model performs!The following are some predictions made by our model. The predictions may not be very good as we have trained our model on fewer data points (around 15 K).Even though the predicted and actual headlines do not match exactly in terms of words but somewhat the same meaning is conveyed. This is quite a decent result we have got having trained our model on a very small dataset. Google /Facebook/Amazon train models on Giga/Tera Bytes of data :).We can still improve our model to a great extent by",14/06/2020,0,40.0,1.0,1015.0,380.0,24.0,10.0,0.0,12.0,en
4200,"Linear Discriminant Analysis: MATLAB, R and Python codes — All you have to do is just preparing data set (very simple, easy and practical)",Medium,DataAnalysis For Beginneｒ,279.0,3.0,259.0,"I release MATLAB, R and Python codes of Linear Discriminant Analysis (LDA). They are very easy to use. You prepare data set, and just run the code! Then, LDA and prediction results for new samples can be obtained. Very simple and easy!You can buy each code from the URLs below.https://gum.co/uVtRo Please download the supplemental zip file (this is free) from the URL below to run the LDA code. http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.ziphttps://gum.co/bZPL Please download the supplemental zip file (this is free) from the URL below to run the LDA code. http://univprofblog.html.xdomain.jp/code/R_scripts_functions.ziphttps://gum.co/JHFt Please download the supplemental zip file (this is free) from the URL below to run the LDA code. http://univprofblog.html.xdomain.jp/code/supportingfunctions.zipTo perform appropriate LDA, the MATLAB, R and Python codes follow the procedure below, after data set is loaded.1. Autoscale explanatory variable (X) Autoscaling means centering and scaling. Mean of each variable becomes zero by subtracting mean of each variable from the variable in centering. Standard deviation of each variable becomes one by dividing standard deviation of each variable from the variable in scaling.2. Construct LDA model3. Calculate confusion matrix between actual Y and calculated Y Accuracy rate, detection rate, precision and so on can be calculated from each confusion matrix, if necessary.4. In prediction, subtract the mean in the autoscalling of X in 1. from X-variables, and then, divide X-variables by the standard deviation in the autoscalling of X in 1., for new samples5. Estimate Y based on LDA model in 2.MATLAB: https://gum.co/uVtRoR: https://gum.co/bZPLPython: https://gum.co/JHFtMATLAB: http://univprofblog.html.xdomain.jp/code/MATLAB_scripts_functions.zipR: http://univprofblog.html.xdomain.jp/code/R_scripts_functions.zipPython: http://univprofblog.html.xdomain.jp/code/supportingfunctions.ziphttps://medium.com/@univprofblog1/data-format-for-matlab-r-and-python-codes-of-data-analysis-and-sample-data-set-9b0f845b565a#.3ibrphs4hEstimated values of Y for “data_prediction2.csv” are saved in ”PredictedY2.csv”.Please see the article below. https://medium.com/@univprofblog1/settings-for-running-my-matlab-r-and-python-codes-136b9e5637a1#.paer8scqy",26/08/2016,0,11.0,0.0,609.0,453.0,3.0,0.0,0.0,14.0,en
4201,Why AlphaGo Zero is a Quantum Leap Forward in Deep Learning,Intuition Machine,Carlos E. Perez,28000.0,12.0,2807.0,"Self-play is Automated Knowledge CreationThe 1983 movie “War Games” has a memorable climax where the supercomputer known as WOPR (War Operation Plan Response) is asked to train on itself to discover the concept of an un-winnable game. The character played by Mathew Broderick asks “Is there any way that it can play itself?”34 years later, DeepMind has shown how this is exactly done in real life! The solution is the same, set the number of players to zero (i.e. zero humans).There is plenty to digest about this latest breakthrough in Deep Learning technology. DeepMind authors use the term “self-play reinforcement learning”. As I remarked in the piece about “Tribes of AI”, DeepMind is particularly fond of their Reinforcement Learning (RL) approach. DeepMind has taken the use of Deep Learning layers in combination with more classical RL approaches to an art form.AlphaGo Zero is the latest incarnation of its Go-playing automation. One would think that it would be hard to top the AlphaGo version that bested the human world champion in Go. AlphaGo Zero however not only beats the previous system, but does it in a manner that validates a revolutionary approach. To be more specific, this is what AlphaGo has been able to accomplish:Each of the above bullet points is a newsworthy headline. The combination of each bullet point and what it reveals is completely overwhelming. This is my honest attempt to make sense of all of this.The first bullet point for many will seem unremarkable. Perhaps it’s because incremental improvements in technology have always been the norm. Perhaps one algorithm besting another algorithm 100 straight times intuitively doesn’t have the same appeal of one human besting another human 100 straight times. Algorithms don’t have the kind of inconsistency that we find in humans.One would expect though that the game of Go would have a large enough search space that there would be a chance of a less capable algorithm to be lucky enough to beat a better own. Could it be that AlphaGo Zero has learned new alien moves that its competitors are unable to reason about the same search space and thus having an insurmountable disadvantage. This apparently seems to be the case and is sort of alluded to by the fact that AlphaGo Zero requires less compute resources to best its competitors. Clearly, it’s doing a lot less work, but perhaps it is just working off a much richer language of Go strategy. Less work is what biological creatures aspire to do. Language compression is a means to arrive at less cognitive work.The second bullet point challenges our current paradigm of supervised only machine learning. The original AlphaGo was bootstrapped using previously recorded tournament gameplay. This was then followed with self-play to improve its two internal neural networks (i.e. policy and value networks). In contrast, AlphaGo Zero started from scratch with just the rules of Go programmed. It also required a single network rather than two. It is indeed surprising that it was able to bootstrap itself and then eventually learning more advanced human strategies as well as previously unknown strategies. Furthermore, the order in what strategies it learned first were sometimes unexpected. It is as if the system had learned a new internal language of how to play Go. It is also interesting to speculate as to the effect of a single integrated neural network versus two disjoint neural networks. Perhaps there are certain strategies that a disjoint network cannot learn.Humans learn languages through metaphors and stories. The human strategies discovered in Go are referred to with names so as to be recognizable by a player. It could be possible that the human language of Go is inefficient in that it is unable to express more complex compound concepts. What AlphaGo Zero seems to be able to do is perform its moves in a way that satisfies multiple objectives at the same time. So humans and perhaps earlier versions of AlphaGo were constrained to a relatively linear way of thinking, while AlphaGo Zero was not encumbered with an inefficient language of strategy. It is also interesting that one may consider this a system that actually doesn’t use the implicit bias that may reside in a language. David Silver, of DeepMind, has an even more bold claim:It’s more powerful than previous approaches because by not using human data, or human expertise in any fashion, we’ve removed the constraints of human knowledge and it is able to create knowledge itself.The Atlantic reports about some interesting observation of the game play of this new system:Expert players are also noticing AlphaGo’s idiosyncrasies. Lockhart and others mention that it almost fights various battles simultaneously, adopting an approach that might seem a bit madcap to human players, who’d probably spend more energy focusing on smaller areas of the board at a time.The learned language is devoid of any historical baggage that it may have accumulated over the centuries of Go study.The third bullet point says that training time is also surprisingly less than its previous incarnation. It is as if AlphaGo Zero learns how to improve its own learning.It took only 3 days to get to a level that beats the best human player. Furthermore, it just keeps getting better even after it surpasses the best previous AlphaGo implementation. How is it capable of improving its learning continuously? This ability to incrementally learn and improve the same neural network is something we’ve seen in another architecture known as FeedbackNet. In the commonplace SGD based learning, the same network is fed data across multiple epochs.Here however, each training set is entirely new and increasingly more challenging. It is also analogous to curriculum learning, however the curriculum is intrinsic in the algorithm. The training set is self generated and the calculation of the objective function is derived from the result of MCTS. The network learns by comparing itself not from external training data but from synthetic data that is generated from a previous version of the neural network.The fourth bullet point, the paper reports that it took only 4 Google TPUs ( 180 teraops each ) as compared to 48 TPUs for previous systems. Even surprisingly, the Nature paper notes that this ran on a single system and did not use distributed computing. So anyone with four Volta based Nvidia GPUs has the horse power to replicate these results. Performing a task with 1/10th the amount of compute resources should be a hint to anyone that something fundamentally different is happening over here. I have yet to analyze this in detail, but perhaps the explanation is due to just a simpler architecture.Finally, the last bullet point where it appears that AGZ advanced its capabilities using less training data. It appears that the synthetic data generated by self-play has more ‘teachable moments’ than data that’s derived from human play. Usually, the way to improve a network is to generate more synthetic data. The usual practice is to augment data by doing all sorts of data manipulations (ex. cropping, translations, etc), however in AGZ’s case, the automation seemed to be able to select richer training data.Almost every new Deep Learning paper that is published (or found in Arxiv) tends to show at best a small percentage improvement over previous architectures. Almost every time, the newer implementation also requires more resources to achieve higher prediction accuracies. What AlphaGo has shown is unheard of, that is, it requires an order of magnitude less resources and a less complex design, while unequivocally besting all previous algorithms.Many long time practitioners of reinforcement learning applied to games have commented that the actual design isn’t even novel and has been formulated decades ago. Yet, the efficacy of this approach has finally been experimentally validated by the DeepMind team. In Deep Learning like in sports, you can’t win on paper, you actually have to play the game to see who wins. In short, no matter a simple an idea may be, you just never know how well it will work unless the experiments are actually run.There is nothing new about the policy iteration algorithm or the architecture of the neural network. Policy iteration is a old algorithm that learns improving policies, by alternating between policy estimation and policy improvement . That is, between estimating the value function of the current policy and using the current value function to find a better policy.The single neural network that it uses is a pedestrian convolution network:The overall network depth, in the 20- or 40-block network, is 39 or 79 parameterized layers, respectively, for the residual tower, plus an additional 2 layers for the policy head and 3 layers for the value head.Like the previous incarnations of AlphaGo, Monte Carlo Tree Search (MCTS) is used to select the next move. AlphaGo Zero takes advantage of the calculations of the tree search as a way to evaluate and train the neural network. So basically, MCTS employing a previously trained neural network, performs a search for winning moves. The policy evaluation estimates the value function from many sampled trajectories. The results of this search is then used to drive the learning of the neural network. So after every game, a new and potentially improved network is selected for the next self-play game. DeepMind calls this “Self-play reinforcement learning”:A novel reinforcement learning algorithm. MCTS search is executed, guided by the neural network fθ. The MCTS search outputs probabilities π of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network fθ(s); MCTS may therefore be viewed as a powerful policy improvement operator.Self-play with search — using the improved MCTS-based policy to select each move, then using the game winner z as a sample of the value — may be viewed as a powerful policy evaluation operator.With each iteration of self-play, the system learns to become a stronger player. I find it odd that the exploitive search mechanism is able to creatively discover new strategies while simultaneous using less training data. It is as if self-play is feeding back into itself and learning to learn better.This self-play reminds me of an earlier writing about “The Strange Loop in Deep Learning.” I wrote about many recent advances in Deep Learning such as Ladder networks and Generative Adversarial Networks (GANs) that exploited a loop based method to improve recognition and generation. It seems that when you have this kind of mechanism, that is able to perform assessments of its final outputs, that the fidelity is much higher with less training data. In the case of AlphaGo Zero, there’s is no training data to speak of. The training data is generated through self-play. A GAN for example, collaboratively improves its generation by having two networks (discriminator and generator) work with each other. AlphaGo Zero, in contrast pits the capabilities of a network trained in a previous game against that of the current network. In both cases, you have two networks that feed of each other in training.An important question that should be in everyone’s mind is: “How general is AlphaGo Zero’s algorithm?” DeepMind has publicly stated that they will be applying this technology to drug discovery. Earlier I wrote about how to assess the appropriateness of Deep Learning technologies (see: Reality Checklist). In that assessment, there are six uncertainties in any domain that needs to be addressed: execution uncertainty, observational uncertainty, duration uncertainty, action uncertainty, evaluation uncertainty and training uncertainty.In the AlphaGo Zero, the training uncertainty, seems to have been addressed. AlphaGo Zero learns the best strategies by just playing against itself. That is, it is able to “imagine” situations and then discover through self-improvement the best strategies. It can do this efficiently because all the other uncertainties are known. That is, there is no indeterminism in the results of a sequence of actions. There is complete information. The effects of actions are predictable. There is a way to measure success. In short, the behavior of the game of Go is predictable, real world systems however are not.In many real world contexts however, we can still build accurate simulations or virtual worlds. Certainly the policy iteration methods found here may seem to be applicable to these virtual worlds. Reinforcement learning has been applied to virtual worlds (i.e. video games and strategy games). DeepMind has not yet reported experiments of using policy iteration in Atari games. Most games of course don’t need this sophisticated look ahead that requires MCTS, however there are some games like Montezuma’s Revenge that do. DeepMind’s Atari game experiments were like AlphaGo Zero, in that there was no need for human data to teach a machine.The difference between AlphaGo Zero and the video game playing machines is that the decision making at every state in the game is much more sophisticated. In fact there is an entire spectrum of decision making required for different games. Is MCTS the most sophisticated algorithm that we will ever need?There is also a question on strategies that require remembering one’s previous move. AlphaGo Zero appears to only care about the current board state and does not have a bias on what it moved previously. A human sometimes may determine its own action based on its previous move. It is a way of telegraphing actions to an opponent, but it usually is more like a head fake. Perhaps that’s a strategy that only works on humans and not machines! In short, a machine cannot see motion if it was never trained to recognize its value.This lack of memory affecting strategy may in fact be advantageous. Humans when playing a strategy game will stick to a specific strategy until an unexpected event disrupts that strategy. So long as an opponent’s moves are as expected, there is no need to change a strategy. However, as we’ve seen in the most advanced Poker playing automation, there is a distinct advantage of always calculating strategy from scratch with every move. This approach avoids telegraphing any plans and therefore a good strategy. However, misdirection is a strategy that is effective against humans but not machines that are not trained to be distracted by them. (Editors Note: Apparently previous board states are used as input to the network, so appears this lack of memory observation is incorrect).Finally, there is a question about the applicability of a turn based game to the real world. Interactions in the real world are more dynamic and continuous, furthermore the time of interaction is unbounded. Go games have a limited number of moves. Perhaps, it doesn’t matter, after all, all interactions require two parties that act and react and predicting the future will always be boxed in time.If I were to pinpoint the one pragmatic Deep Learning discovery in AlphaGo Zero then it would be the fact that Policy Iteration works surprisingly well using Deep Learning networks. We’ve have hints in previous research that incremental learning was a capability that existed. However, DeepMind has shown unequivocally that incremental learning indeed works effectively well.AlphaGo Zero appears also to have evolutionary aspects. That is, you select the best version of the newly latest trained network and you discard the previous one. There is indeed something going on here that is eluding a good explanation. The self-play is intrinsically competitive and the MCTS mechanism is an exploratory search mechanism. Without exploration, the system will eventually not be able to beat itself in play. To be effective, the system should be inclined to seek out novel strategies to avoid any stalemate. Like nature’s own evolutionary process that abhors a vacuum, AGZ seems to discover unexplored areas and somehow take advantage of these finds.One perspective to think about these systems as well as the human mind is in terms of the language that we use. Language is something that you layer more and more complex concepts on top of each other. In the case of AlphaGo Zero, it learned a new language that doesn’t have legacy baggage and it learned one that is so advanced that it is incomprehensible. Not necessarily mutually exclusive. As humans, we understand the world with concepts that originate from our embodiment with our world. That is we have evolved to understand visual-spatial, sequence, rhythm and motion. All our understanding is derived from these basic primitives. However, a machine may possibly discover a concept that may simply not be decomposable to these basic primitives.Such irony, when DeepMind trained an AI without human bias, humans discovered they didn’t understand It! This in another dimension of incomprehensibility. The concept of “incomprehensibility in the large” in that there is just too much information. Perhaps there is this other concept, that is “incomprehensibility in the small”. That there are primitive concepts that we simply are incapable of understanding. Let this one percolate in your mind for a while. For indeed it is one that is fundamentally shocking and a majority will overlook what DeepMind may have actually uncovered!.",22/10/2017,0,2.0,0.0,631.0,385.0,4.0,1.0,0.0,11.0,en
4202,Segmentasi Semantik untuk Klasifikasi Citra,Medium,Ilma Arifiany,15.0,4.0,625.0,"Segmentasi citra merupakan bagian dari proses pengolahan citra. Segmentasi citra (image segmentation) mempunyai arti membagi suatu citra menjadi wilayah-wilayah yang homogen berdasarkan kriteria keserupaan tertentu antara suatu piksel dengan piksel — piksel tetangganya, kemudian hasil dari proses segmentasi ini akan digunakan untuk proses tingkat tinggi lebih lanjut yang dapat dilakukan terhadap suatu citra, misalnya proses klasifikasi citra dan proses identifikasi objek.Segmentasi semantik adalah proses klasifikasi setiap piksel dari sebuah citra sebagai sebuah label kelas untuk memahami citra dalam tingkat per piksel. Label kelas yang yang dimaksud adalah kelas objek, seperti rumah, buku, manusia, dan lain-lain.Selain mengenali dan membedakan pengendara dan motor, segmentasi semantik juga perlu mengenali batas dari objek-objek yang dideteksi. Oleh karena itu, diperlukan dense pixelwise prediction. Sebelum deep learning, segmentasi semantik biasa dilakukan menggunakan metode klasifikasi random forest atau TextonForest. Convolutional Neural Network juga digunakan untuk segmentasi.Salah satu pendekatan deep learning awal yang populer adalah klasifikasi patch dimana setiap piksel secara terpisah diklasifikasikan ke dalam kelas menggunakan sepetak gambar di sekitarnya. Alasan utama untuk menggunakan patch adalah bahwa jaringan klasifikasi biasanya memiliki fully connected layer yang membutuhkan citra dalam ukuran pasti. Selain fully connected layer, masalah lain adalah pooling. Pooling meningkatkan bidang pandang dan mampu menggabungkan konteks namun membuang informasi lokasi objek. Segmentasi semantik membutuhkan penyelarasan peta kelas yang tepat dan dengan demikian, informasi lokasi objek perlu dipertahankan.Pada tahun 2014, Fully Convolutional Networks (FCN) oleh Long et al. dari Berkeley, mempopulerkan arsitektur CNN untuk prediksi padat tanpa fully connected layer. Metode ini memungkinkan peta segmentasi yang dihasilkan untuk gambar ukuran apa pun dan juga jauh lebih cepat dibandingkan dengan pendekatan klasifikasi patch.Ada dua tipe arsitektur yang digunakan untuk segmentasi semantik. Salah satu arsitektur yang banyak digunakan untuk segmentasi semantik adalah arsitektur Encoder-Decoder. Encoder biasanya adalah pre-trained classification network seperti VGG/ResNet diikuti oleh jaringan decoder. Tugas decoder adalah secara semantis memproyeksikan fitur citra yang dipelajari oleh encoder ke ruang piksel (resolusi lebih tinggi) untuk mendapatkan klasifikasi yang lebih padat, proses ini juga disebut sebagai upsampling. Encoder menggunakan konvolusi biasa sementara decoder menggunakan transposed convolution.Arsitektur yang kedua adalah arsitektur yang menggunakan Atrous convolution. Atrous Convolution adalah sebuah tipe konvolusi yang dirancang untuk meningkatkan receptive field output konvolusi tanpa harus memperbesar ukuran kernel. Tipe konvolusi ini sangat efektif ketika beberapa dilated convolutions yang dilebarkan ditumpuk satu demi satu. Atrous Convolution melebarkan kernel dengan menyisipkan ruang kosong di antara elemen-elemen kernel, disebut sebagai dilation rate.Model terbaru untuk segmentasi semantik untuk objek umum menggabungkan konsep encoder-decoder dan atrous convolution, model tersebut disebut model Deeplab. Deeplab menggunakan arsitektur ResNet dan Xception, namun dengan modifikasi di akhir dimana layer fully connected diubah menjadi decoder. Deeplab menyisipkan Atrous Spatial Pyramid Pooling (ASPP) di bagian akhir arsitektur. ASPP bertujuan untuk memberi model informasi secara multiscale. Untuk melakaukan itu ASPP melakukan beberapa konvolusi Atrous dengan rate dilasi yang berbeda-beda.Karena blok Atrous tidak melakukan downsampling, ASPP dijalankan dengan ukuran respon fitur yang sama. Hasilnya, ASPP dapat mempelajari fitur dari konteks multiscale menggunakan rate dilasi yang besar.Referensi:Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2018). Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4), (pp. 834–848).Chen, L. C., Papandreou, G., Schroff, F., & Adam, H. (2017). Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587.Chen, L. C., Zhu, Y., Papandreou, G., Schroff, F., & Adam, H. (2018). Encoder-decoder with atrous separable convolution for semantic image segmentation. arXiv preprint arXiv:1802.02611.Chilamkurthy, S. (2017). A 2017 Guide to Semantic Segmentation with Deep Learning. Diakses dari http://blog.qure.ai/notes/semantic-segmentation-deep-learning-reviewDumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431–3440).",03/09/2018,0,1.0,46.0,532.0,315.0,8.0,0.0,0.0,1.0,id
4203,Beginner’s Guide to RNN & LSTMs,Medium,Dinesh,46.0,13.0,2764.0,"What is RNN?Recurrent Neural Network is basically a generalization of feed-forward neural network that has an internal memory. RNNs are a special kind of neural networks that are designed to effectively deal with sequential data. This kind of data includes time series (a list of values of some parameters over a certain period of time) text documents, which can be seen as a sequence of words, or audio, which can be seen as a sequence of sound frequencies over time.RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation. For making a decision, it considers the current input and the output that it has learned from the previous input.Cells that are a function of inputs from previous time steps are also known as memory cells.Unlike feed-forward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other.Why RNN?The basic challenge of classic feed-forward neural network is that it has no memory, that is, each training example given as input to the model is treated independent of each other. In order to work with sequential data with such models — you need to show them the entire sequence in one go as one training example. This is problematic because number of words in a sentence could vary and more importantly this is not how we tend to process a sentence in our head.When we read a sentence, we read it word by word, keep the prior words / context in memory and then update our understanding based on the new words which we incrementally read to understand the whole sentence. This is the basic idea behind the RNNs — they iterate through the elements of input sequence while maintaining a internal “state”, which encodes everything which it has seen so far. The “state” of the RNN is reset when processing two different and independent sequences.Recurrent neural networks are a special type of neural network where the outputs from previous time steps are fed as input to the current time step.The way RNNs do this, is by taking the output of each neuron (input nodes are fed into a hidden layer with sigmoid or tanh activations), and feeding it back to it as an input. By doing this, it does not only receive new pieces of information in every time step, but it also adds to these new pieces of information a w̲e̲i̲g̲h̲t̲e̲d̲ ̲v̲e̲r̲s̲i̲o̲n̲ of the previous output. As you can see the hidden layer outputs are passed through a conceptual delay block to allow the input of h ᵗ⁻¹ into the hidden layer. What is the point of this? Simply, the point is that we can now model time or sequence-dependent data.This makes these neurons have a kind of “memory” of the previous inputs it has had, as they are somehow quantified by the output being fed back to the neuron.A particularly good example of this is in predicting text sequences. Consider the following text string: “A girl walked into a bar, and she said ‘Can I have a drink please?’. The bartender said ‘Certainly { }”. There are many options for what could fill in the { } symbol in the above string, for instance, “miss”, “ma’am” and so on. However, other words could also fit, such as “sir”, “Mister” etc. In order to get the correct gender of the noun, the neural network needs to “recall” that two previous words designating the likely gender (i.e. “girl” and “she”) were used. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. This type of flow of information through time (or sequence) in a recurrent neural network is shown in the diagram below, which unrolls the sequence (loop unrolled):This unrolled network shows how we can supply a stream of data (intimately related to sequences, lists and time-series data) to the recurrent neural network. For instance, first we supply the word vector for “A” to the network F — the output of the nodes in F are fed into the “next” network and also act as a stand-alone output ( h₀ ). The next network (though it’s the same network) F at time t=1 takes the next word vector for “girl” and the previous output h₀ into its hidden nodes, producing the next output h₁ and so on.NOTE: Although shown for easy explanation in Diagram, but the words themselves i.e. “A”, “girl” etc. aren’t inputted directly into the neural network. Neither are their one-hot vector type representations — rather, an embedding word vector (read Word2Vec) is used for each word.One last thing to note — the weights of the connections between time steps are shared i.e. there isn’t a different set of weights for each time step (it’s the same for all time steps BECAUSE we have the same single RNN cell looped to itself)The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:The problem with RNNs is that as time passes by and they get fed more and more new data, they start to “forget” about the previous data they have seen (vanishing gradient problem), as it gets diluted between the new data, the transformation from activation function, and the weight multiplication. This means they have a good short term memory, but a slight problem when trying to remember things that have happened a while ago (data they have seen many time steps in the past).The more time steps we have, the more chance we have of back-propagation gradients either accumulating and exploding or vanishing down to nothing.Consider the following representation of a recurrent neural network:Here, ht is the new state (current time stamp), ht₋₁is the previous state (previous time stamp) while xₜ is the current input.Where U and V are the weight matrices connecting the inputs and the recurrent outputs respectively. We then often will perform a softmax of all hₜ the outputs. Notice, however, that if we go back three time steps in our recurrent neural network, we have the following:From the above you can see, as we work our way back in time, we are essentially adding deeper and deeper layers to our network. This causes a problem — consider the gradient of the error with respect to the weight matrix U during back-propagation through time, it looks something like this:The equation above is only a rough approximation of what is going on during back-propagation through time. Each of these gradients will involve calculating the gradient of the sigmoid function. The problem with the sigmoid function occurs when the input values are such that the output is close to either 0 or 1 — at this point, the gradient is very small (saturating).For ex:- Lets say the value decreased like 0.863 →0.532 →0.356 →0.192 →0.117 →0.086 →0.023 →0.019.. you can see that there is no much change in last 3 iterations.It means that when you multiply many sigmoid gradients together you are multiplying many values which are potentially much less than zero — this leads to a vanishing gradient problem.The gradient values will exponentially shrink as it propagates through each time step. Because the gradient will become basically zero when dealing with many prior time steps, the weights won’t adjust to take into account these values, and therefore the network won’t learn any relationships separated by a long significant periods of time. So, Vanishing gradient problem results in long-term dependencies being ignored during training.You Can Visualize this Vanishing gradient problem at real time here.Hence, the RNN doesn’t learn the long-range dependencies across time steps. This makes them not much useful.We need some sort of Long term memory, which is just what LSTMs provide.Long-Short Term Memory networks or LSTMs are a variant of RNN that solve the Long term memory problem of the former.They have a more complex cell structure than a normal recurrent neuron, that allows them to better regulate how to learn or forget efficiently from the different input sources.The key to LSTMs is the cell state (cell memory), the horizontal line running through the top of the diagram, through which the information flows along and the internal mechanism called gates that can regulate the flow of information.The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions.Cell State basically encodes the information of the inputs (relevant info.) that have been observed up to that step (at every step).Cell state is a memory of the LSTM cell and hidden state (cell output) is an output of this cell.Cells do have internal cell state, often abbreviated as “c”, and cells output is what is called a “hidden state”, abbreviated as “h”. Regular RNNs have just the hidden state and no cell state. Therefore, RNNs have difficulty of accessing information from a long time ago.Note: Hidden state is an output of the LSTM cell, used for Prediction. It contains the information of previous inputs (from cell state/memory) along with current input (decided according which context is important).Hidden state (hₜ ₋ ₁) and cell input (xₜ) data is used to control what to do with memory (cell state) cₜ : to forget or to write new information.We decide what to do with memory knowing about previous cell output (hidden state) and current input and we do this using gates.Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a point-wise multiplication operation.The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”An LSTM has three of these gates, to protect and control the cell state.These gates can learn which data in a sequence is important to keep or throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions.An LSTM neuron can do this learning by incorporating a cell state and three different gates: the input gate, the forget gate and the output gate. In each time step, the cell can decide what to do with the state vector: read from it, write to it, or delete it, thanks to an explicit gating mechanism. With the input gate, the cell can decide whether to update the cell state or not. With the forget gate the cell can erase its memory, and with the output gate the cell can decide whether to make the output information available or not.LSTMs also mitigate the problems of exploding and vanishing gradients.To reduce the vanishing (and exploding) gradient problem, and therefore allow deeper networks and recurrent neural networks to perform well in practical settings, there needs to be a way to reduce the multiplication of gradients which are less than zero.The LSTM cell is a specifically designed unit of logic that will help reduce the vanishing gradient problem sufficiently to make recurrent neural networks more useful for long-term memory tasks i.e. text sequence predictions. The way it does so is by creating an internal memory state which is simply added to the processed input, which greatly reduces the multiplicative effect of small gradients. The time dependence and effects of previous inputs are controlled by an interesting concept called a forget gate, which determines which states are remembered or forgotten. Two other gates, the input gate and output gate, are also featured in LSTM cells.Here’s a brief summary of all the internal formulation and working of different gates,cell state,hidden state and current input, explained through mathematical formulas, referenced from a research paper https://arxiv.org/abs/1603.03827 (~LSTM for text classification):Let’s first have a look at LSTM cell more carefully.The data flow is from left-to-right in the diagram above, with the current input xₜ and the previous cell output hₜ₋₁ concatenated together and entering the top “data rail”. The long-term memory is usually called the cell state Ct. The looping arrows indicate recursive nature of the cell. This allows information from previous intervals to be stored within the LSTM cell. Here’s where things get interesting.Input Gate:The input gate is also called the save vector. These gates determine which information should enter the cell state / long-term memory OR which information should be saved to the cell state or should be forgotten.First, the (combined) input is squashed between -1 and 1 using a tanh activation function.This squashed input (from tanh) is then multiplied element-wise by the output of the input gate. The input gate is basically a hidden layer of sigmoid activated nodes, with weighted xₜ and input values hₜ ₋ ₁, which outputs values of between 0 and 1 and when multiplied element-wise by the input determines which inputs are switched on and off (actually, the values aren’t binary, they are a continuous values between 0 & 1). In other words, it is a kind of input filter or gate (it tells what to learn and add to the memory from current input and the context its given and also how much {sigmoid gives values between 0&1} of what to learn).Simplistic (could be wrong) view: Tanh gives the Standardized (between -1 & 1) value of the actual unscaled (combined) input vector and the sigmoid layer is the controller of what percentage (values between 0 & 1 or 0 to 100%) of what inputs should be passed on of the scaled (from tanh) values to be added to the memory considering the current and previous context.But Why tanh activation?Because the equation of the cell state is a summation between the previous cell state, sigmoid function alone will only add memory and not be able to remove/forget memory. If you can only add a float number between [0,1], that number will never be zero / turned-off / forget. This is why the input modulation gate has an tanh activation function. Tanh has a range of [-1, 1] and allows the cell state to forget certain memories.Forget Gate:The forget gate is also called the remember vector. The output of the forget gate tells the cell state which information to forget by multiplying 0 to a position in the matrix. If the output of the forget gate is 1, the information is kept in the cell state.Although initially it is randomly initialized, but it basically LEARNS What exactly to FORGET (when the current input and previous Context is given) from the memory (cell state).Output Gate:The output gate is also called the focus vector.It basically highlights, out of all the possible values from the matrix(long memory), which information should be moving forward to the next hidden state.Note: The working memory is usually called the hidden state(ht). It is basically → ht (LSTM OUTPUT) → What part of the existing memory (Ct) should be fed as context for the next round. This is analogous to the hidden state in RNN and HMM.Gates Summarized:Input gate determines the extent to which the current timestamp input should be used , the Forget gate determines the extent to which output of the previous timestamp state should be used, and the Output gate determines the output of the current timestamp.Still Unclear? Doubts? Then I’ll highly recommend you to watch this amazing video by Brandon Rohrer on RNN & LSTM (A Must Watch):ConclusionThe idea is how the learning process is based on context (memory). You forget, you learn and you extract a part of it for the next round, but in next round you again repeat the same process.Basically, we are trying to mimic how human brain tries to learn things through LSTM internal gate mechanism (& this may not be necessarily true, we’re just trying different approaches)GRU — Gated Recurrent UnitThe GRU introduced in 2014 by Kyunghyun Cho et al was a simplified version of LSTM with just two gates instead of 3 and with a far fewer parameters vs LSTM.GRU have been shown to have better performance on certain tasks and smaller datasets, but LSTM tend to outperform GRU in general.There were some additional variants like bidirectional LSTM which don’t just process text left to right, but also do it right to left to give additional performance boost.References:1) RNN: https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/2) https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e8603) https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf09124) http://colah.github.io/posts/2015-08-Understanding-LSTMs/5) https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9& many more …WHAT DOES THE CONTEXT CONTAIN:",05/12/2019,0,71.0,64.0,661.0,311.0,13.0,1.0,0.0,8.0,en
4204,Puppyslugs ‘R Us: Part 1,Medium,Boris Anthony,958.0,6.0,1209.0,"In “Puppyslugs ‘R Us: Part 0”, I started out quite cheekily on a topic I hope to explore here in a bit more serious detail.I am going to start with the recent Google “DeepDream” release and the so-called Puppyslug images you’ve likely encountered, explaining roughly what those are and how they come to be. I will connect that to AI and algorithms in general and then move specifically to how they already appear in your everyday mobile experience. From there we can paint a picture of what’s in store for us, and why I say… the Puppyslugs are Us. I’ll conclude by setting up Part 2, and how all of this lands squarely in the lap of Design to deal with.Puppyslugs. Quick background:About two months ago (early June 2015), Google Researchers start showing off some algorithmically generated visuals which they originally label “Inceptionism”, after the DiCaprio movie of a few years ago, Inception. The algorithms in question use a technology known as Artificial Neural Networks (ANN). You can think of ANN as a sort of database, or “bunch of memories” the structure and mechanism of which is inspired by what we currently know of how our biological brains work.To make an Artificial Neural Network — a.k.a. “to train a model” — you take a bunch of memories (data), run them through the generator and *ding* you have a self-contained understanding-of-the-world based entirely on the memories you fed it…Hold that thought… let it dissolve on your tongue… ;pOk. Two weeks later, the same Researchers open source and release the software code they used in their work. Now because “training a model” requires alot of data and processing, and ostensibly the buzz they were getting around their work had to do with generating funky images, these researchers also made available one or two such “models”, or Neural Networks, or “artificial minds.”One of these was generated entirely out of many many pictures of puppies (and apparently some other animals).What came out of this is what we now call Puppyslugs.How these images are generated is important. Not technically how, but conceptually how.If you ask a person a question, assuming they understood your question, they will answer you based on their knowledge. More specifically, they will formulate an answer to your question out of their Memory, the Situation the question was asked in, and what they believe may be Contextually Relevant in that Situation.If that person’s memory, knowledge and experience is 100% entirely made up of photos of dogs (and places), and you ask them to describe this photo of me:well… you get this:Let me illustrate that.If, using spoken language, you ask a person about me…If you ask a Neural Network trained on photos of places and puppies about [how it understands a photo of] me:The computer spits out what it comprehends based on what it “knows.”Now so far, most of the coverage (that I have seen anyways) of the DeepDream and PuppySlug stuff mainly highlights its “nightmarish imagery” producing capabilities. But you may have already noticed that I am taking us along another route. Something I want you to keep in mind is this: these tools were developed by Google Researchers researching Artificial Intelligence & Deep Learning algorithms and processes as a way to visualize what a given Neural Network “knows and understands.” They did not do this “to make trippy visuals.”(Extra: This is not part of my point per se, but… “Why Google’s new patent applications are alarming”)Google is not alone here. Everyone’s on this Quest for the Grail: Facebook, Microsoft, Amazon, IBM, every startup who’s pitch deck contains the word “graph”… Also, this is not new. More likely than not, you’ve been exposed to “user experiences” backed by some form of AI before: automated call service systems, spam bots… and “he who shall remain unnamed *cough*Clippy*cough*.”More recently, we’ve gotten to know Apple’s Siri, Microsoft’s Cortana, Amazon’s Echo, and of course Google Now.(There are two threads leading in and out of those developments and they are called “Conversational UI” (2010, 2013, 2015) and “AI is the new UI” (c.f. “Intelligent UI”). I will get back to these two in the next instalment of this series, because they involve design.)The question to ask now is “Why? Why are these companies running after this?” There are about a hundred layers of reasons, with the higher levels being “to make money duh” but that’s too broad. Further down the “why stack”, we might arrive at “in order to give the user what they want when they want it, we need to predict what they might want before they ask for it.”As my former colleague Sarah Ferber put it, the service needs to be able to answer “how can I be happy?” without the user saying that or typing it in or punching a button that says “HAPPY! NOW!”(I’ve heard people refer to their smartphones as “joy killers” for precisely failing at this. Every. Time. You. Look at it.)So, ok. Remember I asked you to hold that thought earlier? Let’s bring that back:To make an Artificial Neural Network — a.k.a. “to train a model” — you take a bunch of memories (data), run them through the generator and *ding* you have a self-contained understanding-of-the-world based entirely on the memories you fed it…There is a line of thought, Continuity of Consciousness in Identity, that holds that what makes you, you, aside the meat and bones, is your personality. What makes that personality, what makes you “an individual”, is the accumulation of experiences that have shaped you, all of them somehow stored in your memory, and how they manifest themselves relevantly in a given situation.Now ask yourself: “Who is the one person most likely to know exactly how to make me happy right now?”Pro tip: It’s you! With all your memories and experience and preferences and behaviors and quirks and history and comments and likes and favs and shares…Guess who might be even better? An omniscient, omnipotent version of you! A being that knows everything you know… AND knows everything and can reach anything on the Internet.Getting a bit carried away there. Let me put my point out clearly:If someone has a record of everything you say and do on the Internet, they can create, using Artificial Neural Networks “AI” versions of you who, while keeping an eye on you, can also go and fetch information, products and services for you as you appear to need them, without your having to ask for them.While it most likely isn’t quite the case yet, very soon, very possibly, when you talk to Google Now, Cortana, Siri or others… it won’t be some random generalized AI you’ll be talking to. No. It’ll be yourself.Your own… puppyslug… self.(The point is, you would be the basis of your own highly networked serendipity engine, a ready and aware network bot agent … mapping out your possibility space as it appears on the event horizon…)I’ll let you chew on that for now. In the next Part, I’ll pick up the “this is design’s problem” thread, as well as explore some of the obstacles still preventing this from happening.Until then, please enjoy these classic Deep Learning Advertising Sales Team classics:p.s.: much inspiration for this craziness came from many long conversations with my friendly neighbourhood @samim. :)",06/08/2015,0,25.0,27.0,707.0,717.0,7.0,0.0,0.0,17.0,en
4205,Understanding Deep Self-attention Mechanism in Convolution Neural Networks,AI Salon,Shuchen Du,204.0,4.0,696.0,"Convolution neural networks (CNN) are broadly used in deep learning and computer vision algorithms. Even though many CNN-based algorithms meet industry standards and can be embedded in commercial products, the standard CNN algorithm is still limited and can be improved in many aspects. This post discusses semantic segmentation and encoder-decoder architecture as examples clarify the limitations and why self-attention mechanism can help mitigate the problem with reason.Encoder-decoder architecture (Fig. 1) is standard method in many computer vision tasks, especially pixel-level prediction tasks such as semantic segmentation, depth prediction and some GAN-related image generators. In an encoder-decoder network, an input image is convoluted, relued and pooled to a latent vector and then recovered to an output image with the same size as the input. The architecture is symmetric and comprised with well-designed convolution blocks. Due to its simplicity and accuracy, the architecture is widely used.However, if we look deep into the calculation of convolution (Fig. 2), the limitations of encoder-decoder architecture surface. For example, in a 3x3 convolution, the convolution filter has 9 pixels and the value of a destination pixel is calculated with only referring to itself and the surrounding 8 pixels. This means that only local information can be leveraged to calculate a destination pixel, which may bring some bias, as global information is not seen. There are also some naive ways to mitigate the problem: using larger convolution filters or deeper networks with more convolution layers. However, the computational overhead gets heavier and the results are not improved remarkably.Variance and covariance are both important conceptions in statistics and machine learning. They are defined for random variables. As the name implies, variance describes the deviation of a single random variable from its mean, while covariance describes the similarity of two random variables. If the distributions of two random variables are similar, their covariance is large. Otherwise, their covariance is small. If we consider each pixel in the feature map as a random variable and calculate all the pairing covariances between all the pixels, we can enhance or weaken each predicted pixel value according to its similarity with each other pixel in the image. The similar pixels will be leveraged during training and prediction while the dissimilar ones will be ignored. This mechanism is called self-attention.As a supplement, the covariance is calculated as Equ. 1.In order to implement global reference for each pixel-level prediction, Wang et al. proposed self-attention mechanism in CNN (Fig. 3). Their approach is based on covariance between the predicted pixel and every other pixel, in which each pixel is considered as a random variable. The attended target pixel is just a weighted summation of all the pixel values, where the weights are the relationship between each pixel and the target pixel.If we reduce the original Fig. 3 to the simplest form as Fig. 4, we can easily understand the role covariance plays in the mechanism. Firstly, we have input feature map X with height H and width W. Then we reshape X into three 1-dimensional vectors A, B and C, multiplying A and B to get the covariance matrix with size HWxHW. Finally, we multiply the covariance matrix with C, getting D and reshape it to the output feature map Y with a Resnet connection from input X. Here, every item in D is a weighted sum of input X, with the weight being the covariance between the item and each other item.By leveraging self-attention mechanism, we can realize global reference during model training and prediction. The model will be more reasonable with a good bias-variance trade-off.SAGAN embeds self-attention mechanism into GAN framework. It can generate images by referencing globally rather than from local regions. In Fig. 5, the left image of each row shows the sampled query points with colors, and the other five images show the corresponding attention area of each query point. We can see the attention area spans broadly for background query points like the sky and reed bush, while focuses locally for foreground points like bear eyes and bird legs.Non-local Neural Networks, Wang et al., CVPR 2018Self-Attention Generative Adversarial Networks, Zhang et al. ICML 2019Dual Attention Network for Scene Segmentation, Fu et al., CVPR 2019Wikipedia, https://en.wikipedia.org/wiki/Covariance_matrixZhihu, https://zhuanlan.zhihu.com/p/37609917",08/01/2020,0,2.0,0.0,696.0,382.0,7.0,0.0,0.0,4.0,en
4206,Transfer learning and Image classification using Keras on Kaggle kernels.,Towards Data Science,Rising Odegua,965.0,11.0,1481.0,"In my last post, we trained a convnet to differentiate dogs from cats. We trained the convnet from scratch and got an accuracy of about 80%. Not bad for a model trained on very little dataset (4000 images).But in real world/production scenarios, our model is actually under-performing.Although we suggested tuning some hyperparameters — epochs, learning rates, input size, network depth, backpropagation algorithms e.t.c — to see if we could increase our accuracy.Well, I did try…And truth is, after tuning, re-tuning, not-tuning , my accuracy wouldn’t go above 90% and at a point It was useless.Of course having more data would have helped our model; But remember we’re working with a small dataset, a common problem in the field of deep learning.But alas! there is a way….Transfer learning walks in….But then you ask, what is Transfer learning?Well, TL (Transfer learning) is a popular training technique used in deep learning; where models that have been trained for a task are reused as base/starting point for another model.To train an Image classifier that will achieve near or above human level accuracy on Image classification, we’ll need massive amount of data, large compute power, and lots of time on our hands. This I’m sure most of us don’t have.Knowing this would be a problem for people with little or no resources, some smart researchers built models, trained on large image datasets like ImageNet, COCO, Open Images, and decided to share their models to the general public for reuse.This means you should never have to train an Image classifier from scratch again, unless you have a very, very large dataset different from the ones above or you want to be an hero or thanos.I know you have questions, like…Well Transfer learning works for Image classification problems because Neural Networks learn in an increasingly complex way. i.e The deeper you go down the network the more image specific features are learnt.Let’s build some intuition to understand this better. In a neural network trying to detect faces,we notice that the network learns to detect edges in the first layer, some basic shapes in the second and complex features as it goes deeper.So the idea here is that all Images have shapes and edges and we can only identify differences between them when we start extracting higher level features like-say nose in a face or tires in a car. Only then can we say, okay; this is a person, because it has a nose and this is an automobile because it has a tires.The take-away here is that the earlier layers of a neural network will always detect the same basic shapes and edges that are present in both the picture of a car and a person.Now, taking this intuition to our problem of differentiating dogs from cats, it means we can use models that have been trained on huge dataset containing different types of animals. This works because these models have learnt already the basic shape and structure of animals and therefore all we need to do, is teach it (model) the high level features of our new images.All I’m trying to say is that we need a network already trained on a large image dataset like ImageNet (contains about 1.4 million labeled images and 1000 different categories including animals and everyday objects). Since this model already knows how classify different animals, then we can use this existing knowledge to quickly train a new classifier to identify our specific classes (cats and dogs).I mean a person who can boil eggs should know how to boil just water right?Now that we have an understanding/intuition of what Transfer Learning is, let’s talk about pretrained networks.There are different variants of pretrained networks each with its own architecture, speed, size, advantages and disadvantages.Keras comes prepackaged with many types of these pretrained models. Some of them are:and many more. Detailed explanation of some of these architectures can be found here.We’ll be using the InceptionResNetV2 in this tutorial, feel free to try other models.The InceptionResNetV2 is a recent architecture from the INCEPTION family. It works really well and is super fast for many reasons, but for the sake of brevity, we’ll leave the details and stick to just using it in this post.If you’re interested in the details of how the INCEPTION model works then go here.With the not-so-brief introduction out of the way, let’s get down to actual coding.We’ll be using almost the same code from our first Notebook, the difference will be pretty simple and straightforward, as Keras makes it easy to call pretrained model.If you followed my previous post and already have a kernel on kaggle, then simply fork your Notebook to create a new version. We’ll be editing this version.A fork of your previous notebook is created for you as shown belowNow, run the code blocks from the start one after the other until you get to the cell where we created our Keras model, as shown below.Click the + button with an arrow pointing up to create a new code cell on top of this current one.Now, let’s call our pretrained model…5. Here we specify our input dimension.Click shift+Enter to run the code block.If you get this error when you run the code, then your internet access on Kaggle kernels is blocked.To activate it, open your settings menu, scroll down and click on internet and select Internet connected. Your kernel automatically refreshes. So you have to run every cell from the top again, until you get to the current cell.Rerunning the code downloads the pretrained model from the keras repository on github.We can call the .summary( ) function on the model we downloaded to see its architecture and number of parameters. You notice a whooping 54 million plus parameters. This is massive and we definitely can not train it from scratch. But thanks to Transfer learning we can simply re-use it without training.Next, we create our fully connected layers (classifier) which we add on-top of the model we downloaded. This is the classifier we are going to train. I.e after connecting the InceptionResNetV2 to our classifier, we will tell keras to train only our classifier and freeze the InceptionResNetV2 model.Hold Shift+Enter to run your code.Next, let’s preview our architecture:We can see that our parameters has increased from roughly 54 million to almost 58 million, meaning our classifier has about 3 million parameters.Now we’re going freeze the conv_base and train only our own.Almost done, just some minor changes and we can start training our model.First little change is to increase our learning rate slightly from 0.0001 (1e-5) in our last model to 0.0002(2e-5). I decided to use 0.0002 after some experimentation and it kinda worked better. (you can do some more tuning here)Next, run all the cells below the model.compile block until you get to the cell where we called fit on our model. Here we’ll change one last parameter which is the epoch size.We reduce the epoch size to 20. The reason for this will be clearer when we plot accuracy and loss graphs later.Note: I decided to use 20 after trying different numbers. This is what we call Hyperparameter tuning in deep learning.Well, before I could get some water, my model finished training. So let’s evaluate its performance.Picture showing the power of Transfer Learning.We clearly see that we have achieved an accuracy of about 96% in just 20 epochs. Super fast and accurate.If the dogs vs cats competition weren’t closed and we made predictions with this model, we would definitely be among the top if not the first. And remember, we used just 4000 images from a total of about 25,000.What happens when we use all 25000 images for training combined with the technique ( Transfer learning) we just learnt?Well, a very wise scientist once said…A not-too-fancy algorithm with enough data would certainly do better than a fancy algorithm with little data.And it has been proven true!Okay, we’ve been talking numbers for a while now, let’s see some visuals…Without changing your plotting code, run the cell block to make some accuracy and loss plots.And we get these plots below…So what can we read of this plot?Well, we can clearly see that our validation accuracy starts doing well even from the beginning and then plateaus out after just a few epochs. Now you know why I decreased my epoch size from 64 to 20.Finally, let’s see some predictions. We are going to use the same prediction code. Just run the code block.After running mine, I get the prediction for 10 images as shown below…And our classifier got a 10 out of 10. Pretty nice and easy right?Well, This is it. This is where I stop typing and leave you to go harness the power of Transfer learning.So, Happy coding…Link to this notebook on Kaggle.Link to this notebooks on Github.Some amazing post and write-ups I referenced.Questions, comments and contributions are always welcome.Connect with me on twitter.Connect with me on Instagram.",02/11/2018,0,94.0,19.0,811.0,367.0,31.0,7.0,0.0,29.0,en
4207,Understanding the StyleGAN and StyleGAN2 Architecture,Analytics Vidhya,Prem Chandra Singh,12.0,5.0,844.0,"The article contains the introduction of StyleGAN and StyleGAN2 architecture which will give you an idea. It may help you to start with StyleGAN. You will find some metric or the operations name which you don’t know, to gain a deep understanding of StyleGAN and StyleGAN2 you can go through the paper whose link is provided in the resources section.Let’s start with the StyleGAN and then we move towards StyleGAN 2.The major changes they have done in the Generator part of the “Progressive Growing of GANs” architecture. Below you can see both the traditional and the style-based generator (new one or StyleGAN network) network.In the traditional network, latent vectors directly pass into the block just after the normalization whereas in the StyleGAN network latent vectors after normalization pass through the mapping network (layer of 8 fully connected networks) then the outputs are transformed (A stands for the affine transformation which is the combination of linear transformation and translation) and passed to the blocks and get added with the noise B after the instance normalization (AdaIN i.e Adaptive instance normalization).Above, you can see the formula of AdaIN where x comes from the conv net and y comes from the left side network. Clearly, seen in the equation that after the normalization of x, y(s, i) is used for the scaling and y(b, i) is used for the transformation as a bias. Below you can see the StyleGAN in a simple form.In the official paper, you will see the results on CelebA and FF(Flickr Faces) high-quality datasets where they shown the FIDs (Frechet inception distances) score using 50K randomly drawn images from the training set. Below you can see the resultsThey started from baseline configuration A (Progressive GAN), and after adding bilinear up/downsampling, long training they see improvements. Then added mapping network and AdaIN operations or in config D they removed the traditional inputs from the synthesis network and replaced them with 4x4x512 constant tensor.We can see the improvements in FIDs value over the traditional generator (B) and enabling Mixing regularization (this operation also called style mixing) gives more control over the style and high-level aspects like pose, hairstyle, eyeglasses, etc.So, this is a simple introduction to the StyleGAN architecture and now let’s see what improvements have been made in StyleGAN 2 and understand its architecture.In the below image, you can see the defects or blurry portion in the generated image which comes from the starting 64x64 resolution. This is the major reason behind the redesigning of the generator with that the quality of generated images also improved.So, let’s see what changes in the architecture of the network improves the performance of generated image step by step. Below you can see the improvements in architecturePart A is the same StyleGAN architecture and Part B shows the detailed view of the StyleGAN architecture. In Part C, they replaced the AdaIN (Adaptive Instance Normalization) with the Modulation (or the scaling of the factors) and Normalization. Below you can see the modulation (left side) and normalization (right side) equation.Also, in Part C they shifted the addition of noise and bias outside of the block. Finally, in Part D you can see the weights are adjusted with the style and the normalization is replaced with a “demodulation” operation, combined operations are called “Weight Demodulation”. See the formula below.You can see that this equation seems the combination of the above two modulation and normalization equations (epsilon is a small constant value, used to prevent numerical issues like division by zero). Results can be seen on the below outputs, after replacing the normalization with demodulation removes the droplet-like artifacts.Now, we have seen the improvements in the form of generated images. Let’s see the improvements measured using metrics like FID, Perceptual path length (Introduced in the StyleGAN paper, lower the PPL better the generated image), etc.On the above table, can be seen that the improvements on the configurations after applying different methods. Path length regularization and Lazy regularization are used to keep the PPL score low show that the generated images are more clear or smooth.Progressive growing of the network generates high-quality images but it also causes the characteristic artifacts (or phase artifacts) i.e. eye and teeth of the person seems stuck at one place wherever the face of the person moves, it was shown in the official StyleGAN2 video (video link is attached in the resources section below).To solve this issue they tried other connections (skip connections, residual nets, etc) on the generator and discriminator network and saw that skip connection work best for the generator and residual nets give better results on the discriminator.Above table shows the results with the combination of each type of connection. Also, on the above main result table configuration E and F shows results on the skip connection type generator and residual discriminator network.In the below figure, highlighted part in b is a generator and in c is a discriminator without progressive growing.Almost all the images are taken from the official paper of StyleGAN and StyleGAN 2 whose links are given below in the resource section.",12/02/2021,0,2.0,0.0,657.0,293.0,13.0,1.0,0.0,6.0,en
4208,mAP (mean Average Precision) for Object Detection,Medium,Jonathan Hui,27000.0,7.0,1025.0,"AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision computes the average precision value for recall value over 0 to 1. It sounds complicated but actually pretty simple as we illustrate it with an example. But before that, we will do a quick recap on precision, recall, and IoU first.Precision & recallPrecision measures how accurate is your predictions. i.e. the percentage of your predictions are correct.Recall measures how good you find all the positives. For example, we can find 80% of the possible positive cases in our top K predictions.Here are their mathematical definitions:For example, in the testing for cancer:IoU (Intersection over union)IoU measures the overlap between 2 boundaries. We use that to measure how much our predicted boundary overlaps with the ground truth (the real object boundary). In some datasets, we predefine an IoU threshold (say 0.5) in classifying whether the prediction is a true positive or a false positive.Let’s create an over-simplified example in demonstrating the calculation of the average precision. In this example, the whole dataset contains 5 apples only. We collect all the predictions made for apples in all the images and rank it in descending order according to the predicted confidence level. The second column indicates whether the prediction is correct or not. In this example, the prediction is correct if IoU ≥ 0.5.Let’s take the row with rank #3 and demonstrate how precision and recall are calculated first.Precision is the proportion of TP = 2/3 = 0.67.Recall is the proportion of TP out of the possible positives = 2/5 = 0.4.Recall values increase as we go down the prediction ranking. However, precision has a zigzag pattern — it goes down with false positives and goes up again with true positives.Let’s plot the precision against the recall value to see this zig-zag pattern.The general definition for the Average Precision (AP) is finding the area under the precision-recall curve above.Precision and recall are always between 0 and 1. Therefore, AP falls within 0 and 1 also. Before calculating AP for the object detection, we often smooth out the zigzag pattern first.Graphically, at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.So the orange line is transformed into the green lines and the curve will decrease monotonically instead of the zigzag pattern. The calculated AP value will be less suspectable to small variations in the ranking. Mathematically, we replace the precision value for recall ȓ with the maximum precision for any recall ≥ ȓ.PASCAL VOC is a popular dataset for object detection. For the PASCAL VOC challenge, a prediction is positive if IoU ≥ 0.5. Also, if multiple detections of the same object are detected, it counts the first one as a positive while the rest as negatives.In Pascal VOC2008, an average for the 11-point interpolated AP is calculated.First, we divide the recall value from 0 to 1.0 into 11 points — 0, 0.1, 0.2, …, 0.9 and 1.0. Next, we compute the average of maximum precision value for these 11 recall values.In our example, AP = (5 × 1.0 + 4 × 0.57 + 2 × 0.5)/11Here are the more precise mathematical definitions.When APᵣ turns extremely small, we can assume the remaining terms to be zero. i.e. we don’t necessarily make predictions until the recall reaches 100%. If the possible maximum precision levels drop to a negligible level, we can stop. For 20 different classes in PASCAL VOC, we compute an AP for every class and also provide an average for those 20 AP results.According to the original researcher, the intention of using 11 interpolated point in calculating AP isThe intention in interpolating the precision/recall curve in this way is to reduce the impact of the “wiggles” in the precision/recall curve, caused by small variations in the ranking of examples.However, this interpolated method is an approximation which suffers two issues. It is less precise. Second, it lost the capability in measuring the difference for methods with low AP. Therefore, a different AP calculation is adopted after 2008 for PASCAL VOC.For later Pascal VOC competitions, VOC2010–2012 samples the curve at all unique recall values (r₁, r₂, …), whenever the maximum precision value drops. With this change, we are measuring the exact area under the precision-recall curve after the zigzags are removed.No approximation or interpolation is needed. Instead of sampling 11 points, we sample p(rᵢ) whenever it drops and computes AP as the sum of the rectangular blocks.This definition is called the Area Under Curve (AUC). As shown below, as the interpolated points do not cover where the precision drops, both methods will diverge.Latest research papers tend to give results for the COCO dataset only. In COCO mAP, a 101-point interpolated AP definition is used in the calculation. For COCO, AP is the average over multiple IoU (the minimum IoU to consider a positive match). AP@[.5:.95] corresponds to the average AP for IoU from 0.5 to 0.95 with a step size of 0.05. For the COCO competition, AP is the average over 10 IoU levels on 80 categories (AP@[.50:.05:.95]: start from 0.5 to 0.95 with a step size of 0.05). The following are some other metrics collected for the COCO dataset.And, this is the AP result for the YOLOv3 detector.In the figure above, AP@.75 means the AP with IoU=0.75.mAP (mean average precision) is the average of AP. In some context, we compute the AP for each class and average them. But in some context, they mean the same thing. For example, under the COCO context, there is no difference between AP and mAP. Here is the direct quote from COCO:AP is averaged over all categories. Traditionally, this is called “mean average precision” (mAP). We make no distinction between AP and mAP (and likewise AR and mAR) and assume the difference is clear from context.In ImageNet, the AUC method is used. So even all of them follow the same principle in measurement AP, the exact calculation may vary according to the datasets. Fortunately, development kits are available in calculating this metric.medium.commedium.commedium.comThe PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kitscikit-learn precision-recall",07/03/2018,0,12.0,6.0,1212.0,435.0,18.0,0.0,0.0,7.0,en
4209,GUIDE: POP! Slots Casino — Level 27 & 34 ($13+) [EASY],Medium,EarnSkins,47.0,4.0,844.0,"Guide made for EarnSkins users, by JaxStart the offer now at www.earnskins.gg to earn some side money!Use referral code ‘wolf’ to get yourself a free 50 points to start with.UPDATE: The offer is currently to finish Level 34, this strategy still works, tested and confirmed, however it takes a bit longer now.Video Guide: https://youtu.be/AIEBGMRPe7IThe POP! Slots offer is a casino/slots based app offer that exists for both iOS and Android phones. The offer required me to reach level 27 in the app, which was easily obtainable and you can automate it really easy making the time spent actually doing anything is extremely low.There’s three different types of this offer that I’m aware of. One requires you to reach level 27, one level 26 while the last one requires you to reach level 18. I’d recommend doing the level 27 as it pays higher and still takes a small amount of time. I reached it in less than 4 hours.Also be aware this guide is based on what I did to complete the offer, there might be other and better ways to optimize it further and I also can’t guarantee this method will work for you as slots offers are somewhat random, based on what rewards you receive from cases, lucky wheels and other bonuses. And of course what you win from playing the slots will also factor in.Offer name: POP! Slots Casino Slot GamesOfferwalls: AdGem & AdGate MediaOS: iOS & AndroidCountries that has the offer (that I’m aware of): US, AU, CA, DE, FR, UK, NL, NZ, IN, AT, BE, CO, DK, NO, SE, IS, JP, PL, SG, SK, CH, TW.Amount: Depends on offerwall, OS & country. I got $10Time Limit: 7 Days on AdGate Media.Time Spent: Hands on ~20 minutes. ~4 Hours idle.This guide includes gifs to further help along with the explanations, if you’re confused about something, just hit the Imgur boxes to open a gif and see it!I recommend doing the offer on a 2nd decive that’s not your main phone if possible, something that you can leave running.Go ahead and start the game right away, the game is pretty straight forward. After you’ve made your character, you’re gonna head straight into the MGM Grand and inside there you’re gonna play the slots game ‘MGM Grand Mega Stars’.Next up you’re going to start auto-playing on this slot machine with the lowest bet amount possible. We’re going to want to do this until we hit level 5 or until we run out of chips. It doesn’t take very long!After you’ve hit level 5 or you’ve ran out of chips, we’re going to go ahead and collect more chips and prepare for the final stretch to level 27. You’re going to force close the app (if you don’t know how, google it!) and visit this website: https://pop-the-slots.com/ On this site there are a ton of free coupons for chips for some reason. You can easily claim 30 million chips by claiming it all. Make sure you force close the app between every coupon you claim as it doesn’t seem to register if you don’t (for me at least). Now that we have 30 Million chips AND we’re level 5, we’re going to start playing on a Slots machine that you just unlocked on level 5. The one named ‘Fire and lightning’, this one has great bonus games and offers turbo on it’s auto-spin mode. Go ahead and choose it, set the bet amount to around 25k and choose the turbo auto-spin.When you reach around level 11 you’re going to want to bet a bit higher to keep earning XP. Bet 67 000–100 000 consistently until you reach the target level. Occasionally move out of your seat on the Fire and Lightning game to see if there’s another ‘hot seat’ you can go to instead.Don’t worry if you run out of chips after doing what’s considered a ‘safe’ strategy, there’s more coupons being posted every day on the website I mentioned.That’s it, you should be hitting your level pretty quick! The game also has ‘Rewards’ that you can redeem from some sort of special chips. Not sure if this is legit or not, but if you live in Vegas or plan to visit Vegas, then you might as well try to claim a reward in the app.No. There’s absolutely no need to spend money to complete this offer as long as they keep giving coupons on the website mentioned.Yes, if you’re willing to risk losing more chips then you can speed it up by placing even higher bets. I always recommend to just safe it, you will make in the end and it’s almost entirely AFK.Hope this guide helped you! Make sure to follow our blog to get more guides and tips on how to earn more side money! Also make sure you use the best site (our site) that has very high payouts! Come visit: www.earnskins.ggLeave a comment and let me know if you completed it, what you bought for the money you got and if you claimed any of the app rewards!",08/07/2020,0,8.0,7.0,1400.0,1049.0,1.0,1.0,0.0,7.0,en
4210,Seq2seq pay Attention to Self Attention: Part 2,Medium,Gene Su,436.0,13.0,2400.0,"Part 1 https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aadChinese Version https://medium.com/%40bgg/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4We have talked about Seq2seq and Attention model in the first part. In this part, I will be focusing on Self attention, proposed by Google in the paper “Attention is all you need”. Self attention is the concept of “The transformer”model, which outperforms the attention model in various tasks. Two main concepts of the “transformer” model are “self attention” and “multi-head”.The biggest advantage comes from how The Transformer lends itself to parallelization and self attention.Hope you enjoy it.I will use the figure in part 1 as a quick overview. We now know how attention model works. The disadvantage of attention model is that it cannot be parallelize and ignore the attention information inside source sentence and target sentence. In 2017, Self Attention was born to resolve this problem.Self attention is proposed in the paper “Attention is all you need”, which is the core concept of the model “The transformer”. We can think of the “The transformer” as a black box. Once sending source sentence inside, we will get the output sentence.Especially, “The transformer” abandoned the architecture of RNN and CNN.The transformer consists of two parts. Just as usual, one is the encoder and the other is the decoder. However, The encoder is a stack of encoders (the paper stacks six of them on top of each other). The decoder is a stack of decoders of the same number. These mechanisms are different with attention model.Before we dive into the transformer, some concepts of attention model should be renewed. In attention model, the Encoder generates <h1,h2,h….hm> from the source sentence <X1,X2,X3…Xm>. Context vector c_{i} is a sum of hidden states of the input sequence, weighted by attention scores α. With context vector and hidden state, we can then calculate output sentence<y1…yn>.Let’s translate that in another words.Input word in source sentence are pairs of <(Address)Key, (element)Value> and output word in target sentence is Query — Figure(4 left). We can then turn the calculation of context vector into another interpretation by Key, Query and Value — Figure(4 right). Through the calculation of similarity between Query and each Key, we can get the attention score of the Value, corresponding to the Key. Attention score is the importance of a input word. We then multiply each value vector by the attention score and sum up the weighted value vector, which is Attention/context vector.In my opinion, this is the hardest part when reading paper from attention model to self attention for this renewal translation not being explained in the paper. You can also read the paper Key-Value Memory Networks for Directly Reading Documents, where the idea of key and value appeared.We can now reinterpret the decoder formula in attention model. Calculating attention vector comes mainly in three steps. First, we take the query and each key and compute the similarity between the two to obtain a score e_{ij}. We have met 3 kinds of similarity functions in part 1 figure(11), though we used dot in the end. The second step is to get attention score a_{i} by using a softmax function to normalize these scores, and finally to weight these weights in conjunction with the corresponding values and obtain the attention/context vector, c_{i}.In current NLP work, the key and value are usually came to the same thing, therefore key=value.Now we have the concept of Query, Key and Value, we can go through “the transformer”.The transformer uses a particular form of attention called the “Scaled Dot-Product Attention” which is computed according to the following equation in figure(6). Compared to the standard form of attention described in the attention model, Scaled Dot-Product Attention is a type of attention that utilizes Scaled Dot-Product with division \sqrt_{d_{k}}to calculate similarity. We can find that the attention idea remains the same with attention model, but differs in the addition of division \sqrt_{d_{k}}. The difference is that it has a division \sqrt_{d_{k}} for adjustment that prevents the inner product from becoming too large — figure(6). Also, Attention(q_{t}, K,V) are the same as calculated in attention model from a microcosmic perspective.In other words, The transformer model is similar to attention model, except for their description of terms.The transformer contains three main attention. One is the encoder self attention in encoder. Another is the decoder self attention in decoder. The other is the encoder-decoder attention, which is similar to the concept of attention model. Let’s start with encoder/decoder self attention from their corresponding encoder/decoder.The encoder is the left part of the transformer model. In this paper, the encoder is a stack of encoders (the paper stacks six of them on top of each other). Inside the encoder is multi-head encoder self attention. We will talk about multi-head later.We will take a look from the microcosmic perspective by vectors Attention(q_{t}, K, V), then proceed to look at how it’s actually implemented with matrices.The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word in “Are you very big?”). Then we multiply the embeddings by three different matrices to create a Query vector, a Key vector, and a Value vector for each word. In this paper, outputs of dimension d_{model}=512.The second step in calculating self-attention is to calculate a score <q_{t}, k_{s}> by taking the dot product of the query vector with the key vector of the respective word we’re scoring, which is similar to e_{ij} in attention model. Say we’re calculating the self-attention for the first word in this example, “Are”. We need to score each word of the input sentence such as “you”, ‘very’, ‘big?’ against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1 (“Are vs Are”). The second score would be the dot product of q1 and k2(“Are vs you”).The third step is to divide the scores by \sqrt_{d_{k}} (the paper assumes d_{k} = 64.), then pass the result into exponential with division 1/Z. The result is attention/softmax score. Interestingly, we can turn this structure into softmax description where Z equals the sum of exponential — Figure(9). This attention score determines how much each word will be expressed at this position, just like how attention model did. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.The final step is to multiply each value vector by the attention score, then sum up the weighted value vectors (z_{i}). This produces the output of the self-attention layer at this position (for the first word), similar to context vector in attention model — figure(9).That concludes the self-attention calculation. The resulting vector is the one we can send along to the feed-forward neural network. In the actual implementation, this calculation is done in matrix form — figure(10).If we only computed a single attention weighted sum of the values, it would be hard to capture diverse representations of the input. To improve the performance of the model, instead of doing a single attention function with d_{model}-dimensional keys, values and queries, authors found it beneficial to linearly project the queries, keys and values h times with different linear projections to d_{q}, d_{k} and d_{v} dimensions, respectively. In the paper, d_{k}=d_{v}=d_{model}/h=64.Also, the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder. Each set is used to project the input embeddings into a different representation subspace. If we do the same self-attention calculation we described before, we end up with eight different Z matrices. However, the feed-forward layer is not expecting eight matrices. We need to concatenate them and condense these eight down into a single matrix by multiply them with an additional weights matrix WO — figure(12).One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, feed-forward networks) in each encoder has a residual connection followed by a layer-normalization.A residual connection is basically just taking the input and adding it to the output of the sub-network, making training deep networks easier in the field of computer vision. Layer normalization is a normalization method in deep learning that is similar to batch normalization. In layer normalization, the statistics are computed across each feature and are independent of other examples. The independence between inputs means that each input has a different normalization operation.In encoder and decoder the attention sublayers is being processed by a fully connected FFN. It is applied to each position separately and identically meaning two linear transformations and a ReLU. For example, if input sequence = <x1,x2…xm>, total size of position are m .Linear transformations are the same for each position, but use different parameters from layer to layer. It works similarly to two convolutions of kernel size 1. It is only when kernel size=1 that remains position dependency, similar to CNN. The input/output dimension is d_{model}=512 while the dimension of inner layer is 2048. The idea, proposed by the genius Kaiming He.), is that it reduce the of feature maps when having calculation.Unlike recurrent networks, the multi-head attention network cannot naturally make use of the position of the words in the input sequence. Without positional encodings, the output of the multi-head attention network would be the same for the same sentences in different order. For example, “Are you very big?” and “Are big very you?”. Positional encodings explicitly encode the relative/absolute positions of the inputs as vectors and are then added to the input embeddings.The paper uses the equation PE(pos, 2i)=sin(pos/10000^{2i/d_{model}}) to compute the positional encodings, where pos represents the position, and i is the dimension. Basically, each dimension of the positional encoding is a wave with a different frequency. This allows the model to easily learn to attend to relative positions, since PE[pos+k] can be represented as a linear function of PE[pos], so the relative positon between different embeddings can be easily inferred.If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:In the following figure, each row corresponds the a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains d_{model} values with pos rows, which is the count of input word in a sentece. For example, source sentence has 20 word with each word embedding=512. We’ve colored them so the pattern is visible.Now that we’ve covered most of the concepts on the encoder side, let’s take a look at how decoder works.Similar to the encoder, residual connections are employed around each of the sub-layers, followed by layer normalization.In encoder, self-attention layers process input queries,keys and values that comes from same place i.e. the output of previous layer in encoder. Each position in encoder can attend to all positions from previous layer of the encoder. In decoder, self-attention layer enable each position to attend to all previous positions in the decoder, including the current position.However, to prevent positions from attending to subsequent position. In other words, the self-attention layer is only allowed to attend to earlier positions in the output sequence. Masking multi-head attention is done by masking future positions (setting them to -∞) before the softmax step in the self-attention calculation. This step ensures that the predictions for position i can depend only on the known outputs at positions less than i.The “Encoder-Decoder Attention” layer is different with Encoder/Decoder attention layer. Unlike multi-head self-attention, Encoder-Decoder Attention creates its Queries matrix from the layer below it, which is decoder self attention, and takes the Keys and Values matrix from the output of the encoder stack.Now that we’ve covered most of the concepts on the encoder/decoder side, Let’s take a look at how they work together.The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.The decoder stack outputs a vector and passes into the final Linear layer which is followed by a Softmax Layer.The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a logits vector with score of a unique word. The softmax layer then turns those scores into probabilities. The highest probability is chosen as the word output for this time step.Traditionally, both the encoder and the decoder were composed of recurrent neural networks (RNNs). RNNs sequentially process the input sequence (x1…, xn) into hidden encodings (h1…hn), then sequentially generate the output sequence (y1…yn). However, the sequential nature of RNN means it is impossible to compute in parallel. Also, the total computational complexity per layer is enormous. Most important, learning long-range dependencies in the network is difficult.Through “the transformer”, we can resolve the parallellization and computational complexity by multi-head attention. The problem of long-range dependencies is also improved through self attention with 1-length in each word.In financial industry, it’s fruitful to portrait a customer through customer journey so that we can have a better understanding of how consumers interact and engage with our brand. However, it’s hard to extract information from customer journey without any feature engineering. Especially “journey” is a sequence behavior rather than a specific feature. With the knowledge of self-attention, we can further implement the concecpt and create value from complex data in our company.It is recommended to read the paper ATrank published by Alibaba. Alibaba used the framework of self attention for product recommendation and achieved better performance.[1] Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translationr. arXiv:1406.1078v3 (2014).[2] Sequence to Sequence Learning with Neural Networks. arXiv:1409.3215v3 (2014).[3] Neural machine translation by joint learning to align and translate. arXiv:1409.0473v7 (2016).[4] Effective Approaches to Attention-based Neural Machine Translation. arXiv:1508.0402v5 (2015).[5] Convolutional Sequence to Sequence learning. arXiv:1705.03122v3(2017).[6] Attention Is All You Need. arXiv:1706.03762v5 (2017).[7] ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation. arXiv:1711.06632v2 (2017).[8] Key-Value Memory Networks for Directly Reading Documents. arXiv:1606.03126v2 (2016).[9] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv:1502.03044v3 (2016).[10] Deep Residual Learning for Image Recognition. arXiv:1512.03385v1 (2015).[11] Layer Normalization. arXiv:1607.06450v1 (2016).blog.csdn.netjalammar.github.iodeeplearning.hatenablog.comdaiwk.github.iolilianweng.github.iomlexplained.commlexplained.comblog.waya.ai",03/10/2018,0,30.0,22.0,1358.0,661.0,19.0,0.0,0.0,21.0,en
4211,Attention for time series forecasting and classification,Towards Data Science,Isaac Godfried,1200.0,14.0,3037.0,"Transformers (specifically self-attention) have powered significant recent progress in NLP. They have enabled models like BERT, GPT-2, and XLNet to form powerful language models that can be used to generate text, translate text, answer questions, classify documents, summarize text, and much more. With their recent success in NLP one would expect widespread adaptation to problems like time series forecasting and classification. After all, both involve processing sequential data. However, to this point research on their adaptation to time series problems has remained limited. Moreover, while some results are promising, others remain more mixed. In this article, I will review current literature on applying transformers as well as attention more broadly to time series problems, discuss the current barriers/limitations, and brainstorm possible solutions to (hopefully) enable these models to achieve the same level success as in NLP. This article will assume that you have a basic understanding of soft-attention, self-attention, and transformer architecture. If you don’t please read one of the linked articles. You can also watch my video from the PyData Orono presentation night.The need to accurately forecast and classify time series data spans across just about every industry and long predates machine learning. For instance, in hospitals you may want to triage patients with the highest mortality early-on and forecast patient length of stay; in retail you may want to predict demand and forecast sales; utility companies want to forecast power usage, etc.Despite the successes of deep learning with respect to computer vision many time series models are still shallow. Particularly, in industry many data scientists still utilize simple autoregressive models instead of deep learning. In some cases, they may even use models like XGBoost fed with manually manufactured time intervals. Usually, the common reasons for choosing these methods remain interpretability, limited data, ease of use, and training cost. While there is no single solution to address all these issues, deep models with attention provide a compelling case. In many cases, they offer overall performance improvements (other vanilla LSTMs/RNNs) with the benefit of interpretability in the form of attention heat maps. Additionally, in many cases, they are faster than using an RNN/LSTM (particularly with some of the techniques we will discuss).Several papers have studied using basic and modified attention mechanisms for time series data. LSTNet is one of the first papers that proposes using an LSTM + attention mechanism for multivariate forecasting time series. Temporal Pattern Attention for Multivariate Time Series Forecasting by Shun-Yao Shih et al. focused on applying attention specifically attuned for multivariate data. This mechanism aimed at resolving issues including noisy variables in the multivariate time series and introducing a better method than a simple average. Specifically,The attention weights on rows select those variables that are helpful for forecasting. Since the context vector vt is now the weighted sum of the row vectors containing the information across multiple time steps, it captures temporal information.Simply speaking, this aims to select the useful information across the various feature time series data for predicting the target time series. First, they utilize a 2dConvolution on the row vectors of the RNNs hidden states. This is followed by a scoring function. Finally, they use a sigmoid activation instead of softmax since they expect multiple variables to be relevant for prediction. The rest follows a fairly standard attention practice.In terms of results, the model outperforms (using relative absolute error) other methods including a standard auto-regressive model and LSTNet on forecasting solar energy and electricity demand, traffic and exchange rate.Even though this article doesn’t use self-attention I think this is a really interesting and well-thought-out use of attention. A lot of time-series research seems to focus on univariate time series data. Moreover, the ones that do study multivariate time series often solely expand the dimensions of the attention mechanism rather than apply it horizontally across the feature time-series. It might make sense to see if a modified self-attention mechanism could select the relevant source time series data for predicting the target. The full code for this paper is publicly accessible on GitHub.Lets first briefly review a couple of specifics of self-attention before we delve into the time series portion. For a more detailed examination please see this article on mathematics of attention or the Illustrated Transformer. For self-attention recall that we generally have query, key, value vectors that are formed via simple matrix multiplication of the embedding by the weight matrix. What a lot of explanatory articles don’t mention is that query, key, and value can often come from different sources depending on the task and vary based on whether it is the encoder or the decoder layer. So for instance, if the task is machine translation the query, key and value vectors in the encoder would come from the source language but the query, key, and value vectors in the decoder would come from the target language. In the unsupervised language modeling case however they are all generally formed from the source sequence. Later on we will see that many self-attention time series models modify how these values are formed.Secondly, self-attention generally requires positional encodings as it has no knowledge of sequence order. It usually incorporates this positional information via addition to the word or time step embedding rather than concatenation. This is somewhat odd as you would assume that adding positional encodings directly to the word embedding would hurt it. However according to this Reddit response due to the high dimensionality of the word embeddings the positional encodings you get approximate orthogonality (i.e. the positional encodings and word embeddings already occupy different spaces). Moreover, the poster argues that sine and cosine help to give nearby word similar positional embeddings.But in the end this still leaves a lingering question: wouldn’t straightforward concatenation work better in this respect? This is something that I don’t have a direct answer for at the moment. There are however some good recent papers on creating better positional embeddings. Transformer-XL (the basis for XLNet) has its own specific relational embeddings. Also the NIPs 2019 paper, Self-attention with Functional Time Representation Learning, examines creating more effective positional representations through a functional feature map (though paper is not currently on arxiv at the moment).A number of recent studies have analyzed what actually happens in models like BERT. Although geared entirely towards NLP these studies can help us to understand how to effectively utilize these architectures for time series data as well as anticipate possible problems.In “What Does BERT Look At? An Analysis of BERT’s Attention” the authors analyze the attention of BERT and investigate linguistic relations. This paper is a great illustration of how self-attention (or any type of attention really) naturally lends itself to interpretability. As we can use the attention weights to visualize the relevant parts of focus.Also interesting is the fact that the authors find the following:We find that most heads put little attention on the current token. However, there are heads that specialize to attending heavily on the next or previous token, especially in earlier layers of the networkObviously in time-series data attention heads “attending to the next token” is problematic. Hence, when dealing with time series we will have to apply some sort of mask. Secondly, it is hard to tell if this is solely a product of the language data BERT was trained on or if this is likely to occur with multi-headed attention more broadly speaking. For forming language representations focusing on the closest word makes a lot of sense. However, this is much more variable with time series data, in certain time series sequences causality can come from steps much further back (for instance for some rivers it can take 24+ hours for heavy rainfall to raise the river).Are Sixteen Heads Really Better than OneIn this article, the authors found that pruning several attention heads had a limited effect on performance. Generally, performance only significantly fell when more than 20% of attention heads were pruned. This is particularly relevant for time series data as often we are dealing with long dependencies. Especially only ablating a single attention head seems to have almost no impact on score and in some cases results in better performance.Visualizing the Geometry of BERTThis paper explores the geometrical structures found within the BERT model. They conclude that BERT seems to have geometric representations of parse trees internally. They also discover there are semantically meaningful subspaces within the larger embedding space. Although this probe is obviously linguistically focused, the main question it raises is if BERT learns these linguistically meaningful patterns then would it learn similar temporally relevant patterns. For instance, if we large scale trained a transformer time series, what would we discover in the embedding space? Would for instance we see similar patient trajectories clustered together or if we trained on many different streams for flood forecasting would it group dam fed streams together with similar release cycles, etc… Large scale training of a transformer on thousands of different time series could prove insightful and enhance our understanding of the data as well. The authors include two cool GitHub pages with interactive visualizations that you can use to explore further.Another fascinating research work that came out of ICLR 2019 was Pay Less Attention with Lightweight and Dynamic Convolutions. This work investigates both why self-attention works and proposes dynamic convolutions as an alternative. The main advantage of dynamic convolutions are that they are computationally simpler and more parallelizable than self-attention. The authors find that these dynamic convolutions preform roughly equivalent to self-attention. The authors also employ weight sharing which further reduces the parameters required overall. Interestingly, despite the potential speed improvements I haven’t seen any time series forecasting research adopt this methodology (at least not yet).There have been only a few research papers that use self-attention on time series data with varying degrees of success. If you know of any additional ones please let me know. Additionally, huseinzol05 on GitHub has implemented a vanilla version of attention is all you need for stock forecasting.Attend and Diagnose leverages self attention on medical time series data. This time series data is multivariate and contains information like a patient’s heart rate, SO2, blood pressure, etc.Their architecture starts with a 1-D convolution across each clinical factor which they use to achieve preliminary embeddings. Recall that a 1D Conv will utilize a kernel of a specific length and process it a set number of times. It is important to note that here the 1-D convolution is not applied across the time series steps as is typical. Therefore if the initial time series contains 100 steps it will still contain 100 steps. Rather it is instead applied to create a multi-dimensional representation of each time step. For more information on 1-D convolutions for time series data refer to this great article. After the 1-D convolution step the authors then use positional encodings:The encoding is performed by mapping time step t to the same randomized lookup table during both training and prediction.This is different than standard self-attention which uses cosine and sine functions to capture the position of words. The positional encodings are joined (likely added although…the authors do not indicate exactly how) to each respective output from the 1D Conv layer.Next comes the self-attention operation. This is mostly the same as the standard type of multi-headed attention operation, however it has a few subtle differences. First as mentioned above since this is time series data the self-attention mechanism cannot incorporate the entire sequence. It can only incorporate timesteps up to the time step being considered. To accomplish this the authors appear to use a masking mechanism that also masks timestamps too far in the past. Unfortunately, the authors are very non-specific on the actual formula for this, however, if I had to guess I would assume it is roughly analogous to the masking operation shown by the authors in overcoming the transformer bottleneck.After the multi-headed attention, the now transformed embeddings still need to have additional steps taken before they are useful. Typically in standard self-attention, we have an addition and layer normalization component. The layer normalization will normalize the output of the self-attention and the original embedding (see here for more information on this), however, the authors instead chooses to Dense Interpolation. This means that embeddings outputted from the multi-headed-attention module are taken and used in a manner that is useful for capturing syntactic and structural information.After the dense interpolation algorithm, there is a linear layer followed by a softmax, sigmoid or relu layer (depending on the task). The model itself is multitasking so it aims to forecast length of stay, the diagnosis code, the risk of decompensation, the length of stay and the mortality rate.Altogether I thought this paper was good demonstration of using self-attention on multivariate time series data. The results were state of the art at the time it was released, they have now been surpassed by TimeNet. However, this is primarily due to the effectiveness of transfer-learning based pretraining rather than the architecture. If I had to guess similar pre-training with SAND would result in better performance.My main critcism of this paper is primarily from reproducibility standpoint as no code is provided and various hyperparameters such as the kernal size are either not included or only vaguely hinted at. Other concepts are not discussed clearly enough such as the masking mechanism. I’m currently working on trying to reimplement in PyTorch and will post the code here when I’m more sure about its realiability.Another recent paper that is fairly interesting is “CDSA: Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation” by Jiawei Ma et al. This article focuses on imputing (estimating) missing time series values. Effective data imputation is important for many real world applications as sensors often have periods where they malfunction causing missing data. This creates problems when trying to forecast or classify data as the missing or null values will impact the forecast. The authors setup their model to use a cross attention mechanism that works by utilizing data in different dimensions such as time location and the measurement.The authors evaluate their results on several traffic-forecasting and air-quality datasets. They evaluate with respect to both forecasting and imputation. For testing imputation, they discard a certain percentage of the values and attempt to impute them using the model. They compare these for with the actual values. Their model outperforms other RNN and statistical imputation methods on all missing data rates. In terms of forecasting, the model also achieves the best performance.Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting by Shiyang Li et al.This is a recent article that will appear at NIPS in 2019. It focuses on several of the problems with applying the transformer to time series data. The authors basically argue that classical self attention does fully leverage the contextual data. They argue that this particularly causes problems with dynamic time series data that varies with seasonality (for instance forecasting sales around the holidays vs. the rest of the year or forecasting extreme weather patterns). To remedy this they introduce a new method of generating the query and value vectors.We propose convolutional self attention [mechanism] by employing causal convolutions to produce queries and keys in the self attention layer. Query-key matching aware of local context, e.g. shapes, can help the model achieve lower training error and further improve its forecasting accuracy.Part two of the article focuses on solutions related to the memory use of the transformer model. Self-attention is very memory intensive particularly with respect to very long sequences (specifically it is O(L²)). The authors propose a new attention mechanism that is O(L(log L)²). With this self-attention mechanism, cells can only attend to previous cells with an exponential step size. So for instance cell five would attend to cell four and cell two. They also introduce two variations of this log attention: local attention and restart attention. See their diagram below for more informationThe authors evaluate their approach on several different datasets including electricity consumption (recorded in 15-minute intervals), traffic in San Francisco (20-minute intervals), solar data production hourly (from 137 different power plants) and wind data (daily estimates of 28 counties wind potential as a percentage of overall power production). Their choice of ρ-quantile loss as an evaluation metric is a bit strange as normally I’d expect MAE, MAP, RMSE or something similar for a time series forecasting problem.I’m still trying to grasp what exactly this metric represents, however at least from the results it appears that a lower score is better. Using this metric their convolutional self-attention transformer outperforms DeepAR, DeepState, ARIMA, and other models. They also conduct an ablation study where they look at the effect of kernel size when computing a seven-day forecast. They found that a kernel size of 5 or 6 generally produced the best result.I think this is a good research article that addresses some of the short-comings of the transformer as applied to time-series data. I particularly think that the use of a convolutional kernel (of size greater than one) is really useful in time series problems where you want to capture surrounding context for the key and query vectors. There is currently no code, but NeurIPs is still more than a month away so hopefully, the authors release it between now and then.In conclusion, self-attention and related architectures have led to improvements in several time series forecasting use cases, however, altogether they have not seen widespread adaptation. This likely revolves around several factors such as the memory bottleneck, difficulty encoding positional information, focus on pointwise values, and lack of research around handling multivariate sequences. Additionally, outside of NLP many researchers are not probably not familiar with self-attention and its potential. While simple models such as ARIMA in many cases make sense for industry problems I believe that transformers have a lot to offer as well.Hopefully, the approaches summarized in this article shine some light on effectively applying transformers to time series problems. In a subsequent article, I plan on giving a practical step-by-step example of forecasting and classifying time-series data with a transformer in PyTorch. Any feedback and/or criticisms are welcome in the comments. Please let me know if I got something incorrect (which is quite possible given the complexity of the topic) and I will update the article.",10/04/2019,0,3.0,1.0,1400.0,680.0,10.0,1.0,0.0,23.0,en
4212,Silhouette Coefficient,Towards Data Science,Ashutosh Bhardwaj,44.0,3.0,411.0,"After learning and applying several supervised ML algorithms like least square regression, logistic regression, SVM, decision tree etc. most of us try to have some hands-on unsupervised learning by implementing some clustering techniques like K-Means, DBSCAN or HDBSCAN.We usually start with K-Means clustering. After going through several tutorials and Medium stories you will be able to implement k-means clustering easily. But as you implement it, a question starts to bug your mind: how can we measure its goodness of fit? Supervised algorithms have lots of metrics to check their goodness of fit like accuracy, r-square value, sensitivity, specificity etc. but what can we calculate to measure the accuracy or goodness of our clustering technique? The answer to this question is Silhouette Coefficient or Silhouette score.Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.1: Means clusters are well apart from each other and clearly distinguished.0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.-1: Means clusters are assigned in the wrong way.Silhouette Score = (b-a)/max(a,b)wherea= average intra-cluster distance i.e the average distance between each point within a cluster.b= average inter-cluster distance i.e the average distance between all clusters.Importing libraries:Generating some random data:To run clustering algorithm we are generating 100 random points.Plotting the data:OutputApplying KMeans Clustering with 2 clusters:Calculating the silhouette score:Output: Silhouette Score(n=2): 0.8062146115881652We can say that the clusters are well apart from each other as the silhouette score is closer to 1.To check whether our silhouette score is providing the right information or not let’s create another scatter plot showing labelled data points.Output:It can be seen clearly in the above figure that each cluster is well apart from each other.Let’s try with 3 clusters:Output:Silhouette Score(n=3): 0.5969732708311737As you can see in the above figure clusters are not well apart. The inter cluster distance between cluster 1 and cluster 2 is almost negligible. That is why the silhouette score for n= 3(0.596) is lesser than that of n=2(0.806).When dealing with higher dimensions, the silhouette score is quite useful to validate the working of clustering algorithm as we can’t use any type of visualization to validate clustering when dimensions are greater than 3.We can also use the silhouette score to check the optimal number of clusters. In the above example, we can say that the optimal number of clusters is 2 as its silhouette score is greater than that of 3 clusters.",26/05/2020,7,5.0,0.0,467.0,316.0,4.0,0.0,0.0,0.0,en
4213,Clustering Algorithm for Customer Segmentation,Towards Data Science,Destin Gong,1100.0,11.0,1431.0,"In a business context: Clustering algorithm is a technique that assists customer segmentation which is a process of classifying similar customers into the same segment. Clustering algorithm helps to better understand customers, in terms of both static demographics and dynamic behaviors. Customer with comparable characteristics often interact with the business similarly, thus business can benefit from this technique by creating tailored marketing strategy for each segment.In a data science context: Clustering algorithm is an unsupervised machine learning algorithm that discovers groups of data points that are closely related. The fundamental difference between supervised and unsupervised algorithm is that:After giving an overview of what is clustering, let’s delve deeper into an actual Customer Data example. I am using the Kaggle dataset “Mall Customer Segmentation Data”, and there are five fields in the dataset, ID, age, gender, income and spending score. What the mall is most concerned about are customers’ spending scores, hence the objective of this exercise is to find hidden clusters in respects of the field spending score.Load the dataset and summarize column statistics using describe().Examine the distribution of each field: use bar chart for categorical variables and histogram for numeric variables.If you would like more details about data visualization and EDA, please check out these two articles.towardsdatascience.comtowardsdatascience.comNow that we define the objectives and have a better understanding of our data, we need to preprocess the data to meet the model requirement. In this example, I am choosing K Means clustering as the main algorithm and the reason will be explained later on.The standard k means algorithm is not directly applicable to categorical data. To be more specific, the search space of categorical variables (e.g. Gender in this example) is discrete (either male or female), hence cannot be directly combined with a continuous space and measured the distance in the same manner. Therefore I discarded the “Gender” field as well as “CustomerID” field.Since K means interpret the closeness between data points based on Euclidean distance, it is important to reconcile all dimensions into a standard scale. An appropriate type of data transformation should be selected to align with the distribution of the data. This article from Google provides a general guidance of data transformation in clustering.In Summary:From the earlier univariate analysis, we can see that those variables don’t conform to either normal distribution or law distribution. Therefore, I use MinMaxScaler to shrink the data range to between 0 and 1, while maintaining the distribution shape.Exploratory Data Analysis provides visual clues about whether it is likely to form insightful clusters when combining multiple variables together. It is also an imperative step because choosing an appropriate clustering algorithm is reliant on the shape of the cluster. Some center-based algorithms (e.g. K Means) are more adaptable towards globular shapes clusters and they tend to break linear shaped clusters apart. While density-based algorithms (e.g. DBSCAN) are better at clusters with irregular shape and a lot of noise.I have visualized those three fields in the following way.2D Scatter Plot3D Scatter Plot:In this case, it is apparent that the plot of “annual income vs. spending score” generates some center-based clusters. Therefore, I will use K means algorithm for further exploration.K Means Clustering is a centre-based clustering algorithm, which means that it assigns data points to clusters based on closeness or distance, following these procedures:This procedure also determines that K Means algorithm has the limitation of clustering points into circular shapes with similar size and it is also heavily reliant on the predefined number K.Now let’s take a closer look at how to implement it using python scikit learn.Firstly, I define a KMeans_Algorithm function that pass the dataset and the number of clusters as the parameters.This function calls the KMeans() from sklearn library and specify some key parameters:The output will generates following attributes and evaluation metrics:In this example, I am mainly interested in the formation of clusters with regards to “Spending Score”. Therefore, I used dataset X1, X2, X3 as specified below to feed the model.Notice that, we no longer needs to partition the dataset for training and testing for unsupervised models.Age vs. Spending ScoreThe code below used the dataset X1 — scaled Age vs. scaled Spending Score, and visualize how the clustering changes as the number of clusters increased from 2 to 10.The first part of the code iterates through the number of clusters and generate a scatter plot for each, with the red square representing the centroid. As shown below, the clusters change as the specified “k” value changes.Afterwards, I evaluate the model performance based on two metrics: inertia and silhouette coefficient.Since clustering is an unsupervised algorithm, we cannot directly assess the model quality based on discrepancies between the predicted results and the actual results. Therefore, the evaluation should be based on the principle of minimizing intra-cluster distance while maximizing the inter-cluster distance. There are two evaluation methods that determine the optimal number of clusters based on this principle.1) Elbow method using inertia:Inertia measures the sum of squared distances of samples to their closest cluster centroid. With the same number of cluster, smaller the inertia indicates better clusters. The elbow method determines the optimal number of clusters by looking at the turning point (“elbow” point) of the graph below, in this case is 6.2) Silhouette coefficient:Here is the Silhouette coefficient formula that makes it appear daunting:But basically it translates into minimizing intra-cluster distance while maximizing the inter-cluster distance. a(i) is the average distance of data point i to other data points in the same cluster, and b(i) is the average distance of data point i to all points in the nearest neighbor cluster. The aim is to minimize a(i) and maximize b(i), therefore when the coefficient is closer to 1 indicates better clustering. In this example, both 2 and 6 are showing a peak in the score.Based on the results from both metrics, 6 is most likely to be the optimal number of clusters when plotting “Age” against “Spending Score”. However, either from the score or the visualizations, it is still not convincing enough to say that age and spending score together form insightful clusters.Annual Income vs. Spending ScoreFrom the EDA process, we observed that there are several distinct clusters from the chart. Now, let’s take a closer look at whether the model can differentiate customers into distinct segments as expected. The code is similar to the one above but just changing the input dataset into X2 — scaled Annual Income vs. scaled Spending Score.It is quite obvious that there are decent segmentations when the number of clusters equals 5 and it is clearly shown in the scatter plot as well.Age vs. Annual Income vs. Spending ScoreIf you are interested, we can also take a further investigation of how these three fields interacts with each other.Although, in terms of both the visualization and the metric values, there is no indication of forming any outstanding clusters, unsupervised learning is about discovering and exploring hidden effort. Nevertheless, it may be still worth the effort taking this further investigation.After all, the objective of clustering model is to bring insights to customer segmentation. In this exercise, grouping customers into 5 segments, based on two aspects: Spending Score vs. Annual Income, is most beneficial to create tailored marketing strategies.As shown below, customers can be segmented into five groups:Based on the distinct features of each group, the business can approach each customer groups differently. The first and third types of customers generate the most revenue and they require retention using marketing strategies, such as loyalty program or discounts through newsletter. On the other hand, customer acquisition strategies are more suitable for group 2 and 4. This can be tailored marketing campaigns based on their income levels, since high income group and low income group will have different preferences in products.It is worth noting several limitations using K Means clustering. It doesn’t work well when the clusters vary in sizes and density, or if there are any significant outliers. Therefore, we need to consider other clustering algorithms when encountering situations like this. For example, DBSCAN is more resistance to noise and outliers, and hierarchical clustering does not assume any particular number of clusters. If you would like to know more about comparing with DBSCAN, check out my Kaggle Notebook.Hope you enjoy my article :). If you would like to read more of my articles on Medium, please feel free to contribute by signing up Medium membership using this affiliate link (https://destingong.medium.com/membership).This article introduces clustering algorithm, specifically K Means Clustering, and how we can apply it in a business context to assist customer segmentation. Some key takeaways:towardsdatascience.comtowardsdatascience.commedium.comOriginally published at https://www.visual-design.net on July 4th, 2021.",04/07/2021,1,28.0,24.0,1056.0,737.0,27.0,9.0,0.0,12.0,en
4214,"Train a neural net for semantic segmentation in 50 lines of code, with Pytorch",Towards Data Science,Sagi eppel,4.0,9.0,1444.0,"How to train a neural net for semantic segmentation in less than 50 lines of code (40 if you exclude imports). The goal here is to give the fastest simplest overview of how to train semantic segmentation neural net in PyTorch using the built-in Torchvision neural nets (DeepLabV3).Code is available: https://github.com/sagieppel/Train-Semantic-Segmentation-Net-with-Pytorch-In-50-Lines-Of-CodeThe goal is semantic segmentation is to take images and identify regions belonging to specific classes. This is done by processing the image through a convolution neural network that outputs a map with a class per pixel. The classes are given as a set of numbers. For example, in this case, we will use the LabPics V1 dataset with three classes (shown in the figure below):Class 0: Not a vessel (black),Class 1: Empty region of the vessel(gray),Class 2: Filled region of the vessel(white).The goal of the net is to receive an image and predict for each pixel one of the 3 classes.The first step download the LabPics dataset from here: https://zenodo.org/record/3697452/files/LabPicsV1.zip?download=1You will also need to install Pytorch and OpenCV for image reading.OpenCV can be installed using:pip install opencv-pythonFirst, let's import packages and define the main training parameters:Learning_Rate: is the step size of the gradient descent during the training.Width and height are the dimensions of the image used for training. All images during the training processes will be resized to this size.batchSize: is the number of images that will be used for each iteration of the training.batchSize*width*high will be proportional to the memory requirement of the training. Depending on your hardware, it might be necessary to use a smaller batchSize to avoid out-of-memory problems.Note that since we train with only a single image size, the net once trained is likely to be limited to work with only images of this size. In most cases what you want to do is change the size between each training batch.Next we create a list of all images in the dataset:Were TrainFolder and is the LabPics dataset simple train folder. The images are stored in the “image” subfolder of the TrainFolder.Next, we define a set of transformations that will be performed on the image using the TorchVision transform module:This defines a set of transformations that will apply to the image and annotation map. This includes converting to PIL format, which is the standard format for the transform. Resizing and converting to PyTorch format. For the image, we also normalize the intensity of the pixels in the image by subtracting the Mean and dividing by the standard deviation of pixels intensity. The mean and deviation were calculated beforehand large set of images.Next, we create a function that will allow us to load a random image and the corresponding annotation map for training:In the first part, we pick a random index from the list of images and load the image corresponding to this index.Next, we want to load the annotations masks for the image:These annotations are stored as images/masks that cover the region belonging to the specific class (Filled/Vessel). Each class mask is stored in a separate .png image file. Where pixels belonging to the class have values of 1 (gray) and the others are 0 (black).To train the net, we need to create one segmentation map where the values of pixels belonging to the empty vessel region are 1 (gray), the values of the pixels belonging to the filled region are 2 (white), and the rest are 0 (black).First, we create a segmentation map full of zeros in the shape of the image:Next, we set all the pixels that have a value of 1 in the Vessel mask to have a value of 1 in the segmentation mask. And all the pixels that value of 1 in the Filled mask to have a value of 2 in the segmentation mask:Where “AnnMap[ Filled == 1 ] = 2” means that every position in the Filled mask with a value of 1, will get a value of 2 in the AnnMap.If there is no annotation file for the Vessel and Filled classes (which will happen if a class does not appear in the image), the cv2.imread will return None, and the mask will be ignored.Finally, we convert the annotation into PyTorch format using the transformation we defined earlier:For training, we need to use a batch of images. This means several images stacked on top of each other in a 4D matrix. We create the batch using the function:The first part creates an empty 4d matrix that will store the images with dimensions: [batchSize, channels, height, width], where channels are the number of layers for the image; this is 3 for RGB image and 1 for the annotation map.The next part load set of images and annotation to the empty matrix, using the ReadRandomImage() we defined earlier.Now that we can load our data, its time to load the neural net:The first part is identifying whether the computer has GPU or CPU. If there is Cuda GPU the training will be done on the GPU:For any practical dataset, training using a CPU is extremely slow.Next, we load the deep lab net semantic segmentation:torchvision.models. contain many useful models for semantic segmentation like UNET and FCN . We choose Deeplabv3 since its one best semantic segmentation nets. By setting pretrained=True we load the net with weight pretrained on the COCO dataset. It is always better to start from the Pretrained model when learning a new problem since it allows the net to use the previous experience and converge faster.We can see all the layers of the net we just loaded by writing:print(Net)….(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU() (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))This prints the net of the layer in the order they are used. The final layer of the network is a convolution layer with 256 layers input and 21 layers output. The 21 represent the number of output classes. Since we only have 3 classes in our dataset, we want to replace it with a new convolutional layer with 3 outputs:To be fair this part is optional since a net with 21 output classes can predict 3 classes simply by ignoring the reminder 18 classes. But it's more elegant this way.Next, we load the net into our GPU or CPU device:Finally, we load an optimizer:The optimizer will control the gradient rates during the backpropagation step of the training. Adam optimizer is one of the fastest optimizers available.Finally, we start the training loop:LoadBatch was defined earlier and load the batch of images and annotation maps. images and ann will store the loaded images and annotations.torch.autograd.Variable: convert the data into gradient variables that can be used by the net. We set Requires_grad=False since we don't want to apply the gradient to the image, only to the layers of the net. The to(device) copy the tensor to the same device (GPU/CPU) as the net.Finally, we input the image to the net and get the prediction.Once we made a prediction, we can compare it to the real (ground truth) annotation and calculate the loss:First, we define the loss function. We use the standard cross-entropy loss:We use this function to calculate the loss using the prediction and the real annotation:Once we calculate the loss, we can apply the backpropagation and change the net weights.This covers the full training stage, but we also need to save the trained model. Otherwise, it will be lost once the program stop.Saving is time-consuming, so we want to do it about once every 1000 steps:After running this script about 3000 steps, the net should give decent results.Full code can be found here:github.comAll together 50 lines of code not including spaces, and 40 lines not including imports:-)Finally, once the net has been trained, we want to see its result. We do this using a separate inference script use the net to segment an image:Most of the code here is the same as the training script, with only a few differences:Load the net we trained and saved earlier from the file in modelPathConvert the net from training mode to evaluation mode. This mainly means no batch normalization statistics will be calculated.This means the net is run without collecting gradients. Gradients are only relevant for training and collecting them is resource-intensive.Note that the output in Pred will be mapped with 3 channels per image, with each channel representing the unnormalize probability for one of the 3 classes. To find for each pixel the class it belongs to, we take the channel (class) with the highest value of the 3 using the argmax function:We do it for every pixel in the output map and get one of 3 classes for every pixel.The results:Input Image:Output prediction:",03/12/2021,29,0.0,17.0,1020.0,430.0,4.0,0.0,0.0,6.0,en
4215,Multivariate Time Series Forecasting with Transformers,Towards Data Science,Jake Grigsby,208.0,8.0,1700.0,"Many real-world applications of Machine Learning involve making predictions about the outcomes of a group of related variables based on historical context. We might want to forecast the traffic conditions on connected roads, the weather at nearby locations, or the demand for similar products. By modeling multiple time series together, we hope that changes in one variable may reveal key information about the behavior of related variables. Multivariate Time Series Forecasting (TSF) datasets have two axes of difficulty: we need to learn temporal relationships to understand how values change over time and spatial relationships to know how variables impact one another.Popular statistical approaches to TSF can struggle to interpret long context sequences and scale to complex variable relationships. Deep Learning models overcome these challenges by making use of large datasets to predict rare events far into the future. Many methods focus on learning temporal patterns across long timespans and are based on Recurrent or Convolutional layers. In highly spatial domains, Graph Neural Networks (GNNs) can analyze the relationships amongst variables as a graph of connected nodes. That graph is often pre-defined, e.g., a map of roads and intersections in traffic forecasting.In this post, we hope to explain our recent work on a hybrid model that learns a graph across both space and time purely from data. We convert multivariate TSF into a super-long sequence prediction problem that is solvable with recent improvements to the Transformer architecture. The approach leads to competitive results in domains ranging from temperature prediction to traffic and energy forecasting.This is an informal summary of our research paper, “Long-Range Transformers for Dynamic Spatiotemporal Forecasting,” Grigsby, Wang, and Qi, 2021. The paper is available on arXiv, and all the code necessary to replicate the experiments and apply the model to new problems can be found on GitHub.Transformers are a state-of-the-art solution to Natural Language Processing (NLP) tasks. They are based on the Multihead-Self-Attention (MSA) mechanism, in which each token along the input sequence is compared to every other token in order to gather information and learn dynamic contextual information. The Transformer learns an information-passing graph between its inputs. Because they do not analyze their input sequentially, Transformers largely solve the vanishing gradient problem that hinders Recurrent Neural Networks (RNNs) in long-term prediction. For this reason, Transformers have been applied to datasets with long historical information, including TSF.Multivariate TSF datasets are usually organized by time: the values of all N variables are represented as a single vector. However, this only allows Transformers to learn relationships between the entire stack of variables across time. In complex Multivariate TSF problems, each variable has meaningful relationships to its history as well as different events in the history of other variables. A standard application of Transformers to TSF data can’t learn this because it treats the values of every variable at a given timestep as a single token in its graph; each variable cannot have its own opinion on the context it should prioritize. This is unlike the NLP tasks where Transformers are so popular, where every token represents a unified idea (a single word).We address this by creating a new prediction problem in which each token represents the value of a single variable per timestep. Transformers are then free to attend to the values of any variable at any time in order to make more accurate predictions. The diagram at the top of this post shows the difference between these two types of attention.We use an input format in which N variables at T timesteps are flattened into a sequence of (N x T) tokens. The value of each variable is projected to a high-dimensional space with a feed-forward layer. We then add information about the timestep and variable corresponding to each token. The time and variable embeddings are initialized randomly and trained with the rest of the model to improve our representation of temporal and spatial relationships. The values at future timesteps we want to predict are set to zero, and we tell the model which ones are missing with a binary “given” embedding. The different components are summed and laid out such that Transformer MSA constructs a spatiotemporal graph across time and variable space. The embedding pipeline is visualized in the figure below.Standard Transformers compare each token to every other token to find relevant information in the sequence. This means that the model’s runtime and memory use grows quadratically with the total length of its input. Our method greatly exaggerates this problem by making the sequence N times longer than the timeseries itself. The rest of our approach deals with the engineering challenge of making it possible to train this model without the highest-end GPU/TPUs.Efficient Transformers are an active area of research in applications with long input sequences. These “Long-Range Transformers” look to fit the gradient computation of longer sequences in GPU memory. They often do this by adding heuristics to make the attention graph sparse, but those assumptions don’t always hold up outside of NLP. We use the Performer attention mechanism, which linearly approximates MSA with a kernel of random features. Performer is efficient enough to fit sequences of thousands of tokens, and lets us train our model in a few hours on one node with 10GB GPUs.The context sequence of historical data and the target timestamps we’d like to predict are converted to long spatiotemporal sequences. A Performer-based encoder-decoder architecture processes the sequence and predicts the value of each variable at future timesteps as separate tokens. We can then re-stack the predictions to their original format and train to minimize prediction-error metrics like mean squared error. The model can also create a range of forecasts by outputting both the mean and standard deviation of a normal distribution — in which case we train to maximize the probability of the ground-truth sequence. The full model architecture is shown below.We compare the model against more standard TSF and GNN methods. Linear AR is a basic linear model trained to make auto-regressive predictions, meaning it outputs one token at a time and recycles its output as an input for the next prediction. LSTM is a standard RNN-based encoder-decoder model without attention. LSTNet is an auto-regressive model based on Conv1D layers and RNNs with skip connections to remember long-term context. DCRNN is a graph-based model that can be used when a pre-defined variable graph is available. Like our method, MTGNN is a TSF/GNN hybrid that learns its graph structure from data but does not use Transformers for temporal forecasting. Finally, we include a version of our own model that does not separate the tokens into a spatiotemporal graph; the values of each variable remain stacked together as usual. This “Temporal” ablation is a stand-in for Informer, but it uses all of the rest of our engineering tricks and training process to isolate the benefits of spatiotemporal attention.First, we’ll look at a weather forecasting task. We used the ASOS Network to put together a large dataset of temperature readings from airports in Texas and New York. The geographic separation between the two groups makes spatial relationships more important, and those relationships have to be learned from experience because we do not provide any location information. We predict 40, 80, and 160 hours into the future and compare Mean Squared Error (MSE), Mean Absolute Error (MAE) and Root Relative Squared Error (RRSE). This experiment focuses on TSF models because a graph is not available.Spacetimeformer outperforms the baselines, and its advantage over the Temporal attention version appears to increase with the length of our predictions. Our goal is to learn a spatiotemporal attention graph, and we can verify that this is what Spacetimeformer accomplishes by visualizing its attention network. Attention matrices visualize MSA by revealing the attention given by each token to the full sequence; each row is one token, and the columns of that row show the token’s attention to the other tokens in the sequence, including itself. The figure below shows the weather station variables and attention matrices of Spacetimeformer and the Temporal-only variant, where darker blue coloring corresponds to more attention. The standard Temporal mechanism learns a sliding wave-like pattern where each token focuses mostly on itself (along the diagonal) and on the very end of the sequence. Spacetimeformer flattens that sequence into separate variables, with each variable having its own sub-sequence of tokens (indicated by a green arrow and the variable shape). This results in a ‘block-structured’ attention matrix where all the tokens of a variable tend to prioritize the timesteps of a subset of the other variables. We can interpret these patterns to understand the spatial relationships the model is learning. In this case, the model can correctly cluster the Texas and New York stations together — and if you zoom in, you can see the same wave-like temporal patterns within each subsequence.Next, we look at three benchmark datasets in traffic and energy forecasting. AL Solar measures the solar output of 137 sites in Alabama, while the Metr-LA and Pems-Bay datasets measure vehicle speeds at 200+ road sensors around Los Angeles and San Francisco, respectively. We generate 4-hour solar forecasts and 1-hour traffic forecasts. The results are shown in the tables below.Spacetimeformer learns an accurate prediction model in all cases. The traffic results are interesting because the complex road network makes these problems a common benchmark in Graph Neural Network research, where the map can be turned into a graph and provided to the model in advance. Our model has comparable predictive power while implicitly learning a roadmap from data.If you’d like to apply this method to new problems, the source code for the model and training process is released on GitHub at QData/spacetimeformer. A more detailed explanation with additional background and related work can be found in our paper.— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —Written by Jake Grigsby, Zhe Wang, and Yanjun Qi. This research was done by the QData Lab at the University of Virginia.",28/10/2021,0,8.0,22.0,1262.0,633.0,7.0,0.0,0.0,18.0,en
4216,BLiTZ — A Bayesian Neural Network library for PyTorch,Towards Data Science,Piero Esposito,154.0,5.0,904.0,"This is a post on the usage of a library for Deep Bayesian Learning. If you are new to the theme, you may want to seek one of the many posts on medium about it or just the documentation section on Bayesian DL of our lib repo.As there is a rising need for gathering uncertainty over neural network predictions, using Bayesian Neural Network layers became one of the most intuitive approaches — and that can be confirmed by the trend of Bayesian Networks as a study field on Deep Learning.It occurs that, despite the trend of PyTorch as a main Deep Learning framework (for research, at least), no library lets the user introduce Bayesian Neural Network layers intro their models with as ease as they can do it with nn.Linear and nn.Conv2d, for example.Logically, that causes a bottleneck for anyone that wants to iterate flexibly with Bayesian approaches for their data modeling, as the user has to develop the whole part of Bayesian Layers for its use rather than focusing on the architecture of its model.BLiTZ was created to change to solve this bottleneck. By being fully integrated with PyTorch (including with nn.Sequential modules) and easy to extend as a Bayesian Deep Learning library, BLiTZ lets the user introduce uncertainty on its neural networks with no more effort than tuning its hyper-parameters.In this post, we discuss how to create, train and infer over uncertainty-introduced Neural Networks, using BLiTZ layers and sampling utilities.As we know, the main idea on Bayesian Deep Learning is that, rather than having deterministic weights, at each feed-forward operation, the Bayesian layers samples its weights from a normal distribution.Consequently, the trainable parameters of the layer are the ones that determine the mean and variance of this distribution.Mathematically, the operations would go from:To:Implementing layers where our ρ and μ are the trainable parameters may be hard on Torch, and beyond that, creating hyper-parameter tunable layers may be even harder to craft. BLiTZ has a built-in BayesianLinear layer which can be introduced into the model this easy:It works as a normal Torch nn.Module network, but its BayesianLinear modules perform training and inference with the previously explained uncertainty on its weights.As proposed in its original paper, Bayesian Neural Networks cost function is a combination of a “complexity cost” with a “fitting-to-data cost”. After all the algebra wrangling, for each feed-forward operation, we have:It occurs that the complexity cost (P(W)) consists of the sum of the probability density function of the sampled weights (of each Bayesian layer on the network) relative to a much-simpler, predefined pdf function. By doing that, we ensure that, while optimizing, our model variance over its predictions will diminish.To do that, BLiTZ brings us the variational_estimator decorator that introduces some methods, as nn_kl_divergence method into our nn.Module. Given data points, its labels, and a criterion, we could get the loss over a prediction by doing:Bayesian Neural Networks are often optimized by sampling the loss many times on the same batch before optimizing and proceeding, which occurs to compensate the randomness over the weights and avoid optimizing them over a loss influenced by outliers.BLiTZ’s variational_estimator decorator also powers the neural network with the sample_elbo method. Given the inputs, outputs, criterion and sample_nbr, it estimates does the iterative process on calculating the loss over the batch sample_nbr times and gathers its mean, returning the sum of the complexity loss with the fitting one.It is very easy to optimize a Bayesian Neural Network model:We are now going through this example, to use BLiTZ to create a Bayesian Neural Network to estimate confidence intervals for the house prices of the Boston housing sklearn built-in dataset. If you want to seek other examples, there are more on the repositoryBesides the known modules, we will bring from BLiTZ the variational_estimator decorator, which helps us to handle the Bayesian layers on the module keeping it fully integrated with the rest of Torch, and, of course, BayesianLinear, which is our layer that features weight uncertainty.Nothing new under the sun here, we are importing and standard-scaling the data to help with the training.We can create our class with inheriting from nn.Module, as we would do with any Torch network. Our decorator introduces the methods to handle the Bayesian features, calculating the complexity cost of the Bayesian Layers and doing many feedforwards (sampling different weights on each one) to sample our loss.This function does create a confidence interval for each prediction on the batch on which we are trying to sample the label value. We then can measure the accuracy of our predictions by seeking how much of the prediction distributions did include the correct label for the datapoint.Notice here that we create our BayesianRegressor as we would do with other neural networks.We do a training loop that only differs from a common Torch training by having its loss sampled by its sample_elbo method. All the other stuff can be done normally, as our purpose with BLiTZ is to ease your life on iterating on your data with different Bayesian Neural Networks without trouble.Here is our very simple training loop:BLiTZ is a useful lib to iterate in Deep Learning experiments with Bayesian Layers and very little change in the usual code. Its layers and decorators are very integrated with Torch modules for neural networks and make it easy also to create custom networks and extract their complexity cost have no difficulty.And of course, here is the link for our repo: https://github.com/piEsposito/blitz-bayesian-deep-learninggithub.comarxiv.org",04/04/2020,0,1.0,0.0,445.0,354.0,6.0,0.0,0.0,7.0,en
4217,Neural Art Style Transfer with Keras — Theory and Implementation,GradientCrescent,Adrian Yijie Xu,573.0,8.0,1267.0,"IntroductionOver the past five years, neural networks have received attention through AI-generated art pieces, whether these be paintings, poetry, or music. During October of last year, an AI-generated art piece sold for over $400,000 at an auction at Christie’s, sparking debate and discussion over the intrinsic value and nature of art generated by machines.While most of these mentioned art pieces were original pieces generated through Generative Adversarial Networks (GAN’s, which we will discuss in a future tutorial), apps such as PRISMA have been receiving attention for being able to apply the styles of famous paintings to one’s own photos. The concept, known as neural style transfer (henceforth NST), was first introduced in a paper by Leon Gatys et al. in 2015, and more recently was implemented as a part of Tensorflow’s demo app showcase.NST uses a trained Neural Network to take target image and a reference image, and produce an output which retains both the content of the target image along with the style of the reference image. We can best illustrate this in the example below — here, the target image is an MRI scan , and the reference image the (1832) painting of the Great Wave off Kanagawa.In this tutorial, we will use VGG19 network architecture, pre-trained on over a million images for image classification tasks, to perform style transfer using the Keras framework. Our code is adapted from Francois Chollet’s excellent Deep Learning with Python reference, where the subject was briefly covered in chapter 8. We assume that the reader is familiar with the elements of deep learning, particularly with loss functions and backpropagation training. Those looking for a quick refresher are encouraged to audit Andrew Ng’s original Machine Learning course, which goes into deeper details over the operation and structure of neural networks.TheoryDuring NST, we define two losses in order to preserve both the content of the target image and the style of the reference image. The loss function is a weighted sum of the content loss and style loss which is minimized using gradient descent. Intuitively, we are iteratively updating our output image in such a way that it minimizes our total loss by bringing the output as close as possible to the content of the target image and the style of the reference image.So how do we define content and style losses? Recall that during image classification, a neural network’s earlier layers captures the lower-level features, with later layers focusing on identifying more complex patterns, with the eventual aim of producing a classification output.We hence define content loss simply as the L2 distance between the intermediate content representations , taken from higher (later) layers of a pre-trained neural network, for a input image and the target image. As a high level layer produces filters that possess complex raw information for the input image, this is a suitable approximation for judging similarity in terms of content. The equation is shown below:Similarly, we define the style loss as the L2 distance between the gram matrices of the intermediate style representations for the style image (taken from lower layers of a pre-trained neural network) and the output images. The lower level layers capture more simple image features which best encode the concept of style. Intuitively, Gram matrices we distribute and delocalize spatial information in an image and approximate the “style” of an image. Mathematically, they are matrices are simply multiplication of the image matrix and its transpose.Finally, we add a third loss value known as the total variation loss (TVL) . While not seen in the original paper, TVL was introduced in a paper by Mahendran and Vedaldi in 2015 with the aim to encourage image consistency and special continuity, minimizing pixilation and sharp feature formation. TVL works by penalizing larger gradients during the transfer process, distributing overall changes across larger regions rather than concentrating them at points or curves, ensuring a smoother image at the expense of image sharpness.To conclude, by summing up and minimizing all three aforementioned losses, we generate an output image that best matches the content of our target image, while adopting the new style from the reference image.ImplementationNow that the concepts have been cleared up, let’s take a look at the code itself. We are using the Keras library, so make sure that’s installed before you begin.First off, let’s load our packages and define some variables, as well as our input and output sizes. Note that the larger images will take a longer time to generate.Next, let’s implement some auxiliary functions to preprocess our images for input to the VGG19 network. The deprocess function converts the processed image into its original form for later visualization.Now, let’s load up our pre-trained VGG19 neural network model and feed it our input tensors — namely, the target image, the style reference, and an output image, which we initialize as an appropriately sized placeholder filled with white noise. As other pre-trained networks rely on different datasets, we encourage you to try out other architectures as well. Note that we concatenate all three images into a single input tensor by treating them as a batch of images and not individual inputs.With all of the preparatory work finished, let’s define the content, style, and total variation losses. Additionally, we define the mathematical implementation of the gram matrix in our code.Recall that we calculate content and style losses from different levels of the VGG19 neural network. Let’s define these here, along with the weights of each respective loss toward the overall sum total loss. Feel free to play around with these values — a higher content/style ratio will yield an output image more representative of the original target image, while the opposite will yield an output image with stronger stylistic features.Finally, we define the relationships between our variables and the VGG19 neural network through Keras, and begin summing up our losses. Note how the content loss is defined for one higher layer of the neural network only (‘block5_conv2’) , while the style loss is accumulated across several lower-level layers. This holds for our example, but you may want to play with the layer compositions to observe how the output changes.Now with that complete, we wrap up by defining the overall methods that begin the loss calculation process. We fetch the gradients and loss for our output image using fetch_loss_and_grads. We then use gradient descent to minimize our defined loss and update the gradients, which will ensure maximum similarity between the contents of our target and output images, and the styles of our reference and output images.Finally, we finish by defining and initializing the overall evaluator classes to kickstart the training process. We use Keras’s built-in BFGS optimizer class for gradient descent, over 20 iterations. We also save our images after each epoch for inspection.Running the code, you should be able to visualize iterative evolutions of your image, in a style similar to below (iterations 1 through 20):Note how the features of the stylistic image become stronger with time, particularly along sharply defined, high-contrast elements of the image. These juxtapose against the more muddled colors that take place of the original black background, which may have been inferred from the background of the original painting.I encourage you to run the code with your own images, and play with the weights to achieve interesting results. To finish, below you can see a collage of of the same MRI with multiple different styles applied to it.We hope you’ve found this tutorial interesting and fun. Next time, we automate the generation of ancient script through Generative Adversarial Networks!ReferencesGatys et al., A Neural Algorithm of Artistic StyleChollet, Deep Learning with Python",04/02/2019,8,6.0,5.0,1177.0,429.0,6.0,0.0,0.0,5.0,en
4218,An Overview of Different Text Embedding Models,The Ezra Tech Blog,Maryam Fallah,53.0,12.0,2610.0,"Embeddings are an important component of natural language processing pipelines. They refer to the vector representation of textual data. You can think of embeddings as a transformation from human-readable text to computer-readable numbers or vectors as seen in Fig. 1. These embeddings can be used in any machine learning task that takes text as the input, e.g. question answering, classification, text generation.Different embedding techniques vary in their complexity and capabilities. For instance, the most simple form of word embeddings can be represented with one-hot encodings where each word in the corpus of size V is mapped to a unique index in a vector of the same size. This gives us a vector of all zeros except for one element that indicates the word. For example, let’s assume that the vocabulary only consists of three words: “the”, “dog” and “barks”. We can encode each word by a vector of length 3 where for each word a different index has the value one and the rest are zero as seen below.The limitations of this approach are quite obvious. To name a few, the vector representations are very sparse and can be large given a comprehensive corpus; also, the relation between words cannot be captured. Therefore, some form of dimensionality reduction where latent semantics of the text is captured is necessary.The ability of the embedding model to effectively represent textual data is in direct relation to the overall performance of the machine learning task whether it is sentiment analysis, auto-complete applications, or adverse drug event detection.In this article, we will discuss the general framework of constructing embedding models and go through details of how the most well-known models work. We will also include code snippets for how to use each model in Python.Like any other machine learning model, the performance of the embedding models is heavily dependent on the training data. The structure, style, and vocabulary used in text data may vary based on the domain. For example, the words used in a scientific research paper are very different from the vocabulary and spelling used in tweets. Therefore, depending on the application of embedding models, one has to choose appropriate data. We will elaborate more on this matter in a later article.Text data can be very noisy and a robust pre-processing procedure is necessary before training the model. Similar to the data gathering step, this step is also very domain-specific. For example, in clinical text, converting all characters to lowercase letters can cause ambiguity with certain acronyms, e.g. ADD (attention deficit disorder) will be converted to the verb add.After data has been gathered and pre-processed, a model is fit on the data. Depending on the architecture of the model, there will be a number of hyper-parameters that need to be tuned to optimize model performance. Some common hyper-parameters for embedding models are the window size that determines how many neighboring words to include, the minimum frequency of words for it to be included in the vocabulary, and the dimension of the output vectors.The performance of the embedding model can be assessed using two methods: intrinsic and extrinsic methods.Intrinsic evaluation focuses on the ability of the model to learn the semantic and syntactic relationships between words. In other words, its understanding of the domain language or context.Extrinsic evaluation is a measure of the embedding model’s performance on downstream tasks such as Named Entity Recognition, text classification, question answering, etc.In this section, we will go through a brief overview of some of the well-known embedding models that are trained and used on general text data.Word2Vec is one of the earliest word embedding models. Proposed by Mikolov et al., Word2Vec is a rather simple and shallow (3 layers) neural network with two modes to learn word representations from large unlabeled data. The two training modes are called Continuous Bag Of Words (CBOW) and Skip-gram. As seen in Fig. 3, these two methods are essentially the reverse of each other.In both methods, all the words in the vocabulary are one-hot encoded and a window size is defined to consider a fixed number of words; in the figure below let’s assume there are only 7 words in the vocabulary and the window size is 3; then, with CBOW the goal is to predict the center word in each window based on its surrounding words. Once a prediction is made, the difference between the predicted vector and the ground truth is backpropagated to update two weight matrices. These weights determine the embedding of a given word at inference time.On the other hand, with the skip-gram method, the goal is to predict the context words from the target (central) word as seen in the figure below. The models have been trained on Google News 6B dataset.Either of these training methods can be used on custom data. Generally, Skip-gram is better with a larger corpus and is slower, while CBOW is more suitable if the corpus is small or speed is a factor in your training pipeline.Word2Vec although does a great job at capturing some intricacies, it has its limitations; mainly, it has a single vector representation for a word regardless of the different meanings it may have depending on the context. Another problem with this model is that it does not handle out of vocabulary words since it only learns the representations of the words that are repeated above a certain threshold in its predefined corpus. This can especially be problematic for rare words or words that are specific to a domain, e.g. medicine.Word2Vec in PythonYou can load a pre-trained Word2Vec model or train one on your own data. We will load the pre-trained version from gensim. We are going to look into a famous example where simple algebra can be performed on word embedding to calculate another word.In this example, we take away the vector representation of man from king and add the embedding of woman to the result and see what word embeddings are closest to the output embedding. As seen in the snippet below, the closest result is the embedding for the word queen.This shows that context is captured in the Word2Vec model. However, the function most_similar returns the top most similar words other than the words in the query itself, i.e. any word excluding king, man, and woman. However, we get different results if we compute the cosine similarity between king’s embedding and the embedding of women - man. The snippet below shows that since the words man and woman are close to each other in the vector space, the negative sign behind the embedding of man essentially cancels these two vectors out, leaving us with the result having a higher similarity with king (85%) as opposed to queen (72%). In other words, king - man + woman = kingGloVe is an embedding method introduced by the Stanford NLP Group. The main difference between GloVe and Word2Vec is that a), unlike Word2Vec which is a prediction-based model, Glove is a count-based method and b) Word2Vec only considers the local properties of the dataset whereas GloVe considers the global properties in addition to local ones. Let’s elaborate on this.GloVe creates a weighted, distance-based co-occurrence matrix, V ×V where V is the size of the vocabulary. With a predefined context window (similar to Word2Vec), each entry Xij in this matrix corresponds to the word i occurring within a distance of word j. If i and j occur together with no words in between, Xij is 1, if there is one word in between i and j, the value becomes ½, if two words are in between the value is ⅓ and so on until the maximum context window is reached (see Fig. 4). This distance-based count allows for words that occur sequentially to be valued higher than those that do not.However, with a large corpus, it is expected that most cells in the co-occurrence matrix be 0 and the non-zero cells have large values, i.e. the distribution will be long-tailed. In order to mitigate this, 1 is added to each entry and then its log is computed.In order to further encode the relevance of words appearing in the same context, each Xij entry is weighted based on its value as seen in Fig. 5.This constructed co-occurrence matrix becomes GloVe’s target matrix. During training, matrix factorization with gradient descent is used to predict the values of this co-occurrence matrix. As seen in Fig. 6, the big square co-occurrence matrix (R) on the left side of the equation can be broken down into two smaller matrices, P and Q. At each training time step, the predicted value for the entry Xij is computed by Eq. 1 and then the prediction error is used to update the values of the two matrices.These trained matrices can be interpreted as the embeddings for each word. For example, one can take each row i of matrix P or each column i of matrix Q as the embedding of word i. Alternatively, the average of these two vectors can be used as the embedding of word i. The reasoning behind this is that each co-occurrence Xij in R can be approximated by Eq. 1 , and after training, the correlation between the occurrence of word i with other words is projected in the correlation of vectors in the ith row and ith columns of matrices P and Q respectively.GloVe has been trained on five corpora of different sizes: 2010 & 2014 Wikipedia dumps, Gigaword 5 which has 4.3 billion tokens, and web data using Common Crawl consisting of 42 billion tokens. Note that similar to Word2Vec, GloVe also has limitations when it comes to out of vocabulary (OOV) words.GloVe in PythonIn the snippet below the used GloVe model has been trained on Wikipedia 2014 and Gigaword 5. The model can be downloaded from here. In the snippets below we repeat the example shown with Word2Vec.Computing the cosine similarity between the word embeddings of king and woman - man, shows that the result has a higher similarity to king than to queen (0.86 vs 0.76).In early 2017, Facebook AI Research published a paper introducing a more capable embedding method, FastText. This method is built on top of the Skip-gram method but mitigates the limitation of out of vocabulary words.Handling Out of Vocabulary WordsIn order to generate embeddings for words outside of the trained vocabulary, FastText breaks down words into a smaller sequence of characters called n-grams. For example, for n = 3 the 3-grams of the word dog become: “<do”, “dog”, “og>” and a special sequence “<dog>” denoting the entire word. This method is effective because it learns representations of subwords that are shared among different words, and therefore an unseen word is dissected into its composing n-grams which very likely have been seen during training. The final word embedding is computed as the sum of its constituent n-gram embeddings.The training process of FastText is similar to the Skip-gram approach in Word2Vec except that the input target word is broken down into its n-grams and each n-gram is presented by a unique id. Wikipedia dumps of 9 different languages, Arabic, Czech, German, English, Spanish, French, Italian, Romanian and Russian have been used for training.With the FastText approach, the model learns more information about the morphological transitions of a word, e.g. how a certain prefix can change the meaning of any word it precedes. It also is more robust to misspellings and can more accurately represent the embeddings of rare or unseen words.Although FastText has the benefit of being trained on multiple languages and mitigates the OOV issue of Word2Vec and GloVe; it has the same issue of generating a single embedding for a word regardless of its contextual meaning.FastText in PythonFastText has made some word representations available. However, we cannot see FastText’s ability to embed OOV words in this case. Therefore, we have followed the steps proposed here to train a FastText model using the first 1 billion bytes of English Wikipedia. We then find a word that does not exist in the training data, e.g. hyperintensity, and see if the model produces an embedding based on its n-grams. Then, we find the top similar words to this word to see if any n-gram overlap (that potentially can be inferred as similarity in meaning) exists. As seen in the results below, words such as intensity, high-intensity were found to be close to this unseen example.So far we have discussed embeddings that capture the similarity between different words, but map each word to a single vector. In 2018, Clark et al. introduced an embedding model that generates word embeddings based on the context.ELMo interprets the context by considering the entire input sentence forwards and backwards using 2 layers of bi-directional language models (biLM). In Fig. 7 the architecture of ELMo is shown.The input words are converted into a sequence of ids and embedded using CNNs. At each time step, the vector representation of a given word is passed to the forward and backward LSTM layer with residual connections. The outputs from these two LSTMs are further passed to another forward and backward LSTM layer. Finally, there is a softmax layer used to predict the next word from the forward pass network and the previous word from the backward network. The contribution of each intermediate layer in the final prediction is weighted and normalized during training. The 1B Word Benchmark has been used for training.During inference, as seen in Fig. 8, ELMo passes the input sequence of words, e.g. “The dog barks” to its forward and backward networks and concatenates the outputs of each forward layer with its corresponding backward layer and weights each concatenated vector by the learned weights from training. The summation of the three weighted concatenations gives the final word embedding.In the example shown in Fig. 8, we focus on the embedding of the word “dog” and have grayed out the layers and embeddings for the other words. The intermediate embeddings of each forward and backward layer, circled in red, are concatenated, multiplied by trained weights, and added to give a final embedding vector.ELMo was one of the first models to incorporate LSTMs in order to capture more contextual information. However, it was not designed for transfer learning and needs to be trained for specific tasks using a separate model.ELMo in PythonELMo is available on tfhub. In the code snippet below, we pass two sentences to ELMo for embedding. The word bank has a different meaning between the first sentence and the other two. We want to verify that ELMo uses different embeddings for each and that the embeddings of the word bank in the last two where they both refer to a financial institution are similar to each other and very different from the embedding of bank in the first sentence which refers to the river.In this article, we have discussed a number of embedding methods. Although each model has progressively mitigated its predecessor’s limitations, they all share the same constraints. First, a separate model specific to the NLP task at hand is needed in order to use any of these models for different applications. Second, all the models discussed use a rather small fixed context window, and therefore information from word sequences presented in earlier or later sentences is lost. ELMo tries to fix this by using bi-directional LSTMs, but note that the forward and backward networks are trained independently and the sequential nature of LSTMs, where only one word is considered at each time step still suffers from some information loss (especially with longer input sequences).Transformers are models that can mitigate these issues. In the next article, we will go through the most common transformer-based language models such as BERT and GPT.",05/03/2021,0,18.0,46.0,1082.0,629.0,9.0,0.0,0.0,9.0,en
4219,EQuake,NYT Open,The NYT Open Team,6000.0,3.0,435.0,"By RILEY DAVIS and DAVID SOUTHERRiley Davis and David Souther collaborated on EQuake, a 3D earthquake visualizer they developed in about a day. They discuss their motivations and approach in this piece.We were inspired by this xkcd comic imagining a situation in which tweets about an earthquake spread faster than the earthquake’s seismic waves.We both like to make complex scientific information more accessible by tying it to scales that people already understand. We thought it would be interesting to plot waves from real earthquakes onto a globe to show how fast the waves actually travel through the crust. This tool could easily be used to map out tweets (or any other geographic data) when other earthquakes occurred. To implement this, we were able to use data from the U.S. Geological Survey (USGS) along with Three.js and S3age.We took a day’s worth of earthquake incidents from the USGS database. The USGS data is tagged with (lat, lon, time, magnitude), in addition to other data we disregarded. We placed 3D markers on the surface of a globe, changing their size and color based on magnitude.Three.js handles the rendering and modeling of the project. However, we found the Three.js API and sample code to be more complicated than we needed. David had already started the S3age library, a wrapper API that streamlines creating a Three.js scene. We used it, resulting in simpler initialization code.We created a Globe base class; it provides the following method:This places a marker at the correct location on the globe’s surface. The marker’s local 3D space is oriented with the Z-axis pointing out along the radius, the Y-axis pointing to the North Pole and the X-axis perpendicular to the two, pointing east.In JavaScript, Object.create works as well to extend THREE.Object3D. This allows us to further extend the Quake class to build up more complex geometries and 3D pieces from the Three.js primitives. S3age also provides a physics mechanism, calling update: (clock)-> on any object with an update method in the S3age scene.In the application, the Earth extends the Globe class. It adds a sphere surface textured with a NASA Earth texture. With this setup, the rendering is handled by S3age — the EQuake app only handles the 3D geometry and materials of the markers. S3age also provides the controls for the scene. A library that can easily and quickly orient along the surface of the globe will be a boon for future visualization projects.You can find the code for EQuake here. S3age is documented, and Three.js is a great library, even with its API issues. Please take a look and contribute to the project!",30/09/2013,3,1.0,3.0,480.0,358.0,1.0,0.0,0.0,6.0,en
4220,GANGogh: Creating Art with GANs,Towards Data Science,Kenny Jones,112.0,13.0,2698.0,"Introduction:The work here presented is the result of a semester long independent research performed by Kenny Jones and Derrick Bonafilia (both Williams College 2017) under the guidance of Professor Andrea Danyluk. The code associated with this project can be found at https://github.com/rkjones4/GANGogh. Kenny and Derrick are both heading to Facebook next year as Software Engineers and hope to continue studying GANs in whatever capacity is available to them.Background:Generative Adversarial Networks (GANS) were introduced by Ian Goodfellow et. al. in a 2014 paper. GANs address the lack of relative success of deep generative models compared to deep discriminative models. The authors cite the intractable nature of the maximum likelihood estimation that is necessary for most generative models as the reason for this discrepancy. GANs are thus set up to utilize the strength of deep discriminative models to bypass the necessity of maximum likelihood estimation and therefore avoid the main failings of traditional generative models. This is done by using both a generative and a discriminative model that compete with each other to train the generator. In essence, “The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles.” In more concrete terms:“To learn the generator’s distribution Pg over data x, we define a prior on input noise variables Pz(z), then represent a mapping to data space as G(z; θg), where G is a differentiable function represented by a multilayer perceptron with parameters θg. We also define a second multilayer perceptron D(x; θd) that outputs a single scalar. D(x) represents the probability that x came from the data rather than Pg. We train D to maximize the probability of assigning the correct label to both training examples and samples from G. We simultaneously train G to minimize log(1 − D(G(z))). In other words, D and G play the following two-player minimax game with value function V (G, D): min G max D V (D, G) = Ex∼Pdata(x) [log D(x)] + Ez∼Pz(z) [log(1 − D(G(z)))]. (1)”The game is played by training G and D alternately until the Nash Equilibrium is reached and the samples G produces are indistinguishable from the samples in the dataset. This formulation is immediately suspect due to the fact that the paper’s authors explain that this method results in saturated gradients, and therefore they use a different formulation when actually training. Nevertheless, at the time of publication this formulation produced some of the most promising generative samples of any model that had been released, and its general structure allowed others to tweak the design to create more nuanced instances of the problem (Goodfellow et. al. 2014).The Auxiliary Classifier GAN(AC-GAN) was introduced by Augustus Odena et. al. in 2016. AC-GAN adds a classifier layer to the discriminator and a conditioning vector to the generator. The discriminator is then trained to minimize classification error in addition to the traditional GAN objective of real vs. fake. This approach allows labels to be leveraged and provides additional information to the GAN. In addition, it allows the generator to be conditioned to produce certain classes of samples. At present, this is probably the most effective conditional GAN and tends to do better than GANs using unlabeled data. Transfer learning and multitask learning are leveraged by this model (Odena et. al. 2016).StackGAN was introduced by Zhang et. al. in 2016. StackGAN uses feature information retrieved from a Recurrent Neural Net and a two phase image generation process — -the first phase creates a low resolution image from a noise vector and the second phase uses an encoding of the first image to create high resolution image. The feature information is used to condition both discriminators and both generators. This model creates state of the art high resolution samples.The Wasserstein GAN(WGAN) was introduced by Martin Arjovsky et. al. in 2017 and introduced a new theoretical framework for GANs that is grounded in theory and empirical findings. With this new framework they create the WGAN which minimizes an approximation of the wasserstein distance, which they found to have useful theoretical properties for the problem of generative modeling, as it creates nicer gradients in the discriminator that the generator can learn from more easily (Arjovsky et. al. 2017). The way they enforced some of their constraints was shown to be inconsistent and was improved upon by Ishaan Gulrajani using a gradient based penalty to constrain the slope of the discriminator. This model was found to be extremely robust and effective at training a variety of GANs ranging from the usual model to previously untrainable models (Gulrajani 2017).Wavenet and Conditional PixelCNN are two generative models that came out of Google in 2016. Neither are GANs, but both utilize an interesting idea called global conditioning. The key to global conditioning is using the conditioning vector not as a one time addition to the noise vector at the beginning, but instead as a conditioning vector to each activation function in the neural net. This allows much stronger and more complex conditioning than the traditional method. They also both use gated multiplicative activation functions which seem to mesh well with this global conditioning (van den Oord 2016; van den Oord 2016).Goal:Our primary motivation in studying GANs this semester was to try to apply a GAN-derived model to the generation of novel art. Much of the work in deep learning that has concerned itself with art generation has focused on style, and specifically the style of particular art pieces. In papers such as A Neural Algorithm of Artistic Style, deep learning nets learn to 1) differentiate the style of a piece of art from its content and 2) to apply that style to other content representations. By building off of the GAN model, we hoped to build a deep-net that was capable of not only learning a distribution of the style and content components of many different pieces of art, but was also able to novelly combine these components to create new pieces of art. The task of novel content generation is much more difficult than applying the style from one particular piece of art to the content of another.Dataset and Collection:As the generation component of a GAN can only learn to represent the real distribution that is presented to the discriminator, choosing an appropriate dataset of paintings was a crucial decision for us. We at first experimented with using a dataset of paintings that were only from one artist: Monet. After a few initial tests we found that our models on this dataset were converging poorly as the dataset with only 1200 paintings was too small. We needed to find a way of increasing the size of our dataset, and so we turned to the wikiart database, which is a collection of over 100,000 paintings all labeled on style, genre, artist, year the painting was made, etc (wikiart). From this dataset we knew that we would 1) have enough data to make it likely for our model to converge and 2) that we would be able to use the labeling information to improve the learning capabilities of our net: in GAN scenarios it is often helpful to use extra conditional information when training, especially when learning a distributions with clear sub-distributions (i.e. landscapes vs portraits). In the final version of our model, we constructed our dataset by creating python scripts to scrape over 80,000 individual images directly off of the wikiart website, while retaining both the style and genre labels of each image.Model:In designing our model we wanted to leverage the power of the improved wasserstein metric in an extension of the typical AC-GAN framework. At a high-level this means that we initially set up our discriminator and generator under the improved wasserstein model, but in addition we added a classifying component to our discriminator where it would also try to predict the genre of a given painting. Our generator was then augmented to take in a one-hot-vector of label data whose purpose would be to influence the genre of the produced painting. We could enforce this metric by adding a penalizing term to our discriminator’s cost function that tries to minimize the cross-entropy in its prediction of genre versus the real genre of a given painting, and adding a penalizing term to our generator that tries to minimize the cross-entropy of the discriminator’s prediction versus the genre it was instructed to make based on the conditioning vector.Two other modifications we made to the standard GAN model was the inclusion of pretraining and global conditioning. Both of these additions were made after we observed a tendency for our model to focus heavily on producing ‘real’ versus ‘fake’ images in the first iterations, and only later learning to provide different kinds of representations when conditioning on genre. We hypothesized that one of the reasons for this was that when beginning training, our discriminator had not learned the difference between genres at all, and as a result the generator was unable to start differentiating as early as we would like. Our proposed solution was to pretrain our discriminator, before any generator training, and in this way give our discriminator an initial idea of the differences between genres. In the vanilla GAN formulation, this would lead to problematic behavior and a low chance of convergence, but as we use the wasserstein metric, which limits the power of the discriminator and always provides smooth gradients for the generator, we actually reach a more optimal state under this framework: as in an ‘optimal’ formulation of the GAN setup we would train the discriminator to convergence before each generator training step.Our inclusion of global conditioning was once again included to allow our generator to better differentiate its produced content as a result of its conditioning vector. The idea behind global conditioning is that every time we perform an activation function between layers in our network, we can use the conditioning vector to influence how this activation occurs, and this conditioning vector can be used differently on a layer by layer basis. We chose to use the conditional gated multiplicative activation function from Conditional PixelCNN and Wavenet. To our knowledge this is the first time that global conditioning has been applied to the GAN framework.Detailed Architecture:Generator:Discriminator:Results:Here we present a series of discriminator selected photos from three of our genres.Flowers:Landscapes:Portraits:Due to the nature of our discriminator, we were able to use it to select ‘good’ images that our generator produced. Specifically, as our discriminator was able to both classify and judge the realness of an image, we can produce a set of generator images for a specific genre, and from those images ask the discriminator to choose images that it 1) classifies as that genre with high confidence and 2) also judges has a high realness value. This is the method by which the above images were produced.We can also, under a static noise vector, see how the generator did in differentiating between genres by changing the given conditioning information. These images for all genres can be seen in the Appendix. Here notice that while the generator has definitely learned to produce different kinds of genres, they are not as differentiated as we may like. Also there is a high variance in the quality of the produced art, where some images produced are much better than others, and even for a given noise vector, it is more realistically represented in specific genres. We believe the reason for this phenomenon is once again that it is much easier for our model to learn representations of ‘real’ versus ‘fake’ art rather than producing art in a given ‘genre’. In adding pretraining and global conditioning we alleviated this problem somewhat, but clearly have not fixed it completely. Another challenge we faced is that some genres of art are much more complex or have a higher-variance than others. In particular this led to poor performance in our generator when producing certain genres such as Mythological paintings and Animal paintings, we hypothesize though that given larger datasets our model should also be able to produce realistic examples of even these genres.Although our primary motivation in this project was the generation of images, we were also able to measure the effectiveness of our discriminator by evaluating it as a classifier. We measured our classifying ability both on our training set and a test set 1/20th the size of our training set, and managed to get a very good classifying accuracy of above 90% on genre in our test set. We note that while our classifying accuracy is not strictly rigorous it compares favorably to other classification metrics performed on the same data set, Wai Ren Tan got a classification accuracy of 74.14% on approximately the same dataset in 2016 (Our dataset is just slightly different because we scraped the wikiart website at different points in time) (Tan 2016).Future Work:A substantial limiting factor in our project was our access to computational resources. Working with GPU’s that had only 2GB of RAM we were forced to stick to producing 64x64 pixel images, as any higher resolution forced significant reductions in both our batch size and dimensionality of the model. We noticed significant improvement in our model when comparing the generated images from our 64x64 models to our initial 32x32 models, so we hypothesize that with additional computational resources, that allow scaling to 128x128 images, the quality of the images would also substantially increase under our current architecture. In thinking about scaling the problem of art generation to even bigger image sizes such as 256x256, we believe that an architecture influenced by StackGAN, using two of our proposed GAN models could work very well. One substantial benefit that we believe such an architecture could give is that our stage I GAN could serve as a way of forcing differentiation between the different genres, and thus the stage II GAN when conditioned on stage I images will be better suited to overcome the difficulties we saw in our model where noise vectors with different conditioning information map to similar images. We also note that this two-tiered GAN model could be improved with additional conditioning information, specifically we suggest that in addition to our genre conditioning set-up described in our model, another one hot vector with style information could be used in the input to both the stage II discriminator and the stage II generator, and then further division of generated art combinations could be explored such as the renaissance portrait versus the abstract portrait.Conclusion:While we do not believe that our model ‘solves’ the problem of art generation, we hope to have offered insights into the ways in which GANs can be used to generate novel art. Specifically, we have shown that under the right conditioning metrics, wasserstein-style GANs are able to take into account conditioning information in a useful way for image distributions that are ‘hard’. Moreover, we have shown that global conditioning is a valid addition to the GAN framework. We also point to our discriminator selected images for specific genres to validate this line of research, as it is clear our generator is doing an excellent job of producing novel art for specific genres.Works Cited:Arjovsky, Martin, Soumith Chintala, and Léon Bottou. “Wasserstein gan.” arXiv preprint arXiv:1701.07875 (2017).Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in neural information processing systems. 2014.Gulrajani, Ishaan, et al. “Improved Training of Wasserstein GANs.” arXiv preprint arXiv:1704.00028 (2017).Odena, Augustus, Christopher Olah, and Jonathon Shlens. “Conditional image synthesis with auxiliary classifier gans.” arXiv preprint arXiv:1610.09585 (2016).Tan, Wei Ren, et al. “Ceci n’est pas une pipe: A deep convolutional network for fine-art paintings classification.” Image Processing (ICIP), 2016 IEEE International Conference on. IEEE, 2016.van den Oord, Aäron, et al. “Wavenet: A generative model for raw audio.” CoRR abs/1609.03499 (2016).van den Oord, Aaron, et al. “Conditional image generation with pixelcnn decoders.” Advances in Neural Information Processing Systems. 2016.Wikiart. https://www.wikiart.org/Zhang, Han, et al. “StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.” arXiv preprint arXiv:1612.03242 (2016).Appendix:Images produced with same noise vector but different conditioning vector.Abstract:Animal-Painting:City-Scapes:Figurative:Flower Paintings:Genre-Painting:Landscape:Marina:Mythological-Painting:Nude-Painting:Portrait:Religious-Painting:Still-Life:Symbolic-Painting:",18/06/2017,0,11.0,9.0,753.0,459.0,20.0,0.0,0.0,3.0,en
4221,The Measure of a Measure,Artificial Intelligence in Plain English,Charlie Kufs,267.0,6.0,815.0,"If you can measure a phenomenon, you can analyze the phenomenon. But if you don’t measure the phenomenon accurately and precisely, you won’t be able to analyze the phenomenon accurately and precisely. So in planning a statistical analysis, once you have specific concepts you want to explore you’ll need to identify ways the concepts could be measured.Start with conventional measures, the ones everyone would recognize and know what you did to determine. Then, consider whether there are any other ways to measure the concept directly. From there, establish whether there are any indirect measures or surrogates that could be used in lieu of a direct measurement. Finally, if there are no other options, explore whether it would be feasible to develop a new measure based on theory.Keep in mind that developing a new measure or a new scale of measurement is more difficult for the experimenter and the personnel who have to generate the data, and less understandable for reviewers than using an established measure.Say, for example, that you wanted to assess the taste of various sources of drinking water. You might use standard laboratory analysis procedures to test water samples for specific ions known to affect taste, like iron and sulfate. These would be direct measures of water quality. An example of an indirect measure would be total dissolved solids, a general measure of water quality that includes many dissolved ions besides iron and sulfate. An example of a surrogate measure would be the water’s electrical conductivity, which is positively correlated to the quantity of dissolved ions in the water. Electrical conductivity is easier and less expensive to measure than dissolved solids, which is easier and less expensive to measure than specific analytes like iron and sulfate.Developing a new measure based on theory might also be useful. Sometimes it’s beneficial to think out of the box. That’s how sabermetrics got started. So for example, you might use professional taste testers to judge the tastes of the waters. Or, you might conduct comparison surveys of untrained individuals. Clearly, what you measure and how you measure it will have a great influence on your findings.Of the possible measures you identify, select scales-of-measurement and consider how difficult it would be to generate accurate and precise data. Measurement bias and variability are introduced into a data value by the very process of generating the data value. It’s like tuning an analog radio. Turn the tuning dial a bit off the station and you hear more static. That’s more variance in the station’s signal. Every measurement can be thought of consisting of three elements:Consider the examples of data types shown in the following table. For any particular data type, all three of these elements change over time. Benchmarks change when new measurement technologies are developed or existing meters, gauges and other devices become more accurate and precise. Standardized tests, like the SAT, change to safeguard the secrecy of questions. Likewise, processes change over time to improve consistency and to accommodate new benchmarks. Judgments improve when data collectors are trained and gain work experience. Such changes can create problems when historical and current data are combined because variance differences attributable to evolving measurement systems can produce misleading statistics.Understanding these three facets of measurements is important because it will help you select good measures and measurement scales for a phenomenon, as well as decide how to control extraneous variability in data collection. For example:There’s a special type of analysis aimed at evaluating measurement variance called Gage R&R. The R&R part refers to:Gage R&R is a fundamental type of analysis in industrial statistics, where meeting product specifications requires consistent measurements, but it can be used for any measurement system from medical testing to opinion surveys.Consider building some redundancy into your variables if there is more than one way to measure a concept. Sometimes one variable will display a higher correlation with your model’s dependant variable or help explain analogous measurements in a related measure. For example, redundant measures are often included in opinion surveys by using differently worded questions to solicit the same information. One question might ask “Did you like [something]?” and then a later question might ask “ Would you recommend [something] to your friends?” or “ Would you use [something] again in the future?” to assess consistency in a respondent’s opinion about a product.Finally, take into account your objective and the ultimate use of your statistical models. For example, if you want to predict some dependent variable, quantitative independent-variables would usually be preferable to qualitative-variables because they would provide more scale resolution. Furthermore, you could dumb-down a quantitative variable to a less finely divided scale or even a qualitative scale but you usually can’t go in the other direction. If you want your prediction model to be simple and inexpensive to use, don’t select predictors that are expensive and time-consuming to measure.Originally published at http://statswithcats.net on September 12, 2010.",12/09/2010,0,18.0,15.0,822.0,712.0,7.0,3.0,0.0,6.0,en
4222,Tutorial on Graph Neural Networks for Computer Vision and Beyond,Medium,Boris Knyazev,579.0,17.0,3096.0,"I’m answering questions that AI/ML/CV people not familiar with graphs or graph neural networks typically ask. I provide PyTorch examples to clarify the idea behind this relatively new and exciting kind of model.The questions addressed in this part of my tutorial are:To answer them, I’ll provide motivating examples, papers and Python code making it a tutorial on Graph Neural Networks (GNNs). Some basic knowledge of machine learning and computer vision is expected, however, I’ll provide some background and intuitive explanation as we go.First of all, let’s briefly recall what is a graph? A graph G is a set of nodes (vertices) connected by directed/undirected edges. Nodes and edges typically come from some expert knowledge or intuition about the problem. So, it can be atoms in molecules, users in a social network, cities in a transportation system, players in team sport, neurons in the brain, interacting objects in a dynamic physical system, pixels, bounding boxes or segmentation masks in images. In other words, in many practical cases, it is actually you who gets to decide what are the nodes and edges in a graph.In many practical cases, it is actually you who gets to decide what are the nodes and edges in a graph.This is a very flexible data structure that generalizes many other data structures. For example, if there are no edges, then it becomes a set; if there are only “vertical” edges and any two nodes are connected by exactly one path, then we have a tree. Such flexibility is both good and bad as I’ll discuss in this tutorial.In the context of computer vision (CV) and machine learning (ML), studying graphs and the models to learn from them can give us at least four benefits:As my previous research was related to recognizing and analyzing faces and emotions, I particularly like this figure below.To answer this question, I first give some motivation for using convolution in general and then describe “convolution on images” using the graph terminology which should make the transition to “convolution on graphs” more smooth.Let’s understand why we care about convolution so much and why we want to use it for graphs. Compared to fully-connected neural networks (a.k.a. NNs or MLPs), convolutional networks (a.k.a. CNNs or ConvNets) have certain advantages explained below based on the image of a nice old Chevy.First, ConvNets exploit a natural prior in images, more formally described in (Bronstein et al., 2016), such as:Second, the number of trainable parameters (i.e. filters) in convolutional layers does not depend on the input dimensionality, so technically we can train exactly the same model on 28×28 and 512×512 images. In other words, the model is parametric.Ideally, our goal is to develop a model that is as flexible as Graph Neural Nets and can digest and learn from any data, but at the same time we want to control (regularize) factors of this flexibility by turning on/off certain priors.All these nice properties make ConvNets less prone to overfitting (high accuracy on the training set and low accuracy on the validation/test set), more accurate in different visual tasks, and easily scalable to large images and datasets. So, when we want to solve important tasks where input data are graph-structured, it is appealing to transfer all these properties to graph neural networks (GNNs) to regularize their flexibility and make them scalable. Ideally, our goal is to develop a model that is as flexible as GNNs and can digest and learn from any data, but at the same time we want to control (regularize) factors of this flexibility by turning on/off certain priors. This can open research in many interesting directions. However, controlling of this trade-off is challenging.Let’s consider an undirected graph G with N nodes. Edges E represent undirected connections between nodes. Nodes and edges typically come from your intuition about the problem. Our intuition in the case of images is that nodes are pixels or superpixels (a group of pixels of weird shape) and edges are spatial distances between them. For example, the MNIST image below on the left is typically represented as an 28×28 dimensional matrix. We can also represent it as a set of N=28*28=784 pixels. So, our graph G is going to have N=784 nodes and edges will have large values (thicker edges in the Figure below) for closely located pixels and small values (thinner edges) for remote pixels.When we train our neural networks or ConvNets on images, we implicitly define images on a graph — a regular two-dimensional grid as the one on the figure below. Since this grid is the same for all training and test images and is regular, i.e. all pixels of the grid are connected to each other in exactly the same way across all images (i.e. have the same number of neighbors, length of edges, etc.), this regular grid graph has no information that will help us to tell one image from another. Below I visualize some 2D and 3D regular grids, where the order of nodes is color-coded. By the way, I’m using NetworkX in Python to do that, e.g. G = networkx.grid_graph([4, 4]).Given this 4×4 regular grid, let’s briefly look at how 2D convolution works to understand why it’s difficult to transfer this operator to graphs. A filter on a regular grid has the same order of nodes, but modern convolutional nets typically have small filters, such as 3×3 in the example below. This filter has 9 values: W₁,W₂,…, W₉, which is what we are updating during training using backprop to minimize the loss and solve the downstream task. In our example below, we just heuristically initialize this filter to be an edge detector (see other possible filters here):When we perform convolution, we slide this filter in both directions: to the right and to the bottom, but nothing prevents us from starting in the bottom corner — the important thing is to slide over all possible locations. At each location, we compute the dot product between the values on the grid (let’s denote them as X) and the values of filters, W: X₁W₁+X₂W₂+…+X₉W₉, and store the result in the output image. In our visualization, we change the color of nodes during sliding to match the colors of nodes in the grid. In a regular grid, we always can match a node of the filter with a node of the grid. Unfortunately, this is not true for graphs as I’ll explain later below.The dot product used above is one of so called “aggregator operators”. Broadly speaking, the goal of an aggregator operator is to summarize data to a reduced form. In our example above, the dot product summarizes a 3×3 matrix to a single value. Another example is pooling in ConvNets. Keep in mind, that such methods as max or sum pooling are permutation-invariant, i.e. they will pool the same value from a spatial region even if you randomly shuffle all pixels inside that region. To make it clear, the dot product is not permutation-invariant simply because in general: X₁W₁+X₂W₂ ≠X₂W₁+X₁W₂.Now let’s use our MNIST image and illustrate the meaning of a regular grid, a filter and convolution. Keeping in mind our graph terminology, this regular 28×28 grid will be our graph G, so that every cell in this grid is a node, and node features are an actual image X, i.e. every node will have just a single feature — pixel intensity from 0 (black) to 1 (white).Next, we define a filter and let it be a famous Gabor filter with some (almost) arbitrary parameters. Once we have an image and a filter, we can perform convolution by sliding the filter over that image (of digit 7 in our case) and putting the result of the dot product to the output matrix after each step.This is all cool, but as I mentioned before, it becomes tricky when you try to generalize convolution to graphs.Nodes are a set, and any permutation of this set does not change it. Therefore, the aggregator operator that people apply should be permutation-invariant.As I have already mentioned, the dot product used above to compute convolution at each step is sensitive to the order. This sensitivity permits us to learn edge detectors similar to Gabor filters important to capture image features. The problem is that in graphs there is no well-defined order of nodes unless you learn to order them, or come up with some heuristic that will result in a consistent (canonical) order from graph to graph. In short, nodes are a set, and any permutation of this set does not change it. Therefore, the aggregator operator that people apply should be permutation-invariant. The most popular choices are averaging (GCN, Kipf & Welling, ICLR, 2017) and summation (GIN, Xu et al., ICLR, 2019) of all neighbors, i.e. sum or mean pooling, followed by projection by a trainable vector W. See Hamilton et al., NIPS, 2017 for some other aggregators.For example, for the graph above on the left, the output of the summation aggregator for node 1 will be X₁=(X₁+X₂+X₃+X₄)W₁, for node 2: X₂=(X₁+X₂+X₃+X₅)W₁ and so forth for nodes 3, 4 and 5, i.e. we need to apply this aggregator for all nodes. In result, we will have the graph with the same structure, but node features will now contain features of neighbors. We can process the graph on the right using the same idea.Colloquially, people call this averaging or summation “convolution”, since we also “slide” from one node to another and apply an aggregator operator in each step. However, it’s important to keep in mind that this is a very specific form of convolution, where filters don’t have a sense of orientation. Below I’ll show how those filters look like and give an idea how to make them better.You know how a classical neural network works, right? We have some C-dimensional features X as the input to the net. Using our running MNIST example, X will be our C=784 dimensional pixel features (i.e. a “flattened” image). These features get multiplied by C×F dimensional weights W that we update during training to get the output closer to what we expect. The result can be directly used to solve the task (e.g. in case of regression) or can be further fed to some nonlinearity (activation), like ReLU, or other differentiable (or more precisely, sub-differentiable) functions to form a multi-layer network. In general, the output of some layer l is:The signal in MNIST is so strong, that you can get an accuracy of 91% by just using the formula above and the Cross Entropy loss without any nonlinearities and other tricks (I used a slightly modified PyTorch example to do that). Such model is called multinomial (or multiclass, since we have 10 classes of digits) logistic regression.Now, how do we transform our vanilla neural network to a graph neural network? As you already know, the core idea behind GNNs is aggregation over “neighbors”. Here, it is important to understand that in many cases, it is actually you who specifies “neighbors”.Let’s consider a simple case first, when you are given some graph. For example, this can be a fragment (subgraph) of a social network with 5 persons and an edge between a pair of nodes denotes if two people are friends (or at least one of them think so). An adjacency matrix (usually denoted as A) in the figure below on the right is a way to represent these edges in a matrix form, convenient for our deep learning frameworks. Yellow cells in the matrix represent the edge and blue — the absence of the edge.Now, let’s create an adjacency matrix A for our MNIST example based on coordinates of pixels (complete code is provided in the end of the post):This is a typical, but not the only, way to define an adjacency matrix for visual tasks (Defferrard et al., NIPS, 2016, Bronstein et al., 2016). This adjacency matrix is our prior, or our inductive bias, we impose on the model based on our intuition that nearby pixels should be connected and remote pixels shouldn’t or should have very thin edge (edge of a small value). This is motivated by observations that in natural images nearby pixels often correspond to the same object or objects that interact frequently (the locality principle we mentioned in Section 2.1.), so it makes a lot of sense to connect such pixels.So, now instead of having just features X we have some fancy matrix A with values in the range [0,1]. It’s important to note that once we know that our input is a graph, we assume that there is no canonical order of nodes that will be consistent across all other graphs in the dataset. In terms of images, it means that pixels are assumed to be randomly shuffled. Finding the canonical order of nodes is combinatorially unsolvable in practice. Even though for MNIST we technically can cheat by knowing this order (because data are originally from a regular grid), it’s not going to work on actual graph datasets.Remember that our matrix of features X has 𝑁 rows and C columns. So, in terms of graphs, each row corresponds to one node and C is the dimensionality of node features. But now the problem is that we don’t know the order of nodes, so we don’t know in which row to put features of a particular node. If we just pretend to ignore this problem and feed X directly to an MLP as we did before, the effect will be the same as feeding images with randomly shuffled pixels with independent (yet the same for each epoch) shuffling for each image! Surprisingly, a neural network can in principle still fit such random data (Zhang et al., ICLR, 2017), however test performance will be close to random prediction. One of the solutions is to simply use the adjacency matrix A, we created before, in the following way:We just need to make sure that row i in A corresponds to features of node in row i of X. Here, I’m using 𝓐 instead of plain A, because often you want to normalize A. If 𝓐=A, the matrix multiplication 𝓐X⁽ˡ⁾ will be equivalent to summing features of neighbors, which turned out to be useful in many tasks (Xu et al., ICLR, 2019). Most commonly, you normalize it so that 𝓐X⁽ˡ⁾ averages features of neighbors, i.e. 𝓐=A/ΣᵢAᵢ. A better way to normalize matrix A can be found in (Kipf & Welling, ICLR, 2017).Below is the comparison of NN and GNN in terms of PyTorch code:And HERE is the full PyTorch code to train two models above: python mnist_fc.py --model fc to train the NN case; python mnist_fc.py --model graph to train the GNN case. As an exercise, try to randomly shuffle pixels in code in the --model graph case (don’t forget to shuffle A in the same way) and make sure that it will not affect the result. Is it going to be true for the --model fc case?Here is the full PyTorch code to train two models.After running the code, you may notice that the classification accuracy is actually about the same. What’s the problem? Aren’t graph networks supposed to work better? Well, they are, in many cases. But not in this one, because the 𝓐X⁽ˡ⁾ operator we added is actually nothing else, but a Gaussian filter:So, our graph neural network turned out to be equivalent to a convolutional neural network with a single Gaussian filter, that we never update during training, followed by the fully-connected layer. This filter basically blurs/smooths the image, which is not a particularly useful thing to do (see the image above on the right). However, this is the simplest variant of a graph neural network, which nevertheless works great on graph-structured data. To make GNNs work better on regular graphs, like images, we need to apply a bunch of tricks. For example, instead of using a predefined Gaussian filter, we can learn to predict an edge between any pair of pixels by using a differentiable function like this:To make GNNs work better on regular graphs, like images, we need to apply a bunch of tricks. For example, instead of using a predefined Gaussian filter, we can learn to predict an edge between any pair of pixels.This idea is similar to Dynamic Filter Networks (Brabander et al., NIPS, 2016), Edge-conditioned Graph Networks (ECC, Simonovsky & Komodakis, CVPR, 2017) and (Knyazev et al., NeurIPS-W, 2018). To try it using my code, you just need to add the --pred_edge flag, so the entire command is python mnist_fc.py --model graph --pred_edge. Below I show the animation of the predefined Gaussian and learned filters. You may notice that the filter we just learned (in the middle) looks weird. That’s because the task is quite complicated since we optimize two models at the same time: the model that predicts edges and the model that predicts a digit class. To learn better filters (like the one on the right), we need to apply some other tricks from our BMVC paper, which is beyond the scope of this part of the tutorial.The code to generate these GIFs is quite simple:I’m also sharing an IPython notebook showing 2D convolution of an image with a Gabor filter in terms of graphs (using an adjacency matrix) compared to using circulant matrices, which is often used in signal processing.In the next part of the tutorial, I’ll tell you about more advanced graph layers that can lead to better filters on graphs.Update:Throught this blog post and in the code the dist variable should have been squared to make it a Gaussian. Thanks Alfredo Canziani for spotting that. All figures and results were generated without squaring it. If you observe very different results after squaring it, I suggest to tune sigma.Graph Neural Networks are a very flexible and interesting family of neural networks that can be applied to really complex data. As always, such flexibility must come at a certain cost. In case of GNNs it is the difficulty of regularizing the model by defining such operators as convolution. Research in that direction is advancing quite fast, so that GNNs will see application in increasingly wider areas of machine learning and computer vision.See another nice blog post about GNNs from Neptune.ai.Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of Mohamed Amer (homepage) and my PhD advisor Graham Taylor (homepage).Find me on Github, LinkedIn and Twitter. My homepage.If you want to cite this tutorial in your paper, please use:@misc{knyazev2019tutorial, title={Tutorial on Graph Neural Networks for Computer Vision and Beyond}, author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R}, year={2019}}",04/08/2019,2,20.0,105.0,826.0,381.0,17.0,3.0,0.0,68.0,en
4223,Smartphones vs Tablets: Does size matter?,DataWeave,DataCrafts @ DataWeave,174.0,3.0,361.0,"We have seen a steady increase in the number of smartphones and tablets since the last five years. Looking at the number of smartphones, tablets and now wearables ( smart watches and fitbits ) that are being launched in the mobiles market, we can truly call this ‘The Mobile Age’.We, at DataWeave, deal with millions of data points related to products which vary from electronics to apparel. One of the main challenges we encounter while dealing with this data is the amount of noise and variation present for the same products across different stores.One particular problem we have been facing recently is detecting whether a particular product is a mobile phone (smartphone) or a tablet. If it is mentioned explicitly somewhere in the product information or metadata, we can sit back and let our backend engines do the necessary work of classification and clustering. Unfortunately, with the data we extract and aggregate from the Web, chances of finding this ontological information is quite slim.To address the above problem, we decided to take two approaches.Here we will talk mainly about the second approach since it is more challenging and engaging than the former. To start with, we needed some data specific to phone models, brands, sizes, dimensions, resolutions and everything else related to the device specifications. For this, we relied on a popular mobiles/tablets product information aggregation site. We crawled, extracted and aggregated this information and stored it as a JSON dump. Each device is represented as a JSON document like the sample shown below.From the above document, it is clear that there are a lot of attributes that can be assigned to a mobile device. However, we would not need all of them for building our simple algorithm for labeling smartphones and tablets. I had decided to use the device screen size for separating out smartphones and tablets but I decided to take some suggestions from our team. After sitting down and taking a long, hard look at our dataset, Mandar had an idea of using the device dimensions also for achieving the same goal!Finally, the attributes that we decided to use were,To read the entire article on www.dataweave.com/blog click here.",04/08/2015,1,0.0,0.0,0.0,0.0,0.0,1.0,0.0,2.0,en
4224,Stochastic Gradient Descent for machine learning clearly explained,Towards Data Science,Baptiste Monpezat,23.0,7.0,926.0,"As you may know, supervised machine learning consists in finding a function, called a decision function, that best models the relation between input/output pairs of data. In order to find this function, we have to formulate this learning problem into an optimization problem.Let’s consider the following task: finding the best linear function that maps the input space, the variable X to the output space, the variable Y.As we try to model the relation between X and Y by a linear function, the set of functions that the learning algorithm is allowed to select is the following :The term b is the intercept, also called bias in machine learning. This set of functions is our hypothesis space.But how do we choose the values for the parameters a,b and how do we judge if it’s a good guess or not?We define a function called a loss function that evaluates our choice in the context of the outcome Y.We define our loss as a squared loss (we could have chosen another loss function such as the absolute loss) :The squared loss penalizes the difference between the actual y outcome and the outcome estimated by choosing values for the set of parameters a,b. This loss function evaluates our choice on a single point, but we need to evaluate our decision function on all the training points.Thus, we compute the average of the square of the errors: the mean squared error.where n is the number of data points.This function, which depends on the parameters defining our hypothesis space, is called Empirical risk.Rn(a,b) is a quadratic function of the parameters, hence it's minimum always exists but may not be unique.Eventually, we reached our initial goal: formulating the learning problem into an optimization one!Indeed, all we have to do is to find the decision function, the a,b coefficients, that minimize this empirical risk.It would be the best decision function we could possibly produce: our target function.In the case of a simple linear regression, we could simply differentiate the empirical risk and compute the a,b coefficients that cancel the derivative. It is easier to use matrix notation to compute the solution. It is convenient to include the constant variable 1 in X and write parameters a and b as a single vector β. Thus, our linear model can be written as :and our loss function becomes :The vector beta minimizing our equation can be found by solving the following equation :Our linear regression has only two predictors (a and b), thus X is a n x 2 matrix (where n is the number of observations and 2 the number of predictors). As you can see, to solve the equation we need to calculate the matrix (X^T X) then invert it.In machine learning, the number of observations is often very high as well as the number of predictors. Consequently, this operation is very expensive in terms of calculation and memory.Gradient descent algorithm is an iterative optimization algorithm that allows us to find the solution while keeping the computational complexity low. We describe how it works in the next part of this article.Gradient descent algorithm can be illustrated by the following analogy. Imagine that you are lost in the mountains in the middle of the night. You can’t see anything as it’s pitch dark and you want to go back to the village located in the valley bottom (you are trying to find the local/global minimum of the mean squared error function). To survive, you develop the following strategy :Eventually, you will reach the valley bottom, or you will get stuck in a local minimum …Now that you have understood the principle with this allegory, let’s dive into the mathematics of gradient descent algorithm!For finding the a, b parameters that minimize the mean squared error, the algorithm can be implemented as follow :Then update values of a and b by subtracting the gradient multiplied by a step size :with η, our fixed step size.Compute the mean squared loss with the updated values of a and b.Repeat those steps until a stopping criterion is met. For instance, the decrease of the mean squared loss is lower than a threshold ϵ.On the animation below, you can see the update of the parameter a performed by the gradient descent algorithm as well as the fitting of our linear regression model :As we are fitting a model with two predictors, we can visualize the gradient descent algorithm process in a 3D space!At every iteration of the gradient descent algorithm, we have to look at all our training points to compute the gradient.Thus, the time complexity of this algorithm is O(n). It will take a long time to compute for a very large data set. Maybe we could compute an estimate of the gradient instead of looking at all the data points: this algorithm is called minibatch gradient descent.Minibatch gradient descent consists in using a random subset of size N to determine step direction at each iteration.If we use a random subset of size N=1, it is called stochastic gradient descent. It means that we will use a single randomly chosen point to determine step direction.In the following animation, the blue line corresponds to stochastic gradient descent and the red one is a basic gradient descent algorithm.I hope this article has helped you understand this basic optimization algorithm, if you liked it or if you have any question don’t hesitate to comment!You can find the code I made to implement stochastic gradient descent and create the animations on my GitHub: https://github.com/baptiste-monpezat/stochastic_gradient_descent.You can also find the original post on my blog: https://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained",01/06/2020,0,15.0,43.0,1137.0,298.0,14.0,3.0,0.0,8.0,en
4225,Keyword Extraction with BERT,Towards Data Science,Maarten Grootendorst,3400.0,7.0,1082.0,"When we want to understand key information from specific documents, we typically turn towards keyword extraction. Keyword extraction is the automated process of extracting the words and phrases that are most relevant to an input text.With methods such as Rake and YAKE! we already have easy-to-use packages that can be used to extract keywords and keyphrases. However, these models typically work based on the statistical properties of a text and not so much on semantic similarity.In comes BERT. BERT is a bi-directional transformer model that allows us to transform phrases and documents to vectors that capture their meaning.What if we were to use BERT instead of statistical models?Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1, 2, 3, ), I could not find a simple and easy-to-use BERT-based solution. Instead, I decide to create KeyBERT a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings.Now, the main topic of this article will not be the use of KeyBERT but a tutorial on how to use BERT to create your own keyword extraction model.For this tutorial, we are going to be using a document about supervised machine learning:I believe that using a document about a topic that the readers know quite a bit about helps you understand if the resulting keyphrases are of quality.We start by creating a list of candidate keywords or keyphrases from a document. Although many focus on noun phrases, we are going to keep it simple by using Scikit-Learns CountVectorizer. This allows us to specify the length of the keywords and make them into keyphrases. It also is a nice method for quickly removing stop words.We can use n_gram_range to change the size of the resulting candidates. For example, if we would set it to (3, 3) then the resulting candidates would phrases that include 3 keywords.Then, the variable candidates is simply a list of strings that includes our candidate keywords/keyphrases.NOTE: You can play around with n_gram_range to create different lengths of keyphrases. Then, you might not want to remove stop_words as they can tie longer keyphrases together.Next, we convert both the document as well as the candidate keywords/keyphrases to numerical data. We use BERT for this purpose as it has shown great results for both similarity- and paraphrasing tasks.There are many methods for generating the BERT embeddings, such as Flair, Hugginface Transformers, and now even spaCy with their 3.0 release! However, I prefer to use the sentence-transformers package as it allows me to quickly create high-quality embeddings that work quite well for sentence- and document-level embeddings.We install the package with pip install sentence-transformers. If you run into issues installing this package, then it might be helpful to install Pytorch first.Now, we are going to run the following code to transform our document and candidates into vectors:We are Distilbert as it has shown great performance in similarity tasks, which is what we are aiming for with keyword/keyphrase extraction!Since transformer models have a token limit, you might run into some errors when inputting large documents. In that case, you could consider splitting up your document into paragraphs and mean pooling (taking the average of) the resulting vectors.NOTE: There are many pre-trained BERT-based models that you can use for keyword extraction. However, I would advise you to use either distilbert — base-nli-stsb-mean-tokens or xlm-r-distilroberta-base-paraphase-v1 as they have shown great performance in semantic similarity and paraphrase identification respectively.In the final step, we want to find the candidates that are most similar to the document. We assume that the most similar candidates to the document are good keywords/keyphrases for representing the document.To calculate the similarity between candidates and the document, we will be using the cosine similarity between vectors as it performs quite well in high-dimensionality:And…that is it! We take the top 5 most similar candidates to the input document as the resulting keywords:The results look great! These terms definitely look like they describe a document about supervised machine learning.Now, let us take a look at what happens if we change the n_gram_range to (3,3):It seems that we get keyphrases instead of keywords now! These keyphrases, by themselves, seem to nicely represent the document. However, I am not happy that all keyphrases are so similar to each other.To solve this issue, let us take a look at the diversification of our results.There is a reason why similar results are returned… they best represent the document! If we were to diversify the keywords/keyphrases then they are less likely to represent the document well as a collective.Thus, the diversification of our results requires a delicate balance between the accuracy of keywords/keyphrases and the diversity between them.There are two algorithms that we will be using to diversify our results:The maximum sum distance between pairs of data is defined as the pairs of data for which the distance between them is maximized. In our case, we want to maximize the candidate similarity to the document whilst minimizing the similarity between candidates.To do this, we select the top 20 keywords/keyphrases, and from those 20, select the 5 that are the least similar to each other:If we set a low nr_candidates, then our results seem to be very similar to our original cosine similarity method:However, a relatively high nr_candidates will create more diverse keyphrases:As mentioned before, there is a tradeoff between accuracy and diversity that you must keep in mind. If you increase the nr_candidates, then there is a good chance you get very diverse keywords but that are not very good representations of the document.I would advise you to keep nr_candidates less than 20% of the total number of unique words in your document.The final method for diversifying our results is Maximal Marginal Relevance (MMR). MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. Fortunately, a keyword extraction algorithm called EmbedRank has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases.We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords/keyphrases:If we set a relatively low diversity, then our results seem to be very similar to our original cosine similarity method:However, a relatively high diversity score will create very diverse keyphrases:If you are, like me, passionate about AI, Data Science, or Psychology, please feel free to add me on LinkedIn or follow me on Twitter.All examples and code in this article can be found here:github.com",29/10/2020,1,26.0,0.0,1337.0,348.0,7.0,1.0,0.0,18.0,en
4226,A Basic Introduction to Separable Convolutions,Towards Data Science,Chi-Feng Wang,1200.0,8.0,1585.0,"Anyone who takes a look at the architecture of MobileNet will undoubtedly come across the concept of separable convolutions. But what is that, and how is it different from a normal convolution?There are two main types of separable convolutions: spatial separable convolutions, and depthwise separable convolutions.Conceptually, this is the easier one out of the two, and illustrates the idea of separating one convolution into two well, so I’ll start with this. Unfortunately, spatial separable convolutions have some significant limitations, meaning that it is not heavily used in deep learning.The spatial separable convolution is so named because it deals primarily with the spatial dimensions of an image and kernel: the width and the height. (The other dimension, the “depth” dimension, is the number of channels of each image).A spatial separable convolution simply divides a kernel into two, smaller kernels. The most common case would be to divide a 3x3 kernel into a 3x1 and 1x3 kernel, like so:Now, instead of doing one convolution with 9 multiplications, we do two convolutions with 3 multiplications each (6 in total) to achieve the same effect. With less multiplications, computational complexity goes down, and the network is able to run faster.One of the most famous convolutions that can be separated spatially is the Sobel kernel, used to detect edges:The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels.Unlike spatial separable convolutions, depthwise separable convolutions work with kernels that cannot be “factored” into two smaller kernels. Hence, it is more commonly used. This is the type of separable convolution seen in keras.layers.SeparableConv2D or tf.layers.separable_conv2d.The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can image each channel as a particular interpretation of that image; in for example, the “red” channel interprets the “redness” of each pixel, the “blue” channel interprets the “blueness” of each pixel, and the “green” channel interprets the “greenness” of each pixel. An image with 64 channels has 64 different interpretations of that image.Similar to the spatial separable convolution, a depthwise separable convolution splits a kernel into 2 separate kernels that do two convolutions: the depthwise convolution and the pointwise convolution. But first of all, let’s see how a normal convolution works.If you don’t know how a convolution works from a 2-D perspective, read this article or check out this site.A typical image, however, is not 2-D; it also has depth as well as width and height. Let us assume that we have an input image of 12x12x3 pixels, an RGB image of size 12x12.Let’s do a 5x5 convolution on the image with no padding and a stride of 1. If we only consider the width and height of the image, the convolution process is kind of like this: 12x12 — (5x5) — >8x8. The 5x5 kernel undergoes scalar multiplication with every 25 pixels, giving out1 number every time. We end up with a 8x8 pixel image, since there is no padding (12–5+1 = 8).However, because the image has 3 channels, our convolutional kernel needs to have 3 channels as well. This means, instead of doing 5x5=25 multiplications, we actually do 5x5x3=75 multiplications every time the kernel moves.Just like the 2-D interpretation, we do scalar matrix multiplication on every 25 pixels, outputting 1 number. After going through a 5x5x3 kernel, the 12x12x3 image will become a 8x8x1 image.What if we want to increase the number of channels in our output image? What if we want an output of size 8x8x256?Well, we can create 256 kernels to create 256 8x8x1 images, then stack them up together to create a 8x8x256 image output.This is how a normal convolution works. I like to think of it like a function: 12x12x3 — (5x5x3x256) — >12x12x256 (Where 5x5x3x256 represents the height, width, number of input channels, and number of output channels of the kernel). Not that this is not matrix multiplication; we’re not multiplying the whole image by the kernel, but moving the kernel through every part of the image and multiplying small parts of it separately.A depthwise separable convolution separates this process into 2 parts: a depthwise convolution and a pointwise convolution.In the first part, depthwise convolution, we give the input image a convolution without changing the depth. We do so by using 3 kernels of shape 5x5x1.Each 5x5x1 kernel iterates 1 channel of the image (note: 1 channel, not all channels), getting the scalar products of every 25 pixel group, giving out a 8x8x1 image. Stacking these images together creates a 8x8x3 image.Remember, the original convolution transformed a 12x12x3 image to a 8x8x256 image. Currently, the depthwise convolution has transformed the 12x12x3 image to a 8x8x3 image. Now, we need to increase the number of channels of each image.The pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; in our case, 3. Therefore, we iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.We can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.And that’s it! We’ve separated the convolution into 2: a depthwise convolution and a pointwise convolution. In a more abstract way, if the original convolution function is 12x12x3 — (5x5x3x256) →12x12x256, we can illustrate this new convolution as 12x12x3 — (5x5x1x1) — > (1x1x3x256) — >12x12x256.Let’s calculate the number of multiplications the computer has to do in the original convolution. There are 256 5x5x3 kernels that move 8x8 times. That’s 256x3x5x5x8x8=1,228,800 multiplications.What about the separable convolution? In the depthwise convolution, we have 3 5x5x1 kernels that move 8x8 times. That’s 3x5x5x8x8 = 4,800 multiplications. In the pointwise convolution, we have 256 1x1x3 kernels that move 8x8 times. That’s 256x1x1x3x8x8=49,152 multiplications. Adding them up together, that’s 53,952 multiplications.52,952 is a lot less than 1,228,800. With less computations, the network is able to process more in a shorter amount of time.How does that work, though? The first time I came across this explanation, it didn’t really make sense to me intuitively. Aren’t the two convolutions doing the same thing? In both cases, we pass the image through a 5x5 kernel, shrink it down to one channel, then expand it to 256 channels. How come one is more than twice as fast as the other?After pondering about it for some time, I realized that the main difference is this: in the normal convolution, we are transforming the image 256 times. And every transformation uses up 5x5x3x8x8=4800 multiplications. In the separable convolution, we only really transform the image once — in the depthwise convolution. Then, we take the transformed image and simply elongate it to 256 channels. Without having to transform the image over and over again, we can save up on computational power.It’s worth noting that in both Keras and Tensorflow, there is a argument called the “depth multiplier”. It is set to 1 at default. By changing this argument, we can change the number of output channels in the depthwise convolution. For example, if we set the depth multiplier to 2, each 5x5x1 kernel will give out an output image of 8x8x2, making the total (stacked) output of the depthwise convolution 8x8x6 instead of 8x8x3. Some may choose to manually set the depth multiplier to increase the number of parameters in their neural net for it to better learn more traits.Are the disadvantages to a depthwise separable convolution? Definitely! Because it reduces the number of parameters in a convolution, if your network is already small, you might end up with too few parameters and your network might fail to properly learn during training. If used properly, however, it manages to enhance efficiency without significantly reducing effectiveness, which makes it a quite popular choice.Finally, because pointwise convolutions use the concept, I’d like to touch upon the usages of a 1x1 kernel.A 1x1 kernel — or rather, n 1x1xm kernels where n is the number of output channels and m is the number of input channels — can be used outside of separable convolutions. One obvious purpose of a 1x1 kernel is to increase or reduce the depth of an image. If you find that your convolution has too many or too little channels, a 1x1 kernel can help balance it out.For me, however, the main purpose of a 1x1 kernel is to apply non-linearlity. After every layer of a neural network, we can apply an activation layer. Whether it be ReLU, PReLU, Softmax, or another, activation layers are non-linear, unlike convolution layers. “A linear combination of lines is still a line.” Non-linear layers expand the possibilities for the model, as is what generally makes a “deep” network better than a “wide” network. In order to increase the number of non-linear layers without significantly increasing the number of parameters and computations, we can apply a 1x1 kernel and add an activation layer after it. This helps give the network an added layer of depth.Leave a comment below if you have any further questions! And don’t forget to give this story some claps!",14/08/2018,0,5.0,0.0,1258.0,441.0,8.0,0.0,0.0,2.0,en
4227,Fooling Facial Detection with Fashion,Towards Data Science,Bruce MacDonald,28.0,7.0,927.0,"Usage of facial recognition is on the rise. With the recent debates over the ethics of facial recognition potential adversarial attacks against facial detection have been on my mind. Facial recognition is being used everywhere from airports to social media. It seems to be near impossible to opt-out of having your face scanned.An ideal attack on facial detection would be an article of clothing that looks inconspicuous to the uninformed. With inspiration from the Hyperface project I decided to research and implement a wearable adversarial example. In this article I’ll detail the process of creating an adversarial image to fool a selected type of facial detection and how I implemented a practical example on a face mask.The first thing it’s important to note before delving deeper into this project is the difference between facial detection and facial recognition. Facial detection refers to the ability to detect when a face is present in an image. Facial recognition relies on facial detection to establish a face is present in an image but it goes one step further and attempts to establish whose face it is.For this project I have chosen to focus on facial detection. Mainly for the reason that it is much easier to test. To properly test facial recognition access to a facial recognition database would be ideal.The next step was to choose which facial detection model to build the adversarial example against. There are many different facial detection models currently in use. There’s a great primer on facial detection models and their implementations by Vikas Gupta on “Learn OpenCV” with in-depth explanations. I’m just going to go over them briefly here.www.learnopencv.comThe model that quickly stood out as the simplest candidate for attack was the histogram of oriented gradients. Most notably the expected input of a HOG can be easily visualized and fed back into the facial detection model. The visualization of a face as a histogram of oriented gradients also has the advantage of not being an obvious face to a human observer.In order to test the examples I needed a simple HOG based facial detection implementation. Luckily the dlib library has a HOG facial detector built into its frontal_face_detector.The frontal face detector is run with an input image and an upscaling factor. The upscaling factor of 1 indicates the input image will be upscaled once. Upscaling creates a bigger image which makes it easier to detect faces. The result of the frontal face detection is a list of bounding boxes, one for each detected face.Passing the visualization of the HOG’s expected input you can see that it is detected as a face. Great! We have the base for our adversarial attack.Now that I knew the visualization of the HOG’s expected input will be detected as a false positive for a frontal face I needed to create a design for printing on a mask that would look inconspicuous. However there were still many factors influencing the design that I was not aware of how to optimize. The position of the faces, their orientation, and their size could all influence the amount of faces detected in an image. I could simply have tried out different designs until I found a good one, but it seemed like more fun and less tediousness to let a learning model do the hard work for me.I considered several different models to find an optimal input. I researched reinforcement learning, generative adversarial networks, and Q-learning. Ultimately I decided to use simulated annealing with random optimization as it best suited my problem of finding an input that corresponded to the most faces detected by dlib.I used PIL (the Python Imaging Library) in conjunction with mlrose (a Python library for random optimization) to generate an image and find the best state. Optimization with mlrose requires an initial state and a fitness function. Finding this optimal state was a very expensive computation in my case as the generated state needed to be saved to the disk as an image in order to find the number of faces detected.Starting with the initial state mlrose requires a 1D array (as far as I could tell). This meant I had to use a bit of a hacky solution of giving different array positions different significance (see the index explanation). I chose to optimize an input of 6 faces as I could always replicate the design to increase its size.My fitness function was just composed of a conversion of the state to an image followed by detecting the number of faces in that image. The higher the number of faces found the better the fitness. I also tried modifying the fitness function to be higher based on the size of the input HOG face images. This may be better as the larger faces would be more likely to be detected in a real-life situation. However I found that factoring in the face size resulted in longer computation time with visually similar results.With the fitness and the initial state set configuring mlrose for simulated annealing was simple. I just assigned our inputs and let it run until an optimal result was found. I ran this a few times to find a visually interesting result.Finally with this interesting output I added some finishing touches to obscure its facial design. I decided I was more qualified to do this by hand as the intent was to fool humans.With the final design completed I created some mock mask designs to test how they were evaluated by the HOG facial detection. Initial results seemed promising. The above design consistently returned 4–5 falsely detected faces.",04/06/2019,6,6.0,0.0,1027.0,601.0,7.0,1.0,0.0,12.0,en
4228,StyleGAN2,Towards Data Science,Connor Shorten,1910.0,8.0,1638.0,"The first version of the StyleGAN architecture yielded incredibly impressive results on the facial image dataset known as Flicker-Faces-HQ (FFHQ). The most impressive characteristic of these results, compared to early iterations of GANs such as Conditional GANs or DCGANs, is the high resolution (1024²) of the generated images. In addition to resolution, GANs are compared along dimensions such as the diversity of images generated (avoiding mode collapse) and a suite of quantitative metrics comparing real and generated images such as FID, Inception Score, and Precision and Recall.Frechet Inception Distance (FID) is one of the most common automated metrics used to evaluate images sampled from generative models. This metric is based on comparing activations of a pre-trained classification network on real and generated images. The following tables show the progress of GANs over the last 2 years from StyleGAN to StyleGAN2 on this dataset and metric.This article will discuss the architectural changes that have improved the FID metric by ~3x, as well as qualitative improvements like the removal of artifacts in generated images and smooth latent space interpolations. Smoothness in the latent space results in small changes to the source of a generated image causing small perceptual changes to the resulting image, enabling amazing animations such as this. I have also made a video explaining changes in StyleGAN2 if you are interested:The first edition of StyleGAN produces amazingly realistic faces, from the test provided by whichfaceisreal.com below, can you tell which face is real?Do you think you could train yourself to achieve a perfect score on this game? Do you think you could train a neural network to do it? This is the idea behind the $1M Deepfake Detection challenge listed on Kaggle. The authors of whichfaceisreal.com detail a list of tell-tale “artifacts” that can be used to distinguish StyleGAN-generated images. One such artifact is the appearance of “water droplet” effects in the images. Awareness of this makes this game much easier (shown below).The authors of StyleGAN2 seek to remove these artifacts from generated images. They attribute the source of water droplets to restrictions on the generator imposed by the Adaptive Instance Normalization layers.NVIDIA researchers are masters of using normalization layers for image synthesis applications such as StyleGAN and GauGAN. StyleGAN uses adaptive instance normalization to control the influence of the source vector w on the resulting generated image. GauGAN uses a spatially adaptive denormalization layer to synthesize photorealistic images from doodle sketches. (shown below)In the second version of StyleGAN, the authors restructure the use of Adaptive Instance Normalization to avoid these water droplet artifacts. Adaptive Instance Normalization is a normalization layer derived from research into achieving faster Neural Style Transfer. Neural Style Transfer demonstrates a remarkable disentanglement between low-level “style” features and high-level “content” features evident in Convolutional Neural Networks (shown below):However, Style Transfer (before Adaptive Instance Normalization) required a lengthy optimization process or pre-trained networks that are limited to a single style. AdaIN showed that the style and content could be combined through the sole use of normalization statistics. (an overview of this is shown below):The authors of StyleGAN2 explain that this kind of normalization discards information in feature maps encoded in the relative magnitudes of activations. The generator overcomes this restriction by sneaking information past these layers which result in these water-droplet artifacts. The authors share the same confusion as the reader as to why the discriminator is unable to distinguish images from this droplet effect.Adaptive Instance Normalization is reconstructed as Weight Demodulation in StyleGAN2. (this progression is shown below)Adaptive Instance Normalization (similar to other normalization layers like Batch Norm) scales and shifts the activations of intermediate activations. Whereas Batch Norm does this with learned mean and variance parameters computed from batch statistics, Instance Norm uses a single image compared to a batch. Adaptive Instance Norm uses different scale and shift parameters to align different areas of the source w with different regions of the feature map (either within each feature map or via grouping features channel-wise by spatial location).Weight demodulation takes the scale and shift parameters out of a sequential computation path, instead baking scaling into the parameters of convolutional layers. It looks to me like the shifting of values (done with µ(y) in AdaIN) is tasked to the noise map B.Moving the scaling parameters into the convolutional kernel weights enables this computation path to be more easily parallelized. This results in a 40% training speedup from 37 images per second to 61 images per second.The next artifact introduced in StyleGAN targeted in the second edition may also help you achieve good results on the whichfaceisreal.com Turing test. Described at 1:40 of their accompanying video, StyleGAN images have a strong location preference for facial image features like noses and eyes. The authors attribute this to progressive growing. Progressive growing describes the procedure of first tasking the GAN framework with low-resolution images such as 4² and scaling it up when a desirable convergence property has been hurdled at the lower scale.Although progressive growing may be a headache to implement, introducing hyperparameters with respect to fading in higher resolution layers and requiring a more complicated training loop, it is a very intuitive decomposition of the high resolution image synthesis problem. GANs are notoriously challenging to train and the conventional wisdom particularly behind generating something like a 1024² image is that the discriminator will easily distinguish real and fake images, resulting in the generator unable to learn anything during training.Another recent paper on GANs, “Multi-Scale Gradients for Generative Adversarial Networks” by Animesh Karnewar and Oliver Wang shows an interesting way to utilize multiple scale generation with a single end-to-end architecture. (shown below):Inspired by the MSG-GAN, the authors of StyleGAN2 design a new architecture to make use of multiple scales of image generation without explicitly requiring the model to do so. They do this via a resnet-style skip connection between lower resolution feature maps to the final generated image.The authors show that similar to progressive growing, early iterations of training rely more so on the low frequency/resolution scales to produce the final output. The chart below shows how much each feature map contributes to the final output, computed by inspecting the skip connection additions. Inspection of this inspires the authors to scale up the network capacity so that the 1024x1024 scale contributes more to the final output.StyleGAN2 introduces a new normalization term to the loss to enforce smoother latent space interpolations. Latent space interpolation describes how changes in the source vector z results in changes to the generated images. This is done by adding the following loss term to the generator:The exact details of how this is implemented are outside of my understanding, but the high level idea seems to be that the Jacobian matrix maps small changes in w with changes in the resulting image (going from points in w space to 1024² images). This matrix is multiplied by a random image to avoid getting stuck in a local optima, and the l2 norm of this is multiplied by an exponential moving average of it. Hence, the larger the l2 norm of this, the more it increases the loss, causing the generator to play ball and keep the latent space smooth.Another interesting characteristic of this implementation is denoted as lazy regularization. Since computing this Jacobian matrix is computationally heavy, this normalization is only added to the loss every 16 steps compared to every step.This smooth latent space dramatically facilitates projecting images back into the latent space. This is done by taking a given image and optimizing for the source vector w that could produce it. This has been demonstrated in a lot of interesting twitter threads with researchers projecting images of themselves into the latent space of the StyleGAN2 trained on FFHQ:Some Twitter demonstrations of projection from Gene Kogan: (1, 2)There are a lot of interesting applications of projection into a perfectly smooth latent space. For example, animation workflows generally consist of sketching out high-level keyframes and then manually filling in the more fine-grained intermediate frames. Models like StyleGAN2 would allow you to project the high-level keyframes into the latent space and then search amongst paths between the two w vectors defined by the structure of the prior w~ p(w) until you find a satisfying transition.Say the source of the generated image, w is sampled from this spherical distribution (representing a 3D uniform distribution). You would then project the two keyframes into this sphere to find the vectors that generate each image respectively. There would then be many paths you could walk along between these points in the source distribution. A smooth latent space will ensure that these transitions are smooth as in the fine-grained intermediate points refined by human illustrators and animators.One application of projection would be to use it to find the source of a given generated image to tell if it is real or fake. The idea being that if it is fake, you can find the latent vector that produced it and if it is real, you cannot. I think this idea of projecting images back into the latent space is very interesting, but I’m not sold on it as a deepfake detection technique. It seems impossible to keep a tab on all of the possible datasets people may have to train these models which would be the real contributor to the image being projected back into latent space. Additionally, I don’t understand why the StyleGAN wouldn’t be able to perfectly reconstruct the original training image, since this is really the core of the training objective.I think projection could be used in an interesting way with anomaly detection and lifelong learning. When a new example is misclassified, it could be reprojected back into the same dataset used to train the classifier and some kind of image similarity distance could be computed like l2 distance or pre-trained classifier features.Thanks for reading this overview of StyleGAN2!",17/12/2019,0,0.0,0.0,929.0,545.0,14.0,0.0,0.0,15.0,en
4229,Quick Logistic Regression with TensorFlow,Pankaj Mathur,Pankaj Mathur,168.0,4.0,276.0,"Here is a simple logistic regression model built with TensorFlow. We are using MNIST Image example data set which is provided by default with Tensorflow package.Here are the hyperparameters we choose to run initial model:We achieved impressive 90.8% accuracy in 20 epochs with a learning rate of 0.01 by running simple logistic regression model build with Tensorflow on MNIST dataset.I am using Conda to install TensorFlow. You might already have a TensorFlow environment, but please check below to make sure you have all the necessary packages. If you have never used Conda environments before, please go through my other tutorial What is Anaconda and Why should I bother about it?Assuming you have conda install on your machine, please run the following commands to have tensorflow-playground ready for you to play.Run the following commands to setup your environment:And installing on Windows. In your console or Anaconda shell:After creating conda environment, clone this repository on your local machine via Git or GitHub Desktopunder tensorflow-playground environment on your terminal or shell window, cd to the cloned directory and then run following command:That’s it, now you can play around with various hyperparameters which you feel are good enough to improve accuracy on MNIST dataset.If you a bit more adventurous, you can try replacing MNIST dataset with another sample dataset, such as CIFAR10. However, you have to download this data separately on your hard drive, extract it and access it via another python functions, something like below for your jupyter notebook.Please do let me know your thoughts, questions under the comments section. I would really appreciate getting some feedback on this article & ideas to improve it.In the meanwhile, Happy Thinking…",02/04/2016,4,65.0,16.0,1280.0,853.0,1.0,1.0,0.0,5.0,en
4230,Clustering Analysis in R using K-means,Towards Data Science,Luiz Fonseca,59.0,8.0,1011.0,"The purpose of clustering analysis is to identify patterns in your data and create groups according to those patterns. Therefore, if two points have similar characteristics, that means they have the same pattern and consequently, they belong to the same group. By doing clustering analysis we should be able to check what features usually appear together and see what characterizes a group.In this post, we are going to perform a clustering analysis with multiple variables using the algorithm K-means. The intention is to find groups of mammals based on the composition of the species’ milk. The main points covered here are:The dataset used is part of the package cluster.datasets and contains 25 observations on the following 6 variables:name — a character vector for the name of the animals water — a numeric vector for the water content in the milk sample protein — a numeric vector for the amount of protein in the milk samplefat — a numeric vector for the fat content in the milk sample lactose — a numeric vector for the amount of lactose in the milk sample ash — a numeric vector for the amount of mineral in the milk sampleLet’s take a look at a sample of the dataThe charts below show us the distribution for each variable. Each point represents a mammal species (25 in total).Each variable has different behavior and we could identify groups of mammals on each one individually, but that’s not the purpose here.All the variables will be used in the clustering on a linear scale. Sometimes, when the values (for each feature) are in a big range, for example from 0 up to 1 million, it’s interesting to use a logarithmic scale because on a log scale we would highlight bigger differences between the values and smaller differences would be considered less important. Since the values in our dataset vary between 0 and 100, we are going to use a linear scale, which considers differences between values equally important.The clustering algorithm that we are going to use is the K-means algorithm, which we can find in the package stats. The K-means algorithm accepts two parameters as input:Conceptually, the K-means behaves as follows:The bigger is the K you choose, the lower will be the variance within the groups in the clustering. If K is equal to the number of observations, then each point will be a group and the variance will be 0. It’s interesting to find a balance between the number of groups and their variance. A variance of a group means how different the members of the group are. The bigger is the variance, the bigger is the dissimilarity in a group.How do we choose the best value of K in order to find that balance?To answer that question, we are going to run K-means for an arbitrary K. Let’s pick 3.The kmeans() function outputs the results of the clustering. We can see the centroid vectors (cluster means), the group in which each observation was allocated (clustering vector) and a percentage (89.9%) that represents the compactness of the clustering, that is, how similar are the members within the same group. If all the observations within a group were in the same exact point in the n-dimensional space, then we would achieve 100% of compactness.Since we know that, we will use that percentage to help us decide our K value, that is, a number of groups that will have satisfactory variance and compactness.The function below plots a chart showing the “within sum of squares” (withinss) by the number of groups (K value) chosen for several executions of the algorithm. The within sum of squares is a metric that shows how dissimilar are the members of a group., the greater is the sum, the greater is the dissimilarity within a group.By Analysing the chart from right to left, we can see that when the number of groups (K) reduces from 4 to 3 there is a big increase in the sum of squares, bigger than any other previous increase. That means that when it passes from 4 to 3 groups there is a reduction in the clustering compactness (by compactness, I mean the similarity within a group). Our goal, however, is not to achieve compactness of 100% — for that, we would just take each observation as a group. The main purpose is to find a fair number of groups that could explain satisfactorily a considerable part of the data.So, let’s choose K = 4 and run the K-means again.Using 3 groups (K = 3) we had 89.9% of well-grouped data. Using 4 groups (K = 4) that value raised to 95.1%, which is a good value for us.We may use the silhouette coefficient (silhouette width) to evaluate the goodness of our clustering.The silhouette coefficient is calculated as follows:So, the interpretation of the silhouette width is the following:The silhouette plot below gives us evidence that our clustering using four groups is good because there’s no negative silhouette width and most of the values are bigger than 0.5.The following plot shows the final result of our clustering. The actual plot is interactive, but the image below is not. You can reproduce the plot using the code below. In the interactive plot, you may isolate the groups to better understand each one individually.The purpose of clustering analysis is to identify patterns in the data. As we can see in the plot above, observations within the same group tend to have similar characteristics.Let’s take the green group as an instance to evaluate. The two mammal species that belong to that group, namely seal and dolphin, they have the lowest percentage of water (44.9% and 46.4%); they both have around 10% of protein in their milk; they have the highest percentage of fat in the milk among all other species as well as the lowest percentage of lactose. This is the pattern found that puts seals and dolphins together in the same group. We can identify such patterns in the other groups as well.Thank you for reading. I hope it was a pleasurable and useful reading.",16/08/2019,4,28.0,0.0,1226.0,1082.0,5.0,5.0,0.0,2.0,en
4231,Time Series of Price Anomaly Detection,Towards Data Science,Susan Li,25000.0,9.0,1046.0,"Also known as outlier detection, anomaly detection is a data mining process used to determine types of anomalies found in a data set and to determine details about their occurrences. Automatic anomaly detection is critical in today’s world where the sheer volume of data makes it impossible to tag outliers manually. Auto anomaly detection has a wide range of applications such as fraud detection, system health monitoring, fault detection, and event detection systems in sensor networks, and so on.But I would like to apply anomaly detection to hotel room prices. The reason is somewhat selfish.Have you had experience that, lets say, you travel to a certain destination for business regularly and you always stay at the same hotel. While most of the time, the room rate is almost similar but occasionally for the same hotel, same room type, the rate is unacceptably high, and you’d have to change to another hotel because your travel allowance does not cover that rate. I had been through this several times and this makes me think, what if we could create a model to detect this kind of price anomaly automatically?Of course there are circumstance that some anomaly happens only once a life time and we have known them in advance and probably it will not happen the same time in the future years, such as the ridiculous hotel prices in Atlanta on February 2 to February 4, 2019.In this post, I will explore different anomaly detection techniques and our goal is to search for anomalies in the time series of hotel room prices with unsupervised learning. Let’s get started!It is very hard to get the data, I was able to get some but the data is not perfect.The data we are going to use is a subset of Personalize Expedia Hotel Searches data set that can be found here.We are going to slice a subset of the training.csv set like so:After slice and dice, this is the data we will be working with:At this point, we have detected one extreme anomaly which was the Max price_usd at 5584.If an individual data instance can be considered as anomalous with respect to the rest of the data, we call it Point Anomalies (e.g. purchase with large transaction value). We could go back to check the log to see what was it about. After a little bit investigation, I guess it was either a mistake or user searched a presidential suite by accident and had no intention to book or view. In order to find more anomalies that are not extreme, I decided to remove this one.At this point, I am sure you have found that we are missing something, that is, we do not know what room type a user searched for, the price for a standard room could be very different with the price for a King bed room with Ocean View. Keep this in mind, for the demonstration purpose, we have to continue.In general, the price is more stable and lower when searching Non-Saturday night. And the price goes up when searching Saturday night. Seems this property gets popular during the weekend.k-means is a widely used clustering algorithm. It creates ‘k’ similar clusters of data points. Data instances that fall outside of these groups could potentially be marked as anomalies. Before we start k-means clustering, we use elbow method to determine the optimal number of clusters.From the above elbow curve, we see that the graph levels off after 10 clusters, implying that addition of more clusters do not explain much more of the variance in our relevant variable; in this case price_usd.we set n_clusters=10, and upon generating the k-means output use the data to plot the 3D clusters.Now we need to find out the number of components (features) to keep.We see that the first component explains almost 50% of the variance. The second component explains over 30%. However, we’ve got to notice that almost none of the components are really negligible. The first 2 components contain over 80% of the information. So, we will set n_components=2.The underline assumption in the clustering based anomaly detection is that if we cluster the data, normal data will belong to clusters while anomalies will not belong to any clusters or belong to small clusters. We use the following steps to find and visualize anomalies.It seems that the anomalies detected by k-means clustering were either some of very high rates or some of very low rates.Isolation Forest detects anomalies purely based on the fact that anomalies are data points that are few and different. The anomalies isolation is implemented without employing any distance or density measure. This method is fundamentally different from clustering based or distance based algorithms.A SVM is typically associated with supervised learning, but OneClassSVM can be used to identify anomalies as an unsupervised problems that learns a decision function for anomaly detection: classifying new data as similar or different to the training set.According to the paper: Support Vector Method for Novelty Detection. SVMs are max-margin methods, i.e. they do not model a probability distribution. The idea of SVM for anomaly detection is to find a function that is positive for regions with high density of points, and negative for small densities.Gaussian distribution is also called normal distribution. We will be using the Gaussian distribution to develop an anomaly detection algorithm, that is, we’ll assume that our data are normally distributed. This’s an assumption that cannot hold true for all data sets, yet when it does, it proves an effective method for spotting outliers.Scikit-Learn’s covariance.EllipticEnvelope is a function that tries to figure out the key parameters of our data’s general distribution by assuming that our entire data is an expression of an underlying multivariate Gaussian distribution. The process like so:It is interesting to see that anomalies detected in this way have only observed abnormal high prices but not abnormal low prices.So far, we have done price anomaly detection with four different methods. Because our anomaly detection is unsupervised learning. After building the models, we have no idea how well it is doing as we have nothing to test it against. Hence, the results of those methods need to be tested in the field before placing them in the critical path.Jupyter notebook can be found on Github. Enjoy the rest of the week!References:www.datascience.comscikit-learn.orgscikit-learn.orgscikit-learn.orgwww.kaggle.com",24/01/2019,7,7.0,1.0,964.0,498.0,15.0,5.0,0.0,16.0,en
4232,A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model,Towards Data Science,Zhi Li,130.0,8.0,1045.0,"Word embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:There are two ways for installation. We could run the following code in our terminal to install genism package.Or, alternatively for Conda environments:In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Modelb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.We can train the genism word2vec model with our own custom corpus as following:Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.",31/05/2019,11,10.0,1.0,1031.0,569.0,6.0,2.0,0.0,17.0,en
4233,The Vanishing Gradient Problem,Towards Data Science,Chi-Feng Wang,1200.0,3.0,458.0,"As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.Certain activation functions, like the sigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.As an example, Image 1 is the sigmoid function and its derivative. Note how when the inputs of the sigmoid function becomes larger or smaller (when |x| becomes bigger), the derivative becomes close to zero.For shallow network with only a few layers that use these activations, this isn’t a big problem. However, when more layers are used, it can cause the gradient to be too small for training to work effectively.Gradients of neural networks are found using backpropagation. Simply put, backpropagation finds the derivatives of the network by moving layer by layer from the final layer to the initial one. By the chain rule, the derivatives of each layer are multiplied down the network (from the final layer to the initial) to compute the derivatives of the initial layers.However, when n hidden layers use an activation like the sigmoid function, n small derivatives are multiplied together. Thus, the gradient decreases exponentially as we propagate down to the initial layers.A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network.The simplest solution is to use other activation functions, such as ReLU, which doesn’t cause a small derivative.Residual networks are another solution, as they provide residual connections straight to earlier layers. As seen in Image 2, the residual connection directly adds the value at the beginning of the block, x, to the end of the block (F(x)+x). This residual connection doesn’t go through activation functions that “squashes” the derivatives, resulting in a higher overall derivative of the block.Finally, batch normalization layers can also resolve the issue. As stated before, the problem arises when a large input space is mapped to a small one, causing the derivatives to disappear. In Image 1, this is most clearly seen at when |x| is big. Batch normalization reduces this problem by simply normalizing the input so |x| doesn’t reach the outer edges of the sigmoid function. As seen in Image 3, it normalizes the input so that most of it falls in the green region, where the derivative isn’t too small.Do leave a comment below if you have any questions or suggestions :)Read these articles for more information:",08/01/2019,0,5.0,2.0,970.0,497.0,4.0,1.0,0.0,5.0,en
4234,Regularization in Machine Learning,Towards Data Science,Prashant Gupta,3300.0,7.0,1331.0,"One of the major aspects of training your machine learning model is avoiding overfitting. The model will have a low accuracy if it is overfitting. This happens because your model is trying too hard to capture the noise in your training dataset. By noise we mean the data points that don’t really represent the true properties of your data, but random chance. Learning such data points, makes your model more flexible, at the risk of overfitting.The concept of balancing bias and variance, is helpful in understanding the phenomenon of overfitting.medium.comOne of the ways of avoiding overfitting is using cross validation, that helps in estimating the error over test set, and in deciding what parameters work best for your model.medium.comThis article will focus on a technique that helps in avoiding overfitting and also increasing model interpretability.This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.A simple relation for linear regression looks like this. Here Y represents the learned relation and β represents the coefficient estimates for different variables or predictors(X).Y ≈ β0 + β1X1 + β2X2 + …+ βpXpThe fitting procedure involves a loss function, known as residual sum of squares or RSS. The coefficients are chosen, such that they minimize this loss function.Now, this will adjust the coefficients based on your training data. If there is noise in the training data, then the estimated coefficients won’t generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.Above image shows ridge regression, where the RSS is modified by adding the shrinkage quantity. Now, the coefficients are estimated by minimizing this function. Here, λ is the tuning parameter that decides how much we want to penalize the flexibility of our model. The increase in flexibility of a model is represented by increase in its coefficients, and if we want to minimize the above function, then these coefficients need to be small. This is how the Ridge regression technique prevents coefficients from rising too high. Also, notice that we shrink the estimated association of each variable with the response, except the intercept β0, This intercept is a measure of the mean value of the response when xi1 = xi2 = …= xip = 0.When λ = 0, the penalty term has no eﬀect, and the estimates produced by ridge regression will be equal to least squares. However, as λ→∞, the impact of the shrinkage penalty grows, and the ridge regression coeﬃcient estimates will approach zero. As can be seen, selecting a good value of λ is critical. Cross validation comes in handy for this purpose. The coefficient estimates produced by this method are also known as the L2 norm.The coefficients that are produced by the standard least squares method are scale equivariant, i.e. if we multiply each input by c then the corresponding coefficients are scaled by a factor of 1/c. Therefore, regardless of how the predictor is scaled, the multiplication of predictor and coefficient(Xjβj) remains the same. However, this is not the case with ridge regression, and therefore, we need to standardize the predictors or bring the predictors to the same scale before performing ridge regression. The formula used to do this is given below.Lasso is another variation, in which the above function is minimized. Its clear that this variation differs from ridge regression only in penalizing the high coefficients. It uses |βj|(modulus)instead of squares of β, as its penalty. In statistics, this is known as the L1 norm.Lets take a look at above methods with a different perspective. The ridge regression can be thought of as solving an equation, where summation of squares of coefficients is less than or equal to s. And the Lasso can be thought of as an equation where summation of modulus of coefficients is less than or equal to s. Here, s is a constant that exists for each value of shrinkage factor λ. These equations are also referred to as constraint functions.Consider their are 2 parameters in a given problem. Then according to above formulation, the ridge regression is expressed by β1² + β2² ≤ s. This implies that ridge regression coefficients have the smallest RSS(loss function) for all points that lie within the circle given by β1² + β2² ≤ s.Similarly, for lasso, the equation becomes,|β1|+|β2|≤ s. This implies that lasso coefficients have the smallest RSS(loss function) for all points that lie within the diamond given by |β1|+|β2|≤ s.The image below describes these equations.The above image shows the constraint functions(green areas), for lasso(left) and ridge regression(right), along with contours for RSS(red ellipse). Points on the ellipse share the value of RSS. For a very large value of s, the green regions will contain the center of the ellipse, making coefficient estimates of both regression techniques, equal to the least squares estimates. But, this is not the case in the above image. In this case, the lasso and ridge regression coefficient estimates are given by the ﬁrst point at which an ellipse contacts the constraint region. Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coeﬃcient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coeﬃcients will equal zero. In higher dimensions(where parameters are much more than 2), many of the coeﬃcient estimates may equal zero simultaneously.This sheds light on the obvious disadvantage of ridge regression, which is model interpretability. It will shrink the coefficients for least important predictors, very close to zero. But it will never make them exactly zero. In other words, the final model will include all predictors. However, in the case of the lasso, the L1 penalty has the eﬀect of forcing some of the coeﬃcient estimates to be exactly equal to zero when the tuning parameter λ is suﬃciently large. Therefore, the lasso method also performs variable selection and is said to yield sparse models.A standard least squares model tends to have some variance in it, i.e. this model won’t generalize well for a data set different than its training data. Regularization, significantly reduces the variance of the model, without substantial increase in its bias. So the tuning parameter λ, used in the regularization techniques described above, controls the impact on bias and variance. As the value of λ rises, it reduces the value of coefficients and thus reducing the variance. Till a point, this increase in λ is beneficial as it is only reducing the variance(hence avoiding overfitting), without loosing any important properties in the data. But after certain value, the model starts loosing important properties, giving rise to bias in the model and thus underfitting. Therefore, the value of λ should be carefully selected.This is all the basic you will need, to get started with Regularization. It is a useful technique that can help in improving the accuracy of your regression models. A popular library for implementing these algorithms is Scikit-Learn. It has a wonderful api that can get your model up an running with just a few lines of code in python.If you liked this article, be sure to show your support by clapping for this article below and if you have any questions, leave a comment and I will do my best to answer.For being more aware of the world of machine learning, follow me. It’s the best way to find out when I write more articles like this.You can also follow me on Twitter, email me directly or find me on linkedin. I’d love to hear from you.That’s all folks, Have a nice day :)Content for this article is inspired and taken from, An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani",15/11/2017,0,31.0,30.0,686.0,281.0,6.0,0.0,0.0,6.0,en
4235,GMM: Gaussian Mixture Models — How to Successfully Use It to Cluster Your Data?,Towards Data Science,Saul Dobilas,2100.0,9.0,1066.0,"This article is part of the series that explains how different Machine Learning algorithms work and provides you a range of Python examples to help you get started with your own Data Science project.While it is not always possible to categorize every algorithm perfectly, it is still beneficial to try and do so. The below interactive chart is my attempt to help you see the broader universe of Machine Learning.Make sure to click👇 on different categories to enlarge and reveal more.Note, in many cases, the same algorithm can be used to solve multiple types of problems. E.g., one can use Neural Networks for classification, regression, and as part of the reinforcement learning.If you enjoy Data Science and Machine Learning, please subscribe to get an email whenever I publish a new story.Since Gaussian Mixture Models (GMM) are used in clustering, they sit under the unsupervised branch of Machine Learning.As you may already know, unsupervised techniques, in particular clustering, are often used for segmentation analysis or as a way to find similarities/differences between observations in your dataset. This is different from supervised learning models, which are typically used for making predictions.Not all clustering algorithms are created equal. Different clustering algorithms implement different ideas on how to best cluster your data. There are 4 main categories:Note, variance is used for single variable analysis and covariance for multivariate analysis. Examples in this article use multivariate setup (multiple distributions/clusters).As you might have figured, Gaussian Mixture Models assume that your data follows Gaussian (a.k.a. Normal) distribution. Since there can be multiple such distributions within your data, you get to specify their number, which is essentially the number of clusters that you want to have.Also, since separate distributions can overlap, the model output is not a hard assignment of points to specific clusters. It is based on a probability that the point belongs to a said distribution. Say, if point A has a probability of 0.6 belonging to “Cluster 0” and a probability of 0.4 belonging to “Cluster 1,” then the model would recommend “Cluster 0” to be the label for that point (since 0.6>0.4).To aid the explanation further, let’s look at a few graphs.To understand how GMM works in practice, we need to look at the Expectation-Maximization (EM) algorithm. The EM uses an iterative method to calculate and recalculate the parameters of each cluster (distribution), i.e., mean, variance/covariance, and size.I will not go into the complicated maths on what happens within each step. Instead, I will give you an intuitive explanation starting with the below chart for easy visualization:At the outset, the model initializes a specified number of clusters with a set of parameters that can either be random or specified by the user. Smart initialization options are also available in some implementations (e.g., sklearn’s implementation of GMM by default uses kmeans to initialize clusters).For the above graph, I have specified my own set of mean values (starting centers) to initialize clusters, which helped me to create a nicer visualization. It has also sped up the convergence when comparted to random initialization.However, you have to be very careful with initialization, because GMM’s final result tends to be quite sensitive to the initial starting parameters. Hence, it is recommended to either use smart initialization or to randomely initialize many times and then pick the best result.So, with clusters initialized, we have the mean (μ), covariance (Cov), and size (𝜋) available to use.The process of E-step and M-step is repeated many times until no further improvements can be made, i.e., convergence is achieved.We will use the following data and libraries:Let’s import all the libraries:Then we get the Australian weather data from Kaggle, which you can download following this link: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package.We ingest the data and derive a new variable, “Location2”, which has the right format to extract city coordinates using Geopy.Since our original data only contains location (city) names and not coordinates, we will use Geopy’s Nominatim to get those coordinates. Note that we add a sleep time of 1 second between each call not to overload the server.And this is the snippet of what we get in return:Next, let’s plot cities on a map:There is more than one way to select how many clusters you should have. It can be based on your knowledge of the data or something more data-driven like the Silhouette score. Here is a direct quote from sklearn:The Silhouette Coefficient is defined for each sample and is composed of two scores:a: The mean distance between a sample and all other points in the same class.b: The mean distance between a sample and all other points in the next nearest cluster.The Silhouette Coefficient s for a single sample is then given as:The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.Let’s create multiple GMM models using a different number of clusters and plot Silhouette scores.Generally, the higher the Silhouette score, the better defined your clusters are. In this example, I chose to have 4 clusters instead of 2 despite the score being slightly higher for a 2 cluster setup.Note, if you are well familiar with your data, you may prefer to use the Silhouette score as a guide rather than a hard rule when deciding on the number of clusters.Let’s now build our GMM model:Below is the summary printed by the above code. Note, the convergence has been achieved after 7 iterations with means (cluster centers) displayed:Now, let’s plot clusters on a map:Finally, you can also plot the sample of 10,000 new points generated by the model as seen in a graph earlier in the article:Gaussian Mixture Models are useful in situations where clusters have an “elliptical” shape. While K-Means only use means (centroids) to find clusters, GMMs also include variance/covariance. This is exactly what gives GMMs an advantage over K-Means when identifying non-circular clusters.In general, you can think of GMM as a more advanced version of K-Means, but keep in mind that it may not always give you superior results. This is because a lot depends on the shape of the data and what you are trying to achieve.I hope this article helped you to understand Gaussian Mixture Models better. If you want to learn more about alternative clustering algorithms, you can refer to my articles below.Cheers!👏Saul DobilasIf you have already spent your learning budget for this month, please remember me next time. My personalized link to join Medium is:solclover.comtowardsdatascience.comtowardsdatascience.comtowardsdatascience.com",23/05/2021,0,20.0,6.0,1204.0,713.0,14.0,7.0,0.0,36.0,en
4236,Speeding up your code (2): vectorizing the loops with Numpy,HackerNoon.com,Vincenzo Lavorini,230.0,6.0,1091.0,"From this series:In the previous post I described the working environment and the basic code for clusterize points in the Poincaré ball space. Here I will improve that code transforming two loops to matrix operations.I ended that post with a very promising plot about the speed improvement on a element-wise product of two vectors. So let’s detail it.Suppose we have two arrays:and we want to obtain as result an array where the elements are the element-wise multiplication of them:We can do it in two ways: with a loop over the elements, or with a vectorized operation. Now: what happens in terms of execution time? I did this calculations with arrays of different dimensions, ranging from 100.000 to 10.000.000.In the right plot you see the execution times of the two operations: the vectorized version is MUCH faster than the looped one. How much faster? You see this in the left plot: the vectorized version is executed in less than 1.3% of the time!Actually when we use the broadcasting capabilities of Numpy like we did in the previous post, under the hood all the operations are automatically vectorized. So using broadcasting not only speed up writing code, it’s also faster the execution of it! In the vectorized element-wise product of this example, in fact i used the Numpy np.dot function.Now, how can apply such strategy to get rid of the loops?Let’s begin with the loop in the distance function. From the previous post:We execute this function for each vector of the collection: that’s one of the loops we want to avoid. And we feed the function with all the vectors, one at a time (a) together with the whole collection (A): that’s the other loop which we will vectorize.If you change the perspective, you can see the collection of vectors as a matrix, and the vectors becomes just rows of the matrix. In this vision, the operations between the vectors becomes operations between the rows of the matrix. For example, 1000 vectors with two components will make a (1000,2) matrix:Let’s step back to the original formula, and focus at each part of it separately:where the double pipe character (like ||a||) means the euclidean norm.For better visualize the following operations, I will simplify the notation by omitting the vectors’ components. So when I will write v1, you have to remember that that vector has his own components. As consequence, the 2-d matrices which contains vectors will be 3-d matrices, with the vectors’ components along the third, hidden, dimension.In the numerator of the fraction we have to do a subtraction between vectors, before calculating the squared norm. And we want to vectorize it, i.e. make all the subtractions in a single passage. In other words, we want to obtain this matrix:This will be a symmetric matrix (element (1,2) is equal to element (2,1), etc) and will have all the diagonal elements equal to zero. This means that we actually double the calculations, but that’s the better way of proceed along the next operations.For obtaining such a matrix it’s convenient to leverage on the broadcasting capabilities of Numpy. As example, if we focus for a moment to the first row of it, which is composed by the differences between v1 and all the vectors of the collection S, we can obtain it by simply call the subtraction v1-S. And given that we have to do the same operation with all the vectors, we have to execute the following operation:At this point we have to calculate the squared norm of the obtained elements, i.e. we have to square everything in the matrix and then sum up those squares along the vectors’ component axis, which is the omitted third dimension in the matrices, as already said.Speaking in Python/Numpy language, this is the code for obtaining the numerator:Now the denominator. The two operands between parenthesis can actually be seen as an operation within the same matrix, because we are treating all the vectors at the same time. So beforehand we calculate the squared norm of all the vectors, then subtract the obtained vector from a vector composed by ones. Using broadcasting:Now, to obtain the matrix with all the denominator terms we have to multiply this vector by the transpose of himself:All this is done by the following code:Now we have both the numerator and the denominator. All the rest of the operations to get the distances are just element-wise operations made with broadcasting (Thank you Numpy!):The function we built gives as result a matrix with the distances between points in the Poincaré ball:Now we have to adapt the main function to use this matrix for running the mean shift algorithm, and at the same time we will get rid of the other loop.Recalling the mean shift algorithm, we have to calculate the weights using a Gaussian kernel. For doing this we can use the same function we already prepared, because thanks to broadcasting it will work for a matrix too, and we will get as result a matrix with the same shape.Then we have to use this matrix to weight all the vectors of the collection. This is a usual matrix product:And we have also to normalize each vector by the sum of the weights needed to build it. As example, for having the normalized version of v1w we have to divide it by the sum w(v1,v1) + w(v1,v2) + …. + w(v1, v1000).So we have to sum up the weight matrix along the rows, obtaining an array of summed weights that we will use for divide element-wise the weighted vectors.Here the code needed for all those operations:And that’s all!Now let’s see how much is faster with respect to the looped version. The following plots are made running both the algorithms with collections of two dimensional vectors.As expected, the vectorized version is much faster then the looped one: with 10.000 2-d vectors it took 844 seconds for execute the looped algorithm, while the vectorized version it’s executed in just 28 seconds. It’s to be said that the execution speed decrease with big dataset. This is an expected behavior, because for N vectors we deal with N² tensor elements.Now: what if we want to make it faster? And what if the number of vectors are so big that they cannot fit in memory?In the next post I will show a batch version of the algorithm, i.e. an algorithm which will process the vectors a bunch at time. And thanks to this, we can execute it in a multiprocess way, where each CPU core will take care of each batch.Stay tuned!",18/08/2017,0,0.0,7.0,367.0,126.0,14.0,1.0,0.0,5.0,en
4237,Bayesian Linear Regression in Python via PyMC3,Towards Data Science,Dr. Robert Kübler,1700.0,10.0,1707.0,"In this article, we will see how to conduct Bayesian linear regression with PyMC3. If you got here without knowing what Bayes or PyMC3 is, don’t worry! You can use my articles as a primerYou can view Bayesian linear regression as a more verbose version of standard linear regression. Linear regression gives you single values, for the model parameters as well as the predictions. Bayesian linear regression, in turn, gives you distributions.We will see what this exactly means in a second. Let us quickly introduce a simple dataset to be able to compare both linear regression approaches.We have done it all several times: Grabbing a dataset containing features and continuous labels, then shoving a line through the data, and call it a day. As a running example for this article, let us use the following dataset:We can’t see much by looking at the raw numbers, so let’s visualize it.Look about linear. Now, whatever linear regression tool we use (statsmodels, scikit-learn, R’s lm method, …), we will end up with:We see that the algorithm computes a slope of around 3.2, an intercept of around 2.19, and hence outputs the model y = 3.2x + 2.19. So far, so good. If desired, we can even check the p-value for the slope and determine if this apparent positive connection between x and y is significant.If you are into this: The t-statistic is around 13.8, the p-value is around 0.0 and a 95% confidence interval is [2.7, 3.7]. In short, this means that x has a significant positive influence on y. Alright, enough of this. Let us get to one of the main drawbacks of linear regression.The standard linear regression model outputs a single y value for each input x. Do you want to know the prediction for x = 0? Well, it is y = 2.19, take it or leave it.To understand why this is dangerous, ask yourself the following question:You have to use a standard linear regression model that was trained on only 5 data points, and has a decent r² score. Are you really feeling safe about the predictions made by the model for a new data point?Sure, the model outputs a number like 2.19, but could it be possible that the prediction is 1.1? Or even -17.3? Or is the model certain that it is really closely around 2.19?I will not lie to you: there are ways to address this issue not only for linear regression but for any regression algorithm via bootstrapping. In short:Subsample your data, fit a model on this subsample and use it to make a prediction. Repeat this 1000 times and you obtain 1000 different predictions.Of course, 1000 should just represent a large number.You can compute the mean μ, the standard deviation σ, quantiles, anything you want of these 1000 samples. If you still want to output a single number, just take the mean. However, now you also have the choice to output intervals, such as [μ-3σ, μ+3σ].This approach works well, but it’s extremely computationally expensive since we have to fit a model 1000 times. Let us, therefore, take another road and upgrade the standard linear regression approach instead.First, we have to explore the theory of Bayesian linear regression to then be able to understand the code in PyMC3.In Bayesian statistics, we deal with distribution. This makes it necessary to specify linear regression as a distribution as well. One assumption commonly used for the standard linear regression approach is the following:or equivalentlyThis means that y is normally distributed with mean ax+b (the quantity we always predict) and some standard deviation σ.In Bayesian statistics, we don’t treat a and b as fixed variables, but also as coming from distributions. This is usually the hardest part: How could they be distributed? Let’s just sayThis tells the model that without any data, we expect the slope and intercept of the line to be around zero with a standard deviation of four, i.e. we expect a and b to be around -12 and 12. The prior is something we have to play around with if things don’t work out. This means that PyMC3 does not give a proper result, or the prediction performance of the model is bad.But we missed a parameter: what about σ? We could give it the same normal distributions as a and b. What we do instead is modeling σ² directly because we never need σ directly, but only σ² for the variance of y. Since σ² is a positive number, we choose a prior distribution that yields exclusively positive values as well, such as the Exponential distribution, the Gamma distribution, or the Half-normal distribution. Let us go for the exponential distribution.Examples of exponential distributions are the following:Note that the density of the exponential distribution is always decreasing. Using this distribution as a prior tells the model that we expect a small, but also positive value for σ². That’s all we need! let’s switch from math to PyMC3.We can program the model described before the following way and plot the posterior distributions.We can see that the model comes to the same conclusion as the standard linear regression: A slope of around 3.2 and an intercept of around 2.2. We even see that the standard error is about 1.1. But here, we also get credible intervals for free!Looking at these figures, it seems like the model is still unsure about the parameters. Look at the slope, for example. With a probability of 94%, the slope a is between 2.7 and 3.7, which is still a rather wide interval. The same goes for the other parameters we estimated.The great thing about Bayesian reasoning: All of these distributions narrow down further, i.e. the model gets more certain the more data we provide.This is all good, but the goal was not to estimate the model’s parameters. Don’t get me wrong, it’s nice to have them, but actually, we are interested in getting predictions for new values of x.Unfortunately, this is not as straightforward as in scikit-learns fit-predict paradigm. It’s still easy, and you can create a model with changeable inputs as follows:I have marked the changes for you in bold. It’s basically telling the model to use a placeholder x_ which was initially filled with our training data x. We then train the model, i.e. get posterior distributions for all of the parameters. We can pass the model new data viaThis is basically all you have to do! To have a better understanding, let us check what’s inside posterior : It’s a dictionary that contains the new observations. Let’s grab them and store them as y_pred .The variable y_pred is a numpy array containing 4000 observations for each of the 50 inputs in x_new , hence its dimensions are 4000x50. You can also inspect this via y_pred.shape . You can visualize this array as:For example, y_pred[:, 0] gives us 50 predictions, one for each entry in x_new , while y_pred[0, :] gives us 4000 predictions for the first element in x_new , namely -3.Using this table, we can get the mean and the standard deviation across the 4000 samples for each of the 50 new inputs and visualize the result.Nice, right? That’s just what we wanted! However, it feels like a bit much code, right? Especially if we merely want to do a really simple Bayesian linear regression and nothing fancy. Let’s see how we can achieve the same results faster.Similar to the R syntax, you rewrite our first simple model asThis is a nice feature you should know! Another great property about this submodule is that it integrates with pandas data frames very well. And you can even specify priors if you don’t like the defaults (normal for all coefficients, half-Cauchy for the error as far as I know). Take a look at the following code:In this article, you have learned the difference between regular linear regression and its Bayesian counterpart. Linear regression models output single values as predictions, while Bayesian linear regression models can output distributions as predictions.This has the advantage that we can quantify the uncertainty for our predictions, and be careful when the prediction interval is too large. In our example, we could see that the slope is clearly positive. Maybe it is not 3.2, but with high probability, it is between 1.7 and 3.7. In fact, I used 3 as a slope and 2 as an intercept. So, if it is better to underestimate the true y, let us use 1.7 as the slope. If overestimating is better, we could use 2.7.We have used PyMC3 to implement Bayesian linear regression for one variable. Of course, you can add as many variables as you wish (and your machine can handle). You just have to declare more variables. Either explicitly likeor more compact and scalable asNow, you should try it out on your own! It’s fun to play around with, especially if you leave the path of very simple models. You can use different distributions than the normal distribution for y, for example, the Student’s t-distribution or the Cauchy distribution.The tails of these two distributions are wider than the ones of the normal distribution. This has the effect that outliers don’t surprise, and hence influence the model as much as in the case of the normal distribution.Maybe we don’t want to predict continuous values at all but counts instead. In this case, we could use the Poisson distribution viaYou can do it manually, or provide a family argument to the glm submodule, as inHowever you do it, the possibilities are endless. Check your data, think about which distribution makes sense, or just test a lot of different distributions in the worst case! You can even create more complex models, such as Bayesian neural networks if you want. Just start with your own projects, and you will get more experience with this whole topic and become a Bayesian Master.I hope that you learned something new, interesting, and useful today. Thanks for reading!As the last point, if youwhy not do it via this link? This would help me a lot! 😊To be transparent, the price for you does not change, but about half of the subscription fees go directly to me.Thanks a lot, if you consider supporting me!If you have any questions, write me on LinkedIn!",30/12/2020,11,21.0,49.0,907.0,344.0,14.0,2.0,0.0,17.0,en
4238,Introduction to recommender systems,Towards Data Science,Baptiste Rocca,1200.0,22.0,4722.0,"This post was co-written with During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).Recommender systems are really critical in some industries as they can generate a huge amount of income when they are efficient or also be a way to stand out significantly from competitors. As a proof of the importance of recommender systems, we can mention that, a few years ago, Netflix organised a challenges (the “Netflix prize”) where the goal was to produce a recommender system that performs better than its own algorithm with a prize of 1 million dollars to win.In this article, we will go through different paradigms of recommender systems. For each of them, we will present how they work, describe their theoretical basis and discuss their strengths and weaknesses.In the first section we are going to overview the two major paradigms of recommender systems : collaborative and content based methods. The next two sections will then describe various methods of collaborative filtering, such as user-user, item-item and matrix factorization. The following section will be dedicated to content based methods and how they work. Finally, we will discuss how to evaluate a recommender system.The purpose of a recommender system is to suggest relevant items to users. To achieve this task, there exist two major categories of methods : collaborative filtering methods and content based methods. Before digging more into details of particular algorithms, let’s discuss briefly these two main paradigms.Collaborative methods for recommender systems are methods that are based solely on the past interactions recorded between users and items in order to produce new recommendations. These interactions are stored in the so-called “user-item interactions matrix”.Then, the main idea that rules collaborative methods is that these past user-item interactions are sufficient to detect similar users and/or similar items and make predictions based on these estimated proximities. The class of collaborative filtering algorithms is divided into two sub-categories that are generally called memory based and model based approaches. Memory based approaches directly works with values of recorded interactions, assuming no model, and are essentially based on nearest neighbours search (for example, find the closest users from a user of interest and suggest the most popular items among these neighbours). Model based approaches assume an underlying “generative” model that explains the user-item interactions and try to discover it in order to make new predictions.The main advantage of collaborative approaches is that they require no information about users or items and, so, they can be used in many situations. Moreover, the more users interact with items the more new recommendations become accurate: for a fixed set of users and items, new interactions recorded over time bring new information and make the system more and more effective.However, as it only consider past interactions to make recommendations, collaborative filtering suffer from the “cold start problem”: it is impossible to recommend anything to new users or to recommend a new item to any users and many users or items have too few interactions to be efficiently handled. This drawback can be addressed in different way: recommending random items to new users or new items to random users (random strategy), recommending popular items to new users or new items to most active users (maximum expectation strategy), recommending a set of various items to new users or a new item to a set of various users (exploratory strategy) or, finally, using a non collaborative method for the early life of the user or the item.In the following sections, we will mainly present three classical collaborative filtering approaches: two memory based methods (user-user and item-item) and one model based approach (matrix factorisation).Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about users and/or items. If we consider the example of a movies recommender system, this additional information can be, for example, the age, the sex, the job or any other personal information for users as well as the category, the main actors, the duration or other characteristics for the movies (items).Then, the idea of content based methods is to try to build a model, based on the available “features”, that explain the observed user-item interactions. Still considering users and movies, we will try, for example, to model the fact that young women tend to rate better some movies, that young men tend to rate better some other movies and so on. If we manage to get such model, then, making new predictions for a user is pretty easy: we just need to look at the profile (age, sex, …) of this user and, based on this information, to determine relevant movies to suggest.Content based methods suffer far less from the cold start problem than collaborative approaches: new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities. Only new users or items with previously unseen features will logically suffer from this drawback, but once the system old enough, this has few to no chance to happen.Later in this post, we will further discuss content based approaches and see that, depending on our problem, various classification or regression models can be used, ranging from very simple to much more complex models.Let’s focus a bit more on the main differences between the previously mentioned methods. More especially let’s see the implication that the modelling level has on the bias and the variance.In memory based collaborative methods, no latent model is assumed. The algorithms directly works with the user-item interactions: for example, users are represented by their interactions with items and a nearest neighbours search on these representations is used to produce suggestions. As no latent model is assumed, these methods have theoretically a low bias but a high variance.In model based collaborative methods, some latent interaction model is assumed. The model is trained to reconstruct user-item interactions values from its own representation of users and items. New suggestions can then be done based on this model. The users and items latent representations extracted by the model have a mathematical meaning that can be hard to interpret for a human being. As a (pretty free) model for user-item interactions is assumed, this methods has theoretically a higher bias but a lower variance than methods assuming no latent model.Finally, in content based methods some latent interaction model is also assumed. However, here, the model is provided with content that define the representation of users and/or items: for example, users are represented by given features and we try to model for each item the kind of user profile that likes or not this item. Here, as for model based collaborative methods, a user-item interactions model is assumed. However, this model is more constrained (because representation of users and/or items are given) and, so, the method tends to have the highest bias but the lowest variance.The main characteristics of user-user and item-item approaches it that they use only information from the user-item interaction matrix and they assume no model to produce new recommendations.In order to make a new recommendation to a user, user-user method roughly tries to identify users with the most similar “interactions profile” (nearest neighbours) in order to suggest items that are the most popular among these neighbours (and that are “new” to our user). This method is said to be “user-centred” as it represent users based on their interactions with items and evaluate distances between users.Assume that we want to make a recommendation for a given user. First, every user can be represented by its vector of interactions with the different items (“its line” in the interaction matrix). Then, we can compute some kind of “similarity” between our user of interest and every other users. That similarity measure is such that two users with similar interactions on the same items should be considered as being close. Once similarities to every users have been computed, we can keep the k-nearest-neighbours to our user and then suggest the most popular items among them (only looking at the items that our reference user has not interacted with yet).Notice that, when computing similarity between users, the number of “common interactions” (how much items have already been considered by both users?) should be considered carefully! Indeed, most of the time, we want to avoid that someone that only have one interaction in common with our reference user could have a 100% match and be considered as being “closer” than someone having 100 common interactions and agreeing “only” on 98% of them. So, we consider that two users are similar if they have interacted with a lot of common items in the same way (similar rating, similar time hovering…).To make a new recommendation to a user, the idea of item-item method is to find items similar to the ones the user already “positively” interacted with. Two items are considered to be similar if most of the users that have interacted with both of them did it in a similar way. This method is said to be “item-centred” as it represent items based on interactions users had with them and evaluate distances between those items.Assume that we want to make a recommendation for a given user. First, we consider the item this user liked the most and represent it (as all the other items) by its vector of interaction with every users (“its column” in the interaction matrix). Then, we can compute similarities between the “best item” and all the other items. Once the similarities have been computed, we can then keep the k-nearest-neighbours to the selected “best item” that are new to our user of interest and recommend these items.Notice that in order to get more relevant recommendations, we can do this job for more than only the user’s favourite item and consider the n preferred items instead. In this case, we can recommend items that are close to several of these preferred items.The user-user method is based on the search of similar users in terms of interactions with items. As, in general, every user have only interacted with a few items, it makes the method pretty sensitive to any recorded interactions (high variance). On the other hand, as the final recommendation is only based on interactions recorded for users similar to our user of interest, we obtain more personalized results (low bias).Conversely, the item-item method is based on the search of similar items in terms of user-item interactions. As, in general, a lot of users have interacted with an item, the neighbourhood search is far less sensitive to single interactions (lower variance). As a counterpart, interactions coming from every kind of users (even users very different from our reference user) are then considered in the recommendation, making the method less personalised (more biased). Thus, this approach is less personalized than the user-user approach but more robust.One of the biggest flaw of memory based collaborative filtering is that they do not scale easily: generating a new recommendation can be extremely time consuming for big systems. Indeed, for systems with millions of users and millions of items, the nearest neighbours search step can become intractable if not carefully designed (KNN algorithm has a complexity of O(ndk) with n the number of users, d the number of items and k the number of considered neighbours). In order to make computations more tractable for huge systems, we can both take advantage of the sparsity of the interaction matrix when designing our algorithm or use approximate nearest neighbours methods (ANN).In most of the recommendation algorithms, it is necessary to be extremely careful to avoid a “rich-get-richer” effect for popular items and to avoid getting users stuck into what could be called an “information confinement area”. In other words, we do not want that our system tend to recommend more and more only popular items as well as we do not want that our users only receive recommendations for items extremely close to the one they already liked with no chance to get to know new items they might like too (as these items are not “close enough” to be suggested). If, as we mentioned, these problems can arise in most of the recommendation algorithms, it is especially true for memory based collaborative ones. Indeed, with the lack of model “to regularise”, this kind of phenomenon can be accentuated and observed more frequently.Model based collaborative approaches only rely on user-item interactions information and assume a latent model supposed to explain these interactions. For example, matrix factorisation algorithms consists in decomposing the huge and sparse user-item interaction matrix into a product of two smaller and dense matrices: a user-factor matrix (containing users representations) that multiplies a factor-item matrix (containing items representations).The main assumption behind matrix factorisation is that there exists a pretty low dimensional latent space of features in which we can represent both users and items and such that the interaction between a user and an item can be obtained by computing the dot product of corresponding dense vectors in that space.For example, consider that we have a user-movie rating matrix. In order to model the interactions between users and movies, we can assume that:However we don’t want to give explicitly these features to our model (as it could be done for content based approaches that we will describe later). Instead, we prefer to let the system discover these useful features by itself and make its own representations of both users and items. As they are learned and not given, extracted features taken individually have a mathematical meaning but no intuitive interpretation (and, so, are difficult, if not impossible, to understand as human). However, it is not unusual to ends up having structures emerging from that type of algorithm being extremely close to intuitive decomposition that human could think about. Indeed, the consequence of such factorisation is that close users in terms of preferences as well as close items in terms of characteristics ends up having close representations in the latent space.In this subsection, we will give a simple mathematical overview of matrix factorization. More especially, we describe a classical iterative approach based on gradient descent that makes possible to obtain factorisations for very large matrices without loading all the data at the same time in computer’s memory.Let’s consider an interaction matrix M (nxm) of ratings where only some items have been rated by each user (most of the interactions are set to None to express the lack of rating). We want to factorise that matrix such thatwhere X is the “user matrix” (nxl) whose rows represent the n users and where Y is the “item matrix” (mxl) whose rows represent the m items:Here l is the dimension of the latent space in which users and item will be represented. So, we search for matrices X and Y whose dot product best approximate the existing interactions. Denoting E the ensemble of pairs (i,j) such that M_ij is set (not None), we want to find X and Y that minimise the “rating reconstruction error”Adding a regularisation factor and dividing by 2, we getThe matrices X and Y can then be obtained following a gradient descent optimisation process for which we can notice two things. First, the gradient do not have to be computed over all the pairs in E at each step and we can consider only a subset of these pairs so that we optimise our objective function “by batch”. Second, values in X and Y do not have to be updated simultaneously and the gradient descent can be done alternatively on X and Y at each step (doing so, we consider one matrix fixed and optimise for the other before doing the opposite at the next iteration).Once the matrix has been factorised, we have less information to manipulate in order to make a new recommendation: we can simply multiply a user vector by any item vector in order to estimate the corresponding rating. Notice that we could also use user-user and item-item methods with these new representations of users and items: (approximate) nearest neighbours searches wouldn’t be done over huge sparse vectors but over small dense ones making some approximation techniques more tractable.We can finally notice that the concept of this basic factorisation can be extended to more complex models with, for example, more general neural network like “decomposition” (we cannot strictly speak about “factorisation” anymore). The first direct adaptation we can think of concerns boolean interactions. If we want to reconstruct boolean interactions, a simple dot product is not well adapted. If, however, we add a logistic function on top of that dot product, we get a model that takes its value in [0, 1] and, so, better fit the problem. In such case, the model to optimise iswith f(.) our logistic function. Deeper neural network models are often used to achieve near state of the art performances in complex recommender systems.In the previous two sections we mainly discussed user-user, item-item and matrix factorisation approaches. These methods only consider the user-item interaction matrix and, so, belong to the collaborative filtering paradigm. Let’s now describe the content based paradigm.In content based methods, the recommendation problem is casted into either a classification problem (predict if a user “likes” or not an item) or into a regression problem (predict the rating given by a user to an item). In both cases, we are going to set a model that will be based on the user and/or item features at our disposal (the “content” of our “content-based” method).If our classification (or regression) is based on users features, we say the approach is item-centred: modelling, optimisations and computations can be done “by item”. In this case, we build and learn one model by item based on users features trying to answer the question “what is the probability for each user to like this item?” (or “what is the rate given by each user to this item?”, for regression). The model associated to each item is naturally trained on data related to this item and it leads, in general, to pretty robust models as a lot of users have interacted with the item. However, the interactions considered to learn the model come from every users and even if these users have similar characteristic (features) their preferences can be different. This mean that even if this method is more robust, it can be considered as being less personalised (more biased) than the user-centred method thereafter.If we are working with items features, the method is then user-centred: modelling, optimisations and computations can be done “by user”. We then train one model by user based on items features that tries to answer the question “what is the probability for this user to like each item?” (or “what is the rate given by this user to each item?”, for regression). We can then attach a model to each user that is trained on its data: the model obtained is, so, more personalised than its item-centred counterpart as it only takes into account interactions from the considered user. However, most of the time a user has interacted with relatively few items and, so, the model we obtain is a far less robust than an item-centred one.From a practical point of view, we should underline that, most of the time, it is much more difficult to ask some information to a new user (users do not want to answer too much questions) than to ask lot of information about a new item (people adding them have an interest in filling these information in order to make their items recommended to the right users). We can also notice that, depending on the complexity of the relation to express, the model we build can be more or less complex, ranging from basic models (logistic/linear regression for classification/regression) to deep neural networks. Finally, let’s mention that content based methods can also be neither user nor item centred: both informations about user and item can be used for our models, for example by stacking the two features vectors and making them go through a neural network architecture.Let’s first consider the case of an item-centred classification: for each item we want to train a Bayesian classifier that takes user features as inputs and output either “like” or “dislike”. So, to achieve the classification task, we want to computethe ratio between the probability for a user with given features to like the considered item and its probability to dislike it. This ratio of conditional probabilities that defines our classification rule (with a simple threshold) can be expressed following the Bayes formulawhereare priors computed from the data whereasare likelihoods assumed to follow Gaussian distributions with parameters to be determined also from data. Various hypothesis can be done about the covariance matrices of these two likelihood distributions (no assumption, equality of matrices, equality of matrices and features independence) leading to various well known models (quadratic discriminant analysis, linear discriminant analysis, naive Bayes classifier). We can underline once more that, here, likelihood parameters have to be estimated only based on data (interactions) related to the considered item.Let’s now consider the case of a user-centred regression: for each user we want to train a simple linear regression that takes item features as inputs and output the rating for this item. We still denote M the user-item interaction matrix, we stack into a matrix X row vectors representing users coefficients to be learned and we stack into a matrix Y row vectors representing items features that are given. Then, for a given user i, we learn the coefficients in X_i by solving the following optimisation problemwhere one should keep in mind that i is fixed and, so, the first summation is only over (user, item) pairs that concern user i. We can observe that if we solve this problem for all the users at the same time, the optimisation problem is exactly the same as the one we solve in “alternated matrix factorisation” when we keep items fixed. This observation underlines the link we mentioned in the first section: model based collaborative filtering approaches (such as matrix factorisation) and content based methods both assume a latent model for user-item interactions but model based collaborative approaches have to learn latent representations for both users and items whereas content-based methods build a model upon human defined features for users and/or items.As for any machine learning algorithm, we need to be able to evaluate the performances of our recommender systems in order to decide which algorithm fit the best our situation. Evaluation methods for recommender systems can mainly be divided in two sets: evaluation based on well defined metrics and evaluation mainly based on human judgment and satisfaction estimation.If our recommender system is based on a model that outputs numeric values such as ratings predictions or matching probabilities, we can assess the quality of these outputs in a very classical manner using an error measurement metric such as, for example, mean square error (MSE). In this case, the model is trained only on a part of the available interactions and is tested on the remaining ones.Still if our recommender system is based on a model that predicts numeric values, we can also binarize these values with a classical thresholding approach (values above the threshold are positive and values bellow are negative) and evaluate the model in a more “classification way”. Indeed, as the dataset of user-item past interactions is also binary (or can be binarized by thresholding), we can then evaluate the accuracy (as well as the precision and the recall) of the binarized outputs of the model on a test dataset of interactions not used for training.Finally, if we now consider a recommender system not based on numeric values and that only returns a list of recommendations (such as user-user or item-item that are based on a knn approach), we can still define a precision like metric by estimating the proportion of recommended items that really suit our user. To estimate this precision, we can not take into account recommended items that our user has not interacted with and we should only consider items from the test dataset for which we have a user feedback.When designing a recommender system, we can be interested not only to obtain model that produce recommendations we are very sure about but we can also expect some other good properties such as diversity and explainability of recommendations.As mentioned in the collaborative section, we absolutely want to avoid having a user being stuck in what we called earlier an information confinement area. The notion of “serendipity” is often used to express the tendency a model has or not to create such a confinement area (diversity of recommendations). Serendipity, that can be estimated by computing the distance between recommended items, should not be too low as it would create confinement areas, but should also not be too high as it would mean that we do not take enough into account our users interests when making recommendations (exploration vs exploitation). Thus, in order to bring diversity in the suggested choices, we want to recommend items that both suit our user very well and that are not too similar from each others. For example, instead of recommending a user “Start Wars” 1, 2 and 3, it seems better to recommend “Star wars 1”, “Start trek into darkness” and “Indiana Jones and the raiders of the lost ark”: the two later may be seen by our system as having less chance to interest our user but recommending 3 items that look too similar is not a good option.Explainability is another key point of the success of recommendation algorithms. Indeed, it has been proven that if users do not understand why they had been recommended as specific item, they tend to loose confidence into the recommender system. So, if we design a model that is clearly explainable, we can add, when making recommendations, a little sentence stating why an item has been recommended (“people who liked this item also liked this one”, “you liked this item, you may be interested by this one”, …).Finally, on top of the fact that diversity and explainability can be intrinsically difficult to evaluate, we can notice that it is also pretty difficult to assess the quality of a recommendation that do not belong to the testing dataset: how to know if a new recommendation is relevant before actually recommending it to our user? For all these reasons, it can sometimes be tempting to test the model in “real conditions”. As the goal of the recommender system is to generate an action (watch a movie, buy a product, read an article etc…), we can indeed evaluate its ability to generate the expected action. For example, the system can be put in production, following an A/B testing approach, or can be tested only on a sample of users. Such processes require, however, having a certain level of confidence in the model.The main takeaways of this article are:We should notice that we have not discussed hybrid approaches in this introductory post. These methods, that combine collaborative filtering and content based approaches, achieves state-of-the-art results in many cases and are, so, used in many large scale recommender systems nowadays. The combination made in hybrid approaches can mainly take two forms: we can either train two models independently (one collaborative filtering model and one content based model) and combine their suggestions or directly build a single model (often a neural network) that unify both approaches by using as inputs prior information (about user and/or item) as well as “collaborative” interactions information.As we mentioned in the introduction of this post, recommender systems are becoming essential in many industries and, hence, have received always more attention in the recent years. In this article, we have introduced basics notions required for a better understanding of the questions related to these systems, but we highly encourage interested readers to explore this field further… without giving, ironically, any specific reading recommendations!Thanks for reading!Last articles written with towardsdatascience.comtowardsdatascience.com",03/06/2019,0,0.0,3.0,1173.0,382.0,23.0,2.0,0.0,6.0,en
4239,A guide to transfer learning with Keras using ResNet50,Medium,Kenneth Cortés Aguas,19.0,11.0,1664.0,"In this blog post we will provide a guide through for transfer learning with the main aspects to take into account in the process, some tips and an example implementation in Keras using ResNet50 as the trained model. The task is to transfer the learning of a ResNet50 trained with Imagenet to a model that identify images from CIFAR-10 dataset. Several methods were tested to achieve a greater accuracy which we provide to show the variety of options for a training. However with the final model of this blog we get an accuracy of 94% on test set.Learning something new takes time and practice but we find it easy to do similar tasks. This is thanks to human association involved in learning. We have the capability to identify patterns from previous knowledge an apply it into new learning.When we meet a person than is faster or better than us in something like a video game or coding it is almost certain that he has do it before or there is an association with a previous similar activity.If we know how to ride a bike, we don’t need to learn from zero how to ride a motorbike. If we know how to play football, we don’t need to learn from zero how to play futsal. If we know how to play the piano, we don’t need to learn from zero how to play another instrument.The same is applicable to machines, if we train a model with a database, it’s not necessary to retrain from zero all the model to adjust to a new similar dataset. Both Imagenet and CIFAR-10 have images that can train a model to classify images. Then, it is very promising if we can save time training a model (because it can really take long time) and start using the weights of a previously trained model. We are going through this concept of transfer learning with all what you need to also build a model on your own.Setting our environmentWe are going to use Keras which is an open source library written in Python for neural networks. We work over it with tensorflow in a Google Colab, a Jupyter notebook environment that runs in the cloud.The first thing we do is importing the libraries needed with the line of code below. Running the version as 1.x is optional, without that first line it will run the last version of tensorflow for Colab. We also use numpy and a function of tensorflow but depending on how you build your own model is not necessary to import them.Training a model uses a lot of resources so we recommend using a GPU configuration in the Colab. This will speed up the process and allow more testing. We will talk about some other ways to improve computation soon.DatabaseCIFAR-10 is a dataset with 60000 32x32 colour images grouped in 10 classes, that means 6000 images per class. This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories.The categories are airplane, automobile, beer, cat, deer, dog, frog, horse, ship, truck. We can take advantage of the fact that these categories and a lot more are into the Imagenet collection.To load a database with Keras, we use:PreprocessNow that the data is loaded, we are going to build a preprocess function for the data. We have X as a numpy array of shape (m, 32, 32, 3) where m is the number of images, 32 and 32 the dimensions, and 3 is because we use color images (RGB). We have a set of X for training and a set of X for validation. Y is a numpy array of shape (m, ) that we want to be our labels. Since we work with 10 different categories, we make use of one-hot encoding with a function of Keras that makes our Y into a shape of (m, 10). That also applies for the validation.As we said before, we are going to use ResNet50 but there are also many other models available with pre-trained weights such as VGG16, ResNet101, InceptionV3 and DenseNet121. Each one has its own preprocess function for the inputs.Next, we are going to call our function with the parameters loaded from the CIFAR10 database. It’s important to get to know your data to monitor the steps and know how to build your model. Let’s print the shapes of our x_train and y_train before and after the preprocessing.A pretrained model from the Keras Applications has the advantage of allow you to use weights that are already calibrated to make predictions. In this case, we use the weights from Imagenet and the network is a ResNet50. The option include_top=False allows feature extraction by removing the last dense layers. This let us control the output and input of the model.From this point it all comes to testing and a bit of creativity. The starting point is very advantageous since we have weights that already serve for image classification but since we are using it on a completely new dataset, there is a need for adjustments. Our objective is to build a model that has high accuracy in their classifications. In this case, if an image of a dog is presented, it successfully identifies it as a dog and not as a train, for example.Let’s say we want to achieve an accuracy of more than 88% on training data but we also wish that it doesn’t have overfitting. How do we get this? Well at this point our models may diverge, this is where we test what tools we can use for that objective. The important here is to learn about transfer learning and making robust models. We follow an example but we can run with different approaches that we will discuss.The two aproaches you can take in transfer learning are:This refers on how you use the layers of your pretrained model. We have already a very huge amount of parameters because of the number of layer of the ResNet50 but we have calibrated weights. We can choose to ‘freeze’ those layers (as many as you can) so those values doesn’t change, and by that way saving time and computational cost. However as the dataset is entirely different is not a bad idea to train all the modelIn this case, we ‘freeze’ all layers except for the last block of the ResNet50. The way to do this in Keras is with:We can check that we did it correctly with:The output is something like this (the are more layer that we omit). False means that the layer is ‘freezed’ or is not trainable and True that when we run our model, the weights are going to be adjusted.Later, we need to connect our pretrained model with the new layers of our model. We can use global pooling or a flatten layer to connect the dimensions of the previous layers with the new layers. With just a flatten layer and a dense layer with softmax we can perform close the model and start making classification.The final layers are below, you can see the complete code here. However we explain some more aspects to improve the model and make a good classification. We present the main aspects taken into account to build the model.We have regularizers to help us avoid overfitting and optimizers to get a faster result. Each of them can also affect our accuracy, so we present what to take into account. The most important are:We obtained an accuracy of 94% on training set and 90% on validation with 10 epochs. In the 8th epoch, the values are very similar and it is interesting to note that in the first validation accuracy is higher than training. This is because of dropout use, which in Keras, it has a different behavior for training and testing. In testing time, all the features are ready and the dropout is turned off, resulting in a better accuracy. This readjust on the last epochs since the model continues changing on the training.The summary of the model is below. We found that batch normalization and dropout greatly reduces overfitting and it helps get better accuracy on validation set. The method of ‘freezing layers’ allows a faster computation but hits the accuracy so it was necessary to add dense layers at the end. The shape of the layers holds part of the structure of the original ResNet50 like it was a continuation of it but with the features we mentioned.For ResNet50 what helped more to achieve a high accuracy was to resize the input from 32 x 32 to 224 x 224. This is because of how the model was constructed which in this sense was not compatible with the dataset but it was easy to solve by fitting it to the original size of the architecture. There was the option of using UpSampling to do this task but we find that the use of Keras layers lambda was way faster.We confirmed that ResNet50 works best with input images of 224 x 224. As CIFAR-10 have 32 x 32 images, it was necessary to perform a resize. With this adjustment alone, the model can achieve a high accuracy, I think it was the most important for ResNet50.A good recommendation when building a model using transfer learning is to first test optimizers to get a low bias and good results in training set, then look for regularizers if you see overfitting over the validation set.The discussion over using freezing on the pretrained model continues. It reduces computation time, reduces overffiting but lowers accuracy. When the new dataset is very different from the datased used for training it may be necessary to use more layer for adjustment.On the selecting of hyperparameters, it is important for transfer learning to use a low learning rate to take advantage of the weights of the pretrained model. This choice as the optimizer choice (SGD, Adam, RMSprop) will impact the number of epochs needed to get a successfully trained model.riptutorial.comblog.keras.iotowardsdatascience.comstackoverflow.comhttps://www.cs.toronto.edu/~kriz/cifar.html",04/07/2020,7,6.0,2.0,543.0,196.0,11.0,2.0,0.0,13.0,en
4240,GPT-3 Is an Amazing Research Tool. But OpenAI Isn’t Sharing the Code.,OneZero,Dave Gershgorn,19100.0,8.0,30.0,"For years, A.I. research lab OpenAI has been chasing the dream of an algorithm that can write like a human.Its latest iteration on that concept, a language-generation algorithm called GPT-3…",20/08/2020,0,0.0,0.0,1400.0,787.0,1.0,0.0,0.0,0.0,en
4241,Introduction to Machine Learning for Beginners,Towards Data Science,Ayush Pant,660.0,6.0,874.0,"We have seen Machine Learning as a buzzword for the past few years, the reason for this might be the high amount of data production by applications, the increase of computation power in the past few years and the development of better algorithms.Machine Learning is used anywhere from automating mundane tasks to offering intelligent insights, industries in every sector try to benefit from it. You may already be using a device that utilizes it. For example, a wearable fitness tracker like Fitbit, or an intelligent home assistant like Google Home. But there are much more examples of ML in use.It was in the 1940s when the first manually operated computer system, ENIAC (Electronic Numerical Integrator and Computer), was invented. At that time the word “computer” was being used as a name for a human with intensive numerical computation capabilities, so, ENIAC was called a numerical computing machine! Well, you may say it has nothing to do with learning?! WRONG, from the beginning the idea was to build a machine able to emulate human thinking and learning.In the 1950s, we see the first computer game program claiming to be able to beat the checkers world champion. This program helped checkers players a lot in improving their skills! Around the same time, Frank Rosenblatt invented the Perceptron which was a very, very simple classifier but when it was combined in large numbers, in a network, it became a powerful monster. Well, the monster is relative to the time and in that time, it was a real breakthrough. Then we see several years of stagnation of the neural network field due to its difficulties in solving certain problems.Thanks to statistics, machine learning became very famous in the 1990s. The intersection of computer science and statistics gave birth to probabilistic approaches in AI. This shifted the field further toward data-driven approaches. Having large-scale data available, scientists started to build intelligent systems that were able to analyze and learn from large amounts of data. As a highlight, IBM’s Deep Blue system beat the world champion of chess, the grand-master Garry Kasparov. Yeah, I know Kasparov accused IBM of cheating, but this is a piece of history now and Deep Blue is resting peacefully in a museum.According to Arthur Samuel, Machine Learning algorithms enable the computers to learn from data, and even improve themselves, without being explicitly programmed.Machine learning (ML) is a category of an algorithm that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The basic premise of machine learning is to build algorithms that can receive input data and use statistical analysis to predict an output while updating outputs as new data becomes available.Machine learning can be classified into 3 types of algorithms.In Supervised learning, an AI system is presented with data which is labeled, which means that each data tagged with the correct label.The goal is to approximate the mapping function so well that when you have new input data (x) that you can predict the output variables (Y) for that data.As shown in the above example, we have initially taken some data and marked them as ‘Spam’ or ‘Not Spam’. This labeled data is used by the training supervised model, this data is used to train the model.Once it is trained we can test our model by testing it with some test new mails and checking of the model is able to predict the right output.In unsupervised learning, an AI system is presented with unlabeled, uncategorized data and the system’s algorithms act on the data without prior training. The output is dependent upon the coded algorithms. Subjecting a system to unsupervised learning is one way of testing AI.In the above example, we have given some characters to our model which are ‘Ducks’ and ‘Not Ducks’. In our training data, we don’t provide any label to the corresponding data. The unsupervised model is able to separate both the characters by looking at the type of data and models the underlying structure or distribution in the data in order to learn more about it.A reinforcement learning algorithm, or agent, learns by interacting with its environment. The agent receives rewards by performing correctly and penalties for performing incorrectly. The agent learns without intervention from a human by maximizing its reward and minimizing its penalty. It is a type of dynamic programming that trains algorithms using a system of reward and punishment.In the above example, we can see that the agent is given 2 options i.e. a path with water or a path with fire. A reinforcement algorithm works on reward a system i.e. if the agent uses the fire path then the rewards are subtracted and agent tries to learn that it should avoid the fire path. If it had chosen the water path or the safe path then some points would have been added to the reward points, the agent then would try to learn what path is safe and what path isn’t.It is basically leveraging the rewards obtained, the agent improves its environment knowledge to select the next action.In this blog, I have presented you with the basics concepts of Machine learning and I hope this blog was helpful and would have motivated you enough to get interested in the topic.",07/01/2019,0,4.0,0.0,873.0,521.0,8.0,4.0,0.0,0.0,en
4242,Federated Clusters with Docker Swarm,On Docker,Jeff Nickoloff,969.0,8.0,1345.0,"TL;DR Federated clustering overview with a focus on Swarm. Includes architecture diagrams and tools for building an experiment in AWS. Swarm’s API is a great building block that helps you create much more sophisticated deployment architectures or scale/diversify underlying infrastructure. Whale-Mullet is a Swarm fork I built to make the whole thing work.Docker Swarm provides an abstraction that allows a user to treat a cluster like a single node. That is the case as long as the Swarm API is mostly compatible with the Docker API. This begs the question, “If I can treat a Swarm like a single machine can I create a Swarm of Swarm clusters?” This is called cluster federation. This article describes what how I tried to build a federated Swarm cluster, what problems I encountered, how I made it work, and limitations of the system.Other than random curiosity there are a few reasons why you might build a federated cluster. You might want to create a multi-cloud deployment topology, isolate stages of your deployment pipeline while maintaining control through a centralized hub, or for PaaS multi-tenancy purposes. You might also want to build a cluster that is just bigger than anything any of these clustering technologies support. Until Kubernetes 1.1.2 the largest supported cluster size was 250 nodes, and 1.1.2 only increases that to 1000 nodes. In 1000 nodes is massive for a “large-scale server software” perspective, but that is a fairly narrow view of the world. 1000 nodes is nothing if we are talking about networked consumer products or IoT (sensor networks, vending machines, ATMs, thermostats, cars, etc).If you’re new to Docker or Swarm please consider checking out my book on the topic. The final chapter covers working containers on Swarm clusters in detail. Your support makes this content possible.www.manning.comThe configuration described below does not create networks that span the set of all local clusters. While each local cluster might implement some form of multi-host networking the logical federated cluster does not.It is worth mentioning that people have been doing this with Kubernetes already (at least experimenting with it). This article will not go into the fantastic detail as the Ubernetes proposal, but will provide a great starting place.github.comA Swarm based cluster is made up of three main components: a Swarm manager node (or nodes), a set of Docker nodes, and a node discover mechanism like a key-value store. There are a few popular and open source key-value store options. In this article I’ve used etcd, but could have just as easily used Consul, or ZooKeeper. The cluster works because all components can talk to each other via well known interface specifications. Docker nodes implement the Docker API. All nodes have an etcd driver that can register endpoints with etcd. The Swarm manager exposes the Swarm API — which is mostly compatible with the Docker API — to clients. Docker nodes join the cluster either using libnetwork at daemon startup, or using a Swarm container in join mode.Building a federated cluster typically requires some kind of specialization at higher levels. You might expect to find a special federation scheduler or higher-level deployment abstractions. Building a federated Swarm cluster relies heavily on delegation of responsibilities and hinges on treating each local cluster like a single logical node.Notice in the illustration above that all of the component of a single cluster are present. There is a single key-value store/cluster, a single (or high-availability) manager node, and two logical Docker nodes that have registered with the key-value store. This illustration also includes a client node that should be able to interact with the federated cluster just like a standard Swarm cluster or single Docker node.The illustration below reveals the detail behind the logical node abstraction and shows exactly what needs to be built.Each local cluster is a standard libnetwork or Swarm cluster. The only difference is running an additional “swarm join” container on the manager node that uses the federated cluster key-value store and advertises the Swarm manager port instead of the Docker engine port.This proof of concept is configured for rapid iteration, open inspection, and easy experimentation. It does not include appropriate security group configuration, network isolation, load balancer integration, or special roles for access to other resources.I launched a federated cluster in AWS using a CloudFormation template. After accessing the terminal on my bastion host I used the docker info subcommand to check the cluster health of each local cluster and the federated cluster:I found that the local clusters had come up healthy. The federated cluster had come up and the local cluster managers had joined as nodes but they were stuck in a pending state. I knew that I had come across one of a few minor differences between the Docker Remote API and the Swarm API. This does not work out of the box.There are documented API incompatibilities and Swarm has check to make sure that each node is actually an engine. But the API’s are fundamentally compatible. The Swarm API only adds information, minor data structure changes, and has different version information. Ideally, Swarm would behave like a proper building block and stack. It doesn’t leaving two options: fork Swarm or build an adapter.A real solution would either make the API truly compatible or make the manager of Swarms capable of speaking the Swarm API. In this case I forked Swarm for a proof of concept and made a few adjustments to the response structures. In other words, I made a Swarm manager lie about being an engine. In my experience forking a project like Swarm is a bad idea to own long term, but it is handy for a demo.github.com\The changes in Whale-Mullet subvert API version checking by delegating to the local Docker engine for a few specific calls. It also aggregates cluster information and presents it as if it is a single engine.Whale-Mullet is a quick hack that works as a proof of concept. Try it for yourself by replacing your local Swarm managers with the allingeek/whale-mullet image. You need to add two additional arguments to your standard Swarm manager startup:I started up a demo Swarm-of-Whale-Mullets in AWS and ran a few tests. The following screen captures show the operational federated cluster. The top-level Swarm manager is running on 10.0.0.22:3376; local Whale-Mullet cluster A on 10.0.0.20:3376; local Whale-Mullet cluster B on 10.0.0.21:3376.Running “docker info” on one of the Whale-Mullet endpoints shows the typical output you’d see from a two node Swarm cluster. It is up and healthy. It has two (t2.micro) engines running on randomly assigned private IP addresses.Running “docker info” on the Swarm manager shows an opaque view of the world. Each member of the federated cluster looks like a single machine. Container and image statistics are appropriately rolled up. This proof of concept did not address resource reporting and so CPU and memory numbers are incorrect. A real implementation would need to fix those gaps.You might be thinking, “Okay, docker info works, but does it run anything?” I’m happy to say that it does!The image above shows a “docker run” command being issued to the top-level Swarm manager. It demonstrates that the streams are correctly redirected through both levels of abstraction. The subsequent “docker ps” command also demonstrates how Swarm’s node prefixing of container names stacks correctly onto a federated cluster.I put this thing through a fairly complete functional test and most features worked great.As everyone gets used to orchestration platforms, distributed computing schedulers, and piling abstractions on top of abstractions I think cluster federation is an inevitable concern. Kubernetes has made real efforts toward federation and others will follow. Keeping clusters small and federating helps those platforms grow horizontally without abandoning the strong consistency they need for operation.Whale-Mullet is absolutely silly. But it is an awesome demonstration of what can be accomplished with composable services. Swarm really isn’t that far off, but I’m not sure this is a direction Docker wants to take the project.If you’re interested in learning more about Docker or Swarm please checkout my book, Docker in Action and help support my development of other articles like this one.",30/03/2016,3,4.0,3.0,1261.0,649.0,8.0,0.0,0.0,10.0,en
4243,Clustering Based Unsupervised Learning,Towards Data Science,Syed Sadat Nazrul,2700.0,6.0,805.0,"Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from “unlabeled” data (a classification or categorization is not included in the observations). Common scenarios for using unsupervised learning algorithms include:- Data Exploration- Outlier Detection- Pattern RecognitionWhile there is an exhaustive list of clustering algorithms available (whether you use R or Python’s Scikit-Learn), I will attempt to cover the basic concepts.The most common and simplest clustering algorithm out there is the K-Means clustering. This algorithms involve you telling the algorithms how many possible cluster (or K) there are in the dataset. The algorithm then iteratively moves the k-centers and selects the datapoints that are closest to that centroid in the cluster.Taking K=3 as an example, the iterative process is given below:One obvious question that may come to mind is the methodology for picking the K value. This is done using an elbow curve, where the x-axis is the K-value and the y axis is some objective function. A common objective function is the average distance between the datapoints and the nearest centroid.The best number for K is the “elbow” or kinked region. After this point, it is generally established that adding more clusters will not add significant value to your analysis. Below is an example script for K-Means using Scikit-Learn on the iris dataset:One issue with K-means, as see in the 3D diagram above, is that it does hard labels. However, you can see that datapoints at the boundary of the purple and yellow clusters can be either one. For such circumstances, a different approach may be necessary.In K-Means, we do what is called “hard labeling”, where we simply add the label of the maximum probability. However, certain data points that exist at the boundary of clusters may simply have similar probabilities of being on either clusters. In such circumstances, we look at all the probabilities instead of the max probability. This is known as “soft labeling”.For the above Gaussian Mixure Model, the colors of the datapoints are based on the Gaussian probability of being near the cluster. The RGB values are based on the nearness to each of the red, blue and green clusters. If you look at the datapoints near the boundary of the blue and red cluster, you shall see purple, indicating the datapoints are close to either clusters.Since we have talked about numerical values, let’s take a turn towards categorical values. One such application is text analytics. Common approach for such problems is topic modelling, where documents or words in a document are categorized into topics. The simplest of these is the TF-IDF model. The TF-IDF model classifies words based on their importance. This is determined by how frequent are they in specific documents (e.g. specific science topics in scientific journals) and words that are common among all documents (e.g. stop words).One of my favorite algorithms is the Latent Dirichlet Allocation or LDA model. In this model, each word in the document is given a topic based on the entire document corpus. Below, I have attached a slide from the University of Washington’s Machine Learning specialization course:The mechanics behind the LDA model itself is hard to explain in this blog. However, a common question people have is deciding on the number of topics. While there is no established answer for this, personally I prefer to implement a elbow curve of K-Means of the word vector of each document. The closeness of each word vector can be determined by the cosine distance.Finally, let’s cover some timeseries analysis. For clustering, my favourite is using Hidden Markov Models or HMM. In a Markov Model, we look for states and the probability of the next state given the current state. An example below is of a dog’s life in Markov Model.Let’s assume the dog is sick. Given the current state, there is a 0.6 chance it will continue being sick the next hour, 0.4 that it is sleeping, 05 pooping, 0.1 eating and 0.4 that it will be healthy again. In an HMM, you provide how many states there may be inside the timeseries data for the model to compute. An example of the Boston house prices dataset is given below with 3 states.As with every clustering problem, deciding the number of states is also a common issue. This may either be domain based. e.g. in voice recognition, it is common practice to use 3 states. Another possibility is using an elbow curve.As I have mentioned at the beginning of this blog, it is not possible for me to cover every single unsupervised models out there. At the same time, based on your use case, you may need a combination of algorithms to get a different perspective of the same data. With that I would like to leave you off with Scikit-Learn’s famous clustering demonstrations on the toy dataset:",03/04/2018,3,0.0,0.0,725.0,493.0,12.0,0.0,0.0,0.0,en
4244,Training a Conditional DC-GAN on CIFAR-10,Medium,Utkarsh Desai,135.0,6.0,1250.0,"After some promising results and tons of learning (summarized in my previous post) with a basic DC-GAN on CIFAR-10 data, I wanted to play some more with GANs. One issue with a traditional DC-GAN was that the data is expected to have similar properties in order for the training to converge properly. For instance, in case of CIFAR-10, training the DC-GAN on images of a single class was much easier and more likely to produce sharp images than training on all 10 classes. In that post on GAN learnings, I had casually mentioned Conditional GANs as an improvement over traditional GANs when the training data might come from different classes. This post describes how to setup a Conditional DC-GAN to generate images from all the classes of CIFAR-10 data.Generative Adversarial Networks have two models, a Generator model G(z) and a Discriminator model D(x), in competition with each other. G tries to estimate the distribution of the training data and D tries to estimate the probability that a data sample came from the original training data and not from G. During training, the Generator learns a mapping from a prior distribution p(z) to the data space G(z). The discriminator D(x) produces a probability value of a given x coming from the actual training data.This model can be modified to include additional inputs, y, on which the models can be conditioned. y can be any type of additional inputs, for example, class labels. The conditioning can be achieved by simply feeding y to both the Generator — G(z|y) and the Discriminator — D(x|y).The original paper on Conditional GAN used a fully connected network for both the Generator and the Discriminator and was trained on MNIST data to produce digit images. We will be training a Conditional Deep Convolutional GAN on CIFAR-10 data. As such, we will slightly differ from the paper in how we provide the conditioning input.For the CIFAR-10 data, the conditioning input will be class label of the image, in a One-hot representation. We define a tensor variable to do thisWe then define the Generator to accept this tensor as an input along with the latent variable tensor. This is done using the Keras Concatenate layer.Now that we have the Generator defined, lets define the Discriminator. Given an input image and a class label for the image, the job of the Discriminator is to decide whether the image is a real image of that class or not. To do this, we need to provide the conditioning input to the Discriminator as well and this is where things get tricky due to the Convolutional layers (remember we are using a DCGAN, so the Discriminator has Conv2D layers). It wasn’t really intuitive to me how the conditioning input can be applied to the Convolutional layers of the Discriminator . The only place that seemed appropriate was at the input of the top Dense layer. This setup also made sense because we can think of it as if the Discriminator is learning high level features from the image and using them in conjunction with the conditioning input to make the final decision.We define the discriminator model as follows:Finally, we can define our GAN model, keeping in mind the conditioning inputs that are required:The training process is as usual — alternating between training the discriminator and the generator. We make use of some tricks to make training easier, however. The detailed list is my previous post. We use flipped labels, soft targets and add noise to the discriminator targets.If you train a the GAN with everything mentioned above, you will probably end up getting a result like this:Not impressive, is it? Apart from the fact that the images are blurry, the bigger problem is that several images ended up looking similar (some examples highlighted), even when they belong to different classes.The above problem is extremely common in GAN training and is a major issue. The issue is referred to as Mode Collapse and a lot of work is being done in reducing its impact on the training process. Lets try to understand why it happens and then we can see some simple tricks to fix mode collapse to some extent.What is Mode Collapse?Mode Collapse refers to the scenario when the Generator produces the same (or almost same) images every time and is able to successfully fool the discriminator. Not only is mode collapse pretty common, it gets triggered unpredictably making it very difficult to train and evaluate GANs. The underlying reason behind mode collapse is simple:Real world data has distributions that are usually multi-modal. That is, there are some peaks — corresponding to high probability — in the distribution where the data usually resides. If the Generator is somehow able to identify one of these peaks (modes) and the Discriminator has not been trained well, it will fail to recognize that the generated data is simply coming from a single mode. One way to fix this would be to have the Discriminator assign low probability to samples generated from this mode. The Generator can, however, simply change to a different mode. In essence, the Generator keeps switching between a small number of modes rather than generating from the entire distribution. The Discriminator is unable to keep up with the Generator’s switching and hence, the result is a bunch of similar looking images.Tricks to tackle Mode CollapseMode collapse is an active area of research although there are some tricks that can be used to reduce the severity of the problem:1. Minibatch Discrimination Can we penalize the generator for generating similar looking samples directly? Turns out, yes. The idea is to use samples generated in a batch to determine whether the entire batch is real or fake. A term that represents the diversity in the samples (computed using feature matching) in a batch is added to the Generator’s cost function. If several samples in a batch are similar, the Discriminator is able to easily detect that and hence, the Generator is forced to generate diverse samples.2. Wasserstein GANs Traditional GANs try to minimize the JS divergence between the Generator’s distribution and the real data distribution. Instead, minimizing the Wasserstein distance has been found to work better and mode collapse is much less severe in Wasserstein GANs. Refer to the paper on WGANs for more details.3. Experience Replay Every now and then, we can show previously generated samples to the discriminator and hence prevent the generator from easily fooling the discriminator. This is easy to implement and is what we will be doing for our GAN.There are several more ways to deal with mode collapse, but explaining all would require a separate post.To implement experience replay, ideally, we would maintain a set of previously generated samples and during replay, pick a random subset of samples. As new samples come in, the set is modified to remove the older samples. However, we will implement a very naive way to do this, to avoid storing too many samples in memory. Every minibatch, we randomly pick one generated sample and save it. After N such samples have been collected, we show them to the discriminator and empty the set of samples. The code to do this looks like this:Finally, after adding all these changes, the result of the Conditional GAN is shown below. There is still a lot of room for improvement, but we have surely come a long way. Spending some more time on fine tuning the network should definitely yield better results.The complete code is available here.Related Papers:https://arxiv.org/abs/1411.1784https://arxiv.org/abs/1606.03498https://arxiv.org/abs/1701.07875",08/06/2018,0,2.0,4.0,558.0,363.0,3.0,0.0,0.0,5.0,en
4245,The proper way to use Machine Learning metrics,Towards Data Science,Félix Revert,2600.0,8.0,870.0,"Note I focus on binary classification problems in this article, but the approach would be similar with multi classification and regression problems.Try to convince someone that your ML model is accurate and should be trusted because it has a LogLoss of 0.34. Non data scientists will surely gawk at you while data scientists will ask for a lot more information.As a data scientist, you know it’s hard to make it clear (particularly to non data scientists) why your model should be trusted because you cannot easily translate complex measures of accuracy into tangible elements. And that’s 100% legitimate, models should be understood by everyone, at least their accuracy.On top of it, if your approach is scientifically correct, your model should have at least 2 accuracies — measured on training and validation sets — if not more — holdout, cross-validation.And there are many measures of accuracy. Precisely, 21 on scikit-learn for Classification problems only. So for 1 model, you would have 21 x 2 = 42 values on training and validation sets. What if you try 5 different models, would you look at 5 x 42 values?You see where it’s going:So the questions you should ask are how to use these measures of accuracy, how many of them you should check before validating your model and which ones you should share with non data scientists to convince them your model is accurate and which ones are too confusing to be shared.Some measures of accuracy are simple and straightforward, some are really complex. Some are a positive number, some are a series of numbers and some are charts! How to properly select and validate your model with such variety of measures…So… First thing first:Accuracy, Precision, Recall, Confusion Matrix. These are standard, you should already know them. They are simple to understand and are the ones that you must share with business teams.However, for the data scientist, they’re not good enough to identify the best model among several models. This is because these measures only focus on binary outcomes and not at the confidence (i.e probabilities) at which the model made predictions. This limitation is addressed in different ways by the following measures.Precision Recall curve: if you change the threshold probability of your model (generally by default at 0.5), the Precision and the Recall will vary in the opposite direction. The higher the threshold, the higher the Precision and the lower the Recall. Similarly, the lower the threshold, the lower the Precision and the higher the Recall. The goal is to identify the threshold with the best balance between Precision and Recall.F-1 score (and F-beta score): if selecting the threshold with the Precision-Recall curve is the empirical way, using the F-score is the mathematical way. If for a given problem, Precision is 5 times more important than Recall, then the model with the highest F-5 score (beta=5) should be the best model for this problem.F-beta score ranges from 0 to 1, 1 being a perfect model.ROC curve: ROC is the acronym of Receiver Operating Characteristic which was used by radar engineers in World War-II. This definition is obsolete in Machine Learning. ROC curve is simply a way to visualise a model’s performance. If a ROC curve is highly skewed to the top left, it means the model is very accurate, while a straight diagonal means the model is no better than tossing a coin.AUC: short for Area Under Curve, it’s basically the information contained from the ROC curve in one positive number. AUC is great because it makes it simple to compare multiple models: you select the one with the highest AUC. However it’s very hard to interpret the value of AUC. An AUC of 75% is in no way the same as an Accuracy of 75% (I hear that sometimes…). Usually, good values of AUC start from .75, but again, this depends on the problem and looking at absolute values is generally not helpful. You’d rather use it to compare models. If your model has an AUC of 0.57 that means there’s likely no signal in your data.AUC ranges from 0.5 and 1, 1 being a perfect model.LogLoss: short for logarithmic loss, a more mathematical and abstract notion here. It assigns a weight to each predicted probability. The further the probability from the actual value, the larger the weight. The goal is to minimize the overall sum of all the error weights. Note that the weight drastically increases if the probability is close to 0 and the actual is 1 (same with the opposite values 1 and 0). LogLoss discriminates models that are too confident on wrong predictions and is largely used in Machine Learning because it has useful mathematical properties. The problem is that the value of the LogLoss is impossible to interpret.You could compute these measures of accuracy on all the models you try, find optimal thresholds for each of the models, compare their Confusion Matrices with their optimal threshold and finally take your decision on which model x threshold fits the best to your problem. This would take multiple iterations, thousands of lines of Python code and a solid number of headaches to complete.Or you could follow a simpler approach that’s as efficient. An approach in 3 steps:In details,",10/09/2019,0,22.0,12.0,1148.0,609.0,8.0,7.0,0.0,11.0,en
4246,Few-shot Object Detection in Practice,Moonvision,Alexander Hirner,152.0,5.0,919.0,"Object detection is vital to automate manual tasks, such as checking the completeness of objects and the exact types of its parts. In contrast to segmentation, objects are located and classified as discrete instances. This is achieved by decoding regression and activation maps after a cascade of convolutions. You can read more about state-of-the art in object detection in this survey.However, contemporary issues in object detection are often studied in isolation. In production use cases though, multiple constraints must be solved at once. In this post, we describe the combination of techniques that we’ve developed over time that meet many of these constraints.As with any machine learning task, the amount of training data is limited. As we will review below, there are many approaches to conquer low-data scenarios, each with its own remaining problems.First, raw or weakly supervised data might be available plentifully (think of social media photos or passively recorded video). Such data can be mined by iterative methods: from few ground-truth samples you can generate new training samples and start retraining from the expanded set (fully supervised or weakly supervised). The remaining problem is to achieve competitive accuracy with this approach, especially regarding positions and sizes of the bounding boxes. These so called IoU metrics are often underreported in benchmarks.Secondly, if your target domain is similar to a richly annotated source domain, you can transfer detection know-how from that source domain. Concretely, localizing objects benefits greatly from such techniques and can be done with different but often complicated means (domain transfer with regularization, meta-learning to transfer). Of course, this technique fails if there is no properly annotated source domain. More exotic approaches try to localize and classify by searching for similarity in latent space. This however has a high computational cost per class and is limited to single instances or fragile bounding shapes based on thresholding operations.Another problem, often neglected in works that focuses on low-data regimes, are unbalanced classes. This means that there are many more examples of some classes than for others (think Catahoula Leopard Dog vs. German Shepherd). The problem cannot be fully solved by simply over- or undersampling, because rare items co-occur with frequent items in the same image.A fourth problem is that of extensibility. If you finally have a well trained model for a set of classes, it’s hard to add new ones, even if it’s just a special case to a common category.Our experiments in Summer 2018 showed that ROI-align based descriptors eradicate features between different instances of the same class. The visualization in Fig. 1 shows that ROI-align features from an SSD model are insensitive to differences between instances of the same super class (similar color). Thus intra-class variance cannot be inferred from such features without full retraining on the annotated hierarchy or the risk of catastrophic forgetting. Taken to the extreme, every new example can come from entirely unknown classes.We address the remaining problems mentioned above by a pipeline that combines object mining, fast annotation and few-shot classification.Our domains are often new-to-the-world. Thus the most generic way to acquire raw data is video. We use a pretrained model to mine objects if there is a similar one. On an entirely new domain, hand-engineered detectors or motion patterns can be used instead. In one case, we processed 600GB of videos and produced over 12.000 object candidates automatically to bootstrap that process. Unlike seeding some ground truth examples only in the beginning, we ask experts on demand for a few examples before iterating that process. This provides a way to add details that don’t exist anywhere yet and allows to evaluate models with high rigor. Thanks to our recommendation system, this annotation process is also fast.As seen above, fast object detectors don’t cope well with unbalanced or evolving data. Once we possess the initial training set in the form of shapes and labels, we automatically divide the hierarchy into groups that have enough in common to be determined at first sight and those which differ in subtle differences. Instances of the latter kind are then classified by additional layers at higher resolution.These layers have the objective to transform instances with the same visual attributes to a low dimensional vector within a tight volume in euclidean space or onto the patch of a hypersphere. Conversely, instances with different attributes occupy a distant volume [Metric learning paper].With such a metric, you can easily classify unseen instances by nearest-neighbor search, build classifiers with guaranteed convergence like SVMs or determine the novelty of incoming examples. The exact way to create these embedding layers warrants another post. However, the full body of know-how regarding unbalanced classification can be utilized rather easily.As a result, the combination of smart object mining and few-shot detection has the following properties:The problem of detecting a quickly rotating set classes was used in many applications. In the case of counting dishes, meals and some items appear only rarely and change daily. Here, having just a few (as low as 1) examples for training is paramount to automate the checkout process. Similarly, for our automated checkout solution at Sacher, we were able to accommodate over 50 items from just 2h of raw video data. Finally, many industrial assembly tasks require checking an ever changing catalogue of items.The approach outlined above allows us to focus computation on the details that matter whilst learning continuously. Moreover, the pipeline is very flexible, so that we were able to integrate ever more advanced techniques like generative adversarial training (GAN) easily. Many more improvements are on the way.",12/04/2019,0,7.0,13.0,850.0,368.0,2.0,1.0,0.0,13.0,en
4247,Object detection with Tensorflow model and OpenCV,Towards Data Science,Gabriel Cassimiro,51.0,3.0,452.0,"In this article, I’m going to demonstrate how to use a trained model to detect objects in images and videos using two of the best libraries for this kind of problem. For the detection, we need a model capable of predicting multiple classes in an image and returning the location of those objects so that we can place boxes on the image.We are going to use a model from the Tensorflow Hub library, which has multiple ready to deploy models trained in all kinds of datasets and to solve all kinds of problems. For our use, I filtered models trained for object detection tasks and models in the TFLite format. This format is usually used for IoT applications, for its small size and faster performance than bigger models. I choose this format because I intend to use this model on a Rasberry Pi on future projects.The chosen model was the EfficientDet-Lite2 Object detection model. It was trained on the COCO17 dataset with 91 different labels and optimized for the TFLite application. This model returns:I’m going to divide this section into two parts: Detections on static images and detection on live webcam video.We will start by detecting objects in this image from Unsplash:So the first thing we have to do is load this image and process it to the expected format for the TensorFlow model.Basically, we used OpenCV to load and do a couple of transformations on the raw image to an RGB tensor in the model format.Now we can load the model and the labels:The model is being loaded directly from the website however, you can download it to your computer for better performance on the loading. The text labels CSV is available on the project repo.Now we can create the predictions and put in the image the boxes and labels found:Now if we run plt.imshow(img_boxes) we get the following output:Now we can move on to detecting objects live using the webcam on your pc.This part is not as hard as it seems, we just have to insert the code we used for one image in a loop:Then we get:We used VideoCapture from open cv to load the video from the computer webcam. Then we did the same processing that we used on the static image and predicted the labels and positions. The main difference is that the image input is continuous so we inserted the code inside a while loop.All the code and notebooks used are in this repository:github.comIn the near future, I will load this into a raspberry pi to create some interactions using a model capable of detecting objects, and post the results here.If you like the content and want to support me, you can buy me a coffee:www.buymeacoffee.com",15/07/2021,0,0.0,2.0,1030.0,803.0,4.0,1.0,0.0,8.0,en
4248,Batch Normalization and ReLU for solving Vanishing Gradients,Analytics Vidhya,Lavanya Gupta,178.0,7.0,1012.0,"A logical and sequential roadmap to understanding the advanced concepts in training deep neural networks.We will break our discussion into 4 logical parts that build upon each other. For the best reading experience, please go through them sequentially:1. What is Vanishing Gradient? Why is it a problem? Why does it happen?2. What is Batch Normalization? How does it help in Vanishing Gradient?3. How does ReLU help in Vanishing Gradient?4. Batch Normalization for Internal Covariate ShiftFirst, let’s understand what vanishing means:Vanishing means that it goes towards 0 but will never really be 0.Vanishing gradient refers to the fact that in deep neural networks, the backpropagated error signal (gradient) typically decreases exponentially as a function of the distance from the last layer.In other words, the useful gradient information from the end of the network fails to reach the beginning of the network.❓The crucial question at this stage is: why is it a problem if the initial starting layers of the network receive a very small gradient?To understand this, recollect what is the role of a “gradient”? Well, a gradient is just the measure of how much the output variable changes for a small change in the input. And this gradient is then used to update/learn the model parameters — weights and biases. Below is the parameter updation rule typically followed:Coming back to the issue at hand — what will happen if the derivative term in the above equation is too small, i.e- almost zero? We can see that a very small derivative would update or change the value of Wx only by a minuscule amount and hence the (new) Wx* would be almost equal to the (older) Wx. In other words, no change has been made to the model weights. And no change in the weights means no learning. The weights of the initial layers would continue to remain unchanged (or only change by a negligible amount), no matter how many epochs you run with the backpropagation algorithm. This is the problem of vanishing gradients!Next, we move on to understand the mathematical reasoning of why vanishing gradients take place.❓The crucial question at this stage is: why do the initial starting layers of the network receive a very small gradient? Why do the gradient values diminish or vanish as we travel back into the neural network?Vanishing gradients usually happen while using the Sigmoid or Tanh activation functions in the hidden layer units. Looking at the function plot below, we can see that when inputs become very small or very large, the sigmoid function saturates at 0 and 1 and the tanh function saturates at -1 and 1. In both these cases, their derivatives are extremely close to 0. Let’s call these ranges/regions of the function “saturating regions” or “bad regions”.Thus, if your input lies in any of the saturating regions, then it has almost no gradient to propagate back through the network.As the name suggests, batch normalization is some kind of a normalization technique that we are applying to the input (current) batch of data. Omitting the rigorous mathematical details, batch normalization can be simply visualized as an additional layer in the network that normalizes your data (using a mean and standard deviation) before feeding it into the hidden unit activation function.But how does normalizing the inputs prevent vanishing gradients? It’s now time to connect the dots!❓The crucial question at this stage is: How does normalizing the inputs ensure that the initial layers of the network do not receive a very small gradient?Batch normalization normalizes the input and ensures that|x| lies within the “good range” (marked as the green region) and doesn’t reach the outer edges of the sigmoid function. If the input is in the good range, then the activation does not saturate, and thus the derivative also stays in the good range, i.e- the derivative value isn’t too small. Thus, batch normalization prevents the gradients from becoming too small and makes sure that the gradient signal is heard.Now, although the gradients have been prevented from becoming too small, the gradients are still small because they always lie between [0,1]. Specifically, the derivate of sigmoid ranges only from [0, 0.25], and the derivative of tanh ranges only from [0, 1]. What could be an implication of this?To get an answer, recollect the steps involved in training a deep neural network:Thus, batch normalization alone cannot solve the problem of vanishing gradients when using with sigmoid and tanh.We saw in the previous section that batch normalization + sigmoid or tanh is not enough to solve the vanishing gradient problem. We need to use batch normalization with a better activation function — ReLU!What makes ReLU better for solving vanishing gradients?a) It does not saturateb) It has constant and bigger gradients (as compared to sigmoid and tanh)Below is a comparison of the gradients of sigmoid, tanh, and ReLU.ReLU has gradient 1 when input > 0, and zero otherwise. Thus, multiplying a bunch of ReLU derivatives together in the backprop equations has the nice property of being either 1 or 0. There is no “vanishing” or “diminishing” of the gradient. The gradient travels to the bottom layers either as is or it becomes exactly 0 on the way.There is another reason why batch normalization works. The original batch normalization paper claimed that batch normalization was so effective in increasing the deep neural network performance because of a phenomenon called “Internal Covariate Shift”.According to this theory, the distribution of the inputs to hidden layers in a deep neural network changes erratically as the parameters of the model are updated during backprop.Since one layers’ outputs act as inputs for the next layer, and the weights are also being continuously updated for every layer through backprop — this means that the input data distribution of every layer is also constantly changing.Using batch normalization, we limit the range of this changing input data distribution by fixing a mean and variance for every layer. In other words, the input to each layer is now not allowed to shift around much — constrained by a mean and variance. This weakens the coupling between the layers.stats.stackexchange.comcs231n.github.iodatascience.stackexchange.comtowardsdatascience.comwww.quora.comcs224d.stanford.edu",26/04/2021,0,15.0,9.0,898.0,524.0,5.0,1.0,0.0,8.0,en
4249,K-Means Clustering and the Gap-Statistics,Towards Data Science,Tim Löhr,21.0,10.0,1845.0,"There is a lot of code going on under the hood. That’s why I provide my Github repository at the end of this post and I show just a little code of the K-Means.Clustering is an important technique in Pattern Analysis to identify distinct groups in data. Due to data being mostly more than three-dimensional, we perform dimensionality reduction methods like PCA or Laplacian Eigenmaps before applying a clustering technique. The data is then available in 2D or 3D and this allows us to visualize the found clusters very nicely to humans. Even though this is a basic workflow, it is not always the case.Data is also often unlabeled. This means you have no clear definition of what you want to find within this data. That’s why clustering is a good data exploration technique as well without the necessity of dimensionality reduction beforehand.Common clustering algorithms are K-Means and the Meanshift algorithm. In this post, I will focus on the K-Means algorithm, because this is the easiest and most straightforward clustering technique. We furthermore will assume that the data is either directly provided with two features (so 2D), or someone performed a 2D dimensionality reduction on the data and gave it then to us. Therefore, we directly dive deep into applying K-Means.The K-Means algorithm requires one hyperparameter, namely the number of clusters K you want to find. But, if we wanna find clusters, how can we know how many clusters we are going to need?Example: If we want to find personas for our marketing team in the data, we could assume that we want to find three or four clusters of people.In this case, the number of K would be fixed. But what if not?The choice of hyperparameters is called Model Selection. In the case of K-Means, this is only the number of K, as I already said. In this blog post, I will focus on how we can statistically find out what the optimum value for K is, by performing Tibshirani’s Gap-Statistics on K-Means.Gap-Statistics was introduced by Robert Tibshirani in the year 2000 at Stanford.I want to answer these three questions with this post:K-Means performs three steps. But first you need to pre-define the number of K. Those cluster points are often called Centroids.It can be seen in code 1 above, that line 9 computes with a list comprehension for each data point the euclidian distance to each of the center points ( K = 3 shown in line 3). Numpy.argmin then chooses the closest distance for each point and assigns it to this centroid. Line 13 calculates the third step and sums up the total within-cluster distance. If it’s less than 0.01, the while loop breaks.We can see how the assignment of the points changes for each while loop. It takes five loops in total until convergence.Notice: The initialization for the centroids plays a HUGE role of how the K-Means will perform. Imagine two of the centroid start in the most left and lowest position. Every data point will be assigned to centroid number one and the clustering is useless.Notice: K-Means is only locally optimal and there is no guarantee for a global minimaFigure 1 shows 3 centroids for 300 data points. The code for the data I did not show yet, it was just generated from sklearn with make_blobs.Of course, this within-cluster distance can not sink any further with three globally optimal centroids. But what happens when we add more and more values for K? The within-cluster distance would shrink almost monotonically. Imagine if we split up one of the three clusters of Figure 1 in two with an additional centroid. The within-cluster distance would shrink, because all points are now closer to another centroid, even if this would be clearly a wrong cluster split.When we choose the number of K equal to the number of centroids, the total within-cluster distance would be 0. From a minimization problem’s point of view this is ideal, but not for us.When we chose the number of K equal to eleven, we can see that we will never reach less than 3 total within-cluster distance.Due to a starting point of 1 in the plotting of Figure 2 and 3, the actual within-cluster distance of Figure 2 is zero and for Figure 3 it is two.So how can we prevent that K is chosen to be equal the number of data points?The answer is of course the Gap-Statistics, but what is it?It was invented by Robert Tibshirani 20 years ago. The basic minimization problem looks like the following:Looks weird? No problem! K* is the optimal value of K that we want to find. We need to define different data matrices W_data and W_uniform. Where W_data are the 300 data points from Figure 1 above and W_uniform is basically a simulated and averaged distribution of the within-cluster distances, as explained in the Further investigation part. W_uniform basically looks like Figure 3.We can see in the where above, that S’ (S prime) equals the previous S multiplied by a small value (1.024695). This means that it continuously grows bigger at each timestep.The initial S at the first timestep equals the standard deviation of all data points which were uniformly drawn across, for example 20 runs. It could be more, but Tibshirani wrote that 20 simulation is a sufficient number.Alright. As we already pointed out, we need a way, so that the optimal number for K is not being chosen as the number of data points. For this reason, the simulation average W_unifrom and standard deviation S’(K+1) is introduced.By checking in the minimization problem in Figure for each time step in Formula 1, we take the number of K, which makes the biggest jump into a reduced total within-cluster distance by introducing a new centroid.If you have a close look at Figure 3, you can see that the jumps of distance reduction are especially high for the first few centroids. This makes intuitive sense, because if you have a look back on Figure 1, it is clear that the more centroids you introduce, the smaller the changes in the within-cluster distances become.So the basic idea of the Gap Statistics is to choose the number of K, where the biggest jump in within-cluster distance occurred, based on the overall behavior of uniformly drawn samples. It could be the case that only a very slight reduction in within-cluster distance occurred. For this reason, S’(K+1) acts as a threshold, to sort out too tiny changes and to remove the sampling noise from the data. Only if the change is so big that the threshold S’(K+1) plays no role anymore, the optimal value of K will be selected.Let me visualize it for you:Notice: The values for the curves for the within-cluster distances have been normalized by the maximum cluster distance.I chose the data on the upper left image of Figure 4 on purpose in such a way, the lower two clusters are very nearby. It is almost impossible for K-means to distinguish that these are different clusters.On the lower left image, we can see the Gap Statistics. The optimal value for K=3 is chosen, because we select the first peak point before the value shrinks again. The red line is calculated by subtracting the W_uniform (green) from the W_data (blue) from the lower right plot.One step back: The lower right image shows the W_data (blue) and W_uniform (green) distribution. By looking at Formula 2 on the G(K), we see that we need to subtract the log of W_uniform by the log of W_data to.The total within-cluster distance shrinks as expected for the W_data the same as for the simulated W_uniform. But at K=4, the Gap Statistics detects that the change of total distance for W_data does not behave like the simulated one. This means that it did not decrease as expected.Even when the standard deviation S’(K+1) is subtracted from W_uniform, the optimal value for K will be selected as 3 by the Gap Statistics.Notice: if we pretend that the gap at K=4 was not present, the next point which is lower to its predecessor is at K=6. But since I marked the standard deviation as the vertical fat red lines, we see, that after subtracting the standard deviation, the change in distance is too small such that K would have been selected to K=6. The next K that would be selected is K=7, because at K=8 occurs the next big gap.As I already mentioned, the initialization of the centroids highly affects the optimization of the K-Means. We need to make some consideration with different data distributions. There are two basic cases we need to have in mind when calculating the 2nd step of the K-Means algorithm:If we take those two differentiations into consideration before we calculate the Gap-Statistics, then the Gap-Statistics will be much more robust.For my example, the overall cluster distances were small, so I calculated the average as can be seen in line 12 of the code 1, where I use Numpy.mean.There are three scenarios that could occur:If two or three clusters are very close together and the other clusters are far apart, it tends to underestimate. This is exactly what happened in the example I have shown to you previously. Two of the clusters were too close together, so the Gap-Statistics underestimated the K*.If all clusters are close to together, it rather overestimates than underestimates. I can not really explain why, but this is what happened when I computed it a couple of times.Both underestimation and overestimation depend mostly on the randomly initialized centroids. When some of them get omitted due to random unluck, the break in the within-cluster distance forces the gap statistic to produce the optimum cluster earlier.Even if Gap-Statistics is a good approach to find a suitable K, it is still not perfect. For example, we needed to introduce a new hyperparameter, namely the number of K for which the W_uniform is simulated on. We can’t be sure what the ideal value for this is. Furthermore, the random initialization of the centroids can lead to an over- or underestimation of K*.But by knowing all of the aspects of Gap-Statistics, the best is to apply it and then run the Gap-Statistic plot for a couple of times. Taking the average of the Gap-Statistics can be an increased evaluation criterion.Hopefully you liked this post and it gave you some insights into Gap-Statistics.You can find the entire code for this project on my Github page here. You can write me an Email if you have questions regarding the code. You can find my Email address on my website.[1] Trevor Hastie, Robert Tibshirani and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2009), Springer[2] Trevor Hastie, Robert Tibshirani and Guenther Walther, Estimating the number of clusters in a data set via the gap statistics (2000)This blog post is based from knowledge gained through the course Pattern Analysis from the Friedrich Alexander University Erlangen-Nürnberg. I used some parts of the lecture from Dr. Christian Riess to illustrate the examples for this post. So all rights go to Dr. Christian Riess.                      ",22/10/2020,0,28.0,6.0,906.0,440.0,8.0,5.0,0.0,7.0,en
4250,Of brains and cities; neuroscience and cultures of decision-making,Dark Matter and Trojan Horses,Dan Hill,21000.0,20.0,4068.0,"A chilly December night in 2011. I had been invited to take part in an evening event called the North House Salon, one of a series of salons organised by Dr Sarah Caddick, neuroscience advisor to Lord David Sainsbury (ex-Minister for Science and Innovation in the UK government) and the Gatsby Foundation, bringing together various “expert groups” with select groups of neuroscientists. It was an absolute privilege to share a conversation with some of the UK’s leading scientists. It’s always fascinating to see another discipline at work, and we were also fortunate that they were all great communicators as well as great researchers.This particular event was a collaboration with one of my old bosses at Arup, Dr Chris Luebkeman, Director of Arup’s Global Foresight, and it concerned the potential correlations between our emerging understanding of the brain, and our understanding of cities. (Perhaps we should also say our emerging understanding of cities?) The event was dubbed The Urban Nervous System.As Chris put it in his intro, we do have an increasingly shared vocabulary and way of thinking emerging about the systems of the brain and the systems of cities. This may partly be due to biomimicry shaping design discourse, partly the vogue for “smart cities” strategies, and partly because of recent advances in “brain science” (note: neuroscience is to some extent now seen as part of a continuum including behavioural psychology, behavioural economics, neurology, developmental biology and others. I’ll be using the term brain science as short-hand for all that. At one point, we tried to discuss the limits of neuroscience. We didn’t get very far.)Team Neuroscience (not that we lined up as teams, of course) consisted of Doctors Peter Latham (UCL), Semir Zeki (UCL), Daniel Wolpert (Cambridge University), Troy Margrie (National Institute for Medical Research/UCL), Dmitri Kullmann (UCL), Steve Wilson (UCL) the aforementioned Sarah Caddick, and Geoffrey West (Santa Fe Institute). (Note that Geoffrey is not a neuroscientist but a physicist, and could probably swap sides at half-time, should he want to, at least to some extent; again, boundaries were intriguingly eroding.)On Team Urbanist we had Mark Bidgood and Duncan Wilson (both Arup), Robin Daniels (Living PlanIT), and me. Actually, the ‘teams’ really were non-existent; the presentations were mixed up, as was the conversation — in a good way. (Unfortunately, as you can see, the gender (im)balance was not good, although that’s partly because a few people couldn’t make it, sadly.)The format was papers sent beforehand (I sent this, this and this), and on the evening, three-minute presentations from all participants — some slides, some not — followed by drinks reception and talking, followed by dinner and more talking, followed by pub for a few of us.So how did the conversation start? The presentations in order …Mark Bidgood (Arup) talked of Arup’s work in civil engineering and infrastructure, overlaying a set of biological metaphors — buildings as organs; power,water, sewerage as vascular system; information and communications technologies as the nervous system … He talked of his work in Riyadh (with Carlo Ratti and others), and mentioned Masdar as an exception. He saw the real value in retrofitting existing cities. But in terms of the relationship between brains and cities, he noted that physical utilities infrastructure doesn’t tend to self-repair, learn or adapt (instead, focus is on robustness, reliability, repairability; and so huge networks are underground and highly simple.) He also talks about the biggest roadblocks: the commercial and regulatory side, and generating the political will for change whilst enabling people to have freedom of action and choice. (I like this last point in particular; it resonated with my talk, later.)Duncan Wilson (Arup Foresight) talked about the internet of things, based on his long-standing interest in autonomous networks. He notes haven’t exactly become ubiquitous in the physical world, yet they have on the internet. In this context, he was interested in feedback loops and behaviour change, and so was looking for a better understanding of cognition to aid system design for behaviour change ie. how people might pick up, absorb and act upon cues. (I’ve worked with Duncan for years, and he understands as much about the potential of sensors as anyone; good to see him looking deeper into the psychology.)Peter Latham (UCL) took the cue directly: giving what he described as a naive answer to “what brains tell us about urban planning.” Latham delivered a rapid-fire, entertaining talk, casually noting we have 100 billion neurons and around 8 million kilometers of wires (axons) in the brain. He then extrapolated to transport systems (which is the natural, if problematic, thing to do — given that information transfer does not necessarily imply physical transportation these days), so he quickly painted a picture of local roads within neurons, and large spaces (parks and trees, in Peter’s city) in between dense nodes of “highrises”, or concentrations of activity. So a brain scientist ends up making a case for density too, which is good to hear. (Peter was of course much smarter than his deliberately “naive” answer, and was constantly insightful and entertaining all evening.)Robin Daniels (Living PlanIT) said his interest is in managing data, and particularly in smart urban environments. He talks about Living PlanIT’s work in big regeneration projects, aiming to use resources more efficiently, via sensors that collect data, and then manage it in better way. He talks of their platform, and makes an analogy to the iPhone: he says the applications are what make it interesting. Their apps (“PlaceApps”) could “control everything from luminaries to transportation systems”; the data could be self learning. (To be honest, I haven’t been impressed with Living PlanIT’s vision so far, and there was little this evening to change my mind; to be fair, it probably wasn’t the right setting for them.)Semir Zeki (UCL) is developing the field of “neuroaesthetics” of UCL (and his paper, particularly “The Disunity of Consciousness”, was perhaps the most interesting reading sent around beforehand.) He noted that a quarter to a third of the brain is devoted to vision, and is interested in how these layered activities of perception combine to give us a uniform view of visual world. He describes how we see colour before we see visual motion, for instance, which then brings up the “binding problem”. How do we arrange a unity of vision? Are we asking the right question, even, he says. And then throws in a few examples: What are the minimum conditions necessary for visual consciousness? What does art tell us about the brain? If neural mechanisms are important in the experience of beauty, are there any common characteristics in our experience of desire, love, beauty …? What can we learn about the brain by looking at beauty?Daniel Wolpert (Cambridge University) is researching how the brain controls movement; in fact, he sees the brain as essentially “about” movement. He talks rapidly of how control of motion is so hard to understand or reproduce, because of multiple interacting components, entirely non-linear processes are, long time delays (relatively) and noise, and so on. He then describes Bayes and his algorithms (which I remember from my Comp.Sci degree), and in particular Bayesian decision theory — how you deal with uncertainty. This enables a form of thinking about “probabilistic actions”, which perhaps underpin motion control. Wolpert then switches gear to the idea of “priors”, which may be genetically encoded. In other words, we may be hard coded for some priors — gravity, for example. He asks, intriguingly, whether it’s important to understand “urban priors” (what priors do you need to possess to instinctively understand Tokyo or Los Angeles, for example.) He suggests the richness in a statistical/probabilistic model of the world, noting you can’t control everything, and asks whether there are good generative probabilistic models of cities? (It’s been an emerging field for years, and still is). Is a Bayesian perspective used in urban planning? (With the Black Swan in mind, I’m wary but fascinated.)Dmitri Kullmann (UCL) is a neurologist, and so a little different. His expertise is in how nerve cells talk teach other. Like Zeki, he’s also interested in this idea of a unity of consciousness. He vividly describes how the actual connectivity and wiring in the brain is entirely relevant to this question, and the brain’s ability to “flip between streams”. We hear of the plasticity of synapses, and the plasticity of functional component. He talks about a theory that oscillations can control information flow, as there is evidence that different areas of the brain start oscillating together (this is called “coherence”), such that they’re able to exchange information if they oscillate together. But he notes that there is no good science on how this works. Can we use computational models to “spike a neuron”, he asks? (As with the way that the scientists interact, I hugely enjoy the language at play here, and am immediately intrigued.)Troy Margrie (National Institute for Medical Research/UCL) works in mitral cell diversity, exploring the intrinsic biophysical diversity, particularly in the olfactory bulb, which processes smell.) He shows us the immense diversity in the biophysical property of mitral cells, whereas the cells that wire together networks tend to look similar, in comparison (they are the same functional network.) They explore these things by creating transgenic mice, and looking at the homotypic property that reflects the processing of specific functions. (Margrie is another that lets slip some wonderful language, perhaps inadvertently eg. “The nice thing about working in mice is that we can make mice. So we made a mouse, a transgenic mouse.” And so on.)Steve Wilson (also UCL) is a developmental biologist rather than neuroscientist as such — though again, the genres are blurring heavily here, which is good. He studies how the brain gets to be in its mature form, or in particular, how the vertebrate forebrain develops. He notes it starts simply but over time forms an incomprehensible number of connections.) He’s looking in particular at asymmetry and lateralisation in developing brain (how does left side become different to right, for instance.) They work in little fish embryos, he says, looking directly into the brain when it’s at its most simple, and only consists of a a few neurons (tens, hundreds etc), as opposed to the complexity of humans. Why does the brain work in an asymmetric way; language processing is left side, right-handedness is behavioral motor asymettry; etc). The “little fish brain” (a couple of days old, but has vision, can respond to sensory information) already has left-right asymmetric epithalamic circuitry (this is the major output pathway of limbic system, one of the older systems in our brain) (see zebrafishbrain.org for more.)Last but not least, Geoffrey West (Santa Fe Institute). Many of you will know West through his research indicating a strong correlation between the “metabolic rate” of cities and certain indicators of urban performance. His background is actually physics (working on dark matter, string theory) and then as a physicist working in biology. He eventually ended up researching the idea of “a science of cities” — can there be probabilistic mechanistic science of cities? Are we just looking at biological metaphors, or is it serious science? (He describes the latter as quantified, mathematised and predictive, I seem to recall.) He starts with the now well-known ‘body mass against metabolic rate’ graph, and then extends this rapidly into the mathematics of networks. He says you can derive all scaling laws — in a “coarse-grained average sense” — across a wide range of indicators and essentially all the many cities he’s measured. You can read more about his work here.Basically, West sees cities as networks; the physical city is a representation of networks. (Picking up my cue, he also talks about culture as a form of network of networks.) Looking at these scaling laws, he can be given the size of a city and accurately predict the number of petrel stations, say, alongside pretty much all other physical infrastructure, socio-economic qualities like wages, crime levels, and so on. He can predict how fast the average person will walk. Based on this consistent systemic scaling behaviour, he suggests that doubling the size of the city systematically increases income, wealth, patents, number of colleges, number of creative people, AIDS cases etc etc. by 15%.We immediately get into a discussion here, as West is a good communicator, the data is compelling at face value, and the correlation seems almost magical. West notes that density is not particularly taken into account in their calculations, because there is so little good data about cities (a general problem, he adds.) He knows it’s not irrelevant (though personally I think it’s even more relevant than he suggests.) He is also pressed on why the scaling is .85 for infrastructure and 1.15 for patents, for example. I also find his data to be like much traditional analysis ie. It tells you the way something is, without necessarily uncovering “why” in a really useful way, or suggesting what to do — if you were a mayor, you wouldn’t double the size of your city, even if you could, to gain a ‘15% increase in patents generated’ if it also meant a 15% increase in crime and AIDS cases, right? To take a step forward you need synthesis, not analysis. Still, this is not necessarily the job of science, of course, and in conversation, I found Geoffrey to be more engaging than his data — and he is of course right on the necessity of having more thorough, more insightful data on urban performance. What to do with it is another matter.Chris Luebkeman (Arup) then gave his take on the subject, and the discussion so far. Chris is interested in “What is normal?”. He’s fascinated by the next 20 years and trying to understand how “normal itself” develops over that timeframe. Partly this is Arup projects often take that length of time, if not to develop, but to mature, and so, as he put it, “How do we be sure that life of a project is going to be a full one?” Recalling what he had just heard, Chris said he was fascinated by how the brain is able to adapt, adjust, be plastic. So this adaptation was key: how do we adjust to the new normal? How do environmental factors impact on adaptation? How will a resource-constrained world affect us? What’s driving change — how we understand change? And how do our brains change? How do we perceive that?Sarah Caddick, summing up, talked about her own interest in how the brain works in this context Ie. What can we learn, in terms of urban systems, by observing the brain’s ability to work as a finite entity that can bring resources online when it needs them, looking at the somewhat controversial “silent synapses” as potential points of connection coordinating this (I’m extrapolating a bit here, which is dangerous to say the least.) She ended on a poetic note, recalling how flying over Prague once, before coming into land at night, the city below looked like nothing more than a “basket cell” (a neuron) (See also the juxtaposed illustrations from Steven Johnson’s Emergence, reproduced above, with their allusion to the plan of Hamburg and an illustration of the brain.)My own presentation, in between Wolpert and Kullmann, focused only on a few things and avoided images and movies (unusually for me). I briefly mentioned our own smart services work on Low2No, but my core point was that we need to step back and think about the question we’re trying to ask here — why were we gathered here today? I suggested that ideas themselves are not particularly relevant; that the idea of optimising urban infrastructure as a ‘no-brainer’ (an odd phrase to use in this setting, I admitted.) (I also nodded to Zeki’s paper, which I’d learned a lot from.)But then I made the claim that the city is not psychological or biological, but cultural, and that if anything is holding us back from “better cities” (if that’s our goal), it’s not ideas or technology, but our cultures of decision-making (which is the focus of our work in the Strategic Design Unit at Sitra.)In this sense, I mentioned my boss Marco’s contention that we have 18th century institutions facing 21st century problems; that we need to preference synthesis over analysis (analysis tells us why things are, assuming it’s not too narrowly focused, which it often is; synthesis tells us how things could be); that people are more convinced by narrative than data (if we’re trying to convince people of something); that we need to address the “dark matter” of organisations, culture, policy, as well as physical matter, and connect the two together via prototypes and projects. So data is not enough to actually get things done. I tried to position this as no challenge to the collective knowledge capital in the room, but instead to open up an angle oriented towards “what to do”. So my closing questions were: how can brain science help us better understand the architecture of problems, and what might we learn about cultures of decision-making?;(Ed. I later expanded on these ideas in my book Dark Matter and Trojan Horses: A Strategic Design Vocabulary.)To the discussion, which was freewheeling, entertaining, and well-hosted by Sarah and Chris. Ultimately it was the kind of conversation that is difficult to sum up, so I’ll list some key points I remembered (this is, of course, my interpretation.):The emerging discussion I personally found most interesting — and tested on Geoffrey West and others, who were receptive — was this idea of how we make public decisions. Given our cultures of decision-making, from the individual to the institutional, were designed in another time, is it any wonder these systems are struggling to deliver the kind of complex, longer-term, interdependent decisions we need to make today? Equally, we now know rather more about the way the individual and society works, and so have some idea that fundamental systems within the brain, such as the limbic system, seem to prefer short-term decisions, for example, amongst a series of other unhelpful characteristics.So the thought occurred: how can we better design our approach to public decision-making, in such a way that the structures and cultures mitigates against our inherent “limitations”? (Please note the inverted commas there, indicating the obvious value judgement.)In no way would I want to suggest that we construct systems around what we understand about the brain, given that a) we clearly still understand so little, and b) designing systems based on biological and psychological structures seems inherently dangerous — see note on ‘ecosystem thinking’ below; or Will Davies on the folly of pursuing self-organising decision-making structures derived from ants — given that culture and politics are “higher-order functions”, if I can put it like that, which differentiates us from, well, ants. (With all due respect to ants, and admitting none of us have ever checked with an ant what they think about all this. And more seriously, that we may have little understanding of how sophisticated ants are.)For instance, emergence is a powerful form of organisation for certain contexts, and we might learn much from such processes, but I’m yet to be convinced that it should be primary driver of our public cultures of decision-making. That’s quite a leap: potentially pointless; potentially dangerous.However, perhaps it might be fruitful to use brain science as one core input into the redesign of our cultures of decision-making? (We are already looking into the contributing roles of space, experience, community, social interaction, and other facets.)How to mitigate against our short-termism? How to understand our intrinsic irrationality in decision-making — as Daniel Kahneman’s book does (and see this review) — and yet build systems that enable coherent, responsible, decisive and resilient decision-making nonetheless? How could we construct approaches that mitigate against the likelihood that humans tend to feel greater sympathy for those that resemble them (racially, for instance)? How to compensate for the “planning fallacy”, the demonstrated over-confidence of experts in their abilities, and numerous other cognitive biases that might shape public, representative decision-making? Given research indicates these characteristics, are we sure our current approaches might absorb and compensate for these instincts appropriately? How do we foreground conscious and rational decision-making when our subconscious and irrationality apparently shape our decision-making? What kind of structures and cultures might flex smartly in tension with these forces?A series of great conversations with Steven Johnson in Oslo last week reinforced my interest in these ideas, as did David Brooks’s recent book The Social Animal, which started to approach the idea of re-calibrating policy-making based on our advancing understanding of various aspects of brain science (though do read this excellent critical review in the New York Times, which also attempts to keep science in check, whilst learning from it; and I don’t buy Brooks’s perception of the limits of social policy, which seems very US-context-driven, put it that way.)It’d be interesting to take such science and not have it tend towards “self-help” psychology on personal decision-making, but something more nuanced, public and systemic, (or indeed get misinterpreted into “techniques” like brushing your teeth left-handed to promote mental flexibility, or debating with a full bladder, a bizarrely medievalist notion endorsed by the current Prime Minister of the United Kingdom, and with such winning results.)Note again the desire would be to re-engage with politics, policy, the state, and the richness of our various formal structures and informal cultures of decision-making, as a primary contribution of humanity, rather than deny it or side-step it as previous such approaches have. I see this as a design challenge, at least with a contemporary understanding of design which is not solely tied to the limitations of “problem-solving”: to design a series of prototypes that enable us to learn by doing, in properly blending the natural sciences with culture, social science and the reality of politics.Quite a few of the neuroscientists in the room seemed intrigued enough to pursue this ideas, and I certainly mean to. Any critiques, ideas and leads welcome.Finally, on the way out of the building, chatting with a couple of neuroscientists, I floated that loose critique of ecosystem thinking — as in, denying the idea that ‘nature’, however defined, is an intrinsically ‘better’ way of organising. I asked something along the lines of whether it is the case that the brain, and other natural systems, tend towards any kind of balanced equilibrium, or efficient use of resources, or useful steady state?They smiled, and said something along the lines of “Of course not”, that such systems are often very “wasteful” (even allowing for a construct of conscious thought to be applied to something that clearly isn’t.)Cities are also systems that thrive on instability and imbalance. They never, or rarely, tend towards any kind of equilibrium or steady-state — which challenges much of the philosophy (though that’s hardly the right word) which underpins many models of urban sustainability, including smart cities with its banal emphasis on efficient use of resources through feedback loops.(Do also watch Adam Curtis’ second episode in his BBC series “All Watched Over By Machines Of Loving Grace”, in which he carefully dissects — and then utterly trashes, with his inimitable VHS-rendered sturm und drang — the entire idea of “the ecosystem” as a useful metaphor, as well as most similar “systems thinking”. Curtis points out that, as opposed to efficient equilibrium, “the history of nature was full of radical dislocations and unpredictable change … a raw chaotic instability”. See also this piece in The Guardian.) It may be the brain is also this way — it would make sense if it was, after all — but rather than be disheartened by this, maybe that might be a fruitful avenue to take in terms of our understanding of equally unstable cities? To not search for harmony and equilibrium, but understand instead this raw chaotic instability, and find ways to work with that, within a resource-constrained, increasingly diverse and dynamic world with a greater need than ever to make intelligent long-term as well as short-term decisions.This doesn’t mean that there will not be fruitful approaches extracted from the mess of smart cities, of course, just as I gained immeasurably from this exploratory conversation with brain scientists. Yet the insights will surely not be obvious, will not be immediate, and will require a more concerted, deeper engagement.Ed. This was originally published at cityofsound.com on December 21, 2011, and has been tidied up a little since.",21/12/2011,0,27.0,68.0,700.0,308.0,1.0,1.0,0.0,47.0,en
4251,"Redes Neurais, Perceptron Multicamadas e o Algoritmo Backpropagation",Ensina.AI,Tiago M. Leite,113.0,9.0,1421.0,"Você já se perguntou como funcionam os sistemas de reconhecimento de imagem? Como um aplicativo do seu celular faz para detectar rostos, ou um teclado inteligente sugere a próxima palavra? As chamadas Redes Neurais tem sido amplamente usadas para tarefas como essas, mas mostraram-se úteis também em outras áreas, como aproximação de funções, previsão de séries temporais e processamento de linguagem natural.Neste artigo, explico como funciona um tipo básico de Rede Neural, o Perceptron Multicamadas, e um fascinante algoritmo responsável pelo aprendizado da rede, o backpropagation. Tal modelo de rede serviu de base para os modelos mais complexos hoje existentes, como as Redes Convolucionais, que são o estado da arte para classificação de imagens.Se você estudou Cálculo na universidade e ainda não se esqueceu, não terá dificuldade em entender a parte do texto que está escrita em Matematiquês…Caso contrário, não se sinta incomodado pelas fórmulas, apenas foque na ideia principal; algumas analogias ao mundo real que procurei estabelecer irão ajudá-lo a entender também.Inspirando-se no funcionamento dos neurônios biológicos do sistema nervoso dos animais, estabeleceu-se na área da Inteligência Artificial um modelo computacional de um neurônio conforme ilustrado a seguir:Os sinais da entrada no neurônio são representados pelo vetor x = [x1, x2, x3, …, xN], podendo corresponder aos pixels de uma imagem, por exemplo. Ao chegarem ao neurônio, são multiplicados pelos respectivos pesos sinápticos, que são os elementos do vetor w = [w1, w2, w3, …, wN], gerando o valor z, comumente denominado potencial de ativação, de acordo com a expressão:O termo adicional b provê um grau de liberdade a mais, que não é afetado pela entrada nessa expressão, correspondendo tipicamente ao “bias” (viés). O valor z passa então por uma função matemática de ativação σ, com a característica de ser não linear, responsável por limitar tal valor a um certo intervalo, produzindo o valor final de saída y do neurônio. Algumas funções de ativação usadas são a degrau, sigmoide, tangente hiperbólica, softmax e ReLU (Rectified Linear Unit).Com apenas um neurônio não se pode fazer muita coisa, mas podemos combiná-los em uma estrutura em camadas, cada uma com número diferente de neurônios, formando uma rede neural denominada Perceptron Multicamadas (“Multi Layer Perceptron — MLP”). O vetor de valores de entrada x passa pela camada inicial, cujos valores de saída são ligados às entradas da camada seguinte, e assim por diante, até a rede fornecer como resultado os valores de saída da última camada. Pode-se arranjar a rede em várias camadas, tornando-a profunda e capaz de aprender relações cada vez mais complexas.Para que uma rede dessas funcione, é preciso treiná-la. É como ensinar a uma criança o beabá. O treinamento de uma rede MLP insere-se no contexto de aprendizado de máquina supervisionado, em que cada amostra de dados utilizada apresenta um rótulo informando a que classificação ela se encaixa. Por exemplo, uma imagem de um cachorro contém um rótulo informando que aquilo é um cachorro. Assim, a ideia geral é fazer com que a rede aprenda os padrões referentes a cada tipo de coisa (cada classe), assim, quando uma amostra desconhecida for fornecida à rede, ela seja capaz de estabelecer a qual classe tal amostra pertence. Como isso pode ser feito?“Não é errando que se aprende?”A ideia do algoritmo backpropagation é, com base no cálculo do erro ocorrido na camada de saída da rede neural, recalcular o valor dos pesos do vetor w da camada última camada de neurônios e assim proceder para as camadas anteriores, de trás para a frente, ou seja, atualizar todos os pesos w das camadas a partir da última até atingir a camada de entrada da rede, para isso realizando a retropropagação o erro obtido pela rede. Em outras palavras, calcula-se o erro entre o que a rede achou que era e o que de fato era (era um gato e ela achou que era um cachorro — temos aí um erro!), então recalculamos o valor de todos os pesos, começando da última camada e indo até a primeira, sempre tendo em vista diminuir esse erro.Traduzindo essa ideia para o matematiquês, o backpropagation consiste nas seguintes etapas:Assim, a fórmula geral de atualização dos pesos na iteração fica:Ou seja, o valor do peso na iteração atual será o valor do peso na iteração anterior, corrigido de valor proporcional ao gradiente. O sinal negativo indica que estamos indo na direção contrária à do gradiente, conforme mencionado. O parâmetro η representa a taxa de aprendizado da rede neural, controlando a tamanho do passo que tomamos na correção do peso. Fazendo uma analogia, imagine que você está numa região montanhosa e escura, desejando descer o mais rápido possível à procura de um vale, na esperança de encontrar um lago de águas límpidas. É possível andar em várias direções, mas você tenta achar a melhor ao sentir o solo ao redor e tomar aquela com maior declive. Em termos matemáticos, você está indo na direção contrária à do vetor gradiente. Considere também que você pode controlar o tamanho do passo que pode dar, mas note que passos muito largos podem fazê-lo passar do vale, caindo na outra montanha mais adiante, e outro passo largo o trará de volta à primeira montanha, fazendo-o pular pra lá e pra cá sem nunca alcançar o vale; por outro lado, passos muito curtos o farão demorar muito a descer, podendo morrer de sede do meio do caminho.Voltando ao matematiquês, o conceito-chave da equação anterior é o cálculo da expressão ∂E /∂w, consistindo em computar as derivadas parciais da função de erro E em relação a cada peso do vetor w. Para nosso auxílio, vamos considerar a figura a seguir, que ilustra uma rede MLP com duas camadas e servirá de base para a explicação do backpropagation. Uma conexão entre um neurônio j e um neurônio i da camada seguinte possui peso w[j,i]. Repare que os números sobrescritos, entre parênteses, indicam o número da camada à qual a variável em questão pertence, podendo, neste exemplo, valer (1) ou (2).Sinta-se à vontade para voltar à figura anterior conforme for acompanhando os passos a seguir. Sendo y a saída esperada e ŷ a saída obtida pela rede, definimos a função de erro como sendo:Ou seja, aqui estamos simplesmente calculando a somatória dos quadrados das diferenças entre os elementos dos dois vetores. Agora, calculamos a derivada parcial do erro em relação à camada de saída, ŷ:Seguindo a estrutura da rede para a esquerda, prosseguimos no cálculo da derivada do erro em relação ao potencial de ativação z da camada (2), usando a Regra da Cadeia.Este valor é o gradiente local em relação ao i-ésimo neurônio da camada (2) e, para não tornar as fórmulas excessivamente extensas, será simplesmente indicado como δ:Finalmente, usando a Regra da Cadeia mais uma vez, computamos a derivada parcial do erro em função de um peso w[j,i] da camada (2):O cálculo em relação ao bias é análogo, resultando em:Com esses resultados em mãos, é possível aplicar a fórmula geral de atualização dos pesos dos neurônios da última camada:E a fórmula de atualização do bias:Pronto, já temos a fórmula mágica para atualizar os pesos dos neurônios da última camada. Agora temos que calcular esta expressão para os pesos dos neurônios da camada anterior; logo, precisamos calcular a derivada parcial em relação à saída y[i] da camada (1). A sacada aqui é perceber que tal valor y[i] interfere em todos os neurônios da camada seguinte, assim, temos que considerar a somatória dos erros que ele propaga:Mas:Resultando em:Prosseguindo, temos:Esse δ a que igualamos o resultado segue a mesma ideia anterior: é o gradiente local em relação ao i-ésimo neurônio da camada (1). Finalmente, vem:Substituindo na fórmula de atualização dos pesos:Novamente, as fórmulas para o bias são análogas e ficam como exercício.É isso! Caso existissem mais camadas na rede, o procedimento continuaria, seguindo sempre esse mesmo padrão de calcular as derivadas parciais, retropropagando os erros até a camada de entrada da rede.O backpropagation é um algoritmo elegante e engenhoso. Os atuais modelos deep learning como Redes Neurais Convolucionais, embora mais refinados que o MLP, têm se mostrado muito superiores em tarefas como classificação de imagens e também utilizam como método de aprendizado o backpropagation, assim como as chamadas Redes Neurais Recorrentes, em processamento de linguagem natural, também fazem uso desse algoritmo. O mais incrível é que tais modelos conseguem encontrar padrões inobserváveis e obscuros para nós, humanos, o que é fascinante e permite considerar que em breve estaremos contando com a ajuda do deep learning para resolver muitos dos principais problemas que afligem a humanidade, como a cura do câncer.",10/05/2018,0,39.0,52.0,583.0,200.0,20.0,1.0,0.0,1.0,pt
4252,Spam Classifier in Python from scratch,Towards Data Science,Tejan Karmali,105.0,6.0,901.0,"We all face the problem of spams in our inboxes. Let’s build a spam classifier program in python which can tell whether a given message is spam or not! We can do this by using a simple, yet powerful theorem from probability theory called Baye’s Theorem. It is mathematically expressed asWe have a message m = (w1, w2, . . . . , wn), where (w1, w2, . . . . , wn) is a set of unique words contained in the message. We need to findIf we assume that occurrence of a word are independent of all other words, we can simplify the above expression toIn order to classify we have to determine which is greaterWe are going to make use of NLTK for processing the messages, WordCloud and matplotlib for visualization and pandas for loading data, NumPy for generating random probabilities for train-test split.We do not require the columns ‘Unnamed: 2’, ‘Unnamed: 3’ and ‘Unnamed: 4’, so we remove them. We rename the column ‘v1’ as ‘label’ and ‘v2’ as ‘message’. ‘ham’ is replaced by 0 and ‘spam’ is replaced by 1 in the ‘label’ column. Finally we obtain the following dataframe.To test our model we should split the data into train dataset and test dataset. We shall use the train dataset t0 train the model and then it will be tested on the test dataset. We shall use 75% of the dataset as train dataset and the rest as test dataset. Selection of this 75% of the data is uniformly random.Let us see which are the most repeated words in the spam messages! We are going to use WordCloud library for this purpose.This results in the followingAs expected, these messages mostly contain the words like ‘FREE’, ‘call’, ‘text’, ‘ringtone’, ‘prize claim’ etc.Similarly the wordcloud of ham messages is as follows:We are going to implement two techniques: Bag of words and TF-IDF. I shall explain them one by one. Let us first start off with Bag of words.Preprocessing: Before starting with training we must preprocess the messages. First of all, we shall make all the character lowercase. This is because ‘free’ and ‘FREE’ mean the same and we do not want to treat them as two different words.Then we tokenize each message in the dataset. Tokenization is the task of splitting up a message into pieces and throwing away the punctuation characters. For eg.:The words like ‘go’, ‘goes’, ‘going’ indicate the same activity. We can replace all these words by a single word ‘go’. This is called stemming. We are going to use Porter Stemmer, which is a famous stemming algorithm.We then move on to remove the stop words. Stop words are those words which occur extremely frequently in any text. For example words like ‘the’, ‘a’, ‘an’, ‘is’, ‘to’ etc. These words do not give us any information about the content of the text. Thus it should not matter if we remove these words for the text.Optional: You can also use n-grams to improve the accuracy. As of now, we only dealt with 1 word. But when two words are together the meaning totally changes. For example, ‘good’ and ‘not good’ are opposite in meaning. Suppose a text contains ‘not good’, it is better to consider ‘not good’ as one token rather than ‘not’ and ‘good’. Therefore, sometimes accuracy is improved when we split the text into tokens of two (or more) words than only word.Bag of Words: In Bag of words model we find the ‘term frequency’, i.e. number of occurrences of each word in the dataset. Thus for word w,andTF-IDF: TF-IDF stands for Term Frequency-Inverse Document Frequency. In addition to Term Frequency we compute Inverse document frequency.For example, there are two messages in the dataset. ‘hello world’ and ‘hello foo bar’. TF(‘hello’) is 2. IDF(‘hello’) is log(2/2). If a word occurs a lot, it means that the word gives less information.In this model each word has a score, which is TF(w)*IDF(w). Probability of each word is counted as:Additive Smoothing:So what if we encounter a word in test dataset which is not part of train dataset? In that case P(w) will be 0, which will make the P(spam|w) undefined (since we would have to divide by P(w) which is 0. Remember the formula?). To tackle this issue we introduce additive smoothing. In additive smoothing we add a number alpha to the numerator and add alpha times number of classes over which the probability is found in the denominator.When using TF-IDFThis is done so that the least probability of any word now should be a finite number. Addition in the denominator is to make the resultant sum of all the probabilities of words in the spam emails as 1.When alpha = 1, it is called Laplace smoothing.For classifying a given message, first we preprocess it. For each word w in the processed messaged we find a product of P(w|spam). If w does not exist in the train dataset we take TF(w) as 0 and find P(w|spam) using above formula. We multiply this product with P(spam) The resultant product is the P(spam|message). Similarly, we find P(ham|message). Whichever probability among these two is greater, the corresponding tag (spam or ham) is assigned to the input message. Note than we are not dividing by P(w) as given in the formula. This is because both the numbers will be divided by that and it would not affect the comparison between the two.",02/08/2017,0,5.0,6.0,555.0,174.0,23.0,1.0,0.0,11.0,en
4253,Preparing for Insight,Insight,Insight,2200.0,5.0,1108.0,"John Joo is an Insight alumnus from the August 2013 session with a PhD in applied physics from Harvard. He recently joined Insight as a Program Director in January, leading the most recent cohort of Fellows in their transition from academia to industry.When I was first considering making the transition from applied physics to data science, I had a lot of questions. What skills did I need to develop to get started in data science? What courses should I take? Did I need to know how to program and code? What languages? How much statistics did I need to know? The list goes on. Now that I’ve spent a few months as a Program Director here at Insight, I think it’s time I shared with you the tools and tips that got me, and nearly 100 other Insight Fellows, started on our transition to data science.Data Science Resources from Insight Data SciencePythonPython has become the standard programming language for data science at almost all of Insight’s mentor companies, so the more you can do to practice coding in Python, the better. As a data scientist, you’ll use Python to write scripts that take your data set (in whatever format it happens to be in) and get it into a format that you can work with. Once you have your data cleaned, you’ll store it in a SQL database.Action Item: Complete Google’s Python Class. Insight Fellows from previous sessions have raved about this course and it should only take a few days to complete.MySQLScientists working in academia often just store data in text files, but in industry a database is almost always used to store data because of the increased performance, amongst other reasons. MySQL is one of the most often used databases in industry. Although you will most likely be using Hive or Pig to access “big data,” we have found that these languages are so close to SQL that Fellows have no trouble crossing the bridge to Hive and Pig after mastering SQL.Action Item: Work through Tutorials 1–6 on the SQLzoo website. Feel free to explore the website to be able to complete questions in the tutorials. As a bonus, if you feel comfortable with the SQL on the SQLzoo website, try to work through the SQL homework questions on the Databases course on Coursera. If you want to start a data project using MySQL and Python, follow Zetcode’s tutorial to learn to interface with MySQL through Python.Data Analysis using PythonNow that you have gotten your data into MySQL using your Python coding skills, you can start doing data analysis to find meaningful insights in the data or to make predictions. Up to a few years ago the R statistical programming language was the best way to do this (and continues to be one of the best, if you’re already proficient in R). However, in the past couple of years, Python, with it’s various libraries like SciPy, Numpy, Pandas, iPython and matplotlib, have made it the tool of choice for data scientists.Action Item: Watch Wes Mckinney’s video of how to use pandas in Python for data analysis. Follow along with his examples using the data and iPython notebooks found here. For a quick introduction to pandas, check the 10-minutes to pandas tutorial. Wes McKinney wrote the book Python for Data Analysis, and you may want to order it to use as a reference book.Machine LearningHaving an understanding of the most common machine learning techniques and experience using them is becoming a must-have for many data science teams. You could spend years studying the entire field, of course, but even having basic exposure at the level of an introductory class will get you most of the way toward being able to speak intelligently about the topic, knowing when to use a specific algorithm for a given problem and being able to start working with a given machine learning technique to build your project.Action Item: Take Andrew Ng’s machine learning course on Coursera. This is widely viewed in industry and by Insight Fellows as the best introduction and overview to machine learning that exists today. Insight Fellows from previous sessions found it very helpful to take summary notes while going through the course and attempting the exercises in Python.Computer Science FundamentalsData scientists don’t deal much with the fundamentals of computer science (CS) and most of your interviews to be a data scientist will focus on topics that correspond to doing data science (ie: the topics above). However, for better or worse, a part of the interview process at each company will include coding interviews. At Insight, Fellows spend a decent amount of time doing CS coding exercises in preparation for these types of interviews. In particular, the two (and pretty much the only two) topics you’ll need to understand (at a CS 101 level) are algorithms and data structures.Action Item: Code the examples in Problem Solving with Algorithms and Data Structures in Python. In particular, become familiar with stacks, queues, linked lists, merge sort, quick sort, and searching and hashing. If you prefer to learn by watching lectures, check out the MIT Introduction to Algorithms course. Bonus: For each algorithm or data structure you learn about, try to program it from scratch in Python, from memory. Many Fellows have also found Leetcode to also be useful in the interview prep for their CS section.Daily ReadingHere are some of the primary news sources read by people in tech. We highly recommend starting to skim some of these news sources on a daily basis:Concluding thoughtsWorking through the textbooks and tutorials above is great, but the best way to learn data science is by doing data science. The next step is to actually use these tools and techniques to start a data-related side project.This list was started in the first sessions of Insight as a way to make sure that our Fellows were prepared for Day 1 of the program — we called it “Preparing for Insight.” Each session since, we have worked with the current Fellows and mentors to iterate and improve this list to be one of the most effective “getting started” guides out there. With nearly 100 Insight alumni now working as data scientists, we feel confident that this list will prepare you to begin your own transition.Interested in transitioning to career in data science? Find out more about the Insight Data Science Fellows Program in New York and Silicon Valley, apply today, or sign up for program updates.Already a data scientist or engineer?Find out more about our Advanced Workshops for Data Professionals. Register for two-day workshops in Apache Spark and Data Visualization, or sign up for workshop updates.",16/04/2014,0,11.0,26.0,1400.0,933.0,1.0,1.0,0.0,31.0,en
4254,GOOGLE AI CONTEST,Altsoph’s blog,Aleksey Tikhonov,275.0,1.0,88.0,"Вчера узнал, что сейчас идет Google AI Contest.В двух словах, задача состоит в написании логики бота, играющего в некоторый аналог игры Galcon — космической стратегии, основанной на разделении ресурсов. Прием ботов на конкурс идет до 27 ноября, а потом их будут стравливать и выявлять победителя. Языки доступны из списка C++, C#, Java, Python.Было бы времени побольше, я бы, наверное, поучавствовал.Вспоминаются стародавние времена, когда мы еще в школе рубились в RobotBattle, а потом на первом курсе с группой сотоварищей писали интерпретатор RedCode на придуманной нами модели тороидальной памяти (ToroWars).",16/09/2010,0,0.0,0.0,475.0,357.0,1.0,0.0,0.0,4.0,bg
4255,"🏎 Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT",HuggingFace,Victor Sanh,1000.0,10.0,2113.0,"2019, October 3rd — Update: We are releasing our NeurIPS 2019 workshop paper describing our approach on DistilBERT with improved results: 97% of BERT’s performance on GLUE (the results in the paper superseed the results presented here). The approach is slightly different from the one explained in this present blog post so this blog post should be a good entry point to the paper! We applied the same method to GPT2 and are releasing DistilGPT2! Training code and pre-trained weights for DistilBERT and DistilGPT2 are available here. 🤗In the last 18 months, transfer learning from large-scale language models has significantly improved upon the state-of-the-art on pretty much every Natural Language Processing task.Usually based on the Transformer architecture of Vaswani et al., these pre-trained language models keep getting larger and larger and being trained on bigger datasets. The latest model from Nvidia has 8.3 billion parameters: 24 times larger than BERT-large, 5 times larger than GPT-2, while RoBERTa, the latest work from Facebook AI, was trained on 160GB of text 😵At Hugging Face, we experienced first-hand the growing popularity of these models as our NLP library — which encapsulates most of them — got installed more than 400,000 times in just a few months.However, as these models were reaching a larger NLP community, an important and challenging question started to emerge. How should we put these monsters in production? How can we use such large models under low latency constraints? Do we need (costly) GPU servers to serve at scale?For many researchers and developers, these can be deal-breaking issues 💸To build more privacy-respecting systems, we noticed an increasing need to have machine learning systems operate on the edge rather than calling a cloud API and sending possibly private data to servers. Running models on devices like your smartphone 📲 also requires light-weight, responsive and energy-efficient models!Last but not least, we are more and more concerned about the environmental cost of scaling exponentially computing requirements of these models.So, how can we reduce the size of these monster models⁉️There are many techniques available to tackle the previous questions. The most common tools include quantization (approximating the weights of a network with a smaller precision) and weights pruning (removing some connections in the network). For these technics, you can have a look at the excellent blog post of Rasa on quantizing BERT.We decided to focus on distillation: a technique you can use to compress a large model, called the teacher, into a smaller model, called the student.Knowledge distillation (sometimes also referred to as teacher-student learning) is a compression technique in which a small model is trained to reproduce the behavior of a larger model (or an ensemble of models). It was introduced by Bucila et al. and generalized by Hinton et al. a few years later. We will follow the latter method.In supervised learning, a classification model is generally trained to predict a gold class by maximizing its probability (softmax of logits) using the log-likelihood signal. In many cases, a good performance model will predict an output distribution with the correct class having a high probability, leaving other classes with probabilities near zero.But, some of these “almost-zero” probabilities are larger than the others, and this reflects, in part, the generalization capabilities of the model.For instance, a desk chair might be mistaken with an armchair but should usually not be mistaken with a mushroom. This uncertainty is sometimes referred to as the “dark knowledge” 🌚Another way to understand distillation is that it prevents the model to be too sure about its prediction (similarly to label smoothing).Here is an example to see this idea in practice. In language modeling, we can easily observe this uncertainty by looking at the distribution over the vocabulary. Here are the top 20 guesses by BERT for completing this famous quote from the Casablanca movie:In the teacher-student training, we train a student network to mimic the full output distribution of the teacher network (its knowledge).We are training the student to generalize the same way as the teacher by matching the output distribution.Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:This loss is a richer training signal since a single example enforces much more constraint than a single hard target.To further expose the mass of the distribution over the classes, Hinton et al. introduce a softmax-temperature:When T → 0, the distribution becomes a Kronecker (and is equivalent to the one-hot target vector), when T →+∞, it becomes a uniform distribution. The same temperature parameter is applied both to the student and the teacher at training time, further revealing more signals for each training example. At inference, T is set to 1 and recover the standard Softmax.We want to compress a large language model using distilling. For distilling, we’ll use the Kullback-Leibler loss since the optimizations are equivalent:When computing the gradients with respect to q (the student distribution) we obtain the same gradients. It allows us to leverage PyTorch implementation for faster computation:Using the teacher signal, we are able to train a smaller language model, we call DistilBERT, from the supervision of BERT 👨‍👦 (we used the English bert-base-uncased version of BERT).Following Hinton et al., the training loss is a linear combination of the distillation loss and the masked language modeling loss. Our student is a small version of BERT in which we removed the token-type embeddings and the pooler (used for the next sentence classification task) and kept the rest of the architecture identical while reducing the numbers of layers by a factor of two.Overall, our distilled model, DistilBERT, has about half the total number of parameters of BERT base and retains 95% of BERT’s performances on the language understanding benchmark GLUE.❓Note 1 — Why not reducing the hidden size as well?Reducing it from 768 to 512 would reduce the total number of parameters by ~2. However, in modern frameworks, most of the operations are highly optimized and variations on the last dimension of the tensor (hidden dimension) have a small impact on most of the operations used in the Transformer architecture (linear layers and layer normalisation). In our experiments, the number of layers was the determining factor for the inference time, more than the hidden size.Smaller does not necessarily imply faster…❓Note 2 — Some works on distillation like Tang et al. use the L2 distance as a distillation loss directly on downstream tasks.Our early experiments suggested that the cross-entropy loss leads to significantly better performance in our case. We hypothesis that in a language modeling setup, the output space (vocabulary) is significantly larger than the dimension of the downstream task output space. The logits may thus compensate for each other in the L2 loss.Training a sub-network is not only about the architecture. It is also about finding the right initialization for the sub-network to converge (see The Lottery Ticket Hypothesis for instance). We thus initialize our student, DistilBERT, from its teacher, BERT, by taking one layer out of two, leveraging the common hidden size between student and teacher.We also used a few training tricks from the recent RoBERTa paper which showed that the way BERT is trained is crucial for its final performance. Following RoBERTa, we trained DistilBERT on very large batches leveraging gradient accumulation (up to 4000 examples per batch), with dynamic masking and removed the next sentence prediction objective.Our training setup is voluntarily limited in terms of resources. We train DistilBERT on eight 16GB V100 GPUs for approximately three and a half days using the concatenation of Toronto Book Corpus and English Wikipedia (same data as original BERT).The code for DistilBERT is adapted in part from Facebook XLM’s code and in part from our PyTorch version of Google AI Bert and is available in our pytorch-transformers library 👾 along with several trained and fine-tuned versions of DistilBert and the code to reproduce the training and fine-tuning.We compare the performance of DistilBERT on the development sets of the GLUE benchmark against two baselines: BERT base (DistilBERT’s teacher) and a strong non-transformer baseline from NYU: two BiLSTMs on top of ELMo. We use the jiant library from NYU for ELMo baselines and pytorch-transformers for the BERT baseline.As shown in the following table, DistilBERT’s performances compare favorably with the baselines while having respectively about half and one third the number of parameters (more on this below). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 14 points of accuracy on QNLI). DistilBERT also compares surprisingly well to BERT: we are able to retain more than 95% of the performance while having 40% fewer parameters.In terms of inference time, DistilBERT is more than 60% faster and smaller than BERT and 120% faster and smaller than ELMo+BiLSTM 🐎To further investigate the speed-up/size trade-off of DistilBERT, we compare, in the left table, the number of parameters of each model along with the inference time needed to do a full pass on the STS-B dev set on CPU (using a batch size of 1).We further study the use of DistilBERT on downstream tasks under efficient inference constraints. We use our compact pre-trained language model by fine-tuning it a classification task. A nice way to actually mix distillation pre-training and transfer-learning!We selected the IMDB Review Sentiment Classification which is composed of 50'000 reviews in English labeled as positive or negative: 25'000 for training and 25'000 for test (and with balanced classes). We trained on a single 12GB K80.First, we train bert-base-uncased on our dataset. Our dear BERT 💋 reaches an accuracy of 93.46% (average of 6 runs) without any hyper-parameters search.We then train DistilBERT, using the same hyper-parameters. The compressed model reaches an accuracy of 93.07% (average of 6runs). An absolute difference of 0.4% in performances for a 60% reduction in latency and 40% in size 🏎!❓Note 3 — As noted by the community, you can reach comparable or better score on the IMDB benchmark with lighter methods (size-wise and inference-wise) like ULMFiT. We encourage you to compare on your own use-case! In particular, DistilBERT can give a sensible lower-bound on Bert’s performances with the advantage of faster training.Another common application of NLP is Question Answering. We compared the results of the bert-base-uncased version of BERT with DistilBERT on the SQuAD 1.1 dataset. On the development set, BERT reaches an F1 score of 88.5 and an EM (Exact-match) score of 81.2. We train DistilBERT on the same set of hyper-parameters and reach scores of 85.1 F1 and 76.5 EM, within 3 to 5 points of the full BERT.We also studied whether we could add another step of distillation during the adaptation phase by finetuning DistilBERT on SQuAD using the finetuned BERT model as a teacher with a knowledge distillation loss.Here we are finetuning by distilling a question answering model into a language model previously pre-trained with knowledge distillation! That a lot of teachers and students🎓In this case, we were able to reach interesting performances given the size of the network: 86.2 F1 and 78.1 EM, ie. within 3 points of the full model!Other works have also attempted to accelerate question answering models. Notably, Debajyoti Chatterjee, uploaded an interesting work on arXiv which follows a similar method for the adaptation phase on SQuAD (initializing a student from its teacher, and training a question-answering model via distillation). His experiments present similar relative performances with regards to BERT (base uncased). The main difference with our present work is that we pre-train DistilBERT with a general objective (Masked Language Modeling) in order to obtain a model that can be used for transfer-learning on a large range of tasks via finetuning (GLUE, SQuAD, classification…).We are very excited about DistilBERT’s potential. The work we’ve presented is just the beginning of what can be done and raises many questions: How far can we compress these models with knowledge distillation? Can these technics be used to get further insights into the knowledge stored in the large version? What aspects of linguistic/semantics do we lose in this type of compression?…One essential aspect of our work at HuggingFace is open-source and knowledge sharing as you can see from our GitHub and medium pages. We think it is both the easiest and fairest way for everyone to participate and reap the fruits of the remarkable progress of deep learning for NLP.Thus, together with this blog post, we release the code of our experiments 🎮 (in particular the code to reproduce the training and fine-tuning of DistilBERT) along with a trained version of DistilBERT in our pytorch-transformers library🔥.Many thanks to Sam Bowman, Alex Wang and Thibault Févry for feedback and discussions!",28/08/2019,0,64.0,25.0,977.0,422.0,10.0,0.0,0.0,30.0,en
