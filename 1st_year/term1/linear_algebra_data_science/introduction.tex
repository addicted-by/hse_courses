\chapter{Linear Algebra for Data Science}
\thispagestyle{empty}
\annotation{In the lecture course, we consider some topics of linear algebra beyond the standard first-year course which are extremely important for applications. Mostly, these are applications to data analysis and machine learning, as well as to economics and statistics. We begin with the inversion of rectangle matrices, that is, we discuss pseudo-inverse matrices (and their connections to the linear regression model). Among others, we discuss iterative methods (and their use in models of random walk on a graph applied to Internet search such as PageRank algorithm), matrix decompositions (such as SVD) and methods of dimension reduction (with their connections to some image compression algorithms), and the theory of matrix norms and perturbation theory (for error estimates in matrix computations). The course includes also symbolic methods in systems of algebraic equations, approximation problems, Chebyshev polynomials, functions with matrices such as exponents, etc. We plan to invite some external lecturers who successfully apply linear algebra in their work. The students are also invited to give their own talks on additional topics of applied or theoretical linear algebra.}
\finalformula
\[\text{GRADE} = min(\dfrac{\text{test1} + \text{test2}}{2} + \underbrace{\begin{array}{c} \text{Bonus} \\[-0.1cm] \text{for a talk}\end{array}}_{\leq 5} + \underbrace{\begin{array}{c} \text{Bonus} \\[-0.1cm] \text{for classes}\end{array}}_{\leq 1\ldots 2},10).\]
\newpage