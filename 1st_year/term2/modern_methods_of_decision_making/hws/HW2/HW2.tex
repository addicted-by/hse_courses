\documentclass[12pt]{report}
\usepackage{../mystyle}
\algnewcommand{\Initialize}[1]{%
  \State \textbf{Initialize:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\usepackage{float}
\usepackage{placeins}
\begin{document}
\boldmath
\setcounter{chapter}{1}
\chapter{Modern Method of Decision Making\\[-2.5ex] \hspace*{3.35cm} \large Work has been done by: Ryabykin Aleksey, Osokin Mikhail\vskip1.5ex}
\fancyhead[L]{Homework 2.}
\fancyhead[C]{Modern Methods of Decision Making}
\fancyhead[R]{Ryabykin A., Osokin M.}
\vspace*{-1cm}
\begin{center}
    \Large Adaptive Learning Rates for Online Gradient Descent
\end{center}
\subsection*{Task Description}
Consider:
\begin{itemize}
    \item a closed and convex decision space $K$ with diameter:
    \[
        \operatorname{Diam}(K) = \max\limits_{x, y \in K} ||x - y||_2 \leq D.
    \]
    \item a loss class $\mathcal{F}$ of convex and differentiable $f: K \to \R$
\end{itemize}
The goal of this home assignment is to study, in the context of the Online Convex Optimization problem, the perfomance of the Adaptive Online Gradient Descent algorithm defined as follows:
\begin{algorithm*}
    \begin{algorithmic}
        \Initialize{$x_1 \in K$}
        \For{$t \geq 1$}
        \begin{itemize}
            \item[$\rightarrow$] Play $x_t$;
            \item[$\rightarrow$] Receive loss $f_t$;
            \item[$\rightarrow$] Incur loss $f_t(x_t)$;
            \item[$\rightarrow$] Update:
            \[
                x_{t+1} = \pi_K(x_t - \gamma_t \nabla_t), \hspace*{0.5cm} \nabla_t = \nabla f_t(x_t)  
            \]
            where:
            \[
                \gamma_t = \dfrac{D}{\sqrt{\displaystyle \sum\limits_{s=1}^t || \nabla_s||_2^2}}  
            \]
        \end{itemize}
        \EndFor
    \end{algorithmic}
    \end{algorithm*}

    \begin{note}{}{}
    In the sequel, we denote:
    \[
        R_T = \sum\limits_{t=1}^T f_t(x_t) - \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)  
    \]
    \end{note}
    \subsection*{Solution}

            \begin{enumerate}
                \item Show that, for any value of the learning rates $\gamma_t$, we have:
                \[
                    R_T \leq \dfrac{D^2}{2\gamma_T} + \dfrac{1}{2} \sum\limits_{t=1}^T \gamma_t ||\nabla_t||_2^2.  
                \]
                \begin{proof}
                    \begin{lemma}{}{}
                        Fix $x_1 \in K$ as well as positive numbers $(\gamma_t)_{t\geq 1}$. For any sequence $(g_t)_{t\geq 1}$ of vectors in $\R^d$, define 
                        \[
                            x_{t+1} = \pi_k (x_t - \gamma_t g_t), \hspace*{0.5cm} t \geq 1.  
                        \]
                        Then, $\forall T \geq 1, \ \forall x \in K$:
                        \[
                            \sum\limits_{t=1}^T \langle g_t, x_t - x \rangle \leq \dfrac{1}{2} \sum\limits_{t=1}^T \left\{\left(\dfrac{1}{\gamma_t} - \dfrac{1}{\gamma_{t-1}}\right) ||x_t - x||_2^2 + \gamma_t||g_t||_2^2\right\}
                        \]
                    \end{lemma}
                    $\forall x \in K, \ \forall t \geq 1$, we deduce by convexity of $f_t$ and definition of $\nabla_t$, that:
                    \[
                        f_t(x_t) - f_t(x) \leq \langle \nabla_t, x_t - x \rangle.  
                    \]
                    As a result, 
                    \[
                        \sum\limits_{t=1}^T f_t(x_t) - \inf\limits_{x \in K} \sum\limits_{t=1}^T f_t(x) = \sup\limits_{x\in K} \sum\limits_{t=1}^T \left(f_t(x_t) - f_t(x)\right) \leq \sup\limits_{x\in K}\sum\limits_{t=1}^T \langle \nabla_t, x_t - x \rangle.
                    \]
                    According to the lemma above, and the definition of $D$, we deduce that:
                    \[
                        \begin{array}{c}
                            \displaystyle \sum\limits_{t=1}^T f_t(x_t) - \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x) \leq \sup\limits_{x \in K} \dfrac{1}{2} \sum\limits_{t=1}^T \left\{\left(\dfrac{1}{\gamma_t} - \dfrac{1}{\gamma_{t-1}}\right) \underbrace{||x_t - x||_2^2}_{\leq D^2} + \gamma_t ||\nabla_t||_2^2 \right\} \leq \\ 
                            \displaystyle \leq \dfrac{D^2}{2}\sum\limits_{t=1}^T \left(\dfrac{1}{\gamma_t} - \dfrac{1}{\gamma_{t - 1}}\right) + \dfrac{1}{2}\sum\limits_{t=1}^T \gamma_t||\nabla_t||_2^2 \\
                            \displaystyle = \dfrac{D^2}{2\gamma_T} + \dfrac{1}{2}\sum\limits_{t=1}^T \gamma_t||\nabla_t||_2^2
                        \end{array}
                    \]
                \end{proof}
                
                \item Show that if $\phi: (0, +\infty) \to \R$ is a non-increasing function and $(u_t)_{t\geq 1}$ are positive numbers, then $\forall T \geq 1$:
                \[
                    \sum\limits_{t=1}^T u_t \phi\left(\sum\limits_{s=1}^T u_s\right) \leq \bigintsss\limits_{0}^{\displaystyle \sum\limits_{s=1}^T u_s} \phi (\omega) d\omega 
                \]
                \begin{proof}
                    
                    Let $a,b \in \R^+$. Let us prove that $b \cdot \phi(a+b) \leq \int\limits_{a}^{a+b} \phi (\omega)d\omega$. The integral on the right side is equal to the area under $\phi$ on the segment $[a, a+b]$. $\phi$ is a non-increasing function, which means that $\phi(a+b)$ is the minimal value of $\phi$ on the entire segment. If $\phi$ was equal to $\phi(a+b)$ on the whole segment, then the area would be $b\cdot \phi(a+b)$. Since the value of $\phi$ can only go higher from $\phi(a+b)$ on the segment, the area under $\phi$ there is bounded from below by $b\cdot \phi(a+b)$, which concludes the proof. Now, if we set $a$ to $0$ and $b$ to $u_1$, we get $u_1 \phi(u_1) \leq \int\limits_{0}^{u_1} \phi(\omega) d\omega$, which is the induction base. Also, if $\sum\limits_{t=1}^{T-1} u_t\phi(\sum\limits_{s=1}^t u_s) \leq \int\limits_{0}^{\sum\limits_{s=1}^{t-1}u_s} \phi(\omega) d\omega$, then
                    \[
                        \begin{array}{c}
                            \displaystyle \sum\limits_{t=1}^{T-1} u_t \phi\left(\sum\limits_{s=1}^t u_s\right) + u_T \cdot \phi\left(\sum\limits_{s=1}^T u_s\right) \leq \int\limits_{0}^{\sum\limits_{s=1}^{T-1} u_s} \phi(\omega)d\omega + \int\limits_{\sum\limits_{s=1}^{T-1} u_s}^{\sum\limits_{s=1}^{T} u_s} \phi(\omega)d\omega                    
                        \end{array}  
                    \]
                    and 
                    \[
                        \sum\limits_{s=1}^{T} u_t \phi\left(\sum\limits_{s=1}^{t} u_s\right) \leq \int\limits_{0}^{\sum\limits_{s=1}^{T} u_s}\phi(\omega)d\omega, \hspace*{0.5cm} \text{since } u_1 \cdot \phi\left(\sum\limits_{s=1}^{t} u_s\right) \leq \int\limits_{\sum\limits_{s=1}^{T-1} u_s}^{\sum\limits_{s=1}^{T} u_s} \phi(\omega)d\omega
                    \]
                    which is the induction step.
                \end{proof}
        
                \item Suppose $\gamma_t = \dfrac{D}{\sqrt{\displaystyle \sum\limits_{s=1}^t ||\nabla_s||_2^2}}$. Combining 1. and 2. (for a well chosen value of $u_t$ and $\phi$) show that:
                \[
                    R_T \leq \dfrac{3D}{2} \sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2}  
                \]
                \begin{proof}
                    Let's substitute $\gamma_t$ to the result of first step. 
                    \[
                        R_T \leq \dfrac{D}{2}\sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2} + \dfrac{D}{2}\sum\limits_{t=1}^{T} \dfrac{||\nabla_t||_2^2}{\sqrt{\sum\limits_{s=1}^t ||\nabla_s||^2_2}}.  
                    \]
                    Let $\phi(n) = \dfrac{1}{\sqrt{n}}, \ u_t = ||\nabla_t||_2^2, $ then $\displaystyle \sum\limits_{t=1}^T \dfrac{u_t}{\sqrt{\sum\limits_{s=1}^T u_s}} \leq \bigintsss\limits_{0}^{\sum\limits_{s=1}^T u_s} \dfrac{1}{\sqrt{n}} dn = 2\sqrt{\displaystyle \sum\limits_{s=1}^T u_s} = 2 \sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2}$. This means that:
                    \[
                        \begin{array}{c}                    
                            \displaystyle R_T \leq \dfrac{D}{2}\sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2} + \dfrac{D}{2}\sum\limits_{t=1}^{T} \dfrac{||\nabla_t||_2^2}{\sqrt{\displaystyle \sum\limits_{s=1}^t ||\nabla_s||^2_2}} \leq \dfrac{D}{2}\sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2} + \dfrac{D}{2}\cdot 2 \sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2} = \\
                            = \dfrac{3D}{2}\sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2}.
                        \end{array}
                    \]
                \end{proof}
                \item Explain why this is always a better performance guarantee than the one we provided for the OGD algorithm studied in Lecture 4:
                
                \begin{theorema}{}{}
                    Suppose that 
                    \begin{itemize}
                        \item (Bounded diameter): $\operatorname{Diam} (K) \leq D < +\infty$;
                        \item (Bounded subgradients):
                        \[
                            \forall x \in K, \ \forall f \in \mathcal{F}, \ \forall \nabla \in \partial f(x):\ ||\nabla||_2 \leq G < +\infty.  
                        \]
                        Then $\forall x_1 \in K$, the OGD algorithm with step size $\gamma_t = \dfrac{D}{G\sqrt{t}}, \ \forall t \geq 1$, satisfies:
                        \[
                            R_T \leq \dfrac{3}{2}GD \sqrt{T}.  
                        \]
                    \end{itemize}
                \end{theorema}
                The inequality above can be written in the following way:
                \[
                    R_{T_{Lec 4}} \leq \dfrac{3}{2}GD \sqrt{T} = \dfrac{3D}{2} \sqrt{TG^2} = \dfrac{3D}{2} \sqrt{\displaystyle \sum\limits_{t=1}^T G^2}  
                \] 
                Since $\forall \nabla \in \partial f(x) \ ||\nabla||_2 \leq G$ we can obtain, that:
                \[
                    R_T \leq \dfrac{3D}{2} \sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2} \leq \dfrac{3D}{2} \sqrt{\displaystyle \sum\limits_{t=1}^T G^2} \leq R_{T_{Lec 4}}.
                \]
        
                \item Recall that a function $f: K \to \R$ is called $\beta$-smooth if it is differentiable and such that $||\nabla f(x) - \nabla f(y)||_2 \leq \beta ||x - y||_2, \ \forall x, y \in K.$.
                
                Show that if $f: K\to \R$ is $\beta$-smooth and achieves a minimum at $x^* \in K$, then $\forall x \in K$:
                \[
                    ||\nabla f(x)||_2^2 \leq 2\beta (f(x) - f(x^*)).  
                \]
                \begin{proof}
                    Since smoothness and the optimality of $x^*$ ($f:\ K\to \R$ is $\beta$-smooth and achieves minimum at $x^*\in K$), we have:
                    \[
                        \begin{array}{c}
                            \displaystyle f(x^*) \leq f\left(x - \dfrac{1}{\beta}\nabla f(x)\right) \leq f(x) - \dfrac{1}{\beta} ||\nabla f(x)||_2^2 + \dfrac{1}{2\beta} ||\nabla f(x)||_2^2 \leq \\[1cm]           
                            \displaystyle f(x) - \dfrac{1}{2\beta} ||\nabla f(x)||_2^2 \Rightarrow f(x^*) \leq f(x) - \dfrac{1}{2\beta} ||\nabla f(x)||_2^2                
                        \end{array}
                    \]
                    Multiplying both sides by $2\beta$ we obtain:
                    \[
                        ||\nabla f(x)||_2^2 \leq 2\beta \cdot \left(f(x) - f(x^*)\right).  
                    \]
                \end{proof}
                \item Suppose now that the losses that the losses $f\in \mathcal{F}$ are also $\beta$-smooth and positive. Combine 3. and 5. to show that:
                \[
                    R_T \leq \sqrt{\displaystyle \dfrac{9D^2\beta}{2} \sum\limits_{t=1}^T f_t(x_t)}  
                \]
                \begin{proof}
                    \[
                        \begin{array}{c}
                            \displaystyle \dfrac{3D}{2}\sqrt{\displaystyle \sum\limits_{t=1}^T ||\nabla_t||_2^2} \leq \dfrac{3D}{2}\sqrt{\displaystyle 2\beta \sum\limits_{t=1}^T \left(f_t(x_t) - f_t(x^*)\right)} \leq \\[1cm]
                            \text{Since the loss is positive: } \\[0.5cm]
                            \leq \dfrac{3D}{2}\sqrt{\displaystyle 2\beta \sum\limits_{t=1}^T f_t(x_t)} = \sqrt{\displaystyle \dfrac{9D^2\beta}{2}\sum\limits_{t=1}^T f_t(x_t)}.                    
                        \end{array}
                    \]
                \end{proof}
                \item Conclude from 6. that if the losses are $\beta$-smooth and positive, then:
                \[
                    R_T \leq \dfrac{9D^2\beta}{2} + 2\sqrt{\displaystyle \dfrac{9D^2\beta}{2} \inf\limits_{x \in K}\sum\limits_{t=1}^T f_t(x_t)}  
                \]
                \begin{proof}
                    As we denote:
                    \[
                        R_T = \sum\limits_{t=1}^T f_t(x_t) - \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)  
                    \]
                    We can substitute it in the result of 6.:
                    \[
                        \begin{array}{c}
                            \displaystyle R_T \leq \sqrt{\displaystyle \dfrac{9D^2\beta}{2} \sum\limits_{t=1}^T f_t(x_t)}  = \\[1cm]
                            \displaystyle = \sqrt{\dfrac{9D^2\beta}{2} \left(R_T + \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)\right)} \Longrightarrow \\[1cm]
                            \displaystyle \Rightarrow R^2_T - \dfrac{9D^2\beta}{2}\cdot R_T - \dfrac{9D^2\beta}{2} \cdot \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x) \leq 0
                        \end{array}
                    \]
                    \[
                        \mathcal{D} = \left(\dfrac{81D^4\beta^2}{4}\right) + 4 \dfrac{9D^2\beta}{2} \cdot \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)  
                    \]
                    Hence 
                    \[
                        R_T^+ = \dfrac{\dfrac{9D^2\beta}{2} + \sqrt{\left(\dfrac{81D^4\beta^2}{4}\right) + 4 \dfrac{9D^2\beta}{2} \cdot \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)}}{2}
                    \]
                    So 
                    \[
                        R_T \leq \dfrac{9D^2\beta}{4} + \dfrac{9D^2\beta}{4} + 2\sqrt{\dfrac{9D^2\beta}{2} \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)} = \dfrac{9D^2\beta}{2} + 2\sqrt{\dfrac{9D^2\beta}{2} \inf\limits_{x\in K} \sum\limits_{t=1}^T f_t(x)}
                    \]
                \end{proof}
%                 \begin{align*} R_T & = \sqrt{\sum_{t=1}^T|x_t-x_{t}^*|^2} \\ & \leq \sqrt{\sum_{t=1}^T 2\beta f_t(x_t)} \\ & = \sqrt{2\beta\sum_{t=1}^T f_t(x_t)} \end{align*}

% Now, we can substitute the above inequality in the given inequality and simplify:

% \begin{align*} R_T &\leq \frac{9D^2\beta}{2} + 2\sqrt{\frac{9D^2\beta}{2} \inf_{x \in K}\sum_{t=1}^T f_t(x_t)} \\ &\leq \frac{9D^2\beta}{2} + 2\sqrt{\frac{9D^2\beta}{2} \sum_{t=1}^T f_t(x_t)} \\ &\leq \frac{9D^2\beta}{2} + 2\sqrt{\frac{9D^2\beta}{2} \sum_{t=1}^T f_t(x_t^)} \\ &\leq \frac{9D^2\beta}{2} + 2\sqrt{\frac{9D^2\beta}{2} \inf_{x \in K}\sum_{t=1}^T f_t(x_t^)} \end{align*}

% % In the last step, we have used the fact that $\inf_{x \in K}\sum_{t=1}^T f_t(x_t) \leq \sum_{t=1}^T f_t(x_t^)$, since $x_t^$ is the optimizer of $f_t$ over the set $K$.

% Hence, we have shown that $R_T \leq \frac{9D^2\beta}{2} + 2\sqrt{\frac{9D^2\beta}{2} \inf_{x \in K}\sum_{t=1}^T f_t(x_t)}$, which completes the proof.





%                 % Explain why this is interesting.

%                 % To prove the inequality
%                 %     \begin{equation}
%                 %     R_T \leq \sqrt{\dfrac{9D^2\beta}{2} \sum_{t=1}^T f_t(x_t)}
%                 %     \label{eq:ineq1}
%                 %     \end{equation}
%                 %     implies
%                 %     \begin{equation}
%                 %     R_T \leq \dfrac{9D^2\beta}{2} + 2\sqrt{\dfrac{9D^2\beta}{2} \inf_{x \in K}\sum_{t=1}^T f_t(x_t)},
%                 %     \label{eq:ineq2}
%                 %     \end{equation}
%                 %     we need to use the following inequality, which holds for any non-negative numbers $a$ and $b$:
%                 %     \begin{equation}
%                 %     \sqrt{ab} \leq \dfrac{a+b}{2}.
%                 %     \label{eq:ineq3}
%                 %     \end{equation}
%                 %     Using this inequality, we can write
%                 %     \begin{align*}
%                 %     R_T &= \sqrt{\dfrac{9D^2\beta}{2} \sum_{t=1}^T f_t(x_t)} \\
%                 %     &\leq \dfrac{9D^2\beta}{2} + \sqrt{\dfrac{9D^2\beta}{2} \sum_{t=1}^T f_t(x_t)} \\
%                 %     &= \dfrac{9D^2\beta}{2} + 2\sqrt{\dfrac{9D^2\beta}{2} \cdot \dfrac{1}{T} \sum_{t=1}^T f_t(x_t)} \\
%                 %     &\leq \dfrac{9D^2\beta}{2} + 2\sqrt{\dfrac{9D^2\beta}{2} \cdot \inf_{x \in K} \sum_{t=1}^T f_t(x_t)},
%                 %     \end{align*}
%                 %     where the second line follows from inequality \eqref{eq:ineq3} and the third line follows from the inequality $\frac{1}{T}\sum_{t=1}^T f_t(x_t) \geq \inf_{x \in K} \sum_{t=1}^T f_t(x_t)$.

%                 %     Therefore, we have shown that inequality \eqref{eq:ineq1} implies inequality \eqref{eq:ineq2}. This result shows that if the losses are $\beta$-smooth and positive, then the regret of an online learning algorithm is upper bounded by a term that depends on the smoothness parameter $\beta$, the diameter of the decision set $D$, and the cumulative sum of losses incurred by the algorithm over $T$ rounds.
            \end{enumerate}

\end{document}
